{"id": "fZi8HxJbMO", "number": 1517, "cdate": 1756888830728, "mdate": 1759898204829, "content": {"title": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution", "abstract": "Real-world videos often extend over thousands of frames, posing unique demands far beyond current short benchmarks. Existing video super-resolution (VSR) approaches, however, face two persistent challenges when processing long sequences: (1) Efficiency due to the heavy cost of multi-step denoising for full-length sequences; and (2) Scalability hindered by temporal decomposition that causes artifacts and discontinuities. To break these limits, we propose InfVSR, which novelly reformulate VSR as an autoregressive-one-step-diffusion paradigm. This enables streaming inference while fully leveraging pre-trained video diffusion priors. First, we adapt the pre-trained DiT into a causal structure, maintaining  both local and global coherence via rolling KV-cache and joint visual guidance. Second, we distill diffusion process into a single step efficiently, with patch-wise pixel supervision and cross-chunk distribution matching. Together, these designs enable efficient and scalable VSR for unbounded-length videos. To fill the gap in long-form video evaluation, we build a new benchmark tailored for extended sequences, and further introduce semantic-level metrics to comprehensively assess temporal consistency. Our method pushes the frontier of long-form VSR, achieves state-of-the-art quality with enhanced semantic consistency, and delivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will be released soon.", "tldr": "A one-step diffusion based AR method for video super-resolution", "keywords": ["Video Super-Resolution", "One-Step Diffusion", "Auto-Regression"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/293b210b8a4193f4dd2941f1f881e043007386e0.pdf", "supplementary_material": "/attachment/59019e40292429d41e63f3fcf02217fb8c5b1541.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes InfVSR to tackle the difficulty of unbound-length real-world video sequences. By reformulating VSR as an autoregressive-one-step-diffusion paradigm, it supports efficient streaming inference. To maintain identity consistency across multiple chunks, it intrudces joint visual guidance and cross-chunk distribution matching to incorporate high-level semantic information. Also, it utilizes a two-stage curriculum learning paradigm for efficient training. Experiments demonstrate the effectiveness and efficiency of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* By drawing inspiration on current success in long video generation, it proposes an efficient streaming inference paradigm by autoregressive one-step diffusion model.\n* It proposes joint visual guidance and cross-chunk distribution matching to maintain identity consistency across multiple video chunks.\n* Detailed experiments demonstrate its effectiveness and efficiency."}, "weaknesses": {"value": "* For a low-level vision task, I am skeptical about the actual effectiveness of the two high-level strategies (joint visual guidance and cross-chunk distribution matching) emphasized in the model. Authors need to supplement super-resolution results related to portrait or object identities to demonstrate their effect of maintaining consistency across multiple chunks.\n* For the patch-wise pixel supervision technique in Sec 3.3, it is well known that directly decoding patch latent of smooth area to pixel space often leads to flickering results due to the reconstruction ability of VAE decoder. Have the authors observed the same phenomenon? If so, will the flickering output affect the optimization? \n* At the stage 2 of curriculum learning, low-resolution videos are used for training. Will the low-quality ground truth in stage 2 affect the overall VSR quality?\n* InfVSR only utilizes 1K clips from REDS dataset for training. However, previous VSR methods often use 100K~1M video clips to improve its generalizability. Authors need to demonstrate the effectiveness on more diverse test sets or argue why this method is data-efficient."}, "questions": {"value": "* Typo: ``loacl'' in line 182.\n* Section layout in line 238, 476 and other places are largely changed, which is forbidden in ICLR rules.\n* Authors are strongly recommended to provide a video demo or video files corresponding to the results presented in the paper. The absence of video results may leads to a lower score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zQqmDFkgYz", "forum": "fZi8HxJbMO", "replyto": "fZi8HxJbMO", "signatures": ["ICLR.cc/2026/Conference/Submission1517/Reviewer_AvyW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1517/Reviewer_AvyW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760609455143, "cdate": 1760609455143, "tmdate": 1762915789772, "mdate": 1762915789772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InfVSR, an autoregressive-one-step diffusion (AR-OSD) framework for video super-resolution (VSR). Unlike prior diffusion-based VSR models that are limited to short clips due to high memory and computational cost, InfVSR reformulates VSR as a causal autoregressive process with single-step diffusion inference. Building upon the state-of-the-art T2I diffusion model, i.e., WAN2.1-.3B, \nThe key modifications include:\n- A causal DiT architecture featuring rolling KV-cache for local temporal smoothness and joint visual guidance for global coherence.\n- A training scheme combining patch-wise pixel supervision (for efficient high-resolution detail recovery) and cross-chunk distribution matching (for long-range temporal consistency).\n- A new MovieLQ benchmark of 1000-frame real-world videos and semantic-level temporal metrics (BC, SC, MS from VBench) for long-form consistency evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow.\n- The paper presents a successful practice to build an autoregressive VSR model based on a pretrained T2I model, which has not become a major trend for current diffusion-based VSR models.\n- The proposed new benchmark, MovieLQ, may facilitate further evaluation for future works."}, "weaknesses": {"value": "- The fundamental insights of this paper are somewhat incremental. It is more like an extension of existing technologies for VSR. Specifically, the key components used in the paper, including KV-cache, causal DiT,  DMD loss, multi-stage training, etc. These make the proposed method seem kind of trivial.\n- The paper lacks an in-depth analysis of the proposed components. For example, it is well-known that casual attention may have its drawbacks compared with widely-used full attention, especially for large-scale generative models. While the paper presents positive numbers, it is unclear if the claimed improvement benefits from the proposed components or simply from the improvement of the used generative prior. After all, Wan1.3 is already much stronger than the previous generative priors used in the baselines. The theoretical analysis behind the claimed improvements is vague. Specifically, how does the autoregressive manner affect the performance of VSR compared with the non-autoregressive one? How does causal attention affect the performance of VSR?"}, "questions": {"value": "My concerns are as follows:\n\n1. The two major weaknesses above.\n\n2. In lines 251-260, if my understanding is correct, the paper proposed to calculate the loss between the decoded, cropped latent tensors and the cropped ground-truth in the pixel space. Given the padding operations as well as the receptive field of the CNN layers in the VAE, such supervision may introduce problems on the edges of the tensors.\n\n3. The author should provide more details on the proposed test benchmark MovieLQ, including how the data is collected and why it is suitable to be a benchmark for VSR test.\n\n4. It is unclear how sensitive the proposed autoregressive model is to the size of the KV-cache. Moreover, since the proposed method claims to target at very long videos, it is also unclear if the fixed-size memory can handle long-term content given an extra long video with frequent scene changes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kOjo0rbLxw", "forum": "fZi8HxJbMO", "replyto": "fZi8HxJbMO", "signatures": ["ICLR.cc/2026/Conference/Submission1517/Reviewer_xoPS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1517/Reviewer_xoPS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573427502, "cdate": 1761573427502, "tmdate": 1762915789501, "mdate": 1762915789501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new VSR framework, InfVSR, to handle long videos efficiently. InfVSR uses an autoregressive-one-step-diffusion (AR-OSD) paradigm for temporal modeling."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "### Motivation\n- It is natural to introduce autoregressive sampling idea from video generation to generative VSR task, especially given that most of the VSR models are quite heavy to run.\n\n### Method\n- Rolling KV cache and joint visual guidance is a well-designed recipe for both local and global consistency. \n- DMD loss helps semantic consistency for long video clips.\n- It is quite economic to train InfVSR as it only takes 4 A800-80G GPUs - much affordable compared to other baselines such as SeedVRs. \n\n### Experimental results\n- The paper focuses its evaluation on MovieLQ, which consists of long videos with real-world degradations.\n- InfVSR outperforms  previous methods on many metrics. \n- InfVSR shows good efficiency compared to diffusion-based methods. \n\n### Writing\nThe paper is well-written and easy to follow."}, "weaknesses": {"value": "### Method\n- Patch-wise supervision is common in many classic regression-based VSR papers, such as BasicVSR series. Although it is good to adopt it for latent space diffusion models, it is not quite clear to me to claim it as a novel technique. \n- The local temporal loss does not make sense to me as the local temporal dynamics between two adjacent frames could be very abrupt.  For example, a very large motion between two frames. Without a good alignment (e.g., flow-based warping), it might be harmful for the training. Please feel free to correct me.  \n\n### Experimental results\n- It is **extremely challenging** to see the performance of the proposed method, especially **temporal consistency**, without any video results shown in the supplementary results. Unfortunately at this moment I am inclined to reject because of this. \n- For VSR task, many commercial solutions prefer regression-based methods (e.g., RealBasicVSR and that line of works) for its fidelity and efficiency. How does the proposed InfVSR compare to those classic methods in terms of runtime?"}, "questions": {"value": "I'd strongly recommend authors to present some video results in the rebuttal stage, otherwise it is challenging to measure real performance of InfVSR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sr2d4THucW", "forum": "fZi8HxJbMO", "replyto": "fZi8HxJbMO", "signatures": ["ICLR.cc/2026/Conference/Submission1517/Reviewer_W2TZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1517/Reviewer_W2TZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619191548, "cdate": 1761619191548, "tmdate": 1762915789165, "mdate": 1762915789165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method termed InfVSR, aiming to achieve efficient and temporally-scalable diffusion-based video super-resolution. InfVSR firstly adopts a pretrained DiT into causal structure and maintaining local and global coherence with rolling KV-cache, then distills the model with distribution matching to achieve one-step diffusion inference. This paper also proposes MovieLQ, a long-sequence video benchmark to evaluate the VSR in long-term semantic-level consistency, fidelity and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe idea is easy to follow. It adapts a pretrained T2V DiT-based diffusion model to causal form. Then applies distribution matching distillation to achieve one-step diffusion inference. This is effective and efficient for the model to perform unlimited length super-resolution video prediction together with the rolling KV-cache design.\n2.\tA good and straight-forward solution on long-sequence video super-resolution. Both semantic and pixel consistencies are maintained through DMD loss and pixel-level reconstruction loss.\n3.\tA new benchmark VideoLQ is proposed to evaluate the long-sequence video super-resolution task."}, "weaknesses": {"value": "1.\tThe long-term consistency is not thoroughly discussed in the paper, e.g., what is the visual results and comparison between the SOTA methods after 10, 100 or 1000 frames? Also the visual results in the supplement seems not fidel to the GT in some text images, this seems to be brought by the generative instability. \n2.\tThe main idea of improving the efficiency of video model using DMD and causal structure is trivial since it has been adopted in many other video methods [1][2]. Some more discussions are needed to specify the novelty of this paper.\n3.\tThe InfVSR is built upon the Wan T2V model, also there are models like SeeSR[3] using text to enhance the super-resolution results. Can model achieve better result with proper text prompt or guidance?\n4.\tThe parameter compared with SOTA methods should be provided to better validate the efficiency of the proposed method.\n\n[1] Self-forcing: bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009\n[2] Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model arXiv preprint arXiv:2508.13009\n[3] Seesr: Towards semantics-aware real-world image super-resolution CVPR 2024"}, "questions": {"value": "Refer to the weaknesses. The visualization is not very persuasive. The novelty should be clearified with detailed discussion. How about using prompt to boost the SR performance and guide the semantic content?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EcmZJIZ5uq", "forum": "fZi8HxJbMO", "replyto": "fZi8HxJbMO", "signatures": ["ICLR.cc/2026/Conference/Submission1517/Reviewer_Uppd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1517/Reviewer_Uppd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837778905, "cdate": 1761837778905, "tmdate": 1762915788995, "mdate": 1762915788995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}