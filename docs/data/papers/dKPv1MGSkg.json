{"id": "dKPv1MGSkg", "number": 4739, "cdate": 1757755231505, "mdate": 1763027413616, "content": {"title": "Near-Optimal Convergence of Accelerated Gradient Methods under Generalized and $(L_0, L_1)$-Smoothness", "abstract": "We study first‐order methods for convex optimization problems with functions $f$ satisfying the recently proposed $\\ell$-smoothness condition $\\|\\|\\nabla^{2}f(x)\\|\\| \\le \\ell\\left(\\|\\|\\nabla f(x)\\|\\|\\right),$ which generalizes the $L$-smoothness and $(L_{0},L_{1})$-smoothness. While accelerated gradient descent (AGD) is known to reach the optimal complexity $\\mathcal{O}(\\sqrt{L} R / \\sqrt{\\varepsilon})$ under $L$-smoothness, where $\\varepsilon$ is an error tolerance and $R$ is the distance between a starting and an optimal point, existing extensions to $\\ell$-smoothness either incur extra dependence on the initial gradient, suffer exponential factors in $L_{1} R$, or require costly auxiliary sub-routines, leaving open whether an AGD‐type $\\mathcal{O}(\\sqrt{\\ell(0)} R / \\sqrt{\\varepsilon})$ rate is possible for small-$\\varepsilon$, even in the $(L_{0},L_{1})$-smoothness case. We resolve this open question. Leveraging a new Lyapunov function and designing new algorithms, we achieve $\\mathcal{O}(\\sqrt{\\ell(0)} R / \\sqrt{\\varepsilon})$ oracle complexity for small-$\\varepsilon$ and virtually any $\\ell$. For instance, for $(L_{0},L_{1})$-smoothness, our bound $\\mathcal{O}(\\sqrt{L_0} R / \\sqrt{\\varepsilon})$ is provably optimal in the small-$\\varepsilon$ regime and removes all non-constant multiplicative factors present in prior accelerated algorithms.", "tldr": "", "keywords": ["$(L_0", "L_1)$-smoothness", "generalized smoothness", "accelerated gradient methods", "convex optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19bee87e398c2dc85db12a50940341e9c4bc5446.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a new accelerated gradient descent method for convex optimization under a generalized $\\ell$-smoothness assumption, establishing an AGD-type complexity that improves upon existing convergence guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Originality:** Building on a previously proposed assumption and existing methods, the paper establishes an improved complexity bound.\n\n2. **Quality:** While we have concerns about the writing clarity (discussed in *Weaknesses*), the overall structure is sound and the presentation is generally clear.\n\n3. **Clarity:** The paper clearly explains its motivation and main ideas.\n\n4. **Significance:** The new result may advance $\\ell$-smooth optimization theory. However, given the limited empirical advantage over GD-based methods, it remains unclear whether the method achieves practical acceleration."}, "weaknesses": {"value": "1. The proof technique is not novel. The proposed method is a discretization of a rotated heavy-ball flow introduced in [1]. With a simple change of variables, the Lyapunov function coincides with those in [2], [3], or [4]. Modern continuous-flow–based proofs, such as [5] and [6], could simplify the argument substantially and shorten Appendix B.\n\n2. Lines 272–274 state: “While for $L$–smooth functions the proof technique from (Wei & Chen, 2025) does not offer any advantages over, for example, (Nesterov, 1983) because the result in (Nesterov, 1983) is optimal.” However, (Nesterov, 1983) is not optimal in the information-theoretic sense. There is extensive work on optimal gradient methods (OGM); see, for example, [7].\n\n3. The numerical experiments in Appendix A do not demonstrate the efficiency of the proposed method. The AGD curve shows no improvement over GD, contradicting the theoretical claims. In addition, AGD exhibits severe oscillations, suggesting that it may not achieve genuine acceleration—or even reliable convergence.\n\n**References**\n\n[1] Wei, J., & Chen, L. (2024). Accelerated Over-Relaxation Heavy-Ball Method: Achieving Global Accelerated Convergence with Broad Generalization. *ICLR*, 2025.\n\n[2] Alvarez, F., & Attouch, H. (2001). An inertial proximal method for maximal monotone operators via discretization of a nonlinear oscillator with damping. Set-Valued Analysis, 9(1–2), 3–11. https://doi.org/10.1023/A:1011203001547\n\n[3] Su, W., Boyd, S., & Candès, E. J. (2016). A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights. *Journal of Machine Learning Research*, 17(153), 1–43. http://jmlr.org/papers/v17/15-084.html\n\n[4] Wibisono, Wilson, & Jordan (2016). A variational perspective on accelerated methods in optimization. *Proceedings of the National Academy of Sciences*, 113(47), E7351–E7358. https://doi.org/10.1073/pnas.1614734113\n\n[5] Chen, L., & Luo, H. (2019). First-order optimization methods based on Hessian-driven Nesterov accelerated gradient flow. arXiv preprint arXiv:1912.09276.\n\n[6] Luo, H., & Chen, L. (2022). From differential equation solvers to accelerated first-order methods for convex optimization. *Mathematical Programming*, 195(1), 735-781. https://doi.org/10.1007/s10107-021-01713-3\n\n[7] Kim, D., Fessler, J.A. (2015). Optimized first-order methods for smooth convex minimization. *Mathematical Programming*. https://10.1007/s10107-015-0949-3"}, "questions": {"value": "1. Beyond the generalized smoothness assumption, is there an analogous assumption that extends strong convexity? Under such a condition, can the method achieve linear convergence?\n\n2. What convergence guarantees can be established when applying gradient methods to $\\ell$-smooth functions without assuming convexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NtQOHqm344", "forum": "dKPv1MGSkg", "replyto": "dKPv1MGSkg", "signatures": ["ICLR.cc/2026/Conference/Submission4739/Reviewer_MkRn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4739/Reviewer_MkRn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808658340, "cdate": 1761808658340, "tmdate": 1762917548209, "mdate": 1762917548209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose first-order methods for convex optimization problems with $\\ell$-smoothness condition: $\\|\\nabla ^2 f(x)\\| \\leq \\ell(\\|\\nabla f(x)\\|)$, where $\\ell$ is a non-decreasing, positive, locally Lipschitz function. When $\\psi(x) = \\frac{x^2}{2\\ell(4x)}$ is strictly increasing, the proposed algorithms achieve the oracle complexity of $O(\\sqrt{\\ell(0)}R/ \\sqrt{\\varepsilon})$ for $R = \\|x_0 - x^*\\|$ and small $\\varepsilon$. In particualr, with $(L_0, L_1)$-smoothness, the oracle complexity is $O(\\sqrt{L_0}R/{\\sqrt{\\varepsilon}})$."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-  The proposed algorithms improve the the oracle complexity of accerated gradient methods on a class of convex optimization problems over $\\ell$-smoothness condition. \n\n- The proofs of the main theorems are sound and well-discussed."}, "weaknesses": {"value": "- The assumption that $\\psi(x)$ is strictly increasing restricts the results mainly to $(L_0, L_1)$-smoothness.\n\n\n- The algorithms requires sophiticated choices on parameters which are usually unknown or difficult to estimate; while the optimal convegence region relies on these parameters."}, "questions": {"value": "- What is the motivation of considering the function $\\psi(x)$? \n\n- Beyond $(L_0, L_1)$-smoothness, what are the functions $\\ell$ such that the assumption $\\psi(x)$ is strictly increasing holds?\n\n\n- For Algorithm 1, what is the convegence guarantee after GD and before $\\bar k$ itertations?\n\n\n- There is a non-accelerated phase in Algorithm 1 and results in addtional constant factors in the oracle complexity. The acclerated region relies on $\\delta$ and $\\bar R$.  Should this be viewed as local accelerated convergence (requiring initial guess close to the solution)?\n\n- The paper claims the convergence rate is a significant improvement over previous works. It would be nice to have some numerical experiments compared with other AGD methods to validate the performance of the proposed algorithms.\n\n- Is the set $Q$ defined in line 441 non-empty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w1a5DRHbnb", "forum": "dKPv1MGSkg", "replyto": "dKPv1MGSkg", "signatures": ["ICLR.cc/2026/Conference/Submission4739/Reviewer_XLXR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4739/Reviewer_XLXR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827682468, "cdate": 1761827682468, "tmdate": 1762917547546, "mdate": 1762917547546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes accelerated gradient methods under $(L_0, L_1)$-smoothness condition. It presents two algorithms, one with a brief GD warm start and another that adapts step sizes without a warm start, and establish the best-known oracle complexity $\\sqrt{l(0)}R/\\sqrt{\\epsilon}$ in the small $\\varepsilon$ regime. The analysis also extends to the generalized $(L_0, L_1)$-smoothness setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "They proposed algorithm which established the best-known oracle complexity in the small $\\epsilon$ regime with tailored Lyapunov function. The results align with optimal complexity under $l$-smoothness condition and are empirically validated on a toy problem. The proof sketch is properly presented and Table 1 clearly exhibits the contribution."}, "weaknesses": {"value": "To be honest, I’m uncertain that the paper’s contribution meets the bar for acceptance at this venue. While the paper establishes a best-known bound, the guarantee is confined to the small $\\epsilon$ regime, and the constant factor improvement over Li et al. (2024a) seems somewhat incremental."}, "questions": {"value": "Is a lower-bound result established under this $(L_0, L_1)$-smoothness condition?\n\nWhat are the technical challenges in extending the analysis to arbitrary $\\epsilon$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There is no ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8qGRHkQdaq", "forum": "dKPv1MGSkg", "replyto": "dKPv1MGSkg", "signatures": ["ICLR.cc/2026/Conference/Submission4739/Reviewer_4QVg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4739/Reviewer_4QVg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983363024, "cdate": 1761983363024, "tmdate": 1762917546825, "mdate": 1762917546825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers problems that satisfy $\\ell$-smoothness conditions, which genearlize both standard and $(L_0, L_1)$-smoothness. For this setting, they achieve a $O(\\sqrt{\\ell(0)}R/\\sqrt{\\varepsilon}) + L_1^2R^2$ first-order oracle complexity to find an $\\epsilon$-approximate solution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The primary strength of the paper is that it improves the previous $\\ell(||\\nabla f(x^0)||)$ dependence to $\\ell(0)$ (up to considerations of additive terms, discussed below in \"Weaknesses\"), and this helps better place the result in the context of classic lower bound in smooth convex optimization."}, "weaknesses": {"value": "One issue is that the algorithm needs $\\Gamma_0$, $\\bar{R}$. Do the other algorithms in Table 1 require these? If not, then the results are not directly comparable, and it would then be important to explain these caveats as an additional part of the table. The authors claim (erroneously) the complexity is optimal (line 250). The authors should specify the range of $\\varepsilon$ where they claim optimality, and should emphasize the additive term which prevents them from actually being optimal. The additive term is $L\\_1^2R^2$, which can dominate for some ranges of $\\varepsilon$, $L\\_1$, $ R $, $L\\_0$. This needs clarifying for the exact range of improvements, and to point out when previous works dominate this work, for proper comparison.\n\nFollowing this, the work could benefit from providing a clearer description of why this result is important in the face of previous work, since the overall improvement seems quite slight in that it only affects the smoothness parameters (and furthermore at the cost of a potentially worse additive term in some cases), and the techniques resemble those in Vankov et al. Because of these concerns, there is some hesitance felt about whether these results are significant enough to warrant acceptance, especially in view of the caveats above yet to be completely addressed."}, "questions": {"value": "What precisely is $\\nu$ in Table 1? (Can its dependence on parameters of $f$ be elaborated on?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zbdciWYvKl", "forum": "dKPv1MGSkg", "replyto": "dKPv1MGSkg", "signatures": ["ICLR.cc/2026/Conference/Submission4739/Reviewer_2a6J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4739/Reviewer_2a6J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762019236415, "cdate": 1762019236415, "tmdate": 1762917544990, "mdate": 1762917544990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two new Accelerated GD variants designed to optimize $\\ell$-smooth functions (the generalized version of $L$-smoothness and $(L_0, L_1)$-smoothness). The authors provide convergence guarantees for their proposed methods, showing that in the small accuracy regime their methods attain near-optimal convergence of $\\mathcal{O}(\\sqrt{\\ell(0)}R/\\sqrt{\\varepsilon})$, where $R =\\\\|x_0 - x^{\\star}\\\\|$, thus imporving upon prior approaches. Between the two of them, these algorithms cover both the case of sub- and superquadratic $\\ell$. Preliminary experiments are provided."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The topic is relevant to the research community, and the paper is well-structured and well-written. To the best of my knowledge, the related literature is appropriately covered, and significant parts of the technical approach are novel. The contribution is significant for both theory and practice, since it helps delineate the reach/limitations of classical methods under generalized smoothness, and can provide practitioners in, e.g., scientific computing fields, with potentially improved tools."}, "weaknesses": {"value": "1. **Technical approach**\n\t* Assumption 2.3 states that \"$ f : \\mathbb{R}^d \\to \\mathbb{R} \\cup \\{\\infty\\} $ [...] attains its minimum at a (non-unique)\n$x^\\ast \\in \\mathbb{R}^d $ [...]\". However, none of the motivating examples in line 051 do satisfy this over $\\mathbb{R}^d$ (for the case of $x^p$, consider $p$-odd).\n\t* The method addresses contrained optimization, yet the proof of Lemma B.3 uses the result of Lemma B.1 by replacing $\\nabla f(y)$ with $\\nabla f (x^\\star)$ which is set to zero. The constrained optimum $x^\\star$ does not necessarily satisfy $\\nabla f (x^\\star) = 0$, so the result is problematic. Could you please address this and the possible ramifications of it?\n\n2. **Presentation**\n\t* A comparison between the stepsize's dependence on problem constants of Alg2 vs. [1,2,3] is missing, and would help in understanding the tradeoffs between this and previous methods.\n\n3. **Experiments**\n\t* The experiments are too simplistic, and only compare Algorithm 2 with Tyurin's GD variant (unaccelerated). The method should be compared with the Accelerated versions of [1, 2, 3]. For a fair comparison where auxiliary subroutines are concerned, convergence in terms of wall clock time should be considered. This is useful for understanding the practical behaviours of these methods relative to each other, since it is likely that despite the worse convergence upper bounds, the prior algorithms are still competitive in practice in the small $\\varepsilon$ regime.\n\t* Experiments with various degrees of overestimation for $\\bar{R}$ and $\\Gamma_0$ should be conducted to understand Alg. 2's sensitivity to hyperparameter tuning (even if the $\\varepsilon$-dependent term only depends on $R$, and not $\\bar{R}$).\n\t* Less pressing: ideally, an experiment should be included on a practically-relevant (empirically determined) $(L_0, L_1)$-loss, in order to assess the method's sensitivity to tuning in practice\n\t\n\n[1] Haochuan Li, Jian Qian, Yi Tian, Alexander Rakhlin, and Ali Jadbabaie. Convex and non-convex optimization under generalized smoothness. Advances in Neural Information Processing Systems, 36, 2024a.\n\n[2] Eduard Gorbunov, Nazarii Tupitsa, Sayantan Choudhury, Alen Aliev, Peter Richt´arik, Samuel Horv´ath, and Martin Tak´aˇc. Methods for convex (L0,L1)-smooth optimization: Clipping, acceleration, and adaptivity. In International Conference on Learning Representations, 2025.\n\n[3] Daniil Vankov, Anton Rodomanov, Angelia Nedich, Lalitha Sankar, and Sebastian U Stich. Optimizing (L0,L1)-smooth functions by gradient methods. arXiv preprint arXiv:2410.10800, 2024."}, "questions": {"value": "Please see comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rzEPqh2WxV", "forum": "dKPv1MGSkg", "replyto": "dKPv1MGSkg", "signatures": ["ICLR.cc/2026/Conference/Submission4739/Reviewer_fCE5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4739/Reviewer_fCE5"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762108086751, "cdate": 1762108086751, "tmdate": 1762917544455, "mdate": 1762917544455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}