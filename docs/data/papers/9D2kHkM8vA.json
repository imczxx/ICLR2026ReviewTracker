{"id": "9D2kHkM8vA", "number": 19980, "cdate": 1758301191032, "mdate": 1759897008606, "content": {"title": "Reducing Hallucinations in Generative Models through Truncated Statistics", "abstract": "Hallucinations—where generative models produce invalid or nonsensical outputs—remain a critical challenge for reliable deployment. We present the first computationally and query-efficient algorithm that provably addresses the hallucination problem by actively querying the model’s own invalid outputs. Specifically, we impose a strict constraint on the hallucination rate while maximizing the likelihood of valid target examples via projected stochastic gradient descent. Our method works in very general settings with arbitrary distributions parameterized by sufficiently expressive exponential families. Our approach is enabled by a novel connection to the field of truncated statistics and settles an open problem posed by Hanneke et al.~\\yrcite{pmlr-v75-hanneke18a}.", "tldr": "", "keywords": ["Query-efficient learning", "Hallucination mitigation", "Projected stochastic gradient descent (PSGD)", "Active learning / membership queries"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a695b1ff3a27d102339976d8444427c8be44a4fc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper formulates the problem of hallucination in generative models as a constrained loss minimization task. The model is treated as a probability distribution over the data space, and the authors assume access to an invalidity oracle that determines whether a generated instance constitutes a hallucination. The learning objective is to minimize the negative log-likelihood on the training data while constraining the probability mass assigned to the invalid region to remain below a prescribed threshold.\n\nThe setting is precisely the same as the valid generative modeling framework introduced by Hanneke et al. (2018). The main technical contribution is a computationally efficient algorithm for solving this constrained optimization problem. Crucially, the result holds for a class of so-called “powerful” distribution families, which can be satisfied by any general exponential family. The proof follows by reducing the constrained optimization problem to a truncated negative log-likelihood optimization problem, which allows for efficient resolution."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Although I did not verify every details of the proof due to time constraints, the technical result that establishing an efficient solution to the constrained negative-log-likelihood problem over general exponential families appears both interesting and significant.\n\n2. The paper also introduces novel conceptual and technical ideas, notably the notion of the “powerfulness” of a distribution family, which provides a useful structural condition linking expressivity to computational tractability."}, "weaknesses": {"value": "1. In line 114, the authors claim that “the assumption is mild and purely about expressivity: **any** parametric family ….” I understand that the argument is to expand the support by adding an additional element $x^\\*$ and extending the family via the construction of $p_m$. However, to establish *powerfulness*, shouldn’t the proof operate within this expanded family as well? This reasoning works for exponential families, but I do not see how it holds for arbitrary parametric families.  \n\n2. It seems to me that the proposed algorithmic approach is specifically tailored to exponential families. Can the method can be extended to more general model classes?\n\n3. Given that the paper’s main contribution is computational efficiency, it would be helpful to include some empirical validation, which shouldn't be too hard.\n\n4. Is the sentence in line 200 generated by LLM?"}, "questions": {"value": "Please address the points in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7OhVlXkcEz", "forum": "9D2kHkM8vA", "replyto": "9D2kHkM8vA", "signatures": ["ICLR.cc/2026/Conference/Submission19980/Reviewer_T5Vg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19980/Reviewer_T5Vg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776578859, "cdate": 1761776578859, "tmdate": 1762932882569, "mdate": 1762932882569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel and theoretically-grounded algorithm to mitigate hallucinations in generative models. The authors formulate the problem as a constrained optimization task: maximizing the likelihood of observed valid data while ensuring the model's hallucination rate (the probability mass assigned to a set of invalid outputs) remains below a predefined threshold `α`. The core contribution is a novel connection between this problem and the field of truncated statistics. By using a Lagrangian relaxation, the authors show that the constrained objective can be related to the negative log-likelihood of a truncated distribution, which is a well-studied convex problem for exponential families.\n\nThe paper proves that for \"powerful\" model families (a class they define, which includes exponential families), this approach leads to a computationally and query-efficient algorithm. The proposed method, based on projected stochastic gradient descent (PSGD) over sublevel sets of the loss function, is provably effective and settles an open problem posed by Hanneke et al. (2018) regarding the existence of such an algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. In a field often dominated by empirical results, this work provides rigorous, provable guarantees on computational efficiency, query efficiency, and correctness (i.e., achieving the target hallucination rate). Proving that the problem is tractable under the \"powerful model\" assumption, while being NP-hard in general, is a solid theoretical contribution.\n2. A key strength is the paper's novel and principled formulation. By framing hallucination reduction as a constrained optimization problem and connecting it to truncated statistics, the authors leverage established mathematical tools to develop a rigorous solution."}, "weaknesses": {"value": "1. The theoretical guarantees hinge on the \"powerful model\" assumption, primarily exemplified by exponential families. This creates a significant gap, as it is unclear whether complex Transformer architectures satisfy this property, limiting the direct applicability of the theory to state-of-the-art models.\n2. The work is entirely theoretical and lacks empirical experiments, even on synthetic data. This absence makes it difficult to assess the algorithm's practical performance and whether its polynomial complexity is feasible for large-scale applications.\n3. The framework presumes access to a perfect and cost-free \"invalidity oracle.\" This overlooks the practical challenges of implementing such an oracle, which is often a noisy, biased, and expensive component (e.g., human feedback or another model) in real-world systems.\n4. The algorithm's multi-level structure, involving nested optimization loops and projections, appears complex to implement and tune. The practical difficulties of managing these components, particularly the projection onto sublevel sets, may hinder its adoption compared to simpler methods."}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tcKX9Z9Jg4", "forum": "9D2kHkM8vA", "replyto": "9D2kHkM8vA", "signatures": ["ICLR.cc/2026/Conference/Submission19980/Reviewer_eVZL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19980/Reviewer_eVZL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818128799, "cdate": 1761818128799, "tmdate": 1762932881927, "mdate": 1762932881927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper formulates hallucination reduction as likelihood maximization under an explicit upper bound on the probability of invalid generations. It connects this constrained objective to truncated statistics and proposes a projected SGD–based procedure that actively queries invalid outputs, operating over expressive exponential families. The authors prove computational/query efficiency and provide conditions (“powerful” models) under which proper learning achieves a target hallucination rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The work gives a statistically, query-, and computationally efficient proper learner that drives the hallucination rate to any target \\alpha addressing the efficiency question posed by Hanneke et al. (2018).\n\n2) A key theoretical contribution is the novel bridge established between the generative model hallucination problem and the field of truncated statistics. By reformulating the constrained optimization via a Lagrangian relaxation, the objective function can be mapped to a truncated negative log-likelihood, making the problem tractable and solvable with convex optimization techniques for certain model families.\n\n3) The authors develop a concrete algorithm that is proven to work for \"powerful\" exponential families, a class of models shown to be sufficiently expressive for this task. The algorithm is efficient, requiring only a polynomial number of invalidity queries and computation time to find a solution that meets the desired hallucination rate while remaining near-optimal in data likelihood."}, "weaknesses": {"value": "The paper is entirely theoretical and does not include any experiments to validate its claims. While the theoretical contributions are strong, the work would be significantly strengthened by empirical results, even on a synthetic toy model. Such experiments could demonstrate the algorithm's practical behavior, verify that it achieves the target hallucination rate, and provide insight into its performance in a controlled setting."}, "questions": {"value": "1) The paper's theoretical results are derived for exponential families, which creates a gap between this setting and the large-scale models like LLMs that motivate the work. Could the authors add a discussion on the primary challenges or potential paths for extending this framework to more general architectures, such as Transformers?\n\n2) Could you report the empirical results on some widely-used hallucination benchmark datasets to validate the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BkkqnwqpzT", "forum": "9D2kHkM8vA", "replyto": "9D2kHkM8vA", "signatures": ["ICLR.cc/2026/Conference/Submission19980/Reviewer_d5VF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19980/Reviewer_d5VF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887980326, "cdate": 1761887980326, "tmdate": 1762932881195, "mdate": 1762932881195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}