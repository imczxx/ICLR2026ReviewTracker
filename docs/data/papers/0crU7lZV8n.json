{"id": "0crU7lZV8n", "number": 4819, "cdate": 1757771714834, "mdate": 1762932646949, "content": {"title": "Rank-Preserving Calibration of LLMs Under Model and Distribution Shifts", "abstract": "A central barrier to deploying Large Language Models (LLMs) in safety-critical applications is hallucination, where models generate non-factual content with high confidence. Detecting hallucinations requires well-calibrated confidence estimates, yet calibration is brittle under domain and model shifts. The former renders confidence estimates unreliable in a new environment, while the latter arises because different LLMs exhibit distinct confidence scales, so calibration learned for one model often fails to transfer when another is used at deployment for efficiency or privacy. Addressing this vulnerability is critical for robust model generalisation, as failure to calibrate reliably confidence values across domains and models undermines trust in LLMs at deployment.  Existing prompting-based approaches are label-free and flexible, they perform poorly when domain knowledge of a model is limited. In contrast, explicit calibration for a specialized domain achieves strong in-domain results but fails to generalize to a novel domain. This work discovers although absolute confidence values often fail to transfer across shifts, their relative rankings, which only rely on the relative reliability among samples within a dataset, can prevail in robustness across shifts. Based on this key insight, we propose a two-stage framework Rank-preserving Adaptive Peudo-Calibration (RAPCal). In source calibration, an Expectation-Maximisation stage converts one-hot correctness labels into soft supervisions by bin-wise accuracy estimations for a fine-grained calibration. In target calibration, a stage for preserved ranking of confidence scores is introduced to construct pseudo soft labels, enabling unsupervised cross-domain calibration adaptation without ground-truth labels in test domains. Experiments show that RAPCal reduces ECE by 6.15\\% without sacrificing task performance, advancing the reliability of LLMs in label-scarce settings. Code is given in the supplementary materials.", "tldr": "A calibration method for LLMs under both model and domain shift.", "keywords": ["transfer learning; Question Answering; Cross-domain calibration"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7af6ff705f520b2ac18e12b1bda8b6184cb15d76.pdf", "supplementary_material": "/attachment/045686dac4c857837e425ddeedcd2bd7ab96a023.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes RAPCal (Rank-Preserving Adaptive Pseudo-Calibration), a two-stage framework for calibrating LLMs under both domain and model shifts, even when target-domain labels are unavailable and the source model is black-box.\nRAPCal first performs source pre-calibration using an EM-based refinement that converts binary correctness labels into soft calibration supervision. It then performs target unsupervised calibration by transforming the rank order of predicted confidences into pseudo soft labels via a U-shaped mapping function, enforcing rank consistency across shifts. Empirical results on open-ended QA datasets (TriviaQA → SciQA, etc.) show consistent improvements in ECE, Brier score, and AUROC across model backbones (Llama2–Phi3). The method is theoretically grounded via a rank-stability analysis and achieves label-free calibration that maintains task accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. provide a framework to learn a calibrated confidence\n2. good results"}, "weaknesses": {"value": "1. There are only two benchmark included, this can cause several problems.\n- Authors mention that ranking preserve the uncertainty. However, is this hold for all domain shift benchmarks? We can hardly know with one domain shifted dataset results.\n- like previous domain adaption methods, the adaptation should be verified on multiple OOD benchmark\n2. The Multiple Choice QA performance not included. This is important when people use LLM for a classification task, which is often more related to traditional calibration. I agree the open QA is important, but Multiple Choice QA benchmarks should also be needed.\n3. Does this method keep the accuracy? this should be verified on more datasets.\n4. The design framework is a little bit complex, do we really need that many trainable networks? is there some better signal other than ranking? logit gap might be a good choice for domain shift, see [1]\n5. I will vote yes if you could extend the experiments to a scale that is more reasonable.\n\n[1] Sample Margin-Aware Recalibration of Temperature Scaling"}, "questions": {"value": "1. Why does the lora exist, I don't quite get it. You mentioned in a black box scenario."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zbniHoj9pR", "forum": "0crU7lZV8n", "replyto": "0crU7lZV8n", "signatures": ["ICLR.cc/2026/Conference/Submission4819/Reviewer_VWDo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4819/Reviewer_VWDo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761389376610, "cdate": 1761389376610, "tmdate": 1762917593237, "mdate": 1762917593237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of cross-domain confidence calibration for large language models (LLMs) in open-ended QA tasks. It proposes a two-stage calibration framework called Rank-preserving Adaptive Pseudo-Calibration (RAPCal). In the first stage, RAPCal uses labeled data from the source domain to pre-calibrate the LLM through an EM-based refinement process, after which the calibrated model is provided to the unlabeled target domain as a black box. In the second stage, RAPCal constructs pseudo target labels based on the relative ranking information of the initial target predictions from the source-calibrated LLM. Finally, RAPCal trains a new calibration head using the pseudo-labeled target data. Experiments involving two types of LLMs and two cross-domain QA tasks demonstrate the effectiveness and advantages of RAPCal."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The black-box model calibration setting is practical but challenging, especially for realistic LLM applications. This paper’s investigation of this problem is valuable and should be encouraged.\n\n- The proposed RAPCal framework is intuitively sound and general, capable of addressing both dataset shift and model shift.\n\n- Experiments and ablation studies on two open-ended QA benchmarks demonstrate the effectiveness of RAPCal and its advantages over existing LLM calibration baselines."}, "weaknesses": {"value": "- Literature review is incomplete and partially inaccurate. Since the paper investigates the dataset shift problem in the context of LLMs, the discussion of relevant topics such as domain adaptation should be accurate and comprehensive. However, several key related works are either misrepresented or omitted.\nFirst, in Lines 142–144, “More recent studies in black-box domain adaptation (BBDA) (Liang et al., 2020a; Kundu et al., 2020)” is inaccurate. The two cited works are actually pioneering studies on source-free domain adaptation, which corresponds to a white-box setting where the source data is hidden from the target domain but the source model is accessible.\nIn contrast, works such as [1, 2] actually address the black-box domain adaptation setting that aligns with this paper’s focus—where the source model is only available for inference on target data.\nSecond, a previous ICML paper [3] proposed PseudoCal to tackle cross-domain calibration in both white-box and black-box domain adaptation settings. This relevant work should be discussed, especially in relation to Line 146, which currently states that “calibration under domain shift has received little attention.”\n\n- Hyperparameter tuning issue. RAPCal involves multiple hyperparameters (Lines 470–472 and Figure 4), each of which must be carefully tuned to achieve effective calibration. However, the studied setting is highly challenging, as the target domain is entirely unlabeled and the source data is inaccessible. It remains unclear how the authors determine reasonable or near-optimal values for these hyperparameters in an unsupervised way when applying RAPCal to new cross-domain or cross-model calibration tasks.\n\n- Practicality concern. RAPCal requires a standard pre-calibration pipeline on the source domain, which may be impractical in real-world scenarios—users typically cannot request model vendors to perform custom pre-calibration for their released LLM APIs.\n\n- Experimental scope. The current experiments are limited to two QA benchmarks. To better demonstrate the generality and robustness of RAPCal, evaluations on additional benchmarks or diverse tasks would be beneficial.\n\nReferences: \n\n- [1] DINE: Domain Adaptation from Single and Multiple Black-box Predictors. CVPR 2022\n- [2] Unsupervised Domain Adaptation of Black-Box Source Models. arXiv 2021\n- [3] Pseudo-Calibration: Improving Predictive Uncertainty Estimation in Unsupervised Domain Adaptation. ICML 2024"}, "questions": {"value": "In addition to the main concerns mentioned in the weaknesses, I note the following question about the presentation:\n\n- Figure 3 (c) and (d) are difficult to distinguish because their only difference lies in the terms “generalization” and “adaptation.” The caption should be made clearer and more detailed.\n\n- Table 3 is unclear due to the absence of baseline results, such as those without adaptation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OMwpqXIkv2", "forum": "0crU7lZV8n", "replyto": "0crU7lZV8n", "signatures": ["ICLR.cc/2026/Conference/Submission4819/Reviewer_DwKq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4819/Reviewer_DwKq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838203173, "cdate": 1761838203173, "tmdate": 1762917592847, "mdate": 1762917592847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "d1yucCnbRr", "forum": "0crU7lZV8n", "replyto": "0crU7lZV8n", "signatures": ["ICLR.cc/2026/Conference/Submission4819/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4819/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762932331041, "cdate": 1762932331041, "tmdate": 1762932331041, "mdate": 1762932331041, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RAPCal, a rank-preserving calibration method for LLMs under distribution shifts. The method aims to improve calibration without requiring labeled target samples and is evaluated on open-domain QA transfer tasks. Results show improved calibration metrics compared to baseline approaches, though the evaluation focuses on a narrow QA setting and uses coarse sequence-level representations. Overall, the work extends traditional calibration ideas to LLM adaptation and provides preliminary evidence of effectiveness in limited scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Proposes a simple, computationally efficient adaptation method applicable to black-box LLMs.\n- Extends traditional calibration concepts to the LLM setting.\n- Empirically improves calibration metrics."}, "weaknesses": {"value": "1. The paper motivates calibration as a way to mitigate hallucination, but never measures hallucination in the experiments. In fact, it is not clear to me how a calibrated model can have less hallucinations.\n2. The setting is extremely niche. It's basically a binary-classification, source-free UDA problem framed under LLM+calibration setting. I do not think this setting is impactful enough.\n3. The confidence head is an MLP on the mean hidden state across the generated answer tokens (averaged over token embeddings) and outputs a single confidence score. This discards sequence structure, token-level uncertainty, and decoding dynamics; it’s unclear that such a blunt summary can generalize across prompts, lengths, or style, and hence I do not see this being practically useful.\n4. Target-stage performance depends on multiple hyperparameters ($\\alpha,\\beta,\\tau$). While the sensitivity curves are shown, how are these chosen without peeking at correctness in a label-free target? There is no unsupervised selection criterion.\n5. Writing & quality issues. Unclear sentence: \"resulting in adopting in-domain calibration adaptation invalid\". \"reliability diagrams\" unclear. 'black box' uses single quotation. Notation $y_{i1},\\ldots,y_{im_i}$ is confusing. Figure 3 and 4 are too small to read."}, "questions": {"value": "- Could you provide empirical evidence as well as explanation about calibration reducing hallucinations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2vmXdk4p9P", "forum": "0crU7lZV8n", "replyto": "0crU7lZV8n", "signatures": ["ICLR.cc/2026/Conference/Submission4819/Reviewer_5WWD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4819/Reviewer_5WWD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863684006, "cdate": 1761863684006, "tmdate": 1762917592553, "mdate": 1762917592553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses confidence calibration for large language models (LLMs) under model and distribution shifts, where calibration performance often deteriorates.\nThe proposed method, RAPCal, trains a lightweight calibration head on labeled source-domain data via an EM-like refinement and adapts it to a target domain without labels through rank-preserving pseudo-labeling.\nExperiments on QA datasets (TriviaQA, SciQA) using Llama2 and Phi-3 demonstrate improved ECE, Brier Score, and AUROC over existing baselines."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper tackles calibration under domain/model shift, an under-explored but practically important scenario.\n2. The idea that relative ranking of confidence is more stable than absolute values is interesting and supported both empirically and theoretically (rank-flip bound)."}, "weaknesses": {"value": "1. The proposed EM-based calibration procedure closely mirrors the soft-label smoothing mechanism introduced by Liu et al. (EMNLP 2024), which also computes bin-level expected accuracies and iteratively refines confidence targets. The only difference lies in the explicit EM formulation and the addition of rank-based weighting. The authors should clarify whether this represents a conceptual reinterpretation of that method or a genuinely novel optimization scheme.\n2. Several recent works (e.g., Liu et al., 2024; Kapoor et al., 2024) already employ a linear head on frozen LLM activations to predict confidence under soft-label or ECE-based supervision. The paper lacks the necessary citations and discussion of these related approaches.\n3. The experiments compare only with older logit-based methods (Platt Scaling, Temperature Scaling, Apricot). Without including recent activation-based, head-based, or answer-based baselines (e.g., Ulmer et al., 2024), the reported improvement margins are not convincing.\n4. In Table 1/2, the paper cites three methods, Seq. Likelihood (Kamath et al., 2020), Apricot (Zhao et al., 2021) and ActCab (Si et al., 2022), as baseline approaches.\nHowever, after searching through ACL 2020, NeurIPS 2021/2022 proceedings, arXiv, and Google Scholar, I was unable to locate any published papers matching these titles, authors, or venues:\n- Kamath et al. (2020) “Selectivity in Question Answering: No Free Lunch in QA” (claimed ACL 2020)\n- Zhao et al. (2021) “Calibrating Language Models with Multiple Choice Questions” (claimed NeurIPS 2021)\n- Si et al. (2022) “Actively Calibrated Selective Classification” (claimed NeurIPS 2022)\n\nCould the authors please clarify the provenance of these citations?\nIf these works are unpublished, internally circulated, or fabricated placeholders (e.g., generated by an LLM or citation autocomplete), that should be clearly stated and corrected.\nCurrently, these references undermine the credibility of the reported baselines.\n\n5. The study evaluates only two datasets (TriviaQA and SciQA). Given the paper’s central claim of domain/model-shift robustness, this limited scope substantially weakens the generality of the conclusions.\n\n\nReference:\n1. Liu et al., 2024, Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding.\n2. Kapoor et al., 2024, Large Language Models Must Be Taught to Know What They Don't Know.\n3. Ulmer et al., 2024, Calibrating Large Language Models Using Their Generations Only."}, "questions": {"value": "1. How is the EM refinement in Algorithm 1 different from the “expected confidence label” approach used in ActCab (Liu et al., 2024)?\nIf it is mathematically equivalent, please clarify and cite accordingly."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "The paper cites several works that appear to be non-existent or unverifiable in the claimed venues. Specifically,\n- Kamath et al. (2020) “Selectivity in Question Answering: No Free Lunch in QA” (claimed ACL 2020),\n- Zhao et al. (2021) “Calibrating Language Models with Multiple Choice Questions” (claimed NeurIPS 2021), and\n- Si et al. (2022) “Actively Calibrated Selective Classification” (claimed NeurIPS 2022)\n\ncannot be found in the official ACL or NeurIPS proceedings, arXiv, or Google Scholar.\n\nThese fabricated or unverifiable references call into question the validity of the reported baselines and raise potential research integrity concerns.\nAn ethics review may be necessary to verify the authenticity of these citations and ensure that the experiments were conducted on real, published baselines."}}, "id": "R2xr0FNrbJ", "forum": "0crU7lZV8n", "replyto": "0crU7lZV8n", "signatures": ["ICLR.cc/2026/Conference/Submission4819/Reviewer_shSu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4819/Reviewer_shSu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044405633, "cdate": 1762044405633, "tmdate": 1762917592291, "mdate": 1762917592291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}