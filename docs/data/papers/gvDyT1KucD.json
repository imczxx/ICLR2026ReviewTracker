{"id": "gvDyT1KucD", "number": 14387, "cdate": 1758234280076, "mdate": 1759897373515, "content": {"title": "ELAS: Efficient Pre-Training of Low-Rank Large Language Models via 2:4 Activation Sparsity", "abstract": "Large Language Models (LLMs) have achieved remarkable capabilities, but their immense computational demands during training remain a critical bottleneck for widespread adoption.  Low-rank training has received attention in recent years due to its ability to significantly reduce training memory usage. Meanwhile, applying 2:4 structured sparsity to weights and activations to leverage NVIDIA GPU support for 2:4 structured sparse format has become a promising direction. To achieve efficient pre-training of LLMs, this paper proposes ELAS: $\\textbf{E}$fficient pre-training of $\\textbf{L}$ow-rank LLMs via 2:4 $\\textbf{A}$ctivation $\\textbf{S}$parsity, a novel framework for low-rank models via 2:4 activation sparsity. ELAS applies squared ReLU activation functions to the feed-forward networks in low-rank models and implements 2:4 structured sparsity on the activations after the squared ReLU operation. We evaluated ELAS through pre-training experiments on LLaMA models. The results demonstrate that ELAS maintains performance with minimal degradation after applying 2:4 activation sparsity, while achieving training and inference acceleration. Moreover, ELAS reduces activation memory overhead—particularly with large batch sizes. Code will be made available.", "tldr": "This paper proposes a 2:4 activation sparsity framework for low-rank language models that accelerates training and inference while reducing memory consumption with minimal performance loss.", "keywords": ["2:4 activation sparsity; Low-rank model; Large Language Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/74ae714981fbd23489f9f129be388e7034feada6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "ELAS introduces an interesting and potentially valuable combination of low-rank optimization and activation-level 2:4 sparsity. However, the work remains immature in its current form. The empirical validation is confined to pre-training perplexity without any downstream or transfer evaluation, leaving it unclear whether the proposed sparsity scheme preserves generalization and linguistic capability. The claimed activation memory savings appear to have limited system-level significance under realistic checkpointed pre-training regimes, where weights and optimizer states dominate overall memory usage. Furthermore, the analysis lacks end-to-end runtime profiling, robustness studies, and sensitivity analyses. Overall, the paper reads more as a promising architectural exploration than as a fully substantiated efficiency framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1, The paper presents an original combination of low-rank optimization and structured activation sparsity, bridging two previously orthogonal strategies for improving training efficiency in large language models.\n\n2, ELAS is conceptually straightforward, implementable with minimal architectural changes, and compatible with existing GPU hardware supporting 2:4 sparsity.\n\n3, The work offers consistent pre-training results across multiple model scales (60M–1B) and includes detailed ablations on sparsification methods and warmup dynamics."}, "weaknesses": {"value": "1, The paper provides limited analysis of how low-rank parameterization interacts with structured activation sparsity in shaping representational subspaces or optimization dynamics, leaving open questions about the method’s stability, convergence behavior, and general applicability.\n\n2, The reported activation memory savings are localized to feed-forward layers and may not translate to meaningful end-to-end reductions in realistic large-scale pre-training pipelines, where weights and optimizer states dominate total memory consumption.\n\n3, The study validates ELAS primarily through validation perplexity without downstream or transfer assessments, leaving uncertainty about whether the proposed method preserves generalization, linguistic understanding, and reasoning capabilities beyond pre-training optimization metrics.\n\n4, The empirical results emphasize theoretical speedups and local kernel performance but lack comprehensive wall-clock or multi-node scaling studies, making it unclear how effectively ELAS translates algorithmic sparsity into real-world throughput gains."}, "questions": {"value": "Q1: The claimed memory benefit in ELAS arises from reduced activation storage via 2:4 sparsity. However, in modern text-only LLM pre-training pipelines (e.g., Megatron-LM, FSDP, ZeRO-3), activations are typically recomputed through checkpointing, making weights and optimizer states the dominant contributors to memory usage.\nCould the authors quantify the actual end-to-end memory reduction under realistic checkpointed training, and clarify whether such activation-level savings provide practical benefits beyond layer-local memory?\nGiven that activation tensors are far larger and less checkpointed in multimodal or vision–language models, it would be helpful to discuss whether ELAS’s activation sparsity might be more beneficial in such architectures compared to text-only LLMs.\nFor very long-text inputs, the computational and memory cost of self-attention grows quadratically with sequence length, often dominating total training and inference cost. In this regime, activation sparsity in FFN blocks contributes comparatively little to overall efficiency.\nCould the authors discuss whether ELAS provides tangible benefits for long-context settings, and whether its 2:4 activation sparsity could complement sparse or efficient attention mechanisms (e.g., Longformer, FlashAttention, or Mamba) to achieve end-to-end scalability?\n\nQ2: The 2:4 sparsity is enforced by magnitude-based masking within each group of four activations. While this guarantees structural compliance, it may distort feature distributions and gradient statistics early in training. Have the authors measured the deviation between natural ReLU² sparsity and the enforced 2:4 mask, or quantified how much additional information is lost due to forced block alignment?\n\nQ3: Reported speedups (up to 2.75×) focus on the feed-forward blocks. However, the end-to-end runtime of LLMs also depends heavily on attention kernels, optimizer overhead, and communication latency.\nCould the authors provide wall-clock measurements or total-iteration throughput to demonstrate how much overall training or inference time ELAS saves compared to dense or low-rank baselines?\n\nQ4: The dense warmup and subsequent structured sparsification introduce non-trivial training phases. How sensitive is ELAS to the number of warmup steps, and does applying sparsity earlier or later materially change convergence stability or final perplexity? A stability or convergence plot could clarify whether the method is robust across different schedules.\n\nQ5: ELAS integrates two orthogonal efficiency dimensions, low-rank parameterization and activation sparsity. In contrast, many contemporary large-model architectures (e.g., Mixture-of-Experts, sparse attention, and adaptive routing) already exploit run-time or input-dependent sparsity at the architectural level.\nCould the authors elaborate on whether the proposed 2:4 structured activation sparsity can generalize to, or seamlessly coexist with, such runtime-sparse designs, particularly MoE models, where the active subnetwork varies dynamically across tokens?\n\n\nQ6: The paper evaluates optimization quality primarily through validation perplexity, yet perplexity alone may not sufficiently capture broader aspects such as generalization, calibration, or downstream task performance, particularly under architectural modifications like replacing SwiGLU with ReLU² and introducing STE-based activation sparsification.\nCould the authors clarify why perplexity is considered a sufficient proxy for optimization progress and overall model capability in this setting, and whether improvements in perplexity reliably translate into functional gains?\nSince ELAS focuses on algorithmic and hardware efficiency but provides limited evidence of preserved downstream functionality, it would be valuable to include few-shot or zero-shot evaluations across representative NLP understanding, generation, and reasoning tasks to confirm that perplexity improvements correspond to tangible performance retention.\n\nQ7: The STE is used to approximate gradients through the non-differentiable 2:4 sparsification operation. However, STE introduces a bias–variance trade-off in gradient estimation, especially when masking alters activation distributions across layers.\nHave the authors analyzed or bounded the gradient bias introduced by STE in the context of structured activation sparsity, and how does this affect optimization stability compared to low-rank training alone?\nA visualization or quantitative analysis of gradient norms before and after sparsification could strengthen the justification.\n\nQ8: ELAS performs dual compression, low-rank factorization on weights, and structured sparsification on activations. While low-rank parameterization already constrains updates to a limited subspace, activation sparsity further prunes the feature subspace during both forward and backward passes.\nCould the authors clarify whether these two compression mechanisms interact constructively or redundantly in shaping the model’s representational capacity? In particular, does applying 2:4 activation sparsity reduce the effective expressiveness of the low-rank projections or compound information loss, given that both constrain the flow of information in orthogonal domains?\nFurthermore, to isolate the specific contribution of activation sparsity, it would be important to compare ELAS with a lower-rank baseline (e.g., smaller rank-r LORO) that achieves a similar memory or computational footprint purely through rank reduction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XO8Vp97RcV", "forum": "gvDyT1KucD", "replyto": "gvDyT1KucD", "signatures": ["ICLR.cc/2026/Conference/Submission14387/Reviewer_zwgy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14387/Reviewer_zwgy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761078918242, "cdate": 1761078918242, "tmdate": 1762924804097, "mdate": 1762924804097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ELAS, a method that jointly leverages low-rank decomposition and structured activation sparsity to improve training and inference efficiency in large language models. ELAS first applies Low-rank Riemannian Optimization (LORO) to compress weight matrices, reducing parameter memory. It then uses Squared-ReLU activations to naturally produce high activation sparsity and enforces magnitude-based 2:4 structured sparsity during the forward. They conduc experiments on LLaMA models (60M–1B) trained on C4, showing competitive perplexity versus full-rank baselines while achieving meaningful speedups and memory reduction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed idea to utilize structed activation sparsity is natural.\n2. The paper is well written."}, "weaknesses": {"value": "1. The paper aims to improve pre-training efficiency through 2:4 activation sparsity. However, it includes LoRA as a pre-training baseline, which is confusing since LoRA is typically employed for fine-tuning, not pre-training. The authors should clarify the rationale for this comparison and explain how LoRA is adapted or reinterpreted in their experimental setup.\n\n2. Table 1 reports perplexity as the main evaluation metric. However, the models are trained on a relatively small scale. For example, the 1B-parameter model is trained with only 10B tokens. In addition, zero-shot or few-shot downstream accuracy results are missing, which limits understanding of the model’s generalization performance.\n\n3. While Table 1 presents end-to-end memory savings and Table 3 reports FFN module speedup, the paper does not provide an overall end-to-end speedup measurement. This omission makes it difficult to assess the practical impact of the proposed method on real-world inference performance.\n\n4. The proposed method is applied only to the FFN’s intermediate activations. It would be valuable to discuss whether this approach can be extended to all linear layers and to provide corresponding end-to-end speedup and memory savings under this broader setting."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VvNNUoJL22", "forum": "gvDyT1KucD", "replyto": "gvDyT1KucD", "signatures": ["ICLR.cc/2026/Conference/Submission14387/Reviewer_vbVF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14387/Reviewer_vbVF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761289317864, "cdate": 1761289317864, "tmdate": 1762924803717, "mdate": 1762924803717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method that combines 2:4 sparsification with low-rank architecture. This method involves applying a 2:4 sparsification mask to the activations before the down projection in MLP layers, and it reduces activation memory consumption. Results on language models up to 1B with 13B training tokens show that the sparsification operation leads to small performance decrease, and considerable training efficiency gains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clear. This paper tackles a real challenge that has practical value, since it reduces the training cost (and possibly inference costs) of language models.\n- The proposed method sees considerable gain the activation memory usage reduction."}, "weaknesses": {"value": "- The authors have not clearly explained the numbers reported in Table 1. Particularly, it is unclear why the number of parameters is fewer for CoLA, LORO, and ELAS. If these models indeed have fewer model parameters, why is it appropriate to compare them against larger baselines? Moreover, since the methods compared against have different model architectures, I think the authors should have reported the breakdown of memory usage and compute (FLOPs) of each method. This is essential for understanding the pros and cons of each method.\n- Instead of just comparing the activation memory of ELAS against LORO (in Table 2), the authors should first describe and calculate the memory consumption during both training and inference, and how large proportion of it is occupied by the activations. Also, they should have presented the numbers for a standard Transformer model (without low-rank decompositions) as well, and possibly other appropriate baseline methods. This is necessary for understanding the significance of the method.\n- The number of training tokens for each model size is too low. Nowadays, it is common to train language models with a data-to-parameter ratio beyond 100. At the minimum, the authors should have used 20 tokens per parameter, which is the compute-optimal ratio determined by the Chinchilla law.\n- Table 3: How does this speed compare against LORO? This seems like an essential baseline that is omitted. Moreover, the authors also omitted important information for replicating this experimental result. For instance, what hardware was used and how many of them? What is the batch size? What codebase was used for measuring the inference speed?\n- Table 1: I think it is inappropriate to bold the text in the bottom row of the table, as this confuses the reader to believe that this score outperforms the baselines.\n- The models should have been evaluated with downstream tasks such as HellaSwag, ARC-easy, ARC-challenge, Winogrande, etc., in addition to just language modeling loss.\n- It seems that the proposed method is simply to apply 2:4 sparsification to the activations on top of LORO during training. Are there any unique challenges about the combination of 2:4 sparsification and LORO? It is currently unclear about the differences of this work and existing works.\n- While the authors claim that the performance compromise caused by 2:4 sparsification is small (and partly confirmed by the results in Table 1), there is still a perplexity increase compared to LORO. It is currently unclear how much of an impact this perplexity difference is. One way to better grasp this performance compromise is to train a baseline model that is smaller using LORO (without 2:4 sparsification), such that its inference costs (memory and FLOPs usage) is aligned with the proposed method (ELAS). Then, we can check if ELAS outperforms this smaller baseline model. Either way, the authors should provide some empirical results that more clearly show the performance gap between LORO and ELAS other than language modeling perplexity."}, "questions": {"value": "- Many citations (but not all of them) lack brackets. The authors should have used `\\citep` instead of `\\cite` or `\\citet`.\n- Table 1: If ELAS reduces the activation memory consumption, why does this table report that it uses the same amount of memory as LORO?\n- There is an extra quotation mark at the end of the captions for Table 1.\n- Title of Section 4.2.3 seems wrong.\n- I think Section titles in this paper are improperly capitalized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iBnidqrHI1", "forum": "gvDyT1KucD", "replyto": "gvDyT1KucD", "signatures": ["ICLR.cc/2026/Conference/Submission14387/Reviewer_oe2L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14387/Reviewer_oe2L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723851048, "cdate": 1761723851048, "tmdate": 1762924803190, "mdate": 1762924803190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on efficient LLM pre-training by combining low-rank and 2:4 sparsity. Experimental results indicate that the proposed approach can preserve quality while enhancing resource efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper discusses an important issue related to reducing the resource efficiency of LLM pretraining.\n- Efforts have been made to improve the proposed algorithm's efficiency using Triton kernels.\n- Extensive experiments have been conducted at various scales."}, "weaknesses": {"value": "1. In Table 1, the authors indicate that their data-to-parameter ratio is below 20 for some methods (as suggested by the chinchilla scaling laws, although it often goes higher these days). If resources are limited (which is understandable), I recommend the authors try training smaller models with a sufficient token budget.\n\n2. The focus on activation memory without mentioning activation checkpointing (aka re-materialization or re-computation) seems a bit odd. Also, a bit minor, but it's unclear to me why combining low-rank and sparse methods for pretraining -- is this more efficient than making it lower-rank (but dense) or more sparse but full rank?\n\n3. The speedup results could be improved. For example, activation sparsity might boost training speed, but this isn't discussed—only inference speedup is mentioned. Also, speedups are only measured on FFN layers, which is a somewhat limited view. Finally, the \"inference\" setting is unclear; from Table 3, it seems the speedup is measured in a prefill setting or with speculative decoding (and can be somewhat interpreted as training speedup, but correct me if I misunderstood)."}, "questions": {"value": "Could the authors elaborate on the necessity of compressing activation? GaLore's Figure 1 indicates that activation size is relatively small, though it clearly increases with sequence length and batch size.\n\nIn Table 1, what is the significance of the bold numbers? Are they simply used to highlight results from their method? As far as I can tell, they are not necessarily better.\n\nI believe that activation reduction could be beneficial for longer sequences as well, not just for larger batch sizes. The authors are encouraged to explore this in the next version of the draft. This is only a suggestion and does not influence my ratings.\n\nAlso, there is a typo in the title of Section 4.2.3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xI5mCMZeXA", "forum": "gvDyT1KucD", "replyto": "gvDyT1KucD", "signatures": ["ICLR.cc/2026/Conference/Submission14387/Reviewer_E1Mu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14387/Reviewer_E1Mu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762043923208, "cdate": 1762043923208, "tmdate": 1762924802814, "mdate": 1762924802814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}