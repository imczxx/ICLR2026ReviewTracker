{"id": "zEdZEjNTwf", "number": 1321, "cdate": 1756870562309, "mdate": 1759898215365, "content": {"title": "4DLangVGGT: 4D Language Visual Geometry Grounded Transformer", "abstract": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose *4DLangVGGT*, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. \nExperiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to **2%** gains under per-scene training and **1%** improvements under multi-scene training.\nOur code released in https://anonymous.4open.science/r/4dlangvggt.", "tldr": "", "keywords": ["Spatial Intelligence", "4D Semantic Feed-Forward Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b7f056aa894307e5d12003870f11853cb65e781.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes 4DLangVGGT, a Transformer-based feed-forward framework for 4D language grounding in dynamic scenes. The method addresses the limitations of existing approaches, which typically rely on per-scene optimization (e.g., Gaussian Splatting) and thus suffer from poor scalability and generalization. 4DLangVGGT uses a frozen, pre-trained StreamVGGT as a geometry encoder and introduces a novel, trainable Semantic Bridging Decoder (SBD). The SBD maps spatio-temporal geometric features into a language-aligned semantic space. A key advantage of this architecture is its ability to be trained jointly across multiple scenes and applied directly at inference without per-scene optimization. Experiments on the HyperNeRF and Neu3D datasets show that 4DLangVGGT achieves state-of-the-art performance, outperforming previous methods in both per-scene and multi-scene training setups."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Interesting Problem**: The paper tackles a critical and practical limitation of current 4D semantic field models: their reliance on costly and non-generalizable per-scene optimization. The proposed feed-forward, generalizable framework is a valuable contribution toward scalable 4D scene understanding.\n\n2. **Strong Performance**: The method achieves state-of-the-art results on two standard benchmarks (HyperNeRF and Neu3D). It consistently outperforms the primary baseline, 4DLangSplat, in both time-agnostic and time-sensitive query settings.\n\n3. **Well-motivated Architecture**: The core idea of decoupling the architecture into a frozen geometry encoder (StreamVGGT) and a trainable Semantic Bridging Decoder (SBD) is sound. This design efficiently leverages powerful pre-trained geometry representations while focusing training on the novel task of aligning geometry with language semantics."}, "weaknesses": {"value": "1. **Missing Efficiency Analysis**: The central motivation for the feed-forward design is to avoid \"costly per-scene optimization\" and achieve \"efficiency\". However, the paper provides no quantitative comparison of efficiency. There is no analysis of training time, inference speed (e.g., FPS), or memory footprint compared to the 4DLangSplat baseline. This is a critical gap; without this data, the claimed efficiency advantage is unsubstantiated.\n\n2. **Limited Scale of Generalization**: The core claim of generalization is tested on very small datasets. The paper notes joint training across 6 scenes in HyperNeRF and 6 scenes in Neu3D , and the authors acknowledge this limitation in the appendix. Demonstrating generalization on such a small number of scenes is not a robust validation of a \"scalable\" framework intended for \"large-scale deployment\".\n\n3. **Missing Ablation on Encoder**: The choice to keep the StreamVGGT encoder frozen is a major architectural decision that is not ablated. Freezing is justified for efficiency, but it may also create a bottleneck by preventing the geometry features from adapting to the semantic task. An ablation comparing a frozen encoder versus a fine-tuned one is necessary to understand this trade-off and validate the design choice.\n\n4. **Missing Ablation on Semantic Loss**: The semantic supervision is complex, combining time-agnostic CLIP embeddings with time-sensitive MLLM-generated embeddings. The paper fails to ablate the contributions of these two components. It is unclear how much performance is gained from the expensive MLLM-based dynamic supervision versus using only the simpler static CLIP features. This ablation is essential to justify the complexity of the ground-truth generation pipeline.\n\n5. **Marginal Per-Scene Gains**: While the method is SOTA, its performance gains in the direct per-scene comparison (which is the baseline's intended setting) are sometimes marginal. For instance, on HyperNeRF for time-sensitive queries, the improvement over 4DLangSplat is only +0.03% in Accuracy and +0.8% in vIoU. This suggests that while its generalization is novel, its raw performance in a constrained setting is not a significant leap.\n\n6. **Omission of Related Work**: *Mask Grounding for Referring Image Segmentation* (Chng et al., CVPR 2024) introduces a mask-grounding task for RIS—predicting masked textual tokens given visual features and a segmentation mask—which directly overlaps with your time-agnostic/time-sensitive mask-conditioned language supervision (i.e., region-aligned per-pixel semantic targets inside SAM/DEVA masks); this prior work should be discussed and, where feasible, compared against to clearly situate novelty and empirical gains."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oujOnlAvMf", "forum": "zEdZEjNTwf", "replyto": "zEdZEjNTwf", "signatures": ["ICLR.cc/2026/Conference/Submission1321/Reviewer_1e88"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1321/Reviewer_1e88"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761028932958, "cdate": 1761028932958, "tmdate": 1762915735033, "mdate": 1762915735033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces 4DLangVGGT, a Transformer-based, feed-forward unified framework for 4D language grounding. The core idea is to leverage the strong geometric priors of StreamVGGT and augment it with two heads to enhance semantic modeling. The Semantic Head is supervised by frozen foundation models (SAM, CLIP, and an LLM) to strengthen object-level semantics and temporal consistency. The RGB Head reconstructs per-frame images to preserve perceptual fidelity. Experiments show improvements on both time-agnostic and time-sensitive benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework is clearly structured: semantic capacity is enhanced by adding lightweight heads to a VGGT backbone with targeted semantic supervision.\n2. The paper is well written with clear figures, facilitating readability.\n3. Extensive experiments are conducted under both time-agnostic and time-sensitive settings, with reasonable ablations and visualizations."}, "weaknesses": {"value": "1. The main contribution, semantic supervision in the Semantic Head, is under-analyzed. The paper lacks a thorough breakdown of how the two supervision signals contribute individually, as well as component-wise ablations within the time-sensitive semantic supervision.\n2. Compared to 4DLangSplat, the performance gains appear modest: Tables 1 and 3 seem nearly saturated with +0.1–0.2 improvements, and Table 2 reports only +1.7. (I may be less familiar with these benchmarks; please correct me if I’ve misread.)\n3. Using LLM-based caption features for time-sensitive supervision may be questionable: captions provide global semantics without sufficiently fine-grained temporal signals, and cross-modal alignment could be nontrivial."}, "questions": {"value": "1. Please provide ablations isolating the two semantic supervision signals, and component-wise ablations for the time-sensitive semantic supervision.\n2. Please explain why the improvements over 4DLangSplat are limited, despite the added supervisory signals.\n3. In light of Weakness #3, please justify the use of LLM caption features for time-sensitive supervision or provide experiments demonstrating their effectiveness.\n4. The DPT component lacks empirical analysis, please include experiments quantifying its contribution.\n5. Can you report inference speed comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7ftrpHsKnf", "forum": "zEdZEjNTwf", "replyto": "zEdZEjNTwf", "signatures": ["ICLR.cc/2026/Conference/Submission1321/Reviewer_q5cc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1321/Reviewer_q5cc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763670678, "cdate": 1761763670678, "tmdate": 1762915734814, "mdate": 1762915734814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal of this paper is to introduce a transformer-based approach for constructing 4D semantic fields.\n\nThe proposed approach aligns the spatio-temporal representation of dynamic scenes with semantic fields through time-sensitive and time-agnostic semantic supervision. The pipeline consists of a 4D visual geometry encoder (frozen StreamVGGT) and a DPT-based semantics alignment decoder with RGB and semantic heads. It is trained by minimizing joint reconstruction and the semantic objective.\n\nThe contributions of this work are as follows:\n1) state-of-the-art approach for open-vocabulary 4D understanding;\n2) multi-objective training pipeline;\n3) ablation study of the architecture."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) State-of-the-art results on open-vocabulary 4D scene understanding benchmarks.\n2) Aproach does not require per-scene optimization.\n3) Well-structured, easy to read, and follow manuscript."}, "weaknesses": {"value": "1) Description of how open-vocabulary 4D querying that is missing in the manuscript.\n2) As mentioned in limitations section training and evaluation is conducted on small dataset (HyperNeRF and Neu3D). \n3) Moreover training and evaluation is performed on the same data. In other words the approach was evaluated on the data it was trained on. Thus though approach supports multy-scene training the cross scene/cross dataset generalization is not performed."}, "questions": {"value": "1. How how open-vocabulary 4D querying is done?\n2. L503-504: Supplementary materials are not attached. \n3. L058: \"achieving both efficiency and strong generalization\" Strong generalization to training data (across training scenes)? Have you performed cross scene/cross dataset evaluation (training on the one sсene/dataset and evaluating on the other)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QwyBIZZXsq", "forum": "zEdZEjNTwf", "replyto": "zEdZEjNTwf", "signatures": ["ICLR.cc/2026/Conference/Submission1321/Reviewer_bJWm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1321/Reviewer_bJWm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905259460, "cdate": 1761905259460, "tmdate": 1762915734644, "mdate": 1762915734644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}