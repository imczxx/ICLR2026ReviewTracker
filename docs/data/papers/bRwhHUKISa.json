{"id": "bRwhHUKISa", "number": 7575, "cdate": 1758028223783, "mdate": 1763715195453, "content": {"title": "Differentiable Average Precision Loss in DETR", "abstract": "Average Precision (AP) and mean AP (mAP) remain the dominant metrics for evaluating object detectors, yet most training objectives optimize surrogates that are not well aligned with these ranking-based measures.\nMost prior AP surrogates either rely on pairwise comparisons, incurring quadratic complexity, or only optimize classification, necessitating additional localization losses. We introduce differentiable average precision (DAP) loss, a smooth AP loss that directly optimizes a differentiable approximation to COCO-style mAP for one-to-one detectors.\nOur key idea is to (i) replace non-differentiable sorting by modeling detection scores as continuous distributions and sweeping a series of thresholds to obtain a differentiable precision–recall curve, and (ii) use interpolation techniques to optimize localization task.\nThis yields a differentiable mAP approximation with linear time ($(O(N))$) in the number of predictions, enabling seamless integration with Hungarian matching.\nWe prove that, with respect to prediction scores, the gradient of DAP is sign-consistent—positive for positives and negative for negatives.\nEmpirically, fine-tuning pretrained DETR-family models with DAP for a small number of epochs delivers consistent COCO mAP gains without auxiliary losses or architectural changes.\nDAP is simple to implement, computationally efficient, reduces hyperparameter tuning, and bridges the gap between training and evaluation for one-to-one detection. \nFrom-scratch training also delivers modest but positive improvements, albeit smaller than those obtained through fine-tuning.", "tldr": "", "keywords": ["AP loss; Object Detection; Rank-based loss"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da6a5613a29a6930200cfc930598a1357d7fd395.pdf", "supplementary_material": "/attachment/90bad9763384cf32d8715a05c57a1bcaa89d14ec.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors study the problem of training an object detector by using the performance measure (Average Precision -- AP) as the training objective. This is not the first time that this is attempted. Compared to what has been studied in the literature before, the authors consider mAP, the average of AP over different IoU thresholds. This sets the paper significantly apart from prior work since it includes both classification and localization aspects. mAP is not differentiable, similar to AP. Therefore, the authors introduce some approximations to obtain the proposed loss's derivatives. \n\nThe loss function is used to train DETR-based detectors and evaluated on the COCO dataset. Some noticable improvements are reported over DETR, despite unpromising results on other detectors.\n\n*Disclaimer*: I reviewed the same paper for ICLR 2025 and CVPR 2025. My review is updated based on the changes made by the authors."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ It is worthwhile to use performance measures as training objectives for object detectors.\n+ It is promising to adapt mAP as a loss function.\n+ I acknowledge the novelty of the DAP loss formulation/approximation compared to existing ranking-based losses.\n+ Compared to the previous versions, this time the authors bring forth the benefit of having linear time complexity. This is a big plus since existing methods suffer from quadratic complexity."}, "weaknesses": {"value": "Weaknesses:\n\nAlthough I am extremely fond of ranking-based training of object detectors, I strongly believe that the paper still has many issues, despite the revisions performed after ICLR 2025 and CVPR 2025 submissions:\n\n1. A limitation of the proposed approach is that is can provide improvements only for finetuning and it performs subpar on training from scratch. The authors improved on this regard (both in terms of experiments and how they motivate their contributions). Although it is not ideal, I accept this as a limitation of a novel method/approach that can be addressed maybe in the future with another study.\n\n2. \"We prove that, with respect to prediction scores, the gradient of DAP is sign-consistent—positive for positives and negative for negatives.\" => Although this is highlighted as a significant concern, there is only a single paragraph in Section 3.4 on this.\n\n3. Contrast with pairwise ranking surrogates: \"Under SmoothAP Brown et al. (2020) with temperature τ = 0.01, we empirically observe ∂ SmoothAP/∂st < 0, i.e., the objective pushes down the score of a true positive (verified numerically).\" => Since gradient descent multiplies the gradient with (-1), this should not have been an issue. The short and vague depiction in this paragraph is questionable.\n\n4. Section 3.3: Please talk about alternative approaches to addressing the non-differentiability here.\n\n5. There are too many typos or writing issues (please see Minor comments). I would have expected a more refined manuscript after so many revisions.\n\n\nMinor comments:\n\n- Abstract: \"((O(N)))\" => extra parantheses.\n- \"The Parameterized AP Loss Chenxin et al. (2021)\" => If citations are not a part of the text, they should be enclosed within parantheses as \"The Parameterized AP Loss (Chenxin et al., 2021)\". There is a separate cite command in Latex for this.\n- Please see the following guide for writing equations: https://wp.optics.arizona.edu/kupinski/wp-content/uploads/sites/91/2023/05/MerminEquations.pdf\n- Fig 1 has subcaptions for the subfigures but not for the figure itself.\n- \"threshold; The value G\" => \"threshold. The value G\".\n- Eq 1: What is x?\n- Eq 1: ∂Rα(x)/∂x => Given that Rα(x) is a univariate function, why don't you use dRα(x)/dx? That would also make the derivation in the equation easier to follow.\n- Eq 1: I would add one more step/explanation on ∂Rα(x)/∂x relating it to ΔT. \n- Eq 4: I would replace H() with the indicator function to simplify the equations.\n- \"IoU,the optimization\" => \"IoU, the optimization\".\n- \"According to Equation 5, L(·) is determined by 10 points\" => Equation 4?\n- \"It is clear that within the range of IoU ∈ [0, 1], the gradient of function L(·) with respect to IoU either does not exist or is zero.\" => Please explicitly state that this is because of the H() function in Equation 5.\n- \"Figure 1b, We construct\" => \"Figure 1b, we construct\".\n- \"continuous probabilistic distribution\" => \"continuous probability distribution\".\n- Eq 7: What is y? \"fb(·) is the probability distribution for b\" => But, what is the parameter here?\n- \"When fb(·) is represented as an impulse function δ(·), the equation above becomes equivalent to the original model.\" => Which equation? Please show how.\n- \"fb(·)are\" => \"fb(·) are\"."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wstBEASxsM", "forum": "bRwhHUKISa", "replyto": "bRwhHUKISa", "signatures": ["ICLR.cc/2026/Conference/Submission7575/Reviewer_9XdG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7575/Reviewer_9XdG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811044714, "cdate": 1761811044714, "tmdate": 1762919669908, "mdate": 1762919669908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (1/2): Performance Source, Baselines"}, "comment": {"value": "General Response \nWe thank all reviewers for their constructive feedback. We are encouraged by the recognition of our $O(N)$ complexity and the novelty of our differentiable formulation. We provide new experiments and clarifications below to address the common concerns regarding baselines, generalization, and the source of performance gains.\n\n**Q1: Is Improvement from Continued Training?**\n\nA major concern was whether the gains stem from extended training. As stated in **Line 379 and Sec. 4.1.1** of the submission, the \"Ori\" results in Table 1 already report the result of fine-tuning with the original loss  (reporting $max (r_{pretrain}, r_{finetuned})$).\nSince the baseline models are already converged, further fine-tuning with the original loss typically yields negligible gain or even overfitting. We provide detailed data below to explicitly show this behavior:\n\n**Table A: Detailed Breakdown of Continued Training (COCO val2017)**\n\n| Method | Config | Loss Function | Epochs | mAP | Change |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Co-DETR-R50** | Pre-trained | - | 0 | 49.5 | - |\n| | Continued | Original (Focal+L1+GIoU) | +4 | 49.9 | +0.4 |\n| | **Ours** | **DAP** | +4 | **50.8** | **+1.3** |\n| **Co-DETR-SwinB**| Pre-trained | - | 0 | 57.5 | - |\n| | Continued | Original | +4 | 57.3 | -0.2 |\n| | **Ours** | **DAP** | +4 | **58.1** | **+0.6** |\n\nConclusion: The gains are strictly attributable to the DAP objective bridging the train-eval gap, not merely training time.\n\n**Q2: Why do training epochs vary across models?**\n\nRegarding the varying epoch numbers in Table 1, we clarify our experimental setting as described in Line 371: we typically select the fine-tuning duration to be approximately **10%** of the pre-training length, capped at a maximum of **12 epochs**. This constraint is intentional, aligning with our objective to provide an efficient post-hoc optimization strategy that delivers significant performance gains with minimal additional computational cost.\n\n**E1: Comparison with Other Ranking Losses**\n\nWe fine-tuned the Co-DETR-R50 baseline using other ranking losses (AP-Loss and aLRP Loss) for 4 epochs.\n\n**Table B: Comparison with Ranking Losses (COCO val2017)**\n\n| Method | Complexity | mAP |\n| :--- | :--- | :--- | \n| **Baseline** | - | 49.9 |\n| **AP-Loss** | $O(N^2)$ | 49.5 |\n| **aLRP Loss** | $O(N^2)$ | 50.2 | \n| **DAP-Loss (Ours)**| **$O(N)$** | **50.8** | \n\n**Analysis:** DAP outperforms both AP-Loss and aLRP Loss while achieving linear complexity. We attribute this to the optimization mechanism: AP-Loss and aLRP typically compute gradients based on the error signal from the optimal ranking (\"error-driven\"), whereas DAP directly backpropagates gradients through the smooth approximation of the mAP itself, facilitating better local optimization of the mAP surface.\n\n⁡**E2:  Multi-Threshold AP-Loss (Requested by Reviewer M6wQ)**\nWe further investigated if simply averaging AP-Loss over multiple thresholds could match DAP. We constructed `AP-Loss-multiou` (mean of AP-Loss at thresholds 0.5, 0.55...0.95, plus L1/GIoU).\nWe also conducted an investigative experiment `AP-Loss-multiou_APiou`, where we replaced the L1/GIoU gradient with the localization gradient **derived from DAP** (Eq. 12, assuming impulse function $f_{b}(\\cdot)$), attempting to combine AP-Loss (classification) with DAP's localization logic.\n\nThe gradient used is:\n$$\n\\frac{\\partial DAP}{\\partial b_{iou}} = \\frac{\\partial L_{int}(b_{iou}, b_{scores})}{\\partial b_{iou}}\n$$\n**Table C: Multi-Threshold Ablation**\n\n| Method | Configuration | mAP |\n| :--- | :--- | :--- |\n| **AP-Loss** | ori AP-Loss+ L1/GIoU | 49.5 |\n| **AP-Loss-multiou** | Multi-Thresholds + L1/GIoU | 49.8 |\n| **AP-Loss-multiou\\_APiou**| Multi-Thresholds + **DAP Loc. Gradient** | 48.7 |\n| **DAP-Loss** | **Unified Differentiable Approx.** | **50.8** |\n\n**Insight**: While multi-threshold AP-Loss improves slightly over single-threshold, it still lags behind DAP. Interestingly, mixing gradients (AP-Loss-multiou_APiou) degrades performance. This confirms that the classification and localization approximations in DAP are coupled and consistent, whereas mixing different approximation paradigms (Error-based AP-Loss + Gradient-based DAP) leads to optimization conflicts."}}, "id": "gXOoaybjtG", "forum": "bRwhHUKISa", "replyto": "bRwhHUKISa", "signatures": ["ICLR.cc/2026/Conference/Submission7575/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7575/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7575/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763729484782, "cdate": 1763729484782, "tmdate": 1763730021441, "mdate": 1763730021441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (1/2): Performance Source, Baselines"}, "comment": {"value": "**Note: We have updated the manuscript PDF. All major revisions are highlighted in blue for your convenience.**\n\nWe thank all reviewers for their constructive feedback. We are encouraged by the recognition of our $O(N)$ complexity and the novelty of our differentiable formulation. We provide new experiments and clarifications below to address the common concerns regarding baselines, generalization, and the source of performance gains.\n\n**Q1: Is Improvement from Continued Training?**\n\nA major concern was whether the gains stem from extended training. As stated in **Line 379 and Sec. 4.1.1** of the submission, the \"Ori\" results in Table 1 already report the result of fine-tuning with the original loss  (reporting $max (r_{pretrain}, r_{finetuned})$).\nSince the baseline models are already converged, further fine-tuning with the original loss typically yields negligible gain or even overfitting. We provide detailed data below to explicitly show this behavior:\n\n**Table A: Detailed Breakdown of Continued Training (COCO val2017)**\n\n| Method | Config | Loss Function | Epochs | mAP | Change |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Co-DETR-R50** | Pre-trained | - | 0 | 49.5 | - |\n| | Continued | Original (Focal+L1+GIoU) | +4 | 49.9 | +0.4 |\n| | **Ours** | **DAP** | +4 | **50.8** | **+1.3** |\n| **Co-DETR-SwinB**| Pre-trained | - | 0 | 57.5 | - |\n| | Continued | Original | +4 | 57.3 | -0.2 |\n| | **Ours** | **DAP** | +4 | **58.1** | **+0.6** |\n\nConclusion: The gains are strictly attributable to the DAP objective bridging the train-eval gap, not merely training time.\n\n**Q2: Why do training epochs vary across models?**\n\nRegarding the varying epoch numbers in Table 1, we clarify our experimental setting as described in Line 371: we typically select the fine-tuning duration to be approximately **10%** of the pre-training length, capped at a maximum of **12 epochs**. This constraint is intentional, aligning with our objective to provide an efficient post-hoc optimization strategy that delivers significant performance gains with minimal additional computational cost.\n\n**E1: Comparison with Other Ranking Losses**\n\nWe fine-tuned the Co-DETR-R50 baseline using other ranking losses (AP-Loss and aLRP Loss) for 4 epochs.\n\n**Table B: Comparison with Ranking Losses (COCO val2017)**\n\n| Method | Complexity | mAP |\n| :--- | :--- | :--- | \n| **Baseline** | - | 49.9 |\n| **AP-Loss** | $O(N^2)$ | 49.5 |\n| **aLRP Loss** | $O(N^2)$ | 50.2 | \n| **DAP-Loss (Ours)**| **$O(N)$** | **50.8** | \n\n**Analysis:** DAP outperforms both AP-Loss and aLRP Loss while achieving linear complexity. We attribute this to the optimization mechanism: AP-Loss and aLRP typically compute gradients based on the error signal from the optimal ranking (\"error-driven\"), whereas DAP directly backpropagates gradients through the smooth approximation of the mAP itself, facilitating better local optimization of the mAP surface.\n\n⁡**E2:  Multi-Threshold AP-Loss (Requested by Reviewer M6wQ)**\nWe further investigated if simply averaging AP-Loss over multiple thresholds could match DAP. We constructed `AP-Loss-multiou` (mean of AP-Loss at thresholds 0.5, 0.55...0.95, plus L1/GIoU).\nWe also conducted an investigative experiment `AP-Loss-multiou_APiou`, where we replaced the L1/GIoU gradient with the localization gradient **derived from DAP** (Eq. 12, assuming impulse function $f_{b}(\\cdot)$), attempting to combine AP-Loss (classification) with DAP's localization logic.\n\nThe gradient used is:\n$$ \\frac{\\partial DAP}{\\partial b_{iou}} = \\frac{\\partial L_{int}(b_{iou}, b_{scores})}{\\partial b_{iou}} ,$$\n**Table C: Multi-Threshold Ablation**\n\n| Method | Configuration | mAP |\n| :--- | :--- | :--- |\n| **AP-Loss** | ori AP-Loss+ L1/GIoU | 49.5 |\n| **AP-Loss-multiou** | Multi-Thresholds + L1/GIoU | 49.8 |\n| **AP-Loss-multiou\\_APiou**| Multi-Thresholds + **DAP Loc. Gradient** | 48.7 |\n| **DAP-Loss** | **Unified Differentiable Approx.** | **50.8** |\n\n**Insight**: While multi-threshold AP-Loss improves slightly over single-threshold, it still lags behind DAP. Interestingly, mixing gradients (AP-Loss-multiou_APiou) degrades performance. This confirms that the classification and localization approximations in DAP are coupled and consistent, whereas mixing different approximation paradigms (Error-based AP-Loss + Gradient-based DAP) leads to optimization conflicts."}}, "id": "gXOoaybjtG", "forum": "bRwhHUKISa", "replyto": "bRwhHUKISa", "signatures": ["ICLR.cc/2026/Conference/Submission7575/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7575/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7575/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763729484782, "cdate": 1763729484782, "tmdate": 1763734081881, "mdate": 1763734081881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a differentiable average precision (DAP) loss designed for training object detectors to more directly maximise the mean Average Precision (mAP) metric used in evaluation. It avoids non-differentiable sorting (as used in the standard mAP metric calculation) by treating detection scores as continuous distributions, which enables the direct optimization of a smooth approximation of the precision-recall curve. Evaluated on COCO 2017, the proposed method shows consistent mAP improvements when fine-tuning DETR-style models (versus standard training losses), and also small gains even when training from scratch."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The overall goal of training object detectors specifically for the important evaluation metric of mean average precision is worthwhile, and the method proposed to do so is interesting, in particular the relaxation of sorting by replacing with a distributional interpretation of scores.\n\nThe proposed differentiable relaxation of mAP differs from existing relaxations in the literature, and (unlike existing methods) is specialised for DETR-style models with a one-to-one box matching stage. It separately considers both classification and localisation aspects of the mAP objective.\n\nResults on COCO 2017 show a performance gain in terms of mAP compared with the same (DETR-style) models trained with standard losses. This performance gain is strongest in the case of fine-tuning for mAP following a standard pretraining phase; however there is still a gain even when training for mAP from scratch.\n\nThe method is somewhat flexible – results show it applied to several modern DETR-style transformer-based detection architectures including the original DETR, Deformable-DETR, RT-DETR, and Rank-DETR (which is perhaps the most similar in spirit in terms of how it is trained). The accuracy improvement due to the proposed method is fairly consistent across all architectures considered.\n\nThere are some experiments varying hyperparameters (e.g. batch size and matching cost function); these justify some design decisions.\n\nSome limitations are explicitly discussed in Sec 5, notably the restriction to DETR-type architectures (with a matching stage), and the somewhat weaker performance of from-scratch training (though I do not consider this is a serious limitation in practice).\n\nThe paper is clear, well-structured, and pleasant to read."}, "weaknesses": {"value": "The related work mis-represents some existing works in a way that makes the proposed method sound significantly more innovative than it is. In particular, Song 2016 does not in fact propose a differentiable approximation to mAP; instead it uses mAP itself in a loss-augmented setup similar to classic structured SVMs. Meanwhile, Henderson 2016 *does* in fact consider the localisation aspect of the mAP metric, not just classification, contrary to the statement at line 107. Moreover it also accounts for the full NMS procedure in the loss calculation.\n\nThere is no experimental comparison against well-established plug-and-play methods that are specifically designed for training for mAP, specifically Henderson 2016 and Song 2016. Those methods are architecture agnostic and as such can be applied to DETR. This is a very important baseline to include, since otherwise there is no evidence that the proposed approach to using mAP as loss is in fact better than these much older approaches. There is also no comparison to direct REINFORCE (or variance-reduced score-function gradient) methods that are applicable in this setting, and do not require significantly more technical machinery than the proposed approach.\n\nExperimental results are only given on COCO 2017. It would be interesting to see how well the method works on other domains such as remote sensing, or on other establish natural image benchmarks such as OpenImages or even VOC 2012.\n\nThe training runs were apparently truncated at a fixed threshold of epochs; it is unclear what effect this has on the proposed method versus the baseline methods. Instead all models should be trained to validation-set convergence, to ensure there is no 'late learning' stage that can impact the ranking of different approaches.\n\nThere is no theoretical analysis / discussion of circumstances under which the proposed differentiable relaxation of mAP approximates the true global function, in particular in the minibatch setting. When used as a metric, mAP performs ranking over the entire dataset; this is significantly different to the training setting; it is unclear whether the proposed loss is an unbiased estimator of either non-differentiable minibatch mAP, non-differentiable full-dataset mAP, or differentiable full-dataset mAP."}, "questions": {"value": "Most relevant issues are discussed under \"Weaknesses\" above. In particular…\n\nGiven that established, architecture-agnostic methods for optimizing mAP like Henderson 2016 and Song 2016 exist, as well as REINFORCE-type gradient estimators for the metric itself, please add these as baselines.\n\nConsider adding at least one further dataset, preferably somewhat different in characteristics to COCO 2017. Also properly discuss the implications of stopping training before convergence, preferably showing the baselines do not improve after this point.\n\nThe proposed loss is a differentiable approximation of mAP calculated on minibatches, whereas the true mAP metric is a global ranking over the entire dataset. Can you provide some analysis showing the minibatch-based loss is a sound proxy (i.e. unbiased etc.) for the true, non-differentiable, full-dataset mAP?\n\nThe \"ablation\" experiments are not ablations in the proper sense (i.e. removing novel components of the model to show how important they are to performance); instead they are just varying certain hyperparameters. Perhaps this subsection should be renamed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dh5gkm1QSf", "forum": "bRwhHUKISa", "replyto": "bRwhHUKISa", "signatures": ["ICLR.cc/2026/Conference/Submission7575/Reviewer_mniJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7575/Reviewer_mniJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848338691, "cdate": 1761848338691, "tmdate": 1762919669536, "mdate": 1762919669536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a differentiable loss function to optimize  average precision (AP) as a training loss for transformer-based object detectors, DETRs in particular.  While prior work has explored using AP as the training objective, it typically focused on AP at a fixed IoU threshold (e.g., 0.5). In contrast, this paper targets COCO-style AP, which incorporates localization across multiple IoU thresholds. To this end, the authors propose differentiable approximations for both the localization and classification components of AP. For localization, they replace the step function in the IoU-vs-precision relationship (Figure 2) with a linear interpolation. For classification, they approximate the number of true positives within a score interval using the cumulative distribution function of a Gaussian. Experiments on COCO show that when the proposed loss is used to fine-tune the already trained model, performance slightly improves across several DETR variants."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Unifying training and evaluation objectives in object detection is an important problem. This paper addresses this problem by proposing a differentiable approximation of average precision (AP) to enable its direct use as a training loss for object detectors."}, "weaknesses": {"value": "There are several issues with this paper:\n\nFirst, an important baseline is missing. Since the paper approximates COCO-style AP (i.e., the average of AP across multiple IoU thresholds), it is natural to ask how existing AP-based losses such as AP Loss or Smooth-AP perform when optimized at different IoU thresholds. These comparisons are essential to contextualize the claimed improvements.\n\nSecond, the improvements shown in the main table (Table 1) are generally small (less than 1 AP points). How do we know whether the improvement is due to the DAP loss and not due to continued training of the base detector? Simply training the base detector further could improve the performance. \n\nThird, the proposed loss does not appear to perform well when training detectors from scratch. Prior works on AP-based losses (e.g., AP Loss, Smooth-AP) have demonstrated this capability. This remains a major empirical weakness.\n\nFourth, the paper lacks a direct comparison with Smooth-AP, which is conceptually very similar. The proposed probabilistic classification approximation closely resembles Smooth-AP. A theoretical and empirical discussion contrasting the method with prior differentiable AP formulations, such as AP Loss and aLRP Loss, is needed to clarify the contribution.\n\nFinally, the approximation of the step-wise localization function L is not rigorously defined. Figure 2 suggests a linear interpolation, but the text does not specify the exact formulation. A precise mathematical definition should be provided.\n\nOther minor issues: \nAbout the definition of AP given at line 150: If I am not mistaken, the PR curve is formed by applying a systematic thresholding to the confidence score of the object detector, not by changing IoU thresholds.  It is the COCO style AP (not the PR curve itself), which calculates the average of APs for different IoU thresholds. In fact, Equation 2 supports my view. \n\nAP is not a metric. Metric has a well-definition in mathematics. I think we can call AP a measure."}, "questions": {"value": "- How do we know whether the improvement is due to the DAP loss and not due to continued training of the base detector with its original loss?\n- Second row in table 1 has both Ori Loss and DAP loss checked. what does this mean? is this a typo?\n- It is not clear how many epochs the fine-tuning is done. Does the epoch column in table 1 show that? If yes, what does it mean for the Ori Loss rows? Do you fine tune with Ori Loss with that many epochs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "V7mUZ3rXZU", "forum": "bRwhHUKISa", "replyto": "bRwhHUKISa", "signatures": ["ICLR.cc/2026/Conference/Submission7575/Reviewer_M6wQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7575/Reviewer_M6wQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940143686, "cdate": 1761940143686, "tmdate": 1762919669072, "mdate": 1762919669072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes Differentiable Average Precision (DAP), a smooth, efficient loss that directly optimizes COCO-style mAP for one-to-one detectors like DETR and its variants, narrowing the gap between training objectives and evaluation metrics. DAP replaces non-differentiable sorting with continuous score distributions (Gaussian instantiation) and applies piecewise-linear interpolation for localization, achieving O(N) time without pairwise comparisons and integrating seamlessly with Hungarian matching. It also guarantees sign-consistent gradients (positive for positives, negative for negatives) under mild assumptions. Empirically, fine-tuning DETR-family models for a few epochs consistently improves COCO mAP without architectural changes or auxiliary losses."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed DAP loss cleverly bridges the gap between the evaluation metric in detection task, and the original BCE/L1 loss. \n2. DAP achieves linear time complexity by eliminating pairwise comparisons and thus can integrate naturally with Hungarian matching in DETR.\n3. The experiments are conducted in strong baselines, like Co-DETR, Rank-DETR.\n4. Extensive results show its effectiveness."}, "weaknesses": {"value": "1. The key difference between existing AP losses needs to be thoroughly discussed in the related work section.\n2. The comparsion with related works is missing, e.g. Parameterized AP Loss (Tao et al, NeurIPS 2022).\n3. As the author say in Sec 5, DAP loss is designed for one-to-one matching detector. However, H-DETR (DETRs with Hybrid Matching, CVPR 2023) can do one-to-many matching, still use bipartite matching. Adding the results on H-DETR would extend the scope of this manuscript.\n\nMinor\n1. Missing caption in Figure 1. A global caption is missing, only two sub-caption.\n2. Each formula should have a comma or full stop at the end."}, "questions": {"value": "1. Why the post-training epoch varies for different models in Tab. 1?\n2. Can the proposed method extend to non-DETR detectors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l9u57NruzB", "forum": "bRwhHUKISa", "replyto": "bRwhHUKISa", "signatures": ["ICLR.cc/2026/Conference/Submission7575/Reviewer_UHGD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7575/Reviewer_UHGD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001414304, "cdate": 1762001414304, "tmdate": 1762919668473, "mdate": 1762919668473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}