{"id": "mVCsM9xJMd", "number": 20662, "cdate": 1758308723480, "mdate": 1759896965367, "content": {"title": "TALES: Text Adventure Learning Environment Suite", "abstract": "Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 20% on games designed for human enjoyment. Visualization of the experiments can be found at https://github.com/tale-suite/tale-suite-anonymized.", "tldr": "Unified text-adventure game benchmark with qualitative analysis of top performing game logs", "keywords": ["Benchmark", "text-adventure games", "LLM Agents"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/452acbda45bd025e857b8e9948d695629f122e2a.pdf", "supplementary_material": "/attachment/7c293fb66c45658dcad65c3add41ed85b8ac9a4e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces TALES, a unified benchmark spanning five text-adventure frameworks, with minimal scaffolding to probe raw, compositional reasoning. The authors evaluate 42 models in a zero-shot setting and find that while systems perform well on synthetic environments, they struggle markedly on human-written interactive fiction like JERICHO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The definition of the four reasoning skills feels well-grounded and convincing, and the descriptions and setups of the tasks are clear and reasonable.\n2. This paper contributes to evaluation stability and reproducibility, which are often overlooked but crucial for benchmarks.\n3. The benchmark covers models very comprehensively, giving a broad and useful picture of how current models perform in different types of reasoning.\n4. The paper reads smoothly overall"}, "weaknesses": {"value": "1. The benchmark imposes a fixed 100-step cap for all environments with different complexity. A dynamic step setting may be more meaningful. And Table 5 shows extremely low scores of all the LLMs on Jericho. This raises concerns about whether the reported performance reflects actual reasoning limitations or just under-allocated interaction budgets.\n2. TEXTWORLD tasks seem trivial for modern LLMs, which all reach near 100%. It's unclear whether this task provides discriminative metric.\n3. Lack of justification on the benchmark’s validity through, for example, ablative studies, consistency analyses. Clear empirical evidence of construct validity would strengthen the benchmark.\n4. The final average score seems to be computed uniformly across environments. However, Table 8 show variance on different tasks. And the score in different task can be quite high or quite low. The paper does not discuss weighting strategies like whether certain environments dominate the final score distribution.\n5. Although the paper mentions “strong evidence of data contamination” in human-written games, it's not clear how the reported results can be interpreted as measurements of reasoning rather than memorization.\n6. Since the reward sparsity varies across the tasks. It is unclear whether observed performance differences reflect reasoning or reward shaping artifacts.\n6. Reporting results in Table 6 to two decimal places is unnecessary."}, "questions": {"value": "1. Although it’s necessary to evaluate the LLMs’ raw capabilities, I still wonder what’s their performance under the same rule instruction setting.\n2. How does TALES ensure construct validity that the measured scores truly reflect the four proposed reasoning skills rather than other factors?\n3. Has the benchmark been validated against human baselines or expert heuristics to confirm the intended difficulty hierarchy?\n4. Is the results sensitive to small perturbations like the change in prompt?\n5. More analysis on the four proposed skills in the experiments is needed"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hLoLifN2bH", "forum": "mVCsM9xJMd", "replyto": "mVCsM9xJMd", "signatures": ["ICLR.cc/2026/Conference/Submission20662/Reviewer_93MW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20662/Reviewer_93MW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568768853, "cdate": 1761568768853, "tmdate": 1762934050536, "mdate": 1762934050536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors have introduced the Text Adventure Learning Environment Suite (TALES), a collection of text-adventure games designed to rigorously evaluate the reasoning abilities of LLMs. TALES presents a challenge in reasoning for the current state-of-the-art AI, revealing that even the most advanced models struggle with complex reasoning problem.\n\nKey Contributions of the TALES paper:\n\nA Unified Evaluation Framework: TALES provides a standardized benchmark, allowing for consistent and comparable evaluation across different models. The suite includes games from various frameworks like Jericho, TextWorld, TextWorldExpress, ALFWorld,\nScienceWorld, offering a broad spectrum of challenges.\n\nBenchmarking Leading LLMs: The authors tested a wide range of both open- and closed-weight LLMs on the TALES benchmark. \nAnalysis of Model Failures: They also conduct a qualitative analysis of the top-performing models identified common failure points."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Both of the strengths and the weaknesses of the paper are quite evident.\n\n1. The text-adventure games provide a good test bed to assess LLM's reasoning ability, which are verifiable (whether succeed in completing the tasks) and challenging enough with long-horizon tasks.\n2. The paper provides a unified suite for the existing benchmarks like TextWorld, TextWorldExpress, ALFWorld, ScienceWorld, Jericho, which would make it more convenient for a systematic evaluation in various text adventure based games.\n3. The paper includes a large amount of experiments across these games and mainstreaming open-source and close-source LLMs, and provides detailed results and solid setup for evaluation."}, "weaknesses": {"value": "The provided experiments are robust and the provided unifying of the existing text-adventure benchmarks would be beneficial from a engineering perspective. However, from the novelty view, this paper doesn't include new design of gaming benchmark. This work does not include new games, nor new design of evaluation. \n\nMeanwhile, leveraging interactive environments to assess various reasoning abilities is not new. And it is not surprising to see LLMs would fail in gaming where complex reasoning is needed. [1][2][3]\n\n[1] Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. 2024b. Gtbench: Uncovering the strategic reasoning capabilities of llms via game-theoretic evaluations. In NeurIPS.\n\n[2] Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, and Michael R Lyu. 2024. How far are we on the decision-making of llms? evaluating llms’ gaming ability in multi-agent environments. arXiv preprint arXiv:2403.11807.\n\n[3] Wenye Lin, Jonathan Roberts, Yunhan Yang, Samuel Albanie, Zongqing Lu, and Kai Han. 2025. GAMEBoT: Transparent Assessment of LLM Reasoning in Games. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7656–7682, Vienna, Austria. Association for Computational Linguistics."}, "questions": {"value": "Except convenience, what are the other advantages of TALES compared to respectively evaluating LLMs on each of these benchmarks: TextWorld, TextWorldExpress, ALFWorld, ScienceWorld, Jericho and combine the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Dc4uXrZzoJ", "forum": "mVCsM9xJMd", "replyto": "mVCsM9xJMd", "signatures": ["ICLR.cc/2026/Conference/Submission20662/Reviewer_XTMp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20662/Reviewer_XTMp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724622733, "cdate": 1761724622733, "tmdate": 1762934050189, "mdate": 1762934050189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unified benchmark suite integrating five existing text-adventure learning environments. The proposed benchmark is used to assess the zero-shot capabilities of LLMs. The evaluation measures the maximum score attainable within a fixed number of turns. Additionally, the analysis identifies common failure modes related to spatial, deductive, inductive, and grounded reasoning. The findings indicate that LLM-based agents can struggle even with a simplified \"Simon says\"-style game—a task considerably simpler than solving complex virtual puzzles. The results also suggest that these agents remain far from achieving optimal performance in games designed for human players."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The topic of this work is both compelling and valuable; the integration of diverse reasoning skills into a unified framework offers significant practical utility.\n\n2. The experiment section demonstrates thorough engineering effort, encompassing a wide range of models, and clearly highlights a critical limitation in their long-horizon reasoning capabilities.\n\n3. The design of the SIMON SAYS task offers a natural and well-grounded method for evaluating a model's ability to follow instructions.\n\n4. With multiple concrete examples and detailed explanations provided in the appendix, this paper delivers a benchmark that will greatly benefit the research community and facilitate future studies."}, "weaknesses": {"value": "1. The four reasoning skills proposed by the authors raise concerns regarding comprehensiveness, and there is a lack of discussion on the interrelationships among these skills. The classification comes across as more of an intuitive listing rather than a systematically constructed framework.\n2. The discussion of \"TO THINK OR NOT TO THINK\" could be more thorough; it would benefit from a comparative analysis between the reasoning modes in this benchmark and those commonly found in pre-training data for LLMs."}, "questions": {"value": "1. I want to know whether It would be valuable if the authors could provide results and analysis from simple fine-tuning of open-source models on this benchmark, which could further encourage the research community to adopt it for training and optimization purposes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fldMofpV1E", "forum": "mVCsM9xJMd", "replyto": "mVCsM9xJMd", "signatures": ["ICLR.cc/2026/Conference/Submission20662/Reviewer_4NwH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20662/Reviewer_4NwH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762437390620, "cdate": 1762437390620, "tmdate": 1762934049546, "mdate": 1762934049546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Note: This is a review by an emergency reviewer.\n\n\nThis paper presents the TALES benchmark for evaluating LLMs on text-adventure game environments that require 4 core reasoning abilities: deductive, inductive, spatial, and grounded reasoning. The authors argue that the compositional reasoning capabilities needed to perform well on TALES are critical for real-world applications of LLM-based agents. The benchmark is created from 5 existing frameworks, each comprised of multiple games, and introduces an initial instruction-following task (Simon Says).  The authors present the scores for 10 strong LLMs on TALES, finding low scores on the games from one framework but very high scores on the others. Further analysis evaluates the reasoning traces of some of the evaluated models, identifies reasoning failures, and compares thinking vs non-thinking models. Finally, prompting strategies are compared for a weak open-source model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "**Concept**: the idea of integrating existing text-based adventure games into a reasoning-focused benchmark as an evaluation of reasoning for real-world applications is interesting.\n\n**Evaluation breadth**: the authors evaluate a wide range of frontier LLMs on TALES with results for additional weaker models also reported in the appendix (42 LLMs in total). This gives a comprehensive overview of current capabilities.\n\n**Evaluation approach**: the decision to use a standardised, lightweight evaluation approach that does not include domain knowledge strengthens the evaluation. \n\n**Limitations**: the authors acknowledge and discuss several limitations with their approach.\n\n**Visualizations**: the anonymized github link includes numerous visualizations of per-game performance, along with a measure of score spread across different runs."}, "weaknesses": {"value": "**Difficulty**: for me, a major limitation of this benchmark lies in its difficulty. Specifically, frontier models score very highly on 4 of the 5 frameworks (88-100% for o3 medium) – the games in these frameworks are either approaching saturation or are saturated, and evaluating on them offers limited insights. Therefore, I see the core value lies in the results of the games from the Jericho framework, which do prove challenging for current frontier models (though they are acknowledged by the authors as potentially suffering from data contamination). Given this, the significance of the contribution of TALES seems limited. Down-selecting just the most challenging subset of games from the 4 high-scoring frameworks could increase the overall difficulty, but given the lack of headroom, this would filter out most games.\n\n**Curation**: the description of TALES lacks reasoning for why the 5 frameworks were selected. Statistics on the reasoning types covered by each game would be useful here, as would a clearer overview of the types of games included in each. How do these games relate to real-world applications, and what specific real-world applications do you think attaining strong performance on TALES unlocks?\n\n**Analysis**: the analysis (Sec. 5) would be strengthened by adding example reasoning traces with failures (from the Appendix). This would help contextualise the qualitative insights. \n\n**Clarity**: several aspects of the paper could be made clearer:\n\n-\tThe authors compare the results of synthetic games vs games “designed for human enjoyment” but do not specify which games/frameworks correspond to which.\n\n-\tThe TALES score calculation is not defined in the paper.\n\n-\tIn Tab. 1, the units for walkthrough length are not stated\n\n-\tMore examples of the games from the different frameworks (e.g., Fig. 1) would help provide context.\n\n\n**Minor errors/typos**: the following lists some of the minor errors, inconsistencies and typos in the paper:\n\n-\tLine 156: “Figure 1 illustrates a simple task in a text-adventure game where multiple reasoning skills are required at each step…”. Most steps in the figure don’t require multiple reasoning skills\n\n-\tLine 40 missing word in “need apply”\n\n-\tLines 49, 401 missing space between text and citation e.g., “task(Paglieri…)”\n\n-\tLine 93 missing word in “a collection games”\n\n-\tLine 214: “the player receive” -> receives\n\n-\tLine 239: remove comma after 54\n\n-\tLine 257: missing word: “coming from human expert”\n\n-\tLine 288: missing word: “but find”\n\n-\tLine 351: missing word “when a subgoal was and”\n\n-\tLine 358: missing word “failures still when multiple…”\n\n-\tLine 406: missing word: “are same as main results”\n\n-\tLine 434: missing word: “reduce the space possible commands”\n\n-\tLine 446: missing full stop/period.\n\n-\tTable 1 is not referenced in the paper.\n\n-\tLine 243: “For example, 9:05 follows the morning of an ordinary office worker where ANCHORHEAD is a Lovecraftian Horror Story” – where should be while?\n\n-\tIntroduction: “games” and “tasks” are used interchangeably to refer to the suite of 122 games.\n\n-\tcitep / citet usage is inconsistent\n\n-\tTALES is presented as both a collection of frameworks (Line 179) and also described as a framework itself (Line 99)."}, "questions": {"value": "How are the TALES scores calculated? It’s not the mean of the average scores per framework but appears to be a weighted average based on the number of games in each framework.\n\nHave you explored evaluating a human baseline on TALES or the Jericho games in particular?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j4v6zCMROG", "forum": "mVCsM9xJMd", "replyto": "mVCsM9xJMd", "signatures": ["ICLR.cc/2026/Conference/Submission20662/Reviewer_rY5d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20662/Reviewer_rY5d"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762515223281, "cdate": 1762515223281, "tmdate": 1762934049178, "mdate": 1762934049178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}