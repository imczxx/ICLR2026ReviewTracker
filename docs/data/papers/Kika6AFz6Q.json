{"id": "Kika6AFz6Q", "number": 2939, "cdate": 1757304021971, "mdate": 1763562137888, "content": {"title": "GFMate: Empowering Graph Foundation Models with Pre-training-agnostic Test-time Prompt Tuning", "abstract": "Graph prompt tuning has shown great potential in graph learning by introducing trainable prompts to enhance the model performance in conventional single-domain scenarios. Recent research has extended graph prompt methods to Graph Foundation Models (GFMs), aiming to improve their cross-domain generalisability from source domains to an unseen target domain by tuning auxiliary prompts using few-shot samples. Despite their progress, most existing GFM prompt methods embed domain-specific information from source domains into prompts, which serve either as input to GFMs or encoded during the GFM pre-training process. This entanglement of prompts with specific source domains and particular GFM pre-training strategy restricts their generalisability to target domains and different GFMs. Furthermore, existing methods merely rely on few-shot data for prompt tuning, neglecting the rich information in unlabelled target domain test data. Motivated by these insights, this paper aims to empower GFMs with a pre-training-agnostic test-time graph prompt tuning framework, named GFMate. GFMate introduces a centroid prompt and a layer prompt applied after pre-training on target domains, avoiding entanglement with the source domains and model pre-training. In addition, a test-time complementary learning objective is devised to exploit both labelled and unlabelled target domain data for effective test-time prompt tuning. Extensive experiments on 12 benchmark datasets across diverse domains demonstrate the superior performance and efficiency of GFMate, achieving improvements of up to 30.63\\%. Code will be released upon acceptance.", "tldr": "", "keywords": ["Prompt Tuning", "Graph Foundation Models", "Test Time Prompt Tuning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c26f2fd341c0a3d0f26a6b7d6d9d7648f9a24c5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes GFMate, a framework for pre-training-agnostic test-time prompt tuning of GFMs. The key idea is to decouple prompt optimization from pre-training, introducing two lightweight prompts that are tuned only at test time. Additionally, a Test-time Graph Complementary Learning objective leverages both few-shot labeled and unlabeled samples through complementary labeling to improve target-domain generalization. Experiments on twelve benchmark datasets claim up to 30% improvement over existing GFM prompt methods with large gains in efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important problem: cross-domain generalization of GFMs without re-pretraining, and positions itself within an emerging research trend of test-time adaptation for graphs.\n\n2. The method is conceptually simple and computationally light, requiring only prompt updates without retraining the GFM backbone.\n\n3. The empirical section is extensive, covering multiple datasets, backbones, and pre-training objectives.\n\n4. The writing is clear and the figures provide an intuitive illustration of the pre-training-entanglement problem and the proposed test-time workflow."}, "weaknesses": {"value": "1. Conceptual novelty is overstated. The claimed “pre-training-agnostic” feature is somewhat misleading: most existing few-shot or prompt-based GFM methods (e.g., SAMGPT, MDGPT) also freeze the backbone and do not require coupling with pre-training. The difference between few-shot fine-tuning and test-time tuning is incremental.\n\n2. The paper repeatedly asserts that prior prompts are “pre-training-entangled” but does not empirically demonstrate that this entanglement actually harms transferability.\n\n3. The theoretical contribution (the “Excess Risk Bound”) offers no genuine insight into the TGCL mechanism. It restates a generic Rademacher complexity bound without linking assumptions to the graph setting or validating them empirically.\n\n4. The role of complementary labeling is weakly justified. The entropy-based pivot-layer strategy is heuristic, and there is no analysis comparing it with pseudo-labeling or entropy minimization baselines.\n\n5. Some comparisons seem selective: BRIDGE (ICML 2025), a closely related multi-domain GFM with generalization guarantees, is discussed in related work but not included in experiments, which undermines the completeness of evaluation.\n\n6. The datasets used are small-scale academic benchmarks (Cora, Citeseer, Texas, etc.), which makes it unclear whether the approach scales to true “foundation” settings."}, "questions": {"value": "1. What concrete evidence shows that “pre-training entanglement” limits cross-domain generalization? Could you provide an ablation where pre-training and prompt learning are decoupled within SAMGPT or MDGPT for comparison?\n\n2. How sensitive is the TGCL loss to noisy complementary labels? Have you compared it with simple entropy minimization or self-training?\n\n3. Can the proposed method handle real large-scale GFMs (e.g., GraphMAE, OpenGraph) or text-attributed graphs, or is it limited to small GNN-based backbones?\n\n4. Since GFMate tunes prompts on unlabeled test data, how do you avoid potential label leakage or overfitting to test distribution shifts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2cDLHJVCYK", "forum": "Kika6AFz6Q", "replyto": "Kika6AFz6Q", "signatures": ["ICLR.cc/2026/Conference/Submission2939/Reviewer_Fb27"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2939/Reviewer_Fb27"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574461627, "cdate": 1761574461627, "tmdate": 1762916450907, "mdate": 1762916450907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a pre-training-agnostic test-time graph prompt tuning framework, named GFMate. It consists of centroid prompt and a layer prompt to align the distribution shift between the source graph and target graph and exploit the rich neighborhood information from unlabelled node, respectively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The extensive experiments demonstrate the effectiveness and efficiency of the proposed method.\n2. The presentation of the proposed method is good and the paper is easy to follow.\n3. This paper provides the generalization bound of test-time learning loss."}, "weaknesses": {"value": "1. I am not fully convinced by the design of test-time graph complementary learning. The authors mention that the test-time learning loss encourages centroids to be distant from testing samples being predicted to the most dissimilar class, which might be useful when the number of classes is not large, such as 2-class or 3-class. Table 2 and Table 17 show the performance comparison in the node classification and graph classification tasks, respectively. These results in 1-shot setting show that the improvement of the proposed method over other baseline methods in binary graph classification task (e.g., around 4%) is less significant than that in the node classification task (5-8 classes classification). Intuitively, if the design of test-time graph complementary learning can remove one wrong answer (by pushing the centroid away from the most dissimilar class), the performance improvement in the binary class task can be greatly improved compared with the 5-class classification task.\n2. The experimental setup is not quite clear. See question below.\n3. In the introduction, the authors claim that existing GFM prompt designs are generally entangled with pre-training on source domains and cannot easily generalize to unseen target domains. This statement seems overstated, as several existing methods have incorporated mechanisms to adapt to target domains. For instance, MDGPT [1] introduces dual prompts, consisting of a unifying prompt and a mixing prompt, to enable adaptation to target domains by leveraging both unified multi-domain knowledge and tailored mixtures of domain-specific knowledge. Then, the authors assert that the domain-specific information learned in the prompts cannot be effectively transferred from source to target domains due to substantial differences in structural and feature patterns. If figure 3 is used to validate the claim, the authors should visualize both the source graph and the target graphs by GFM prompts methods **explicitly**. However, current version of figure 3 fail to convey the misalignment between test distribution and the training distribution. This makes the first challenge less important.\n4. Although the paper presents a generalization bound, the authors do not provide any substantive discussion or interpretation of the theoretical analysis. It remains unclear how the derived gap contributes to understanding the model’s generalization capability, particularly under domain shift or prompt adaptation scenarios. The absence of theoretical insights, such as which factors (e.g., number of classes?) most influence the gap, makes this analysis appear superficial. Without further explanation or empirical validation linking the theoretical findings to observed performance, the inclusion of the generalization gap offers limited theoretical or practical value to the paper’s overall contribution.\n5. The code is not provided.\n\n[1] Xingtong Yu, Chang Zhou, Yuan Fang, and Xinming Zhang. Text-free multi-domain graph pretraining: Toward graph foundation models. arXiv preprint arXiv:2405.13934, 2024c."}, "questions": {"value": "1. The experimental setup is not quite clear. In Figure 3, which graph is the GFM pretrained on? Cora? Which GFM method is used for visualization? In table 1, which graphs are used for model pretraining?\n2. I am not fully convinced by the design of test-time graph complementary learning. The authors mention that the test-time learning loss encourages centroids to be distant from testing samples being predicted to the most dissimilar class, which might be useful when the number of classes is not large, such as 2-class or 3-class. Table 2 and Table 17 show the performance comparison in the node classification and graph classification tasks, respectively. These results in 1-shot setting show that the improvement of the proposed method over other baseline methods in binary graph classification task (e.g., around 4%) is less significant than that in the node classification task (5-8 classes classification). Intuitively, if the design of test-time graph complementary learning can remove one wrong answer (by pushing the centroid away from the most dissimilar class), the performance improvement in the binary class task can be greatly improved compared with the 5-class classification task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SPidPAwoqx", "forum": "Kika6AFz6Q", "replyto": "Kika6AFz6Q", "signatures": ["ICLR.cc/2026/Conference/Submission2939/Reviewer_WLs1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2939/Reviewer_WLs1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672315071, "cdate": 1761672315071, "tmdate": 1762916450630, "mdate": 1762916450630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GFMate, empowering GFMs with a pre-training-agnostic test-time graph prompt tuning framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It's important to explore GFM.\n2. The paper is well motivated. It's reasonable to leverage abundant unlabeled data.\n3. Centroid shifting and layer re-weighting are simple, efficient, and intuitive."}, "weaknesses": {"value": "1. Pivot-layer selection in TGCL may be unstable under data noise or heterophily; sensitivity analysis could be stronger.\n2. Assumes full transductive access to the test graph; performance under inductive or streaming settings is unclear."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "99pJ3xEDgu", "forum": "Kika6AFz6Q", "replyto": "Kika6AFz6Q", "signatures": ["ICLR.cc/2026/Conference/Submission2939/Reviewer_pevV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2939/Reviewer_pevV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739054816, "cdate": 1761739054816, "tmdate": 1762916450396, "mdate": 1762916450396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles two major limitations of adapting Graph Foundation Models (GFMs) to unseen target domains: (1) existing prompt tuning methods are tightly coupled with pre-training, limiting generality, and (2) few-shot supervision alone cannot capture target-domain distribution, leaving unlabeled test nodes under-utilized. To address these issues, the authors propose (1) **pre-training-agnostic test-time prompt tuning**, which decouples prompt learning from pre-training, enabling flexible adaptation, and (2) **test-time graph complementary learning**, which leverages unlabeled target nodes to mitigate distribution shift. Together, these components significantly improve GFM adaptation under scarce labels."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed approach achieves **substantial performance gains** while maintaining superior efficiency compared to existing cross-domain GFM methods.\n\n2. The **experimental evaluation is thorough**, covering 12 benchmark datasets with extensive comparisons, ablation studies, efficiency analyses, and compatibility tests, providing strong empirical evidence for the method’s effectiveness.\n\n3. The paper **is clearly written and logically structured**, making the methodology and insights easy to follow."}, "weaknesses": {"value": "1. The paper's **motivation is not sufficiently sound**. Specifically, the claim that the \"pre-training–entangled\" nature of existing GFM prompt designs is a significant disadvantage is not thoroughly substantiated. Intuitively, given that graph data distributions and node behaviors can vary significantly across domains, distinguishing between them during training (e.g., via domain tokens or vectors) appears to be a reasonable approach. The theoretical support for the idea that this \"entanglement\" is harmful is currently insufficient, which makes the core premise confusing, even in light of the strong experimental results.\n2. The authors attempt to elaborate on the problem in the PRELIMINARY section, but the explanation remains unclear. It is axiomatic that node behaviors differ across domains, which necessitates the injection of domain-related information. The paper would be strengthened if the authors could emphasize **the distinction between injecting domain information at *pre-training time* versus *test-time*.** A thorough discussion of the advantages and disadvantages of each approach is needed.\n\n3. The experimental section **lacks crucial implementation details**, making reproducibility difficult. The following key information appears to be missing:  (1) For the *Single-domain Training and Testing baselines*: The specific training tasks used and the methodology for the training, validation, and testing splits. (2) For all *GFM* baselines: The specific pre-training tasks and datasets (i.e., the source graphs) that were used."}, "questions": {"value": "1. Why would ignoring domain information during pre-training be beneficial?\n2. During pre-training, given the different data distributions of source domains, would omitting domain information lead to training instability?\n3. The datasets vary significantly in size. Is it possible that a single large source domain dominates the pre-training process?\n4. What is the specific meaning of \"different-hop neighbourhood aggregation accuracy\" in Figure 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i5wIIIsMpC", "forum": "Kika6AFz6Q", "replyto": "Kika6AFz6Q", "signatures": ["ICLR.cc/2026/Conference/Submission2939/Reviewer_ZdsV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2939/Reviewer_ZdsV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158451017, "cdate": 1762158451017, "tmdate": 1762916450101, "mdate": 1762916450101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Revision"}, "comment": {"value": "Dear Program Chairs, Senior Area Chairs, Area Chairs, and all Reviewers,\n\n**We thank all reviewers for their time and constructive feedback.** We have uploaded a revised version of our manuscript in response to the reviewers’ concerns. All modifications are clearly described in our detailed responses to each reviewer’s official comments. To ensure that new material is easy to identify, all added text, figures and tables are **highlighted in blue font**. For clarity, we summarise the main updates with their corresponding question below.\n\n### For Reviewer ZdsV\n* In response to w1, we explained why prompts entangled with pretraining domains limit generalisation. The added discussion is at **line 126** and supported by Figure 3.\n\n* In response to w2 and q1, we clarified the difference between adding domain information during pretraining and at test time. The comparison is now in the introduction, at line 126, and in **Appendix G**.\n\n* In response to w3, we added the missing implementation details in **Section 4 and Appendix A**, and clarified the leave one out pretraining setup in **Section 4 and Table 3.**\n\n* In response to q2, we clarified training stability and referred to supporting evidence in **Section 3 and Appendix E.3.**\n\n* In response to q3, we discussed the effect of large source domains and added the clarification at **line 137, supported by Table 11.**\n\n* In response to q4, we clarified the meaning of **Figure 2 and updated its caption.**\n\n### For Reviewer pevV\n\n* In response to w1, we clarified that pivot-layer selection adaptively chooses the most confident layer, ensuring robustness under noise and heterophily. Results are highlighted in **Appendix E.3, Table 9, Table 1, and Table 4, with a robustness section added in Section 4.3 and Table 2.**\n\n* In response to w2, we explained the transductive setting used in the main experiments (Remark 4), and reported inductive and streaming results in **Appendix E.6, Table 10, Appendix E.8, and Section 4.2.**\n\n### For Reviewer WLs1\n* In response to w1 & q2, TGCL shows a larger improvement in binary classification. Results are added in **Table 9.**\n\n* In response to w3, pre-training-entangled prompts assume target domains are related to source domains, limiting generalisation. Details in **Appendix G.**\n\n* In response to w4 & 1, theoretical insights show excess risk decreases with more unlabelled samples and fewer classes. **Verified in Table 10 (Section E.4) and Table 9 (Section E.3).**\n\n* In response to w2 & q1, models are pre-trained on all source domains except the target (line 375). Figure 3 shows SAMGPT embeddings. Pre-training domain ablations are highlighted in **Appendix E.2 and Table 8.**\n\n### For Reviewer Fb27\n\n* In response to w1, pre-training agnostic prompts focus on decoupling prompt tuning from model pre-training. Test-time tuning actively uses target domain data, unlike few-shot fine-tuning **(Details highlighted in Remark 2 line 121, Remark 4 line 202 and Appendix H).**\n\n* In response to w2 & q1, pre-training-entangled prompts limit cross-domain generalisation **(Figures 2–3, line 126)**. Ablation shows existing work cannot fully decouple prompts from pre-training due to dependence on source-domain knowledge.\n\n* In response to w3, theoretical insights show excess risk decreases with more unlabelled samples and fewer classes, verified in **Tables 9–10, Appendix E.3–E.4, line 330.**\n\n* In response to w4 & q2, complementary labelling improves accuracy over pseudo-label/self-training. Entropy-based pivot layer strategy stabilises predictions across target domains **(Details highlighted in Section 3.4 line 287, Tables 10–11 and Appendix E.6–E.7).**\n\n* In response to w5, BRIDGE is added as an additional baseline **(Table 1, Table 3).**\n\n* In response to w6 & q3, GFMate scales to large GFMs and text-attributed graphs using GNN or text encoders, without backbone limitation **(Details highlighted in Section 4.4, Table 3, Appendix C.2).**\n\n* In response to q4, label leakage is avoided in the standard transductive setup, and GFMate remains robust under test distribution shifts **(Details highlighted in Section 4.3, Table 2 and Appendix E.3).**\n\n**We hope this summary offers a clear and transparent review of our revisions and supports a more informed reassessment of our work. We will continue to refine the manuscript in line with the ongoing discussion throughout the rebuttal stage.**\n\nBest regards,\n\nAuthors"}}, "id": "21mrhT7ZrS", "forum": "Kika6AFz6Q", "replyto": "Kika6AFz6Q", "signatures": ["ICLR.cc/2026/Conference/Submission2939/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2939/Authors"], "number": 21, "invitations": ["ICLR.cc/2026/Conference/Submission2939/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763564774452, "cdate": 1763564774452, "tmdate": 1763564774452, "mdate": 1763564774452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}