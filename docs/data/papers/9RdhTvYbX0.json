{"id": "9RdhTvYbX0", "number": 20326, "cdate": 1758304765900, "mdate": 1763667093017, "content": {"title": "Preventing Model Collapse Under Overparametrization: Optimal Mixing Ratios for Interpolation Learning and Ridge Regression", "abstract": "Model collapse occurs when generative models degrade after repeatedly training on their own synthetic outputs. We study this effect in overparameterized linear regression in a setting where each iteration mixes fresh real labels with synthetic labels drawn from the model fitted in the previous iteration. We derive precise generalization error formulae for minimum-$\\ell_2$-norm interpolation and ridge regression under this iterative scheme. Our analysis reveals intriguing properties of the optimal mixing weight that minimizes long-term prediction error and provably prevents model collapse. For instance, in the case of min-$\\ell_2$-norm interpolation, we establish that the optimal real-data proportion converges to the reciprocal of the golden ratio for fairly general classes of covariate distributions. Previously, this property was known only for ordinary least squares, and additionally in low dimensions. For ridge regression, we further analyze two popular model classes -- the random-effects model and the spiked covariance model -- demonstrating how spectral geometry governs optimal weighting. In both cases, as well as for isotropic features, we uncover that the optimal mixing ratio should be at least one-half, reflecting the necessity of favoring real-data over synthetic. We study three additional settings: (i) where real data is fixed and fresh labels are not obtained at each iteration, (ii) where covariates vary across iterations but fresh real labels are available each time, and (iii) where covariates vary with time but only a fraction of them receive fresh real labels at each iteration. Across these diverse settings, we characterize when model collapse is inevitable and when synthetic data improves learning.\nWe validate our theoretical results with extensive simulations.", "tldr": "This paper establishes optimal ratios for mixing real data with synthetic data for high dimensional interpolation learning and ridge regression that optimizes prediction performance while preventing model collapse.", "keywords": ["Model Collapse", "High Dimensional Regression", "Overparametrization"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f11ff1b85023ed31d980f4aeea89670e02432549.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper mainly studies how to prevent model collapse in a setting where in each iteration, a batch of new real data and a batch of synthetic data from the previous round are used to train the next round's generative model. The paper first studies this problem for overparametrized linear regression, which seems to be an extension of a previous paper.\n\nNext, the ridge regression is also analyzed for two popular model classes:  random-effects model and the spiked covariance model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and easy to understand.\n\nMost results presented in this paper are sound.\n\nTheorems in this paper are supported by both theoretical proofs and experiments."}, "weaknesses": {"value": "The first main concern for this paper is that the assumptions are too idealized, making it difficult for the results to be meaningful in practice. To be more specific, this paper mainly considers standard linear regression and ridge regression  in a **fixed design setting**. That is, the design matrix X is fixed across iterations. This setting is unrealistic in practice, as when people retrain a language model using synthetic data, they typically do not keep the prompts fixed across iterations. Also, this fixed design setting makes the ratio of the synthetic data and real data to be 1 in each iteration, which seems to be rare in practice.\n\nSecond, the contribution in this paper seems to be incremental. For example, if my understanding is correct, both this paper and the earlier paper cited by this paper(https://arxiv.org/pdf/2502.18049) aim to study how to use weighting to prevent model collapse. Theorem 3.1 in this paper aims to study this for overparametrized linear regression, but when I read the proof, it seems the weighting strategy can be derived from Lemma A.2 in the appendix directly, and from Lemma A.2 we can see that the weighting strategy for linear regression does not depend on the dimension. Therefore, it is unclear to me what important new insight can be brought by this section.\n\nAlso, there exists many typos in the paper that need to be fixed. For example, in appendix C, “Further, $w_1^{\\star 2} + (1 - w_1^{\\star})^2 = w_1^{\\star}.$”  should be \"Further, $w_1^{\\star 2} + 2(1 - w_1^{\\star})^2 = w_1^{\\star}.$\". In (C.3), the term $\\Sigma$ should not appear. \nIn the proof of Lemma B.1, (appendix page 22), the identity\n$\\frac{1}{zm(-z)} = \\frac{f(z)}{z} - 1$\nappears to be incorrect.  The term \"-1\" should be replaced by \"+1\".\nIn equation (B.4), $ \\frac{\\lambda}{2}-w$ should be corrected to $\\frac{\\lambda}{2-w}$.\n\nMoreover, in the proof for proposition 3.3, the authors seem to assume $\\lim_{ n \\to \\infty}$ and $\\lim_{t \\to \\infty}$ can be exchanged. In the main text, it takes the limit \nwith respect to $n$ first (i.e., $\\lim_{n\\to\\infty}\\lim_{t\\to\\infty}$), \nbut in the proof in the appendix the analysis is carried out in the opposite order \n(i.e., $\\lim_{t\\to\\infty}\\lim_{n\\to\\infty}$) without justification of their interchangeability."}, "questions": {"value": "Please address the issues mentioned in the weaknesses section. If at least some of them are addressed, or even the typos are fixed, I would be happy to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "y9nwuP0l0Q", "forum": "9RdhTvYbX0", "replyto": "9RdhTvYbX0", "signatures": ["ICLR.cc/2026/Conference/Submission20326/Reviewer_LCBS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20326/Reviewer_LCBS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760662859232, "cdate": 1760662859232, "tmdate": 1762933786696, "mdate": 1762933786696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper mainly studies how to prevent model collapse in a setting where in each iteration, a batch of new real data and a batch of synthetic data from the previous round are used to train the next round's generative model. The paper first studies this problem for overparametrized linear regression, which seems to be an extension of a previous paper.\n\nNext, the ridge regression is also analyzed for two popular model classes:  random-effects model and the spiked covariance model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written and easy to understand.\n\nMost results presented in this paper are sound.\n\nTheorems in this paper are supported by both theoretical proofs and experiments."}, "weaknesses": {"value": "The first main concern for this paper is that the assumptions are too idealized, making it difficult for the results to be meaningful in practice. To be more specific, this paper mainly considers standard linear regression and ridge regression  in a **fixed design setting**. That is, the design matrix X is fixed across iterations. This setting is unrealistic in practice, as when people retrain a language model using synthetic data, they typically do not keep the prompts fixed across iterations. Also, this fixed design setting makes the ratio of the synthetic data and real data to be 1 in each iteration, which seems to be rare in practice.\n\nSecond, the contribution in this paper seems to be incremental. For example, if my understanding is correct, both this paper and the earlier paper cited by this paper(https://arxiv.org/pdf/2502.18049) aim to study how to use weighting to prevent model collapse. Theorem 3.1 in this paper aims to study this for overparametrized linear regression, but when I read the proof, it seems the weighting strategy can be derived from Lemma A.2 in the appendix directly, and from Lemma A.2 we can see that the weighting strategy for linear regression does not depend on the dimension. Therefore, it is unclear to me what important new insight can be brought by this section.\n\nAlso, there exists many typos in the paper that need to be fixed. For example, in appendix C, “Further, $w_1^{\\star 2} + (1 - w_1^{\\star})^2 = w_1^{\\star}.$”  should be \"Further, $w_1^{\\star 2} + 2(1 - w_1^{\\star})^2 = w_1^{\\star}.$\". In (C.3), the term $\\Sigma$ should not appear. \nIn the proof of Lemma B.1, (appendix page 22), the identity\n$\\frac{1}{zm(-z)} = \\frac{f(z)}{z} - 1$\nappears to be incorrect.  The term \"-1\" should be replaced by \"+1\".\nIn equation (B.4), $ \\frac{\\lambda}{2}-w$ should be corrected to $\\frac{\\lambda}{2-w}$.\n\nMoreover, in the proof for proposition 3.3, the authors seem to assume $\\lim_{ n \\to \\infty}$ and $\\lim_{t \\to \\infty}$ can be exchanged. In the main text, it takes the limit \nwith respect to $n$ first (i.e., $\\lim_{n\\to\\infty}\\lim_{t\\to\\infty}$), \nbut in the proof in the appendix the analysis is carried out in the opposite order \n(i.e., $\\lim_{t\\to\\infty}\\lim_{n\\to\\infty}$) without justification of their interchangeability."}, "questions": {"value": "Please address the issues mentioned in the weaknesses section. If at least some of them are addressed, or even the typos are fixed, I would be happy to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "y9nwuP0l0Q", "forum": "9RdhTvYbX0", "replyto": "9RdhTvYbX0", "signatures": ["ICLR.cc/2026/Conference/Submission20326/Reviewer_LCBS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20326/Reviewer_LCBS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760662859232, "cdate": 1760662859232, "tmdate": 1763617705661, "mdate": 1763617705661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper mainly studies how to prevent model collapse in a setting where in each iteration, a batch of new real data and a batch of synthetic data from the previous round are used to train the next round's generative model. The paper first studies this problem for overparametrized linear regression, which seems to be an extension of a previous paper.\n\nNext, the ridge regression is also analyzed for two popular model classes:  random-effects model and the spiked covariance model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written and easy to understand.\n\nMost results presented in this paper are sound.\n\nTheorems in this paper are supported by both theoretical proofs and experiments."}, "weaknesses": {"value": "The first main concern for this paper is that the assumptions are too idealized, making it difficult for the results to be meaningful in practice. To be more specific, this paper mainly considers standard linear regression and ridge regression  in a **fixed design setting**. That is, the design matrix X is fixed across iterations. This setting is unrealistic in practice, as when people retrain a language model using synthetic data, they typically do not keep the prompts fixed across iterations. Also, this fixed design setting makes the ratio of the synthetic data and real data to be 1 in each iteration, which seems to be rare in practice.\n\nSecond, the contribution in this paper seems to be incremental. For example, if my understanding is correct, both this paper and the earlier paper cited by this paper(https://arxiv.org/pdf/2502.18049) aim to study how to use weighting to prevent model collapse. Theorem 3.1 in this paper aims to study this for overparametrized linear regression, but when I read the proof, it seems the weighting strategy can be derived from Lemma A.2 in the appendix directly, and from Lemma A.2 we can see that the weighting strategy for linear regression does not depend on the dimension. Therefore, it is unclear to me what important new insight can be brought by this section.\n\nAlso, there exists many typos in the paper that need to be fixed. For example, in appendix C, “Further, $w_1^{\\star 2} + (1 - w_1^{\\star})^2 = w_1^{\\star}.$”  should be \"Further, $w_1^{\\star 2} + 2(1 - w_1^{\\star})^2 = w_1^{\\star}.$\". In (C.3), the term $\\Sigma$ should not appear. \nIn the proof of Lemma B.1, (appendix page 22), the identity\n$\\frac{1}{zm(-z)} = \\frac{f(z)}{z} - 1$\nappears to be incorrect.  The term \"-1\" should be replaced by \"+1\".\nIn equation (B.4), $ \\frac{\\lambda}{2}-w$ should be corrected to $\\frac{\\lambda}{2-w}$.\n\nMoreover, in the proof for proposition 3.3, the authors seem to assume $\\lim_{ n \\to \\infty}$ and $\\lim_{t \\to \\infty}$ can be exchanged. In the main text, it takes the limit \nwith respect to $n$ first (i.e., $\\lim_{n\\to\\infty}\\lim_{t\\to\\infty}$), \nbut in the proof in the appendix the analysis is carried out in the opposite order \n(i.e., $\\lim_{t\\to\\infty}\\lim_{n\\to\\infty}$) without justification of their interchangeability."}, "questions": {"value": "Please address the issues mentioned in the weaknesses section. If at least some of them are addressed, or even the typos are fixed, I would be happy to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "y9nwuP0l0Q", "forum": "9RdhTvYbX0", "replyto": "9RdhTvYbX0", "signatures": ["ICLR.cc/2026/Conference/Submission20326/Reviewer_LCBS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20326/Reviewer_LCBS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760662859232, "cdate": 1760662859232, "tmdate": 1763675250621, "mdate": 1763675250621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical analysis of model collapse in overparameterized linear models in the ridge and ridgeless regression regime. The paper considers the fresh data augmentation framework with fixed covariates and fresh labels at each iteration. Interestingly, the authors identify optimal mixing ratios that minimize prediction error in this context which demonstrate how mixing real-data with synthetic outputs mitigates model collapse, notably with the appearance of the golden ratio. Specifically, the authors consider a fixed design matrix $X$ with i.i.d. rows in the proportional regime, and repeatedly refit on responses $ w y_t+(1-w) \\tilde{y}_t$. The paper considers the limit of $n/p \\to \\gamma > 1$ when $t \\to \\infty$ and derives the exact asymptotic risk as a function of $w$."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-organized and clearly written. The theory is sound as far as I checked. \n* The paper provides high dimensional generalization of the conclusion in He et al. (2025), showing the same optimal mixing ratio of $\\varphi^{-1}$ persists in high dimension.\n* The technical result of the paper is fairly generic, covering general $\\Sigma$ structure and ridge regression with penalty $\\lambda$. The paper also provides simulations that visualize the theoretical claims.\n\nHe, Hengzhi, Shirong Xu, and Guang Cheng. \"Golden ratio weighting prevents model collapse.\" arXiv preprint arXiv:2502.18049 (2025)."}, "weaknesses": {"value": "Major comments:\n\n* The model setup is somewhat too simplistic. The paper essentially assumes the following: (i) infinite supply of fresh labels (ii) the covariate matrix is fixed, and at each data point $X$, it is guaranteed to couple with (infinitely many) observations $y$. A very natural question in this setup is then: why don't we remember all new labels $y_t$, and simply taking $\\bar y_t = \\sum_{i=1}^t y_i / t$ , and solve the min-norm interpolator on the data $(X, \\bar{y}_t)$? This will completely get rid of the noise term and leave only the bias, which is strictly better than any mixing estimator with synthetic data.\n* The conclusion of model collapse is a bit narrow. It seems as long as $w > 0$, the estimator will always be reasonable (albeit, suboptimal) even if $t \\to \\infty$. This does not quite support the empirical evidence in Shumailov et al. (2024) where there is an actual degeneration of the learned distribution.\n* The major weakness of the paper is that the contributions are more technical rather than insightful: it provides limiting risks of mixing estimators in high dimensions under general $\\Sigma$ and $\\lambda$, but the setup is essentially the same as is discussed in He et al. (2025). For example in Eq. (A.5) and Appendix C, the nonasymptotic formula without making use of Marchenko-Pastur limits is sufficient to see the optimal $w$---the conclusion of optimal mixing ratio is not a high dimensional phenomenon, but only the limiting risk formulae. I feel the limiting risk formulae themselves provide little insights about model collapse. \n\nShumailov, Ilia, et al. \"The Curse of Recursion: Training on Generated Data Makes Models Forget.\" CoRR (2023).\nHe, Hengzhi, Shirong Xu, and Guang Cheng. \"Golden ratio weighting prevents model collapse.\" arXiv preprint arXiv:2502.18049 (2025).\n\n\nMinor comments:\n* It could be helpful in the statement of Theorem 3.3 showing the numerical value of $\\phi^{-1} \\approx 0.618$ so the presentation is more clear. Although it was pointed out in P5, Line 235, but I think it still helps the presentation."}, "questions": {"value": "* My main questions are in the weaknesses section. The following are additional questions that I think the paper could incorporate to discuss. Overall I think the paper is technically sound, but the contributions in high dimension is somewhat tangential to the main theme of model collapse.\n* What do the authors think of model collapse if fresh labels are only provided in a selected portions of covariates? It is more natural to make use of unlabeled covariates by synthetic labeling. How would this perform in an online setup?\n* I appreciate the numerical simulations that unpack the mathematical formulae---however, how would those connect to empirical findings in model collapse? I would like to see more discussions.\n* The general $\\Sigma$ with bounded spectra is essentially a corollary of the isotropic case. Would the authors comment on model collapse in the scaling law setups? Namely the spectra decay to 0 with infinite number of features such as in Cheng & Montanari (2024). \n\nCheng, Chen, and Andrea Montanari. \"Dimension free ridge regression.\" The Annals of Statistics 52.6 (2024): 2879-2912."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S2Lsr6N0Z8", "forum": "9RdhTvYbX0", "replyto": "9RdhTvYbX0", "signatures": ["ICLR.cc/2026/Conference/Submission20326/Reviewer_Fx5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20326/Reviewer_Fx5M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854823193, "cdate": 1761854823193, "tmdate": 1762933786404, "mdate": 1762933786404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comment describing major changes"}, "comment": {"value": "We thank the reviewing team for the thorough points. The manuscript has undergone major revision with addition of the entirely new Section 5 and the associated Appendix E. The major changes are summarized below. Throughout the below discussion, recall that $\\gamma= \\lim p/n$.\n\n# (A) Addition of new settings and analyses: \nIn Section 5, we have included analyses under three different settings: (i) no fresh real data each iteration, (ii) time-varying covariates, that is, covariate matrix no longer stays fixed throughout iterations, and (iii) time-varying covariates with real labels observed on a fraction of them each iteration. Across these, we have addressed effectively all comments raised by reviewers on potential other settings that could be interesting to study beyond the fresh real data augmentation setting we study here. For these additions, we focus on behavior of the min-norm interpolator. \n\nFor setting (i), we establish Theorem 5.1, which shows that when fresh real data is not available at each iteration, and only available once at the beginning, the risk is minimized for $w^\\star=1$. This means there is no gain from repeatedly adding synthetic data generated from the fitted models. We illustrate this in the newly added Figure 3(c). The details for this setting can be found in Appendix E.1. \n\nFor settings (ii) and (iii), we describe our new theorems in the appendix, due to space constraints. In (ii), we allow fresh covariates to be observed at each iteration, keeping all else same. We provide a characterization of the risk in Theorem E.1 (Appendix E.2) but in a reverse n,t order (we explain in Remark E.1 why this is necessary). For isotropic covariates, we further characterize the weight that uniquely minimizes the limiting risk. We demonstrate behavior of this optimal weight in Figure 4 for various values of the dimension to sample size ratio and the SNR. \n\nFinally, in setting (iii), we consider that fresh covariates are available at each iteration but real labels are observed for only half a fraction of these. For the rest, synthetic labels are generated using the model fitted at previous iteration. Thus at each iteration, we effectively observe a semi-supervised problem. The details of this setting can be found in Appendix E.3. We prove that there are regimes of $\\gamma$ values where the risk diverges since the bias actually diverges. Thus, model collapse is inevitable for the characterized ranges of $\\gamma$ values. This is illustrated in Figure 5 in the Appendix. \n\nDespite these significant additions, we emphasize that our work here is motivated by findings  in [1], which suggest that among interesting possible combinations of real and synthetic data mixing, fresh data augmentation (the setting studied in the original version of our paper) is the only scenario where model collapse can be prevented. Our goal is more to study how model collapse can be prevented in settings where it can.\n\n# (B) Importance/relevance of studying our high-dimensional framework: \nWe were also asked about the necessity of our high-dimensional framework or rather the necessity of taking limits. We emphasize that this is absolutely critical for ridge regression, which forms a major chunk of our contribution; for instance, see Sections 3.2-3.4. What the reviewers note that the optimal mixing ratio can be derived from the non-asymptotic results in Eq. (A.5) and Appendix C is incorrect for ridge regression. These results apply only for the min-norm interpolator. For the ridge, taking limits is important for obtaining a neat formula for the risk which can then be minimized to characterize the optimal mixing ratio. To illustrate the dependence of this ratio further on the dimensionality, we have added the new Figure 3(a) where we plot the optimal mixing ratio as a function of $\\gamma$ for different choices of positive $\\lambda$. The dimensionality effect becomes clear here and illustrates how studying this high-dimensional framework is actually important to understand best mixing strategies for ridge regression. We now emphasize this at the end of Pg. 8. \n\nFurthermore, among the added new settings in Section 5 that focus solely on the min-norm interpolator, taking limits is also critical, particularly in setting (ii) on time-varying covariates. This becomes clear on examining Appendix E.2, Theorem E.1, which clearly shows the dependence of the optimal weight on $\\gamma$ and also the newly added Figure 4(a), which demonstrates how this weight changes as a function of $\\gamma$. \n\n# (C) Expansion on prior work discussion: \nWe also appreciated comments on the need to expand the literature review, which we have now done in the introduction.\n\nThe remaining additions to the manuscript are described in response to reviewer comments below. \n\n[1] Alemohammad, Sina, et al. \"Self-consuming generative models go mad.\" The Twelfth International Conference on Learning Representations. 2023."}}, "id": "CnGzTw8GG1", "forum": "9RdhTvYbX0", "replyto": "9RdhTvYbX0", "signatures": ["ICLR.cc/2026/Conference/Submission20326/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20326/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20326/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763610967048, "cdate": 1763610967048, "tmdate": 1763613519360, "mdate": 1763613519360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies model collapse – i.e. when generative models experience performance degradation when repeatedly trained on its own synthetic data – in overparameterized linear regression for iterative training with a mixture of fresh real labels and synthetic labels. For min-l2-norm and ridge regression, the authors derive asymptotically precise generalization errors and uncover meaningful insights. Core results include: (i) for min-norm interpolation, the optimal mixing ratio equals 1/ golden ratio for general covariance ${\\Sigma}$ under certain assumptions; (ii) for ridge, the formula is log-convex in the mixing ratio w, and the optimal ratio is an increasing function of SNR. The paper also treats two case studies on random effect & spiked covariance models and validates theory with simulations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The precise risk formulae are clean and leverage tools from random matrix theory (beyond isotropic features for the ridge model). The optimal mixing ratio is a meaningful extension from prior works on OLS and low-dimensional results. \n* Though technical at times, the paper is written clearly and easy to follow. It shows precise formulae for both min-l2-norm and ridge, including random effect and spiked covariance as examples for the latter. \n* The empirical results validate the theory well and show robustness even when certain assumptions are violated."}, "weaknesses": {"value": "I do not identify key weaknesses, and overall the paper is technically sound. There are some small concerns I have: \n\n* **Modeling Gap to Practical Model Collapse**: In iterative training, the synthetic labels are generated from the previous linear weight on the same covariates, but in practice, the model collapse setting touches both features and labels. The bridge from this linear supervised setting to practice seems under-argued. While I appreciate the need for tractable theory, I would like to see some discussions regarding this. \n\n* **Dynamic Mixing**: The idea of dynamic mixing in Section 3.1 is an interesting extension. Though the end result is somewhat similar, I believe it is worth having some synthetic simulations on this too. Additionally, would it be possible to verify these empirical observations on any small-scale experiments with real data? \n\n* **Notations**: at times, vectors and matrices are not bold (e.g. to name a few, Lines 404, 412, 419). It is good to keep the notations consistent."}, "questions": {"value": "Some questions are raised in the Weakness section. Additionally, \n\n1. In the interpolation risk (Thm 3.1), the mixing ratio only affects the variance, but it affects both for the ridge model (Thm 3.2). This is an intriguing phenomenon (something similar also happened in double descent literature as I recall). Do you think there is any possible interpretation for this? \n\n2. How can the results possibly relate to the scenarios using real data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qxnOrJN8NE", "forum": "9RdhTvYbX0", "replyto": "9RdhTvYbX0", "signatures": ["ICLR.cc/2026/Conference/Submission20326/Reviewer_EFFX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20326/Reviewer_EFFX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893777395, "cdate": 1761893777395, "tmdate": 1762933786046, "mdate": 1762933786046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies model collapse in the context of overparameterized linear regression. The authors analyze the minimum-$\\ell_2$₂-norm interpolator and ridge regression, in a setting where each iteration mixes real and synthetic labels with a fixed proportion.\nThe paper provides precise asymptotic risk characterizations using random matrix theory and discuss the dependence on the mixing proportion and the interplay with regularization and SNR. They validate theory with simulations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a novel and technically solid contribution to the study of model collapse. It builds on a rigorous mathematical foundation, leveraging random matrix theory to generalize prior results that were limited to low-dimensional or Gaussian settings. The presentation is clear and well-structured, offering useful intuition for interpreting the theoretical findings, which could inform strategies for mitigating model collapse, when fresh labels are available. The numerical experiments are extensively detailed and effectively complement the theoretical analysis."}, "weaknesses": {"value": "The proposed framework may appear somewhat artificial, and its motivation is not sufficiently discussed in the introduction. This limits the perceived practical scope of the work and raises questions about its connection to real-world training dynamics. See also the Questions section."}, "questions": {"value": "1. Could the authors better justify the choice of this framework from a practical point of view? In particular, are there real-world scenarios that resemble the training procedure illustrated here? For instance, how should one interpret the assumption that at iteration $t$, the statistician looses access to the true labels $y_{t-1}, ..., y_1, y$, yet maintains constant access to new labels for the same covariates?\n2. Do you have intuition on how would the results change in a setting where, at iteration $t$, the dataset is obtained by union of the fresh and synthetic labels rather than their mixing? Would this alternative formulation still prevent model collapse?\n3. As a suggestion, the paper would benefit from a more extended discussion of prior literature. While several recent works on model collapse are cited, a short summary of their different frameworks and main findings would help clarify how the present contribution fits within the literature and emphasize its novelty."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xfG3m2vMYQ", "forum": "9RdhTvYbX0", "replyto": "9RdhTvYbX0", "signatures": ["ICLR.cc/2026/Conference/Submission20326/Reviewer_8T4F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20326/Reviewer_8T4F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995320640, "cdate": 1761995320640, "tmdate": 1762933785771, "mdate": 1762933785771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}