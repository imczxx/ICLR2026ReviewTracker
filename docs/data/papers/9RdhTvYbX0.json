{"id": "9RdhTvYbX0", "number": 20326, "cdate": 1758304765900, "mdate": 1759896983672, "content": {"title": "Preventing Model Collapse Under Overparametrization: Optimal Mixing Ratios for Interpolation Learning and Ridge Regression", "abstract": "Model collapse occurs when generative models degrade after repeatedly training on their own synthetic outputs. We study this effect in overparameterized linear regression in a setting where each iteration mixes fresh real labels with synthetic labels drawn from the model fitted in the previous iteration. We derive precise generalization error formulae for minimum-$\\ell_2$-norm interpolation and ridge regression under this iterative scheme. Our analysis reveals intriguing properties of the optimal mixing weight that minimizes long-term prediction error and provably prevents model collapse. For instance, in the case of min-$\\ell_2$-norm interpolation, we establish that the optimal real-data proportion converges to the reciprocal of the golden ratio for fairly general classes of covariate distributions. Previously, this property was known only for ordinary least squares, and additionally in low dimensions. For ridge regression, we further analyze two popular model classes -- the random-effects model and the spiked covariance model --demonstrating how spectral geometry governs optimal weighting. In both cases, as well as for isotropic features, we uncover that the optimal mixing ratio should be at least one-half, reflecting the necessity of favoring real data over synthetic. We validate our theoretical results with extensive simulations.", "tldr": "This paper establishes optimal ratios for mixing real data with synthetic data for high dimensional interpolation learning and ridge regression that optimizes prediction performance while preventing model collapse.", "keywords": ["Model Collapse", "High Dimensional Regression", "Overparametrization"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b5056904dbb84e82399eee1b403f75af2e6fb06.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper mainly studies how to prevent model collapse in a setting where in each iteration, a batch of new real data and a batch of synthetic data from the previous round are used to train the next round's generative model. The paper first studies this problem for overparametrized linear regression, which seems to be an extension of a previous paper.\n\nNext, the ridge regression is also analyzed for two popular model classes:  random-effects model and the spiked covariance model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and easy to understand.\n\nMost results presented in this paper are sound.\n\nTheorems in this paper are supported by both theoretical proofs and experiments."}, "weaknesses": {"value": "The first main concern for this paper is that the assumptions are too idealized, making it difficult for the results to be meaningful in practice. To be more specific, this paper mainly considers standard linear regression and ridge regression  in a **fixed design setting**. That is, the design matrix X is fixed across iterations. This setting is unrealistic in practice, as when people retrain a language model using synthetic data, they typically do not keep the prompts fixed across iterations. Also, this fixed design setting makes the ratio of the synthetic data and real data to be 1 in each iteration, which seems to be rare in practice.\n\nSecond, the contribution in this paper seems to be incremental. For example, if my understanding is correct, both this paper and the earlier paper cited by this paper(https://arxiv.org/pdf/2502.18049) aim to study how to use weighting to prevent model collapse. Theorem 3.1 in this paper aims to study this for overparametrized linear regression, but when I read the proof, it seems the weighting strategy can be derived from Lemma A.2 in the appendix directly, and from Lemma A.2 we can see that the weighting strategy for linear regression does not depend on the dimension. Therefore, it is unclear to me what important new insight can be brought by this section.\n\nAlso, there exists many typos in the paper that need to be fixed. For example, in appendix C, “Further, $w_1^{\\star 2} + (1 - w_1^{\\star})^2 = w_1^{\\star}.$”  should be \"Further, $w_1^{\\star 2} + 2(1 - w_1^{\\star})^2 = w_1^{\\star}.$\". In (C.3), the term $\\Sigma$ should not appear. \nIn the proof of Lemma B.1, (appendix page 22), the identity\n$\\frac{1}{zm(-z)} = \\frac{f(z)}{z} - 1$\nappears to be incorrect.  The term \"-1\" should be replaced by \"+1\".\nIn equation (B.4), $ \\frac{\\lambda}{2}-w$ should be corrected to $\\frac{\\lambda}{2-w}$.\n\nMoreover, in the proof for proposition 3.3, the authors seem to assume $\\lim_{ n \\to \\infty}$ and $\\lim_{t \\to \\infty}$ can be exchanged. In the main text, it takes the limit \nwith respect to $n$ first (i.e., $\\lim_{n\\to\\infty}\\lim_{t\\to\\infty}$), \nbut in the proof in the appendix the analysis is carried out in the opposite order \n(i.e., $\\lim_{t\\to\\infty}\\lim_{n\\to\\infty}$) without justification of their interchangeability."}, "questions": {"value": "Please address the issues mentioned in the weaknesses section. If at least some of them are addressed, or even the typos are fixed, I would be happy to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "y9nwuP0l0Q", "forum": "9RdhTvYbX0", "replyto": "9RdhTvYbX0", "signatures": ["ICLR.cc/2026/Conference/Submission20326/Reviewer_LCBS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20326/Reviewer_LCBS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760662859232, "cdate": 1760662859232, "tmdate": 1762933786696, "mdate": 1762933786696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical analysis of model collapse in overparameterized linear models in the ridge and ridgeless regression regime. The paper considers the fresh data augmentation framework with fixed covariates and fresh labels at each iteration. Interestingly, the authors identify optimal mixing ratios that minimize prediction error in this context which demonstrate how mixing real-data with synthetic outputs mitigates model collapse, notably with the appearance of the golden ratio. Specifically, the authors consider a fixed design matrix $X$ with i.i.d. rows in the proportional regime, and repeatedly refit on responses $ w y_t+(1-w) \\tilde{y}_t$. The paper considers the limit of $n/p \\to \\gamma > 1$ when $t \\to \\infty$ and derives the exact asymptotic risk as a function of $w$."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-organized and clearly written. The theory is sound as far as I checked. \n* The paper provides high dimensional generalization of the conclusion in He et al. (2025), showing the same optimal mixing ratio of $\\varphi^{-1}$ persists in high dimension.\n* The technical result of the paper is fairly generic, covering general $\\Sigma$ structure and ridge regression with penalty $\\lambda$. The paper also provides simulations that visualize the theoretical claims.\n\nHe, Hengzhi, Shirong Xu, and Guang Cheng. \"Golden ratio weighting prevents model collapse.\" arXiv preprint arXiv:2502.18049 (2025)."}, "weaknesses": {"value": "Major comments:\n\n* The model setup is somewhat too simplistic. The paper essentially assumes the following: (i) infinite supply of fresh labels (ii) the covariate matrix is fixed, and at each data point $X$, it is guaranteed to couple with (infinitely many) observations $y$. A very natural question in this setup is then: why don't we remember all new labels $y_t$, and simply taking $\\bar y_t = \\sum_{i=1}^t y_i / t$ , and solve the min-norm interpolator on the data $(X, \\bar{y}_t)$? This will completely get rid of the noise term and leave only the bias, which is strictly better than any mixing estimator with synthetic data.\n* The conclusion of model collapse is a bit narrow. It seems as long as $w > 0$, the estimator will always be reasonable (albeit, suboptimal) even if $t \\to \\infty$. This does not quite support the empirical evidence in Shumailov et al. (2024) where there is an actual degeneration of the learned distribution.\n* The major weakness of the paper is that the contributions are more technical rather than insightful: it provides limiting risks of mixing estimators in high dimensions under general $\\Sigma$ and $\\lambda$, but the setup is essentially the same as is discussed in He et al. (2025). For example in Eq. (A.5) and Appendix C, the nonasymptotic formula without making use of Marchenko-Pastur limits is sufficient to see the optimal $w$---the conclusion of optimal mixing ratio is not a high dimensional phenomenon, but only the limiting risk formulae. I feel the limiting risk formulae themselves provide little insights about model collapse. \n\nShumailov, Ilia, et al. \"The Curse of Recursion: Training on Generated Data Makes Models Forget.\" CoRR (2023).\nHe, Hengzhi, Shirong Xu, and Guang Cheng. \"Golden ratio weighting prevents model collapse.\" arXiv preprint arXiv:2502.18049 (2025).\n\n\nMinor comments:\n* It could be helpful in the statement of Theorem 3.3 showing the numerical value of $\\phi^{-1} \\approx 0.618$ so the presentation is more clear. Although it was pointed out in P5, Line 235, but I think it still helps the presentation."}, "questions": {"value": "* My main questions are in the weaknesses section. The following are additional questions that I think the paper could incorporate to discuss. Overall I think the paper is technically sound, but the contributions in high dimension is somewhat tangential to the main theme of model collapse.\n* What do the authors think of model collapse if fresh labels are only provided in a selected portions of covariates? It is more natural to make use of unlabeled covariates by synthetic labeling. How would this perform in an online setup?\n* I appreciate the numerical simulations that unpack the mathematical formulae---however, how would those connect to empirical findings in model collapse? I would like to see more discussions.\n* The general $\\Sigma$ with bounded spectra is essentially a corollary of the isotropic case. Would the authors comment on model collapse in the scaling law setups? Namely the spectra decay to 0 with infinite number of features such as in Cheng & Montanari (2024). \n\nCheng, Chen, and Andrea Montanari. \"Dimension free ridge regression.\" The Annals of Statistics 52.6 (2024): 2879-2912."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S2Lsr6N0Z8", "forum": "9RdhTvYbX0", "replyto": "9RdhTvYbX0", "signatures": ["ICLR.cc/2026/Conference/Submission20326/Reviewer_Fx5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20326/Reviewer_Fx5M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854823193, "cdate": 1761854823193, "tmdate": 1762933786404, "mdate": 1762933786404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies model collapse – i.e. when generative models experience performance degradation when repeatedly trained on its own synthetic data – in overparameterized linear regression for iterative training with a mixture of fresh real labels and synthetic labels. For min-l2-norm and ridge regression, the authors derive asymptotically precise generalization errors and uncover meaningful insights. Core results include: (i) for min-norm interpolation, the optimal mixing ratio equals 1/ golden ratio for general covariance ${\\Sigma}$ under certain assumptions; (ii) for ridge, the formula is log-convex in the mixing ratio w, and the optimal ratio is an increasing function of SNR. The paper also treats two case studies on random effect & spiked covariance models and validates theory with simulations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The precise risk formulae are clean and leverage tools from random matrix theory (beyond isotropic features for the ridge model). The optimal mixing ratio is a meaningful extension from prior works on OLS and low-dimensional results. \n* Though technical at times, the paper is written clearly and easy to follow. It shows precise formulae for both min-l2-norm and ridge, including random effect and spiked covariance as examples for the latter. \n* The empirical results validate the theory well and show robustness even when certain assumptions are violated."}, "weaknesses": {"value": "I do not identify key weaknesses, and overall the paper is technically sound. There are some small concerns I have: \n\n* **Modeling Gap to Practical Model Collapse**: In iterative training, the synthetic labels are generated from the previous linear weight on the same covariates, but in practice, the model collapse setting touches both features and labels. The bridge from this linear supervised setting to practice seems under-argued. While I appreciate the need for tractable theory, I would like to see some discussions regarding this. \n\n* **Dynamic Mixing**: The idea of dynamic mixing in Section 3.1 is an interesting extension. Though the end result is somewhat similar, I believe it is worth having some synthetic simulations on this too. Additionally, would it be possible to verify these empirical observations on any small-scale experiments with real data? \n\n* **Notations**: at times, vectors and matrices are not bold (e.g. to name a few, Lines 404, 412, 419). It is good to keep the notations consistent."}, "questions": {"value": "Some questions are raised in the Weakness section. Additionally, \n\n1. In the interpolation risk (Thm 3.1), the mixing ratio only affects the variance, but it affects both for the ridge model (Thm 3.2). This is an intriguing phenomenon (something similar also happened in double descent literature as I recall). Do you think there is any possible interpretation for this? \n\n2. How can the results possibly relate to the scenarios using real data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qxnOrJN8NE", "forum": "9RdhTvYbX0", "replyto": "9RdhTvYbX0", "signatures": ["ICLR.cc/2026/Conference/Submission20326/Reviewer_EFFX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20326/Reviewer_EFFX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893777395, "cdate": 1761893777395, "tmdate": 1762933786046, "mdate": 1762933786046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies model collapse in the context of overparameterized linear regression. The authors analyze the minimum-$\\ell_2$₂-norm interpolator and ridge regression, in a setting where each iteration mixes real and synthetic labels with a fixed proportion.\nThe paper provides precise asymptotic risk characterizations using random matrix theory and discuss the dependence on the mixing proportion and the interplay with regularization and SNR. They validate theory with simulations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a novel and technically solid contribution to the study of model collapse. It builds on a rigorous mathematical foundation, leveraging random matrix theory to generalize prior results that were limited to low-dimensional or Gaussian settings. The presentation is clear and well-structured, offering useful intuition for interpreting the theoretical findings, which could inform strategies for mitigating model collapse, when fresh labels are available. The numerical experiments are extensively detailed and effectively complement the theoretical analysis."}, "weaknesses": {"value": "The proposed framework may appear somewhat artificial, and its motivation is not sufficiently discussed in the introduction. This limits the perceived practical scope of the work and raises questions about its connection to real-world training dynamics. See also the Questions section."}, "questions": {"value": "1. Could the authors better justify the choice of this framework from a practical point of view? In particular, are there real-world scenarios that resemble the training procedure illustrated here? For instance, how should one interpret the assumption that at iteration $t$, the statistician looses access to the true labels $y_{t-1}, ..., y_1, y$, yet maintains constant access to new labels for the same covariates?\n2. Do you have intuition on how would the results change in a setting where, at iteration $t$, the dataset is obtained by union of the fresh and synthetic labels rather than their mixing? Would this alternative formulation still prevent model collapse?\n3. As a suggestion, the paper would benefit from a more extended discussion of prior literature. While several recent works on model collapse are cited, a short summary of their different frameworks and main findings would help clarify how the present contribution fits within the literature and emphasize its novelty."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xfG3m2vMYQ", "forum": "9RdhTvYbX0", "replyto": "9RdhTvYbX0", "signatures": ["ICLR.cc/2026/Conference/Submission20326/Reviewer_8T4F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20326/Reviewer_8T4F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995320640, "cdate": 1761995320640, "tmdate": 1762933785771, "mdate": 1762933785771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}