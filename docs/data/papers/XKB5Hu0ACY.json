{"id": "XKB5Hu0ACY", "number": 4837, "cdate": 1757774516398, "mdate": 1759898010244, "content": {"title": "Hyperbolic Aware Minimization: Implicit Bias for Sparsity", "abstract": "Understanding the implicit bias of optimization algorithms is key to explaining and improving the generalization of deep models. The hyperbolic implicit bias induced by pointwise overparameterization promotes sparsity, but also yields a small inverse Riemannian metric near zero, slowing down parameter movement and impeding meaningful parameter sign flips. To overcome this obstacle, we propose Hyperbolic Aware Minimization (HAM), which alternates a standard optimizer step with a lightweight hyperbolic mirror step. The mirror step incurs less compute and memory than pointwise overparameterization, reproduces its beneficial hyperbolic geometry for feature learning, and mitigates the small–inverse-metric bottleneck. Our characterization of the implicit bias in the context of underdetermined linear regression provides insights into the mechanism how HAM consistently increases performance --even in the case of dense training, as we demonstrate in experiments with standard vision benchmarks. HAM is especially effective in combination with different sparsification methods, advancing the state of the art.", "tldr": "We propose Hyperbolic Aware Minimization (HAM), a method that improves sparse training by preserving the benefits of hyperbolic implicit bias while avoiding slowdown caused by the vanishing inverse metric for parameter updates.", "keywords": ["Sparsity", "Implicit bias", "Sign flip", "Exponential update", "Training dynamics", "Bregman function"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8c3b5f4c70800f2fe8c41c0f9c80775c8253b68f.pdf", "supplementary_material": "/attachment/43ea44ada80ad5dfbde79ac6a56d6e6786c82c5f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Hyperbolic Aware Minimization (HAM), an alternating optimization step that combines the beneficial sparsity-promoting implicit bias of Hadamard $m \\odot w$ overparameterizations in a lightweight two-step minimization routine that aims to avoid the optimization and computational difficulties of direct $m \\odot w$ parameterization. Classical $m \\odot w$ parameterizations, in which each parameter is represented as a product of two factors, are known to be able to improve generalization and induce sparsity regularization, but suffer from a small inverse metric near zero that slows down small weights and impedes often important sign changes. \nHAM aims to overcome this issue by alternating a standard optimizer update like SGD/Adam with a hyperbolic mirror step whose inverse metric is lower bounded by 1 at the origin and thus promotes flexible learning dynamics even for parameters close to zero, while maintaining a similar implicit bias to the $m \\odot w$ parameterization. This mechanism facilitates sign plasticity, allowing parameters to adapt their sign in response to changing gradients, and introduces an implicit bias that interpolates between $L_2$ and $L_1$ regularization, promoting ‘mild sparsity’. Theoretical analyses based on Riemannian gradient flow and connections to exponential gradient descent as an approximation show that HAM circumvents the small-metric problem and that it can be shown to converge using standard methods and assumptions in the literature. Empirical results on ImageNet and CIFAR benchmarks demonstrate consistent generalization improvements in both dense and sparse regimes, whereas further experiments on a range of tasks showcase the broad use of HAM from linear regression to LLMs and graph neural networks. Combined with other sparse learning approaches, HAM is demonstrated to enhance popular methods like  AC/DC, RiGL, and STR, and works complementarily with Sharpness Aware Minimization. Overall, the work positions HAM as a geometry-utilizing optimization method that reproduces the positive effects of overparameterized training, like its implicit bias, but ensures stable sign dynamics, while remaining computationally efficient and broadly applicable."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The submission’s main strength lies in its conceptual originality and theoretical depth. It proposes a, to the best of my knowledge, novel and interesting idea of replacing the pointwise product parameterization with a direct modification of the training routine that reproduces a similar optimization geometry and implicit bias on the original weights. This reframing of the $m \\odot w$ parameterization as an alternating optimization scheme is both interesting and practically relevant, as it retains the sparsity-promoting benefits of overparameterized training without increasing model size, computational cost, or running into small inverse metric issues. The work also provides a clear and well-structured overview of existing sparsification methods, placing HAM within a broad literature that includes pruning at initialization, dense-to-sparse, and dynamic sparsity training. By focusing on the gradient dynamics and implicit bias of reparameterized models, the paper contributes to an important and active area of research that links optimization geometry to generalization (and sparsity), advancing the theoretical understanding of how optimizer design influences learning behavior and generalization. The analysis is mathematically rigorous, building on (Riemannian) gradient flow and mirror descent concepts, and derives the implicit regularization induced by HAM. \nThe empirical evaluation is performed on a large variety of tasks ranging from the simplest linear regression models to common vision benchmarks, LLM finetuning, and graph learning tasks. Overall, the paper nicely balances theoretical results with experimental evaluations, offering an innovative and well-substantiated contribution to the study of how geometry induces implicit bias for optimization in deep learning."}, "weaknesses": {"value": "Overall, I think the submission proposes an interesting and novel method that makes a neat paper contribution. However, some major weaknesses prohibit recommending acceptance in the submission’s current state. However, I think the majority of my concerns could be addressed in the rebuttal, which would change my rating if addressed adequately. In the following, the weaknesses are presented point-by-point.\n\n- W1: The paper’s main weakness is its exposition. This pertains to several aspects.\n\n- W1a.i  quality of exposition: In parts, the submission reads rather “unfinished”, including awkward theorem referencing, using inconsistent notation, referencing undefined equations, or unfinished sentences. Some proofs were poorly written and hard to follow due to a lack of contextualization (e.g., Lemma E.1, Theorem E.2, Theorem B.6), missing definitions, or a lack of explanations of why certain steps are performed. Actionable examples are in a separate suggestions list below. \n\n- W1a.ii clarity of exposition: I got the feeling that some statements or sections implicitly only cover a certain setting (like $\\beta=0$) without making this clear enough. It was not always obvious to me, e.g., if a statement is made about discrete vs continuous time, $\\beta=0$ vs $\\beta \\neq 0$ (e.g., line 298), if weight decay on $m$ and $w$  or Frobenius (product) weight decay was used for the spred/$m \\odot w$ experiments and analyses, or whether GD+HYP/GD+HYP* was used.\n\n\n- W1a.iii precision of exposition: the submission contains many statements that come off a bit too bold or sweeping, or make imprecise claims (e.g., a potential exaggeration of the small inverse metric problem for $m \\odot w$, or claiming that Frobenius weight decay is required for $m \\odot w$ to induce sparsity (line 1360); whereas,  to my knowledge, sparsity using this parameterization can be achieved either through implicit regularization, or by using weight decay on $m$ and $w$ separately, bur not with the mentioned Frobenius decay of the product). Actionable details are in the suggestions list below.\n\n\n- W1b: The submission seems a bit “all over the place” with the use cases of HAM. It remains unclear to me if its proposed use is mainly a general-purpose optimization step improving generalization, a method to induce and control implicit biases without direct overparameterization, a way to enhance existing pruning pipelines, or a sparse learning method on its own. \n\n\n- W1c: I am not entirely convinced by one of the main motivating arguments for HAM, namely, to mitigate the small inverse metric problem caused by direct $m \\odot w$ parameterization, which causes parameters to get stuck/decelerate around 0 while crossing signs. Fig. 1 displays the authors’ argument nicely. However, the inverse metric only vanishes for $\\gamma=0$, which only happens for balanced initializations of $m,w$, i.e., $m_0^2-w_0^2=0$ (or also for $t \\to \\infty$ if $\\beta>0$ by Eq. (2)). By choosing a random/imbalanced initialization for $m,w$, as used by Ziyin and Wang (2023) or Kolb et al. (2025), sign crossings are unproblematic since $\\gamma_0>0$. Since both works report no issues of achieving competitive performances directly using $m \\odot w$ in both linear or DNN models, it makes me wonder how relevant the sign flip/inverse metric problem for the $m \\odot w$ parameterization is in practice when not using the balanced initialization that was avoided in previous works? Ziyin and Wang (2023) even report “Our initial experiments find no significant difference between making the norm balanced or not at initialization”, contradicting the small inverse metric problem, which would be intensified by using a balanced initialization. Furthermore, the submission derives the inverse metric as $1+\\alpha |\\theta|$ when $\\beta=0$, being lower bound by 1. Similarly, the inverse metric for $m \\odot w$ with $\\beta=0$  is $\\sqrt{\\theta^2+\\gamma^2}$, reducing to $|4*(m_0^2-w_0^2)|$ at the origin, also serving as a positive lower bound for imbalanced initializations. What is the advantage of HAM over imbalanced initializations then? \n\n- W2: The selection of hyperparameters is not discussed for the main experiments; they are merely listed in the Appendix. Without further explanation/justification of their selection, this significantly weakens the empirical comparisons, since the results can only be assumed to hold for some arbitrary hyperparameter setting (which could potentially be biased to favor HAM). While extensive hyperparameter tuning for each method is infeasible for large-scale experiments, typically, some established set of hyperparameters, e.g., as found in related benchmark papers, is used to establish some form of comparability. This weakness is aggravated by the observation that the performances of their comparison methods sometimes differ strongly from the reported values in the original works (e.g., spred for ResNet50 on ImageNet at 80% sparsity is reported as >77% in Fig. 5 of Ziyin and Wang (2023), while the submission reports only 72.6% in Table 4; similarly STR for ResNet50 on ImageNet at 90% sparsity is reported as 74.31% (Table 1 of Kusupati et al, 2020), while the submission reports only 68.36% for the same setting in Table 10).\n\n\n**Sources**:\n\nZiyin and Wang (ICML, 2023): https://proceedings.mlr.press/v202/ziyin23a/ziyin23a.pdf\n\nKolb et al. (ICLR, 2025): https://openreview.net/pdf?id=vNdOHr7mn5\n\nKusupati et al. (ICML, 2020): https://arxiv.org/pdf/2002.03231"}, "questions": {"value": "In the following, questions (Q) and comments/suggestions (S) are given point-by-point. A dedicated response to all points, especially the suggestions, is not expected. \n\n\n- Q1: Related to W1b: What is the main purpose or application of HAM? Is it proposed mainly as a general-purpose optimization step, a modification to boost sparsification methods, a sparsification method in its own right, or a replacement of the $m \\odot w$ parameterization by the HAM step? Clearly, the method could be all of the above, but the exposition should highlight the principal use case.\n\n- Q2: Lines 17-18: The abstract states that HAM incurs less computational overhead than $m \\odot w$, but, e.g., Ziyin and Wang (2023) state that $m \\odot w$ on ResNet-18 takes less than 5% more time than without, which is almost negligible. Why or when is the claimed computational cost advantage (which is not empirically verified in the submission) of HAM over $m \\odot w$ relevant? \n\n- Q3: Why is Lemma 4.4 limited to $\\beta=0$?\n\n- Q4: Line 237: What is scaled and why by that factor? Context is missing here.\n\n- Q5: Why do the results between Tables 4 and 10 differ significantly for some of the overlapping methods, i.e.,  AC/DC and STR, as well as their sign-in/HAM variants? This seems particularly strange since the reported performances for Random(+Sign-In/HAM) are identical, suggesting identical settings.\n\n- Q6: Why does random pruning (baseline) outperform 4 out of 5 competitor methods in Table 10? This seems strange, particularly since Random performs worst across settings in Table 4.\n\n- Q7: Clearly, HAM provides an advantage over $m \\odot w$ with balanced initializations by enabling sign flips. But a simple remedy to the sign-flip problem could be to use a standard initialization for $m,w$. What additional purpose does HAM serve over $m \\odot w$ in that case?\n\n- Q8: Line 933: What justifies the use of the linearization $\\exp(x) \\approx 1+x$, which holds only around 0, specifically?\n\n- Q9: I don’t fully understand the difference between Theorem 4.2/B.3 (GF for Eqs GD;HYP) and Theorem B.6 (GF of HAM). Does Theorem B.6 correspond to Eqs GD;HYP*?\n\n- Q10: line 1252: Why does $w_i$ have an index $i$ but not $x$? And why does the setup define $z_i$ but the objective contains only $y_i$ and $x$?\n\n- Q11: Lemma E.1 is incomprehensible to me. Why a teacher-student set-up? Which four cases? The writing is also hard to understand.\n\n- Q12: Line 1435: In Fig. 5, it seems like none of the methods converge anywhere near close to the ground truth, with the lowest error still being larger than $0.01$ and already having leveled out. Why do seemingly all methods perform badly on this example?\n\nComments/suggestions:\n\n- S1: The exposition would benefit tremendously by using boldface small letters for vectors to distinguish them from scalars, and boldface capital letters for matrices.\n- S2: The re-statement of the theoretical results using a different label in the Appendix is very confusing (lines 923-927). Simply use the same names/labels in both the main text and the appendix for the same result\n- S3: Hyperbolic aware minimization implies that the proposed optimization step somehow utilizes the hyperbolicity of the loss landscape to improve generalization, but the proposed optimization step imitates increased hyperbolicity through point-wise overparameterization and then mitigates the resulting small inverse metric problem at the origin. Maybe the choice of the method's name is not optimal. Also, I think it should have been “hyperbolicity aware” and not “hyperbolic aware”.\n- S4: Lines 162-171: This paragraph misrepresents the scope of alternating or two-step optimization schemes, which have a long-standing history in optimization for ML. Important examples are proximal methods, soft thresholding, ADMM, alternating least squares/ridge, expectation maximization, etc. The paragraph should either give an appropriate overview or restrict itself to a certain scope of optimizers and modifications thereof. \n- S5: Consistently use $\\theta_{\\text{init}}$ instead of variants like $\\theta_{init}$ or $\\theta_{in}$\n- S6: Theorem 3.1 (line 211): The assumption on the initialization does not make sense to me, I think it should read $\\sqrt{|\\theta_0|}$ since theta is real-valued.  \n- S7: Clarify that Fig. 1 only holds for $\\gamma=0$ where the vanishing inverse metric problem occurs.\t\n- S8: lines 39-40: The methods by Kusupati et al. (2020) and Kolb et al. (2025) do not involve iterative parameter pruning as claimed\n- S9: line 199: Why is the condition $|w_0| \\leq m_0$ required? More explanation is needed.\n- S10: line 271: Many analyses are performed for $\\beta=0$, so shouldn’t the algorithm line read $\\geq 0$?\n- S11: line 291: Formulations like “mild sparsity” should be avoided and made more precise and rigorous.\n- S12: line 298: Why does this equation not contain the previously used weight decay parameter $\\beta$ anymore? Especially since the sentence in line 300 states that the inverse metric can change due to regularization.\n- S13: lines 315-316: The references are mixed up, Theorems A.7 and A.6 are applied, which are Theorems A.3 and 4.14 in the stated references.\n- S14: line 342: Sweeping statements like the one on convergence speed should be avoided if they are not made precise.\n- S15: lines 355-356: The target and input vectors should have $d$ not $n$ entries as defined in 353. Also the input should be a matrix, since each $z_i$ already is a vector.\n- S16: line 431: it is claimed that the optimal $\\alpha, \\beta$ are stable across the two tasks. But the best setting for CIFAR100 (Fig. 8) is $\\alpha=100, \\beta=3.2 \\times 10^{-2}$, whose closest neighbor in the ImageNet grid (Fig. 9) actually yields the worst performance. Looking at other experiments, the optimal values of $\\alpha$ seem to be unstable, ranging from as small as 1 to 200, or even -200.\n- S17: line 870: $\\lambda$ is undefined, I think this should be $\\Lambda$.\n- S18: line 954: The hyperlink in Eq. (3) actually references Eq. (HYP), not Eq. (3).\n- S19: line 1018: ‘indicate’ should be ‘indicates’\n- S20: lines 1035-1036: Sentence unclear to me, writing could be improved.\n- S21: line 162: Shouldn’t $\\delta$ be a function of $\\theta_{j+1/2}$ and $\\theta_{j}$, and the conditions in the parenthesis depend on both their signs?\n- S22: line 1044: $\\hat{\\theta}$ does not appear in Equation 8.\n- S23: lines 1088-1094: I can’t follow the writing. The exposition here could be improved.\n- S24: line 1246: ‘vain’ should be ‘vein’.\n- S25: line 1249: $\\mathcal{N}$ should be used consistently for a Gaussian. Moreover, previously, the main text used $d$ to denote the number of samples, not $k$.\n- S26: line 1252: I think there are incorrect indices in the equation (see Q10).\n- S27: lines 1260-1262: The text references an undefined balance equation and stops in the middle. \n- S28: line 1268: Text references undefined ‘balance equation’.\n- S29: line 1286-1287: I think the statement might be incorrect. If $a_{in}=w_{in}>0$, then it can’t be true that $a_{in}<-w_{in}$, which is the requirement for belonging to $\\Gamma_0$. Also $\\bar{\\Gamma}_{0}$ is undefined.\n- S30: line 1357: Why is suddenly Frobenius decay applied to the product weight instead of weight decay separately for $m$ and $w$ as in lines 192-193? This would not induce sparsity, only separately applying weight decay induces sparsity.\n\n\n**Sources**:\n\nZiyin and Wang (ICML, 2023): https://proceedings.mlr.press/v202/ziyin23a/ziyin23a.pdf\n\nKolb et al. (ICLR, 2025): https://openreview.net/pdf?id=vNdOHr7mn5\n\nKusupati et al. (ICML, 2020): https://arxiv.org/pdf/2002.03231"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F6QHcWHIjp", "forum": "XKB5Hu0ACY", "replyto": "XKB5Hu0ACY", "signatures": ["ICLR.cc/2026/Conference/Submission4837/Reviewer_qGRF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4837/Reviewer_qGRF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761057509353, "cdate": 1761057509353, "tmdate": 1762917603476, "mdate": 1762917603476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new optimizer by adding one additional hyperbolic mirror step to the original gradient descent (GD). This modification is originally inspired by the sparsity implicit bias of point-wise overparameterization. The main contribution of this paper lies in that the newly designed approach further mitigates the vanishing inverse metric issue near zero of prior approaches. The authors analyze their method in the linear regression setup, and further demonstrate the effectiveness of their method in different benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-motivated and well-organized. Specifically, \n   - The authors first state the sparsity implicit bias of point-wise overparameterization, which can be applied to achieve sparsity of training models by realizing the resulted Riemannian gradient flow. This gives the start point of this paper.\n   - Then the vanishing inverse metric issue is identified, which induce the hardness of sign flip prior approaches. This gives the motivation of this paper.  \n   - The authors then solve this issue by adding a constant 1 to $| \\theta |$ (this is in fact an approximation of $\\sqrt{ \\theta^2 + \\gamma^2}$ when $\\gamma \\to 0$) in the inverse metric. This is well-motivated in my view. \n   - Finally, the authors propose to use a two-step update manner to apply this newly designed inverse metric of  Riemannian flow. \n\n2. This paper provides a theoretical foundation of their method, which can help the readers understand some basic properties of this newly proposed optimizer in a very simple setting. Then this theoretical analysis is further supported by empirical evaluations on vision benchmarks where several methods are compared and discussed."}, "weaknesses": {"value": "1. There lacks intuitive explanation on the connection between sign flip and performance gaining. While the authors motivate that the vanishing inverse metric is an issue of prior approaches, there still lacks clear evidence that solving this issue is strongly related to the performance gaining. This makes the scope of the applicability of HAM unclear.\n\n2. The theoretical analysis still has limitations. The theoretical analysis is based on very limited settings. This leaves several important questions unanswered, e.g., while the authors claimed a faster convergence rate compared to GD in the simple setting, the readers are still unclear whether this is true in more general setting. \n\n3. This HAM highly depends on the value of $\\nabla f$, hence how to choose a suitable $\\alpha$ can be very tricky. For example, the range of $\\alpha$ is not restricted so that it goes from 0 to a very large constant. This makes it very hard and non-intuitive to pick a suitable $\\alpha$. This unavoidably gives parameter tuning burden in practical training. \n\n4. Minor: The replacement of $sign(\\theta_0)$ with $sign(\\theta_k)$ is confusing: the study of object in Theorem 3.1 applies $\\theta_0$ but this is replaced by $\\theta_k$ (and further replaced by $\\theta_{k + 1 / 2}$ later). I'm a bit confused about why the authors start with analyzing the version with $\\theta_0$ instead of directly proposing HYP."}, "questions": {"value": "1. The questions raised in the Weaknesses: a. What is the connection between sign flip and performance improvements? b. How do we define the range of $\\alpha$? \n\n2. The two-step update manner makes me wonder the comparison with other two-step methods, e.g., momentum methods. Can the authors discuss the comparison regarding several aspects such as convergence speed (as the authors claim that HAP has a faster convergence rate compared to GD) with some of other two-step methods?\n\n3. Is it possible to design a one-step update optimizer inspired by HAM?\n\n4. Prior works revealed that stochasticity can boost sparsity implicit bias of point-wise overparameterization, then how does stochasticity affect HAM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PW2yOh03lZ", "forum": "XKB5Hu0ACY", "replyto": "XKB5Hu0ACY", "signatures": ["ICLR.cc/2026/Conference/Submission4837/Reviewer_FKu7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4837/Reviewer_FKu7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569861799, "cdate": 1761569861799, "tmdate": 1762917603085, "mdate": 1762917603085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the training dynamics of pointwise overparametrization, where a parameter is replaced by a product $m \\odot w$.\nThe authors argue that this reparametrization, when studied as a Riemannian gradient flow, suffers from a small inverse metric near $\\theta = 0$.\nThis can cause parameters to get \"stuck\" and prevent crucial sign flips, and existing remedies like Sign-In introduce perturbations that do not clearly improve training dynamics.\nTo address this, the authors propose Hyperbolic Aware Minimization (HAM), an optimization step designed to capture the benefits of pointwise overparametrization without getting stuck near zero.\nThe main idea in HAM is to alternate a standard optimizer step with a hyperbolic mirror step.\nExperiments on vision models show that HAM improves the final generalization of state-of-the-art sparse training methods like AC/DC and RiGL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and clearly identifies a gap in the existing literature.\n\n2. The theoretical contribution is clear.\n\n3. The experiments appear to support the claims."}, "weaknesses": {"value": "While I did not see any obvious weaknesses, I also cannot strongly endorse this paper as I am an outsider to this area."}, "questions": {"value": "Q1. It's not entirely clear to me why HAM's hyperbolic steps on a \"dense\" $\\theta$ should preserve the benefits of optimizing on the product $m \\odot w$. Could the authors please give me an intuitive explanation as to why?\n\nQ2. Could the authors please expand on what the advantages of a product overparametrization are compared to the others discussed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "DjCME2ISMX", "forum": "XKB5Hu0ACY", "replyto": "XKB5Hu0ACY", "signatures": ["ICLR.cc/2026/Conference/Submission4837/Reviewer_q7Do"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4837/Reviewer_q7Do"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889373056, "cdate": 1761889373056, "tmdate": 1762917602575, "mdate": 1762917602575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new optimizer step: Hyperbolic-Aware Minimization (HAM), where first a nominal step is taking by the base optimizer (e.g. Gradient Descent), then a \"hyperbolic\" step analogous to exponential gradient descent. By taking the sign of the previous iterate into account, this method reduces the updated iterate's magnitude for the components of the previous iterate and the gradient that agree, otherwise the exponential step blows up the updated iterate's magnitude to exaggerate the descent step on disagreeing components. The optimizer behavior is supported by deriving the corresponding Riemannian gradient flow, and applying it to the toy problem of underdetermined linear regression. Therein, it is found that HAM induces an in-between of $L^1$ and $L^2$-flavored regularization, with the behavior approaching $L^1$ when the hyperparameter $\\alpha$ is large. It is noted that this implicit bias toward sparsity is rather weak. The efficacy of HAM is then demonstrated on a wide range of training tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes an interesting optimizer modification that attempts to reap the benefits of \"point-wise\" overparameterization via parameter-wise multiplication without introducing a memory overhead. The sign-adjustment using the previous iterate in the hyperbolic step seems to be novel, and has an interesting interpretation via Riemannian gradient flow. The experiments seem to demonstrate that HAM provides modest improvements across many scenarios as a drop-in adjustment, which seems to imply its general-purpose nature. Furthermore, it seems that the \"sparsity-friendly\" nature of HAM synergizes well with pruning or sparse training methods."}, "weaknesses": {"value": "Overall, to me it is not made clear in Sections 3 and 4 the main motivating purpose of HAM. In particular, it is claimed via a Riemannian gradient flow analysis that HAM converges faster than gradient flow, but the main numerical results do not seem to focus on this. It is then claimed that HAM encourages a particular kind of implicit sparsity bias, but it is noted by the authors in Remark 4.7 that this is relatively weak alone. As such, it would be good to clarify the core motivation of HAM, and how the claimed benefits are supported by the theoretical derivations. Furthermore, neither theoretical intuition lends an explanation to why HAM seems to get better *generalization* on dense training tasks. Though the effect is good to see, does any of the theoretical motivation suggest this?\n\nSecondly, though the optimizer is presented as a drop-in adjustment, it does introduce two new hyperparameters in $\\alpha, \\beta$ without intuitive guidance of how these should be set. Indeed, the hyperparameter sweeps in the appendix seem to suggest the choice of $\\alpha$ is important, and that the optimal choice of $\\alpha$ can vary to be quite large, and the optimal corresponding choice of $\\beta$ also varies quite significantly on a logarithmic scale between $0$ and $1$. Without additional guidance or intuition, this means HAM introduces two additional independent hyperparameters to sweep, which is non-trivial to grid-search over."}, "questions": {"value": "On the note of the hyperparameter scale, in line with recent lines of literature studying the effective learning rate induced by various hyperparameters, is it possible that the \"correct\" scale of $\\alpha$ (and $\\beta$) actually has an implicit dependence on the scale of the parameter block (e.g. width)? For example, the Maximum Update Parameterization framework (see e.g. [1]) suggests that layer-wise descent methods require a per-layer adjustment based on the dimensionality in order to guarantee stable evolution across model scales. It seems that an entrywise modification like HAM should also naturally admit some layerwise dimensionality scaling. In particular, did the authors experience the optimal choice of $\\alpha, \\beta$ changing across different model sizes?\n\nThe HAM step compares the sign of the gradient with the iterate. When using a different descent method than GD, is the hyperbolic step adjusted accordingly, or does it still compare to the gradient?\n\n[1] Yang et al. \"Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dnzimLowDT", "forum": "XKB5Hu0ACY", "replyto": "XKB5Hu0ACY", "signatures": ["ICLR.cc/2026/Conference/Submission4837/Reviewer_vz7i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4837/Reviewer_vz7i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006356999, "cdate": 1762006356999, "tmdate": 1762917602291, "mdate": 1762917602291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper studies the training of sparse networks, specifically those which learn the mask as $\\theta = m \\odot w$\n- This paper builds on previous work which identify a limitation of these models in learning sign flips due to the small inverse Riemannian metric as $m$ or $w$ approaches zero\n- Motivated by the reparameterized gradient flow of these masked models, the authors propose interleaving hyperbolic updates between regular gradient steps.\n- The hyperbolic updates inherit the favourable sparsity inducing dynamics from training masked models, while the regular gradient steps handle sign flips\n- Empirically, HAM outperforms methods which only use the base optimizer on ImageNet and CIFAR-100 on both dense and sparse training"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation and presentation\n- The method is compatible with existing optimizers (such as SAM), and requires little computational overhead\n- Strong empirical results - with the method surprisingly also improving dense training\n- Thorough theoretical analysis"}, "weaknesses": {"value": "- The method additionally adds two hyperparameters ($\\alpha$, $\\beta$). Based on figure 8 and 9 in the appendix, the performance of HAM is quite sensitive of these hyperparameters, and optimal hyperparameters seems to differ between models, making it difficult to tune\n- It seems that although HAM is motivated from the perspective of sparse training, it doesn't actually improve sparse training directly without pairing it with another method (remark 4.7 and 4.8). Given this, the motivation behind HAM seems less clear"}, "questions": {"value": "- I'm generally interested in evaluating whether the regular optimization step is strictly to facilitate sign flips, or offers more performance beyond that. To this end, I was wondering, (1) what is the effect of only the hyperbolic step, and (2) what is the effect of only applying the regular gradient step for small magnitude weights (i.e. ones whose signs could be flipped).\n- What kind of sparsity does HAM trained models get (without combining with any sparse training). I.e. is direct magnitude pruning at the end of initialization more successful with HAM than with standard training due to HAM's implicit bias to sparse networks?\n- Can the authors offer any intuition why HAM improves performance in dense training? Given that the motivation of the paper was directed towards sparse training, the improvement in dense training is surprising."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wMlaXHK2yB", "forum": "XKB5Hu0ACY", "replyto": "XKB5Hu0ACY", "signatures": ["ICLR.cc/2026/Conference/Submission4837/Reviewer_PXeq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4837/Reviewer_PXeq"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762295503021, "cdate": 1762295503021, "tmdate": 1762917602089, "mdate": 1762917602089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}