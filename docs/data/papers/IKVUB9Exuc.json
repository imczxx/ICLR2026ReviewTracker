{"id": "IKVUB9Exuc", "number": 24071, "cdate": 1758352397146, "mdate": 1759896783176, "content": {"title": "Adaptive Collaboration with Humans: Metacognitive Policy Optimization for Multi-Agent LLMs with Continual Learning", "abstract": "While scaling individual Large Language Models (LLMs) has delivered remarkable progress, the next frontier lies in scaling collaboration through multi-agent systems (MAS). However, purely autonomous MAS remain ``closed-world'' systems, constrained by the static knowledge horizon of pre-trained models. This limitation makes them brittle on tasks requiring knowledge beyond training data, often leading to collective failure under novel challenges. To address this, we propose the Learning to Intervene via Metacognitive Adaptation (LIMA) framework, a principled paradigm for human--agent collaboration. LIMA trains agents to learn a metacognitive policy that governs when to solve problems autonomously and when to defer to a human expert. To operationalize this policy, we introduce Dual-Loop Policy Optimization (DLPO), which disentangles immediate decision-making from long-term capability growth. The inner loop applies Group Relative Policy Optimization (GRPO) with a cost-aware reward to optimize deferral decisions, while the outer loop implements continual learning, transforming expert feedback into high-quality supervised signals that strengthen the agent's reasoning ability. Experiments on challenging mathematical and problem-solving benchmarks show that LIMA, equipped with Dual-Loop Optimization, consistently outperforms state-of-the-art MAS, establishing a principled foundation for collaborative and continually improving agentic systems.", "tldr": "", "keywords": ["Multi-Agent", "Adaptive Collaboration", "Metacognitive"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fad6e9b88e982b5aa257f1daba707a33bc9b3d1f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the \"closed-world\" limitation of autonomous multi-agent LLM systems (constrained by pre-trained knowledge, prone to collective failure) by proposing the LIMA framework for human-agent collaboration and DLPO dual-loop optimization. LIMA gives agents a metacognitive policy to choose autonomous problem-solving (evaluate/generate solutions) or human deferral, using a structured cognitive state space. DLPO’s inner loop (GRPO with cost-aware rewards) refines deferral decisions, while the outer loop turns expert feedback into supervised signals for continual learning. Experiments on reasoning benchmarks (MMLU, HumanEval, GSM8K, etc.) show LIMA outperforms state-of-the-art autonomous MAS across LLM backbones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses multi-agent LLM systems’ \"closed-world\" flaw with the LIMA framework, integrating metacognitive assessment (via structured cognitive states) to let agents strategically choose autonomy or human help, surpassing passive \"human-in-the-loop\" designs. The DLPO dual-loop optimization creatively unites short-term decision refinement (GRPO) and long-term capability growth (expert feedback-driven learning), solving prior disconnections between decision and knowledge enhancement. Formal mathematical formulations boost reproducibility.\n\n2. The experimental design is comprehensive. It tests across three key domains (general knowledge, program synthesis, math reasoning) with diverse benchmarks, ensuring generality. A broad baseline set (single-agent, interactive, system-level) enables fair SOTA comparisons. Cross-model tests (Llama3, Qwen2.5) show adaptability, even improving small models. Ablation studies and configuration analyses strengthen conclusions, while AI proxy and real human expert tests enhance real-world relevance.\n\n3. The paper is clear, well-structured, and accessible."}, "weaknesses": {"value": "1. While the paper identifies \"4 agents\" as the optimal configuration, it provides no analysis of why performance plateaus beyond this number, nor tests scalability to larger agent populations (e.g., 8+ agents) , a key gap for deploying the framework in complex tasks (e.g., multi-step scientific research). The current LIMA design relies on full inter-agent consistency checks to inform metacognitive decisions, which would incur prohibitive communication and computation costs as agent count grows (e.g., O(n²) consistency comparisons for n agents)\n\n2. The paper frames \"long-term capability growth\" as a core goal of DLPO’s outer loop, but its experiments only measure performance improvements over fixed benchmarks (e.g., MMLU, GSM8K) rather than tracking sustained learning across sequential, novel tasks. For example, it does not test whether the framework retains previously learned skills when fine-tuned on new task feedback (e.g., first learning to solve GSM8K, then AMC, then retaining GSM8K performance). This omission hides potential catastrophic forgetting."}, "questions": {"value": "Please see weaknesses.  I will adjust the paper’s score accordingly in my rebuttal to reflect the strengths identified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y4PCt80Sdd", "forum": "IKVUB9Exuc", "replyto": "IKVUB9Exuc", "signatures": ["ICLR.cc/2026/Conference/Submission24071/Reviewer_mLVp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24071/Reviewer_mLVp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665272905, "cdate": 1761665272905, "tmdate": 1762942921639, "mdate": 1762942921639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The Learning to Intervene via Metacognitive Adaptation (LIMA) framework introduces a novel approach to human–agent collaboration, enabling agents to metacognitively determine when to defer to human expertise. To train these agents, the authors propose Dual-Loop Policy Optimisation (DLPO), which separates short-term deferral decisions from long-term skill development. DLPO’s inner loop uses GRPO with a cost-aware reward, while its outer loop incorporates expert feedback for continual learning. Experiments on mathematical reasoning and general problem-solving tasks show that LIMA with DLPO outperforms existing autonomous multi-agent systems, offering a strong foundation for adaptive, continually improving collaboration between humans and AI, especially investigating when and how to consider human intervention."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novelty of the idea is good"}, "weaknesses": {"value": "- Could include further related work such as \"LLM-Mediated Guidance of MARL Systems\" (Siedler et al) to \"human-in-the-loop\" paragraph\n- Figure 1 has not been incorporated in the text (sorry if i missed this)\n- A flow/process diagram supporting the methodology would elevate comprehension drastically\n- I would not consider MMLU as a \"general knowledge reasoning\" benchmark - its language understanding\n- I think there should have been either more experiments for models such as \"GPT-4o-mini as a proxy human expert\" to verify the \"human expert\" llm, or real human (at least partly) in the loop to verify the llm based \"human expert\".\n- For the \"Human involvement under two modes\" Table 3, why are there only reports on GSM8K and AMC? This feels cherry-picked\n- For the results in \"Figure 3: Effect of scaling collaborative configurations.\", I think you should also consider additional inference costs per performance increase\n- \"identify their knowledge boundaries\" - Unfortunately, there are no results supporting this claim\n- \"assess its own certainty\", \"its relation to the collective\", \"intrinsic quality of its current reasoning path\" - Unfortuantely also here, no results supporting this claim at all"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YpjqJz787R", "forum": "IKVUB9Exuc", "replyto": "IKVUB9Exuc", "signatures": ["ICLR.cc/2026/Conference/Submission24071/Reviewer_TKNM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24071/Reviewer_TKNM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829602754, "cdate": 1761829602754, "tmdate": 1762942921315, "mdate": 1762942921315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how to learn \"when to seek help from humans and how to continually learn from human feedback\" in human–agent collaboration. The core idea is to enable multi-agent systems to proactively trigger \"deferral actions\" (defer action) in situations of uncertainty (confidence) or beyond their capabilities. Then leverage feedback for continuous optimization. The inner loop is used for immediate decision optimization, while the outer loop is used for long-term capability growth. The experiments cover various reasoning tasks (GSM8K, MATH, AIME, AMC, HumanEval and MMLU), demonstrate the framework's effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel Framework: The idea that an agent can adaptively seek help from humans based on its own capabilities while continually improving itself through higher-level feedback is novel.\n- Method's Effectiveness: From paper's table 1, LIMA and LIMA (w/ DLPO) acquire a significant gains compared to baseline models across six representative benchmarks."}, "weaknesses": {"value": "- For human expert: The paper mentions “real human experts” but does not clarify who these participants are, how they were selected, or whether the framework accounts for human cognitive cost or fatigue.\n- Performance gains: The relationship between this performance gain and external feedback requires careful analysis.\n- Presentation: Table 2 column 1 Model.\n- Although the proposed method achieves improvements across the six benchmarks in Table 1, its performance remains below the sota （broad perspective) results on these benchmarks, which weakens the overall significance of the approach."}, "questions": {"value": "- Corresponding to weakness1: How human expert were selected? who these participants are?\n- Corresponding to weakness2: How much of the performance gain comes from external high-level feedback? Could the presence of such advanced feedback (from human experts) lead to unfair comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qNLWpMgP8y", "forum": "IKVUB9Exuc", "replyto": "IKVUB9Exuc", "signatures": ["ICLR.cc/2026/Conference/Submission24071/Reviewer_QNPF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24071/Reviewer_QNPF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911022834, "cdate": 1761911022834, "tmdate": 1762942920908, "mdate": 1762942920908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LIMA, a metacognitive framework where LLM agents can decide whether to exploit current knowledge or to defer to a human expert for help. This decision policy is trained with Dual-Loop Policy Optimization (DLPO), where the inner loop is GRPO with a cost-aware reward for deferring, and the outer loop is continual learning where expert demonstrations are turned into lasting capabilities. Extensive experiments across multiple datasets all confirm the effectiveness of the proposed method."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* Paper is well-written, technically sound, and easy to follow.\n\n* The method proposed is novel and principled. The option to defer to expert is interesting for a multi-agent system. And the dual-loop training process combined into a single DLPO loss is principled and easy to implement.\n\n* The empirical evaluations are extensive across multiple benchmarks, as well as ablation and scaling studies, which all demonstrate the superiority of the method proposed."}, "weaknesses": {"value": "* Cost model is constant $C$, which seems to be a strong assumption. There are many scenarios where querying the expert with different levels of question would incur different costs. It is also unclear how sensitive the outcomes are to C.\n\n* \"Human\" is proxied by gpt-4o-mini for the main experiments. It is interesting to see how this would scale with more capable models like gpt-4o."}, "questions": {"value": "1. How should the deferring cost C be set in practice?\n\n2. The goal of GRPO is to maximize the cumulative reward for each episode. Why can't the agents first learn to always defer to experts to learn the capabilities, and then choose to always not defer to maximize the return?\n\n3. Do you have any mechanism to prevent forgetting the knowledge given by the expert?\n\n4. It will also be interesting to see how well this method perform in single-agent domains as I think this method is perfectly applicable to single-agent as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8dWxcHtuMf", "forum": "IKVUB9Exuc", "replyto": "IKVUB9Exuc", "signatures": ["ICLR.cc/2026/Conference/Submission24071/Reviewer_3GsK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24071/Reviewer_3GsK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24071/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977351465, "cdate": 1761977351465, "tmdate": 1762942920723, "mdate": 1762942920723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}