{"id": "BaG67KNavy", "number": 24035, "cdate": 1758351982212, "mdate": 1759896785259, "content": {"title": "Resolving the Security-Auditability Dilemma with Auditable Latent Chain-of-Thought", "abstract": "Reasoning-based methods have emerged to overcome the limitations of 'shallow alignment' by exposing the model's Chain-of-Thought (CoT), enabling auditability through both training-phase supervision and post-generation verification. However, this transparency creates a critical vulnerability, a tension we define as the \\textbf{Security-auditability Dilemma}: the very mechanism of exposing the model’s safety reasoning for auditability inadvertently leaks harmful information and creates a vulnerable attack surface against adaptive\nattacks. To address this, we propose \\textbf{Auditable Latent CoT Alignment (ALCA)}, a framework that decouples internal reasoning from external output. ALCA shifts the safety deliberation process into a continuous latent space, rendering it opaque to adversaries. Yet, this process is not a black box; we introduce a \\textbf{Self-Decoding} mechanism that allows the model to reconstruct its latent reasoning into human-readable text for supervisory auditing. Extensive experiments show that ALCA achieves robustness alignment, reducing the success rate of adaptive jailbreak attacks by over 54\\% compared to strong baselines, while preserving performance. Our framework presents a path toward building LLMs that are both robustly secure and auditable.", "tldr": "", "keywords": ["Safety Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2e63d4fc3d0fdbd2df552d087e9a06f92a25affa.pdf", "supplementary_material": "/attachment/e71b4c138f712caf0794ec5954d92e982c231467.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes **Auditable Latent Chain-of-Thought Alignment (ALCA)**, a framework that aims to balance *security* and *auditability* in safety-aligned large language models.  \nThe authors argue that exposing explicit reasoning traces (Chain-of-Thoughts) improves transparency but simultaneously enables jailbreaks and prompt-injection attacks.  \nALCA attempts to solve this by encoding reasoning in a **latent space**, inaccessible to adversaries, while providing an **auditing mechanism** that can decode latent representations into interpretable rationales for safety verification.  \nExperiments are conducted on several LLMs (LLaMA-3-8B, Mistral-7B, Qwen2-7B) using multiple jailbreak benchmarks (GCG, AutoDAN, PAIR)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper raises an important and underexplored issue in model alignment: the inherent tradeoff between **security** and **auditability**.  \n- The idea of performing safety reasoning in a **latent space** is conceptually appealing and may inspire future research.  \n- Evaluation includes several modern jailbreak methods (GCG, AutoDAN, PAIR), which shows awareness of the current security landscape.  \n- The ablation experiments (latent-only vs. causal-only vs. hybrid) offer some insight into how different supervision components contribute to robustness."}, "weaknesses": {"value": "- **Lack of experimental rigor:** Attack success rate results are not averaged across runs or accompanied by standard deviations.  \n- **Unclear evaluation metrics:** GPT-4-based judgments are used without assessing consistency or inter-run variability.  \n- **Incremental novelty:** The method builds upon prior latent reasoning and safe decoding techniques without introducing a fundamentally new idea.  \n- **Missing cost analysis:** There is little discussion of computational overhead or latency introduced by latent decoding and probing.  \n- **Presentation issues:** Figures are difficult to interpret, and **approximately half of the paper’s text is rendered in bold font**, which significantly reduces readability and suggests formatting errors in the submission.\n- **Reproducibility gaps:** Experimental details (training hyperparameters, dataset splits, and attack configurations) are missing, making replication difficult."}, "questions": {"value": "1. How many independent runs were performed for each Attack Success Rate (ASR) in Table 2, and were statistical confidence intervals reported?  \n2. Since GPT-4 is used for evaluation, did the authors validate the consistency of its jailbreak-judgment outcomes across random seeds or prompt rephrasings?  \n3. Could the authors provide quantitative measurements (e.g., GPU hours, latency) comparing ALCA with baseline safe-decoding methods such as STAIR or COCONUT?\n4. How does ALCA fundamentally differ from frameworks like **CoIn**, which already achieve auditability of hidden reasoning through token-level verification and cryptographic attestation?  Specifically, what additional capability does latent-space reasoning provide beyond CoIn’s measurable and verifiable auditing approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3zhk2fl4Kc", "forum": "BaG67KNavy", "replyto": "BaG67KNavy", "signatures": ["ICLR.cc/2026/Conference/Submission24035/Reviewer_d8ZB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24035/Reviewer_d8ZB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761335184634, "cdate": 1761335184634, "tmdate": 1762942907317, "mdate": 1762942907317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the \"Security-Auditability Dilemma\" in LLM safety alignment: exposing chain-of-thought reasoning for auditability creates vulnerabilities to adaptive attacks. The authors propose ALCA (Auditable Latent CoT Alignment), which performs safety reasoning in continuous latent space (invisible to adversaries) while maintaining auditability through self-decoding. Experiments across three models show 54% reduction in adaptive attack success rates compared to baselines while preserving downstream performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel problem formulation: The Security-Auditability Dilemma identifies a real tension in current safety alignment approaches that deserves attention.\n\n2. Comprehensive empirical evaluation: Testing across multiple models (Llama-3, Mistral-7B, Qwen2) and attack methods provides breadth.\n\n3. Creative technical approach: Moving reasoning to latent space while maintaining decodability through self-decoding is innovative.\n\n4. Strong motivating experiments: Section 2 effectively demonstrates the dilemma through controlled experiments."}, "weaknesses": {"value": "1.  Circular evaluation methodology: Using the model itself to evaluate reconstruction fidelity (Table 3, semantic similarity 0.96) is methodologically flawed. Independent human evaluation or external metrics are essential for trustworthy assessment.\n\n2. Missing theoretical foundations: The \"equivalent conditions\" (Section 3.1) assume idealized scenarios. No formal analysis proves latent reasoning preserves safety properties or that self-decoding is faithful.\n\n3. No adversarial analysis of ALCA: The paper doesn't consider attacks targeting the probe classifier or attempting to manipulate mode selection. For a security paper, this is a critical omission. Adversaries could learn to trigger incorrect mode selection.\n\n4. Training instability: Figure 4 shows latent-only training catastrophically collapses mid-training. This suggests the method is fragile and may be difficult to reproduce reliably.\n\n5. Insufficient ablation studies: Only N (latent steps) is studied. What about probe architecture, trigger mechanisms, loss weights? The selection of layer 28 for probing appears arbitrary."}, "questions": {"value": "1. How do you handle probe misclassification? What's the false positive/negative rate under adversarial pressure specifically targeting the probe?\n\n2. Can you provide independent evaluation of self-decoding fidelity using human judges rather than the model itself?\n\n3. What happens when the 4-10% semantic information lost during reconstruction includes safety-critical details?\n\n4. How does ALCA perform against adversaries aware of its architecture who specifically try to exploit the probe or latent mechanism?\n\n5. Why choose layer 28 for probing? Did you experiment with other layers or adaptive layer selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w3ZNkPMksk", "forum": "BaG67KNavy", "replyto": "BaG67KNavy", "signatures": ["ICLR.cc/2026/Conference/Submission24035/Reviewer_H3gY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24035/Reviewer_H3gY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761612672742, "cdate": 1761612672742, "tmdate": 1762942906823, "mdate": 1762942906823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Auditable Latent CoT Alignment (ALCA) to address a key vulnerability in CoT-based safety alignment: when explicit safety reasoning is visible to attackers, jailbreaks can exploit it. ALCA:\n\nUses a probe to detect safety-relevant reasoning steps\n\nExecutes those steps as latent autoregressive deliberation in hidden states\n\nSupports self-decoding for auditability\n\nExperiments show reduced attack success rate (ASR drops from ~65% → ~9%) without harming helpfulness. Hidden CoT significantly improves resilience vs. explicit safety-CoT baselines.\n\nThis work is timely and provides a practical path to improve alignment robustness under adversarial prompting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Clear motivation:\nThe paper articulates the security–auditability dilemma clearly and supports it empirically (Table 1).\n\nWell-designed architecture:\nThe three-component alignment strategy — probing → latent reasoning → self-decoding — is conceptually coherent and technically implementable.\n\nSuperior robustness against jailbreak attacks:\nAttack Success Rate significantly drops compared to all explicit-CoT safety baselines (Table 2)."}, "weaknesses": {"value": "Although ALCA improves robustness against jailbreak attacks, it remains limited to security-related CoT. Many real-world failures involve broader hallucinations (e.g., fabricated facts, URLs, or numbers), and it is unclear whether the approach generalizes to these cases. Clarifying this applicability would strengthen the practical impact."}, "questions": {"value": "Can ALCA handle hallucinations unrelated to safety refusals, such as fabricated URLs, incorrect medical facts, or misleading numeric claims? If not, how do the authors envision extending ALCA to these scenarios?\n\nHow does the probe distinguish between “security reasoning” and other forms of critical reasoning (e.g., factual validation)? Could misclassification lead to harmful latent reasoning being output without scrutiny?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ORCNgzczVn", "forum": "BaG67KNavy", "replyto": "BaG67KNavy", "signatures": ["ICLR.cc/2026/Conference/Submission24035/Reviewer_oHig"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24035/Reviewer_oHig"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761690061092, "cdate": 1761690061092, "tmdate": 1762942906477, "mdate": 1762942906477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper highlights a “Security-Auditability Dilemma” that exists with reasoning models, where exposing reasoning traces can useful for transparency but can create vulnerabilities and information leakages.\n- The paper first performs experiments to provide evidence of this dilemma. They show that reasoning can improve safety to non-adaptive attacks but that reasoning is still vulnerable to adaptive attacks. They also show that masked reasoning methods greatly outperform non-reasoning, highlighting the value of maintaining reasoning (despite vulnerabilities in vanilla reasoning).\n- ALCA is proposed as a solution to the security vulnerability dilemma. ALCA works as follows: (1) Trains a probe to identify when a future reasoning step may be harmful to reveal. (2) Trains the LLM to generate reasoning in a latent space when the probe triggers. (3) Trains the model to decode its latent reasoning into text when a special token is inserted.\n- Experimental results are presented which provide evidence that ALCA maintains the models capabilities, while reducing attack success rates versus baselines. They also show that the decoded latents have semantic similarity to ground-truth texts. Combined, these results are evidence of ALCA producing an improvement in the security-auditability pareto fronteir."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper highlights the Security-Auditability Dilemma. This appears to be a novel contribution and an important dilemma worth noting and addressing. They also provide empirical results to validate the existence of this dilemma.\n- The explanation of the proposed method and solution is mostly clear\n- They provide evidence that their ALCA method functions as intended, and could be a solution to the Security-Auditability Dilemma: (i) It reduces attack success rates (ASR), demonstrating mostly reduced ASR versus the presented baselines, (ii) they present evidence that the decoding method works, meaning auditability can be maintained, (iii) they provide evidence that ALCA models maintain good performance on capabilities benchmarks."}, "weaknesses": {"value": "**Motivating the utility of providing user-facing reasoning traces**\n\nThe authors could perhaps do a better job at motivating the utility of presenting CoT reasoning traces to users (who could be potential attackers). A solution to the security issue is to hide the reasoning completely from users. However, there may be reasons we would still like to show users as much reasoning as possible. The paper does not seem to describe these reasons very well - the reasons it does touch on, such as “transparent reasoning traces as supervision target in training”, do not seem relevant for user-facing applications.\n\n**ALCA seems a convoluted solution, simpler baselines may exist**\n\nALCA seems to be an overly convoluted and complex solution to the security-auditability dilemma. It does not seem to be properly baselined against simpler, potentially more natural solutions. The main goal of ALCA appears to be to provide a method that shows the user all harmless reasoning while hiding any reasoning that could create potential vulnerabilities or information leakages. A more natural and simpler solution here, for exampe, is to simply have another LLM redact sensitive parts of the reasoning before providing them to the user. A simplification of ALCA would be to simply mask reasoning tokens from the user where the probes fires. These are methods that do not alter the models actual generations and so will not impact the “auditability” axis. The paper uses a relevant masked reasoning method in Section 2, but does not seem to baseline ALCA against masked reasoning in the main results, which seems a problematic omission. \n\nMoreover, none of the existing baselines used in the paper are described, motivated or contextualized. It is unclear if they are meaningful baselines for a security-auditability evaluation (they may only be good baselines for the \"security\" component).\n\n**Lack of empirical focus on auditability**\n\nIn general, the paper seems to heavily focus on the “security” component, and neglects the “auditability” component. Namely, the auditability of ALCA is not compared to any baseline methods, and the metrics used for measuring auditability in Table 3 seem unconvincing (these metrics are proxies for auditability, not direct measures). The paper does not seem to include any model generations - it would be useful to qualitatively compare decoded latents to the ground-truth text.\n\n**Presentation issues**\n\nI have some concerns regarding the care gone into the preparation of the paper. All text in the paper is in bold from page 5 onwards. There are a few other formatting issues throughout (e.g., line 447). The conclusion is very minimal and there is no discussion of limitations. Table 2 seems to, on multiple occasions, highlight the ALCA result as the best performing when it appears a different method was the best performing (e.g., for Llama-3, ALCA GCG is worse than STAIR GCG?). Citations are sometimes missing, e.g., for TAP method in line 132.\n\n**Other**\n\nPrevious papers have proposed methods for latent reasoning. This paper does not ground their latent generation approach in existing literature."}, "questions": {"value": "- Can you confirm you are the first paper to introduce the “security-auditability dilemma” in this context?\n- Is there a reason you did not try the simpler baseline approaches I touched on above? Could you run these baselines?\n- Do you agree that the “auditability” axis is neglected in your experiments? Could you run additional experiments to better validate the auditability of ALCA?\n- In Section 2, how exactly is the 'masking' in the masked reasoning performed?\n- What dataset do you use for training the probe?\n- Why did you choose the decoding method you used? Did you try other approaches? Why did you not just decode the latents directly through lm_head ?\n- Can you include some example generations from the model? In particular, generations decoded from latents would be interesting.\n- For the “downstream %” results in Table 2, was the model generating its reasoning in latent mode?\n- In section 4.3, the plots mention L_decode, but the text mentions L_causal - is this a mistake?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IEK7AIE8Mi", "forum": "BaG67KNavy", "replyto": "BaG67KNavy", "signatures": ["ICLR.cc/2026/Conference/Submission24035/Reviewer_B74G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24035/Reviewer_B74G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941045411, "cdate": 1761941045411, "tmdate": 1762942906250, "mdate": 1762942906250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}