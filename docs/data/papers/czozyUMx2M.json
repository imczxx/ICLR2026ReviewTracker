{"id": "czozyUMx2M", "number": 10633, "cdate": 1758178160528, "mdate": 1759897639132, "content": {"title": "Contrastive Thinking Decoding can Improve Answers for Reasoning Models", "abstract": "Large reasoning models (LRMs) expose an explicit thinking phase prior to the answer. While recent work has focused on the optimization of the thinking phase, answer phase, given a thinking trace, remains under-explored. This paper investigates the behavior of the answer phase. First, the answer can diverge from the thinking trace even when the trace already contains the correct solution, and the converse can also occur. Second, budgeted thinking alters the answer in non obvious ways. Small budgets trigger extra reasoning in the answer, and large budgets move verification into thinking yet drift can remain. Third, complete thinking prompts that live only inside the thinking block steer the answer pattern and provide practical control of answer behavior. Motivated by these observations, we propose Contrastive Thinking Decoding (CTD), a test-time logit correction method that explicitly targets answer phase alignment. Unlike prior contrastive decoding, which contrasts outputs from a strong model against an auxiliary weaker model, CTD operates within a single model by contrasting the primary thinking trace with a deliberately perturbed noisy trace. This contrast steers token-level decoding in the answer phase, requires no additional training, and preserves budget control. Across standard math reasoning (e.g., MATH500, AIME'24/'25) and code benchmarks, CTD achieves higher accuracy at similar or lower token counts and reduces mismatch between the provided thinking and the final answer.", "tldr": "This paper presents contrastive thinking decoding that improves answers and mitigates the disagreement issue between thinking and answer by token level extrapolation between a primary thinking trace and a noisy reference.", "keywords": ["Large reasoning model", "Decoding", "Reasoning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5bb98c674f034288f18edc0d201b11704f5cb74c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper first discusses several phenomena in Large Reasoning Models (LRMs), including thinking-answer disagreement, extra reasoning in the answer triggered by small reasoning budget in thinking phase, the controlling effects of thinking prompts on the answers. Then, the authors propose a test-time logit correction method Contrastive Thinking Decoding (CTD) to effectively align the answer with the thinking outcome. The proposed method achieves good thinking-answer agreement and effective token reduction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The studied problem about the thinking-answer disagreement is novel, and the revealed phenomena may provide some new insights to the community.\n\n2. The ablation experiments are comprehensive, the ablation experiments are thorough. The main claims are well-supported by the experiments.\n\n3. I also appreciate the additional experiments and discussion in the Appendix, which clears some confusions and makes the paper stronger."}, "weaknesses": {"value": "1. The research scope is limited. The output format of the LRMs studied by the author consists of a complex thinking process followed by a concise summary process. Under this condition, the author finds that the answer derived in the summary process can be inconsistent with the answer obtained from the thinking process. However, there is another output format where the model provides the final answer directly after the thinking process, without a summary [1,2]. I expect that in this case, the inconsistency between the thinking and the answer will disappear, as this avoids errors caused by re-reasoning in the summary. I hope the author provides a discussion and analysis of this output format.\n\n2. The paper's writing can be greatly improved. The logic of the paper is not very clear. The motivation for using noisy preference traces for contrastive decoding, from the preliminary observations, is not that clear. I hope the authors provide a more intuitive explanation.\n\n3. The empirical effectiveness is not remarkable. The reasoning token reduction is not significant, and the accuracy improvement is satisfactory when the thinking budget is high. This might be because the longer the reasoning trace, the more likely the model is to lose track of the correct steps in the reasoning process during the summary phase, leading to incorrect answers in the summary. So, would this method still be effective (or even necessary) in a reasoning model without a summary process (refer to W1)?\n\n\n[1] Hu, Jingcheng, et al. \"Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model.\" arxiv 2025\n\n[2] He, Zhiwei, et al. \"Deepmath-103k: A large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning.\" arxiv 2025"}, "questions": {"value": "Refer to the Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bWFiejx7YB", "forum": "czozyUMx2M", "replyto": "czozyUMx2M", "signatures": ["ICLR.cc/2026/Conference/Submission10633/Reviewer_mZRz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10633/Reviewer_mZRz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630587514, "cdate": 1761630587514, "tmdate": 1762921888082, "mdate": 1762921888082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes some patterns in large reasoning models, including disagreement between thinking and answer, changeability of thinking depending on the budget, and steerability of thinking by the prompts. Based on these observations, the authors propose contrastive thinking decoding (CTD), which computes token-level scores for a single model with different prompts. By upweighting tokens supported by the primary prompt and downweighing tokens supported by the noisy reference prompt, the model is able to achieve better performance with same or slightly less thinking budgets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method is simple yet (somewhat) effective. Since CTD is an inference time method, no additional parameters or training is required, making it easy to adopt. The method is based on empirical observations and has good intuitions."}, "weaknesses": {"value": "* The performance gain is marginal, in terms of both answering accuracy and the efficiency. For example, Table 4 presents the mixed results: in most settings the accuracy improvements are rather small, and under some settings the results are even worse than baselines. The number of tokens reduced is also not significant (even if Deepseek reasoning models are internally easy to produce redundant tokens, the reduction is still marginal), and tend to require more tokens for Qwen3 models. \n* Although claimed to be \"noisy reference prompts\", the prompt producing incorrect reasoning chain is actually fixed. Did you try out other forms of prompts, or even random prompts to inject sufficient noise? How to guarantee the noisy prompts would lead to valuable negative directions that could help the positive ones? I am worried about the controllability and universality of this method.\n* The theoretical analysis is somewhat irrelevant and seems like LLM assisted or generated. Only the proof that CTD provably decreases budgets and improves accuracy would be valuable (which is not super clear right now), and other claims seem distracting to me.\n* Despite that the appendix admits the limitations and discuss future directions, the actual implemented contents of the paper is rather simple and limited. It would be better if some of the limitations could be addressed or some future directions are implemented."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qa7vFZ58pW", "forum": "czozyUMx2M", "replyto": "czozyUMx2M", "signatures": ["ICLR.cc/2026/Conference/Submission10633/Reviewer_xPmi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10633/Reviewer_xPmi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030179503, "cdate": 1762030179503, "tmdate": 1762921887568, "mdate": 1762921887568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the answer stage of large reasoning models (LRMs), where outputs can drift from the content of the prior “thinking” trace. It proposes Contrastive Thinking Decoding (CTD), a purely test-time method: generate a main reasoning trajectory and a deliberately low-quality “noise” trajectory (using NULL or OPPOSITE think prompts), then decode the final answer with a contrastive rule that subtracts the noise logits from the main logits. CTD aims to suppress over-answering and align answers with the evidence already present in the thinking segment. Experiments on math and code benchmarks (e.g., AIME’24/’25, MATH500, LiveCodeBench) show consistent Pass@1 gains at similar or fewer tokens on models up to 8B parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a concrete failure mode, thought–answer mismatch and over-answering， and reframes answer calibration as a lightweight decoding-time operation. \n2. The method remains training-free and easy to deploy. It requires no finetuning, verifier, or external classifier; fits naturally into existing decoding pipelines with only minor code changes; and uses a small, stable set of hyperparameters. \n3. The experimental analysis provides interpretable control and diagnostic insight. Variations of think prompts and token budgets illustrate how reasoning effort shifts between thinking and answering, clarifying when over-answering arises."}, "weaknesses": {"value": "1. The experimental scope could be broadened to include larger models. Most reported results are based on ≤8B models, so the conclusions might depend on model capacity. It would strengthen the paper to test CTD on ≥30B models, which could provide a clearer view of its scalability, generalization behavior, and potential interaction with more capable reasoning traces.\n2. The range of comparisons in the main text could be expanded for better context. Tables such as Table 5 and 6 mainly contrast the Base system with CTD, which makes it difficult to assess the relative improvement over existing decoding and control strategies. Bringing Appendix Table 12 (with Contrastive/Instructional Decoding results) into the main text and adding a few representative recent baselines under the same settings would make the empirical section more comprehensive and fair.\n3. The design of the “noise” trajectories could be diversified. Using only NULL and OPPOSITE as strong negative prompts is a reasonable starting point, but it may not capture common error modes such as verbosity, repetition, or minor reasoning drift. Including an ablation study with additional noise types and reporting results for cases where the main reasoning is correct, incorrect, or incomplete would help demonstrate the robustness and generality of CTD’s contrastive mechanism."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0MuFcWuWyL", "forum": "czozyUMx2M", "replyto": "czozyUMx2M", "signatures": ["ICLR.cc/2026/Conference/Submission10633/Reviewer_D3vq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10633/Reviewer_D3vq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762099362789, "cdate": 1762099362789, "tmdate": 1762921887168, "mdate": 1762921887168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}