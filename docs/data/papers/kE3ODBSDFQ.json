{"id": "kE3ODBSDFQ", "number": 6366, "cdate": 1757974264033, "mdate": 1759897919453, "content": {"title": "ReSpace: Text-Driven 3D Indoor Scene Synthesis and Editing with Preference Alignment", "abstract": "Scene synthesis and editing has emerged as a promising direction in computer graphics. Current trained approaches for 3D indoor scenes either oversimplify object semantics through one-hot class encodings (e.g., 'chair' or 'table'), require masked diffusion for editing, ignore room boundaries, or rely on floor plan renderings that fail to capture complex layouts. LLM-based methods enable richer semantics via natural language (e.g., 'modern studio with light wood furniture'), but lack editing functionality, are limited to rectangular layouts, or rely on weak spatial reasoning from implicit world models. We introduce ReSpace, a generative framework for text-driven 3D indoor scene synthesis and editing using autoregressive language models. Our approach features a compact structured scene representation with explicit room boundaries  that enables asset-agnostic deployment and frames scene editing as a next-token prediction task. We leverage a dual-stage training approach combining supervised fine-tuning and preference alignment, enabling a specially trained language model for object addition that accounts for user instructions, spatial geometry, object semantics, and scene-level composition. For scene editing, we employ a zero-shot LLM to handle object removal and prompts for addition. We further introduce a voxelization-based evaluation capturing fine-grained geometry beyond 3D bounding boxes. Experimental results surpass state-of-the-art on addition and achieve superior human-perceived quality on full scene synthesis.", "tldr": "ReSpace introduces a text-driven 3D indoor scene framework using structured representation and preference-aligned language models, achieving state-of-the-art object addition and superior human-perceived quality on full scene synthesis.", "keywords": ["indoor scene synthesis", "3D scene synthesis", "large language models", "structured scene representation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87ce266d4cce645b212db9b7fc6518eb0400cbbd.pdf", "supplementary_material": "/attachment/f3df5a3bf1977d2840cfb7ab4f656e4b7e0a6dd2.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a next-token prediction pipeline for language-driven indoor scene editing.  Specifically, a preference optimization with reinforcement learning with verifiable rewards is used for finetuning. Additionally, a voxel-based loss function metric is used for capturing geometric interactions beyond bounding boxes. Experiment results show better results on object addition in indoor scene synthesis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper has proposed several interesting modules for object addition in given indoor scenes, which includes: \n1. a compact structured scene representation with explicit room boundaries that enables asset-agnostic placement\n2. voxelization-based Loss rather than bbox constraint"}, "weaknesses": {"value": "1. It is not clear how Group Relative Policy Optimization (GRPO) is used for training the AR model (L211-212). There is no motivation of using this and advantage $A_i$ is not explained either, which is claimed to be the contribution of the paper. \n2. Computation cost comparison is missing. \n3. I do not find where the overall loss fuction is designed,  since there is envoved with several loss functions. \n4. How to enable the proposed OOB and MBL loss functions in training/optimization the model? The details are not clearly explained. \n5. Missing discussion and comparison with other related autoregressive based indoor scene synthesis/editing works such as: \n[1] FOREST2SEQ: Revitalizing Order Prior for Sequential Indoor Scene Synthesis, https://arxiv.org/abs/2407.05388\n[2] CASAGPT: Cuboid Arrangement and Scene Assembly for Interior Design, https://arxiv.org/abs/2504.19478"}, "questions": {"value": "Some parts are not explained clearly: \n1. the modeling of rectilinear polygons in L161\n2. What is the differences among A, B and L in L303?\n3. How to calculate PMS when ATISS and Mi-Diff do not support language-driven editing? \n4. Curious about whether the model support scene rearrangement. \n5. Any visual limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2INkfudNKc", "forum": "kE3ODBSDFQ", "replyto": "kE3ODBSDFQ", "signatures": ["ICLR.cc/2026/Conference/Submission6366/Reviewer_EZ1o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6366/Reviewer_EZ1o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761099429461, "cdate": 1761099429461, "tmdate": 1762918657166, "mdate": 1762918657166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ReSpace, a text-driven framework for 3D indoor scene synthesis and editing built around a compact Structured Scene Representation (SSR), a specialized SG‑LLM for object addition, and a zero-shot LLM that decomposes user instructions and performs object removal via direct SSR edits. The system adds objects autoregressively, decouples asset selection from layout via a probabilistic sampler, and evaluates layouts with a new Voxelization‑Based Loss (VBL) that counts out‑of‑bounds voxels and mesh‑mesh overlaps.\nThe SG-LLM is trained in a dual-stage pipeline: first with Supervised Fine-Tuning (SFT) and then with preference alignment (using GRPO), where the VBL metric serves as a verifiable reward signal. Experiments show that ReSpace achieves state-of-the-art results on object addition and, despite mixed results on rendering-based metrics (FID/KID), achieves superior human-perceived quality for full scene synthesis, as validated by a large-scale user study."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Editable Generative Representation: The SSR (a JSON format) and SG-LLM successfully frame 3D scene generation as an editable, next-token prediction task.\n2. Accurate, Rewarded Alignment: The framework leverages GRPO, using the novel VBL metric as a verifiable reward, to fine-tune the model for more geometrically accurate placements.\n3. Superior Performance: ReSpace achieves state-of-the-art results on object addition and, more importantly, is rated by human evaluators as having superior perceived quality for full scene synthesis."}, "weaknesses": {"value": "1. Lacks other baselines: The other LLM-based baselines, such as LayoutVLM, which also fine-tune the LLM for scene configuration generation, are not included in the paper. Besides, for the scene synthesis task, at least one end-to-end method needs to be compared in the paper, like InstructScene.\n2. Modest Impact of GRPO: The paper highlights the SFT+GRPO pipeline as a key contribution. While Table 1 shows GRPO improves quantitative VBL metrics, the appendix (A.6) reveals a crucial finding: a second user study comparing SFT-only vs. SFT+GRPO found no statistically significant human preference (51% vs 49%). This significantly weakens the claim that preference alignment is a key driver of perceived quality. The authors also note \"training fragility\" and \"reward hacking\", suggesting this component is difficult to implement and its benefits may be marginal.\n3. Limited Editing Capability: The framework is marketed for \"synthesis and editing,\" but the editing functionality is split and weak. Object removal is handled by the ZS-LLM with low accuracy (75.2% on the ‘liv’ split). More complex edits like \"move,\" \"rotate,\" or \"resize\" are not supported at all, as the authors admit."}, "questions": {"value": "1. The removal accuracy is a major bottleneck. Is this failure primarily due to semantic ambiguity (e.g., two \"chairs\" in the scene), or does the ZS-LLM also fail at the task of correctly manipulating the JSON string?\n2. For full scene synthesis, why does the model handle the objects one by one? Can we directly generate the SSR for the full scene?\n3. What is the per‑object addition latency (SG‑LLM + retrieval + VBL check) and how does BoN scale in interactive settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F2XP94xhU4", "forum": "kE3ODBSDFQ", "replyto": "kE3ODBSDFQ", "signatures": ["ICLR.cc/2026/Conference/Submission6366/Reviewer_pMta"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6366/Reviewer_pMta"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711890356, "cdate": 1761711890356, "tmdate": 1762918656714, "mdate": 1762918656714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a text-driven scene editing method using LLM. The proposed system employs dual-LLM architecture: a primary LLM acts as a natural language interface to translate user instructions into specific prompts, and a second, specially fine-tuned Scene-Graph LLM (SG-LLM) generates and edits a Structured Scene Representation (SSR) based on those prompts. The SG-LLM is trained using supervised fine-tuning followed by preference alignment. The authors demonstrate state-of-the-art performance on the 3D-FRONT dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors introduces a text-to-indoor scene editing method by leveraing LLMs to autoregressively predict next object with preference alignment.\nThe paper proposes a new evaluation metric Voxelization-Based Loss (VBL) to measure fine-grained geometric interactions among the room boundaries and 3D objects."}, "weaknesses": {"value": "Lack of Representation Justification: The paper claims to use a Structured Scene Representation (SSR), but it does not sufficiently justify its advantages over other contemporary scene representations formulated as language, such as those in LayoutGPT [1], SceneScript [2], or SpatialLM [3]. Furthermore, encoding scene boundaries is not novel, as methods like Ctrl-Room [4] explicitly model complex walls as part of the generative process. In contrast, this work treats boundaries only as a fixed input. A comparative analysis of SSR's expressiveness and efficiency is needed.\n\nUnmotivated Dual-LLM Architecture: The rationale for the two-LLM pipeline is unclear. A critical question is whether the SG-LLM, trained specifically on SSR data, is prone to overfitting and fails to generalize to direct natural language input, thus necessitating the first LLM as an intermediary. This relates to a core challenge in domain-specific LLMs. The authors should analyze this potential issue and strengthen their experimental validation by including more complex and realistic datasets like Structured3D[5] or the one used in SpatialLM [3] to test the model's robustness. \n\nLimited Experimental Scope: the chosen baselines are not the most recent, and newer methods like InstructScene[6] and SceneWeaver[7] also support intuitive, text-driven editing. More importantly, the 3D-FRONT dataset is known for its simplicity in room diversity and layout complexity. To fully establish the method's efficacy, experiments on more challenging datasets with greater object count and layout variety are encouraged.\n\nSome minor weaknesses:\n•Inconsistency in Claims: The authors criticize previous works for being limited by simplified object categories and fixed floor plans. However, the proposed method itself operates under similar constraints, as it starts from a partial scene and retrieves objects from a fixed dataset. The authors should more clearly articulate how their method advances beyond these limitations.\n•Clarification of Formulation: Equation 1 requires clarification.\n1) What does the p_i represent ?\n2) Is \\mathcal U_i equivalent to Tok(S_i) ? The relationship between these terms should be explicitly stated.\n•Clarification of Prompt Bank: The role and composition of the prompt bank \\mathcal P(o) are not well-explained. How does it relate to the sequence modeling process defined in Equation 1?\n\n\n[1] LayoutGPT: Compositional Visual Planning and Generation with Large Language Models \n[2] SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model \n[3] SpatialLM: Training Large Language Models for Structured Indoor Modeling \n[4] Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints \n[5] Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling\n[6] InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior"}, "questions": {"value": "See my discussion in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ksdeltiNg2", "forum": "kE3ODBSDFQ", "replyto": "kE3ODBSDFQ", "signatures": ["ICLR.cc/2026/Conference/Submission6366/Reviewer_xQ5n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6366/Reviewer_xQ5n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811484352, "cdate": 1761811484352, "tmdate": 1762918656330, "mdate": 1762918656330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ReSpace, a framework for autoregressive indoor scene generation and editing from natural language prompts, representing the current scene with a Structured Scene Representation (SSR). The framework is designed to perform three main tasks. For (a) object removal, a zero-shot LLM directly edits the SSR, and for (b) full scene generation, the same zero-shot LLM produces object prompt list, which is fed to SG-LLM. SG-LLM is trained for (c) single object addition with SFT+GRPO, which predicts the next object placement given the SSR and an object prompt. Additionally, a voxelization-based loss is introduced to capture fine-grained geometric details that bounding-box metrics fail to reflect."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) Prior work on scene synthesis predominantly employs global optimization, which is not well-suited to scene editing. Sequential synthesis is a natural way to support editing tasks.\n2) The paper introduces a voxelization-based loss that captures fine-scale details and evaluates spatial arrangements more accurately than bounding-box metrics.\n3) Preference alignment via GRPO is novel, and seems to achieve good results."}, "weaknesses": {"value": "1) Missing comparison with recent baselines: ATISS (NeurIPS 2021), LayoutGPT (NeurIPS 2023), and Mi-Diff (2024) are relatively old for full scene synthesis evaluation, making it difficult to assess the proposed model’s performance in the current landscape. More recent baselines such as LayoutVLM [2] report stronger results than those selected here.\n2) Generation time: Autoregressive generation is likely slower than in-context learning methods like LayoutGPT. However, end-to-end generation time for a full scene is not reported.\n3) Lack of global re-optimization/correction: Although an autoregressive approach is natural for editing, as stated in L460–462, it is not directly suitable without a global correction/re-optimization step (e.g., when there is no space for a large object). A global layout solver or feedback mechanism applied before each insertion could mitigate these limitations and achieve the best of both worlds.\n\n[1] Sun, F. Y., Liu, W., Gu, S., Lim, D., Bhat, G., Tombari, F., ... & Wu, J. (2025). Layoutvlm: Differentiable optimization of 3d layout via vision-language models. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 29469-29478)."}, "questions": {"value": "All of my concerns are listed in the weaknesses section, and I may adjust the rating if they are well addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UBR88Hc8BL", "forum": "kE3ODBSDFQ", "replyto": "kE3ODBSDFQ", "signatures": ["ICLR.cc/2026/Conference/Submission6366/Reviewer_fzd3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6366/Reviewer_fzd3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925165059, "cdate": 1761925165059, "tmdate": 1762918655767, "mdate": 1762918655767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}