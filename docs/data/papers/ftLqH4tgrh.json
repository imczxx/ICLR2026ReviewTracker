{"id": "ftLqH4tgrh", "number": 375, "cdate": 1756736956502, "mdate": 1759898264433, "content": {"title": "Dialogue as Discovery: Navigating Human Intent Through Principled Inquiry", "abstract": "A fundamental bottleneck in human-AI collaboration is the ``intention expression gap,\" the difficulty for humans to effectively convey complex, high-dimensional thoughts to AI.\nThis challenge often traps users in inefficient trial-and-error loops and is exacerbated by the diverse expertise levels of users.\nWe reframe this problem from passive instruction following to a Socratic collaboration paradigm, proposing an agent that actively probes for information to resolve its uncertainty about user intent.\nwe name the proposed agent Nous, trained to acquire proficiency in this inquiry policy.\nThe core mechanism of Nous is a training framework grounded in the first principles of information theory.\nWithin this framework, we define the information gain from dialogue as an intrinsic reward signal, which is fundamentally equivalent to the reduction of Shannon entropy over a structured task space.\nThis reward design enables us to avoid reliance on costly human preference annotations or external reward models.\nTo validate our framework, we develop an automated simulation pipeline to generate a large-scale, preference-based dataset for the challenging task of scientific diagram generation.\nComprehensive experiments, including ablations, subjective and objective evaluations, and tests across user expertise levels, demonstrate the effectiveness of our proposed framework. Nous achieves leading efficiency and output quality, while remaining robust to varying user expertise.\nMoreover, its design is domain-agnostic, and we show evidence of generalization beyond diagram generation.\nExperimental results prove that our work offers a principled, scalable, and adaptive paradigm for resolving uncertainty about user intent in complex human-AI collaboration.", "tldr": "", "keywords": ["Entropy reward，Information gain，Socratic paradigm"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/74890ff2b1db9c8bbcda59de86a2d63d54db0f14.pdf", "supplementary_material": "/attachment/b331d36d65f826c779cfc0b94581ab195aa2362d.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Nous, a Socratic agent trained with information-gain intrinsic rewards using entropy reduction to improve conversation interaction efficiency and output quality"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The use of intrinsic reward is well motivated. The ractable reward shows that the factorized attribute view supports interpretable uncertainty accounting and question selection with intrinsic signal. The final equation is intrinsic, computationally efficient, and suitable for optimizing the agent’s inquiry policy.\n\n- I find it novel for the dataset, where using a diagram dataset as a testbed makes the information-theoretic objective measurable and the training pipeline feasible for dialogue as a discovery problem. The high-dimensional yet logically structured dataset is ideal for inquiry learning while remaining challenging.\n\n- The experiment is well designed with multiple settings and proper metrics to demonstrate the main claim\n\n- The paper is generally well written and presented."}, "weaknesses": {"value": "- The hard-constraint update assumes a perfect parsing and unambiguous answer and according to which the paper attains using templated Oracle responses. This is likely overestimating IG vs. real users and real-world answers.\n\n- The VisPainter metric set is interesting. But for it to construct validity as a proxy for “faithful intent capture” is not fully justified; some  lower results  are showing a trade-off between richer descriptions and errors, where single-score comparison might not be enough."}, "questions": {"value": "- How does performance change with real-world user answers ( where  it may include synonyms, underspecification, contradictions), and with imperfect semantic parsing compared to the current setting?\n\n- The use of word “world model” can be somewhat misleading in my opinion, as in the paper it really means an empirical prior but not a dynamics-transition model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2ltNuVfar6", "forum": "ftLqH4tgrh", "replyto": "ftLqH4tgrh", "signatures": ["ICLR.cc/2026/Conference/Submission375/Reviewer_QBG3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission375/Reviewer_QBG3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966284592, "cdate": 1761966284592, "tmdate": 1762915506970, "mdate": 1762915506970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Nous, an agent trained to elicit user intent through strategic questioning in human-AI collaboration. The core technical contribution is using Shannon entropy reduction over a structured attribute space as an intrinsic reward signal, eliminating the need for human preference annotations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper formulates information gain as a closed-form reward function through mathematical derivation from Shannon entropy, eliminating explicit human annotation during training. The experimental design includes multiple training methods (SFT, DPO, online/offline GRPO), multiple evaluation dimensions (efficiency metrics, subjective quality judgments, objective VisPainter scores), and several ablation studies examining reward function design and user expertise robustness."}, "weaknesses": {"value": "**The relationship to traditional task-oriented dialogue is unclear.** Standard slot-filling systems in TOD also ask questions to complete structured specifications. MultiWOZ and other benchmarks have been doing this for years. The paper does not explain what is fundamentally different here beyond applying it to diagram generation instead of restaurant booking. Is the contribution that attributes are more complex? That there are more attributes? That LLMs are used instead of specialized dialogue managers? Without explicit comparison to TOD methods or clear articulation of what makes this problem distinct, the novelty relative to existing goal-oriented dialogue research remains ambiguous.\n\n**All evaluation occurs in simulation with idealized conditions.** All training and evaluation dialogues are generated through this automated simulation process without any real human involvement. Every experiment uses an Oracle that has perfect memory, gives instant responses, never contradicts itself, never changes its mind, and has unlimited patience. This is not how real users behave. Real collaboration involves users who are unsure, who realize mid-conversation they want something different, who provide vague answers, who get tired of answering questions. Table 4 mentions \"Human Users\" but this refers to three doctoral students whose specific role and contribution are not clearly described. The main experimental results in Tables 1-3 appear to rely entirely on simulated Oracle interactions. Real human dialogue involves uncertainty, preference changes, vague expressions, and varying communication styles. Without evaluation on actual human users, the results demonstrate only that the system functions within its simulation environment, not that it supports real human-AI collaboration.\n\n**The entropy objective lacks justification for this specific task.** Shannon entropy treats all attributes uniformly. Resolving uncertainty about minor details (line thickness) receives equal reward to resolving uncertainty about core concepts (diagram purpose). There is no weighting by semantic importance or user priorities. The ablation only compares against counting resolved attributes. The paper never tests weighted entropy, utility-based objectives considering question complexity, or importance-ranked strategies.\n\n**Generalization evidence is weak.** The paper claims to provide a \"domain-agnostic framework\" but validates on one task type.\n\n**The independence assumption oversimplifies the problem structure.** In diagram design, attributes have extensive constraints and mutual effects. Layout choice constrains connection types (hierarchical implies directed, circular implies undirected). Component count affects spatial arrangement possibilities. Visual style determines appropriate color schemes. Treating these as independent produces incoherent question sequences. The paper acknowledges this as a simplifying assumption but never quantifies the cost."}, "questions": {"value": "**Q1: What is the actual cost of the independence assumption?**  How much error does this introduce? What is the quantitative difference in information gain estimation between the independence assumption and actual attribute dependencies?\n\n**Q2: Why is unweighted Shannon entropy the correct objective for collaborative design?** All attributes receive equal weighting regardless of semantic importance or user priorities. What is the theoretical or empirical justification for this choice? How does information gain correlate with actual output quality across the evaluated models? \n\n**Q3: How does the system perform with real human users?** All evaluation uses simulated Oracles with perfect responses. What happens when real users provide contradictory information, change their preferences mid-dialogue, give vague answers, or experience cognitive fatigue? Does the system's performance degrade gracefully or fail catastrophically? What is the actual user satisfaction, cognitive load, and task completion rate in realistic scenarios?\n\n**Q4: How are attribute spaces constructed for new tasks?** (Please explain with more detail)\n\n**Q5: Why are established clarification dialogue methods not used as baselines?** The paper cites multiple works that use information-gain-based question selection for clarification but includes none of them in experimental comparisons. How can the contribution be evaluated without comparing to these directly relevant prior methods?\n\n**Q6: What defines the boundary of applicability?** The paper claims domain-agnosticism but shows weak results on novel writing. What task characteristics determine whether this method will work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1N7INhf2r9", "forum": "ftLqH4tgrh", "replyto": "ftLqH4tgrh", "signatures": ["ICLR.cc/2026/Conference/Submission375/Reviewer_BJFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission375/Reviewer_BJFA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969500015, "cdate": 1761969500015, "tmdate": 1762915506831, "mdate": 1762915506831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Socratic collaboration paradigm called Nous, which actively asks clarifying questions to address user intent ambiguity in high-dimensional structured tasks. The method formalizes conversational progress as information gain, equivalent to a reduction in Shannon entropy within the task attribute space, and utilizes this intrinsic signal to train the questioning strategy without requiring human preference labels. The approach introduces an automated simulation process that uses an oracle with knowledge of the true specifications to generate conversational data with preference rankings, and applies a group relative policy optimization method in an offline setting. Experiments on a scientific diagram generation task show that Nous outperforms SFT, DPO, and prompt-based GPT/Qwen baselines in both interaction efficiency and output quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel formulation of the collaborative task. This paper proposes a novel framework that uses principle-based uncertainty reduction process and defines the reward signal as information gain (entropy reduction) for intention expression gap problem in human-AI collaboration.\n\nProposes automated simulation pipeline to generate a large-scale, preference-based dataset for the challenging task of scientific diagram generation. considers practicality and scalability."}, "weaknesses": {"value": "1. The information-theoretic framework relies on the simplifying assumption that task attributes are conditionally independent. This might not be true for many complex, real-world tasks where choices are coupled (e.g., a specific graph layout choice might constrain the types of available components). The paper does not sufficiently analyze the impact of violating this assumption. \n\n2. The evaluation is conducted in a simulated environment using a template Oracle agent that provides structured responses. This setup does not capture the complexity of real human interaction, where user inputs may be ambiguous noisy, unstructured. It leaves a significant gap in demonstrating the system's robustness and utility in a real-world collaborative setting."}, "questions": {"value": "Regarding the conditional independence assumption, could you elaborate on its potential impact in more complex domains? What would be the trade-offs of moving beyond the independence assumption?\n\nYour work relies heavily on a simulated Oracle with templated responses. Could you provide more detail on the robustness of the semantic parser to noisy, non-templated, or ambiguous user inputs that are more representative of real human language? Even a small-scale human study involving real users interacting with Nous would strengthen the paper's claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "bwYce1CpbM", "forum": "ftLqH4tgrh", "replyto": "ftLqH4tgrh", "signatures": ["ICLR.cc/2026/Conference/Submission375/Reviewer_afyP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission375/Reviewer_afyP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981573138, "cdate": 1761981573138, "tmdate": 1762915506625, "mdate": 1762915506625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the goal-oriented dialogue generation for scientific diagram generation. The author proposes to leverage the information gain from dialogue as an intrinsic reward signal and leverage the standard post-training methods, i.e., offline GRPO (OfG), to post-train the LLM to efficiently and effectively capture the user intent for scientific diagram generation, and then use the user answers and other text2image (T2I) models to generate the scientific diagram. To enable the training, the author proposes an automatic data generation pipeline to curate the data from the public academic research papers. Experiments are conducted on the generated datasets under different variants of post-training methods and the choice of LLM, where the OfG is better than other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The studied problem lies in the interaction between HCI and LLM, which is quite interesting. \n\n2. The idea of leveraging the classic information theory to measure the uncertainty over the QA rounds to understand the user intent is standard and sound. The usage of offline GRPO shows effectiveness when compared with other baselines."}, "weaknesses": {"value": "1. The practicality of the problem setup. This paper considers the problem setup in which the image, e.g., the scientific diagram, is generated after the LLM has fully resolved the user's intent in text. However, asking too much information may be potentially redundant and unnecessary. In reality, it is more common to see the use cases that the image is generating alongside the QA, and the user can also consider the intermediate generation results to determine whether it is still necessary to proceed with the QA. How will the proposed method work under such a practical setup? \n\n2. The comparison methods are limited. There are no existing methods in scientific diagram generation via LLM that are compared in Section 4, while only a simple baseline, e.g., zero-shot inference on GPT and Qwen, is presented. This may not be sufficient to evaluate the significance of the proposed method comprehensively."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cTSk7AhKcP", "forum": "ftLqH4tgrh", "replyto": "ftLqH4tgrh", "signatures": ["ICLR.cc/2026/Conference/Submission375/Reviewer_WZwJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission375/Reviewer_WZwJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762733506286, "cdate": 1762733506286, "tmdate": 1762915506509, "mdate": 1762915506509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}