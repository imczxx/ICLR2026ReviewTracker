{"id": "rajioNWfRs", "number": 23930, "cdate": 1758350472409, "mdate": 1759896789890, "content": {"title": "TNT: Improving Chunkwise Training for Test-Time Memorization", "abstract": "Recurrent neural networks (RNNs) with deep test-time memorization modules, such as Titans and TTT, represent a promising, linearly-scaling paradigm distinct from Transformers. While these expressive models do not yet match the peak performance of state-of-the-art Transformers, their potential has been largely untapped due to prohibitively slow training and low hardware utilization.\nExisting parallelization methods force a fundamental conflict governed by the chunksize hyperparameter: large chunks boost speed but degrade performance, necessitating a fixed, suboptimal compromise. To solve this challenge, we introduce TNT, a novel training paradigm that decouples training efficiency from inference performance through a two-stage process. Stage one is an efficiency-focused pre-training phase utilizing a hierarchical memory. A global module processes large, hardware-friendly chunks for long-range context, while multiple parallel local modules handle fine-grained details. Crucially, by periodically resetting local memory states, we break sequential dependencies to enable massive context parallelization. Stage two is a brief fine-tuning phase where only the local memory modules are adapted to a smaller, high-resolution chunksize, maximizing accuracy with minimal overhead. Evaluated on Titans and TTT models, TNT achieves a substantial acceleration in training speed—up to 17$\\times$ faster than the most accurate baseline configuration—while simultaneously improving model accuracy. This improvement removes a critical scalability barrier, establishing a practical foundation for developing expressive RNNs and facilitating future work to close the performance gap with Transformers.", "tldr": "", "keywords": ["Recurrent Neural Networks", "Sequence Modeling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d13d5a5f228ce2d80c12f080573f9bd0e60ed51.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents TNT, a novel training paradigm designed to address the efficiency–accuracy trade-off in Recurrent Neural Networks (RNNs) with deep test-time memorization modules such as Titans and TTT. Existing parallelization approaches rely on a fixed chunk size, where larger chunks improve training speed but degrade performance, creating a fundamental bottleneck. TNT overcomes this limitation through a two-stage training process. In the first, efficiency-oriented pretraining stage, a hierarchical memory structure is employed: a global module processes large chunks to capture long-range context efficiently, while multiple parallel local modules handle fine-grained details. By periodically resetting local memory states, TNT removes sequential dependencies and enables large-scale context parallelization. In the second, fine-tuning stage, only the local memory modules are adapted to smaller chunk sizes, restoring high-resolution accuracy with minimal computational overhead. Experiments on Titans and TTT models demonstrate that TNT achieves up to 17× faster training while simultaneously improving accuracy, effectively removing a key scalability barrier and establishing RNNs as a promising alternative to Transformers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The work could bring some advantages: 1) stage 1 in the method can increase training throughput by introducing a novel hierarchical memory architecture that enables unprecedented parallelism. By the clear framework overview figure and TNT Memory Compression Rule, authors provide clear explanation for their method. 2) stage 2 can bridge the gap between the large chunk sizes required for efficient training and the small chunk sizes that yield the best performance at inference. 3) By the experiments given by authors, the results are impressive that can support the main points in the paper."}, "weaknesses": {"value": "1) The presentation of the paper still needs more improvement. For example, in the main experimental results, I can not understand the meaning of column in table 2.\n2) The datasets used by the paper are not clear. I am confused on this.\n3) The four baselines seems to be not enough to better support the efficacy of method.\n4) I think the table1 can show the effectiveness of method. But other results in paper can not show obviously better performance."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OJ8JO5giqd", "forum": "rajioNWfRs", "replyto": "rajioNWfRs", "signatures": ["ICLR.cc/2026/Conference/Submission23930/Reviewer_2mkd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23930/Reviewer_2mkd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761497754438, "cdate": 1761497754438, "tmdate": 1762942862263, "mdate": 1762942862263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a two-stage training method for test-time memorization modules, which typically suffer from low hardware efficiency during training. In the first stage, a global memory module and several local memory modules are employed to model long-sequence dependencies while maintaining efficiency. The second stage further fine-tunes the memory modules to achieve finer context lengths for the local memories, thereby enhancing inference performance. Experimental results demonstrate the effectiveness and efficiency of the proposed TNT method, showing its potential to achieve performance comparable to standard Transformer models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well structured and easy to follow.\n- The proposed two-stage training method effectively balances performance and training efficiency.\n- Extensive experiments across different model architectures demonstrate the method’s effectiveness and robustness."}, "weaknesses": {"value": "- There is no hyperparameter study on $C_G$. How does the global chunk size influence performance?\n- TNT only outperforms the FlashAttention method at a 32K sequence length. Is this due to an under-optimized kernel or the limitation of maintaining additional memory modules?\n- What are the sizes of the global and local memory modules? Since TNT introduces additional parameters for the global memory module, comparable parameter sizes should be used for Titans when comparing performance."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZQszUhksyS", "forum": "rajioNWfRs", "replyto": "rajioNWfRs", "signatures": ["ICLR.cc/2026/Conference/Submission23930/Reviewer_NKDU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23930/Reviewer_NKDU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761596022338, "cdate": 1761596022338, "tmdate": 1762942862058, "mdate": 1762942862058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, proposes TNT, a two-stage training paradigm for deep memory modules in RNNs to address the conflict between training efficiency and inference performance. Stage 1 uses a hierarchical memory (global for long-range context, local with periodic resets for parallelism) and Q-K Projection to boost efficiency. Stage 2 fine-tunes local modules for small chunks. Experiments show up to 17.37× faster training and better accuracy on Titans. Its contributions include identifying three challenges, introducing Q-K Projection, hierarchical memory with resets, efficient fine-tuning, and the TNT paradigm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper shows strong originality by proposing the two-stage TNT paradigm, decoupling deep memory modules’ training efficiency and inference performance to break the balancing bottleneck in existing work. Its hierarchical memory (with locally periodic resets) and Q-K Projection solve non-linear module parallelization and memory compression-retrieval mismatch, as targeted innovations.\n2. It features rigorous technicality and thorough experiments, defining core mechanisms via clear formulas for reproducibility. Covering multiple baselines and scenarios, with ablation studies verifying key components, its 17.37× training speedup and performance gains are supported by detailed tables/figures, ensuring high reliability. \n3. It has research and application value: breaking deep memory modules’ scalability bottleneck to enable RNNs as Transformer alternatives, identifying three core challenges. Its linear runtime and low fine-tuning overhead fit long-sequence tasks, and it applies to various deep memory modules"}, "weaknesses": {"value": "1. The paper only tests up to 4 local memory modules and does not analyze performance saturation points or optimal chunk size selection for multi-local configurations, leaving gaps in guiding practical hierarchical memory setup.\n2. TNT lacks custom kernel optimization, and the speed comparison with optimized Transformer baselines (e.g., Gated Transformer with FlashAttention) is unfair due to hardware optimization mismatch, failing to highlight inherent efficiency advantages.\n3. The paper does not validate TNT on sequences longer than 32K or complex long-sequence tasks (e.g., document summarization), limiting demonstration of its practical utility.\n4. It fixes global memory chunk size ($C_G=2048$) without testing variations or analyzing the impact of ($C_G$)-($C_L$) ratio, leaving gaps in understanding optimal hierarchical memory coordination."}, "questions": {"value": "1. Could you clarify if adding more than 4 local modules brings further performance gains or overhead, and is there a method to select optimal chunk sizes for multi-local setups?\n2. Have you done preliminary tests on custom kernels for TNT, and would TNT’s speed advantage be more pronounced if Transformer baselines use only JAX (no custom kernels)? \n3. Have you validated TNT on sequences longer than 32K (e.g., 64K, 128K) to check linear scaling, and why not test it on complex long-sequence tasks like document summarization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fJ7iHcaBYU", "forum": "rajioNWfRs", "replyto": "rajioNWfRs", "signatures": ["ICLR.cc/2026/Conference/Submission23930/Reviewer_g52o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23930/Reviewer_g52o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883765016, "cdate": 1761883765016, "tmdate": 1762942861791, "mdate": 1762942861791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}