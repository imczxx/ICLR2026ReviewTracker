{"id": "9QYA3DiPl8", "number": 17317, "cdate": 1758274662727, "mdate": 1759897182731, "content": {"title": "EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models", "abstract": "We introduce EgoMem, the first lifelong memory agent tailored for full-duplex models that process real-time omnimodal streams. EgoMem enables real-time models to recognize multiple users directly from raw audiovisual streams, to provide personalized response, and to maintain long-term knowledge of users’ facts, preferences, and social relationships extracted from audiovisual history. EgoMem operates with three asynchronous processes: (i) a retrieval process that dynamically identifies user via face and voice, and gathers relevant context from a long-term memory; (ii) an omnimodal dialog process that generates personalized audio responses based on the retrieved context; and (iii) a memory management process that automatically detects dialog boundaries from omnimodal streams, and extracts necessary information to update the long-term memory. Unlike existing memory agents for LLMs, EgoMem relies entirely on raw audiovisual streams, making it especially suitable for lifelong, real-time, and embodied scenarios. Experimental results demonstrate that EgoMem’s retrieval and memory management modules achieve over 95% accuracy on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot, the system achieves fact-consistency scores above 87% in real-time personalized dialogs, establishing a strong baseline for future research.", "tldr": "The first memory agent for full-duplex omnimodal models that delivers lifelong, personalized, real-time spoken dialog", "keywords": ["Memory Agent", "Multimodal", "Full-duplexity", "LLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8b55b2c51618a379767f2419febb2f39018b67d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents **EgoMem**, a lifelong memory agent for **full-duplex, omnimodal** (audio-visual-text) dialogue.  \nThe system is composed of three processes:  \n(1) a polling-based retrieval module that performs face and voice verification followed by multimodal content retrieval;  \n(2) an omnimodal dialogue backbone that integrates retrieved results via a **MemChunk** conditioning mechanism;  \n(3) a memory management unit that detects conversation boundaries and continuously updates long-term user memories.  \nThe paper further defines two system levels: **Level-1**, which focuses on profile-based personalization, and **Level-2**, which introduces content-driven retrieval over a social-graph structure.  \nExperiments report high accuracy for sub-modules such as recognition, retrieval, and boundary detection, and achieve strong LLM-as-judge ratings for personalized dialogue quality, all within real-time latency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **well designed system decomposition**  \n   The authors clearly separate the three main modules—retrieval, dialogue and memory management. This modularization demonstrates careful engineering and facilitates potential scalability to larger multimodal agents.\n\n2. **practical MemChunk conditioning mechanism**  \n   The authors provide a concrete and lightweight solution for injecting long-term memories into ongoing dialogue without retraining or heavy memory fusion, reflecting a good understanding of real-world system constraints.\n\n3. **meaningful distinction between Level-1 and Level-2 personalization**  \n   Level-1 focuses on static user profiles and personalized attributes, while Level-2 extends this to dynamic retrieval from a user’s social-graph knowledge base.  This hierarchical personalization framework is well motivated and provides a natural trade-off between performance and complexity.\n\n4. **strong sub-module evaluations**  \n   Detailed results are given for the face recognition, speaker verification and retrieval modules, including EER, accuracy, and latency statistics.  Such fine-grained reporting makes the work reproducible from a system benchmarking perspective."}, "weaknesses": {"value": "1. **Overly narrow problem setting**  \n   The paper explicitly limits its experiments to single-speaker full-duplex dialogues and does not handle overlapping or concurrent speech.  \n   This simplification makes the system much less representative of real-world applications such as in-car assistants, conference agents, or smart-home dialogue systems, where multiple speakers often interact simultaneously.  \n\n2. **Heavy reliance on synthetic and TTS-generated data**  \n   The training and evaluation data are largely synthesized using TTS and scripted dialogues, leading to a clear real-world gap,which weakens the realism of both acoustic and conversational characteristics, such as accent diversity, background noise, and spontaneous speech.  \n3. **Circularity and bias in LLM-based judging**  \n   The same DeepSeek-V3 is used for both generating memory content and evaluating dialogue quality, which risks introducing correlated biases.  This design choice can lead to inflated factuality or personalization scores, since the evaluation model shares the same prior with the generator.  Without human assessment or cross-model evaluation, the reported improvements lack objectivity and external credibility.  \n\n4. **Insufficient robustness analysis for the Level-2**  \n   Although the Level-2 model achieves strong retrieval accuracy on synthetic datasets, the paper does not analyze how recognition and retrieval errors propagate through the system.  The lack of an error-cascade analysis makes it unclear how failures in earlier modules (e.g., speaker verification or retrieval mismatch) affect dialogue correctness and user experience.  This omission undermines the claim that the system achieves stable and robust lifelong personalization.  \n\n5. **Missing ablations on critical design parameters**  \n   The paper does not provide ablation experiments for polling frequency, MemChunk allocation, retrieval method variants, or ASR performance under noise.  Without these studies, it remains unclear which components most affect system performance and which are sensitive to hyperparameter changes.  This absence of detailed ablation limits interpretability and reduces the explanatory power of the results."}, "questions": {"value": "1. **Need for double-blind human evaluation**  \n   The paper currently relies entirely on LLM-as-judge metrics to assess dialogue quality and personalization. While automated evaluation is convenient, it often fails to capture subtle aspects such as emotional alignment, context recall, and long-horizon coherence.  Could the authors conduct or plan to conduct **double-blind human evaluations** with multiple annotators to validate the claimed improvements?  \n\n2. **Handling of overlapping and rapidly switching speakers**  \n   The current design appears to assume a single active speaker in each time window, which may not hold in realistic full-duplex environments.  Real-world audio streams often involve overlapping speech, interruptions, and rapid turn-taking between users.  Could the authors elaborate on how the system might be extended to handle **multi-speaker overlap or rapid speaker switching**?  \n\n3. **Ablation studies on critical design parameters**  \n   The paper introduces several hyperparameters and architectural decisions—such as polling interval, MemChunk allocation size, retrieval strategy, and verification thresholds—but does not analyze their influence.  These choices likely affect latency, memory usage, and personalization quality in different ways.  Would the authors provide **systematic ablation experiments** to reveal how each parameter contributes to overall system performance?  \n\n4. **Analysis of Level-2 error cascades and failure modes**  \n   Level-2 integrates multiple sub-modules—recognition, retrieval, and dialogue generation—each of which can introduce errors that propagate through the pipeline.  Understanding how upstream recognition or retrieval failures influence downstream dialogue responses is essential for diagnosing model stability.  Could the authors provide **quantitative error-cascade analyses or failure case studies** to clarify how the system behaves under noisy or incorrect retrieval conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fp4MpSN0L7", "forum": "9QYA3DiPl8", "replyto": "9QYA3DiPl8", "signatures": ["ICLR.cc/2026/Conference/Submission17317/Reviewer_XmQx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17317/Reviewer_XmQx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814398791, "cdate": 1761814398791, "tmdate": 1762927248003, "mdate": 1762927248003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed EgoMem, the first lifelong memory agent tailored for real-time omnimodal full-duplex models. The proposed model tried to solve the humanoid cognition problem of robots deployed for a user group by exploring lifelong memory capability. EgoMem consists of three processes: the retrieval process to identify the user and gather related documents, the dialog process to realize real-time response, and the memory management process to detect dialog boundaries, extract information, and update memories. The paper follows a two-level development of chatbots to solve the user-specific service and cross-user references step-by-step. Deployed on RoboEgo chatbot, the system could handle the multi-user conversation problem with only raw audiovisual inputs, and realized high fact-consistency and answering quality scores under LLM scoring evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem is well-motivated and significant. This paper tackles the messy reality of continuous, multimodal, multi-user interaction head-on, which is a crucial and challenging direction for the field.\n2. The modeling of multi-user interaction is reasonable and sound, with some special cases such as interruption considered.\n3. The paper provides a detailed analysis of the data organization in training, which is well aligned with the proposed multi-user interacting model and real-world scenarios."}, "weaknesses": {"value": "1. The proposed method is an assembly of several existing models, and the paper did not discuss the challenges this work addresses. The technical innovation of this paper appears limited, and further clarification of the novel aspects would strengthen the work.\n2. As a study containing interaction with user groups in reality, this paper lacks a real user study. The experiments are entirely based on synthetic data generated by LLMs, causing concerns about the model's real-world performance.\n3. The evaluation of the experiments is entirely based on LLM scoring. The LLM-based evaluation result alone is not sufficiently trustworthy. A crowdsourced study or case studies could be introduced to enhance the credibility.\n4. The scores in Table 3 lack comparison to other models or human behaviors. The authors claim this is the \"first try\" in this field, which is used to justify the lack of comparative baselines. However, this is not entirely satisfying. Could a simpler (but still strong) baseline have been implemented? For example, a system that simply runs ASR on the stream, chunks it by speaker, and feeds it all into a large-context RAG system.\n5. While the proposed EgoMeM uses the DeepSeek API extensively, it does not describe the prompts used. Including this information would improve reproducibility and clarity."}, "questions": {"value": "1. How is this framework's compatibility with other progresses in long-term memory construction and retrieval to enhance performance?\n2. Could the model update the relevant information to other users according to the conversation with the current user? For example, if user A mentions a schedule with user B during a conversation with the model, could the model update user B's profile too after this conversation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "elVTbvwWwP", "forum": "9QYA3DiPl8", "replyto": "9QYA3DiPl8", "signatures": ["ICLR.cc/2026/Conference/Submission17317/Reviewer_X4Wc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17317/Reviewer_X4Wc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834026851, "cdate": 1761834026851, "tmdate": 1762927247705, "mdate": 1762927247705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EgoMem, the first lifelong memory agent designed for full-duplex omnimodal models that process real-time audiovisual streams. The agent enables real-time personalized dialogue and interaction with users by maintaining long-term memory of users' preferences, social relationships, and facts derived from audiovisual history. The EgoMem system operates asynchronously through three processes: retrieval (for recognizing users and gathering context), real-time, full-duplex, omnimodal dialog generation, and memory management. It is evaluated in real-time personalized dialogue scenarios using RoboEgo, an omnimodal chatbot, and shows promising results with high accuracy and fact-consistency scores."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This study addresses an important problem, introducing a comprehensive and useful pipeline, which integrates lifelong memory into real-time full-duplex omnimodal systems.\n2. The paper presents a well-organized and structured approach to EgoMem's operation, including its division into Level-1 (profile-only) and Level-2 (content-driven) memory. The description of each module (retrieval, dialog, and memory management) is clear, and the use of both face and voice recognition, along with text retrieval, is well-motivated and effectively integrated.\n3. I appreciate that the authors have done a substantial amount of engineering and evaluation. They evaluated each component of their complex system individually with strong quantitative results.\n4. The paper demonstrates strong experimental results, especially with the retrieval and memory management modules.\n5. The use of Memory in the omni-modal, real-time interaction is well-motivated and clearly demonstrated."}, "weaknesses": {"value": "1. While I acknowledge the significant engineering effort behind the system; however, its architecture appears to rely heavily on existing off-the-shelf components (face verification, speaker verification and external LLMs for memory extraction and updating). I think I am not in the best position to assess the technical novelty of the work, and I defer that judgment to AC. \n\n2. The entire system is trained and evaluated on synthetic data only. User profiles and connections are synthesized, dialogs are generated by LLMs, and audio is generated via TTS—without any real-world user interactions or human-subject studies, raising concerns about its generalization and effectiveness in real-world scenarios.\n\n3. Also, the primary evaluation is performed by LLM only, there is no human evaluation to validate these results. I am skeptical that an AI judge can reliably score nuanced aspects like \"helpfulness\" in a personalized, long-term context.\n\n4. There is no detailed error analysis regarding the system's failure modes or edge cases. Understanding these failure modes would offer deeper insights into its real-world applicability."}, "questions": {"value": "1. Is user privacy taken into consideration in this model? When a query from user A contains some private information of user B, how could this model respond?\n\n2. The results show that the Level-2 system (with the social graph) has a lower Fact Score (87.6% on noised) than the Level-1 system (93.1%). The paper attributes this to \"more frequent MemChunk updates and error cascading\". Could you elaborate on this? Is the query-generation model itself prone to error, or is the text retrieval  the main source of failure? Does this drop in performance suggests the L2 system is less reliable than L1 under noise?\n\n3. As there have been continuing advances in long-term memory, how compatible is the proposed EgoMem framework with these developments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ej5EgNVNG0", "forum": "9QYA3DiPl8", "replyto": "9QYA3DiPl8", "signatures": ["ICLR.cc/2026/Conference/Submission17317/Reviewer_rerw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17317/Reviewer_rerw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835173784, "cdate": 1761835173784, "tmdate": 1762927247430, "mdate": 1762927247430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EgoMem, a lifelong memory agent designed for full-duplex omnimodal models that process real-time audiovisual inputs. Unlike existing textual memory agents for large language models (LLMs), EgoMem directly operates on raw audiovisual streams, supporting user identification, memory retrieval, and long-term knowledge updating. The framework comprises three asynchronous processes: (1) Retrieval, identifying users and fetching related content; (2) Omnimodal Dialog, generating personalized responses in real time; and (3) Memory Management, detecting dialog boundaries and updating user-specific memory. EgoMem is integrated into the RoboEgo model and evaluated on multimodal benchmarks, achieving high accuracy for retrieval and memory tasks, and strong personalization performance in real-time dialogs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Attempt to design a lifelong memory system for full-duplex omnimodal models, addressing an emerging research frontier.\n2. Well-motivated framework with clear modular design (retrieval, dialog, memory management).\n3. Strong experimental performance and latency benchmarks on a large-scale synthetic multimodal dataset.\n4. The paper defines a new benchmark and data pipeline for personalized omnimodal dialogs, potentially impactful for future research."}, "weaknesses": {"value": "1. Experiments rely entirely on synthetic data, limiting confidence in real-world generalization.\n2. Lack of comparative baselines against other multimodal or memory-augmented agents, only presenting the results of the proposed method is not convincing.\n3. Lack of results from ablation experiments on sub-modules, such as the impact of the accuracy of speech recognition (ASR) results on system final performance."}, "questions": {"value": "1. How well does EgoMem perform with real user data, given all training is synthetic?\n2. How does EgoMem compare to recent LLM memory frameworks in textual or multimodal contexts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MUUAo3W7GH", "forum": "9QYA3DiPl8", "replyto": "9QYA3DiPl8", "signatures": ["ICLR.cc/2026/Conference/Submission17317/Reviewer_5v1E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17317/Reviewer_5v1E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162576627, "cdate": 1762162576627, "tmdate": 1762927247029, "mdate": 1762927247029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}