{"id": "WHrmlyQEM7", "number": 12748, "cdate": 1758210025457, "mdate": 1759897489604, "content": {"title": "Evaluating Text Creativity across Diverse Domains: a Dataset and Large Language Model Evaluator", "abstract": "Creativity evaluation remains a challenging frontier for large language models (LLMs). Current evaluations heavily rely on inefficient and costly human judgments, hindering progress in enhancing machine creativity. While automated methods exist, ranging from psychological testing to heuristic- or prompting-based approaches, they often lack generalizability or alignment with human judgment. To address these issues, in this paper, we propose a novel pairwise-comparison framework for assessing textual creativity, leveraging shared contextual instructions to improve evaluation consistency. We introduce CreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic creative instruction-response pairs spanning diverse open-domain tasks. Through training on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval demonstrates remarkable superiority over existing methods in alignment with human judgments. Experimental results underscore the indispensable significance of integrating both human-generated and synthetic data in training highly robust evaluators, and showcase the practical utility of CrEval in boosting the creativity of LLMs. We will release all data, code, and models publicly to support further research.", "tldr": "We propose a dataset for text creativity evaluation and an LLM-based evaluator that can assess text creativity in a pairwise comparison manner, which outperforms existing evaluation methods in alignment with human judges.", "keywords": ["creativity evaluation", "text evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a16651879722c65caf1d8c819756e961a8769545.pdf", "supplementary_material": "/attachment/8070744be7b938c61eb0b639c631b26bcc8c1cef.zip"}, "replies": [{"content": {"summary": {"value": "The paper tackles the challenge of automatically evaluating textual creativity in large language models. Current creativity assessment rely heavily on human judgement, which is subjective, inconsistent, and expensive.\nThe author propose a new evaluation framework which is a context-aware, pairwise comparison method that judges creativity between two responses to the same prompt. The author propose a new dataset containing over 1 million instruction-response pairs across 87 domains, mixing human and synthetic data. Over this, the author train a new LLM-based creativity evaluation on the dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall idea is clear and well-motivated. Focusing on fine-grained text-level creativity rather than broad model-level creativity makes the work much more practically valuable.\n2. The authors explicitly explore the challenge of human annotation inconsistency through empirical analysis, which shows solid motivation and thoughtful experimental design.\n3. The two key assumptions used for weak supervision are both empirically validated, and the paper carefully considers position bias in pairwise evaluation that is easily to be ignored.\n4. The experimental setup is comprehensive and convincing. The baselines cover a wide range of categories—traditional metrics, general LLMs, and fine-tuned evaluators. The study includes analyses of consistency, ablation on data composition, OOD generalization, and even the ability to enhance creativity through DPO training. Altogether, this gives the results strong credibility."}, "weaknesses": {"value": "1. There’s no discussion of whether the model maintains interpretative correctness—in other words, does it still “understand” why a response is creative under its defined criteria?\n2. The OOD evaluation is relatively small in scale, which limits how strongly the generalization claim can be made.\n3. Some references, such as Zhao (2024), have already been officially published, but the citations still point to arXiv versions. The bibliography needs to be updated."}, "questions": {"value": "1. The CreataSet-Base domain distribution seems quite unbalanced. Is creativity really isotropic across categories? Some subdomains (like business writing vs. poetry) might not be equally suited for creativity evaluation. A more fine-grained per-domain analysis would help clarify this.\n2. Were the choices of Qwen2.5-14B-Instruct and MiniCPM-2B-SFT arbitrary? Why were these two particular models selected for response generation and augmentation?\n3. The pairwise comparison framework works well for controlled benchmarking, but its practical usability might be limited. Do you think it could be extended to continuous or scale-based scoring while preserving consistency?\n4. It would be interesting to test CrEval on more classical creativity benchmarks—like Alternative Uses or other divergent-thinking tasks—where the “good” and “bad” responses are clearer.\n5. In real-world creative contexts, can CrEval replace or complement the Consensual Assessment Technique (CAT)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ea0rRsMwYW", "forum": "WHrmlyQEM7", "replyto": "WHrmlyQEM7", "signatures": ["ICLR.cc/2026/Conference/Submission12748/Reviewer_RwCK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12748/Reviewer_RwCK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839014509, "cdate": 1761839014509, "tmdate": 1762923567655, "mdate": 1762923567655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on evaluating the creativity of LLM-generated text. Specifically, it identifies two key challenges in evaluating creativity: 1) ensuring consistency in creative annotation for human experts; and 2) training a large-scale model for evaluating creativity given the scarcity of evaluation corpora. To address these challenges, this paper proposes CreataSet and CrEval, a large-scale pairwise comparison framework for cross-domain text creativity evaluation and a logic learning model (LLM)-based evaluator, respectively. Testing on various baseline models validates the effectiveness of CreataSet in evaluating creativity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) Large-scale and multi-domain dataset. CreataSet includes 100K+ human-level and 1M+ synthetic creative instruction-response pairs across 87 domains, which is promising in providing a scalable fundation for studying creative generation and evaluation.\n2) Improved human label protocol. The proposed context-aware pairwise comparison protocol improves inter-annotator consistency (evaluated by ICC).\n3) Comprehensive experiments. Multiple metics are applied for providing an through evaluation, such as F1score, Kappa score and Agreement rate."}, "weaknesses": {"value": "1. The rules for quantifying creativity are not differentiated across different domains.For example, creativity is manifested differently in poetry and scientific writing. Future work should further differentiate the measurement of creativity for each domain.\n2. Insufficient example/failure case analysis. The paper presents overall statistics but does not systematically list typical examples of discrepancies between CrEval and human behavior.\n3. Insufficient generalization analysis. The paper lacks an assessment of the enhancement effect of CreataSet on diverse baseline models."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cXaHooiGIC", "forum": "WHrmlyQEM7", "replyto": "WHrmlyQEM7", "signatures": ["ICLR.cc/2026/Conference/Submission12748/Reviewer_s6ik"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12748/Reviewer_s6ik"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905613789, "cdate": 1761905613789, "tmdate": 1762923567370, "mdate": 1762923567370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CreateSet, a dataset with evaluation pipeline including a performant evaluator—CrEval. The author enriched the responses to existing prompts by generating more responses (and formed CreateaSet-Ext). The work annotated over 3,000 samples, with more using weak labels. The author extensively studied if the dataset helped create a more power evaluator."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- A reasonably well-curated dataset with good mix of topics for coverage.\n- Good amount of human labels, and put into good use in training evaluator models with good comparison with other metrics and very extensive model lineup. I appreciate the authors showing many proprietary results. \n- Evaluation of the trained evaluator is complete and convincing."}, "weaknesses": {"value": "- The definition of creativity, which is subjective, should be detailed better in this work. This is a key bottleneck of this work's quality and rigor.\n- CrEval are comparison pairs of creativity. However, there might be some value to an absolute scale of creativity, especially if we want to rank model responses quickly. Motivation here is less clear.\n- This work suffers from a few overstatements:\n    - The paper prides itself over context awareness (i.e., showing a prompt when evaluating responses for creativity), including using an entire Fig 1 to emphasize. But the authors fail to explicitly demonstrate if this is common.\n    - \"87 domains\" is a stretch. \"Domain\" is defined too loosely across the paper.\n- While I believe performance parity with o3 is not so important given the model scale difference, the authors could use more motivation on why a smaller model as an evaluator is important when a large model can do better. \n\nI found these weaknesses not fundamental and am happy to update my views during the coming discussion period."}, "questions": {"value": "- Line 43, How is problem-solving a single domain?\n- It is important to break down the language used in the dataset. Is this dataset 100% (Simplified) Chinese?\n- Line 47 \"most methods evaluate creativity at the model or subject level rather than at the level of individual responses\" Please better support this claim.\n- How are deepseek models \"Proprietary LLMs\"?\n- How are annotators compensated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gtO3p4Vcxm", "forum": "WHrmlyQEM7", "replyto": "WHrmlyQEM7", "signatures": ["ICLR.cc/2026/Conference/Submission12748/Reviewer_AWwk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12748/Reviewer_AWwk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964289392, "cdate": 1761964289392, "tmdate": 1762923567034, "mdate": 1762923567034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of evaluating text creativity by building a large-scale dataset (CreataSet and its extended version, CreataSet-Ext) and training a pairwise creativity evaluator CrEval. Each instruction is paired with multiple responses generated by different models under both ordinary and creativity-oriented prompts, with human-annotated labels serving as ground truth. The paper finds that CrEval aligns well with human judgments, generalizes to unseen domains, and can further help enhance model creativity. The results show that creativity-oriented prompts and stronger models tend to produce more creative responses, and data composition meaningfully affects CrEval’s performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The experiments are fine. CrEval consistently outperforms strong baselines including large proprietary models across proposed metrics. The paper includes appropriate ablations, along with OOD tests on external datasets. The authors further show CrEval can be used to improve model creativity"}, "weaknesses": {"value": "-  The paper mentions constructing tuples (I, R1, ..., Rk) but does not specify the exact value of k used in experiments. How many responses are generated and used per instruction? Does this vary across data sources?\n- In constructing CreataSet-Ext, they prompt two models to generate more responses for augmenting each instruction. But there is no testing of whether these k responses actually exhibit meaningful diversity. If these responses are similar to each other, it could limit the model’s ability to learn fine-grained distinctions.\n- At the beginning of the paper, they define creativity as “ideas or artifacts that are new, surprising and valuable”, which I personally appreciate. While novelty and surprise are well-measured, the “valuable” aspect is not explicitly evaluated. Aside from gpt-4o-mini filtering, there is no systematic check for helpfulness. A response can be novel but incoherent or unhelpful, which weakens the notion of genuine creativity.\n-  The two core assumptions are validated with only 50 samples each with three annotators. This seems insufficient given the scale of the dataset (1M+ samples).\n- The paper treats creativity evaluation as a pairwise comparison task without deeply analyzing what constitutes creativity or providing any meaningful understanding. What specific features, semantic patterns, or structural elements does CrEval learn to recognize as creative?\n- Personally the task are a bit strange and adhoc. I would not treat them as something that requires creativity \n-Data contamination could inflate the performance of models being evaluated, especially for proprietary models whose training data composition is not fully disclosed.\n- I am not convinced LLM eval is a surrogate for human eval. Are your 18 humans experts ?? Without knowing much we cant conclude anything"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tE7i5yxurx", "forum": "WHrmlyQEM7", "replyto": "WHrmlyQEM7", "signatures": ["ICLR.cc/2026/Conference/Submission12748/Reviewer_Re9M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12748/Reviewer_Re9M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978001618, "cdate": 1761978001618, "tmdate": 1762923566655, "mdate": 1762923566655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}