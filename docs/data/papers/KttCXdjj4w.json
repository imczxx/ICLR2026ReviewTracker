{"id": "KttCXdjj4w", "number": 2898, "cdate": 1757299020077, "mdate": 1759898120398, "content": {"title": "Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward", "abstract": "Enhancing the multimodal reasoning capabilities of Multimodal Large Language Models (MLLMs) is a challenging task that has attracted increasing attention in the community. Recently, several studies have applied Reinforcement Learning with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the reasoning abilities of MLLMs. However, these works largely overlook the enhancement of multimodal perception capabilities in MLLMs, which serve as a core prerequisite and foundational component of complex multimodal reasoning. Through McNemar's test, we find that existing RLVR method fails to effectively enhance the multimodal perception capabilities of MLLMs, thereby limiting their further improvement in multimodal reasoning. To address this limitation, we propose Perception-R1, which introduces a novel visual perception reward that explicitly encourages MLLMs to perceive the visual content accurately, thereby can effectively incentivizing both their multimodal perception and reasoning capabilities. Specifically, we first collect textual visual annotations from the CoT trajectories of multimodal problems, which will serve as visual references for reward assignment. During RLVR training, we employ a judging LLM to assess the consistency between the visual annotations and the responses generated by MLLM, and assign the visual perception reward based on these consistency judgments. Extensive experiments on several multimodal math and general benchmarks demonstrate the effectiveness and robustness of our Perception-R1, which achieves superior performance on all benchmarks using only 1,442 training data.", "tldr": "We observe that standard RLVR fails to enhance the MLLMs perception. Therefore, we propose a novel visual perception reward to improve the MLLMs perception in RLVR, effectively boosting performance on several multimodal benchmarks with limited data.", "keywords": ["Multimodal Large Language Models", "Multimodal Reasoning", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce79e7f9b7021d13f0a85a4cb2212729485028fb.pdf", "supplementary_material": "/attachment/bcc6cca405ab7fffec2a2b3de06b24eef58e01d6.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Perception-R1, a method to enhance multimodal reasoning in Multimodal Large Language Models (MLLMs) by introducing a visual perception reward alongside the standard accuracy reward in Reinforcement Learning with Verifiable Rewards (RLVR).\n\nKey Contributions:\n\n1. Problem Identification: Through McNemar's test, the authors demonstrate that existing accuracy-only RLVR fails to improve MLLMs' multimodal perception capabilities, which they identify as a major bottleneck.\n2. Method: They introduce a visual perception reward that extracts textual visual annotations from CoT trajectories and uses a judging LLM to assess consistency between these annotations and model responses, which provides additional training signal beyond answer correctness.\n3. Results: Using only 1,442 training samples, Perception-R1 achieves SOTA performance across multiple benchmarks, outperforming Vision-R1 (which uses 200K samples) and other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The methods introduced in the paper provide denser reward for the reinforcement learning process, which accurately solve the problem identified by the authors.\n\n2. The experiments are extensive and thorough.\n\n3. Using only 1,442 training samples, Perception-R1 achieves SOTA performance across multiple benchmarks, outperforming Vision-R1 (which uses 200K samples) and other baselines."}, "weaknesses": {"value": "Concern 1: Marginal Improvement Beyond GRPO Baseline. According to Table 2, the performance improvements appear to be primarily driven by GRPO rather than the proposed visual perception reward. The reviewer notes that GRPO is also used in Vision-R1, making it unclear how much of the improvement is attributable to the novel contribution versus the baseline RL algorithm.\n\nConcern 2: Judging LLM Quality Dependency. Figure 3b shows that when using smaller judging LLMs (e.g., Qwen2.5-7B or 14B), the performance sometimes drops below even the base model performance (e.g., MathVerse: 46.1% vs 47.4% baseline; MathVision: 24.2% vs 25.1% baseline). This raises questions about the robustness and practical applicability of the method when high-quality judging models are unavailable."}, "questions": {"value": "The reviewer suggests that the author could conduct additional experiments where: \n\n(1) Vision-R1 trained on the same 1,442 samples used in this paper.\n\n(2) Using the same model to generate CoT trajectories for fair comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "ToNPpTeCjw", "forum": "KttCXdjj4w", "replyto": "KttCXdjj4w", "signatures": ["ICLR.cc/2026/Conference/Submission2898/Reviewer_keG9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2898/Reviewer_keG9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537574941, "cdate": 1761537574941, "tmdate": 1762916434421, "mdate": 1762916434421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Perception-R1, a method to improve multimodal reasoning by fixing the key bottleneck of poor visual perception. The authors find that standard accuracy-only reinforcement learning (RLVR) fails to correct these perception errors, as models can guess the right answer despite flawed visual understanding. Perception-R1 addresses this by adding a visual perception reward. This reward is calculated by a judging LLM that compares the model's response to pre-extracted \"visual annotations\" (key visual facts) from correct solutions. Experiments show this method achieves superior performance on multiple benchmarks with high data efficiency, using only 1,442 training samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a clear and compelling statistical analysis (using McNemar's test) of accuracy-only RLVR-trained MLLMs. This builds a strong case that a significant bottleneck for current models is indeed multimodal perception, not just high-level reasoning.\n\n2. The proposed visual perception reward is intuitive and cleverly designed. By having an LLM judge responses against verifiable, extracted annotations rather than training a holistic reward model, the method directly targets the identified bottleneck. This approach appears significantly more robust to the reward hacking that can harm end-to-end MLLM-as-reward-model RLVR.\n\n3. The performance gains achieved using only 1,442 training samples are impressive. This strongly suggests that a higher-quality, more targeted reward signal (i.e., combining perception and accuracy) can be far more sample-efficient than simply scaling up data for a sparser, accuracy-only reward.\n\n4. The method delivers substantial performance improvements not only on its training domain (math/geometry) but also, surprisingly, across several general-domain benchmarks, outperforming baselines that used orders of magnitude more data."}, "weaknesses": {"value": "1. Limited analysis of generalization: The model's strong generalization from geometry-only training (Geometry3K) to general-domain benchmarks (like MMMU and MMStar) is a key result, but it is not fully explained. The authors hypothesize that they are improving a foundational perception capability, but the link between 'perceiving geometry diagrams' and 'perceiving real-world images' could be strengthened. To make this claim more concrete, the authors could include:\n\n- Qualitative analysis on general benchmarks: Provide qualitative examples from MMMU or MMStar. Does the Perception-R1 model now exhibit the same \"describe-then-solve\" behavior on these general-domain images? Where do the baseline models fail on perception in these tasks? Is Perception-R1 delivering more accurate visual perception in these examples?\n\n- Error breakdown on general benchmarks: Conduct a small-scale error analysis on a subset of a general benchmark (like MMMU-Pro, where they show strong results). What percentage of the baseline's failures on these tasks are due to perception errors, and what percentage of those specific errors does Perception-R1 fix? This would directly support the claim of foundational perception improvement.\n\n2. Dependence on a single training data domain: The reliance on Geometry3K, while clearly effective, is a potential limitation. The data curation pipeline itself seems general, but its effectiveness has only been demonstrated on this one domain. An ablation study training on a different domain (e.g., general textbook diagrams, or even a VQA dataset) using the same pipeline would be highly valuable. This would help demonstrate the general applicability of the Perception-R1 framework, distinguishing its contribution from the (clearly very effective) choice of geometry data as a training source."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "teA3V3XA8R", "forum": "KttCXdjj4w", "replyto": "KttCXdjj4w", "signatures": ["ICLR.cc/2026/Conference/Submission2898/Reviewer_5AFL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2898/Reviewer_5AFL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889177449, "cdate": 1761889177449, "tmdate": 1762916434257, "mdate": 1762916434257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Perception-R1, a reinforcement learning framework that enhances multimodal reasoning in MLLMs by explicitly improving their visual perception. Specifically, the authors (1) extract visual annotations from correct chain-of-thought trajectories as ground-truth perceptual references, (2) employ a judging LLM to evaluate the consistency between these annotations and the model’s generated reasoning, and (3) aggregate this feedback with accuracy and format rewards under the GRPO optimization scheme."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of augmenting RL with a verifiable visual perception signal represents a clear conceptual advance over prior RLVR frameworks (e.g., Vision-R1, MM-Eureka) that focus solely on final answer correctness.\n- The authors conduct extensive evaluations on multiple multimodal benchmarks, demonstrating the method's effectiveness and robustness.\n- The paper is well-structured and clearly written."}, "weaknesses": {"value": "- The paper lacks systematic exploration of critical parameters such as the perception reward weight (γ) and judgment thresholds, leaving robustness questions unanswered.\n- Although data-efficient, the additional judging and reward assignment stages may increase computational overhead, which is not quantitatively discussed.\n- The paper would benefit from more qualitative evidence demonstrating how the model’s perception improves—e.g., visual attention maps, step-by-step perception-reasoning examples, or case studies showing corrected misperceptions. Such analyses would strengthen interpretability and directly connect the proposed reward to perceptual behavior."}, "questions": {"value": "- The method’s success relies heavily on the quality and alignment of the judging LLM used to evaluate perceptual consistency. As shown in Figure 3(b–c), weaker judges introduce reward hacking and degrade performance, but the paper stops short of analyzing why this happens or proposing safeguards (e.g., calibration, ensemble judgment, or confidence filtering). Further discussion or mitigation strategies would make the approach more robust and reproducible across settings.\n- The perception reward weight (γ) and the number/quality of visual annotations are central to the method, yet their interactions are not fully studied. Figure 3(a) provides only coarse exploration. More systematic experiments varying γ and annotation noise would clarify stability and guide practitioners in tuning the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cC0eid4GTQ", "forum": "KttCXdjj4w", "replyto": "KttCXdjj4w", "signatures": ["ICLR.cc/2026/Conference/Submission2898/Reviewer_2S2F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2898/Reviewer_2S2F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927226134, "cdate": 1761927226134, "tmdate": 1762916434036, "mdate": 1762916434036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a key but often neglected limitation in reinforcement learning for Multimodal Large Language Models (MLLMs): existing Reinforcement Learning with Verifiable Rewards (RLVR) methods focus solely on final answer correctness, overlooking the accuracy of visual perception during reasoning. The authors show that such outcome-only rewards allow models to guess correct answers despite severe perception errors. To address this, they propose Perception-R1, which introduces a verifiable visual perception reward into RLVR. This reward is derived from textual visual annotations extracted from high-quality CoT trajectories and evaluated by a judging LLM that measures consistency between model outputs and these annotations.\nContributions:\n1. Empirically and statistically demonstrate that accuracy-only RLVR fails to enhance multimodal perception.\n2. Introduce a novel, verifiable visual perception reward that alleviates reward sparsity and improves perceptual grounding.\n3. Achieve state-of-the-art performance on multiple multimodal reasoning benchmarks using only 1,442 training samples, showing exceptional data efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper reveals the impact of poor perception on reasoning performance. Current RLVR methods fail to enhance multimodal perception, which fundamentally limits the reasoning performance of MLLMs.\n2. The introduced Perception-R1 framework incorporates a novel visual perception reward that significantly strengthens the visual understanding and reasoning capabilities of MLLMs, particularly in mathematical reasoning tasks.\n3. Extensive experiments across multiple benchmarks verify that Perception-R1 substantially improves both perception and reasoning performance, achieving superior results even with highly limited training data."}, "weaknesses": {"value": "1. The paper claims that it enhances the multimodal reasoning capabilities of MLLMs through improved perception. However, the presented results do not provide direct evidence that the observed performance gains stem specifically from enhanced perception. I suggest including an analysis or ablation that directly links perception improvement to the reasoning gains.\n2. While the paper reports significant improvements on multimodal math benchmarks, these results primarily reflect reasoning performance rather than perception itself. To convincingly demonstrate perception enhancement, it would be helpful to include evaluations on dedicated perception-level benchmarks (e.g., BLINK, MMBench, MME, or similar datasets).\n3. The method employs Gemini-2.5 Pro to generate CoT trajectories and uses an LLM to extract visual annotations, followed by GRPO training on these annotations. This pipeline closely resembles a distillation process from Gemini-2.5 Pro, which may primarily transfer reasoning knowledge rather than genuinely improving perception. It would strengthen the paper to disentangle and clarify whether the observed gains truly originate from improved perception rather than implicit reasoning distillation."}, "questions": {"value": "1.After distilling the CoT trajectories from Gemini 2.5 Pro, could you clarify why an LLM is used to transform these trajectories into atomic statements? In particular, how does this approach differ from directly inputting the trajectory data into the LLM to evaluate the atomic statements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sss6HZewDF", "forum": "KttCXdjj4w", "replyto": "KttCXdjj4w", "signatures": ["ICLR.cc/2026/Conference/Submission2898/Reviewer_cpnq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2898/Reviewer_cpnq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970313294, "cdate": 1761970313294, "tmdate": 1762916433838, "mdate": 1762916433838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}