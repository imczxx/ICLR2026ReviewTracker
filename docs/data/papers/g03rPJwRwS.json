{"id": "g03rPJwRwS", "number": 6980, "cdate": 1758004087784, "mdate": 1763403785453, "content": {"title": "Delta-MIA: Measuring Membership Inference Attacks in Large Language Models via self-Contrast Framework", "abstract": "Membership inference attack (MIA) underpins privacy risk assessment, provenance, and compliance for large language models (LLMs). \nObservational evaluations confound membership with distribution shift, hide sample-level behavior, and assume access to proprietary corpora. \nWe present Delta-MIA, an interventional self contrast framework that isolates genuine membership signals by comparing a model before and after controlled exposure to the same dataset. \nThe pipeline records pre exposure responses on verifiably unseen data, performs full-parameter fine tuning on that data followed by stabilization, and computes sample level deltas. \nWe introduce three diagnostics: explained variance ratio (EVR), mean vertical distance (MVD), and above diagonal ratio (ADR), which quantify noise, separation, and baseline detectability. \nRe-evaluating $9$ MIA methods, several remain robust once shift is removed, while others such as DC-PDD and Con-ReCaLL decline markedly; \nMin K\\%++ shows strong separation with high MVD. \nDelta-MIA enables bias-free, interpretable, and transferable evaluation for MIA in LLMs.", "tldr": "Delta-MIA is an interventional self-contrast framework comparing pre- and post-exposure model states, isolates genuine membership signals, enabling unbiased, fine-grained and transferable evaluation.", "keywords": ["Large Language Models", "Membership Inference Attacks", "Self Contrast"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b82c4ae6c23b3fcd17dca020c63cacaa0869c4e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Delta-MIA (Δ-MIA), an interventional self-contrast framework for evaluating membership inference attacks in large language models. The work argues that existing MIA benchmarks are fundamentally limited by data distribution shift, coarse-grained analysis, and dependence on proprietary corpora, which confound evaluation and inflate performance metrics.\n\nThe key contribution is the interventional paradigm: instead of constructing separate member/non-member datasets, Δ-MIA compares a model’s responses before and after exposure to the same dataset. This self-contrast design eliminates distributional confounders and enables sample-level analysis through newly proposed diagnostics—ADR, MVD, EVR, and n-MVD—which quantify detection accuracy, discriminative strength, and noise sensitivity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The research problem is clearly defined, and the paper's writing is good, making it easy for the reader to follow. \nThe paper introduces four well-motivated diagnostic metrics (ADR, MVD, EVR, n-MVD) that provide fine-grained insights into model behavior. \nAlso, the paper presents a solid empirical result from an experiment on their proposed evaluation method.\nThe authors commit to open-sourcing all code and evaluation data, and clearly articulate ethical safeguards to prevent misuse."}, "weaknesses": {"value": "While the paper raises an important issue—the risk that current MIA benchmarks overestimate attack effectiveness due to distributional shortcuts—the motivation and the validation of Δ-MIA are not fully aligned.\nThe central claim is that Δ-MIA provides a faithful, bias-free framework for assessing membership inference in LLMs. However, the experiments mainly show that some existing MIA methods still yield non-trivial results under the Δ-MIA setting. This empirical observation alone does not demonstrate that Δ-MIA truly measures the same construct as “real-world” MIA evaluation.\n\nSpecifically, the paper does not theoretically or empirically establish equivalence between the Δ-MIA setting (before/after fine-tuning on a held-out dataset) and the conventional MIA problem (evaluating a pretrained model on candidate data with no distribution shift). Without such validation, it remains unclear whether the deltas measured by Δ-MIA genuinely reflect membership signals rather than overfitting signals of fine-tuning or optimization.\n\nA stronger demonstration—either a theoretical argument showing that Δ-MIA preserves the same membership decision boundary as the standard setting, or an empirical study comparing Δ-MIA scores with ground-truth membership probabilities in a controlled environment—would significantly strengthen the paper’s central claim."}, "questions": {"value": "1. What if the MIA method relies on the fine-tuning process? Can the D-MIA still be a good evaluation framework for MIA?  For example, the technique proposed in the paper \"FINE-TUNING CAN HELP DETECT PRETRAINING DATA FROM LARGE LANGUAGE MODELS\"\n\n2. As I mentioned in the weakness part, can you show some evidence that if the MIA works under the Delta-MIA, which means it can evaluate a pretrained model on candidate data with no distribution shift. Because I still think fine-tuning cannot approximate pretraining, even if you add extra samples during fine-tuning to avoid overfitting. If you can persuade me that the MIAs working under Delta-MIA indicate a clear membership signal, I will raise the score.\n\n3. Which subset did you use in MIMIR?  7-gram? 13-gram? It is essential because the MIA methods you showed in the paper all fail (close to random guessing) on 13-gram subsets of MIMIR (Wiki, arXiv, PubMed)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qpRty4ngon", "forum": "g03rPJwRwS", "replyto": "g03rPJwRwS", "signatures": ["ICLR.cc/2026/Conference/Submission6980/Reviewer_nYoz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6980/Reviewer_nYoz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629374207, "cdate": 1761629374207, "tmdate": 1762919199059, "mdate": 1762919199059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Delta-MIA, a comprehensive framework for measuring membership inference attacks (MIAs) on fine-tuned large language models (LLMs). The authors propose a set of novel metrics (Above-Diagonal Ratio, Mean Vertical Distance, Explained Variance Ratio, and Noise-Normalized MVD) that provide a more nuanced evaluation of MIA effectiveness beyond traditional dataset-level metrics like AUC. The framework validates the core assumption that fine-tuning doesn't significantly change model behavior by comparing pre- and post-fine-tuning model outputs. The paper evaluates nine representative MIA methods across three domains (Pile-CC, PubMed Abstracts, Wikipedia) using the proposed metrics, providing insights into the relative effectiveness of different approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: The paper introduces a comprehensive evaluation framework that goes beyond standard metrics (AUC, TPR@FPR) to provide deeper insights into MIA effectiveness through multiple complementary metrics.\n\nS2: The proposed metrics (ADR, MVD, EVR, n-MVD) are theoretically well-motivated and address key limitations of existing evaluation approaches, particularly the lack of nuance in current evaluation practices.\n\nS3: The framework successfully validates the core assumption that fine-tuning doesn't significantly alter model behavior, which is crucial for the validity of the Delta-MIA evaluation framework.\n\nS4: The paper provides a clear benchmark for evaluating MIA methods across different domains, which will be valuable for future research in this area.\n\nS5: The empirical results (Table 4) clearly demonstrate the utility of the proposed metrics, showing that methods like ReCall and Min-K%++ consistently perform well across different domains."}, "weaknesses": {"value": "W1: The paper lacks sufficient comparison with existing MIA evaluation frameworks, making it difficult to fully appreciate the novelty of the proposed metrics.\n\nW2: The evaluation is limited to Pythia models across only three domains, which limits the generalizability of the findings to other LLM architectures and datasets.\n\nW3: The paper doesn't adequately address the practical implications of the proposed metrics for real-world privacy risk assessment and defense mechanisms.\n\nW4: The theoretical justification for the proposed metrics could be strengthened with more detailed mathematical analysis and comparison to related work.\n\nW5: The paper doesn't explore the relationship between the proposed metrics and the actual privacy risk in fine-tuned LLMs, which is the ultimate concern for the field."}, "questions": {"value": "Q1: Could you provide a more detailed comparison between your proposed metrics and existing evaluation metrics (AUC, TPR@FPR) to better demonstrate the added value of your framework?\n\nQ2: How would the proposed metrics perform when evaluated against different LLM architectures (e.g., GPT, LLaMA, Qwen) beyond the Pythia models used in your experiments?\n\nQ3: Could you explore the relationship between your proposed metrics (especially n-MVD) and actual privacy risk, potentially by comparing with real-world privacy leakage measurements?\n\nQ4: How would the Delta-MIA framework be adapted to evaluate MIAs on different types of fine-tuned models (e.g., instruction-tuned, domain-specific, or reinforcement learning fine-tuned models)?\n\nQ5: Could you investigate the potential for using your metrics to guide the development of better privacy defenses, rather than just evaluating existing attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O46KsPcif9", "forum": "g03rPJwRwS", "replyto": "g03rPJwRwS", "signatures": ["ICLR.cc/2026/Conference/Submission6980/Reviewer_1iHS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6980/Reviewer_1iHS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814630661, "cdate": 1761814630661, "tmdate": 1762919198503, "mdate": 1762919198503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes $\\delta$-MIA, an interventional, self-contrast framework that evaluates membership inference by comparing a model’s behavior on the same data before and after controlled exposure, thereby removing cross-dataset distribution shift and enabling sample-level diagnostics (ADR, MVD, EVR, n-MVD). The pipeline logs pre-exposure responses, fine-tunes then stabilizes the model, and computes per-sample deltas; it is instantiated on the Pythia family with The Pile and used to benchmark nine representative MIA methods. Empirically, methods like DC-PDD and Con-ReCaLL decline markedly, while Min-K%++ (and Ref when available) remain strong, with trends strengthening for larger models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper defines a clear evaluation framework that isolates membership effects by comparing pre- and post-fine-tuning behavior on the same data.\n\n\n2. The experiments are comprehensive including various MIA probing methods.\n\n\n3. The introduction of new sample-level metrics (ADR, MVD, EVR, n-MVD) provides additional diagnostic views of membership signals, even if the conceptual novelty is moderate.\n\n\n4. The paper is generally well organized and readable, with clear visualizations that support its main claims."}, "weaknesses": {"value": "1. The approach requires access to both pre- and post-fine-tuning models (and control the training data to get rid of cross-dataset distribution shift), which may limit practical applicability in real-world privacy audits.\n2. The methodological novelty appears limited. The four metrics (ADR, MVD, EVR, n-MVD) appear to be designed as heuristic diagnostics to visualize per-sample score changes before and after exposure.\n3. The fine-tuning process isolates the 1.5k target instances, rather than mixing them with non-target samples. Evaluating under more realistic mixed-batch settings would strengthen the validity of the conclusions."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3X5ETsGG5A", "forum": "g03rPJwRwS", "replyto": "g03rPJwRwS", "signatures": ["ICLR.cc/2026/Conference/Submission6980/Reviewer_nrkd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6980/Reviewer_nrkd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969464105, "cdate": 1761969464105, "tmdate": 1762919197877, "mdate": 1762919197877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new evaluation framework, Delta-MIA, for evaluating MIA performance on LLMs to tackle three major limitations of typical observational evaluation framework: 1) unintended distribution shift due to challenges with partitioning member/non-member data (e.g., temporal shifts) resulting in inflated performance, 2) lack of coarse-grained analysis, obscuring sample-level behavior, and 3) poor transferability due to reliance on training corpora access. The authors instead propose an alternative perspective using an interventional paradigm. The main idea is to measure pure membership signals by performing a self-comparison between a model before and after exposure to some sample data using scores from a candidate MIA. The authors also introduce new sample-level metrics (e.g., ADR, MVD, EVR, and n-MVD) that allow for more granular analysis over captured membership signals. They then conduct a broad evaluation over nine modern MIAs on subsets of Pile data over the Pythia model family. Most MIA methods remain robust, but certain methods such as DC-PDD and Con-ReCaLL are shown to not capture membership signals as well as previously claimed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Delta-MIA tackles several important challenges in MI evaluation that are crucial for validating the effectiveness of current and future MIAs. The paper is well-written, easy to follow, and the problem is clearly motivated.\n- The proposed framework is conceptually-straightforward and the introduced metrics are also intuitive. The authors also take care to clearly interpret visualizations to help readers understand nuanced performance differences between the different MIAs. \n- The evaluation is conducted over a broad range of models, MIAs, and data domains."}, "weaknesses": {"value": "- This framework is a useful, necessary check to ensure that a candidate MIA is not capturing spurious signals (e.g. from unintended distribution shifts between benchmark members/non-members) and can actually detect true membership signals. However, I’m unconvinced this framework is sufficient to determine the effectiveness (i.e., vulnerability risk) of an MIA. Unless I have a misunderstanding, without comparing MIA signals between members and non-members under the same model, the discriminatory power of the MIA isn’t interpretable. For example, what if the MVD (or n-MVD) is also high for a set of non-members (to both the pre-/post-exposure models) from the same distribution as the target data? Then the model may not truly be that vulnerable to the MIA and the signal being captured may be something more than just membership. Perhaps it would be stronger to more clearly frame it as a complementary method (e.g., sanity check) to standard observational evaluation? If this is not the case, then it is still a little unclear to me how delta-MIA would be sufficient on its own.\n- This framework also seems to be heavily dependent on the tuning process. I feel that there could be more discussion about the impact of the tuning phase, such as the choice of target tuning data (e.g., what domains), how much target data is used, and other design choices. Ablations in these and similar directions would be appreciated.\n- Closely related to the above comment, it’s not clear to me how this framework bypasses the issue of choosing non-member samples. For example, target tuning data still needs to be verified as non-members to the pre-exposure model, which remains difficult for modern frontier models. \n\nI include more specific questions related to these points in the question section."}, "questions": {"value": "- Could the scores presented still be inflated due to the recency of the target data relative to the entire training lifecycle? What if the injected data was instead inserted, for example, halfway through training. Currently, the scope seems more like “evaluating MIAs on finetuned LLMs”.\n- Similar to the subexperiment in Appendix A, could the authors show how the tuning impacts performance on data from roughly the same distribution (e.g., another sub-sample from Pile test for Pile-CC, Wikipedia, etc.)? \n- Using the post-exposure model as the target model, could the authors also conduct a standard observational evaluation (using the target tuning data as members and another random sample of Pile test data selected the same way as non-members). It’d be interesting to concretely see if performance trends in this observational setting align with those under delta-MIA (e.g., maybe some attacks still seem performant in this observational setting, but under delta-MIA aren’t).\n- What is the reason for having two-stage finetuning? For example, why not batch/randomize the selected 1500 samples with the 100000 samples, training in one stage?\n- Do the authors have any results on different model families (e.g., Llama) to demonstrate the transferability of their approach?\n\nMinor comments:\nIn Figure 1, target is misspelled as “traget”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gLhaYKr9ha", "forum": "g03rPJwRwS", "replyto": "g03rPJwRwS", "signatures": ["ICLR.cc/2026/Conference/Submission6980/Reviewer_wA5G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6980/Reviewer_wA5G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163329273, "cdate": 1762163329273, "tmdate": 1762919197368, "mdate": 1762919197368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}