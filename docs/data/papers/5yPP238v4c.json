{"id": "5yPP238v4c", "number": 8988, "cdate": 1758106099211, "mdate": 1759897749742, "content": {"title": "MT-DAO: Multi-Timescale Distributed Adaptive Optimizers with Local Updates", "abstract": "Training large models with distributed data parallelism (DDP) requires frequent communication of gradients across workers, which can saturate bandwidth. Infrequent communication strategies (e.g., Local SGD) reduce this overhead but, when applied to adaptive optimizers, often suffer a performance gap relative to fully synchronous DDP.\nWe trace this gap to a time-scale mismatch: the optimizer's fast-moving momentum, tuned for frequent updates, decays too quickly to smooth gradients over long intervals, leading to noise-dominated optimization. To address this, we propose MT-DAO, a family of optimizers that employs multiple slow- and fast-moving first momenta or the gradient to track update dynamics across different time scales, for which we provide the first convergence guarantees. \nEmpirically, for language-model pre-training, this eliminates the performance gap with DDP, outperforming infrequent-communication baselines in perplexity and reducing iso-token wall-clock time by 6-27% on Ethernet interconnects. At the 720M scale, MT-DAO reaches a target perplexity in 24% fewer steps and 35% less time than the single-momentum DDP baseline.  MT-DAO enables effective cross-datacenter training and training over wide geographic areas.", "tldr": "MT-DAO, a multi-timescale optimizer, closes the performance gap from infrequent communication in distributed training. It cuts wall-clock time by 6-27% and allows a 720M model to reach its target 35% faster with 5-24% fewer steps than standard DDP.", "keywords": ["Distributed Training", "Foundation Models", "Large Language Models", "Optimizers", "Communication Efficiency", "Federated Learning", "Distributed Systems", "Optimization Theory", "Scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9a9df3df83c51b3051e18c075d2823b707f3d8c7.pdf", "supplementary_material": "/attachment/36616ad4c84406df936af35b4376c20310334047.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MT-DAO, a novel family of optimizers designed to bridge the performance gap between DDP and adaptive optimizers using low-frequency communication strategies (e.g., Local SGD). The authors identify \"timescale mismatch\" as the root cause of this gap and propose employing multiple slow and fast-moving first momenta to capture update dynamics across different time scales. In large-scale language model pretraining experiments, MT-DAO successfully eliminates the performance disparity with synchronous DDP and outperforms existing low-communication baselines in perplexity. Combining rigorous theoretical analysis, solid empirical validation, and practical significance, this work constitutes a major contribution to distributed deep learning optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novelty: Identifies the performance gap between low-frequency communication strategies and DDP as stemming from temporal scale mismatch, and innovatively adapts multi-momentum strategies to the distributed setting.  \n- Empirical Rigor: Comprehensive experiments demonstrate MT-DAOâ€™s reliability.\n- Clarity and Readability: The paper is well-structured, clearly articulating motivation, methodology, and results."}, "weaknesses": {"value": "Please refer to Questions."}, "questions": {"value": "- The novelty of this paper lies in the introduction of a multi-momentum strategy. However, all experimental results were obtained under MT-DAO with N=1, meaning only a single momentum was used. I wonder whether increasing the value of N would lead to further improvements in model performance.\n- If we set K=1, effectively turning MT-DAO into a DDP optimizer, will its performance surpass the DDP baseline? And does it hold any advantages over other DDP optimizers that adopt multi-momentum strategies, e.g. adEMAMix?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Oo0u7bpmz6", "forum": "5yPP238v4c", "replyto": "5yPP238v4c", "signatures": ["ICLR.cc/2026/Conference/Submission8988/Reviewer_KKBR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8988/Reviewer_KKBR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654066428, "cdate": 1761654066428, "tmdate": 1762920718491, "mdate": 1762920718491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MT-DAO, a family of optimizers to stabilize and accelerate training in a communication-limited environment. The authors observed that when the synchronization interval K is large, a relatively large $\\beta$ leads to training instability. They introduced the Quasi-hyperbolic method into training, provided convergence proofs, and experimentally validated its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper has a clear structure with well-defined motivation\n- Convergence proofs are provided, and experiments address the questions of interest"}, "weaknesses": {"value": "Most of my concerns are already mentioned in the Limitations of the paper itself.\n\n- The scalability of MT-DAO. Other than the largest model is at the size of 720M, the experiments are conducted under four workers with 1 GPU per each.\n- A possibly weak baseline with ADOPT instead of Adam. Similarly seen in Figure 2, Local SGD converges faster than MT-DAO-SGDM in this toy example.\n- There is a suspicion of overclaiming because the paper only conducted experiments with N=1. When N=1, it degenerates into each worker storing momentum similar to AdEMAMix and combining updates like QHM."}, "questions": {"value": "1. Could the choice of batch size impact the results? The paper chooses a fixed global batch size settings from [1], which is not in a Local SGD setting. According to [2], DiLoCo seems to tolerate a larger batch size than DDP for a better wall-clock time performance, which I think is pretty reasonable because it does local updates.\n2. The paper seems not to discuss the choice of the weight parameter w, does it have a clear pattern of range or recommended value?\n\n[1] Benchmarking optimizers for large language model pretraining  \n[2] Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JURFpvK7vo", "forum": "5yPP238v4c", "replyto": "5yPP238v4c", "signatures": ["ICLR.cc/2026/Conference/Submission8988/Reviewer_CDpB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8988/Reviewer_CDpB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742570723, "cdate": 1761742570723, "tmdate": 1762920718090, "mdate": 1762920718090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors hypothesize that the gap between the fully synchronous DDP and sparse comm strategies is due to a time scale mismatch; that is, the optimizer's fast moving momentum decays too quickly between synchronization steps, causing replicas to diverge. To remedy this, they propose a bandwidth-efficient distributed data parallel training method for LLMs, which incorporates multiple fast and slow moving momentums. They show that using this approach they can close the gap between DDP and sparse comm methods, and can preserve mutual information better between model replicas."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important question. The convergence gap between infrequent communication strategies and synchronous DDP settings is a glaring problem. I like the authors approach to the problem which is formulated around the fact that this is due to a time scale mismatch between synchronization intervals."}, "weaknesses": {"value": "The narrative of the paper is a bit unclear to me. The method is proposed as a generic plugin across many optimizers and the authors even provide algorithms for SGDM and Adam. However, the experiments are only demonstrated on ADOPT. This also harms the claim that this method is useful for optimizers that use multiple first order momentums, however, as far as I am aware, ADOPT only use a single first order moment? I invite the authors to correct me if I am missing something."}, "questions": {"value": "1. Why are the experiments only conducted on Adopt? In my opinion, this significantly harms your claims.\n2. Infrequent communications mean that each replica is effectively seeing a smaller batch size (compared to the global batch size across replicas). In such cases, it has been previously shown that increasing the betas work robustly [1]. How does your work differ from the insights given in such works.\n3. Can you compare your method with more recent methods such as streaming DiLoCo? I think such comparisons are crucial to validate the efficacy of the proposed work.\n4. When tuning the (beta, w), did you conduct a grid search? How well do these parameters scale across model size? \n5. What are the hyper parameters used in Fig. 5?\n\n[1] - Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZYqybZRhwc", "forum": "5yPP238v4c", "replyto": "5yPP238v4c", "signatures": ["ICLR.cc/2026/Conference/Submission8988/Reviewer_BGje"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8988/Reviewer_BGje"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813327568, "cdate": 1761813327568, "tmdate": 1762920717523, "mdate": 1762920717523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unified perspective on momentum-based distributed optimizers. A novel multiple time-scale distributed optimizer (MT-DAO) with convergence guarantees is proposed that maintains momentum at different time scales. Results on language modelling task with 3 different sizes (16M, 125M and 720M) demonstrate that MT-DAO outperforms prior approaches and also match DDP."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clean framework to develop momentum-based distributed optimizers, nice analytical studies.\n- Strong set of language modelling experiments with 3 sizes upto 720M using modern optimization tools including completeP for LR transfer, WSD scheduler, standard data mixtures, etc - great job!"}, "weaknesses": {"value": "See questions below."}, "questions": {"value": "- The MT-DAO approach is proposed and motivated with multiple fast/slow-moving momenta but experiments show results with N=1, why? \n- Following up on above, the proposed framework looks like a generalization of AdEMAMix optimizer -- how does it directly compare for LLM training in DDP? Is it feasible to match the performance of AdEMAMix optimizer (and other recent matrix-based optimizers such as Muon) with much less communication steps?\n- How does the proposed approach compare with DiLoCo in terms of performance and memory requirement? Moreover, DiLoCo supports complex adaptive rules in the inner optimization and not just limited to momentum, so was wondering if MT-DAO supports that and how it affects theoretical results.\n- How critical is gradient clipping for this approach to work well or can it work without clipped gradients as well? Does DDP baseline also use the same clipping values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kRCqDNo0yw", "forum": "5yPP238v4c", "replyto": "5yPP238v4c", "signatures": ["ICLR.cc/2026/Conference/Submission8988/Reviewer_uewt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8988/Reviewer_uewt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762218387987, "cdate": 1762218387987, "tmdate": 1762920717215, "mdate": 1762920717215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}