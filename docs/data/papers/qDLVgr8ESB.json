{"id": "qDLVgr8ESB", "number": 1500, "cdate": 1756887789373, "mdate": 1759898205746, "content": {"title": "Cannistraci-Hebb Training on Ultra-Sparse Spiking Neural Networks", "abstract": "Inspired by the brain's spike-based computation, spiking neural networks (SNNs) inherently possess temporal activation sparsity. However, when it comes to the sparse training of SNNs in the structural connection domain, existing methods fail to achieve ultra-sparse network structures without significant performance loss, thereby hindering progress in energy-efficient neuromorphic computing. This limitation presents a critical challenge: how to achieve high levels of structural connection sparsity while maintaining performance comparable to fully connected networks. To address this challenge, we propose the Cannistraci-Hebb Spiking Neural Network (CH-SNN), a novel and generalizable dynamic sparse training framework for SNNs consisting of four stages. First, we propose a sparse spike correlated topological initialization (SSCTI) method to initialize a sparse network based on node correlations. Second, temporal activation sparsity and structural connection sparsity are integrated via a proposed sparse spike weight initialization (SSWI) method. Third, a hybrid link removal score (LRS) is applied to prune redundant weights and inactive neurons, improving information flow. Finally, the CH3-L3 network automaton framework inspired by Cannistraci-Hebb learning theory is incorporated to perform link prediction for potential synaptic regrowth. These mechanisms enable CH-SNN to achieve sparsification across all linear layers. We have conducted extensive experiments on six datasets including CIFAR-10 and CIFAR-100, evaluating various network architectures such as spiking convolutional neural networks and Spikformer. The proposed method achieves a maximum sparsity of 97.75% and outperforms the fully connected (FC) network by 0.16% in accuracy. Furthermore, we apply CH-SNN within an SNN training algorithm deployed on an edge neuromorphic processor. The experimental results demonstrate that, compared to the FC baseline without CH-SNN, the sparse CH-SNN architecture achieves up to 98.84% sparsity, an accuracy improvement of 2.27%, and a 97.5$\\times$ reduction in synaptic operations, and the energy consumption is reduced by an average of 55$\\times$ across four datasets. To comply with double-blind review requirements, our code will be made publicly available upon acceptance.", "tldr": "", "keywords": ["Sparse Spiking Neural Network", "Dynamic Sparse Training", "Pruning and Regrowth"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a686f42d11bac4296f459a9231dd83e5bee316e5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CH-SNN (Cannistraci-Hebb Spiking Neural Network), which is a four-stage dynamic sparse training framework for ultra-sparse spiking neural networks (SNNs). Extensive experiments on six datasets show CH-SNN achieves performance comparable to FC networks even at ultra-high levels of sparsity"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality\n(1) Integrates Cannistraci-Hebb theory, originally from complex network science, into SNN sparse training.\n(2) Introduces two novel initialization schemes (SSCTI, SSWI) specifically designed for spike-based learning.\n\nQuality\n(1) Extensive experiments across six datasets show robustness and generalizability.\n(2) Includes ablation, sensitivity, and hardware efficiency analyses, showing methodological thoroughness.\n\nClarity\nThe paper is clearly structured, with each stage of CH-SNN well explained. The biological and theoretical motivations are well linked to the computational framework."}, "weaknesses": {"value": "(1) Insufficient analysis of temporal dynamics: \nThe paper emphasizes structural sparsity but offers limited insight into the temporal spike dynamics.\n(2) Clarity of comparison fairness:\nIt is not entirely clear whether all baseline methods were reimplemented under identical experimental conditions. The paper does not specify whether these results were reproduced using a unified experimental setup or directly taken from prior publications, which affects the transparency and comparability of the reported performance gains."}, "questions": {"value": "(1) Temporal sparsity–accuracy trade-off: Have the authors analyzed the effect of increasing temporal sparsity on latency or robustness?\n(2) Reproducibility: Please clarify whether all baseline methods were reimplemented under identical training conditions or if their results were directly adopted from previous studies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h2FnwTKMZ9", "forum": "qDLVgr8ESB", "replyto": "qDLVgr8ESB", "signatures": ["ICLR.cc/2026/Conference/Submission1500/Reviewer_nSp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1500/Reviewer_nSp9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568276078, "cdate": 1761568276078, "tmdate": 1762915786286, "mdate": 1762915786286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CH-SNN, a novel four-stage dynamic sparse training framework for spiking neural networks (SNNs) that achieves ultra-high structural sparsity while maintaining/improving accuracy compared to baselines. Extensive experiments on six datasets and three architectures demonstrate consistent performance and energy advantages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong novelty and cross-disciplinary contribution: bridges network science (Cannistraci-Hebb theory) with neuromorphic learning, introducing a biologically and topologically inspired sparse training approach.\n2. Comprehensive experimental validation: covers multiple datasets, architectures.\n3. Clear modular structure: the four-stage design (SSCTI, SSWI, LRS, CH3-L3) is intuitive and extensible to other SNNs, which provides a reasonable baseline for SNN training."}, "weaknesses": {"value": "1. Theoretical insufficiency: The paper lacks formal analysis of the convergence and stability of the CH3-L3 regrowth dynamics.\n2. Scalability questions: Experiments are limited to medium-scale datasets. The framework’s behaviour on larger datasets (e.g., ImageNet or DVS-CIFAR100) remains untested.\n3. Biological claim ambiguity: The connection to Hebbian principles is mostly conceptual; empirical neuroscientific grounding is minimal."}, "questions": {"value": "1. Could the authors provide a theoretical argument or empirical evidence that the CH3-L3 regrowth mechanism guarantees stability or avoids redundant regrowth loops?\n2. How sensitive is CH-SNN to the hyperparameters controlling sparsity ratio, pruning frequency, and regrowth sampling distribution?\n3. The timestep of SNN is 8 according to this paper, could the authors explained the performance on different timesteps?\n4. Is the CH3-L3 topological regrowth biologically interpretable in terms of synaptic rewiring or STDP-like plasticity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QZnQOQ0GU4", "forum": "qDLVgr8ESB", "replyto": "qDLVgr8ESB", "signatures": ["ICLR.cc/2026/Conference/Submission1500/Reviewer_9DG8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1500/Reviewer_9DG8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761596668659, "cdate": 1761596668659, "tmdate": 1762915786090, "mdate": 1762915786090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Cannistraci-Hebb Spiking Neural Network (CH-SNN), a dynamic sparse training framework for ultra-sparse SNNs. Extensive experiments are conducted on six datasets, demonstrating that CH-SNN achieves high sparsity while retaining or improving accuracy over fully connected baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents an approach to ultra-sparse SNN training, integrating initialization, pruning methods, and Cannistraci-Hebb-inspired topological regrowth. \nExperiments were conducted on multiple datasets, and thorough ablation experiments and sensitivity analyses were carried out. \nThe framework is implemented on a hardware-friendly algorithm S-TP, achieving significant gains in energy efficiency."}, "weaknesses": {"value": "Some key areas of the mathematical description, particularly around pruning and regrowth, lack sufficient clarity."}, "questions": {"value": "Can the authors clarify the multinomial sampling procedure in the pruning step, how exactly the link removal score (LRS) is converted into actual pruning decisions?\nHow accurate is the SSWI initialization method under varying degrees of input temporal sparsity, structural connection sparsity,and spike threshod？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "f5BE2kuWXw", "forum": "qDLVgr8ESB", "replyto": "qDLVgr8ESB", "signatures": ["ICLR.cc/2026/Conference/Submission1500/Reviewer_zWas"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1500/Reviewer_zWas"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814666523, "cdate": 1761814666523, "tmdate": 1762915785839, "mdate": 1762915785839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CH-SNN, a dynamic sparse training framework for SNNs. The method sparsifies all linear layers in SNNs through four stages: (1) sparse topology initialization (SSCTI), (2) sparse weight initialization (SSWI), (3) hybrid pruning based on link removal scores (LRS), and (4) link regrowth using CH3-L3. Experiments shows that CH-SNN achieves ultra-high structural sparsity while maintaining accuracy. The authors also report significant energy savings when deploying CH-SNN on hardware-friendly algorithm S-TP."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. CH-SNN achieves ultra-high sparsity (>90% on some datasets) without performance degradation.\n\n2. The four-stage framework is well-structured and includes ablation studies showing the necessity of SSCTI and SSWI for stable training under extreme sparsity."}, "weaknesses": {"value": "1. All experiments are conducted on relatively simple tasks using very shallow networks. The lack of evaluation on more complex datasets and deeper SNNs raises serious doubts about scalability. For example, the extremely high sparsity achieved on MNIST is likely attributable to the simplicity of the task, whereas the sparsity drops significantly on CIFAR. It can be inferred that on more challenging benchmarks like ImageNet, the claimed “ultra-sparse” may not be achievable. In contrast, works like SRigL (cited in Section 2.1) have been validated on ResNet-scale models. If the authors can provide relevant evidence, I am willing to increase my rating accordingly.\n\n2. The Spikformer results lack meaningful comparison. First, no other sparse training method is evaluated on Spikformer, making any performance comparison meaningless. Second, Transformers are typically designed for large-scale datasets, applying them to MNIST-level tasks offers little insight. Moreover, the paper never specifies the depth or width of the Spikformer used, making it impossible to assess the result’s significance.\n\n3. The paper repeatedly highlights marginal gains (e.g., outperforms FC network by 0.16% on MNIST) as evidence of superiority. However, MNIST is nearly saturated (~99% accuracy), and such a gain is statistically negligible. This risks overstating the method’s effectiveness."}, "questions": {"value": "1. Section 3.2.1 states that for intermediate layers (e.g., after conv or attention), SSCTI is inapplicable and replaced by uniform random initialization. Does this mean the core topological initialization method is effectively limited to the first layer? How can CH-SNN ensure stable convergence or meaningful structure learning in deeper networks where feature correlations are nontrivial?\n\n2. On CIFAR-100, sparse models report large improvements over the baseline (Table 1). Can the authors explain why there are such large improvements? Intuitively, such a large gap strongly suggests the baseline FC models may not have been sufficiently tuned.\n\n3. Table 1 shows Grad R achieves 91.95% accuracy on DVS-Gesture with an accuracy improvement of +7.83%. However, CH-SNN reports 95.45% accuracy with only +0.38% improvement. Is there a reporting error in the accuracy or the improvement values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RqvvH3oFPl", "forum": "qDLVgr8ESB", "replyto": "qDLVgr8ESB", "signatures": ["ICLR.cc/2026/Conference/Submission1500/Reviewer_iDtv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1500/Reviewer_iDtv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977188512, "cdate": 1761977188512, "tmdate": 1762915785701, "mdate": 1762915785701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}