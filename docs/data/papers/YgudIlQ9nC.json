{"id": "YgudIlQ9nC", "number": 4878, "cdate": 1757781745654, "mdate": 1763727839280, "content": {"title": "Never Saddle: Reparameterized Steepest Descent as Mirror Flow", "abstract": "How does the choice of optimization algorithm shape a model’s ability to learn features? To address this question for steepest descent methods—including sign descent, which is closely related to Adam—we introduce steepest mirror flows as a unifying theoretical framework. This framework reveals how optimization geometry governs learning dynamics, implicit bias, and sparsity and it provides two explanations for why Adam and AdamW often outperform SGD in fine-tuning. Focusing on diagonal linear networks and deep diagonal linear reparameterizations (a simplified proxy for attention), we show that steeper descent facilitates both saddle-point escape and feature learning. In contrast, gradient descent requires unrealistically large learning rates to escape saddles, an uncommon regime in fine-tuning. Empirically, we confirm that saddle-point escape is a central challenge in fine-tuning. Furthermore, we demonstrate that decoupled weight decay, as in AdamW, stabilizes feature learning by enforcing novel balance equations. Together, these results highlight two mechanisms how steepest descent can aid modern optimization.", "tldr": "The connection between reparameterizations and steepest mirror flows shows that the geometry of steepest descent is directly shaped, affecting feature learning by enabling saddlepoint escape, promoting sparsity, and stabilizing invariances.", "keywords": ["Implicit bias", "mirror flow", "sign gradient descent", "Adam", "AdamW", "steepest descent", "reparameterization", "diagonal linear networks"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e9d99d37a95014e156da37656e2c0d8202577e1.pdf", "supplementary_material": "/attachment/8d715f07cd233124356d6747081f8f5124c12b9a.zip"}, "replies": [{"content": {"summary": {"value": "This paper studied and compared the dynamical behaviors of different types of steepest descent algorithms. The authors focused on deep linear diagonal models and showed that a steepest flow in the weight matrices with respect to $L_p$ norm induces a \"steepest mirror flow\" in the end-to-end matrix. They analyzed how the choice of $p$ influences the dynamics, convergence rate and the effects of weight decay. Experiments on both linear and practical models are provided for the comparison between different algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper extends the study of reparametrization in gradient flow to steepest flow. This provides a useful platform for studying how reparametrization and optimization geometry interact to shape the training dynamics. I find this extension meaningful and valuable. Several established results are quite interesting: \n\n* Lemma 4.4: The balance equation for steepest flow in training deep diagonal linear network is new and interesting. \n\n* Theorem 4.6: The equivalence between steepest flow with reparametrization and \"steepest mirror flow\" is quite interesting. To my knowledge, previous work only showed the equivalence between gradient flow with reparametrization and mirror flow. \n\n* Corollary 4.8: It sheds light on how different choices of optimization geometry and reparametrization (network depth) could affect the convergence rate."}, "weaknesses": {"value": "The main weakness is that the main claims are not well supported by the presented theory. The title begins with \"Never saddle\" and the authors emphasized both in the abstract and in Contributions that \"we prove that steeper descent (lower $q$) simultaneously escapes saddles faster and supports feature learning\", and that \"decoupled weight decay ... yielding more stable feature learning\". While these statements are strong and intriguing, I do not find them adequately supported by the theoretical results. \n\n**Regarding saddle escaping**\n\nAfter Lemma 4.4 and in Figure 3, the authors claimed that \"the (curved) path away from zero is shorter for smaller $q$, indicating faster saddle escape\". This reasoning is not logically sound: First, the notion of a \"shorter path\" is ambiguous (e.g., between which endpoints?). Second, the geometry of the invariant manifold alone does not determine how fast the algorithm moves along the manifold. \n\nBesides the above, the only theoretical evidence for the claim that \"lower $q$ escapes saddles faster\" is based on two results: (i) Corollary 4.8: under initialization \"$w_1 = 0$ and $w_i = \\lambda > 0$\", a smaller $q$ yields a larger coercivity constant $\\mu$; and (ii) Theorem 4.2: when $R$ is a separable Bregman function, a larger $\\mu$ yields a faster linear convergence rate. However, I do not find these two results sufficient for the claim, for the following reasons: \n\n* It is not established that the $R$ used in Corollary 4.8 (or Theorem 4.6) is a separable Bregman function. Thus it is unclear whether Theorem 4.2 applies; \n\n* The considered initialization \"$w_1 = 0$ and $w_i = \\lambda > 0$\" lies in a measure-zero set; \n\n* Only a subset of saddles (those near the considered initializations) are analyzed, whereas, as noted by the authors, other saddles exist elsewhere in the parameter space. \n\nTherefore, even if Theorem 4.2 is applicable, the results only indicate that: for certain specific saddles and for a measure-zero set of initializations in their neighborhoods, a smaller $q$ leads to a faster escape. I thus think it is overstated to use the title \"never saddle\", or to claim that smaller $q$ results in faster saddle escaping, which sounds like a general statement, just based on these results. \n\n\n\n**Regarding feature learning** \n\nThe authors claimed that \"smaller $q$ supports feature learning\". However, in the theory section (Section 4), I could not find a clear discussion on how the choice of $q$ affects feature learning. The following two aspects might be relevant, but it is unclear how they support the claim: \n\n* In Corollary 4.11 and 4.12, the authors discussed whether the function $R_{L_p, L}$ is a Legendre function under different $q$, network depth $L$, and metric exponent $m$. Then in the left panel of Figure 2, the authors indicated that a metric exponent slightly smaller than $1$ correspond to a feature learning regime (the green band in the figure). However, it is not explained why this is true and how does metric exponent relate to feature learning. For example, why does $m=0.95$ lead to feature learning, while $m=0.5$ or $m=1.5$ does not?\n\n* In Theorem 4.14, the authors discussed the on manifold regularization induced by weight decay under different $q$. However, as shown in Table 1, when $q=2, L=2$, the decoupled weight decay induces an $L_1$ bias; whereas when $q=1, L=2$, it induces an $L_{3/2}$ bias, which is less sparse than $L_1$. As sparsity biases have been linked to feature learning in some settings, this observation actually suggests that $q=2$ may facilitate feature learning instead of a smaller $q$.  \n\nThe authors also claimed that \"decoupled weight decay, as in AdamW, stabilizes feature learning by enforcing novel balance equation\". The balance equation in Lemma 4.4 indicates that the weight decay encourages the weights to become more balanced during training. But it is not clear to me how this balancing translates into a more \"stable\" feature learning. \n\nI suggest that the authors clarify in the paper what they mean by \"feature learning\" and \"stable feature learning\", and then compare the algorithms with different values of $q$ as well as with and without weight decay.\n\n\n**Initial recommendation**\n\nOverall, while the study of how optimization geometry and reparametrization affect the dynamics and the proposed framework are very interesting, I find the main claims of this paper, in particular those on saddle escaping and feature learning, insufficiently supported. Therefore, my initial recommendation is rejection."}, "questions": {"value": "I find some parts of the presentation unclear. Please see the questions below. \n\n**Notations** \n\n* In Example 3.2, is the expression \"a deep diagonal linear network $g(m,w)=m \\odot w$\" actually referring to a shallow network, as there are only two weight matrices $\\operatorname{Diag}(w), \\operatorname{Diag}(m)$? \n\n* In Definition 3.3 and 4.5, does $\\mathbb{I}_n$ denote the all-one vector? This symbol is commonly used for the identity matrix, so clarification would be helpful. \n\n* In Corollary 4.8, the notation $w_i=\\lambda>0$ is ambiguous. Does it mean the vector $w_i$ has all entries equal to $\\lambda$? \n\n**Regarding Theorem 4.6**\n\n* $\\lambda−L_p-$balancedness in Definition 4.5 is defined for shallow network with two weight matrices. Then what does \"$\\lambda−L_p-$balanced with respect to the first parameter $w_1$\" mean in Theorem 4.6, which is stated for deep networks? \n\n* In Lemma 4.4 and Theorem 4.6, in what sense does \"almost everywhere\" mean (e.g.,\"steepest descent satisfies a separable $L_p$-mirror flow almost everywhere\")? Does it mean the result may fail only for a measure-zero set of initializations?\n\n* Corollary 4.12 discussed when the function $R_{L_p,\\ L}$ is a Legendre function. However, in Theorem 4.6, there seems no restriction on $p$ or $L$. Does Theorem 4.6 indicate that such an $R_{L_p,\\ L}$ always exists, even in cases where it is not a Legendre function?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JyZhHJuMj3", "forum": "YgudIlQ9nC", "replyto": "YgudIlQ9nC", "signatures": ["ICLR.cc/2026/Conference/Submission4878/Reviewer_NKAm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4878/Reviewer_NKAm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760823374260, "cdate": 1760823374260, "tmdate": 1762917636243, "mdate": 1762917636243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces steepest mirror flows as a unifying geometric framework to study how optimization algorithms influence reparametrization. By analyzing diagonal linear networks and deep diagonal reparameterizations, the authors show that steeper descent methods, such as sign-based variants, escape saddle points more efficiently than gradient descent. Empirical results from linear regression, classification, and fine-tuning experiments confirm the theoretical predictions of faster saddle escape, stable learning, and distinct sparsity dynamics between coupled and decoupled weight decay."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is an interesting study with insightful findings. Combining mirror descent with reparameterization is a great idea."}, "weaknesses": {"value": "The statement of the first contribution is misleading. This is not the first work studying GF and reparametrization. It is probably meant with respect to the family. \n\nFor this type of work, assumptions are always problematic. They are very simplistic (diagonal networks) but it is very hard to potentially show more general results. \n\nIn experiments, rather than studying the networks considered in the analysis, they should see if the results hold when the assumptions are not fulfilled (for example, other networks)."}, "questions": {"value": "What is meant by: iterates x_t converge? I assume it is meant lim_t->\\infinity x_t"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qAkkYpBxIR", "forum": "YgudIlQ9nC", "replyto": "YgudIlQ9nC", "signatures": ["ICLR.cc/2026/Conference/Submission4878/Reviewer_e8j7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4878/Reviewer_e8j7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945052256, "cdate": 1761945052256, "tmdate": 1762917633617, "mdate": 1762917633617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes “steepest mirror flows” to explain why Adam/AdamW beat SGD in fine‑tuning via faster saddle escape and different implicit regularization.\nSteeper (sign‑like) geometry helps saddle escape and that decoupled weight decay (AdamW) stabilizes feature learning, with theory for deep diagonal reparameterizations and small fine‑tuning case studies. Most formal results hold only in separable/diagonal settings; the transformer link is indirect; and empirical support for fine‑tuning claims is narrow."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The message that adam escapes saddles better than SGD is believable and may be cool. However, it is not well established. The rest of the claims are not substantiated."}, "weaknesses": {"value": "#### How is this about fine-tuning transformers?\nThere is not even a linear diagonal attention there, no one ever claimed that a diagonal network is a good model for a transformer, because it is not. How do you argue this? \n\n#### Mirror flow study is incremental and does not adequately support the thesis.\nWhile the diagonal‑network analysis is neat, it is very similar to existent ones and does not bring any real novelty to the community. \n*I believe, it is extremely incremental.*\nIt is way too limited to show that your sign-mirror-descent escapes saddles to claim that *adam* escapes saddles *in transformers*.\nThere is a too big of a jump from the mathematical argument to the goal. Moreover, I think that such a result is a perturbation of existent ones.\n\n#### Order-2 saddles\nSo what? We know they are present in neural networks, actually, arguably also of higher order. A long line of works that you do not cite addresses this issue much more in general. That same line of work empirically and sometimes theoretically notice that they are not a problem in practice. You should discuss this line of research. On top of it saddles are not an issue in linear networks generally cause the standard initialization is with high probability outside of the area with saddles.\n\n#### Title mismatch\n\nEven the title is an oversell. It missmatch with the paper, this is not a paper about generally reparameterizing steepest descent as mirror flow. This is a paper about adam escaping saddles which lack novelty and does not support their claims. \n\n#### Experiments\nFor cifar10 actually SGD generalizes better than Adam with CNNs, and this is also a classical result. I really do not understand the whole point of the paper and how these are supporting experiments.\n\n#### Conclusions\nEven though experiments are present, I thus believe the central claims are not adequately supported and the research methodology is not sound."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Pk1qk3Apd2", "forum": "YgudIlQ9nC", "replyto": "YgudIlQ9nC", "signatures": ["ICLR.cc/2026/Conference/Submission4878/Reviewer_iSvT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4878/Reviewer_iSvT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965655638, "cdate": 1761965655638, "tmdate": 1762917632271, "mdate": 1762917632271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies continuous-time steepest descent methods, specifically algorithms taking the form:\n$$\\begin{align} d x_t = - \\mathrm{sign} \\left( \\nabla_x f(x_t) \\right) \\odot \\left| \\nabla_x f(x_t) \\right|^{q-1}, \\end{align}$$\nwhere $q \\in [1,2]$.\n\nDifferent values of $q$ result in different trajectories. For example:\n For $q=2$, the algorithm becomes gradient flow, which is the continuous-time approximation of gradient descent. For $q=1$, it becomes SignGF (Sign Gradient Flow), which serves as a good proxy for studying optimizers like Adam. The architecture on which the paper focuses is deep diagonal reparameterization, defined as $x = g(w) = \\prod_{i=1}^{L} w_i$, where $x$ is represented as the product of $L$ scalars.\n\nThe authors demonstrate that under $\\lambda$-balanced initializations, these steepest flows can be re-parameterized as steepest mirror flows. Using this reparameterization, the paper analyzes how quickly the flow escapes saddle points and investigates the effect of both coupled and decoupled weight decay."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a seperation result for signGD with coupled and decoupled weight decay and show that they have different regularization properties which is interesting."}, "weaknesses": {"value": "a) The paper focuses on deep diagonal reparameterizations which is a product of one-dimensional variables with a particular initialization shape. The setting is restrictive to generalize the results of the paper."}, "questions": {"value": "1) How is the manifold regularizer derived ? Is it due to the fact that steepest descent with de coupled weight decay can be written as \n\n$$ d( \\nabla_x R(x_t) ) = - \\mathrm {sign} \\left( \\nabla_x f(x_t) \\right) \\odot \\left[   \\nabla_x f(x_t) \\right]^{q-1} dt  - \\nabla M_{reg}(x) dt  $$\n\nit would be nice to detail this as the manifold regularizer appears a bit abrupt. \n\n2) The saddle points defined in Theorem 4.3 are not saddle for with coupled or decoupled weight decay so the feature learning results only work without the weight decay. This point needs to be clearly mentioned in the manuscript. \n\n3) From the abstract, how is the deep diagonal reparameterizations a proxy for attention ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "19kme4BkUP", "forum": "YgudIlQ9nC", "replyto": "YgudIlQ9nC", "signatures": ["ICLR.cc/2026/Conference/Submission4878/Reviewer_f8Av"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4878/Reviewer_f8Av"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4878/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131822870, "cdate": 1762131822870, "tmdate": 1762917631024, "mdate": 1762917631024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Part1"}, "comment": {"value": "**General response**\nWe would like to thank all the reviewers for their efforts in reviewing our work and providing valuable comments. We have incorporated the comments of all reviewers and updated our draft accordingly, which we believe makes our work stronger. To make it easy for the reviewers to go over these changes, the updated text is in blue in the draft.\n\nWe address here one of the main concerns regarding our work: the scope of the analysis. Furthermore, we provide additional experiments: finetuning for  vision transformers and LLMs and a larger scale sparsity experiment for a ResNet50 on Imagenet.\n\n**Scope**\nOne of the main goals of theory in deep learning is to describe and explain phenomena that occur in practice. While our model is simple, it does give multiple valuable insights into the mechanics of relevant practical algorithms. Concretely, it allows us to make predictions for real world settings.\n\n**New geometric mechanism**\nOur geometric approach using mirror flow reveals a different principle at work that previous work has shown to escape saddle points. We show that the geometry is sufficient for escaping saddles in our setup, while previous work relies on noise and large learning rate or time rescaling [6]. Therefore, we believe revealing other mechanisms is of great value. In the revised manuscript, we include this discussion on noise and how it aids in saddle point escape in the related work. Our mechanism is corroborated by experiments, see for example Figure 5a where SGD with small learning rate cannot escape the saddle point indicating that noise is not sufficient. Moreover we have included additional experiments on saddle point escape in the appendix.\nFinally, revealing a structural mechanism helps us to identify where to look for developing new theoretical tools to study training dynamics. While we agree that a straightforward generalization using mirror flows may be hard, it can be used as a stepping stone for guiding other theoretical approaches and motivating identifying geometric properties. This is also in line with recent position papers calling for studying algebraic properties of models [4] and studying linear models [5].\n\n**Main use case: finetuning and saddle escape**\nSince Adam and its variants remain the most widely used optimizers for finetuning, and sign decent (steepest descent wit respect $L_{\\infty}$ norm) serves as a proxy for Adam, the steepest mirror flow analysis naturally extends to the finetuning setting. In particular, finetuning typically employs a small learning rate to avoid large deviations from the pretrained parameters and to mitigate catastrophic forgetting. This aligns well with our flow analysis. In contrast, SGD has to use a significantly larger learning rate for saddle point escape, which is not desired in finetuning as it would induce catastrophic forgetting. This observation is substantiated by our real world experiments with transformers. We highlighted this in Figure 5a and in additional ablations in Appendix K with new additions on ViTs and Transformers in K.1 and K.2..\n\n**A theoretical use case: feature learning**\nRecent work has characterized the implicit bias for steepest descent flows for homogeneous neural networks on binary classifications for separable data [3]. We have applied their result in Theorem F.1 and highlighted that the $L_{\\infty}$ max margin optimization problem cannot distinguish between solutions that correspond to overfitting or feature learning (sparsity). In contrast our theory predicts that depth changes the geometry and with that the bias towards sparsity and feature learning, which is also illustrated in the experiment depicted in Figure 4. This is a clear sign that we need additional tools and theory to describe the implicit bias further.\n\n**A practical use case: sparsity**\nAs discussed, pointwise reparameterizations have been proposed together with weight decay to induce sparsity [1,2]. An integration with modern optimizers is valuable, as it leads to concrete design principles. Our theory predicts that for smaller $q$, so for SignGF, we need to have higher depth to induce sparsity (and thus feature learning). Moreover, Figure 5b shows that using AdamW requires higher depth to induce sparsity. This may lead to better sparse training algorithm design in the future."}}, "id": "Ii7axq2EFA", "forum": "YgudIlQ9nC", "replyto": "YgudIlQ9nC", "signatures": ["ICLR.cc/2026/Conference/Submission4878/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4878/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission4878/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763727479503, "cdate": 1763727479503, "tmdate": 1763727479503, "mdate": 1763727479503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Part 2"}, "comment": {"value": "**Additional experiment: Finetuning**\nWe also provide experiments in both the vision and language domain with transformer architectures. \nSpecifically, we finetune a ViT-Large pretrained on ImageNet on CIFAR-10 for 30 epochs and on Flowers for 15 epochs. As is standard in finetuning, the original classifier head is replaced with a newly initialized one. We evaluate two optimizers—SGD and Adam—with learning rates selected via a sweep: $\\eta \\in \\{9e-5, 1e-4, 1e-4, 5e-4\\}$ for Adam and $\\eta \\in \\{1e-3, 5e-3, 1e-2, 5e-2, 1e-1\\}$ for SGD. Additionally, we run SGD with the best Adam learning rate to further illustrate our observations on saddle escape. All experiments use batch size $128$, weight decay $0$, cosine annealing learning rate scheduling, and label smoothing of 0.1. The table below reports validation accuracy along with the parameter shift measured in $L_1$ and $L_2$ norms. Adam consistently outperforms SGD and induces a more uniform parameter update pattern, reflected in its substantially larger $L_1$ norm.\n\n| Dataset   | Metric | SGD ($\\eta=0.0001$) | SGD ($\\eta=0.01$) | Adam ($\\eta=0.0001$) |\n|-----------|--------|----------------------|--------------------|------------------------|\n| **CIFAR-10** | Val Acc | $73.27 \\pm 3.68$ | $99.07 \\pm 0.35$ | $99.28 \\pm 0.07$ |\n|           | $L_1$   | $460.47 \\pm 219.86$ | $24617.83 \\pm 14406.4$ | $453934.91 \\pm 22278.43$ |\n|           | $L_2$   | $0.48 \\pm 0.059$ | $6.25 \\pm 4.29$ | $39.47 \\pm 1.01$ |\n| **Flowers** | Val Acc | $1.03 \\pm 0.82$ | $98.94 \\pm 0.05$ | $99.37 \\pm 0.08$ |\n|           | $L_1$   | $25.71 \\pm 30.48$ | $4655.83 \\pm 576.49$ | $108583.62 \\pm 2078.48$ |\n|           | $L_2$   | $0.04 \\pm 0.02$ | $1.50 \\pm 0.12$ | $8.35 \\pm 0.16$ |\n\n\n\nIn addition to vision experiments, we conduct a parallel study on language models. Specifically, we finetune a pretrained BERT-base model on the MRPC task from the GLUE benchmark. The model is finetuned for 5 epochs using both SGD and Adam. Learning rates are selected via a sweep:\n$\\eta \\in \\{5\\times10^{-5},\\, 7\\times10^{-5},\\, 9\\times10^{-5}\\}$ for Adam, and\n$\\eta \\in \\{10^{-2},\\, 5\\times10^{-2},\\, 10^{-1},\\, 5\\times10^{-1}\\}$ for SGD.\nWe additionally evaluate SGD using the best learning rate obtained for Adam. The table below reports the validation accuracy along with the parameter displacement measured in $L_1$ and $L_2$ norms. Similar conclusions can be made in language tasks.\n| Metric | SGD ($\\eta=7\\times10^{-5}$) | SGD ($\\eta=0.1$) | Adam ($\\eta=7\\times10^{-5}$) |\n|--------|-----------------------------|------------------|-------------------------------|\n| **Val Acc** | $43.87 \\pm 24.02$ | $84.80 \\pm 1.00$ | $85.95 \\pm 0.64$ |\n| **$L_1$** | $5002.44 \\pm 0.00$ | $6066.93 \\pm 34.33$ | $31079.54 \\pm 754.26$ |\n| **$L_2$** | $0.73 \\pm 0.00$ | $1.26 \\pm 0.01$ | $5.57 \\pm 0.24$ |\n\n\nFurther visualisations, such as eigenvalue spectra, are provided in Appendix K of the revised manuscript. \n\n\n**Additional experiment: Sparse training**\nWe furthermore conduct a similar sparse training experiment where we use the reparameterization with varying depths and compare coupled and decoupled weight decay for a ResNet-50 on Imagenet. We confirm the same finding as for our previous experiment for a ResNet-20 on CIFAR-10: Coupled weight decay needs higher depth and weight decay to induce sparsity.\nThis demonstrates that our insight on coupling also holds in larger scale settings. Results are reported in Figures 13, 14, and 15 and the validation accuracies in Table 6 of the revised manuscript.\n\n| Optimizer   | Weight Decay | depth L | L1 Mean   | L1 95% Std | Val Accuracy ± CI (%) |\n|-------------|--------------|---------|-----------|------------|-------------------|\n| AdamW       | 1e-1         | 2       | 29,417    | 26         | 76.23 ± 0.07      |\n| AdamW       | 1e-1         | 10      | 10,616    | 157        | 62.20 ± 0.25      |\n| AdamW       | 1e-4         | 2       | 3,532,879 | 1,282      | 73.32 ± 0.11      |\n| AdamW       | 1e-4         | 10      | 8,369,848 | 26,023     | 73.19 ± 0.04      |\n| Adam + wd   | 1e-1         | 2       | 0         | 0          | 1.95 ± 0.48       |\n| Adam + wd   | 1e-1         | 10      | 0         | 0          | 0.58 ± 0.06       |\n| Adam + wd   | 1e-4         | 2       | 4,473     | 18         | 73.35 ± 0.05      |\n| Adam + wd   | 1e-4         | 10      | 324       | 32         | 9.78 ± 0.94       |"}}, "id": "0BrxlrPleM", "forum": "YgudIlQ9nC", "replyto": "YgudIlQ9nC", "signatures": ["ICLR.cc/2026/Conference/Submission4878/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4878/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission4878/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763727543277, "cdate": 1763727543277, "tmdate": 1763727543277, "mdate": 1763727543277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Part 3"}, "comment": {"value": "[1] Jacobs, Tom and Rebekka Burkholz. “Mask in the Mirror: Implicit Sparsification.” ArXiv abs/2408.09966 (2024): n. pag.\n\n[2] Kolb, Chris et al. “Deep Weight Factorization: Sparse Learning Through the Lens of Artificial Symmetries.” ArXiv abs/2502.02496 (2025): n. Pag.\n\n[3] Tsilivis, Nikolaos et al. “Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks.” ArXiv abs/2410.22069 (2024): n. Pag.\n\n[4] Marchetti, Giovanni Luca et al. “Algebra Unveils Deep Learning -- An Invitation to Neuroalgebraic Geometry.” (2025).\n\n[5] Nam, Yoonsoo et al. “Position: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena (Neural Collapse, Emergence, Lazy/Rich Regime, and Grokking).” ArXiv abs/2502.21009 (2025): n. pag.\n\n[6] Pesme, Scott and Nicolas Flammarion. “Saddle-to-saddle dynamics in diagonal linear networks.” Journal of Statistical Mechanics: Theory and Experiment 2024 (2023): n. pag."}}, "id": "pY7WwwKiwT", "forum": "YgudIlQ9nC", "replyto": "YgudIlQ9nC", "signatures": ["ICLR.cc/2026/Conference/Submission4878/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4878/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission4878/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763727576308, "cdate": 1763727576308, "tmdate": 1763727576308, "mdate": 1763727576308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}