{"id": "1pTzWVvwEd", "number": 23386, "cdate": 1758343009749, "mdate": 1759896817571, "content": {"title": "Incremental Learning in Transformers for In-Context Associative Recall", "abstract": "Transformers acquire in-context learning abilities in abrupt phases during training, often unfolding over multiple stages, during which certain keys circuits like induction heads emerge. In this work, we characterize the dynamics behind the emergence of such circuits during these stages. We focus on a synthetic associative recall task, where sequences are drawn from random maps between a permutation group and a vocabulary range and the model is required to complete the mapping of a permutation by retrieving it from the context. On this task, we study the trajectories of gradient flow of a simplified two-layer, attention-only transformer. Leveraging symmetries in both the transformer architecture and the data, we derive conservation laws that guide the dynamics of the parameters. These conservation laws crucially reveal how initialization —both in shape and scale— determines the order of learning as well as the timescales over which such circuits emerge revealing the implicit curriculum. Finally, we provide empirical evidence across different architectural choices, validating  our simplifications and generalizing the insights from our analysis beyond the simple setting.", "tldr": "", "keywords": ["incremental learning", "stage-wise learning", "training dynamics", "incontext learning", "associative recall"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/93777ae0e564c9d7c07e3eef68c56b052635ed89.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates how a small, two-layer attention-only transformer learns an n-gram recall in-context learning (ICL) task, where the model must retrieve the correct output by matching the final \\(n=4\\) tokens of a cue seen earlier in the same sequence. Building on previously proposed attention-based solutions for n-gram recall (Varré et al.), the paper constructs a minimal model that implements those solutions. This model admits a fully analytic treatment under gradient flow and captures the learning behavior observed in practice. Experiments training the model are consistent with the analytic predictions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation for introducing a minimal transformer model is clear. The proposed minimal model capture n-gram recall ICL solution.\n\n2. The minimal transformer model is fully analytic under gradient flow and yields closed-form training dynamics. Analyzing these dynamics reveals a conservation law, initialization dependence, and phase ordering"}, "weaknesses": {"value": "1. The abstracted transformer architecture may be too minimal. While recent work aims to reduce complexity without losing fidelity, it remains unclear how much intuition from this abstraction transfers to real LM systems.\n\n2. The focus is limited to n-gram ICL, a setting already known to work as described; the incremental insights appear modest.\n\n3. Experiments are conducted in a specific setting without varying key factors such as \\(N\\) (the n-gram length) or architectural choices."}, "questions": {"value": "1. Ablations (A1--A4): How do ablations of A1, A2, A3, and A4 affect empirical training outcomes? Does the phenomenology seen in the full simple model persist under these ablations?\n\n2. Beyond n-gram ICL: How can the findings be extended to tasks outside n-gram recall? In particular, some classes of algorithmic ICL tasks (e.g., linear regression) have known transformer implementations (e.g., Lu et al., 2025). How can the paper’s findings be generalized to that class?\n\n3. Timing of transitions: Does the model predict the time points of performance ``jumps''? If so, do these predictions match experimental results?\n\n4. Initializations: Can you demonstrate how varying initialization affects the results in Figure 3?\n\nReferences\n[1] Varre, A., Yüce, G., Flammarion, N. “Learning In-context n-grams with Transformers: Sub-n-grams Are Near-stationary Points.” 2025\n\n[2] Lu, Yue M., et al. \"Asymptotic theory of in-context learning by linear attention.\" Proceedings of the National Academy of Sciences 122.28 (2025): e2502599122."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uZZs670nFO", "forum": "1pTzWVvwEd", "replyto": "1pTzWVvwEd", "signatures": ["ICLR.cc/2026/Conference/Submission23386/Reviewer_g8Xr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23386/Reviewer_g8Xr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761501411231, "cdate": 1761501411231, "tmdate": 1762942639558, "mdate": 1762942639558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyses how in-context learning circuits (e.g. induction heads) emerge during training in transformers.\nTo investigate this, the authors design:\n* a controlled in-context associative recall task where the model must retrieve the correct response to a query by matching that query to an earlier key-value pair in the same prompt, and\n* a heavily simplified 2-layer attention-only transformer where most parameters are fixed and only a few scalar/logit parameters are learned.\n\nIn this setting, they derive continuous-time gradient flow dynamics and prove that learning proceeds in stages: the model sits on plateaus where a partial recall circuit exists (e.g. can match only part of the query), then undergoes sharp “jumps” to a more complete recall mechanism. They link these jumps to saddle escape and show a conserved quantity that couples parameters across layers.\nThey further argue that which attention head “activates” first is determined by initialization scales, implying that heads specialize sequentially rather than all at once. Small experiments on order-4 recall show plateau to jump loss curves and sequential head specialization consistent with the theory."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Mechanistic training dynamics.\nThe work gives a concrete dynamical story for how in-context recall circuits form over time, including why capabilities appear in sharp jumps instead of smoothly.\n\n2. Elegant task/model co-design.\nThe associative recall task and the simplified transformer still capture key ingredients of induction-style retrieval (causal attention, multi-head structure), but are analysable enough to get closed-form ODEs and a conservation law.\n\n3. Theory explains qualitative phenomena people see in practice.\nThe staged plateaus, sudden capability jumps, and head specialisation order are all things observed in real small transformers; here we get a mechanistic account (ordered head activation driven by initialization scale, long dwell times near saddles).\n\n4. Potential practical implication.\nThe result that init scales $\\beta_h$ bias which head learns first suggests we can steer emergent specialization and maybe training curricula just by tuning initialization, which is actionable."}, "weaknesses": {"value": "1. Gap to realistic models.\nThe theoretical guarantees rely on a very stylised transformer: fixed value structure, no MLPs, one-hot tokens, continuous-time gradient flow on full-population loss, etc. It's unclear how directly the results carry over to standard GPT-style models trained with SGD on natural data. The paper argues informally that the story should generalize, but does not really show it.\n\n2. Light empirical support.\nExperiments are on a tiny recall task with tiny vocab, and are mostly qualitative (plots of plateau/jump behavior, sequential $\\beta_h$  growth). There’s little quantitative matching between the theoretical predictions (e.g. plateau duration scaling, ordering of head activation by init magnitude) and actual measured numbers.\n\n3. SGD / noise is under-discussed.\nThe analysis assumes noiseless gradient flow. In practice, SGD noise helps models escape saddles. If saddle escape timing is central to the staged-learning story, then we need at least a discussion (or small ablation) of how noise affects plateau lengths and ordering.\n\n4. Accessibility / reproducibility.\nSome of the most interesting claims (like the conservation law and the staged head activation sequence) depend on fairly dense math and on training details that aren’t fully specified in the experimental section. This makes it harder for a broad ICLR audience to verify or reproduce."}, "questions": {"value": "1. Generalization to standard transformers:\nIf you allow a more realistic transformer (trainable value matrices, MLPs, standard CE loss, SGD noise), do you still see the same staged head activation and ordered specialization? Have you run even small ablations in that more general setting?\n\n2. Effect of SGD noise:\nYour analysis is in deterministic gradient flow on the population loss. In actual training, SGD noise is often viewed as the mechanism that helps leave plateaus. Do you expect noise to (a) only change the timing of the jumps, or (b) potentially reorder which head “wins” first? \n\n3. Quantitative match to theory:\nYou argue that the model dwells near partial circuits for long periods and then jumps. Can you report measured plateau durations vs. the theoretically predicted scaling (e.g. O(1/ϵ)) and show how close they are?\n\n4. Scaling the task:\nThe associative recall task is “clean”: the answer always appears in the prompt, there’s exactly one correct continuation, and there’s no ambiguity. How do the dynamics change if the model has to generalize from incomplete evidence or noisy matches (i.e. more like natural text)?\n\n5. Conservation law intuition:\nThe conserved quantity tying together first- and second-layer parameters is one of the most interesting parts of the paper. Can you give more geometric or mechanistic intuition for what it “means” operationally, in a way that a practitioner could try to measure in a non-simplified model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JtVfRiXdHd", "forum": "1pTzWVvwEd", "replyto": "1pTzWVvwEd", "signatures": ["ICLR.cc/2026/Conference/Submission23386/Reviewer_XNc8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23386/Reviewer_XNc8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761587731079, "cdate": 1761587731079, "tmdate": 1762942638923, "mdate": 1762942638923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on understanding the dynamics of emergence of mechanisms for in-context tasks. In particular, they characterize the emergence of circuits for an in-context recall task in a two-layer Transformer, and demonstrate that shape/scale of initialization can influence the timescales on which circuits are learned."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The \"methodological\" contribution of the paper, i.e. understanding how internal mechanisms are influenced by optimization pressure, is extremely strong and very valuable, and will I think encourage interesting discussion.\n* The work is theoretically and methodologically sound.\n* The paper is very well written, figures are super helpful."}, "weaknesses": {"value": "* I think (as with any work of this kind) there are some significant limitations in terms of realism of the setting: e.g. the paper focuses on a very small Transformer, most of the analysis is done in the vanishing scale of initialization, the synthetic task might not generalize to things we actually care about."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S7FybEl6pd", "forum": "1pTzWVvwEd", "replyto": "1pTzWVvwEd", "signatures": ["ICLR.cc/2026/Conference/Submission23386/Reviewer_jH7v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23386/Reviewer_jH7v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968292119, "cdate": 1761968292119, "tmdate": 1762942638627, "mdate": 1762942638627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors consider an in-context associative recall task trained using a simplified two-layer attention-only network. The main contribution is a quantitative description of the learning dynamics based on key parameters, which include the positional bias and diagonal elements of the key-query matrices."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a series of simplifications to obtain insight into the training dynamics of a transformer. The simplifications are phenomenology-driven: by considering the key parameters that make up the sub-circuit that performs associative recall, they obtain a simplified description of the training dynamics that yet captures most elements of sub-circuit formation. I believe this is a strength of the paper -- the analysis provides some insight in scenarios where an exact solution cannot be derived."}, "weaknesses": {"value": "In my opinion, the paper has three major weaknesses:\n\nFirst, while the authors derive a series of mathematical expressions, none of these calculations are quantitatively tested in numerical simulations (except for a qualitative one in Figure 3). This seems like a missed opportunity to connect theory to experiment, which is noteworthy here because the simulations are not computationally expensive and the authors presumably already have an implementation (as evidenced by Figure 3, 4). Moreover, none of the details of the experiments are presented, which makes it impossible to interpret Figure 3,4. It is not clear whether the experiments were performed using a standard two-layer model or the simplified model. Without these experiments, it is impossible to say whether the theory the authors have written down using the simplified model accurately captures the dynamics of a full transformer model. Are there specific predictions that the theory offers which can be tested using numerics?\n\nSecond, there is a key positivity assumption made in assumption A1 (page 5 bottom). Here the query-key product is expressed in terms of \\beta^2 rather than \\beta. However, there is no constraint in a full transformer that imposes positivity of the diagonal entries, so it is unclear why one should assume positivity in the simplified model. How does relaxing the positivity assumption on \\beta^2 change the analysis?  \nThis is an important point, because if \\beta^2 were replaced with \\beta, one would find a saddle point close to initialization: this is apparent when one writes the expression for the ODE for \\beta_1 (Theorem 3.1) in terms of \\beta_1^2 rather than \\beta_1. That is, the dynamics will flow to different basins depending on how \\beta and \\alpha are initialized. A similar phenomenon has been shown to occur in previous analyses. For example, see eqns 7-9 in https://openreview.net/forum?id=INyi7qUdjZ, who also derive the training dynamics of the induction head circuit in terms of effective parameters. In that setting, the saddle point is absent when one takes into account randomness in sampling input examples. \n\nThird, the authors seem to be assuming that all (k+1)! permutations are shown in the context. This is an unrealistic assumption. I understand that this simplifies the analysis, but as pointed above, the randomness in sampling an input can indeed matter. It is unclear how this assumption will impact the generality of the results."}, "questions": {"value": "Please see the questions raised in the weaknesses section above. In my opinion, the first point regarding the lack of numerical tests is sufficiently serious to warrant rejection -- at this stage, the theory has no empirical grounds on which it has firm footing, though this is of course not unresolvable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8Yee2bW7OX", "forum": "1pTzWVvwEd", "replyto": "1pTzWVvwEd", "signatures": ["ICLR.cc/2026/Conference/Submission23386/Reviewer_UrhB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23386/Reviewer_UrhB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140385852, "cdate": 1762140385852, "tmdate": 1762942638001, "mdate": 1762942638001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}