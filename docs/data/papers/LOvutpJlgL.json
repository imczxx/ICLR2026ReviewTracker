{"id": "LOvutpJlgL", "number": 24685, "cdate": 1758359345666, "mdate": 1763670864929, "content": {"title": "Practical Diffusion Planning via Temperature-Guided Reward Conditioning", "abstract": "Diffusion planners address sequential decision-making by framing plan generation as a generative modeling task over trajectories, mitigating compounding errors and myopic predictions typical of autoregressive methods. They sample long-horizon, globally consistent plans in a single pass, enabling parallel refinement and robust handling of multimodal futures. Reward conditioning is typically achieved through classifier guidance or classifier-free guidance (CFG), with CFG favored for its performance and flexibility but requiring extensive, task-specific hyperparameter tuning that limits scalability and generalization. Our analysis reveals that guidance performance hinges on careful adaptation to the data manifold and reward distribution, contributing to CFG's hyperparameter fragility. In this work, we propose the temperature-guided diffusion planner (TGDP), which adapts CFG for reward conditioning by self-calibrating to these task-specific characteristics. TGDP leverages temperature-based sample reweighting during training and adaptive guidance scaling at inference, yielding robust high-reward plan generation without per-task hyperparameter optimization. Across standard reward-driven benchmarks, TGDP matches performance of prior methods while maintaining a single set of default hyperparameters, establishing a practical, scalable, and generalizable approach to diffusion-based planning.", "tldr": "Temperature-Guided Diffusion Planning is a guidance approach for diffusion planning that overcomes the per-task hyperparameter optimization of classifier-free guidance.", "keywords": ["offline reinforcement learning", "diffusion planning", "generative models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3200efefdf4c34ffdad0a829fc0917b10c6bf377.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper “Practical Diffusion Planning via Temperature-Guided Reward Conditioning (TGDP)” introduces a diffusion-based planning method that removes the need for task-specific tuning in reward-conditioned decision-making. Building on Classifier-Free Guidance (CFG), the authors propose a temperature-guided framework where trajectory samples are reweighted during training according to a temperature parameter that biases learning toward higher-return trajectories while preserving data diversity. At inference, TGDP employs an adaptive guidance mechanism that dynamically adjusts the influence of reward conditioning based on the geometric alignment of denoising directions across temperature scales, thereby preventing under- or overguidance. Experiments on standard D4RL locomotion, Maze2D, and Kitchen benchmarks show that TGDP achieves performance on par with or exceeding existing diffusion planners, all while using a single default hyperparameter configuration and maintaining computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well motivated and clearly written, providing a logical flow from problem statement to solution.\n\n- The proposed TGDP method is technically sound and conceptually reasonable, effectively addressing CFG’s sensitivity to guide-scale tuning.\n\n- The method design is elegant and practical, integrating temperature-guided weighting and adaptive guidance without significant computational or architectural overhead.\n\n- Experiments are comprehensive and well executed, covering multiple standard benchmarks (D4RL, Maze2D, Kitchen) and demonstrating consistent performance gains.\n\n- The analysis is insightful, linking theoretical intuition with empirical behavior and validating the method’s robustness and generality."}, "weaknesses": {"value": "- The novelty is limited, as the contribution primarily targets a known issue (guide-scale sensitivity) within Classifier-Free Guidance rather than introducing a fundamentally new planning paradigm.\n\n- The method introduces additional parameters, such as βₘₐₓ, which slightly increase algorithmic complexity and may require their own tuning for optimal stability.\n\n- The experimental evaluation relies mainly on D4RL, an older and largely saturated benchmark suite; validation on more realistic or modern robotic planning tasks (e.g., navigation or manipulation with sensory inputs) would strengthen the paper’s practical relevance.\n\n- While the adaptive guidance mechanism is elegant, it remains heuristic—its theoretical guarantees or convergence behavior under diverse diffusion architectures are not thoroughly analyzed."}, "questions": {"value": "- How sensitive is TGDP to the choice of the **βₘₐₓ** parameter? Could the authors provide guidance or heuristics for selecting it, or explore whether it can be learned automatically?  \n- Since TGDP mainly addresses **guide-scale sensitivity** in CFG, how do the authors envision extending this framework to **other conditioning paradigms** or more general decision-making problems?  \n- The adaptive guidance mechanism is based on **collinearity heuristics** — can the authors provide deeper theoretical justification or empirical evidence (e.g., ablation or visualization) showing that this metric consistently distinguishes inter- and intra-modal cases?  \n- Have the authors evaluated TGDP on **modern or realistic robotic benchmarks**, such as continuous manipulation or navigation tasks with raw sensory input? If not, what challenges do they foresee in transferring TGDP to such settings?  \n- The method adds some computational overhead (~1.5× CFG). Could the authors analyze how this scales with **planning horizon** and **environment complexity**, and whether approximate or truncated versions can retain performance?  \n- Can the **temperature-based reweighting** framework be extended to **learned or adaptive temperature schedules**, potentially improving flexibility across nonstationary datasets?  \n- How does TGDP interact with **online fine-tuning or real-time replanning**, where data distributions evolve — does the adaptive mechanism remain stable in such dynamic conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7chCx6qykz", "forum": "LOvutpJlgL", "replyto": "LOvutpJlgL", "signatures": ["ICLR.cc/2026/Conference/Submission24685/Reviewer_1nMT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24685/Reviewer_1nMT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760531943168, "cdate": 1760531943168, "tmdate": 1763004300739, "mdate": 1763004300739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “Practical Diffusion Planning via Temperature-Guided Reward Conditioning (TGDP)” introduces a diffusion-based planning method that removes the need for task-specific tuning in reward-conditioned decision-making. Building on Classifier-Free Guidance (CFG), the authors propose a temperature-guided framework where trajectory samples are reweighted during training according to a temperature parameter that biases learning toward higher-return trajectories while preserving data diversity. At inference, TGDP employs an adaptive guidance mechanism that dynamically adjusts the influence of reward conditioning based on the geometric alignment of denoising directions across temperature scales, thereby preventing under- or overguidance. Experiments on standard D4RL locomotion, Maze2D, and Kitchen benchmarks show that TGDP achieves performance on par with or exceeding existing diffusion planners, all while using a single default hyperparameter configuration and maintaining computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well motivated and clearly written, providing a logical flow from problem statement to solution.\n\n- The proposed TGDP method is technically sound and conceptually reasonable, effectively addressing CFG’s sensitivity to guide-scale tuning.\n\n- The method design is elegant and practical, integrating temperature-guided weighting and adaptive guidance without significant computational or architectural overhead.\n\n- Experiments are comprehensive and well executed, covering multiple standard benchmarks (D4RL, Maze2D, Kitchen) and demonstrating consistent performance gains.\n\n- The analysis is insightful, linking theoretical intuition with empirical behavior and validating the method’s robustness and generality."}, "weaknesses": {"value": "- The novelty is limited, as the contribution primarily targets a known issue (guide-scale sensitivity) within Classifier-Free Guidance rather than introducing a fundamentally new planning paradigm.\n\n- The method introduces additional parameters, such as βₘₐₓ, which slightly increase algorithmic complexity and may require their own tuning for optimal stability.\n\n- The experimental evaluation relies mainly on D4RL, an older and largely saturated benchmark suite; validation on more realistic or modern robotic planning tasks (e.g., navigation or manipulation with sensory inputs) would strengthen the paper’s practical relevance.\n\n- While the adaptive guidance mechanism is elegant, it remains heuristic—its theoretical guarantees or convergence behavior under diverse diffusion architectures are not thoroughly analyzed.\n\n- The temperature-based reweighting is conceptually intuitive but still handcrafted; there is no exploration of whether β or weighting could be learned adaptively or optimized end-to-end.\n\n- The paper’s computational trade-offs (≈1.5× CFG cost) are reasonable but may still pose challenges for real-time or large-scale deployment scenarios."}, "questions": {"value": "- How sensitive is TGDP to the choice of the **βₘₐₓ** parameter? Could the authors provide guidance or heuristics for selecting it, or explore whether it can be learned automatically?  \n- Since TGDP mainly addresses **guide-scale sensitivity** in CFG, how do the authors envision extending this framework to **other conditioning paradigms** or more general decision-making problems?  \n- The adaptive guidance mechanism is based on **collinearity heuristics** — can the authors provide deeper theoretical justification or empirical evidence (e.g., ablation or visualization) showing that this metric consistently distinguishes inter- and intra-modal cases?  \n- Have the authors evaluated TGDP on **modern or realistic robotic benchmarks**, such as continuous manipulation or navigation tasks with raw sensory input? If not, what challenges do they foresee in transferring TGDP to such settings?  \n- The method adds some computational overhead (~1.5× CFG). Could the authors analyze how this scales with **planning horizon** and **environment complexity**, and whether approximate or truncated versions can retain performance?  \n- Can the **temperature-based reweighting** framework be extended to **learned or adaptive temperature schedules**, potentially improving flexibility across nonstationary datasets?  \n- How does TGDP interact with **online fine-tuning or real-time replanning**, where data distributions evolve — does the adaptive mechanism remain stable in such dynamic conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7chCx6qykz", "forum": "LOvutpJlgL", "replyto": "LOvutpJlgL", "signatures": ["ICLR.cc/2026/Conference/Submission24685/Reviewer_1nMT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24685/Reviewer_1nMT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760531943168, "cdate": 1760531943168, "tmdate": 1763036060353, "mdate": 1763036060353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author introduces the Temperature-Guided Diffusion Planner (TGDP), an approach that addresses the hyperparameter fragility of Classifier-Free Guidance (CFG). Conventional CFG in diffusion planning often necessitates extensive, task-specific hyperparameter tuning because guidance performance critically depends on adapting to the data manifold and reward distribution. TGDP mitigates this by self-calibrating to these task-specific characteristics through a proposed training and inference scheme. During training, it reweights the diffusion loss for each sample based on its return and a randomly sampled temperature. At inference, TGDP computes an adaptive guidance scale by measuring the geometric collinearity between denoising targets conditioned on high, zero, and low temperatures. The authors provide emprical evidence on D4RL locomotion, Maze2D, and Kitchen benchmarks demonstrating that TGDP consistently matches or surpasses the performance of prior diffusion planners (e.g., CFG, CG, MCSS) while utilizing a single, fixed set of default hyperparameters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a highly practical and impactful problem, \"hyperparameter brittleness of CFG\".\n2. The motivation is highly intuitive and reasonbale.\n3. The suggested method is simple and novel to my understanding."}, "weaknesses": {"value": "1. The method replace the tuning of CFG's guidance scale and target with its own hyperparameters. However, the maximum temperature is also a critical hyperparameter that defines the training objective and guidance targets at inference. The paper uses a fixed value but does not provide an anylsis of how this value was chosen or how sensitive the model's performance is to it. \n2. The adaptive scaling relies on the assumption that the collinearity of diffusion targets is a reliable proxy for distinguishing between intra-mode and inter-mode guidance. While this appears to hold true for the tested D4RL benchmarks, these environments, while standard, may not cover all possible return landscape complexities. The paper would be strengthened by a brief discussion on potential failure modes or types of data distributions where this geometric heuristic might be less effective.\n3. Although this pepr focused on improving CFG, I suggest the author include more related works based on CG that tackles inaccurate guidance [1-3].\n\n[1] Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning, 2023\n\n[2] Inference-Time Policy Steering through Human Interactions, 2025\n\n[3] Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning, 2025\n\nComment: I believe this work has promise. While I currently recommend borderline reject, I am willing to raise my score if the authors adequately address my concerns through revisions or clarifications."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DuIX161Niz", "forum": "LOvutpJlgL", "replyto": "LOvutpJlgL", "signatures": ["ICLR.cc/2026/Conference/Submission24685/Reviewer_McDw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24685/Reviewer_McDw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760779207085, "cdate": 1760779207085, "tmdate": 1762943163091, "mdate": 1762943163091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author introduces the Temperature-Guided Diffusion Planner (TGDP), an approach that addresses the hyperparameter fragility of Classifier-Free Guidance (CFG). Conventional CFG in diffusion planning often necessitates extensive, task-specific hyperparameter tuning because guidance performance critically depends on adapting to the data manifold and reward distribution. TGDP mitigates this by self-calibrating to these task-specific characteristics through a proposed training and inference scheme. During training, it reweights the diffusion loss for each sample based on its return and a randomly sampled temperature. At inference, TGDP computes an adaptive guidance scale by measuring the geometric collinearity between denoising targets conditioned on high, zero, and low temperatures. The authors provide emprical evidence on D4RL locomotion, Maze2D, and Kitchen benchmarks demonstrating that TGDP consistently matches or surpasses the performance of prior diffusion planners (e.g., CFG, CG, MCSS) while utilizing a single, fixed set of default hyperparameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a highly practical and impactful problem, \"hyperparameter brittleness of CFG\".\n2. The motivation is highly intuitive and reasonbale.\n3. The suggested method is simple and novel to my understanding."}, "weaknesses": {"value": "1. The method replace the tuning of CFG's guidance scale and target with its own hyperparameters. However, the maximum temperature is also a critical hyperparameter that defines the training objective and guidance targets at inference. The paper uses a fixed value but does not provide an anylsis of how this value was chosen or how sensitive the model's performance is to it. \n2. The adaptive scaling relies on the assumption that the collinearity of diffusion targets is a reliable proxy for distinguishing between intra-mode and inter-mode guidance. While this appears to hold true for the tested D4RL benchmarks, these environments, while standard, may not cover all possible return landscape complexities. The paper would be strengthened by a brief discussion on potential failure modes or types of data distributions where this geometric heuristic might be less effective.\n3. Although this pepr focused on improving CFG, I suggest the author include more related works based on CG that tackles inaccurate guidance [1-3].\n\n[1] Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning, 2023\n\n[2] Inference-Time Policy Steering through Human Interactions, 2025\n\n[3] Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning, 2025\n\nComment: I believe this work has promise. While I currently recommend borderline reject, I am willing to raise my score if the authors adequately address my concerns through revisions or clarifications."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DuIX161Niz", "forum": "LOvutpJlgL", "replyto": "LOvutpJlgL", "signatures": ["ICLR.cc/2026/Conference/Submission24685/Reviewer_McDw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24685/Reviewer_McDw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760779207085, "cdate": 1760779207085, "tmdate": 1763690825529, "mdate": 1763690825529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Temperature-Guided Diffusion Planning (TGDP), a practical and scalable framework for reward-conditioned diffusion planning. TGDP improves upon Classifier-Free Guidance (CFG), which is widely used in diffusion models but suffers from fragile, task-specific hyper-parameter tuning."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This work presents a Practical Diffusion Planning pipeline after DV [1] in the field of diffusion planning. I think this paper will be another attempt on improving diffusion planning, providing inspiration for robotics (especially for the manipulation field), and also offline RL community. The motivation behind this paper is solid: CFG/CG is fast but require heavy hyper-parameter tuning for target return and CFG scale. MCSS (DV), requiring no parameters tuning, provides high-quality inference, but is slow for generating since it requires more unbiased data for selection. This paper provide experiments on the classic D4RL dataset, which should be good for other researcher to have a try.\n\n[1] What Makes a Good Diffusion Planner for Decision Making? Lu et.al ICLR 2025"}, "weaknesses": {"value": "I think the main weakness is the paper writing. Till now I still not very sure what is actually temperature conditioned diffusion planning. So if the author can provide a detailed answer to my question, I am considering raising my score. I will be staying online during the rebuttal period. I hope the authors can provide active response during rebuttal.\n\n1) What is $\\beta_{max}$ for? Is that a task-specific parameter? Why do we need $\\beta_{max}$? Is $\\beta_{max}$ also a condition that is required to feed into the network of diffusion?\n2) What is the intuition behind designing eq (5)? Could the author elaborating more on the introduction of **cosine-similarity** between the 0/high/low-temperature diffusion output? Any math / intuition supported is quite encouraged. My understanding: \"if predictions come from different modes → reduce scale (avoid over-guidance) If within same mode → strengthen guidance (avoid underguidance)\" but this makes no sense to me, I can also say \"if predictions come from different modes → you should improve scale (to quickly passing the confusion state).\" I am hoping to get more insights here.\n3) For Algorithm 1 line 1322, there's $\\tau_{low}$, but it is never used below? This is quite confusing. Is that a typo?\n4) Can the authors provide more experiments on other datasets, except for the 9 classic dataset in D4RL?\n5) I do not find any codes in OpenReview or from the paper."}, "questions": {"value": "If CFG can be represented as:\n\n `sample = D(zero) + s * (D(target_return) - D(zero))`, \n\nwhere target_return requires to be tuned.\n\nCan TGDP be summarized in one sentence, as:\n\n`sample = D(eps, zero) + s_adaptive * (D(eps, $\\beta_{max}$) - D(eps, zero))`, \n\nwhere s_adaptive is large when D(eps, $\\beta_{max}$) is similar to D(eps, -$\\beta_{max}$), otherwise it is small?\n\nIs my understanding is correct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "y8eRp7e70j", "forum": "LOvutpJlgL", "replyto": "LOvutpJlgL", "signatures": ["ICLR.cc/2026/Conference/Submission24685/Reviewer_Payn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24685/Reviewer_Payn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761377973883, "cdate": 1761377973883, "tmdate": 1762943162866, "mdate": 1762943162866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Temperature-Guided Diffusion Planner (TGDP) to address the limitations of traditional reward guidance based on classifier-free guidance (CFG), which is often inflexible and not scalable due to its high sensitivity to hyperparameters. In this work, the authors incorporate a temperature parameter as an auxiliary input to the diffusion model. During training, the diffusion model is optimized with temperature-weighted sampling to capture reward-aware distributions, and during inference, the temperature is adaptively utilized to control the guidance strength in the sampling process. Experimental results demonstrate that TGDP achieves significant improvements over conventional diffusion-based planners."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel idea of adaptively adjusting the level of reward guidance, which is both meaningful and practical for diffusion-based planning.\n\n2. The authors provide comprehensive theoretical analysis on the application of temperature in both training and guidance, presented in the appendix.\n\n3. Extensive experimental results are included to validate the effectiveness of the proposed method, along with detailed implementation information."}, "weaknesses": {"value": "1. The method section is somewhat disorganized, as parts of it include content that would be more appropriate for the experimental section (e.g. detailed descriptions of the D4RL implementation). This mixing of methodological explanation and implementation details makes it difficult to follow the core ideas of the proposed approach.\n\n2. In addition, there are numerous formatting issues throughout the paper, including unusually large spacing between text, figures, and tables. Several figure legends overlap with the plots themselves, obscuring key details and making it hard to interpret the results clearly."}, "questions": {"value": "1. How can the authors ensure that trajectory returns are effectively encoded through temperature-weighted training? Would this implicit formulation reduce scalability or generalizability compared to traditional reward-guided inference methods, especially when the reward function changes across tasks?\n\n2. The paper states that traditional reward guidance is sensitive to the hyperparameter controlling guidance strength. However, could the maximum temperature $\\beta_\\text{max}$ itself become an important hyperparameter that significantly affects the performance of TGDP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EUddvEn4Q6", "forum": "LOvutpJlgL", "replyto": "LOvutpJlgL", "signatures": ["ICLR.cc/2026/Conference/Submission24685/Reviewer_grLj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24685/Reviewer_grLj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979071499, "cdate": 1761979071499, "tmdate": 1762943162580, "mdate": 1762943162580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "**Anonymized Code**:\\\nhttps://github.com/iclr26-submission24685/tgdp_submission.git\n\n**Changes**:\n- Revision of the methodology section, including an explanatory figure, a clearer structure, and added cross-references to corresponding parts in the Appendix.\n- Added new section \"Limitations and Future Work\", discussing computational overhead and generalization of the method.\n- Added discussion of the $\\beta_{max}$ parameter in the \"Experiments and Results\" section.\n- Improved the formatting of several figures for better visibility.\n- Expanded the SotA, discussing recent approaches to avoid manifold deviation in diffusion planning.\n- We now sample the temperature from a continuous range [$-\\beta_{max}, \\beta_{max}$] rather than a discrete set {$-\\beta_{max},0,\\beta_{max}$}. To stabilize this we adapt the dropout probability used in return-conditioned CFG.  We have adapted the text and algorithm accordingly and reran all experiments.  As the continuous temperature distribution yields more stable training, the results are marginally improved. The most significant improvement can be seen in the results of TGDP(-V) on the kitchen-partial dataset. We attribute the stabler training to an effective increase in training sample diversity.\n- Minor changes in wording for better readability.\n- Increased level of detail in Appendix C."}}, "id": "f3utMixFZZ", "forum": "LOvutpJlgL", "replyto": "LOvutpJlgL", "signatures": ["ICLR.cc/2026/Conference/Submission24685/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24685/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission24685/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763672263699, "cdate": 1763672263699, "tmdate": 1763672263699, "mdate": 1763672263699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}