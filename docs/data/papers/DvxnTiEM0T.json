{"id": "DvxnTiEM0T", "number": 5111, "cdate": 1757849863483, "mdate": 1759897993972, "content": {"title": "Adaptive Punishment for Cooperation in Mixed-Motive Games", "abstract": "Mixed-motive scenarios are ubiquitous in real-world multi-agent interactions, where self-interested agents often defect for immediate rewards, overlooking the potential of altruistic cooperation to improve long-term gains and collective welfare. Peer punishment can deter defection, but as costly second-order altruism, its persistent imposition may undermine the punisher’s interests. Existing approaches often struggle to effectively implement punishment to promote cooperation. To balance the efficacy and cost of punishment, we propose Adaptive Punishment for Cooperation (APC), a distributed method that determines punishment intensity based on both a dynamic punishment probability and the severity of defection. This dynamic probability substantially reduces costly and ineffective punishment while also promotes cooperation. To accurately assess defection and its severity, we use a defection awareness module, whose learning is guided by game reward. Theoretical analysis and empirical results show APC performs effectively in iterated public goods game. Empirically, APC also significantly outperforms existing baselines across sequential social dilemmas, learning rational and effective punishment policies that foster cooperation by strategically deterring defection.", "tldr": "", "keywords": ["decentralized multi-agent system", "reinforcement learning", "opponent modeling"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eae8e59c17a749e2df397ccaa90638b2e230e4d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a distributed multi-agent reinforcement learning framework called Adaptive Punishment for Cooperation (APC), which aims to promote cooperation among self-interested agents in mixed-motive environments. The approach integrates two components: a defection awareness module that learns to recognize and quantify the severity of opponents’ defections based on observed behavior and rewards, and an adaptive punishment mechanism that dynamically adjusts both the probability and intensity of sanctions according to their past effectiveness. This enables agents to punish selectively—strongly when defections are harmful, and lightly or not at all when punishment fails to reduce defection—balancing the cost of enforcement with its benefits. Theoretical analysis and experiments on multiple benchmark environments, including Iterated Public Goods Game and several sequential social dilemmas, demonstrate that APC achieves high levels of cooperation, fairness, and robustness, outperforming standard reinforcement learning baselines and fixed-rule punishment schemes. The work provides new insight into how adaptive, context-aware sanctioning can stabilize cooperation in decentralized multi-agent systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes the first decentralized adaptive punishment mechanism that simultaneously adjusts both probability and intensity of punishment according to defection severity and historical effectiveness. Also, it introduces a Defection Predictor Network (DPN) — a learned defection-awareness model that quantifies opponent defection rather than using rule-based detection, marking a significant step beyond static or heuristic punishment systems in MARL. Therefore, it bridges social norm enforcement and multi-agent reinforcement learning through an explicitly learnable and theoretically grounded mechanism.\n2. This paper provides a mathematical formulation of adaptive punishment, including explicit update rules for punishment probability $p_{ij,t}$​ and intensity $w_{ij,t}$​. The theoretical analysis clarifies conditions under which cooperation becomes a rational equilibrium (e.g., when penalty $\\delta > 0.1$), linking empirical results to formal reasoning. Carefully distinguishing between defection severity detection (via DPN) and punishment decision-making (via adaptive probability), shows strong internal logic and interpretability.\n3. The proposed approach is evaluated across four distinct social dilemma environments — Iterated Public Goods Game (IPGG), Coingame, Sequential Snowdrift Game (SSG), and Sequential Stag-Hunt (SSH)—covering both classical and spatiotemporal settings. The experimental result demonstrates substantial improvements in cooperation rate, fairness, and robustness compared to IA2C, IPPO, and RL-Punish baselines.\n4. The proposed cost-efficient mechanism could have some realistic implication: reduces unnecessary punishment and stabilizes cooperation without requiring a central authority or added reward resources.\n5. This paper is generally well-structured exposition follows the logic: conceptual motivation → formalism → algorithm → theoretical and empirical validation. Visualizations (Figures 3–7) effectively illustrate performance, fairness, and sensitivity trends."}, "weaknesses": {"value": "1. All four tested environments (IPGG, Coingame, Sequential Snowdrift, Sequential Stag-Hunt) are toy or small-scale social dilemmas with simple discrete actions.\n2. No analysis is provided on whether DPNs converge to stable opponent models or oscillate due to non-stationarity. This raises concerns about reproducibility in other scenarios.\n3. The theoretical justification (e.g., δ > 0.1 ⇒ cooperation rational) is derived under idealized assumptions that don’t hold in stochastic sequential games. The analysis is more illustrative than rigorous, limiting theoretical strength.\n4. Baselines (IA2C, IPPO, RL-Punish) are relatively weak. Other relevant baselines required to highlight the performance and significance of the proposed approach, e.g., LOLA [1], D3C [2] and Social Influence [3].\n\n[1] Foerster, J., Chen, R. Y., Al-Shedivat, M., Whiteson, S., Abbeel, P., & Mordatch, I. (2018). _Learning with Opponent-Learning Awareness (LOLA)._ In Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems (AAMAS).\n\n[2] Gemp, I., McKee, K. R., Everett, R., Duéñez-Guzmán, E., Bachrach, Y., Balduzzi, D., & Tacchetti, A. (2022). _D3C: Reducing the Price of Anarchy in Multi-Agent Learning._ In Proceedings of AAMAS 2022.\n\n[3] Jaques, N., Lazaridou, A., Hughes, E., Gulcehre, Ç., Ortega, P. A., Strouse, D. J., Leibo, J. Z., & de Freitas, N. (2019). _Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning._ In Proceedings of the 36th International Conference on Machine Learning (ICML)."}, "questions": {"value": "Please address the weaknesses and answer the following technical questions:\n1. The punishment probability $p_{ij,t}$ is adjusted using defection frequency over rolling windows via Eq. (3). Is there any theoretical guarantee that this stochastic update rule converges to a stable equilibrium (e.g., a fixed point or bounded oscillation)? How sensitive is it to window length L and threshold ε ?\n2. The DPN learns to predict defection severity via a proxy objective involving $−r_i(t)$ and entropy regularization. Since this optimization does not use ground-truth defection labels, how do the authors ensure that the predictor does not conflate noise or random fluctuations in rewards with genuine defection behaviors? Can the learned DPN generalize across different opponents or environments with altered reward structures?\n3. Fairness is evaluated using the equality metric $E = 1 − (\\sum_i \\sum_j |R_i − R_j|) / (2N \\sum_i R_i)$. Is this metric appropriate when negative rewards or punishments dominate? How does it behave when collective reward is close to zero (denominator instability)?\n4. The paper provides an analytical result that under APC, expected defection reward is −4δ and cooperation reward is −0.4. How were these values derived, and do they hold generally across all parameter ranges or only under specific assumptions of IPGG? Is this analysis extendable to multi-step, non-tabular SSDs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ExdnJuJzZI", "forum": "DvxnTiEM0T", "replyto": "DvxnTiEM0T", "signatures": ["ICLR.cc/2026/Conference/Submission5111/Reviewer_duR2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5111/Reviewer_duR2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761230287358, "cdate": 1761230287358, "tmdate": 1762917888414, "mdate": 1762917888414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of sustaining cooperation in mixed-motive multi-agent environments, where agents must balance individual interests with collective welfare. The authors propose Adaptive Punishment for Cooperation (APC), a distributed framework that dynamically adjusts punishment intensity and frequency to discourage defection while minimizing its associated costs. APC comprises two key components: a Defection Awareness Module, which detects and quantifies agents’ defection behaviors, and an Adaptive Punishment Module, which adaptively regulates punishment based on the extent and reduction of defections. Extensive experiments on four mixed-motive games demonstrate the effectiveness and robustness of APC in promoting stable cooperation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written, clearly structured, and easy to follow.\n2. The insight into the necessity of punishment for defection is both novel and valuable.\n3. The design of the Defection Awareness Module and the Adaptive Punishment Module is sound, and the definition of defection is particularly insightful. The authors also provide a thorough and thoughtful discussion on the methods.\n4. The paper provides sufficient experimental details, which enhances the credibility and reproducibility of the results."}, "weaknesses": {"value": "1. The experiments lack comparisons with other mixed-motive approaches, and the environments used are relatively simplified. Additional experiments in more complex settings with heterogeneous agents would make the results more convincing.\n2. Although the authors provide qualitative discussions and empirical evidence, the method lacks rigorous theoretical analysis regarding the convergence properties and long-term stability of the adaptive punishment dynamics."}, "questions": {"value": "1. The position of the legends in the figures should be adjusted, as they currently obscure parts of the plot lines.\n2.The ablation result of APC w/o APr is somewhat unclear. Based on my understanding, since the adaptive punishment mechanism is removed, the punishment frequency should remain stable, as shown in the figure 5. However, the relationship between punishment frequency and learning performance is not well illustrated, which raises some concern — particularly because, in Figures 3 and 9, the performances of APC w/o APr and APC appear very close.\n3. The caption of Appendix C should possibly be “Experiments” rather than “Environments.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vUJ8pKb6xP", "forum": "DvxnTiEM0T", "replyto": "DvxnTiEM0T", "signatures": ["ICLR.cc/2026/Conference/Submission5111/Reviewer_qEjW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5111/Reviewer_qEjW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761464709939, "cdate": 1761464709939, "tmdate": 1762917885043, "mdate": 1762917885043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adaptive Punishment for Cooperation (APC), a distributed method that determines punishment intensity based on both a dynamic punishment probability and the severity of defection. This dynamic probability substantially reduces costly and ineffective punishment while also promotes cooperation. To accurately assess defection and its severity, The authors use a defection awareness module, whose learning is guided by game reward. Theoretical analysis and empirical results show APC performs effectively in iterated public goods game. Empirically, APC also significantly outperforms existing baselines across sequential social dilemmas, learning rational and effective punishment policies that foster cooperation by strategically deterring defection."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important problem of sustaining cooperation in mixed-motive games by integrating punishment mechanisms from evolutionary game theory into MARL. The proposed APC is conceptually sound and technically interesting, combining defection awareness and adaptive punishment. Experiments on several SSD benchmarks show clear performance gains over baselines, and ablation studies support the method’s design. The paper is generally clear and well organized."}, "weaknesses": {"value": "The paper lacks formal discussion on convergence and stability, and how APC addresses the second-order free-rider problem. Baseline descriptions are insufficient, making it hard to isolate APC’s contribution. Experiments are small-scale, and related work on punishment is underdeveloped. Adding pseudocode or clearer algorithm steps would improve readability."}, "questions": {"value": "The paper makes a potentially valuable contribution to addressing the challenge of sustaining cooperation in mixed-motive games by bridging punishment mechanisms rooted in evolutionary game theory with MARL frameworks. However, the manuscript requires several substantive revisions before it can reach its full potential. Should the authors address the issues raised in this review, I would be willing to raise my score.\n1.\tThe proposed Adaptive Punishment for Cooperation (APC) mechanism is motivated by the idea that “punishment can promote cooperation.” However, the current manuscript lacks a formal dynamical analysis to support this claim. In the Main Results section, the authors primarily present simulation results, while providing little explanation of the underlying mechanisms driving the effectiveness of APC. It is strongly recommended that the authors further elaborate, from an evolutionary dynamics perspective, how APC theoretically mitigates or avoids the second-order free-rider problem.\n2.\tWhile the introduction of a punishment mechanism is indeed the key innovation of this work, the Related Work section does not adequately cover the existing literature. The review of previous punishment mechanisms is limited in both scope and depth; conversely, the discussion on reward mechanisms is relatively lengthy but not directly aligned with the paper’s core contribution. The authors are advised to streamline the discussion on reward mechanisms and expand the review of relevant punishment mechanisms to make this section more focused and informative.\n3.\tThe paper compares the baseline results of APC with those of IA2C, IPPO, and RL Punish. However, the specific settings and distinctions among these baselines are not clearly described. As I understand it, IA2C and IPPO do not incorporate any punishment mechanisms, while RL Punish relies on a fixed punishment mechanism rather than an adaptive one. The authors should explicitly clarify the punishment settings of each baseline and discuss whether introducing an adaptive punishment scheme into IPPO would yield similar performance gains.\n4.\tAlthough APC shows promising theoretical results in promoting cooperation, the theoretical analysis of convergence and system stability is currently insufficient. The manuscript does not address whether the dynamic update process is guaranteed to converge to a stable cooperative equilibrium in the long run, or whether oscillations and divergence may occur. It is recommended that the authors provide formal theoretical support, such as fixed-point analysis, Lyapunov stability analysis or stochastic approximation theory, to specify the conditions for convergence.\n5.\tThe authors are also encouraged to evaluate APC in larger-scale social systems (e.g., with N > 10 agents) to assess its scalability and stability in more complex collective environments.\n6.\tThe Method section would benefit significantly from including a pseudocode representation of the APC training and execution procedure. Although the textual description is detailed, the current structure is somewhat intricate; a structured algorithmic framework would make the methodology clearer, more concise, and easier to reproduce."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iVu72CVCAn", "forum": "DvxnTiEM0T", "replyto": "DvxnTiEM0T", "signatures": ["ICLR.cc/2026/Conference/Submission5111/Reviewer_MTXx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5111/Reviewer_MTXx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761696517920, "cdate": 1761696517920, "tmdate": 1762917881596, "mdate": 1762917881596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In multi-agent mixed-motive RL scenarios, the paper proposes to train a network to judge if other agents are defecting (defined based on whether the actions caused damage to other agents' rewards), and places punishment on such actions. Results show in 4 grid-based tasks, the method increases collaborative rewards."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Results show that the proposed method increases collective reward in mix-motive scenarios.\n1. Interesting and reasonable ablation study on the effect of hyperparameters.\n1. The definition of defection gets rid of access to other agent's internal information (such as their rewards, intentions), which makes sense in real-life domains where such information might be private."}, "weaknesses": {"value": "1. The definition of defection, \"when the probability of ajt exceeds the mean, it indicates that agent jtends to favor actions unfavorable to the focal agent i’s payoff\", seems a bit arbitrary, and might over-punish any action to rationally optimize self rewards.  To answer this question, can you analyze mathematically how this definition rigorously distinguishes various mixed-motive cases, such as \"harm others to benefit oneself\" and \"benefit oneself while also benefiting others, but at a smaller scale compared to the benefit to oneself\"? Similarly, how it treats \"harm others and harm oneselves as well, in cases where a harm is inevitable\"? Or alternatively, do you have a mathematical description of what this definition equivalently captures so we can look at the expression and understand its mechanism, instead of the intuitive and subjective descriptions in lines 131-137?\n1. The evaluation is only compared to 3 baselines by 2017. I wonder why various methods introduced in the related work section are not compared to. In specific, the authors mention \"However, experimental results, shown as in Figure 3, have shown that merely relying on such punishment action combined with standard Independent MARL methods often fails to promote cooperation in SSDs.\". But these methods are not directly compared to, which makes the statement in lack of support."}, "questions": {"value": "1. grammar: line 131: \"µi outputs predict probability distribution ...\"\n1. It would be clearer to compare the various definitions of defections in related work or preliminaries, and how you chose yours. This is a core concept in the motivation, but only explained subjectively.\n1. Any experimental validation on how accurately the defection awareness network correlates with other agents' defection intentions? This also helps clarify my previous questions on what the network really measures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w483OEQm1M", "forum": "DvxnTiEM0T", "replyto": "DvxnTiEM0T", "signatures": ["ICLR.cc/2026/Conference/Submission5111/Reviewer_t21e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5111/Reviewer_t21e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020278274, "cdate": 1762020278274, "tmdate": 1762917881297, "mdate": 1762917881297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}