{"id": "oWagByDNPc", "number": 14351, "cdate": 1758233380077, "mdate": 1763609356338, "content": {"title": "MIRA: Memory-Integrated Reinforcement Learning Agent  with Limited LLM Guidance", "abstract": "Reinforcement learning (RL) agents often face high sample complexity in sparse or delayed reward settings, due to limited prior knowledge.\nConversely, large language models (LLMs) can provide subgoal structures, plausible trajectories, and abstract priors that support early learning.\nYet heavy reliance on LLMs introduces scalability issues and risks dependence on unreliable signals, motivating ongoing efforts to integrate LLM guidance without compromising RL’s autonomy.\nWe propose MIRA (\\underline{M}emory-\\underline{I}ntegrated \\underline{R}einforcement Learning \\underline{A}gent), which augments learning with a structured and evolving \\textit{memory graph}. \nThis graph stores decision-relevant information, such as trajectory segments and subgoal decompositions, and is co-constructed from the agent’s high-return experiences and LLM outputs.\nFrom this structure, we derive a \\textit{utility} signal that integrates with advantage estimation to refine policy updates without overriding the reward signal. By incorporating LLM-derived priors in memory rather than relying on continuous queries, MIRA reduces dependence on real-time supervision.\nAs training progresses, the agent’s policy outgrows the initial LLM-derived priors, and the utility term decays, leaving long-term convergence guarantees intact.\nWe establish theoretical guarantees that this utility-based shaping improves early-stage learning in sparse reward settings.\nEmpirically, MIRA outperforms RL baselines and achieves final returns comparable to approaches that depend on frequent LLM supervision, while requiring substantially fewer online LLM queries.", "tldr": "MIRA integrates LLM guidance into RL through a memory graph and utility-shaped advantages, achieving faster learning with far fewer queries.", "keywords": ["Reinforcement learning (RL)", "Large language models (LLMs)", "Memory Graph", "LLM-derived priors", "Sample Efficiency", "Sparse-Reward Environments"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0e29540ace74032f3c8167caa11407fcd385bd66.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces MIRA, a policy gradient method that adds a memory graph built from high-return rollouts and occasional LLM outputs. The memory induces a per-step utility signal that is added to the advantage to form a shaped advantage. The shaping weight decays while the standard advantage weight rises, so the influence of the utility fades during training. The claim is that this improves early learning in sparse reward tasks without hurting final convergence, and reduces the number of online LLM calls relative to teacher or reward shaping approaches. The paper evaluates on FrozenLake and several MiniGrid and BabyAI tasks, compares with PPO and two LLM-based baselines, and presents a PPO-style improvement bound under boundedness and scheduling assumptions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The shaped advantage $\\tilde{A}_t=\\eta_tA_t + \\zeta_tU_t$  drops in with no change to policy or critic structure. The paper repeatedly stresses that the utility is additive and scheduled to zero, which keeps the core PPO update intact. This design choice makes the method easy to implement and likely to work across actor-critic variants. \n\n- On several MiniGrid and BabyAI tasks, the method reaches higher success faster than PPO and a hierarchical baseline, and does so with modest LLM usage. The narrative connects each gain to either offline memory, occasional online advice, or both."}, "weaknesses": {"value": "- The paper describes utility as a similarity-weighted score over stored trajectory segments that also uses a goal alignment factor and LLM confidence, and a predicted reward for the memory node. However, the exact form of the similarity function and the cost of matching are not specified in the main text. Without a precise definition, it is hard to reason about bias and computational overhead, and to reproduce the effect. \n\n- The improvement relation includes a term with a utility bonus minus a uniform cap. If the utility is badly calibrated early, the cap term may dominate and make the bound weak. The paper does not give conditions under which the utility contribution is reliably positive beyond the boundedness itself.\n\n-The screening unit filters online suggestions using sequence likelihoods or agreement across samples. In many LLM APIs, calibrated token log probabilities are limited or absent, and agreement can be brittle. The paper does not specify thresholds, how they are tuned, nor show sensitivity. Since confidence and a predicted reward weigh the utility, miscalibration can bias the shaped advantage.\n\n-For accepted online guidance, the method injects penalties into logits to suppress actions. The bounds on the penalty and its interaction with PPO clipping are not made precise in the main text. This matters for stability, since even bounded penalties can alter action selection in a way that conflicts with the critic and with the clipping rule if the scaling is not set with care.\n\n- Using offline priors with full environment context can give extra information not available to the agent or to baselines. This can inflate gains attributed to MIRA rather than to the prior. The paper should make this explicit, and either restrict priors or give fair matched baselines. The FrozenLake description makes this a real concern."}, "questions": {"value": "- Can the authors discuss what global information the offline LLM can see and whether baselines are allowed the same view. Provide runs where offline priors are restricted to the same partial view as the agent to show robustness. The FrozenLake discussion already notes that slipperiness is hidden from both, but the global grid is seen by the LLM. Please add a matched setting. \n\n- It is better to state the exact thresholds, number of samples in the agreement test, and the mapping from token log probabilities to confidence. Include a sensitivity study. If token log probabilities are not available, explain the substitute and its effect.\n\n- Can the aughoors give full formulas for the similarity function, the goal alignment term, the confidence mapping, and the way predicted reward and confidence combine? Report the run time cost of matching and the memory size growth as a function of steps.\n\n-Since the method relies on screening and logit penalties, the lack of precise settings may hide brittle behavior or conflicts with PPO clipping. This is a correctness risk rather than a style only, because poor settings can cause learning to diverge."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "47h7B1OJCi", "forum": "oWagByDNPc", "replyto": "oWagByDNPc", "signatures": ["ICLR.cc/2026/Conference/Submission14351/Reviewer_VZaE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14351/Reviewer_VZaE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868400813, "cdate": 1761868400813, "tmdate": 1762924773549, "mdate": 1762924773549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MIRA, a reinforcement learning agent that integrates LLM-generated subgoals into a dynamic memory graph to accelerate learning in sparse-reward environments. By computing utility signals from this structured memory to guide early training while gradually reducing LLM dependence, MIRA achieves superior sample efficiency and matches LLM-teacher performance with significantly fewer queries, supported by theoretical convergence guarantees and empirical validation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong methodological integration: MIRA's use of a structured memory graph, which is co-populated by both agent experience and LLM-derived subgoal decompositions (see Section 2.1), is a compelling hybridization of model-based memory with language-derived task priors. This approach is well-justified, especially for environments where exploration is bottlenecked by sparse feedback.\n\n2. Empirical rigor and benchmark variety: MIRA is evaluated in breadth across Gymnasium ToyText, MiniGrid, and BabyAI environments, occupying both tabular and partially observable/visual input regimes (see Section 3.1, Figure 2). The baselines (PPO, hierarchical RL, LLM-based reward shaping, and teacher models) are appropriate and state-of-the-art.\n\n3. Efficient and transparent ablation studies: The experiments systematically examine online vs. offline LLM guidance, varying query budgets, effects of unreliable LLM outputs, and different LLM models (Figure 6), elucidating the value and robustness of the proposed approach."}, "weaknesses": {"value": "1. Insufficient Detail on Memory Graph Mechanics: While the memory graph is central to MIRA's design, the main text lacks operational clarity on key aspects—such as criteria for adding or pruning subgoal nodes, triggers for new LLM queries, and mechanisms for resolving conflicts between LLM suggestions and agent experience in dynamic environments. Critical implementation specifics are deferred to the appendix, and Figure 1 (the purported schematic of the graph) is absent from the main paper, impeding reproducibility and obscuring the precise novelty of the graph construction process.\n\n2. Narrow Empirical Scope: All experiments are confined to low-dimensional, grid-world environments. The absence of evaluations in high-dimensional, continuous, or real-world settings (e.g., robotics, vision-based control, or multimodal tasks) limits confidence in the method’s claimed generality, despite assertions of broad applicability and memory efficiency."}, "questions": {"value": "1. Handling Conflicting or Erroneous LLM Guidance in Memory. Could the authors clarify the exact mechanism for dynamic graph updates—specifically, how conflicting agent experience and LLM-derived subgoals/trajectory recommendations are resolved in the presence of incorrect LLM priors? What is the recourse if LLM hallucinations are initially \"locked in\" to memory?\n\n2. Scalability Beyond Grid Worlds. All experiments are conducted in discrete, grid-based environments. Have the authors attempted to apply MIRA to more complex domains—such as continuous control, vision-based robotic tasks, or high-dimensional state spaces? If not, what are the anticipated bottlenecks (e.g., graph scalability, LLM prompting overhead, or similarity computation in pixel space)? Addressing this would clarify the method's potential for real-world deployment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kLlMpeQ7Lc", "forum": "oWagByDNPc", "replyto": "oWagByDNPc", "signatures": ["ICLR.cc/2026/Conference/Submission14351/Reviewer_9USL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14351/Reviewer_9USL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903241434, "cdate": 1761903241434, "tmdate": 1762924772772, "mdate": 1762924772772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose MIRA (Memory-Integrated Reinforcement Learning Agent), a novel framework that integrates LLM guidance into RL using adaptive advantage shaping along with a structured, evolving memory graph. Empirically, MIRA outperforms standard RL and hierarchical RL baselines on a suite of sparse-reward tasks (MiniGrid, BabyAI). It achieves final performance comparable to heavily-supervised, query-intensive LLM-RL methods while requiring substantially fewer LLM queries."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "[S1] The core idea of ​​“adaptive advantage shaping” is well-motivated and has a solid theoretical grouding. Compared with heavy supervision using LLM (e.g., modifying rewards, which will change the structure of the MDP and affect asymptotic convergence), advantage shaping allows for initial training under LLM guidance and gradual evolution without being limited by the imperfections of the initial LLM guidance."}, "weaknesses": {"value": "[W1] The adaptive advantage shaping mechanism requires annealing to zero to guarantee asymptotic convergence. This makes MIRA only effective for the initial learning phase, such as accelerating exploration. It cannot sustain an advantage using imperfect LLM signals.\n\nBesides, the annealing strategy introduces new hyperparameters ($\\eta_t$ and $\\xi_t$), which are crucial to performance."}, "questions": {"value": "[Q1] How about the performance on longer runs? Will HRL gradually surpass MIRA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2zP8MACTVP", "forum": "oWagByDNPc", "replyto": "oWagByDNPc", "signatures": ["ICLR.cc/2026/Conference/Submission14351/Reviewer_tfVi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14351/Reviewer_tfVi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762092206972, "cdate": 1762092206972, "tmdate": 1762924769620, "mdate": 1762924769620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to use LLM's capability of generating trajectory-based plans to guide the online training of an RL agent. Since LLMs are expensive to query, the goal is to query it only a few times while training a performant RL policy. The paper proposes to integrate LLM guidance by maintaining a graph of goals, subgoals, and trajectories, and use such a graph to compute auxiliary utility signals (similar to reward shaping) to accelerate policy learning with PPO. Experiments show that the proposed algorithm can outperform RL without LLM guidance and some other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Using LLM to provide guidance on learning for decision making is a problem that has gained a lot of recent attention..\n\n- The idea of using a graph to represent the current knowledge learned, including subgoals, and final goals is interesting\n\n- Experiments in FrozenLake and MiniGrid are helpful in illustrating the utility of the proposed approach"}, "weaknesses": {"value": "- Despite having good performance in gridworld environments, the paper did not discuss the applicability of the proposed method in other RL environments, e.g. continuous control (Pendulum, Mountain Car), and stochastic transition (e.g. Pacman). I assume that in continuous control, defining subgoals can be a challenge? When one has stochastic transition, then having a trajectory-based plan may not be feasible?\n\nAlso, looking at Figure 8, it looks like significant prompt engineering is needed to allow the LLM to output useful trajectories. \n\n- For the experiments, I see that the proposed method can improve over LLM4Teach in Distracted DoorKey, but it looks like from Table 5 that LLM4Teach slightly outperforms the proposed method. Am I missing something? Is the benefit of MIRA more on the computational side, in that it makes fewer LLM queries than LLM4Teach? A comparison between the query costs between these algorithms seems important.\n\n- (Clarity) it would be nice if the paper can provide a full pseudocode that incorporates screening unit, the maintaining of the memory graph, the calculation of the utility signal. Some parts, e.g. the similarity function s, the \\rho function, and the \\hat{r}_m, and c_m in (2) are not clear to me. \n\n- I am not sure if Theorem 1 reflects the utility of the shaped advantage. Is U_k^bonus - U_max <= 0? Then the improvement rate of the proposed algorithm can be slower than that of the PPO baseline?\n\n- After reading the paper, I don't have a good idea about when to use MIRA(offline) versus MIRA(online). From Figure 14, it looks like MIRA (offline) does quite well despite being simpler. But from Figure 6 it looks like MIRA(online) can significantly improve MIRA(offline). Can the authors comment on this?"}, "questions": {"value": "(See questions above)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hKLcVa8a2G", "forum": "oWagByDNPc", "replyto": "oWagByDNPc", "signatures": ["ICLR.cc/2026/Conference/Submission14351/Reviewer_e51T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14351/Reviewer_e51T"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143749679, "cdate": 1762143749679, "tmdate": 1762924769119, "mdate": 1762924769119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}