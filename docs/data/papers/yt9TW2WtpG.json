{"id": "yt9TW2WtpG", "number": 19741, "cdate": 1758298947583, "mdate": 1759897022129, "content": {"title": "How Catastrophic is Your LLM? Certifying Risk in Conversation", "abstract": "Large Language Models (LLMs) can produce catastrophic responses in conversational settings that pose serious risks to public safety and security. \n    Existing evaluations often fail to fully reveal these vulnerabilities because they rely on fixed attack prompt sequences, lack statistical guarantees, and do not scale to the vast space of multi-turn conversations.  \n    In this work, we propose C$^3$LLM, a novel, principled Certification framework for Catastrophic risks in multi-turn Conversation for LLMs that bounds the probability of an LLM generating catastrophic responses under multi-turn conversation distributions with statistical guarantees. We model multi-turn conversations as probability distributions over query sequences, represented by a Markov process on a query graph whose edges encode semantic similarity to capture realistic conversational flow, and quantify catastrophic risks using confidence intervals. We define several inexpensive and practical distributions—random node, graph path, and adaptive with rejection.\n    Our results demonstrate that these distributions can reveal substantial catastrophic risks in frontier models, with certified lower bounds as high as 70\\% for the worst model, highlighting the urgent need for improved safety training strategies in frontier LLMs.", "tldr": "", "keywords": ["Large Language Models", "Catastrophic risks", "Multi-turn Attack", "Certification", "Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00fec7f929b600011c50ec666d0441e2e0f3705c.pdf", "supplementary_material": "/attachment/40ee7378c2c28cfe648c421bb8b979ee720b227a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new framework named C^3LLM, which is used to quantify and certify the risk of catastrophic responses (such as manufacturing weapons, biological terrorism, etc.) generated by LLMs in multi-turn conversations.\n\nC^3LLM models multi-turn conversations as a probability distribution on a \"query graph\". It provides a confidence interval with statistical guarantees for the probability of catastrophic risks. This method not only reveals the vulnerability of the model in a vast conversation space but also allows for quantitative comparisons of the security of different models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper proposes the first statistical framework (C^3LLM) for certifying catastrophic risks of LLMs in multi-turn dialogues. This changes the previous evaluation paradigm that relied on fixed benchmarks and empirical attacks. \n\n2) The multi-turn dialogue process is abstracted as a Markov process on a \"query graph,\" and three attack strategies (Random Node, Graph Path, Adaptive with Rejection) are designed to well simulate strategies of attackers at different levels."}, "weaknesses": {"value": "1) In Section 3.2, the writing of Graph Distributions is too chaotic. A Markov state transition equation does not require so many symbolic representations; the author should express it from a more specific perspective. Moreover, the meanings of some symbols are not clearly defined, such as \\pi_{V\\S}, \\pi_{N(v)\\S}, and \\pi_(N(v)).\n\n2) The paper used a large number of hyperparameters, and different hyperparameter settings as an evaluation benchmark can affect the objectivity of the evaluation results."}, "questions": {"value": "1) The paper mentioned a large number of hyperparameters, and the reviewers were very curious about how to determine these parameters. What if the final hyperparameters of different LLMs are different when choosing the hyperparameters with the lowest score?\n\n2) Hyperparameter analysis should be more comprehensive, including the analysis of hyperparameters for all evaluated LLMs, and further analysis of how these hyperparameters affect the evaluation scores."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QHO4cJmgop", "forum": "yt9TW2WtpG", "replyto": "yt9TW2WtpG", "signatures": ["ICLR.cc/2026/Conference/Submission19741/Reviewer_7DWM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19741/Reviewer_7DWM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779029766, "cdate": 1761779029766, "tmdate": 1762931574639, "mdate": 1762931574639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces C³LLM, a novel framework to move beyond fixed benchmarks and statistically certify the catastrophic risk of Large Language Models (LLMs) in multi-turn conversations. The authors argue that simply testing a few attack sequences fails to capture the vast space of possible dialogues. Their method models conversations as a Markov process on a \"query graph,\" where nodes are individual queries and edges represent semantic similarity, capturing realistic conversational flow. By sampling query sequences from this graph using various distributions (e.g., random, path-based, or adaptive to model refusals), the framework calculates high-confidence probability bounds (confidence intervals) for catastrophic failures. Applying this to frontier models, they find that some, like Claude-Sonnet-4, are certifiably safer, while others, like DeepSeek-R1 and Mistral-Large, exhibit high certified lower bounds for catastrophic risk, reaching over 70% in some cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's primary strength is its conceptual leap from empirical benchmarking to probabilistic certification.\n2. The finding that some frontier models have certified catastrophic risk lower bounds as high as 70% is a powerful and alarming result that underscores the urgency of the problem."}, "weaknesses": {"value": "The entire certification framework is conditioned on the initial query set $Q$ (100 queries per scenario). While the space of conversations sampled from $Q$ is vast (e.g., $100^5$), the certification is only valid for that specific set $Q$. The paper's \"actor-based\" method for generating $Q$ is a reasonable heuristic, but the sensitivity of the final certified bounds to this initial set is not fully explored. If the initial query set is \"weak\" or biased, the certification might be misleadingly optimistic."}, "questions": {"value": "Clarify my comments in \"Weaknesses section\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Not Applicable"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qQGojuhgop", "forum": "yt9TW2WtpG", "replyto": "yt9TW2WtpG", "signatures": ["ICLR.cc/2026/Conference/Submission19741/Reviewer_pvwt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19741/Reviewer_pvwt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809623261, "cdate": 1761809623261, "tmdate": 1762931573655, "mdate": 1762931573655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes C³LLM, a statistical certification framework to evaluate the catastrophic risk of large language models (LLMs) in multi-turn conversations. It models dialogues as probability distributions over query sequences on a semantic graph and samples conversation paths using Markov processes (e.g., random node, graph path, adaptive rejection). By estimating the probability of harmful responses with Clopper–Pearson confidence intervals, the framework provides statistically guaranteed lower bounds on risk. Experiments on the HarmBench dataset show that conventional fixed-prompt evaluations significantly underestimate LLMs’ true safety risks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors define a distributional framework where conversations are modeled as query sequences sampled from a Markov process on a semantic graph."}, "weaknesses": {"value": "While the paper introduces a statistically principled framework for assessing catastrophic risks in LLMs, its novelty and methodological depth are limited, and several aspects could be strengthened to align better with its stated goals.\n1. Limited novelty in “statistical certification” – The claim of introducing statistical guarantees is overstated. The use of the Clopper–Pearson exact interval for binomial estimation is a standard statistical tool rather than a new certification technique. Prior benchmark studies could easily adopt the same confidence estimation without requiring a new framework.\n2. “Non-fixed” conversation modeling is only partial – Although the paper claims to move beyond “fixed attack sequences,” the sampling still occurs over a predefined, finite query graph. The space of possible prompts remains fixed; only the sequence order is randomized. As a result, the method does not fully capture the semantic or generative variability that naturally occurs in real multi-turn attacks. \n3. Dependence on hand-crafted graphs and thresholds – The semantic query graph relies on manually tuned similarity thresholds (lth,hth) using sentence embeddings. This manual specification introduces bias and limits generalization.\n4. Limited evaluation domains and scale – The experiments cover only two domains (chemical/biological and cybercrime) and sample 50 sequences per setting. This narrow scope limits the statistical validity of the “certified” results, particularly for rare catastrophic behaviors.\n5. Ambiguous notion of “certification” – The term “certification” suggests formal safety guarantees, but the framework only provides empirical confidence intervals based on finite samples."}, "questions": {"value": "1. The paper argues that prior works used “fixed attack sequences,” while C³LLM is non-fixed. However, the sampling still happens within a predefined query graph and does not dynamically generate new prompts.Can the authors clarify how this approach truly differs from fixed-sequence evaluation beyond sampling random orderings?\n2. The term “certification” may suggest formal safety guarantees, yet the current approach provides empirical confidence intervals from finite samples.How do the authors justify this terminology choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PS1zYKS2lf", "forum": "yt9TW2WtpG", "replyto": "yt9TW2WtpG", "signatures": ["ICLR.cc/2026/Conference/Submission19741/Reviewer_e15j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19741/Reviewer_e15j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877742555, "cdate": 1761877742555, "tmdate": 1762931573144, "mdate": 1762931573144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an interesting probabilistic view to quantify the safety risks of large language models (LLMs) in multi-turn scenarios. It models multi-turn conversations as probability distributions over query sequences, thus quantifying catastrophic risks using confidence intervals. This paper defines 3 practical distributions. Their results demonstrate the serious safety risks of frontier LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "+ interesting perspective: the probabilistic modeling of safety risks extends the limited risk evaluation via fixed attack trajectories towards continuous and broader, and formal risk evaluation.\n\n+ The flexibility of their mathematical framework enables the community to extend their evaluation method by defining corresponding sequence distributions."}, "weaknesses": {"value": "The implementation of their methodology may not generalize well. The initial node distribution of their graph depends on a multi-turn attack baseline. This brings up the question of whether their sampled sequences cover the common distribution of malicious users in the real world. I think authors should provide more comprehensive analyses to verify the validity of their results."}, "questions": {"value": "This paper only evaluates the safety of models on a subset of one benchmark. is there more results to provide more evidence of the effectiveness of their method? \n\nsome typos: \nline 156 net -> let"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4Gzqm1MEdP", "forum": "yt9TW2WtpG", "replyto": "yt9TW2WtpG", "signatures": ["ICLR.cc/2026/Conference/Submission19741/Reviewer_7oyG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19741/Reviewer_7oyG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896705204, "cdate": 1761896705204, "tmdate": 1762931572685, "mdate": 1762931572685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}