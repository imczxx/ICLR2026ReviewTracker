{"id": "WZlS3c78Lf", "number": 13948, "cdate": 1758225710813, "mdate": 1759897400909, "content": {"title": "EquiReg: Equivariance Regularized Diffusion for Inverse Problems", "abstract": "Diffusion models represent the state-of-the-art for solving inverse problems such as image restoration tasks. Diffusion-based inverse solvers incorporate a likelihood term to guide prior sampling, generating data consistent with the posterior distribution. However, due to the intractability of the likelihood, most methods rely on isotropic Gaussian approximations, which can push estimates off the data manifold and produce inconsistent, unstable reconstructions. We propose Equivariance Regularized (EquiReg) diffusion, a general plug-and-play framework that improves posterior sampling by penalizing those that deviate from the data manifold. EquiReg formalizes manifold-preferential equivariant functions that exhibit low equivariance error for on-manifold samples and high error for off-manifold ones, thereby guiding sampling toward symmetry-preserving regions of the solution space. We highlight that such functions naturally emerge when training non-equivariant models with augmentation or on data with symmetries. EquiReg is particularly effective under reduced sampling and measurement consistency steps, where many methods suffer severe quality degradation. By regularizing trajectories toward the manifold, EquiReg implicitly accelerates convergence and enables high-quality reconstructions. EquiReg consistently improves performance in linear and nonlinear image restoration tasks and solving partial differential equations.", "tldr": "Propose a equivariance-based regularization to improve diffusion models for solving inverse problems.", "keywords": ["Diffusion models", "inverse problems", "image restorations", "equivariance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c2145e912cef602afe4ba2d5b3263275f053efcd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces EquiReg, a plug-and-play regularization framework for diffusion-based inverse problem solver by keeping sampling trajectories close to the data manifold. It leverages Manifold-Preferential Equivariant (MPE) functions, whose equivariance error is small for on-manifold and large for off-manifold data, to penalize implausible samples. EquiReg integrates seamlessly with existing diffusion solvers across pixel, latent, and PDE domains. Experiments on diverse restoration and physical modeling tasks show effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The work connects geometric deep learning (equivariance) with probabilistic sampling (diffusion) in a theoretical way.\n2. The work demonstrated improvements across pixel diffusion, latent diffusion, and PDE solvers, showing strong versatility.\n3 The definitions of distribution-dependent equivariance and manifold-constrained equivariance are well-motivated."}, "weaknesses": {"value": "1. No formal analysis shows why equivariance error correlates with manifold distance, and the impact of the EquiReg term on sampling stability and likelihood gradients is mostly qualitative. A detailed study (e.g., trajectory visualization, convergence proofs) would strengthen understanding.\n2. The framework’s success hinges on selecting a good MPE function and appropriate symmetry group, which may be non-trivial or domain-specific. The paper provides examples but lacks systematic guidelines.\n3. Although the authors mention low overhead, the additional forward passes through MPE networks could be significant for high-resolution diffusion.\n4. The paper compares against general diffusion baselines but omits direct comparisons to manifold-preserving or geometry-constrained approaches\n5. Figures 2 could be better visualized on how equivariance loss actually affects trajectory correction."}, "questions": {"value": "1. Can MPE functions be learned jointly with the diffusion model instead of being pre-trained? Would that further improve alignment?\n2. Whether EquiReg conflict with guidance-based conditioning (e.g., classifier-free guidance) in diffusion models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TmZnQtsAVH", "forum": "WZlS3c78Lf", "replyto": "WZlS3c78Lf", "signatures": ["ICLR.cc/2026/Conference/Submission13948/Reviewer_jppv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13948/Reviewer_jppv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668234624, "cdate": 1761668234624, "tmdate": 1762924452088, "mdate": 1762924452088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an explicit regularizer, called EquiReg, for solving inverse problems in imaging with diffusion models. The method is framework-agnostic. Because natural images have equivariance, the paper suggests adding the regularizer's gradient when differentiating the likelihood. They apply this to ReSample, DPS, and PSLD and get better LPIPS, FID, and PSNR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The story is not too complex, so the message is clear.\n\n2. It is model-agnostic, so many users can adopt EquiReg for their own framework.\n\n3. It generally improves fidelity metrics like PSNR and LPIPS."}, "weaknesses": {"value": "This paper might be sacrificing the true strength of diffusion models for inverse problems. The reason diffusion models are used (like in DPS - Diffusion Posterior **Sampling**) is that they are **Samplers**, not mean-estimators. The advantage is that they can sample many different solutions that are all good. This paper has no consideration for this. However, this paper just says EquiReg helps getting good PSNR and SSIM. This is obvious. Equivariance is a somewhat classical regularization method. Of course, if one add any classical regularizer, the PSNR and SSIM will improve. This is a common idea. For example, a paper rejected at ICLR last year (https://openreview.net/forum?id=GQnR7L6SmA) used Total Variation (TV) with ADMM, another classical regularization, and it worked well, but I think that paper shares the same problem with this one: DIVERSITY. Diversity is a core philosophy of using diffusion models for inverse problems. The author of DPS has already pointed out that subtle PSNR improvement is not the thing for this problem (https://x.com/hyungjin_chung/status/1788861058309902633). Without showing a proper consideration for diversity, it is hard to prove that this simple regularizer is truly beneficial.\n\nMinor weaknesses:\n- L34: There is a citation error for Charles W Groetsch and CW Groetsch.\n\n- It is better importing figures as pdf, rather than png or jpg."}, "questions": {"value": "In L89, 97, this paper points out relying on the isotropic Gaussian assumption is one of limitations of prior work. Could you please elaborate why is it so, and how this paper addressed it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yv4WQJtHra", "forum": "WZlS3c78Lf", "replyto": "WZlS3c78Lf", "signatures": ["ICLR.cc/2026/Conference/Submission13948/Reviewer_2i6e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13948/Reviewer_2i6e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796008831, "cdate": 1761796008831, "tmdate": 1762924451232, "mdate": 1762924451232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EquiReg, an equivariance-based regularization method for diffusion inverse problems. By penalizing samples that break learned symmetries, it keeps diffusion trajectories closer to the data manifold and stabilizes posterior sampling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Introduces equivariance-based regularization as a proxy for manifold consistency, connecting geometric symmetry with probabilistic sampling in a fresh way. \n- The method is architecture-agnostic and simple to implement that can be directly added to existing diffusion frameworks (DPS, PSLD, SITCOM) without retraining."}, "weaknesses": {"value": "- The link between low equivariance error and on-manifold behavior is intuitive but not rigorously proven.\n- The approach assumes an explicit group $G$ (e.g., rotation, reflection), which may not exist or be meaningful for all tasks. \n- The method relies on pre-trained MPE encoders, but how to systematically obtain or generalize them is not well discussed. \n- The paper frequently refers to an Appendix for details of tasks, proofs, and additional experimental results, but the Appendix is not provided. As a result, several important aspects cannot be verified. This omission limits the paper’s clarity."}, "questions": {"value": "- See weaknesses\n- The statement “MPE can emerge when functions are trained with symmetry-preserving mechanisms such as data augmentation” is somewhat ambiguous. Almost all modern pretrained models are trained with some form of data augmentation, yet clearly not all of them behave as MPEs. How do the authors determine whether a given model qualifies as an MPE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gSpXQv0DzH", "forum": "WZlS3c78Lf", "replyto": "WZlS3c78Lf", "signatures": ["ICLR.cc/2026/Conference/Submission13948/Reviewer_Y4hA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13948/Reviewer_Y4hA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937613979, "cdate": 1761937613979, "tmdate": 1762924450549, "mdate": 1762924450549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed Equivariance Regularized (EquiReg) diffusion, a plug-and-play framework to solve Bayesian inverse problems with pre-trained diffusion prior. In inference time, equivariance loss is incorporated as reward gradient guidance, penalizing reconstructions that deviate from the data manifold. Experimental results across various tasks and existing inverse problem solver demonstrates the effectiveness of EquiReg."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. Novelty: The paper proposed a novel reward gradient guidance that leads to on-manifold sampling. Instead of digging deep into the underlying data distribution, the reward discriminates on-manifold samples from off-manifold samples simply using symmetry arising from data itself or the training process. \n2. Effectiveness: Numerical experiments demonstrates that EquiReg loss could be easily incorporated into gradient guidance-based diffusion inverse problem solvers, and achieve better performances."}, "weaknesses": {"value": "1. Though it is mentioned in the main paper that various details are deferred to the appendix, the appendix is not included in the submission, which substantially affects readability.\n2. Theoretical insights of the EquiReg loss is not sufficiently explored. It remains unknown how EquiReg loss could affect generation consistency. The authors mentioned some insights through Wasserstein gradient flow. It will be helpful if it could be further discussed.\n3. Though it is natural to use symmetry when data is inherently symmetric, there seems to be little intuition in the paper on how to choose a symmetry group and the corresponding MPE functions in general. Yet the choice could be crucial in sampling quality. \n4. Symmetry group size too small among all experiments. It remains unclear whether a large symmetry group, or even an infinite group such as SO(3), could affect the algorithm."}, "questions": {"value": "My questions follow from the Weaknesses.\n1. Is it possible to solve box inpainting so that reviewers could recover the appendix? Will be happy to see this paper published if it is complete.\n2. Is it possible to demonstrate the effectiveness of EquiReg loss through low-dimensional experiments?\n3. Can the authors compare different MPE functions and symmetry groups on the same task and same diffusion inverse problem solver?\n4. Is it possible to experiment with a symmetry group with large cardinality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZeTvXuihMl", "forum": "WZlS3c78Lf", "replyto": "WZlS3c78Lf", "signatures": ["ICLR.cc/2026/Conference/Submission13948/Reviewer_86xx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13948/Reviewer_86xx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950998848, "cdate": 1761950998848, "tmdate": 1762924450154, "mdate": 1762924450154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}