{"id": "dggfgPzGFW", "number": 6146, "cdate": 1757954559033, "mdate": 1759897932991, "content": {"title": "PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis", "abstract": "Interactive world models that simulate object dynamics are crucial for robotics, VR, and AR. However, it remains a significant challenge to learn physics-consistent dynamics models from limited real-world video data, especially for deformable objects with spatially-varying physical properties. To overcome the challenge of data scarcity, we propose PhysWorld, a novel framework that utilizes a simulator to synthesize physically plausible and diverse demonstrations to learn efficient world models. Specifically, we first construct a physics-consistent digital twin within MPM simulator via constitutive model selection and global-to-local optimization of physical properties. Subsequently, we apply part-aware perturbations to the physical properties and generate various motion patterns for the digital twin, synthesizing extensive and diverse demonstrations. Finally, using these demonstrations, we train a lightweight GNN-based world model that is embedded with physical properties. The real video can be used to further refine the physical properties. PhysWorld achieves accurate and fast future predictions for various deformable objects, and also generalizes well to novel interactions. Experiments show that PhysWorld has competitive performance while enabling inference speeds 47 times faster than the recent state-of-the-art method, i.e., PhysTwin. The code and pre-trained models will be publicly available.", "tldr": "We propose PhysWorld, a novel framework that synthesizes physics-aware demonstrations to learn efficient world models.", "keywords": ["World Models", "Deformable Objects", "Demonstration Synthesis"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b5ad8556a295a07ef16c3dd51cee737310f9672.pdf", "supplementary_material": "/attachment/675f5ce44444cb7da292f6568ada0a79712054d7.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on reconstructing a digital twin of a deformable object from videos. Technically, this paper extends PhysTwin: (1) it refines physical motion simulation by a part-based physical property variation method and a tailored optimization method; (2) it improves simulation speed by distilling physics simulation onto a GNN for dynamics prediction. Experiments show that the proposed method is much faster in simulation, and the simulated motion improves over PhysTwin."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Fast simulation that is useful for motion planning.\n- Improved quality upon prior work."}, "weaknesses": {"value": "- I'm not convinced by the proposed additional components including part-based perturbation and the optimization strategy. In particular, looking at the L343 and L344, by adding these somewhat complicated components, the benefits over PhysTwin are marginal. Do they really justify the complication added? To see this, I recommend a baseline: PhysTwin + the GNN distillation. If this baseline works equally well, then I think only the GNN distillation is needed.\n- There are only 3 examples in video results. More video results are needed to be convincing."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QjBfc0Luug", "forum": "dggfgPzGFW", "replyto": "dggfgPzGFW", "signatures": ["ICLR.cc/2026/Conference/Submission6146/Reviewer_oNhM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6146/Reviewer_oNhM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698879394, "cdate": 1761698879394, "tmdate": 1762918501532, "mdate": 1762918501532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a framework for building physics-consistent world models of deformable objects directly from short real-world videos. It bridges the gap between high-fidelity physical simulation and efficient neural dynamics learning by combining a MPM-based digital twin with GNNs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is clear and well-written\n* The VLM-assisted constitutive model selection introduces a creative and automated way to align material modeling with observed deformation behaviors.\n* Experiments on 22 scenarios demonstrate strong performance"}, "weaknesses": {"value": "* Heavy dependence on differentiable MPM simulation. However, the MPM-based simulator's sim-2-real gap is not well studied.\n* The generalization tests, though promising, focus on limited or scenarios from one dataset; real-world complexities (e.g., lighting, occlusion) are underexplored.\n* The impact and robustness of the VLM-based material selection are not deeply analyzed—especially in ambiguous or noisy video settings."}, "questions": {"value": "* Recent paper (https://arxiv.org/abs/2506.23126) has shown that using transformer as dynamics model is better than GNNs. Could you include the results of comparison between these two structures?\n* If a fully-optimized MPM model can serve as a digital twin of the real world?\n* How does the system handle failure cases in the VLM-based constitutive model selection—does it fall back to default physics priors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gJs2S7m5dc", "forum": "dggfgPzGFW", "replyto": "dggfgPzGFW", "signatures": ["ICLR.cc/2026/Conference/Submission6146/Reviewer_aLrE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6146/Reviewer_aLrE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803450655, "cdate": 1761803450655, "tmdate": 1762918500231, "mdate": 1762918500231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to learn world models of deformable objects by grounding videos rendered from a physical simulator. The authors builds material-point methods simulator to generate data with physical plausibility, in contrast to learning from limited real world video streams. The demonstrations collected from the simulator are then used to train a graph neural network that amortizes the prediction, with the physical properties as the embedded attributes. With an inverse rendering pipeline, real videos are used to fine-tune the physical properties for a better alignment with the reality. The results show faster prediction due to the graph neural network inference and better accuracy comparing a baseline method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Well articulated approaches and easy to grasp the central idea;\n* Modelling and predicting deformable object behavior is an important topic so the paper may bring about impacts;\n* Promising to publish code and pre-trained models. Important for reproducibility of the work."}, "weaknesses": {"value": "* All technical methods are well known and the paper reads like a concatenation of them without much scientific novelty or insights;\n* Validation should be stronger. Especially to the point of using a GNN to amortize a simulation. Better to have more direct evidence to show the benefits of having a faster predictive model when the simulator is already there. See more elaborated points in questions.\n* Validation should also be more extensive. I think this is especially the case given the paper is more like a system paper with slim scientific contributions. The tested scenes look a bit trivial and hardly representative of interesting deformable tasks."}, "questions": {"value": "* What is the scientific observation or insight beyond the system work presented in the paper?\n* What is the point of using a neural network to fit a simulator when the latter is already readily available for generating as much data as needed? If the argument lies in prediction efficiency, can this efficiency be shown necessary in some applications, such as real-time control or speeding-up reinforcement learning environment queries?\n* Can the results significance be demonstrated with a statistical analysis as many numbers in tables look very close to each other? \n* The performance is most demonstrated as the prediction quality while it is hard to tell whether this is necessarily translating into a good model for policy learning or model-based planning. Can experiments like in Figure 4 be expanded to more realistic and extensive scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "llPm2KKUKG", "forum": "dggfgPzGFW", "replyto": "dggfgPzGFW", "signatures": ["ICLR.cc/2026/Conference/Submission6146/Reviewer_WAMx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6146/Reviewer_WAMx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904081372, "cdate": 1761904081372, "tmdate": 1762918499735, "mdate": 1762918499735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PhysWorld, a framework for learning accurate and efficient world models of deformable objects from short, real-world videos. It tries to address the trade-off between high-fidelity, physics-based simulators (which are accurate but slow) and learning-based models (which are fast but data-hungry and often physically inconsistent). The proposed framework includes two augmentation methods Various Motion Pattern Generation (VMP-Gen) and Part-aware Physical Property Perturbation (P3-Pert) for increasing the diversity of synthesized demonstrations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a practical problem: learning usable dynamic models from limited, real-world data. The proposed method offers a compelling solution that addresses the issues inherent in existing methods: the accuracy of a high-fidelity physics engine and the speed of a lightweight neural network. The demonstrated 47x speedup over the SOTA baseline is a massive practical gain that unlocks real-time applications, as evidenced by the successful MPPI planning experiment.\n\nP3-Pert (Part-aware Physical Property Perturbation) utilizes semantic part features to generate spatially correlated noise for physical properties, which is far more physically plausible than naive random perturbations. It appears to be an effective method for generating meaningful data diversity, which the ablations confirm is superior. Using a VLM to automate the selection of a physics model from a library based on video is a valuable idea that simplifies a traditionally manual and expertise-driven process. Lastly, the two-stage optimization strategy is a methodologically sound and robust approach to parameter estimation, avoiding the pitfalls of optimizing all parameters from a cold start."}, "weaknesses": {"value": "1) Lacking robustness analysis for 3D data preprocessing:\n\nThis paper proposes a pipeline for constructing world models from video. The entire pipeline begins with extraction of object point clouds from real interaction videos. This is a critical, non-trivial preprocessing step. The quality (density, noise, completeness) of this initial 3D tracking is fundamental to the accuracy of the digital twin optimization. The paper does not analyze the framework's sensitivity to this input quality. If the tracking is poor, it's likely the digital twin will be inaccurate, and this error will propagate through the entire system.\n\n2) Lacking analysis for external models:\n\nThe framework's novelty relies on two other sophisticated models: a VLM (Qwen3) for model selection and a PartField model. This introduces potential failure points. What happens if the VLM misidentifies the material? Or if the PartField model provides a poor or trivial segmentation for a new object category? The paper doesn't discuss the robustness of the pipeline to failures in these helper modules."}, "questions": {"value": "1) Robustness to VLM Failure:\n\nThe 100% accuracy for the VLM model selection is noted in Appendix B2. What will happen if a suboptimal model is chosen? For instance, if the VLM mistakenly selects \"Neo-Hookean\" for the \"cloth\" object, how much does this error degrade the final accuracy of the fine-tuned PhysWorld GNN?\n\n2) Setup cost:\n\nThe paper introduces the fast inference speed of the proposed method. However, the training/preparation cost would be important for practitioners. Could you please provide an approximate wall-clock time for the entire setup process for a new object? (e.g., for the \"double lift sloth\" scene). This would include digital twin optimization, demonstration synthesis, and GNN training/fine-tuning. This would provide a complete picture of the method's computational cost."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CU14O1QaAY", "forum": "dggfgPzGFW", "replyto": "dggfgPzGFW", "signatures": ["ICLR.cc/2026/Conference/Submission6146/Reviewer_kicv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6146/Reviewer_kicv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972754114, "cdate": 1761972754114, "tmdate": 1762918499343, "mdate": 1762918499343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PhysWorld, a framework for constructing physics-consistent and efficient world models of deformable objects from short real-world videos.\nThe method first builds a digital twin in an MPM simulator, where a Vision-Language Model (Qwen3) selects the appropriate constitutive material models and a global-to-local optimization refines physical parameters (e.g., friction, density, Young’s modulus).\nThe calibrated simulator then generates diverse synthetic demonstrations via Various Motion Pattern Generation (VMP-Gen) and Part-aware Physical Property Perturbation (P³-Pert).\nA GNN-based world model is trained on these demonstrations and fine-tuned using real videos.\nExperiments show that PhysWorld achieves accurate predictions and 47× faster inference than PhysTwin while maintaining visual and physical consistency.\nThe paper also provides a qualitative demonstration of model-based planning using MPPI for rope manipulation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Comprehensive and well-integrated framework**  \n   The paper thoughtfully combines VLM-based material selection, multi-stage parameter optimization, and physics-guided data augmentation into a coherent pipeline bridging simulation and learning.\n\n2. **Strong accuracy–efficiency trade-off**  \n   The proposed GNN-based world model achieves high predictive accuracy while running at **799 FPS**, demonstrating its potential for real-time applications that require fast yet physically consistent inference.\n\n3. **Visual prediction capability**  \n   Integration of 3D Gaussian Splatting and Linear Blend Skinning enables **action-conditioned video generation**, evaluated with PSNR, SSIM, and LPIPS metrics.\n\n4. **Generalization to unseen interactions**  \n   The model generalizes well to new manipulation sequences and shows physically plausible deformation behavior (Fig. 3, Table 2).\n\n5. **Detailed ablation studies**  \n   The effects of global-to-local optimization, VMP-Gen, and P³-Pert are clearly quantified (Tables 3–5), and the results support the design choices."}, "weaknesses": {"value": "1. **Lack of quantitative downstream (control) evaluation — a key remaining limitation**  \n   While the framework’s real-time capability is convincingly demonstrated (47× faster inference), the paper does not provide **quantitative results in downstream control or planning tasks**.  \n   The MPPI example (Fig. 4) is qualitative only. Demonstrating success rates, trajectory errors, or computation times in model-based control would substantially strengthen the practical significance.  \n   Given that real-time inference is the method’s major advantage, connecting it to tangible control improvements would enhance the paper’s overall impact.\n\n2. **Action-conditioned requirement**  \n   The world model relies on explicit control inputs (\\(a_t\\)) extracted from video, and learning from action-free observational data is not addressed. This limits applicability to passive video datasets.\n\n3. **Architectural diversity**  \n   The study focuses solely on GNNs. Comparisons with alternative architectures (e.g., Transformers or MLP-based dynamics models) could clarify whether the observed advantages are model-specific or framework-driven.\n\n4. **Realism of synthetic demonstrations**  \n   Although the proposed perturbation and trajectory generation methods improve diversity, there is no quantitative analysis of how closely these synthetic motions resemble real physical dynamics.\n\n5. **VLM-based model selection robustness**  \n   The Qwen3-based constitutive model selection is promising, but evaluation is limited to 22 clean scenarios. Its reliability under noise, occlusion, or mixed-material settings remains unclear.\n\n6. **Scalability and physical consistency metrics**  \n   Experiments are conducted on relatively small particle systems (≈100–150 nodes). Performance and stability on larger, multi-object, or more complex scenes are not analyzed.  \n   Additionally, metrics directly assessing physical-law consistency (e.g., conservation of momentum or energy) are not reported.\n\n7. **Reproducibility details**  \n   While optimization procedures are well described, training sensitivity (e.g., to initialization, hyperparameters) and convergence analyses are not provided, making it difficult to assess robustness.\n\n8. **Conceptual scope of “World Model”**  \n   The method models object-level deformable dynamics rather than a full scene-level world model. Clarifying this scope would help set appropriate expectations."}, "questions": {"value": "1. Could you provide **quantitative control results** (e.g., success rate, trajectory error, or control frequency) to demonstrate how PhysWorld’s high inference speed translates to better control performance?  \n2. How physically realistic are the synthetic demonstrations generated by VMP-Gen and P³-Pert?  \n3. How stable is the **VLM-based material selection** under noisy or complex real-world videos?  \n4. How transferable are the fine-tuned physical parameters (Φ) to new objects or scenes?  \n5. How does inference speed scale with larger particle counts or multiple interacting objects?  \n6. Could the model be extended to **action-free settings** through action inference or inverse dynamics?  \n7. Have you considered comparing the GNN to a Transformer-based or MLP-based world model for efficiency and accuracy?  \n8. Would adding explicit physical consistency losses (e.g., for momentum or volume preservation) further improve realism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WkTfglyr68", "forum": "dggfgPzGFW", "replyto": "dggfgPzGFW", "signatures": ["ICLR.cc/2026/Conference/Submission6146/Reviewer_YYty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6146/Reviewer_YYty"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6146/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976323468, "cdate": 1761976323468, "tmdate": 1762918498943, "mdate": 1762918498943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}