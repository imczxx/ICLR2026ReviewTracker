{"id": "dOS3e1kC5u", "number": 1529, "cdate": 1756889849534, "mdate": 1759898204199, "content": {"title": "Stealing and Defending the Ends of LLMs", "abstract": "Soft prompt tuning has emerged as a powerful and automated approach for adapting large language models (LLMs) to new tasks, eliminating the need for manual prompt engineering. The practical relevance of soft prompts is underscored by their support in major toolkits and APIs such as NVIDIA NeMo and IBM Watsonx AI. However, as soft prompts encode valuable, task-specific information, they have become attractive targets for adversarial extraction. In this work, we demonstrate that attackers can extract functionally equivalent soft prompts from prompt-tuned LLMs, effectively replicating their capabilities without access to the original training data or resources. By training a dedicated inversion model, we show that such extraction generalizes, enabling recovery of soft prompts for any downstream task on the given model. To counter this threat, we introduce CAP (**C**overage-**A**ware **P**erturbation), an active defense that substantially impairs extraction while maintaining task performance for legitimate use. Our framework highlights both new risks and practical solutions, paving the way for more trustworthy deployment of adapted LLMs.", "tldr": "We show that adversaries can extract functionally equivalent soft prompts from prompt-tuned LLMs and introduce CAP, a defense that impairs such extraction while preserving task performance.", "keywords": ["Model Stealing", "Model Extraction", "Soft prompt tuning", "soft prompts", "last layer extraction", "defenses", "LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cffde0598acaa89789f981fb94a37b18e4ed84e.pdf", "supplementary_material": "/attachment/99b220c67bbb4e6f13ed15bf1d2e789553cebecc.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies the security of soft-prompt–tuned LLMs exposed via APIs that return next-token probabilities. It presents a two-stage attack: (i) distillation, which learns a functionally equivalent soft prompt by minimizing KL divergence between victim and surrogate outputs; and (ii) inversion, which trains a Transformer to map probability vectors directly to soft-prompt embeddings and claims cross-task generalization. To mitigate, the paper proposes CAP, an active defense that estimates query coverage in embedding space (via LSH bucket coverage, new-bucket rate, and spread) and injects Gaussian noise into the served prompt or logits in proportion to coverage. Experiments on T5 variants (plus some roberta-base) show strong Stage-1 performance; Stage-2 reports transfer across related tasks; CAP reduces extraction and increases last-layer RMSE. Additionally, a timing side-channel (SPLIT) for estimating prompt length appears in the appendix."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- S1: KL-based distillation on black-box probabilities seems simple, effective, and broadly applicable.\n- S2: The authors target a realistic API settings (full/top-k probabilities), and frame risks/defenses at both the input (soft prompt) and output (last layer) ends.\n- S3: Results show some transferability on T5 sizes and roberta-base; defense also touches Pythia/GPT-2 for last-layer extraction.\n- S4: The separation into distillation and inversion clarifies assumptions and makes the attack path easy to reason about."}, "weaknesses": {"value": "- W1 (Baselines): The Random prompt in Table 1 hovers around $50$% (even for MNLI where random chance should be $33$%?), which is hard to interpret. It would be great to include as a baseline T5-base with no soft prompt or a discrete prompt. This would clarify whether random soft prompts actively harm the model.\n- W2 (Missing comparative baselines): Beyond ``random'', I believe this paper needs at least one additional baseline to contextualize the performance. For example this could be [Morris et al,. 2024: Language Model Inversion] to recover a discrete text prompt and compare its downstream performance to the presented results.\n- W3 (Defense utility & adaptivity): I am skeptical about the assumption that benign queries are low-diversity (e.g. multi-task users). As this seems to be a fundamental assumption for the proposed defense method, it would be great to justify this with actual user data (e.g. WildChat dataset).\n- W4 (CAP results interpretation): Table 2 shows strong perturbation on the attack queries; reporting per-run coverage metrics (C/N/S) would explain why these are deemed diverse. Additionally, in table 3, it would be important to add a row for LEGIT + CAP-OFF to show absolute utility drop. \n- W5 (Model choice): T5 seems a bit dated by now. Running some of the T5 experiments in stage 1 with e.g. Pythia would improve this.\n\nIn its current state I cannot recommend acceptance of the paper. However, the points I raised are adequately addressed, I am willing to increasing my score."}, "questions": {"value": "- Q1: Could you clarify how exactly the LEGIT/ATTACK classification in Table 3 is defined?\n- Q2: The adversary sometimes even outperforms the target (Table 2), and in summarization (Table 10) random $\\approx$ target while reconstructed underperforms. Do you have an explanation for these?It might be good to add multi-seed means and standard deviations to make the findings more trustworthy.\n- Q3: If trained on a single task, it is unclear to me why the inversion model should generalize? A small ablation varying the number/diversity of training tasks, and a naive cross-task baseline (i.e. reuse the $D_1$ recovered sot prompt on $D_2$) would clarify the incremental value."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PJyaAbEb8Z", "forum": "dOS3e1kC5u", "replyto": "dOS3e1kC5u", "signatures": ["ICLR.cc/2026/Conference/Submission1529/Reviewer_poZL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1529/Reviewer_poZL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761409595798, "cdate": 1761409595798, "tmdate": 1762915797442, "mdate": 1762915797442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a technique to distill and invert prompt tuned models through soft prompts.\nFrom the system provider side, the paper also introduces a defense against the proposed inversion attack through monitoring the adversaries' query diversity.\nExperiment are carried out on small scale models and datasets showing promising results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The main strengths of this work are:\n\n1) The proposed approach is generic. Distilling the inverting through soft prompts does not assume any specific architecture nor dataset.\n\n2) The related work section in this paper is thorough.\n\n3) The proposed attack is easy to implement and provides good results under small scale experiments."}, "weaknesses": {"value": "Despite the aforementioned strengths, this paper has major weaknesses that need to be addressed before getting it published.\n\n1) Experiments are conducted under small scale models and datasets. It is hard to measure actual usefulness. To demonstrate the effectiveness of the proposed attack, experiments should be carried out on realistic size models (such as Llama3-7B), and on realistic tasks/datasets.\n\n2) The assumption that the architecture is shared seems to be restrictive. It is often the case that the API does not provide any information about the architecture. Experiments should also include the case where there is a mismatch between the architecture of the target and the victim models. \n\n3) The baseline (LLM with random SP initialization) is not convincing. At least, the performance of the base LLM without any SP should be included as a weak baseline. Further, the impact of the learnt soft prompts on different tasks (e.g. reasoning/coding/solving math problems) should be measured.\n\n4) Experiments with comparison with other PEFT methods are missing (e.g. LoRA). While I understand the advantages of soft-prompts compared to LoRA, the argument in this paper would have been significantly strengthen if a comprehensive comparison against other PEFT methods is included.\n\n5) The writing of this paper can be vastly improved. Many parts (such as Figure 1, Section 5.2) are very hard to parse and overcomplicates the simple message of this paper. For example, Equation (2) can be simplified to $TotalCost= \\lambda + w_c (\\frac{\\alpha C}{\\lambda \\beta} -1 ) + \\alpha w_n N + \\alpha w_s \\min (\\frac{S}{S_{max}}, 1)$. Further, the tables can be located in the page they were mentioned in to ease the reading of the manuscript.\n\n6) The proposed defense CAP is both naive and makes unrealistic assumptions. For instance, such defense can be surpassed with multiple attackers setting (each attach does not diversify the queried topics, but different attacks query different buckets). \n\n7) The paper mentions that the inversion accelerates the distillation (e.g. in line 361), without providing a comparison nor quantification of the time/cost saved. A discussion along these lines is necessary with its corresponding experiments.\n\nOverall, while I extremely appreciate the practicality of the proposed method, I believe that this work has to address significant concerns, provide important extra experiments and ablations before getting published."}, "questions": {"value": "Please refer to the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qpxLvccI5x", "forum": "dOS3e1kC5u", "replyto": "dOS3e1kC5u", "signatures": ["ICLR.cc/2026/Conference/Submission1529/Reviewer_n4Zu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1529/Reviewer_n4Zu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761561790521, "cdate": 1761561790521, "tmdate": 1762915797270, "mdate": 1762915797270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a systematic framework for auditing LLMs' susceptibility to generating malicious code, demonstrating significant real-world impact through the discovery of active scam sites. However, the analysis of model-specific vulnerabilities lacks depth, and the scope is limited to URL-based threats, potentially overlooking other malicious code vectors. The guardrail evaluation is limited to a single system without exploring alternative defenses (Sec. 6.1, Sec. 6.2)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Novel Attack Formulation\n  - Introduces a two-stage attack combining distillation (KL divergence minimization, Eq. 1) and inversion, enabling cross-task prompt extraction (Sec. 3).\n  - Demonstrates generalization to unseen tasks (e.g., 87.2% vs. 88.8% on YELP when trained on AMAZON, Table 2), reducing computational costs vs. tuning from scratch (Table 5).\n  - Validated on diverse architectures (T5/Roberta) and tasks (classification/NLI), showing robustness to OOD queries (Table 1).\n\n* Practical Defense Design\n  - CAP leverages embedding-space coverage metrics (bucket coverage, spread) to distinguish adversarial vs. benign users (Sec. 5.1).\n  - Perturbs prompts/outputs based on coverage (Eq. 2), reducing stolen utility to random levels (Table 3) while preserving benign user performance (e.g., 86.7% vs. 91.08% on AMAZON).\n  - Extends to defend against last-layer extraction (RMSE increase from 1.96e-5 to 18.21 for T5-base, Table 4).\n\n* Comprehensive Experimental Validation\n  - Tests 4 LLMs in 2024 with varied Prompt/Codegen LLM combinations (Table 1), demonstrating robustness across model pairs and architectures.\n  - Applies benchmark to 7 state-of-the-art 2025 models (Table 2), showing consistent vulnerability across diverse providers and model sizes."}, "weaknesses": {"value": "* Limited Threat Model Realism\n  - Assumes full access to next-token probabilities, but real-world APIs (e.g., OpenAI) often expose only top-k tokens (Sec. 4.1). Experiments with top-5 access (Table 8) show minor degradation, but broader constraints (e.g., rate limits) are unexplored.\n  - Ignores ethical implications: No discussion of misuse risks (e.g., IP theft) or mitigation beyond CAP (Sec. 7).\n\n* Inadequate Analysis of Guardrail Effectiveness\n  - Only tests one guardrail (NeMo Guardrails), with no comparison to alternative safety mechanisms or discussion of why it failed.  \n  - No investigation into specific policy gaps (e.g., S24: \"Use of scam API/website\" in Fig. 9) that might improve detection.  \n  - Fails to address why guardrails missed all malicious outputs despite clear scam API references, leaving mitigation strategies unexplored.  \n\n* Insufficient Discussion of Training Data Poisoning Mechanisms\n  - Claims data poisoning but provides no evidence of how poisoning occurred (e.g., specific training data sources or crawl processes).  \n  - Hypothesizes OpenAI models’ higher malicious rates due to \"more extensive data containing scam-related content\" (Sec. 6.1) without supporting evidence or analysis.  \n  - No investigation into why certain models (e.g., gpt-4o-mini) consistently produce higher malicious rates across combinations (Table 1), limiting understanding of root causes."}, "questions": {"value": "1. In Section 1, the authors state that \"the actual rate of malicious code generation likely exceeds this figure when considering attack vectors beyond URLs.\" Could the authors provide a preliminary analysis of other malicious code vectors (e.g., backdoors or worms) or discuss how their framework could be extended to include them? (Sec. 1)\n\n2. In Table 1, the authors report varying malicious rates based on Prompt and Codegen LLM combinations. Could they provide more details on how specific Prompt LLM characteristics (e.g., keyword diversity or prompt specificity) correlate with higher malicious rates? (Sec. 6.1)\n\n3. The paper claims \"malicious content contamination is an industry-wide problem persisting despite advances in safety alignment\" (Sec. 8). Could the authors discuss potential training data sanitization strategies that could mitigate this issue, and whether any such strategies were tested in their framework? (Sec. 8)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zv74NSM99Z", "forum": "dOS3e1kC5u", "replyto": "dOS3e1kC5u", "signatures": ["ICLR.cc/2026/Conference/Submission1529/Reviewer_VEkW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1529/Reviewer_VEkW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578066487, "cdate": 1761578066487, "tmdate": 1762915797057, "mdate": 1762915797057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a two-stage black-box attack for extracting functionally equivalent soft prompts from prompt-tuned LLMs. Stage 1 distills a prompt by optimizing a new soft prompt to match the victim model’s output distributions. Stage 2 trains an inversion model to map next-token probabilities to soft-prompt embeddings that generalize across tasks. To counter these attacks, the authors propose Coverage-Aware Perturbation, an active defense that tracks query diversity using LSH-based coverage metrics and injects perturbations when behavior indicates adversarial extraction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The methodology for both distillation and inversion is well motivated, mathematically grounded, and experimentally validated.\n- The paper evaluates multiple model families (T5, RoBERTa, GPT-2, Pythia) and tasks, providing strong evidence that the attacks generalize and are practical.\n- CAP is a realistic defense for API-hosted LLMs, aligning well with operational constraints and outperforming prior approaches that rely on brittle OOD detection.\n- As soft prompting gains traction in commercial APIs, the proposed attacks and defense highlight important security considerations for model providers."}, "weaknesses": {"value": "- The evaluation does not deeply explore adversaries who modulate query diversity to evade CAP, limited analysis of adaptive attackers. How would CAP respond to such adaptive strategies?\n- Many real-world APIs restrict logits heavily; more systematic evaluation under severe output truncation would strengthen the practicality assessment. The attack is shown to work with top-k probabilities, but how does performance degrade as k decreases, especially down to top-1 output, which many APIs enforce?\n- The description of the inversion model is brief; more clarity on architecture choices and failure modes would help interpret its capabilities.\n- Could an adaptive attacker interleave benign-looking task-specific queries with diverse probing queries to lower their coverage signal?"}, "questions": {"value": "See the questions in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zv74NSM99Z", "forum": "dOS3e1kC5u", "replyto": "dOS3e1kC5u", "signatures": ["ICLR.cc/2026/Conference/Submission1529/Reviewer_VEkW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1529/Reviewer_VEkW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578066487, "cdate": 1761578066487, "tmdate": 1763438178197, "mdate": 1763438178197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the vulnerability of soft-prompt-tuned LLM deployments and proposes: (1) Attack: A two-stage black-box attack, including distillation stage (learn a functionally equivalent soft prompt by minimizing KL divergence to the victim’s next-token probabilities) and inversion stage (train a small transformer to map next-token probability vectors to soft-prompt embeddings that transfer across tasks); (2) Defense: Coverage-Aware Perturbation (CAP), track query diversity via LSH-based bucket coverage, new-bucket rate, and spread; map coverage to a perturbation budget and inject Gaussian noise either into the soft prompt or outputs to frustrate extraction, claiming minimal harm to benign users."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper solved a timely and important problem, and the authors provide a clear and realistic threat model.\n2. Proposed attack is simple and effective at stage 1 and the learnt inversion model yields usable prompts on unseen tasks at stage 2.\n3. The paper provides a concise end-to-end runtime analysis showing that the proposed two-stage attack is far faster than prompt tuning. CAP adds only moderate overhead suitable for deployment."}, "weaknesses": {"value": "1. Limited evaluation beyond mid-scale classification: experiments focus on T5/RoBERTa/GPT-2/Pythia and classification tasks. Evidence for generation tasks (e.g., summarization) is weak or inconsistent and should be expanded with generation metrics (ROUGE/BERTScore) and CAP-on vs CAP-off comparisons.\n2. Table 2 shows when trained on YELP and evaluated on MOVREV, the Adversary (CAP Off) accuracy is 89.33% (+7.2 points to target). This is intriguing, could we provide some insight for the underlying reason? Or a variance analysis with multiple seeds may help."}, "questions": {"value": "1. Will identical inputs from the same benign user may produce slightly different behavior over time? Can we guarantee stability and user-visible variance?\n2. Can CAP be fooled by coherent multi‑task users (e.g., a legitimate pipeline with naturally diverse inputs)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P6lRdqbPzx", "forum": "dOS3e1kC5u", "replyto": "dOS3e1kC5u", "signatures": ["ICLR.cc/2026/Conference/Submission1529/Reviewer_1dpA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1529/Reviewer_1dpA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929912580, "cdate": 1761929912580, "tmdate": 1762915796357, "mdate": 1762915796357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}