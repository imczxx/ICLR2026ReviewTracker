{"id": "x7NKufaPUu", "number": 17304, "cdate": 1758274458214, "mdate": 1763116569375, "content": {"title": "ComposeAnything: Composite Object Priors for Text-to-Image Diffusion Models", "abstract": "Generating images from text with complex object arrangements remains a major challenge for current text-to-image (T2I) models. Existing training-based solutions, such as layout-conditioned models or reinforcement learning methods, improve compositional accuracy but often distort realism, leading to floating objects, broken physics, and degraded image quality. \nIn this work, we introduce ComposeAnything, an inference-only framework that enhances compositional generation without retraining. Our key idea is to replace stochastic noise initialization with \\emph{composite object priors}— interpretable structured composite of objects, created using 2.5D layouts generated from large language models and pretrained image generators. We further propose prior-guided diffusion, which integrates these priors into the denoising process to enforce compositional correctness while preserving visual fidelity.\nThis training-free strategy enables seamless generation of compositional objects and coherent backgrounds, while allowing refinement of inaccurate priors. ComposeAnything consistently outperforms state-of-the-art inference-only methods on T2I-CompBench and NSR-1K benchmarks, especially for prompts with complex spatial relations, high object counts, and surreal scenes. Human evaluations confirm that our method generates images that are not only compositionally faithful but also visually coherent.", "tldr": "We propose to use \"2.5D Composite object priors for structured noise initialization along with prior guided diffusion and spatial controlled denoising for out of distribution highly surreal compositional image generatio", "keywords": ["Diffusion models", "text to image generation", "image layout with LLM planning", "structured noise initialization", "prior guided diffusion"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f2cf93e161a4e8b4e652b269ca2750f78db59f08.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles an important challenge in text-to-image generative models, achieving correct compositionality and precise text-image alignment. The authors propose a framework that employs a large language model (LLM) as a high-level planner to first produce a scene layout, accompanied by localized captions for each component and corresponding depth estimations. These structured elements are then used to generate individual image parts separately, which are subsequently composed into a coherent whole. The resulting composite image serves as a prior to guide the diffusion process, enhancing the compositional consistency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly presents and thoroughly explains the proposed method, leaving no ambiguity.\n- Ablation studies are conducted to evaluate the individual and combined effects of prior reinforcement and spatial control used in their method.\n- The proposed approach demonstrates improvements over baseline models across evaluation metrics.\n- The paper evaluates performance not only on benchmarks such as T2I-CompBench and NSR-1K, but also through human evaluations."}, "weaknesses": {"value": "- The method lacks strong novelty, as it largely combines previously explored ideas, such as using an LLM as a planner, generating individual objects as conditional inputs, and employing object-prior reinforcement and spatially controlled denoising, all of which have been discussed in earlier works.\n- The proposed framework has limited practical applicability. It requires multiple computationally intensive steps, including invoking an LLM, generating separate images for each component, performing segmentation, and adding extra computation during the main diffusion process, making it impractical for real-world or large-scale deployment.\n- The evaluation is not thorough, as the paper only covers selected subsets of T2I-CompBench, omitting other aspects such as attribute binding (i.e., color, texture, shape categories of T2I-CompBench).\n- The authors argue that fine-tuned models (e.g., FlowGRPO) perform worse in terms of image quality, yet this claim is supported only by qualitative evidence without any quantitative evaluation to substantiate it.\n- Although the paper mentions generating and segmenting all objects in parallel using multiple GPUs to reduce computation time, this setup makes the reported runtime comparisons unfair. A fair evaluation should consider inference time on a single GPU to accurately reflect the method’s efficiency."}, "questions": {"value": "- I noticed that the RPG results reported in the paper differ from those in the original RPG publication. Could you clarify whether these results were re-evaluated to verify their accuracy or directly taken from the original paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "laDoqQebSt", "forum": "x7NKufaPUu", "replyto": "x7NKufaPUu", "signatures": ["ICLR.cc/2026/Conference/Submission17304/Reviewer_Rwmd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17304/Reviewer_Rwmd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761018483840, "cdate": 1761018483840, "tmdate": 1762927239896, "mdate": 1762927239896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "7neYjbypC1", "forum": "x7NKufaPUu", "replyto": "x7NKufaPUu", "signatures": ["ICLR.cc/2026/Conference/Submission17304/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17304/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763116568545, "cdate": 1763116568545, "tmdate": 1763116568545, "mdate": 1763116568545, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed ComposeAnything, a training-free framework to enhance compositional image generation. It first uses LLM to plan for object boxes and depths, then generates corresponding objects as priors. A novel prior-guided diffusion is then leveraged to generate the final image. Extensive experiments demonstrate that ComposeAnything outperforms state-of-the-art methods across multiple compositional benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is conceptually intuitive and easy to understand. The idea of introducing structured object priors to guide diffusion generation is simple yet effective.\n2. ComposeAnything achieves consistent quantitative and qualitative improvements over prior state-of-the-art methods across various compositional benchmarks."}, "weaknesses": {"value": "1. The method shows limited novelty. The LLM-based layout planning and spatial-controlled attention are largely borrowed from RPG, and such mechanisms have already been widely used in recent compositional generation approaches.\n2. The idea of replacing noise initialization with a composite object prior, though intuitive and easy to follow, is conceptually similar to FreeCompose[1], which also optimizes a composed prior image within the diffusion process. \n3. The additional step of generating per-object priors introduces considerable computational overhead and latency. \n\n\n\n[1] Chen, Zhekai, et al. \"Freecompose: Generic zero-shot image composition with diffusion prior.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024."}, "questions": {"value": "The paper claims that “RPG uses region crops instead of object layout, so its LLM planning is less efficient than ours.”\nCould the authors clarify why this is the case?\nFrom the description, both methods seem to require the LLM to output multiple object positions (and ComposeAnything even adds depth ordering). So why is RPG less efficient?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "290dWPVEDp", "forum": "x7NKufaPUu", "replyto": "x7NKufaPUu", "signatures": ["ICLR.cc/2026/Conference/Submission17304/Reviewer_gqK8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17304/Reviewer_gqK8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724112357, "cdate": 1761724112357, "tmdate": 1762927239201, "mdate": 1762927239201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Plan-and-Paint, a framework combining two reasoning paradigms in T2I task for better image generation: (1) semantic-level planning via an Adaptive Length Prediction for CoT (ALP-CoT) that expands the prompt only as needed, and (2) noise-level reasoning trained with GRPO so the initial noise prior aligns with the semantic plan. A multi-reward ensemble (HPSv2 for preference, GroundingDINO for existence/count/relations, and a VQA model for attribute/theme) is used as the reward signal. On GenEval and WISE, the method reports SOTA performance and the ablations indicate ALP-CoT is beneficial: fixed-length CoT either over- or under-elaborates and hurts spatial tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+Modular two-level design and targeted rewards. The pipeline (Fig. 2) and reward breakdown (Fig. 3) are modular and reasonable; the ensemble explicitly covers aesthetic, composition (existence/count/relations), and attribute/theme fidelity\n\n+Strong results on GenEval/WISE versus both open and closed baselines; GenEval tops three of six subtasks, with large Position/Attr-Binding gains.\n\n+Ablation studies validated the modular design, for example, Length sweeps (L=30…2048) show how fixed-length CoT fails on spatial reasoning."}, "weaknesses": {"value": "-My first concern is that the novelty is largely integrative: ALP-CoT extends prior semantic CoT approaches (e.g., T2I-R1) with adaptive length; noise-level reasoning follows the idea of optimizing the initial noise prior (e.g., NoiseAR); GRPO is an existing RL algorithm. The contribution is a solid composition of known pieces rather than a new core algorithm. \n\n-The ensemble is well-motivated, but the paper does not quantify robustness to reward drift/hallucinations or report sensitivity to reward weights, or is the method robust to the change of the reward weights?\n\n-Qualitative prompts are mostly short, single-sentence directives: Examples across Figs. 4, 6, 7 use one-liners like “A photo of a bird below a skateboard,” “A photo of four computer keyboards,” etc. The experiment lacks long, fine-grained, multi-clause prompts (styles, attributes, scene context), which is important where the user needs fine-grained control for the generation.\n\n-Compute/efficiency trade-offs is unknown (and might be higher than the baselines). GRPO with N=8, plus multi-expert scoring, implies non-trivial training cost; the paper doesn’t report inference-time overhead relative to the base generator (Qwen-Image)"}, "questions": {"value": "-For the reward weights, are they carefully tuned for different evaluation metrics/tasks? \n\n-From the ablation studies, although there are ablations for with/without ALP-CoT and with/without noise-reasoning (NR), a dedicated fine-grained/long-prompt could be helpful to manifest the controllability of the generation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5CF1WmRhP2", "forum": "x7NKufaPUu", "replyto": "x7NKufaPUu", "signatures": ["ICLR.cc/2026/Conference/Submission17304/Reviewer_4HMv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17304/Reviewer_4HMv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969155073, "cdate": 1761969155073, "tmdate": 1762927238828, "mdate": 1762927238828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ComposeAnything, a training-free framework for compositional text-to-image generation. The method replaces random Gaussian noise with composite object priors — structured initializations built by combining objects generated from pretrained diffusion models, arranged using LLM-derived 2.5D layouts. During sampling, two mechanisms are introduced: (1) object-prior reinforcement and (2) spatial-controlled attention, both intended to better preserve compositional structure. Experiments on T2I-CompBench and NSR-1K show modest quantitative improvements and visually more coherent multi-object results compared to prior inference-only approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Simple, practical design: The approach can be easily applied to existing diffusion backbones (e.g., SDXL, SD3, FLUX) without additional training.\n\nReasonable empirical validation: The paper provides quantitative and qualitative comparisons, as well as ablation studies for the two proposed modules.\n\nDirectionally relevant: The work contributes to a growing trend of inference-time structure control — integrating symbolic reasoning (via LLMs) with generative diffusion processes.\n\nReadable and reproducible: The implementation details are clear and the modular design is straightforward for replication."}, "weaknesses": {"value": "1. Limited originality — essentially RealCompo + RPG + InitNO.\n\nThe main components of ComposeAnything are all well-established:\n\nLLM-based prompt decomposition and regional layout reasoning come directly from RPG (Yang et al., 2024), which already performs prompt parsing and region-wise diffusion using MLLMs.\n\nComposite priors are conceptually equivalent to the layout-conditioned inputs used in RealCompo (Zhang et al., 2024), which integrates spatial-aware priors during inference for improved compositional control.\n\nNoise initialization as an optimization variable mirrors InitNO (Guo et al., CVPR 2024), which formalized structured or optimized noise for compositional alignment.\n\nComposeAnything combines these three ingredients (layout → structured noise → spatial attention) in a straightforward way without introducing a new modeling mechanism or theoretical insight. The technical novelty is minimal.\n\n2. Dependence on external systems.\nThe system’s success hinges heavily on GPT-4.1 for layout reasoning. \n\n3. Lack of mechanistic understanding.\nThe paper doesn’t analyze why composite priors improve generation — whether they guide global attention, stabilize denoising, or simply bias the output distribution. Without such analysis, it’s unclear what the method contributes beyond empirical heuristics.\n\n4. Evaluation is adequate but not conclusive.\nImprovements on internal benchmarks (T2I-CompBench, NSR-1K) are moderate and not validated with broader or standardized compositional datasets. Missing baselines such as LayoutDiffusion or ControlNet limit the scope of the claimed “state-of-the-art” status."}, "questions": {"value": "How does the method differ technically and conceptually from partial-denoising approaches like RPG?\nTo what extent do improvements depend on GPT-4.1’s layout accuracy?\nCan the model handle complex overlapping objects or scenes with more than 5–10 regions?\nDoes the “composite prior” generalize to other modalities (e.g., depth, segmentation) or only visual layout?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1BvyG7PyO6", "forum": "x7NKufaPUu", "replyto": "x7NKufaPUu", "signatures": ["ICLR.cc/2026/Conference/Submission17304/Reviewer_XjjY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17304/Reviewer_XjjY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148013834, "cdate": 1762148013834, "tmdate": 1762927238329, "mdate": 1762927238329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}