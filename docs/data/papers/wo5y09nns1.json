{"id": "wo5y09nns1", "number": 2302, "cdate": 1757054610149, "mdate": 1762999003525, "content": {"title": "SAFE: Improving LLM Systems using Sentence-Level In-generation Attribution", "abstract": "Large Language Models (LLMs) are increasingly applied in various science domains, yet their broader adoption remains constrained by a critical challenge: the lack of trustworthy, verifiable outputs. Current LLMs often generate answers without reliable source attribution, or worse, with incorrect attributions, posing a barrier to their use in scientific and high-stakes settings, where traceability and accountability are paramount. To be reliable, attribution systems require high accuracy for short-length attribution on retrieved data, i.e., attribution to a sentence within a document rather than the entire document. We propose SAFE, a Sentence-level A ttribution FramEwork for Retrieve-Augmented Generation (RAG) systems that attributes generated sentences during generation. This allows users to verify sentences as they read them and correct the model when the attribution indicates the generated text is not grounded in the documents, increasing the safety of LLM systems. This framework consists of two steps: predicting the required number of references for a sentence, and attributing the sentence. Our approach achieved 95% accuracy in the first step, which translated to 2.1∼6.0% improvements in the accuracy (normalized for maximum possible accuracy) of all attribution algorithms in our clean dataset, when compared to their top-1 accuracy. We also applied SAFE in real-world scenarios with documents containing hundreds to thousands of sentences. In these settings, SAFE reliably attributed sentences to their source documents, demonstrating that the method generalizes beyond controlled benchmarks. The SAFE framework and the training dataset are publicly available on GitHub.", "tldr": "This paper proposes a sentence-level attribution framework that attributes generated text back to the RAG'ed documents during generation, allowing to verify the truthfulness of the text before the answer is complete.", "keywords": ["Large Language Models; Attribution; Software"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3a6158f4e582ff1a703f2b20e22533ed242d1191.pdf", "supplementary_material": "/attachment/f386768325355ffa5ab20a9305217d624473314c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SAFE, a sentence-level in-generation attribution framework for RAG systems. The framework is model-agnostic and can be deployed on any downstream task of any LLM. For each sentence generated by the LLM, it searches for appropriate references to help users verify the reliability of the generated content. The framework consists of two main steps: when the LLM generates a sentence, a small classifier predicts how many citations (0/1/>1) are needed, and then a standardized attributor assigns the sentence to the most relevant reference sentences."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper presents a clear and practical engineering work.\n* The provided code repo is easy to read and use, showing good reproducibility.\n* The framework can be effectively applied on low-end machines, such as personal devices that only have API access to LLMs.\n* The authors also contribute a manually cleaned version of the HAGRID dataset."}, "weaknesses": {"value": "* **The proposed framework lacks novelty.** It mainly combines two steps, pre-attribution (predicting the number of citations) and attribution (assigning references). This limits the paper’s contribution to an engineering solution instead of a innovation.\n* **The technical methods used are quite standard and outdated.** For pre-attribution, the authors test Random Forest (RF), XGBoost (XGB), Multi-Layer Perceptron (MLP), and TabularNet (TN), all of which are common lightweight models. The attribution part also relies on conventional techniques.\n* **The experimental section is not convincing.** There is a lack of comparison with existing solutions, and the real-world application section only provides example-level demonstrations, which do not sufficiently prove the framework’s effectiveness."}, "questions": {"value": "As noted in the limitation section, sentence-level attribution has an inherent issue: retrieved sentences often lose contextual meaning, which may cause misinterpretation in certain cases. Conversely, a single generated sentence by the LLM can be very complex and may require multiple references. I hope the authors can further explain how these issues are mitigated in downstream tasks, especially when using the proposed framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FbUHA7FbAD", "forum": "wo5y09nns1", "replyto": "wo5y09nns1", "signatures": ["ICLR.cc/2026/Conference/Submission2302/Reviewer_EhhC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2302/Reviewer_EhhC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760779684279, "cdate": 1760779684279, "tmdate": 1762916184694, "mdate": 1762916184694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "eADMJ3nuYE", "forum": "wo5y09nns1", "replyto": "wo5y09nns1", "signatures": ["ICLR.cc/2026/Conference/Submission2302/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2302/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762999002710, "cdate": 1762999002710, "tmdate": 1762999002710, "mdate": 1762999002710, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SAFE, a Sentence-level Attribution Framework for RAG systems designed to improve the trustworthiness and verifiability of LLM outputs. SAFE attributes generated sentences using a light-weight design: First is through a Pre-attribution step: A classifier predicts whether a generated sentence requires zero, one, or multiple references; Then a \"Attribution\" step: Based on the pre-attribution prediction, the system selects an appropriate attribution algorithm (such as embedding-based methods, fuzzy string matching, BM25, or SPLADE) to find the matching quote(s) from the source document."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Open source framework that is very light weight\n2. HAGRID-Clean Dataset: The authors provide a manually cleaned version of the HAGRID attribution dataset to address issues with noisy data and inconsistent referencing in existing benchmarks. (not sure this dataset will be released, though)"}, "weaknesses": {"value": "1. Lack comparison to various methods working on grounded generation, for example directly use NLI model [1], or more advanced methods([2], [3], just to named a few.)\n2. The evaluation and comparison is on the same distribution, showing little generalization testing\n3. The real-world testing section do not have evaluation results\n\n\n[1] Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generatetext with citations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.\n[2] Hsu, I., Wang, Z., Le, L. T., Miculicich, L., Peng, N., Lee, C. Y., & Pfister, T. (2024). Calm: Contrasting large and small language models to verify grounded generation. arXiv preprint arXiv:2406.05365.\n[3] Maheshwari, H., Tenneti, S., & Nakkiran, A. (2025). CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction. arXiv preprint arXiv:2504.15629."}, "questions": {"value": "1. During cleaning the dataset, how do you make sure the consistency on \"how many references are needed\"? How aligned it is across different annotators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YCc2YJ5CW7", "forum": "wo5y09nns1", "replyto": "wo5y09nns1", "signatures": ["ICLR.cc/2026/Conference/Submission2302/Reviewer_mk1a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2302/Reviewer_mk1a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864997143, "cdate": 1761864997143, "tmdate": 1762916184455, "mdate": 1762916184455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an attribution method for Retrieval-Augmented Generation (RAG) systems. Current RAG systems often provide no citations or cite entire documents, making fact-checking impractical, and the  SAFE framework, does so by having a classifier to determine the number of references it requires and then uses different attribution methods for references.\n\nTo train the pre-attribution classifier, the authors created HAGRID-Clean, a manually cleaned and re-labeled version of the HAGRID dataset, which they identified as being too noisy for this task. Their results show the PA classifier achieves ~95% accuracy on this new dataset and that the full SAFE framework improves the final attribution accuracy by 2.1–6.0% (normalized) compared to a standard top-1 baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well written paper:** The paper is very thorough and easy to follow and reproduce if wanted.\n2. **Data Contribution:** The creation of the HAGRID-Clean dataset is a good resource for the community. The authors identified that existing attribution datasets suffer from noise, such as \"over-referencing\" and \"under-referencing\". By manually reviewing and re-labeling the data based on the ideal number of references, they created a high-quality benchmark.\n3. **Strong Empirical Validation:** The framework consistently improves the (normalized) accuracy of all tested attribution algorithms (e.g., +5.96% for SC2, +2.09% for SPLADE) over a simple top-1 baseline."}, "weaknesses": {"value": "1. **Misleading \"In-Generation\" Claim and Outdated Baselines:** The paper's central claim of \"in-generation\" attribution is misleading. The framework operates post-hoc on a per-sentence basis; it classifies and attributes a fully generated sentence. This is fundamentally different from true in-generation attribution models (e.g., Gao et al., 2023; RARR) that co-generate text and citation markers. By not comparing against this dominant and more recent line of research (listed a few below), the paper's evaluation is disconnected from the current state-of-the-art. The baselines used (e.g., Fuzzy String Matching, BM25) are classical IR methods and do not represent strong, modern attribution baselines.\n\n2.  **High Risk of False Positives (Poor \"No Quote Found\" Detection):** The framework's reliability is critically undermined by its inability to handle ungrounded sentences. The authors note that most of their chosen attributors (SC1, SC2, BM25, MMR) *always* return a quote, even if it's irrelevant. This forces the system to rely almost entirely on the pre-attribution classifier's \"ZERO\" class to filter un-attributable content. If the PA classifier makes a mistake (e.g., classifies a hallucination as \"ONE\"), the system will *always* find and present an incorrect attribution, creating a false positive and violating the system's core goal of building trust.\n\n3.  **Fragile Pre-Attribution Classifier:** The best-performing PA classifier relies on a set of 24 hand-crafted textual features. This approach is notoriously fragile and prone to poor generalization. The fact that sentence embeddings performed significantly worse on the HAGRID-Clean dataset is highly concerning. It suggests the classifier is not learning the *semantic complexity* of a sentence but rather overfitting to superficial, stylistic heuristics present in the HAGRID-Clean data. This classifier is unlikely to be robust when applied to text from different domains (e.g., legal, medical) or output from more advanced LLMs.\n\n4.  **Limited Benchmark Evaluation:** The entire evaluation is conducted on HAGRID and WebGLM-QA. While the creation of HAGRID-Clean is a good contribution, the paper fails to evaluate its framework on other standard attribution or open-domain QA benchmarks (e.g., ASQA). This makes it impossible to contextualize its performance against the wider field of attribution research and assess its generalizability.\n\n5.  **Impractical Interaction Model and Unsubstantiated Latency Claims:** The proposed user-in-the-loop system requires a synchronous, blocking confirmation from the user after *every single sentence*. This would be impractically slow and tedious for any real-world application. The paper provides no discussion of the significant latency this introduces or any user studies to validate that this interaction is genuinely preferred. While the framework is designed for \"low-end machines, the claims about latency are unsubstantiated.\n\n[1] RARR: Researching and Revising What Language Models Say, Using Language Models. Gao et al., 2022\n\n[2] Enabling Large Language Models to Generate Text with Citations. Gao et al., 2023.\n\n[3] Attribute First, then Generate: Locally-attributable Grounded Text Generation. Slobodkin et al., 2024\n\n[4] GenerationPrograms: Fine-grained Attribution with Executable Programs. Wan et al., 2024\n\n[5] ContextCite: Attributing Model Generation to Context, Cohen-Wang et. al., 2024"}, "questions": {"value": "1.  **Comparison to SOTA:** How does SAFE's performance (in both attribution accuracy and end-to-end latency) compare to generative attribution models (e.g., Gao et al., 2023; RARR) that generate citations directly as part of the text? Why were these state-of-the-art methods, which are also \"in-generation,\" omitted as baselines?\n\n2.  **Dataset Annotation Quality:** Regarding the HAGRID-Clean dataset, what was the detailed annotation protocol? What were the qualifications of the human reviewers, and what was the inter-annotator agreement (IAA) for assigning the \"ideal\" number of references? This is critical for assessing the validity of the new dataset.\n\n3. **Robustness of Pre-Attribution:** Given that the textual-feature-based classifier outperformed embeddings on HAGRID-Clean, what evidence suggests it will generalize? Have you tested its robustness on out-of-domain data or text from more capable LLMs (e.g., GPT-4), which may not share the same stylistic artifacts as the dataset?\n\n4.  **Usability and User Preference:** Was any user study conducted to validate the sentence-by-sentence confirmation loop? Is this synchronous interaction truly preferred by users over a fully generated, asynchronously citable response? Furthermore, does the interactive correction shown in Use Case 1 measurably improve the factual groundedness of the LLM's *subsequent* sentences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Eo9bjInMGT", "forum": "wo5y09nns1", "replyto": "wo5y09nns1", "signatures": ["ICLR.cc/2026/Conference/Submission2302/Reviewer_mrMd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2302/Reviewer_mrMd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871716874, "cdate": 1761871716874, "tmdate": 1762916184153, "mdate": 1762916184153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the need for user-verifiable outputs in Retrieval-Augmented Generation (RAG) systems. The authors argue that providing entire documents is impractical for users who need to fact-check an LLM's response. Therefore, the paper proposes SAFE, a Sentence-level Attribution Framework that operates in-generation, providing users with the most relevant quotes from source documents as each sentence is being generated. This allows for immediate verification and correction.\n\nA secondary contribution is the manual cleaning and re-labeling of the HAGRID dataset to create HAGRID-Clean. This new dataset addresses noise and inconsistent labeling in the original, providing a more reliable benchmark for sentence-level attribution tasks.\n\nThe SAFE pipeline itself consists of two main stages: (1) train a \"Pre-attribution Classifier\" (PA) that predicts the number of quotes required (0, 1, or 2+) for a given sentence, and (2) select and operate a set of attribution algorithms (e.g., SPLADE, embedding similarity) that retrieve the actual quotes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposal of an \"in-generation\" (sentence-by-sentence) attribution system is a novel and highly practical contribution. This design directly addresses the user-experience bottleneck of post-hoc, document-level verification and empowers users to correct hallucinations in real-time.\n\nThe use of a lightweight Pre-attribution (PA) classifier first predicts the required number of quotes is an efficient design. As shown in Table 5, this two-step process improves the accuracy of the final attribution by 2.1-6.0% (normalized) over a simple top-1 retrieval baseline, optimizing the pipeline for its real-time goal.\n\nFinally, the authors creates HAGRID-Clean by manually cleaning, merging, and re-labeling the original HAGRID dataset, to provide a valuable, high-quality resource for future research in this specific domain."}, "weaknesses": {"value": "The two-step design creates its own limitations. As the paper notes, the system's ability to identify sentences requiring zero attribution is weak, as most attributors are not optimized for \"no match\" detection. The system is thus forced to conservatively find a quote, even for common knowledge.\n\nThe framework requires a dedicated dataset (like HAGRID-Clean) with sentence-level labels (0, 1, 2+) to train the Pre-attribution Classifier. This multi-stage pipeline, which requires pre-training a classifier on a highly specific dataset, presents a significant barrier to adoption compared to simpler, training-free retrieval methods.\n\nA major concern is the \"roofline\" accuracy in Table 5. Even when the Pre-attribution Classifier is perfect (using the \"True Label\"), the best-performing attributor (SPLADE) only achieves 78.11% accuracy. This suggests the core retrieval algorithms are a significant bottleneck, and even a perfect classifier cannot compensate for their low performance.\n\nThe system is strictly sentence-level, which may not attribute generated sentences that are synthesized from multiple, non-contiguous source sentences or that rely on the surrounding context of a quote for their meaning. The paper's own limitations section acknowledges this."}, "questions": {"value": "* The evaluation metric (Section 3.2.3) is very strict, requiring a match in the number of quotes (0, 1, or 2+) and a subset match of the quotes. Would a standard IR metric like Recall@k be a more practical measure?\n\n* The \"roofline\" accuracy in Table 5 at ~78% seems low, implying the core attributors are the main bottleneck. Given this low ceiling, what do the authors see as the most promising path forward: developing better attributors?\n\n* Could the authors comment on the latency vs. accuracy trade-off of using a more powerful, small LLM (e.g., 3B-8B parameters) as PA+the attributor themselves, rather than the proposed two-step process?\n\n* The limitations section notes that sentence-level attribution can miss context. Has a hybrid approach been considered, such as retrieving a \"chunk\" (e.g., the surrounding +/- n sentences) to present to the user alongside the primary attributed quote?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LmVZIfkN4j", "forum": "wo5y09nns1", "replyto": "wo5y09nns1", "signatures": ["ICLR.cc/2026/Conference/Submission2302/Reviewer_hF1X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2302/Reviewer_hF1X"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998057191, "cdate": 1761998057191, "tmdate": 1762916183950, "mdate": 1762916183950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}