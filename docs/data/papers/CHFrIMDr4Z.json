{"id": "CHFrIMDr4Z", "number": 21140, "cdate": 1758314203264, "mdate": 1763668037214, "content": {"title": "An Asset Foundation Model for Industrial Asset Performance Management", "abstract": "We introduce the Asset Foundation Model (AFM), a generative framework for asset performance management (APM) spanning high-value industrial assets and manufacturing processes. AFM applies across sectors such as energy, chemicals, manufacturing, utilities etc., by leveraging rich time-series data and event streams to provide a robust basis for next-generation APM solutions. A shared transformer backbone with lightweight heads supports forecasting, anomaly detection, and event querying. The model is pretrained on operational and simulator corpora, then fine-tuned on asset-specific histories for minimal effort adaptation, using per-sensor discrete tokenization for robustness. Beyond sensors, the AFM incorporates alarms, set-point changes, and maintenance logs via event tokens, enabling time-aligned “what/when” queries and high value applications such as root-cause triage, alarm suppression, and maintenance planning. In representative field deployments (e.g., ESPs and compressors), the AFM exceeds prior gains, delivers earlier warnings, and reduces false alarm minutes. Operator-oriented explanations based on attention rollout and integrated gradients highlight which sensors/events drove each alert, while natural language querying allow experts to “talk to the data” features. Calibrated prediction intervals from discrete to continuous with isotonic calibration support risk aware thresholds. On the theory side, we prove closed form bounds on quantization error and a Lipschitz stability result for discretization noise through the encoder, justifying sample efficient adaptation with frozen backbones. Public benchmarks corroborate competitive accuracy and calibrated coverage. The result is a versatile, scalable, and interpretable foundational framework that reduces the need for bespoke per-asset models. This is one of the foundational generative AI modeling works in the industrial domain, proposing a versatile foundational model with significant business impact on industrial asset management.", "tldr": "We propose a foundation model for high value industrial asset management which models the sensor signals, critical events, alarms with interpretable transformer architecture enabling multiple end applications for industrial operations and planning.", "keywords": ["Asset Foundation Model", "Foundation Model for Timeseries", "Industrial Asset Performance Management", "Generative AI for Operations", "Interpretable Transformer Architecture"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3be92d58dfe930ecaf74cb4c16e125ea7426c4d2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces an asset foundation model designed for industrial asset performance management, focusing on applying a shared transformer backbone across multiple tasks such as forecasting, anomaly detection, and event querying. The paper presents both theoretical insights and a real-world industrial case study to demonstrate the model’s effectiveness. The work is highly application-oriented, aiming to bridge modern foundation model design with practical infrastructure management challenges."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and well-motivated from an industrial perspective. The problem setting is relevant and timely. The proposed idea of a shared transformer backbone that can adapt to different tasks, such as detection and forecastin,g is intuitively appealing. The connection to real-world deployment adds credibility, and the paper does a good job of aligning the method with the realities of applied industrial settings."}, "weaknesses": {"value": "The paper lacks sufficient novelty and depth for a research-focused conference. The overall contribution feels more business-oriented and application-driven rather than technically innovative. There is no convincing empirical evidence that modeling forecasting, detection, and event querying jointly through a shared backbone brings measurable advantages over separate task-specific models. The paper also lacks comparative experiments on public benchmarks, which makes it difficult to assess the claimed benefits or generalizability of the approach.\n\nThe related work discussion is also incomplete. Prior works on unified representations for multiple time-series tasks, such as UniTS, are not discussed or compared against. Without this context or experimental validation, it is hard to position the paper’s contribution in the broader research landscape.\n\nGao, Shanghua, et al. \"Units: A unified multi-task time series model.\" Advances in Neural Information Processing Systems 37 (2024): 140589-140631."}, "questions": {"value": "Since the work targets industrial applications, what is the computational cost of deploying the shared transformer model in production? Are there latency or resource trade-offs compared to using separate models for each task?\n\nHow does this work differ from existing unified time-series foundation models such as UniTS? These models also use shared representations across multiple tasks; a clear comparison in methodology and results would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9G1ECaK5rb", "forum": "CHFrIMDr4Z", "replyto": "CHFrIMDr4Z", "signatures": ["ICLR.cc/2026/Conference/Submission21140/Reviewer_HsNm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21140/Reviewer_HsNm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760763986559, "cdate": 1760763986559, "tmdate": 1762941490279, "mdate": 1762941490279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents the Asset Foundation Model (AFM) for industrial asset performance management, featuring a pre-trainable transformer backbone with per-sensor tokenization and other advanced components. The model is designed for industrial environments with diverse sensors and critical real-time deployment. Theoretical analyses support its design, and a field case indicates promising results in detection and forecasting. AFM seeks to consolidate and simplify model deployment for tasks such as forecasting and anomaly detection, thereby promoting interpretability. However, the validity of the empirical claims is limited by concerns about dataset scale, unclear metric units, the lack of standard baselines, and the absence of public code."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The motivation is clearly stated and important for industrial operations, addressing real issues such as false alarms, transferability, and interpretability.\n- The methodology is well-designed and integrated, incorporating tokenization, event-channel design, calibrated prediction intervals, and interpretability, all of which are crucial for deployment.\n- There is a solid theoretical basis, supported in the appendices."}, "weaknesses": {"value": "- The errors reported in Table 1 are very small and difficult to interpret, as it seems the metrics are based on normalized signals, which could be misleading.\n- No standard baselines are provided, making it unclear whether the improvements are due to architecture choices or pretraining scale.\n- The dataset and pretraining scale are not specified, such as the number of sequences, total tokens/hours, or event counts.\n- The manuscript could clearly specify whether improvements are due to tokenization, pretraining, calibration, or other factors.\n- No public code or comprehensive reproducibility artifacts are available."}, "questions": {"value": "1) What are the dataset and pretraining corpus statistics regarding the number of sequences, total tokens/hours, or event counts?\n2) What parts of AFM influence the gains? Is it due to tokenization, pretraining, calibration, or other factors? Please provide a concise component ablation study to assess it.\n3) How do the obtained results compare to a baseline?\n4) A detailed reproducibility appendix (including full hyperparameters, training steps) should be included, and possibly the pseudo-code."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MRTZrSbsUi", "forum": "CHFrIMDr4Z", "replyto": "CHFrIMDr4Z", "signatures": ["ICLR.cc/2026/Conference/Submission21140/Reviewer_dQem"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21140/Reviewer_dQem"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669978046, "cdate": 1761669978046, "tmdate": 1762941488718, "mdate": 1762941488718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Asset Foundation Model (AFM), a transformer-based framework for industrial asset performance management that unifies forecasting, anomaly detection, and event querying. It leverages multimodal sensor and event data with tokenization and uncertainty calibration to improve interpretability and robustness across diverse industrial systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a relevant and timely problem in the field, with a clearly stated motivation and problem formulation.\n\n2. The authors conduct experiments across multiple datasets or settings with good results\n\n3. The manuscript is well organized and written in a clear and logical manner, making it easy to follow."}, "weaknesses": {"value": "1. The proposed method does not sufficiently highlight its novelty. The paper should better emphasize how it differs from or advances beyond existing approaches in this area.\n\n2. The experimental section lacks strong and representative baselines. It would be more convincing to compare against state-of-the-art large models, or at least include fine-tuned versions of existing models as baselines to demonstrate the relative effectiveness of the proposed method.\n\n3. The work is difficult to reproduce due to the absence of released code and model weights. The authors are encouraged to make their implementation publicly available to enhance transparency and reproducibility."}, "questions": {"value": "What about the training cost in this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s2Ci6c9Ofb", "forum": "CHFrIMDr4Z", "replyto": "CHFrIMDr4Z", "signatures": ["ICLR.cc/2026/Conference/Submission21140/Reviewer_KEX6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21140/Reviewer_KEX6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973770743, "cdate": 1761973770743, "tmdate": 1762941487886, "mdate": 1762941487886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Asset Foundation Model (AFM), a transformer-based framework for industrial asset performance management that unifies forecasting, anomaly detection, and event querying. It leverages multimodal sensor and event data with tokenization and uncertainty calibration to improve interpretability and robustness across diverse industrial systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a relevant and timely problem in the field, with a clearly stated motivation and problem formulation.\n\n2. The authors conduct experiments across multiple datasets or settings with good results\n\n3. The manuscript is well organized and written in a clear and logical manner, making it easy to follow."}, "weaknesses": {"value": "1. The proposed method does not sufficiently highlight its novelty. The paper should better emphasize how it differs from or advances beyond existing approaches in this area.\n\n2. The experimental section lacks strong and representative baselines. It would be more convincing to compare against state-of-the-art large models, or at least include fine-tuned versions of existing models as baselines to demonstrate the relative effectiveness of the proposed method.\n\n3. The work is difficult to reproduce due to the absence of released code and model weights. The authors are encouraged to make their implementation publicly available to enhance transparency and reproducibility."}, "questions": {"value": "What about the training cost in this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s2Ci6c9Ofb", "forum": "CHFrIMDr4Z", "replyto": "CHFrIMDr4Z", "signatures": ["ICLR.cc/2026/Conference/Submission21140/Reviewer_KEX6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21140/Reviewer_KEX6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973770743, "cdate": 1761973770743, "tmdate": 1763752011035, "mdate": 1763752011035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}