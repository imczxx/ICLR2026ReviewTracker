{"id": "i55jA7FSvZ", "number": 1009, "cdate": 1756827966599, "mdate": 1759898232250, "content": {"title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs", "abstract": "Processing long-form audio is a major challenge for Large Audio Language models (LALMs). These models struggle with the quadratic cost of attention ($\\mathcal{O}(N^2)$) and with modeling long-range temporal dependencies. Existing audio benchmarks are built mostly from short clips and do not evaluate models in realistic long context settings. To address this gap, we introduce **AudioMarathon**, a benchmark designed to evaluate both understanding and inference efficiency on long-form audio. **AudioMarathon** provides a diverse set of tasks built upon three pillars: long-context audio inputs with durations ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of 2,250 to 7,500 audio tokens, respectively, full domain coverage across speech, sound, and music, and complex reasoning that requires multi-hop inference. \nWe evaluate state-of-the-art LALMs and observe clear performance drops as audio length grows. \nWe also study acceleration techniques and analyze the trade-offs of token pruning and KV cache eviction. The results show large gaps across current LALMs and highlight the need for better temporal reasoning and memory-efficient architectures. \nWe believe **AudioMarathon** will drive the audio and multimodal research community to develop more\nadvanced audio understanding models capable of solving complex audio tasks.", "tldr": "", "keywords": ["Benchmark", "Audio LLM", "Multi-modal Large Language Models", "Efficiency"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5138ae3a1a99cdc450de1a729f17be666369d0df.pdf", "supplementary_material": "/attachment/401c70c11fccc97a11fb08063b314de965ef2be9.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces AudioMarathon, a comprehensive benchmark for long audio understanding, spanning 10 audio tasks. The authors evaluate several sota LALMs on this benchmark, revealing that the methods struggle more as audio lengths increases.  They also study various acceleration techniques, such as token pruning and KC-cache eviction, to analyze their tradeoffs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and the objectives are clearly stated.\n- The paper includes several results.\n- The focus of the paper in evaluating LALMs on long-duration clips is of sufficient importance."}, "weaknesses": {"value": "- The papers claims that most of the existing benchmarks entail short audio clips, and therefore they don't capture \"complexity of real-world scenarios such as meetings, podcasts, and extended dialogues\" (lines 107-108). However, the duration of the clips in AudioMarathon is in the range [90,300]seconds, which in my opinion is quite shorter than the average duration of meetings/podcasts. This is why the benchmark provided in \"Orevaoghene Ahia et al., BLAB: Brutally Long Audio Bench, 2025\" seems more appropriate as the average audio duration is 51minutes.\n- The results about compression and peak memory usage are somewhat expectable and they do not add much value to the paper. \n- The paper lacks multiple details and clarifications --> see questions below.\n-  It's not clear to me how the proposed \"Frame\" compression method works in practice. The description at lines 268-268 is insufficient.\n- In Figure 7, it seems like the SnapKV line is never displayed in the 4 plots, and so does it seem for KNorm method on figures (b), (c), and (d). Are they completely overlapping with the other methods or what?"}, "questions": {"value": "**Questions**:\n- Regarding step 2/3, can the authors elaborate more on the logic behind the concatenation of and merging of clips to have longer sequences and the corresponding “tool design”? The description in Section 2.2 is a bit vague.\n- Regarding step 5, why did the authors only review 10% of the data? How many samples needed to be fixed? Unless the failure ratio is extremely low, then I think it would be better to double check all the samples. \n- Why Flamingo-2 achieves only 1% of WAR? It seems a big outlier.\n- Why in Table 4, random pruning for Qwen2.5-Omni-3B oftentimes perform better than the other methods which perform more accurate compressions?\n- In the paragraph “How do token pruning methods affect performance in LALMs?”, how does the authors explain the fact that Qwen2.5-Omni-3B “benefits from token pruning”? Do they mean that the model benefits when the ratio is 30% or even up to 90%? It seems to deteriorate rather than improve, and this makes sense given the high compression ratio. \n- Can the authors provide some examples of failure cases in some of the considered tasks? Or at least compare when some models fail/succeed. Based on how and where the models make the mistakes, I think it would be easier to devise better techniques to improve their capabilities on long-context scenarios. \n\n\n**Typos**:\n\n- Duplicated reference: Qwen2.5-omni technical report by Jin Xu et al., 2025.\n- Missing full stops on all image captions.\n- Missing acronym definition: MCQ.\n- In Table 3: Close-source —> Closed-source."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2uzGmmtuaU", "forum": "i55jA7FSvZ", "replyto": "i55jA7FSvZ", "signatures": ["ICLR.cc/2026/Conference/Submission1009/Reviewer_Ct46"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1009/Reviewer_Ct46"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761522017737, "cdate": 1761522017737, "tmdate": 1762915656019, "mdate": 1762915656019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AUDIOMARATHON, a long-audio benchmark that aggregates 10 tasks spanning speech, environmental audio, and music domains. By studying the performance of open-source and closed-source models on AUDIOMARATHON, the paper shows that current LALMs are still underperforming with long input. Moreover, this paper studies inference-time acceleration techniques for LALMs, including token pruning and KV-cache eviction, and shows their trade-offs to performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Focus on long-form audio and well-thought-out tasks spanning across each domain in audio.\n2. Comprehensive experiment results comparing open-sourced and closed-sourced LALMs\n3. Analysis on performance trade-off focusing on inference-time efficiency\n4. Clear presentation in the first half of the paper for the dataset specification"}, "weaknesses": {"value": "Formulation of the “long-audio” datasets needs clearer specification.\n1. The work appears to add limited new “long-audio” material. Figure 2 suggests that all tasks are long-form and created by concatenation, which is misleading. Appendix D indicates that the only newly introduced long-form set is RACE, and its audio is produced by TTS from a text dataset. For environmental audio and music tasks, it seems the audio is taken directly from the original sources.\n2. For the “multi-hop” or “complex reasoning” component, namely RACE, the evaluation largely probes ASR capability and downstream text understanding. An audio-specific, more acoustic-centered multi-hop design would better substantiate the “complex reasoning” claim.\n\nPresentation of experiments and the path from results to conclusions is confusing. The paper would offer more actionable guidance for LALM development with tighter exposition, more focused ablations, and conclusions that are directly supported by the evidence.\n1. Frame pruning is under-specified. The text motivates time-aligned pruning and claims that Frame preserves rare short acoustic events, but it does not provide a precise algorithmic description that enables reproduction, for example window size, stride, scoring rule, tie breaking, and any layer-wise schedules. Appendix A mainly explains why pruning occurs at the second layer rather than detailing how Frame operates.\n2. KV-cache eviction analysis is limited. Figure 7 shows peak memory across policies, yet the curves look similar and appear driven primarily by the number of tokens. The paper does not report accuracy trade-offs under different eviction settings.\n3. Length attribution is missing. The claim that “current models fail at long audios” would be stronger with a length-controlled comparison. The paper aggregates minute-scale results and human baselines, but it does not include short-context baselines for the same tasks or a duration sweep. Without a length-matched ablation, it is difficult to attribute errors to long context rather than task or domain effects.\n4. Figure-level clarity issues. The caption of Figure 9 does not make clear which pruning method each panel uses, and the nearby text only mentions “varying token pruning ratios” and “maximum F1 across settings,” which prevents mapping curves to methods. In Figure 5, the caption states that “Frame consistently outperforms other methods,” which seems overstated, since in later segments of ASC and SGR other methods match or surpass Frame at higher token counts.\n5. Evidence for “task-aware audio token compression is essential” is insufficient. The Results section makes a broad necessity claim, but the curves mainly show about a 20 percent latency reduction in several regimes. The paper does not provide controlled accuracy-at-fixed-latency comparisons against strong baselines, nor latency-at-fixed-accuracy tables that would justify essentiality rather than usefulness.\n6. Actionable request tied to the claim (line 365). Where the paper asserts that token compression is “essential,” please add controlled accuracy-at-fixed-latency or latency-at-fixed-accuracy tables. The current evidence supports that certain pruning methods can maintain similar accuracy with about a 20 percent latency drop. It does not yet support the stronger claim that token compression is essential."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mlZzXdxt6y", "forum": "i55jA7FSvZ", "replyto": "i55jA7FSvZ", "signatures": ["ICLR.cc/2026/Conference/Submission1009/Reviewer_tjz8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1009/Reviewer_tjz8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761594635957, "cdate": 1761594635957, "tmdate": 1762915655831, "mdate": 1762915655831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce AudioMarathon, a novel and comprehensive benchmark designed to evaluate Large Audio Language Models (LALMs) on long-form audio. The work is motivated by the significant gap in existing benchmarks, which primarily use short audio clips and thus fail to test models on realistic, minute-scale tasks. AudioMarathon features audio inputs from 90 to 300 seconds across 10 diverse tasks covering speech, music, and environmental sounds. Using this benchmark, the paper conducts a large-scale evaluation of 16 state-of-the-art LALMs, revealing significant performance degradation as audio length increases and a substantial gap compared to human performance, particularly on tasks requiring temporal reasoning. A key part of the study is a systematic analysis of inference efficiency techniques like token pruning and KV-cache eviction, highlighting the critical trade-offs between computational cost and model accuracy for long-context audio processing."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Significant Motivation: The motivation for AudioMarathon is highly significant. Most existing benchmarks and datasets for large audio models focus on short audio clips, whereas many real-world applications require the processing of long-form audio. This benchmark effectively addresses that gap.\n2. Comprehensive and Diverse Scenarios: The benchmark includes 10 different sub-tasks that cover a wide range of audio-related scenarios, with tasks designed to evaluate both semantic and acoustic capabilities.\n3. Robust Data Pipeline: The paper details a well-designed data pipeline that includes source selection, automated construction, and manual verification, which together ensure the reliability and quality of the benchmark.\n4. Sufficient Model Evaluation: The study provides a thorough evaluation by testing 16 different audio models and including human evaluation scores as a baseline. This clearly demonstrates the current capabilities of models on this new benchmark.\n5. Valuable Exploration of Efficiency Optimization: In the context of long audio, efficiency is critical. The paper's exploration of token pruning and KV-cache eviction demonstrates that while these techniques can reduce memory usage and inference latency, there remains significant room for improvement and further research for the audio modality."}, "weaknesses": {"value": "1. The Definition of \"Long Audio\" Could Be Extended: While the 300-second duration pushes the limits of many current LALMs, real-world scenarios such as meetings, movies, lectures, and podcasts often feature much longer audio, potentially exceeding 30 minutes. It would be more impactful if the paper could demonstrate model performance on even longer audio inputs.\n2. Lack of Specific Multi-Speaker Tasks: As noted in the paper, tasks like Speaker Age Recognition (SAR) are performed in multi-speaker contexts. However, more direct multi-speaker tasks, such as speaker counting or diarization, are missing. Future work could benefit from incorporating such tasks.\n3. Limited Evaluation Format: With the exception of ASR, all other tasks are framed as multiple-choice questions (MCQs). This format, while easy to evaluate, may not fully capture a model's deep reasoning or summarization capabilities. The benchmark would be more valuable if it included more open-ended question-answering tasks.\n\nMinor comments that did not impact the score:\n1. The caption for Figure 6 appears to be incomplete.\n2. The abbreviation \"MCQ\" is used without prior definition.\n3. As the authors mention in their limitations, it is hoped that this work can be extended to more languages in the future."}, "questions": {"value": "1. Details of Human Participation: Could you provide more details about the human involvement in this study? Specifically, for the benchmark construction (Step 5. Manual Verification), what were the detailed criteria used for the review? I could not find them in Appendix D. Furthermore, for the Human Evaluation scores in Table 3, could you elaborate on the methodology, including the number of participants, whether they were native English speakers, and their level of expertise for the tasks?\n2. Random vs. Well-Designed Strategies: I noticed that in both token pruning and KV cache eviction, the Random method performs surprisingly well. What do you believe is the reason for this? Does it suggest that the more sophisticated, well-designed strategies are not well-suited for the audio modality, or are there other factors?\n3. ASR Curve in Figure 5: In Figure 5(d), the performance of the Frame method continuously improves as latency decreases from 65s to ~45s. This is an expected trade-off, where less pruning (higher latency) leads to better accuracy. However, the curves for other strategies seem to start at a lower latency point. Could you provide the performance of the other strategies in the 45s-65s latency range for a more direct comparison, or is it that their maximum latency (with minimal pruning) is already below 45s?\n4. Details of the Frame Strategy: Could you provide a more detailed technical description of the Frame pruning strategy? The paper describes it as a \"time-aligned token pruning strategy,\" but more specifics on its implementation would be very helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "87aqQvewG3", "forum": "i55jA7FSvZ", "replyto": "i55jA7FSvZ", "signatures": ["ICLR.cc/2026/Conference/Submission1009/Reviewer_Eib9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1009/Reviewer_Eib9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742281961, "cdate": 1761742281961, "tmdate": 1762915655674, "mdate": 1762915655674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AudioMarathon that focuses on evaluation of long audio understanding. AudioMarathon is large-scale, over 200 seconds per sample on average, and across all domains (speech, sound, and music). AudioMarathon is curated with 7 different sub-categories, which covers more capabilities than prior benchmarks. The paper also studies token pruning methods and KV-cache methods in baselines for efficient long audio inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed AudioMarathon benchmark is a solid contribution to the audio understanding community. Current long audio understanding benchmarks such as BLAB are limited to speech and other challenging benchmarks are focused on reasoning/skills instead of length. AudioMarathon combines the strengths of both sides and brings the distinct challenging capabilities to long audio. \n\nThe dataset curation pipeline seems quite delicate. Fig 2 makes it very easy to understand and also straightforward for the community to reproduce or curate related training data for research development. There is also manual verification to keep quality high."}, "weaknesses": {"value": "Despite the fact that the AudioMarathon benchmark quite solid, it is more of an engineering contribution rather than methodology contribution. Its novelty is limited in that BLAB introduced long audio understanding (despite focused on speech) and Audio Flamingo 2 introduced LongAudioBench (focused on AQA). From this perspective, the proposed AudioMarathon is a complement to existing benchmarks with limited novelty and methodological contribution. \n\nThe baselines are not extensive enough: Table 3 only includes a limited number of models, but there are many more good ones that are publicly available. It is unimaginable that a benchmark paper does not include enough major models. \n\nIn the token pruning part, it is unclear what the proposed FRAME method is. It only has one sentence of description (L267-268). While the paper lists this as contribution, the so-called FRAME does not show any improvements in Table 4. \n\nI appreciate the large-scale ablations for the token pruning and KV cache methods, but they are just ablation studies and the paper does not seem to reveal any valuable information from these ablations. There is no in-depth analysis on why certain methods are better than others, or what are some newly discovered weaknesses of baseline models that should be addressed. \n\nIn summary, I think the paper proposes a solid benchmark that complements prior ones in terms of audio lengths, capabilities, and domains. However, there is limited novelty and scientific discoveries presented in this paper."}, "questions": {"value": "- What is the FRAME method?\n- Why is the proposed FRAME only similar to baseline pruning methods and you list it as a contribution?\n- What are some scientific discoveries of the large-scale ablation studies done in section 4-5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PTF7ow5XKD", "forum": "i55jA7FSvZ", "replyto": "i55jA7FSvZ", "signatures": ["ICLR.cc/2026/Conference/Submission1009/Reviewer_K8yV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1009/Reviewer_K8yV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899524205, "cdate": 1761899524205, "tmdate": 1762915655499, "mdate": 1762915655499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}