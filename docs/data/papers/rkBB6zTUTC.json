{"id": "rkBB6zTUTC", "number": 24505, "cdate": 1758357481910, "mdate": 1759896762564, "content": {"title": "Hacking LM Arena via LLM Identification with Interpolated Preference Learning", "abstract": "Voting-based leaderboards, such as LM Arena, have become the predominant\nmethod for evaluating large language models (LLMs) on open-ended tasks, with\ntheir fairness fundamentally depending on the anonymity of model responses.\nWhile prior work has shown that simple statistical features can be used for LLM\nidentification, these methods could be easily defended and lack the power to distinguish\nbetween stylistically similar models. To further investigate such a risk in\nmore sophisticated ways, we introduce a model-driven LLM identification framework\nvia learning from Interpolated preference data (I-PREF). Our approach\nutilizes a triplet ranking loss to train the detector model, a process augmented\nwith synthetic hard negatives generated via copy model fine-tuning and model\ninterpolation. This strategy enables the detector to learn deep relational patterns\nbeyond superficial statistics. We further enhance performance and stabilize training\nthrough adaptive and iterative curriculum learning. Experimental results show\nthat I-PREF significantly outperforms the existing baselines, achieving improvements\nof about 30% in accuracy and 24% in AUROC.", "tldr": "We presented I-PREF , a new model-driven approach to identify the source of LLM responses,  which leverages triplet-based training with adaptive and iterative curriculum learning, supported by synthetic negatives generated via model interpolation", "keywords": ["LLM identification"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d5b0080cd2d62858bcb16d884742d7a23807ef9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces I-PREF (Interpolated Preference Learning), a novel framework for identifying the source of large language model (LLM) outputs. The work exposes a potential vulnerability in open evaluation platforms such as LM Arena, where anonymity is assumed in human preference voting.\nI-PREF combines model interpolation (to synthesize hard negative samples) with triplet preference learning and adaptive curriculum training, enabling a detector to learn subtle stylistic cues of specific models. Experiments on datasets including the LM Arena human preference data show that I-PREF outperforms baseline methods (e.g., TF-IDF, BoW) by roughly +30% accuracy and +24% AUROC across multiple models (GPT-4o, Gemini-Pro, Claude-4). The findings highlight that even anonymized evaluation systems can be compromised through model-specific fingerprinting."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Innovative framework – Combines model interpolation and preference learning for LLM source detection.\n\nStrong empirical performance – Significantly surpasses traditional feature-based baselines.\n\nData-efficient and low-cost – Requires only small datasets and no API access to the target model.\n\nRobustness – Performs consistently across different architectures and settings."}, "weaknesses": {"value": "The paper’s overall motivation is not entirely convincing. If the goal is to increase a model’s ranking or manipulate leaderboard outcomes, simpler techniques such as model watermarking could achieve this more directly without the need for a complex preference-learning framework. Moreover, the threat model remains vague throughout the paper. It is unclear whether the attacker is assumed to be a third-party observer attempting to de-anonymize model outputs, or a model developer seeking to improve their system’s ranking. This ambiguity weakens the justification and makes it difficult to assess the real-world relevance of the proposed attack scenario.\n\n\nThe interpolation parameter (α) is a key design choice in the proposed I-PREF framework, yet the paper does not include any ablation or sensitivity analysis on how varying α influences detection performance. While the authors describe a curriculum that gradually increases α, a quantitative study would be necessary to demonstrate robustness and justify the chosen configuration.\n\nThe authors argue that they train a copy model to avoid high API costs, implying access to GPU resources but limited API usage. In such a setup, it would be reasonable to leverage strong open-source models for local inference to obtain more diverse responses (but they did not do that). Although I understand that the target models (e.g., GPT-4o, Claude, Gemini) are closed-source, exploring additional open-source substitutes could have strengthened the experimental design and improved the generalizability of results."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IgKpTD04Ie", "forum": "rkBB6zTUTC", "replyto": "rkBB6zTUTC", "signatures": ["ICLR.cc/2026/Conference/Submission24505/Reviewer_xXHV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24505/Reviewer_xXHV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761354723905, "cdate": 1761354723905, "tmdate": 1762943106035, "mdate": 1762943106035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the risk that voting-based, anonymity-preserving leaderboards (e.g., LM Arena) can be de-anonymized — i.e., that a voter (or attacker) can identify which model produced a response and then use that ability to manipulate leaderboard outcomes. The authors propose I-PREF, a model-driven identification pipeline that trains a detector to prefer responses from a specific target LLM. Key ideas are: (1) constructing hard negatives by fine-tuning a copy model to mimic the target and then interpolating model weights to produce intermediate (“middle-difficulty”) responses; (2) triplet preference learning (target > interpolant > other) with a weighted triplet loss; and (3) adaptive, iterative curriculum learning that switches between triplet and pairwise losses and progressively increases interpolation difficulty. Experiments on two datasets (Alpaca and an Arena human-preference sample) and 12 LLMs (open and API) show I-PREF substantially outperforms simple statistical baselines (BoW, TF-IDF, length) on Accuracy and AUROC, including within-family distinctions (e.g., GPT-4o vs GPT-4o-mini). Ablations confirm benefits of triplet loss, adaptive curriculum, and iterative training. The paper discusses dual-use risks and frames the work as defensive research intended to inform more robust leaderboard defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear, practical method. The interpolated negative synthesis is an economical way to produce graded hard negatives without additional API calls; combined with triplet learning and adaptive curriculum it is conceptually simple and effective.\n\n* Strong empirical gains over surface baselines. Across multiple target LLMs and two datasets, I-PREF shows large improvements in Accuracy/AUROC (e.g., average Accuracy up to ≈0.887–0.982 for different targets) and is robust in within-family comparisons where TF-IDF / BoW fail. Results and ablations (Tables 1–4, Table 3 ablation) support the claims."}, "weaknesses": {"value": "* Baselines are too narrow. Comparisons are limited to simple feature-based detectors (TF-IDF, BoW, length). Stronger baselines (fine-tuned detectors trained on more sophisticated features, instruction-fingerprinting methods, watermark/fingerprint detectors, or recent instruction-fingerprinting papers) are missing; this makes it hard to position I-PREF’s advantage relative to state-of-the-art identification defenses/attacks.\n\n* Limited evaluation on defender countermeasures. The paper notes LM Arena style/length normalization defenses but does not evaluate whether those or more aggressive countermeasures (e.g., paraphrasing, stochastic decoding, automated post-processing) degrade I-PREF effectiveness. Empirical tests of plausible defenses would strengthen the defensive framing."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Djjj1IeGTD", "forum": "rkBB6zTUTC", "replyto": "rkBB6zTUTC", "signatures": ["ICLR.cc/2026/Conference/Submission24505/Reviewer_QwZn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24505/Reviewer_QwZn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710752580, "cdate": 1761710752580, "tmdate": 1762943105589, "mdate": 1762943105589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces I-PREF, a training framework for LLM source attribution that targets the identification of a specific model from its responses. The method builds a detector by first training a copy model to imitate the target, then generating intermediate hard negatives via weight interpolation between the copy and its initialization. The detector is optimized with triplet preference learning that contrasts the target response, the interpolated negative, and the most similar non-target response selected by embeddings. An adaptive curriculum switches between pairwise and triplet objectives based on the margin, and an iterative schedule increases difficulty by raising the interpolation coefficient. The aim is to enable practical de-anonymization of LLM outputs and to assess potential risks to rating systems such as LM Arena.\n\nExperimentally, the paper evaluates across 12 models from multiple families and reports consistent gains over feature-based baselines such as length, BoW, and TF IDF with logistic regression. The study includes per-target accuracy and AUROC, ablations on interpolation and curriculum, backbone swaps that show transfer, and analyses of family-level cases where closely related variants are harder to separate. The results suggest that identity cues persist beyond shallow features, and the authors discuss implications for evaluation security and mitigation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Significance of the Problem.** The paper addresses a timely research problem. Anonymous, voting-based leaderboards like LM Arena are a primary standard for evaluating SOTA models, and this paper investigates a critical security vulnerability in their core assumption of anonymity.\n\n2. **A New Detector Framework.** The authors propose I-PREF, a well-reasoned framework for training an identification detector. The strategy of synthesizing hard negatives via model interpolation is a cost-effective solution. When combined with iterative curriculum learning, this approach enables the detector to learn deep stylistic patterns rather than just superficial statistics.\n\n3. **Strong Empirical Results.** The experiments demonstrate that I-PREF outperforms all statistical feature-based baselines. The method achieves high identification accuracy and proves particularly effective in the setting of distinguishing between closely-related family models (e.g., Claude-4 vs. Claude-3.5)."}, "weaknesses": {"value": "1. **Disconnect between Motivation and Experiments.** The paper's motivation (Hacking) and its experiments (Identification) are disconnected. The paper's goal is to \"hack\" LM Arena, but its methods and experiments are heavily focused on model response identification. It is unclear if a binary detector for a single target model can have a significant impact on Arena, especially given the low probability (~1%) of sampling any single target model from a pool of hundreds. While prior work (e.g., Min et al., 2025) proposed attack strategies, this paper does not demonstrate whether I-PREF can be effectively used within any similar strategies. It would be nice to add simulation experiments to demonstrate a quantifiable impact on Elo ratings with I-PRF.\n2. **Incomplete Analysis.** Given the paper's claim to \"hack\" LM Arena, an analysis connecting the reported identification accuracy to the potential impact on Arena ratings is missing. Quantifying how an identification accuracy connects with Elo ratings would have made the paper's claims much stronger.\n3. **Unrealistic Experimental Setup.** The authors selected 12 models with a wide range of capabilities, including 3B-sized models and frontier models, and selected only 3 top-performing LLMs as the target model. This significant capability gap may make the identification task artificially easy. A more realistic hacking scenario would involve selecting a set of ~10 models with very close Arena rating (e.g., within a $\\pm 20$ Elo range) and testing the method. This would better reflect the real-world goal of making a model outperform its direct competitors.\n4. **Weak Baselines.** Since the paper's experiments are primarily focused on model identification, or \"source attribution,\" using only feature-based logistic models (e.g., TF-IDF, BoW) as baselines is insufficient. The authors should have included stronger baselines from the literature on model source attribution/identification, such as perplexity-based methods, neural-based classifiers, or LLM-as-a-judge baselines."}, "questions": {"value": "1. According to Table 2, the identification accuracy between GPT-4o and GPT-4o-mini is only 67.5%, which is quite low compared to other pairs11. Could the authors provide more insights or analysis on why this is the case?\n2. How does LM Arena's 'Style Control' feature, which is designed to mitigate stylistic biases, impact the effectiveness of the hacking method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xCqXqhzJoh", "forum": "rkBB6zTUTC", "replyto": "rkBB6zTUTC", "signatures": ["ICLR.cc/2026/Conference/Submission24505/Reviewer_PG4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24505/Reviewer_PG4o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952153834, "cdate": 1761952153834, "tmdate": 1762943105311, "mdate": 1762943105311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes I-PREF, a model-driven framework for LLM identification leveraging interpolated preference learning. The core idea is to train a detector to distinguish a target LLM’s responses from others by creating synthetic hard negatives via model interpolation, and to improve identification performance via adaptive and iterative curriculum learning. Experimental results show that I-PREF markedly outperforms feature-based identification baselines in accuracy and AUROC across both standardized and crowdsourced human preference datasets"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The work introduces a novel preference learning methodology that demonstrates superior performance and considerable robustness compared to traditional feature-based baselines like TF-IDF and BoW.\n\n- The manuscript features excellent presentation and writing quality, coupled with clear experimental design, resulting in a substantial and rich content.\n\n-  The experimental design, including comprehensive ablation studies and direct comparisons against advanced current baselines, is well-executed and provides significant empirical insights."}, "weaknesses": {"value": "- Limited Scalability Analysis for Large Models: The study lacks experimentation on substantially larger model scales. Current experiments are confined to models below 10 billion parameters (<10B), omitting a thorough analysis for models exceeding 30 billion parameters (>30B). It remains unclear whether I-PREF maintains comparable efficacy when applied to LLMs possessing greater capabilities and scale."}, "questions": {"value": "same as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3GItwwhbEk", "forum": "rkBB6zTUTC", "replyto": "rkBB6zTUTC", "signatures": ["ICLR.cc/2026/Conference/Submission24505/Reviewer_fTGT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24505/Reviewer_fTGT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991158895, "cdate": 1761991158895, "tmdate": 1762943105060, "mdate": 1762943105060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}