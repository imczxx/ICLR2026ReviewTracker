{"id": "s17DZs56XJ", "number": 3847, "cdate": 1757553334557, "mdate": 1759898066649, "content": {"title": "Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization", "abstract": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently scaling large language models without a proportional increase in computational cost. However, the standard training strategy of Top-K router prevents MoE models from realizing their full potential for elastic inference. When the number of activated experts is altered at inference time, these models exhibit precipitous performance degradation. In this work, we introduce Matryoshka MoE (M-MoE), a training framework that instills a coarse-to-fine structure directly into the expert ensemble. By systematically varying the number of activated experts during training, M-MoE compels the model to learn a meaningful ranking: top-ranked experts collaborate to provide essential, coarse-grained capabilities, while subsequent experts add progressively finer-grained detail. We explore this principle at multiple granularities, identifying a layer-wise randomization strategy as the most effective. Our experiments demonstrate that a single M-MoE model achieves remarkable elasticity, with its performance at various expert counts closely matching that of an entire suite of specialist models, but at only a fraction of the total training cost. This flexibility not only unlocks elastic inference but also enables optimizing performance by allocating different computational budgets to different model layers. Our work paves the way for more practical and adaptable deployments of large-scale MoE models.", "tldr": "", "keywords": ["Large Language Model", "Mixture-of-Experts", "Pre-training", "Matryoshka"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b5d4b7480c74bf4234b82c893c3042fbcd919c3a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Matryoshka MoE, that activates different number of experts per layer. This elastic activation can fit different computational budgets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The exploration of M-MoE variants (such as batch-wise, layer-wise, etc) is comprehensive.\n\nThe writting is clear."}, "weaknesses": {"value": "The biggest weakness of the work is its impracticality. Varying activation levels across layers introduce notable complexity for practical deployment, kernel implementation, and communication engineering. First, the model must be deployed on devices that can accommodate the maximum memory and computational requirements, right? However, when some layers activate fewer experts, this leads to more pipeline bubbles and results in resource waste. If the top-k activation is too large to fit on a device, your results indicate that continual training a specialist model with a smaller k performs well enough, and is more practical and cost-effective.  \n\nAdditionally, the technical contribution is somewhat weak, as the proposed method is relatively straightforward.\n\nBelow are some weaknesses related to the method and its results.\n\n1. Figure 1 is misleading because its x-axis omits the range from 4 to 7. I conducted some experiments on OLMOE (pre-trained with topk=8) by adjusting k during inference. The 0-shot results are shown below:\n|k|arc-c|arc-e|piqa|hellaswag|winogrande|\n|-|-|-|-|-|-|\n|8|47.1|78.16|80.14|58.01|68.67|\n|7|45.9|78.37|79.87|58.08|68.11|\n|5|44.97|76.94|79.16|57.14|66.22|\n\nWhen using an inference k smaller than the pre-trained k=8, performance does drop—but the decline is far less drastic than Figure 1 suggests. This discrepancy arises because the figure omits the x-axis range where the slight drop occurs. I recommend the authors include the full x-axis and corresponding experimental results to avoid overstating the severity of the issue illustrated.\n\n2. The experiment setup in Table 1 is unrealistic and unconvincing. The base model for continual training only activates 1 expert per layer (topk=1), but pre-training MoE models with topk=1 is rarely practiced in real-world scenarios. This setup makes increasing k during continual training easy: the model could simply add some useless but harmless experts, while the single expert trained during pre-training remains the primary contributor to performance. How to rule out this possibility?\n\n3. For the specialist baseline (k*=6) in Table 2, the authors should report results for Inf.k = 2, 3, 4. As noted earlier, performance drops only slightly when Inf.k is marginally smaller than the pre-trained k. Currently, Table 2 only includes Inf.k=1 and the pre-trained k, omitting intermediate values. This incomplete comparison might overstates M-MoE’s advantage.\n\n4. Table 3 has confusing bold text and lacks significance tests. Is the value 55.42 incorrectly bolded? Moreover, I do not think there are statistically significant differences in the results in Table 3. It is hard to support the claim that \"earlier layers are more critical.\"\n\n5. If you consider a low MODS to be a good metric and believe that earlier layers are more critical, in Figure 3, the Matroshka model performs worse than the Top-K model in shallow layers due to its higher MODS scores. How can this be explained?"}, "questions": {"value": "1. What is your base model? Is it an open or private model? I did not find detailed descriptions of this model. Additionally, why was this model pre-trained with only one expert activated per layer? This is not a commonly used setup.\n\n2. What is the formulation of the Focused Spearman Correlation?\n\n3. See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ygnhqyI3mX", "forum": "s17DZs56XJ", "replyto": "s17DZs56XJ", "signatures": ["ICLR.cc/2026/Conference/Submission3847/Reviewer_tUR1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3847/Reviewer_tUR1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761541811939, "cdate": 1761541811939, "tmdate": 1762917062519, "mdate": 1762917062519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Matryoshka-MoE (M-MoE), a training pipeline designed to enable elastic MoE inference. \nThe authors argue that fixed-*k* training limits the flexibility of MoE inference. \nA reduction in activated experts will lead to significant performance degradation in a well-trained MoE. \nThe design of M-MoE is straightforward, involving the activation of varing numbers of experts during training.\nM-MoE achieves competitive results across the full suite of specialist models under different activation setups.\nThe authors propose different M-MoE strategies and demonstrate that their layer-wise M-MoE is particularly effective. \nExtensive experiments are presented to support the authors’ claims and highlight the effectiveness of M-MoE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The research question addressed in this paper is interesting and meaningful, providing researchers with insights into advancing MoE inference efficiency.\n\n- The idea of introducing Matryoshka routing into MoE training is straightforward and well-motivated.\n\n- The authors perform extensive experiments to demonstrate the effectiveness of their M-MoE, including continual pre-training with 1T tokens for a 20B language model and from-scratch pre-training, which strengthen their claims."}, "weaknesses": {"value": "- I have concerns regarding some of the experimental results, as they do not adequately support the authors' claims or demonstrate the effectiveness of M-MoE (See Q1 & Q2).\n\n- The authors lack empirical evidence to show that M-MoE can truly improve MoE inference efficiency, raising doubts about its practical applicability in real-world scenarios (See Q3 & Q4)."}, "questions": {"value": "---\nQ1: What's the performance when number of Activated Experts is set between 3 and 8 in Figure 3.\n\nI suggest the authors provide a finer-grained analysis of activated expert counts to better illustrate the trend and shape of the performance degradation.\n\n---\nQ2: Can the authors also inlude the top-k sepecialist baseline results beyond Inf. k = 1 and its native activation count in Table 1 and 2 ?\n(e.g Top-k (k=6), *Inf.k* = 2,4)\n\nI have the following concerns regarding Table 1 and 2.\n\n(1) The authors state that the Top-k baseline suffers a severe performance drop when evaluated with a different number of activated experts.\nHow does an MoE trained with 6 experts perform when evaluated with 2 or 4 activated experts? Is its performance comparable to that of your M-MoE variants?\n\n(2) The contitual pre-training setup in Table 1 seems unusual, where only a single expert is activated during per-training.\nWhat is the performance of M-MoE when more experts are activated during the pre-training phase?\n\n--- \nQ3: What's the end-to-end training throughput of your M-MoE variants and top-k specialist baselines?\n\nWhat is the actual training efficiency of M-MoE relative to the baseline methods?\n\nDoes M-MoE introduce additional complexity to the pipeline, thereby reducing compatibility with existing MoE training and deployment frameworks?\n\nCould the authors include a discussion of the following points:（1）training throughput and (2) inference speed comparisons, to provide further clarity.\n\n---\nQ4: What's the loading balance of M-MoE models?\n\nI am curious whether Matryoshka routing introduces a shortcut in MoE routing — i.e., some experts are more easily activated and consistently assigned large routing weights, which could explain the observed performance improvement over Top-1.\n\nA comparison of load balance across experts would help clarify any potential loading imbalances and shed light on the shortcut issue."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CQMUzBBsVV", "forum": "s17DZs56XJ", "replyto": "s17DZs56XJ", "signatures": ["ICLR.cc/2026/Conference/Submission3847/Reviewer_g7C7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3847/Reviewer_g7C7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640303047, "cdate": 1761640303047, "tmdate": 1762917062257, "mdate": 1762917062257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an interesting setting for MoE models: the number of activated experts (K) is altered at inference time. The authors identify that the fixed Top-K training paradigm leads to severe performance degradation if K is changed, preventing the elastic trade-off between computational cost and model quality. Then, the author proposes some strategies to handle this and conducted corresponding experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The author proposes a training framework to train a moe flexible to different K while inference, while achieving performance similar to top-K pretrained ones.\n2. Many experiments have been done.\n3. The elastic inference setting is interesting."}, "weaknesses": {"value": "1. I’m not sure whether this method should be compared with dynamically-activated MoEs (i.e., those that allow the model to choose the computing budget at inference time, rather than training with top‑p but inferring with top‑k), or whether the authors want to emphasize the necessity of allowing humans, rather than the model, to choose the computing budget.  \n\n2. Could you also report the perplexity or the loss curve, since benchmark results may vary due to randomness (especially in the from-scratch setting)?  \n\n3. Have you tried continuing training the Top‑K model for a while? I mean, using the same training cost as M-MoE to train a Top‑K model, then applying a short period of continued training to obtain versions with different K values. (This is somewhat similar to your current setting, but I would prefer that the base model not be an MoE with top‑k = 1.)"}, "questions": {"value": "1. It is quite interesting that your M-MoE model achieves better performance than Top-k = 4 model with an average k < 4. Can you provide some insights or explanations?\n2. I suggest focus on from-scratch setting more since the continue-training setting is a little wired (the base model is top-1 yet you continue it to increase topk)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4ViCSfKWmK", "forum": "s17DZs56XJ", "replyto": "s17DZs56XJ", "signatures": ["ICLR.cc/2026/Conference/Submission3847/Reviewer_kZjX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3847/Reviewer_kZjX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882705999, "cdate": 1761882705999, "tmdate": 1762917061881, "mdate": 1762917061881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Matryoshka Mixture-of-Experts (M-MoE), a training framework designed to enable elastic inference in sparse MoE language models. Standard Top-K routing results in sharp performance degradation when the number of activated experts is reduced at inference time. M-MoE mitigates this by randomizing the number of active experts (K) during training. The authors compare three levels of stochasticity — global batch, micro-batch, and layer-wise — and find that layer-wise performs best in achieving stable, elastic performance.\nExperiments include continual pre-training from a 1T-token single-expert model (additional 80B and 208B tokens), showing that M-MoE maintains high accuracy across K=1–6."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Identifies a critical limitation of Top-K routing: sharp accuracy drop when reducing K.\n2. Proposes a simple yet effective training method that yields elastic inference.\n3. Demonstrates strong empirical validation across multiple training regimes (continual and from-scratch).\n4. Provides clear pseudo-code and detailed experimental settings, supporting reproducibility."}, "weaknesses": {"value": "1. Missing quantitative comparison of training throughput (FLOP/s, GPU utilization) between Top-K and M-MoE.\n2. Conceptually incremental relative to previous Matryoshka and dynamic MoE literature."}, "questions": {"value": "1. Could the authors provide explicit FLOP/s or wall-clock throughput comparisons with fixed-K training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vOB10TsjZN", "forum": "s17DZs56XJ", "replyto": "s17DZs56XJ", "signatures": ["ICLR.cc/2026/Conference/Submission3847/Reviewer_WfL3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3847/Reviewer_WfL3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913803812, "cdate": 1761913803812, "tmdate": 1762917061665, "mdate": 1762917061665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}