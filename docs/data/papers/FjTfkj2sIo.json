{"id": "FjTfkj2sIo", "number": 14119, "cdate": 1758228694323, "mdate": 1759897388870, "content": {"title": "Modeling Visual Cortex by Maximizing Layerwise Multiscale Manifold Capacity", "abstract": "Task-optimized deep neural networks have risen to prominence as the most predictive phenomenological models of responses in primate visual cortex, but leave much to be desired from the perspective of biological plausibility. One such limitation is the reliance on precise credit assignment through global backpropagation of error signals. Recent work has shown that this weakness can be circumvented by requiring each subsequent stage to solve a distinct and increasingly complex task, allowing for layerwise local learning signals. We propose a novel strategy for crafting such intermediate losses that uses an efficient coding framework formulated in terms of manifold capacity, which can be computed using a sequence of  *canonical cortical computations*. In particular, we leverage the relationship between the multiscale nature of visual signals and the dilation of receptive field sizes in cascaded visual representations to modulate complexity, allowing for the reapplication of these common loss computations at each stage of the hierarchy. We evaluate our approach on its ability to predict neural datasets spanning three areas of the ventral stream hierarchy in macaque, as well as human psychophysical data on an object classification task. We find that our unsupervised layerwise model matches or exceeds the performance of competitive architecture-matched baselines on all evaluations considered.", "tldr": "", "keywords": ["Biological Plausibility", "Efficient Coding", "Visual Cortex", "Layerwise Learning", "Self-supervised Learning"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/071906e77eb59486d678e1c16ca0f11e8214c528.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ST-MMCR, a layerwise self-supervised learning approach for predicting neural data from the ventral visual stream of primates and human psychophysical data on an object classification task. The key innovation, building on top of the manifold capacity framework, is using spatiotemporal pooling regions that scale with receptive field size to implement \"complexity matching\" - ensuring task difficulty aligns with computational capacity at each stage. The approach trains on synthetic videos generated from ImageNet images and evaluates on neural recordings from macaque V1, V2, and V4, as well as human behavioral data. Results show competitive or superior performance compared to supervised and adversarially-trained baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "## Overall Assesment \n\nThis is a strong and well-executed submission that will be of significant interest to the ICLR community, particularly researchers working at the intersection of neuroscience and machine learning. The paper makes meaningful contributions to biologically-plausible visual representation learning while achieving competitive empirical results. I believe the strengths substantially outweigh the weaknesses, though several points warrant clarification during the discussion phase (detailed below).\n\n\n## Strengths\n\n1. **Biologically-motivated architecture and learning scheme.** The layerwise learning approach with gradient isolation directly addresses a well-known implausibility of backpropagation in biological systems. Importantly, this work demonstrates that such biologically-constrained learning can achieve state-of-the-art neural predictivity, challenging the notion that biological plausibility requires sacrificing performance. The use of dilating spatiotemporal pooling regions is particularly elegant, directly motivated by the expanding receptive field sizes observed across the ventral visual stream.\n\n2. **Well-designed synthetic video approach.** The decision to use synthetic videos generated from ImageNet images cleverly balances ecological validity with experimental control. This avoids confounding distribution shifts that would arise from switching to real video datasets while allowing precise parametric control over spatiotemporal transformations. While I believe real video validation would further strengthen the work (see weaknesses), this represents a reasonable and methodologically sound first step.\n\n3. **Comprehensive neural evaluation.** The evaluation framework is thorough and multi-faceted: (a) three cortical areas spanning early-to-mid visual hierarchy (V1, V2, V4), (b) stimulus partitioning analyses revealing which response components are captured, (c) sparse regression analysis examining representational format, and (d) behavioral alignment on out-of-distribution tasks. This goes well beyond standard aggregate predictivity metrics.\n\n4. **Competitive performance with biological constraints.** Achieving performance comparable to or exceeding adversarially-trained models—while using simpler, more biologically plausible learning rules—is a significant achievement. This demonstrates that biological constraints need not be a handicap and may actually provide useful inductive biases.\n\n5. **Analysis of model-brain alignment.** The sparse regression analysis (Fig. 5) and stimulus partitioning (Fig. 4) provide important insights into how models achieve predictivity, not just how much. This reveals that models with similar aggregate scores can differ substantially in their representational structure. Such analysis is crucial for moving beyond superficial alignment metrics and I believe it's complete and serves the paper's goal. \n\n6. **Clear presentation.** The paper is well-written with effective visualizations, particularly Figure 1, which immediately conveys the core approach. The organization facilitates understanding of both the technical contributions and their biological motivation."}, "weaknesses": {"value": "## Weaknesses \n\n1. **Biological plausibility of MMCR loss.** The paper emphasizes biological plausibility but acknowledges (pg. 9) that \"the final MMCR loss, which relies on nuclear norm computation, does not currently have an obvious counterpart in the world of biological modeling.\" This is a significant limitation for the central framing. The computation requires: (a) global normalization across all centroids, (b) SVD computation, which are both implausible. More discussion of how this might be implemented biologically is needed.\n\n2. **Limited architectural generalization.** Using only AlexNet limits the impact. While the authors justify this choice, modern architectures (ResNets, Vision Transformers) would strengthen claims about the general utility of the approach. It's unclear if the spatiotemporal pooling strategy would work as well with skip connections or attention mechanisms, that are part of standard (supervised) sota models in the [brainscore](https://www.brain-score.org/vision/leaderboard/) leaderboard (M. Schrimpf et al, BioArxiv 2018).\n\nMinor Weaknesses\n\n1. **Incomplete comparison to layerwise learning methods.** The paper mentions CLAPP and LPL but doesn't directly compare to them experimentally. Given that layerwise learning is central to the contribution, head-to-head comparison on neural predictivity would be valuable. The end-to-end ablations (Fig. 8) are useful but don't substitute for comparison to other layerwise approaches. \n\n2. **Limited ablation studies** Key design choices lack thorough ablation: Why 8 frames specifically? How sensitive are results to this choice? How important is the specific schedule of pooling sizes (sf, sf/2, sf/4)? What about different video transformation strategies?\n\n3. **Hyperparameter sensitivity** The paper doesn't discuss sensitivity to key hyperparameters (learning rate, batch size, projection head architecture). This makes reproducibility and extension challenging."}, "questions": {"value": "I want to emphasize that I am on the high end of 6 and would rate this a 7 if that option were available. The work has substantial merit and clear acceptance-worthy contributions. I am enthusiastic about this paper and would be happy to raise my score to 8 following the discussion period if the authors can satisfactorily address the weaknesses above and questions below.\n\n## Questions\n\n1. Can you provide intuition or preliminary ideas for how the nuclear norm computation might be approximated with biologically plausible operations?\n\n2. How does computational cost compare to supervised training and adversarial training? Is there a trade-off between biological plausibility and computational efficiency?\n\n3. Have you considered applying this approach to modern architectures (ResNets, Transformers)? Even a preliminary result would strengthen generalization claims.\n\n4. What happens with longer videos (16, 32 frames)? Does performance continue to improve or saturate?\n\n5. V2 frontend superiority (Fig. 10, Appendix A.4): The finding that classifiers trained on V2-like representations outperform both V1 and V4 frontends on behavioral tasks is intriguing but under-explored. This is counter-intuitive since V2 has fewer trainable parameters in the classifier (more frozen stages) and V4 representations are deeper/more abstract. Can you provide mechanistic insight into why V2 is the \"sweet spot\"? Is this related to: (a) the level of invariance/selectivity, (b) superior texture representations (as suggested by Fig. 4), (c) manifold geometry properties, or (d) something specific about how ST-MMCR structures V2 representations? This could be a key finding that warrants more prominence and discussion in the main text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b0hWjZJAt0", "forum": "FjTfkj2sIo", "replyto": "FjTfkj2sIo", "signatures": ["ICLR.cc/2026/Conference/Submission14119/Reviewer_QTgA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14119/Reviewer_QTgA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761231704440, "cdate": 1761231704440, "tmdate": 1762924586940, "mdate": 1762924586940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel self-supervised, layerwise learning strategy, termed ST-MMCR, to train deep neural networks as more biologically plausible models of the primate ventral visual stream. The work is motivated by the biological implausibility of global backpropagation and the potential for single, end-to-end objectives to insufficiently constrain internal representations. The authors' method trains the network in stages, applying a local learning objective at each stage based on maximizing multiscale manifold capacity (MMCR). The core novelty is the reapplication of this same loss computation at each stage, arguing this is analogous to the replication of canonical cortical circuits. The model supposedly \"matches complexity\" by linking this loss to expanding receptive fields (both spatial and temporal) of the network.\nThe model is evaluated on its ability to predict neural responses in macaque visual areas (V1, V2, V4) and its alignment with human psychophysical data on an out-of-distribution (OOD) object classification task. The results suggest the ST-MMCR model matches or outperforms undefined \"architecture-matched baselines\" in neural predictivity and shows stronger alignment with human OOD generalization, despite lower standard ImageNet accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's motivation is strong. It addresses the central challenge of biological plausibility by attempting to replace global backpropagation with a local, layerwise learning signal. \n\n- The idea of a \"common computational objective\" (MMCR) that is reapplied at each stage, modulated by the expansion of receptive fields is an interesting hypothesis. It aligns conceptually with the biological principle of canonical microcircuits."}, "weaknesses": {"value": "- Problematic and Inaccurate Engagement with Prior Literature: This is a major concern. The paper states that \"handcrafted filters that are tuned for orientation and spatial frequency substantially outperform learned models on the V1 dataset.\" This claim is in direct contradiction with other works in the field. For example, Cadena et al., 2019, PloSCB [1] have definitively shown that DNNs trained on tasks (task-driven models) or end-to-end to predict neural responses (data-driven models) substantially outperform models based on handcrafted Gabor-like filters. Similarly, other literature that cites \n\n\n- Narrow Framing and Lack of Essential Baselines: The paper almost exclusively cites and builds upon a single prior work [2], while failing to engage with the vast and diverse field of data-driven and task-driven models of the ventral stream. The comparison is limited to \"architecture-matched baselines\" which are not clearly defined. Crucially, the paper fails to cite or discuss highly relevant work on self-supervised models of visual cortex, such as [3]. That paper also found that task-agnostic, self-supervised objectives can be superior matches to neural data and lead to general-purpose representations. A discussion of how ST-MMCR compares to other self-supervised approaches (e.g., contrastive learning) is a critical and missing piece.\n\n\n- Superficial Neural Analysis: The analysis is limited to population-level performance metrics. The authors do not leverage their model to investigate why it aligns with the neural data. If the model is interpretable, as implied, the authors should have provided a deeper analysis. For example, can learned properties of model units (e.g., receptive field size, tuning properties) be directly related to the measured properties of the biological neurons they predict? This is a significant missed opportunity to provide mechanistic insight beyond a simple performance number.\n\n\n- Stimulus Mismatch Between Training and Evaluation: The model is named ST-MMCR, implying \"spatio-temporal,\" and the discussion mentions \"temporal pooling duration,\" suggesting it is pre-trained on videos. However, the model is evaluated on neural data collected from primates viewing static images. The authors must justify why a model trained on temporal dynamics would be expected to produce superior representations for static image processing. This choice complicates any interpretation of the results.\n\n\n- Surprising Performance of Random Networks: The sparse regression results in Figure 5 are highly counter-intuitive. The fact that a random-weight model achieves respectable performance, particularly in the sparse regime, requires a much deeper explanation. Does this imply that the architecture, pooling, and connectivity itself—rather than the learned weights—are responsible for a large portion of the alignment? \n\n- Biological Plausibility of the Loss Function Itself: This is a central contradiction. The authors admit in the discussion that \"the final MMCR loss, which relies on nuclear norm computation, does not currently have an obvious counterpart in the world of biological modeling.\" However, this undercuts the paper's primary claim of biological plausibility. If the learning signal itself is biologically implausible, then replacing global backpropagation with a local but equally implausible computation is not a step forward.\n\n\n- Vagueness of \"Manifold Capacity\": The paper's central mechanism, \"multiscale manifold capacity,\" is never clearly defined. It is unclear what these \"manifolds\" are. Are they manifolds of object classes? Image augmentations? Stimulus identity across time? Without a concrete definition, \"manifold capacity\" remains an abstract mathematical construct that is difficult to connect to a clear computational goal of the visual system.\n\n[1] https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006897\n\n\n[2] https://arxiv.org/abs/2312.11436\n\n\n[3] https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011506"}, "questions": {"value": "1. Could the authors please address the claim that \"handcrafted filters... outperform learned models\"? This appears to directly contradict findings from Cadena et al. (2019) and others. Can you clarify which V1 dataset and handcrafted models are being referenced that produce this result?\n\n\n2. Why was the analysis limited to population-level predictivity? Can the authors provide any analysis linking learned model parameters or unit properties (e.g., RF size, feature tuning) to the corresponding properties of the biological neurons they predict?\n\n\n3. What is the justification for using video (spatio-temporal) data to pre-train a model that is then evaluated on neural responses to static images? Have the authors tested whether training on a large dataset of static images with the same MMCR loss yields comparable or better results?\n\n\n4. How do the authors interpret the surprisingly high performance of the random-weight network in the sparse regression analysis? What does this imply about the relative contributions of architecture versus learned weights in explaining neural responses?\n\n\n5. Given that the nuclear norm computation is admittedly not biologically plausible, what is the authors' view on the model's contribution? Are there known (or hypothetical) local, Hebbian-like or dynamic circuit computations that could approximate the maximization of nuclear norm, thereby bridging this plausibility gap?\n\n\n6. Could the authors please provide a concrete definition of the \"neural manifolds\" that the MMCR objective is separating? What specific stimulus properties, transformations, or classes define these manifolds at each stage of the hierarchy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "guRzi8g0C0", "forum": "FjTfkj2sIo", "replyto": "FjTfkj2sIo", "signatures": ["ICLR.cc/2026/Conference/Submission14119/Reviewer_naJ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14119/Reviewer_naJ4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955548203, "cdate": 1761955548203, "tmdate": 1762924586325, "mdate": 1762924586325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Present an unsupervised method with layer-by-layer training (rather than end-to-end training) by optimizing a previously suggested measure of manifold capacity. A smart choice of the scaling in each layer, which corresponds to the scale of the receptive fields, allows for representing the multiscale nature of visual stimuli. A new method for training on ImageNet-derived video-like stimuli is used. The resulting method is competitive with other methods in terms of predicting responses along the mammalian visual system, and also in terms of the classification accuracy achieved, despite being unsupervised."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* An unsupervised method for layer-by-layer training.\n * Achieving interesting results in both accuracy and prediction of neural responses.\n * An interesting method for training a static model using \"synthetic videos\", derived by smoothly sampling still images from the ImageNet dataset, where training is done over the \"temporal responses\" of the model."}, "weaknesses": {"value": "* Insufficient baselines: the chosen baselines for Figures 3-6 are somewhat arbitrary. The authors could have compared with \"a simple unsupervised method\", as well as \"SOTA unsupervised method\", thus placing the suggested method in between in terms of the range of unsupervised methods. The authors could have compared with \"a supervised version of the same architecture\", or \"the same architecture without training on video-like stimuli\", thus making the conclusions much clearer (see next point).\n * It is unclear how much of the results should be attributed to (i) the loss function used; (ii) the training method used; (iii) the stimuli used for training; or (iv) the smart choice of the scaling. For example, it might be that the good predictability of neural data is achieved only through the smart scaling from layer to layer."}, "questions": {"value": "* Can you suggest what part of the results presented should be attributed to (i) the loss function used; (ii) the training method used; (iii) the stimuli used for training; or (iv) the smart choice of the scaling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PbEv5rgvul", "forum": "FjTfkj2sIo", "replyto": "FjTfkj2sIo", "signatures": ["ICLR.cc/2026/Conference/Submission14119/Reviewer_EJNc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14119/Reviewer_EJNc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988804611, "cdate": 1761988804611, "tmdate": 1762924585814, "mdate": 1762924585814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission proposes a local learning appraoch in deep networks based on manifold capacity. The method relies on the change in the receptive field size of the network across layers and applies MMCR loss at different layers independently. The proposed method relies on methods from previous papers, most importantly Yerxa et al. 2023 and Parthasarathy et al. 2024. The empirical results show that the proposed method does better than other baselines in predicting neural activity in areas V1,V2, and V4 of monkeys."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. the proposed method is more biologically plausible. \n2. the figures were clear and easy to understand \n3. the paper was well written"}, "weaknesses": {"value": "Overall there were several issues that affected my impression of its Soundness and Contribution. \n\n1. the specific contribution of this submission and what factors distinguished it from some of the related work was unclear. I’m not sure how the contribution of this paper is different from that in Parthasarathy et al. 2024. My understanding is that this paper has the same approach as that paper but the implementation is slightly different to improve biological plausibility. Is this true? Please explain in more detail what the differences are and why they warrant considering the present work as novel and impactful.\n\n2. Some of the choices made in the model were not well justified/explained. \n\n- what is the reason for restricting the network to only 3 stages? The proposed approach seems to be general, why not trying it in a deep architecture? (line 64) \n\n- also see my questions\n\n3. Additional experiments needed.\n\n- the synthetic dataset is somewhat unconventional (line 154). A version of the model trained on natural videos would be very informative both in terms of the importance of the naturality of the videos but also scalability of the method. \n\n- given the close relationship between the current work and Parthasarathy et al. 2024, I was surprised to see that it was not considered as a baseline for the main results. \n\n4. Unclear/unjustified statements \n\n- line 55. Typically there are more artificial neurons in the first stages of processing in CNNs due to larger spatial dimensions. It’s unclear in what sort of neural network model this statement is true for. \n\n- line 170. please provide supporting evidence for this statement. Several work from James DiCarlo including BrainScore has shown that deeper models such as ResNet50 are better than shallower ones like Alexnet at predicting brain activity across regions. \n\n- alexnet has 9 layers, but the model is said to use Alexnet architecture but only three stages. Please explain more clearly what’s the relation to Alexnet exactly. Is it adopting the first few layers of that architecture? \n\n- it was unclear what the experiments in section 3.1 revealed. The opening of that section speaks about previous literature suggesting that models with similar architectures and training datasets share representations. Looks like the results in fig 4 support that view. Is that so? I didn’t see that discussed \n\n5. Statistical significance. \n\n- values reported in figure 2 and 8 are extremely close to each other. Have you repeated the analyses multiple times? Are the differences statistically significant? Similar question for the main results in fig 3 and 5\n\n6. Others \n\n- Line 67, acronym used before being defined"}, "questions": {"value": "- what kind of pooling operation is used? line 225\n\n- I believe BrainScore also includes several neural datasets from V1 and V2. Have you tested using that data as well? Is the data considered here better or larger than that? \n\n- Please explain more clearly whether the global average pooling strictly considered to calculate the loss or it affects the output generated by each layer?  line 267\n\n- given the results showing improved OOD generalization, does the model perform better against adversarial attacks as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7H4cPOkbbh", "forum": "FjTfkj2sIo", "replyto": "FjTfkj2sIo", "signatures": ["ICLR.cc/2026/Conference/Submission14119/Reviewer_fHVb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14119/Reviewer_fHVb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059891882, "cdate": 1762059891882, "tmdate": 1762924585340, "mdate": 1762924585340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}