{"id": "tgnXCCjKE3", "number": 8562, "cdate": 1758090994927, "mdate": 1763453360006, "content": {"title": "CPiRi: Channel Permutation-Invariant Relational Interaction for Multivariate Time Series Forecasting", "abstract": "Current methods for multivariate time series forecasting can be classified into channel-dependent and channel-independent models. Channel-dependent models learn cross-channel features but often overfit the channel ordering, which hampers adaptation when channels are added or reordered. Channel-independent models treat each channel in isolation to increase flexibility, yet this neglects inter-channel dependencies and limits performance. To address these limitations, we propose CPiRi, a channel permutation invariant (CPI) framework that infers cross-channel structure from data rather than memorizing a fixed ordering, enabling deployment in settings with structural and distributional co-drift without retraining. CPiRi couples spatio-temporal decoupling architecture with permutation-invariant regularization training strategy: a frozen pretrained temporal encoder extracts high-quality temporal features, a lightweight spatial module learns content-driven inter-channel relations, while a channel shuffling strategy enforces CPI during training. We further ground CPiRi in theory by analyzing permutation equivariance in multivariate time series forecasting. Experiments on multiple benchmarks show state-of-the-art results. CPiRi remains stable when channel orders are shuffled and exhibits strong inductive generalization to unseen channels even when trained on only half of the channels, while maintaining practical efficiency on large-scale datasets. The source code and models will be released.", "tldr": "CPiRi enables channel-permutation-invariant MTSF by combining frozen temporal encoding with lightweight spatial attention trained via channel shuffling, achieving SOTA accuracy with zero performance drop under dynamic sensor changes.", "keywords": ["Multivariate Time Series Forecasting", "Channel Permutation Invariance", "Spatio-temporal Decoupling", "Meta-Learning", "Foundation Models"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c718f1d59453e77676669bb960b75ed13909e60.pdf", "supplementary_material": "/attachment/d97244b751a26d44a55ffc50dce2af39628fc8ed.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a central paradox in multivariate time series forecasting: channel-dependent (CD) models overfit to channel order rather than learning true inter-channel relationships, while channel-independent (CI) models achieve robustness by sacrificing performance, as they neglect these critical dependencies. To resolve this trade-off, the authors propose CPiRi, a novel framework that employs a spatio-temporal decoupling architecture. CPiRi leverages a frozen, pre-trained foundation model to independently extract robust temporal features, which are then fed into a lightweight, trainable spatial module. This module is specifically trained with a permutation-invariant regularization strategy (channel shuffling) to learn content-driven, permutation-equivariant relationships. This design allows CPiRi to achieve state-of-the-art forecasting accuracy while demonstrating exceptional robustness, maintaining its performance even when channel orders are permuted during inference."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed spatio-temporal decoupling architecture, which integrates a *frozen* time series foundation model (for temporal features) with a *lightweight, trainable* spatial module (for relational learning), is a novel and efficient design.\n2. The introduction of the permutation-invariant regularization strategy (channel shuffling) is a simple yet highly effective training technique to enforce the desired invariance.\n3. The experiments are pointedly designed to test the central hypothesis. The 'channel shuffling robustness analysis' (Table 2) is particularly impactful, providing a stark and convincing contrast between CPiRi's stability and the fragility of other CD models."}, "weaknesses": {"value": "1. The paper *does* apply the shuffling strategy to baselines (Table 2, \"Train Shuffle\"), which is a strong point. Could the authors elaborate on *why* this strategy fails to rescue models like Informer or STID?\n2. The \"w/o regularization strategy\" ablation (Table 4) shows only a *minor* performance drop (e.g., 9.21% vs 9.14% on METR-LA; 10.80% vs 9.43% on PEMS-08). This suggests that the permutation-equivariant architecture alone provides almost all of the robustness, and the \"potent\" and \"critical\" regularization strategy actually has a minimal impact.\n3. The calculation cost of CPiRi, which adds an $O(C^2)$ spatial module, is non-trivial. However, in some cases (e.g., METR-LA, Table 1), it does not achieve SOTA performance compared to lighter models like STID."}, "questions": {"value": "1. The paper convincingly shows that CPiRi is robust to channel shuffling, while standard CD models are not. However, it's unclear how much of CPiRi's gain comes from its permutation-equivariant *architecture* versus its \"permutation-invariant regularization\" *training strategy*.\n2. The \"channel shuffling\" in Figure 4 is a key experiment. The methodology is unclear. How is \"25% shuffle\" defined? Does it mean 25% of the channel indices are randomly permuted *among themselves*, or 25% are swapped with other random channels? This needs a precise definition."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Z7IsTu4793", "forum": "tgnXCCjKE3", "replyto": "tgnXCCjKE3", "signatures": ["ICLR.cc/2026/Conference/Submission8562/Reviewer_Kq3U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8562/Reviewer_Kq3U"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622579438, "cdate": 1761622579438, "tmdate": 1762920418498, "mdate": 1762920418498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CPiRi is a new framework for multivariate time series forecasting that stays accurate even when input channels are reordered or changed. It combines a frozen pre-trained temporal encoder with a lightweight spatial module trained using random channel shuffling, forcing the model to learn true content-based relationships instead of memorizing positions. This design achieves state-of-the-art accuracy, strong generalization to unseen channels, and robust performance under channel permutations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear Problem Identification: The paper clearly diagnoses a critical flaw in existing CD models using a simple \"channel shuffling\" diagnostic test. This test reveals that many SOTA models rely on \"positional memorization,\" leading to catastrophic performance collapse (e.g., >400% error increase for Informer) when channel order is changed.\n2. Effective and Sound Design: The \"spatio-temporal decoupling\" is an elegant solution. It leverages the power of a robust, pre-trained temporal model (CI strength) while using a separate, lightweight module to explicitly learn cross-channel interactions (CD strength). The channel shuffling strategy directly enforces the desired permutation-invariant property.\n3. Comprehensive Experimental Validation: The experiments are thorough and directly support the claims. CPiRi achieves SOTA accuracy , remains perfectly stable under channel shuffling , and shows strong inductive generalization to unseen channels. The ablation studies (Table 4) clearly demonstrate the necessity of each component: the spatial module, the pre-trained weights, and the shuffling strategy."}, "weaknesses": {"value": "1. The framework's success is entirely dependent on the frozen Sundial encoder. The ablation study \"w/o pretrained weights\" results in \"complete failure\". This makes it difficult to separate the contribution of the novel CPiRi training strategy from the powerful priors of the (very large) foundation model it relies on.\n2. The individual components are standard: a \"standard Transformer encoder block\" for the spatial module and a data augmentation technique for training. The novelty is in the clever system design and training methodology that combines these parts, rather than a fundamentally new architectural mechanism.\n3. By freezing the temporal encoder, the model can handle structural co-drift (changes in channel order) but cannot adapt its temporal feature extraction to new patterns or \"abrupt trend shifts\". This limitation is noted by the authors and means it may struggle in scenarios where the underlying temporal dynamics of the data change over time."}, "questions": {"value": "1. Why frozen temporal encoder? Will fine-tuning help improve performance?\n2. Will applying different temporal encoders, e.g., Chronos and Moment [1], affect performance?\n3. Does the strategy work for even higher-dimensional time series [2]?\n4. Can the authors visualize the latent representations before and after spatial encoding across different permutations?\n\n[1] Goswami M, Szafer K, Choudhry A, Cai Y, Li S, Dubrawski A. Moment: A family of open time-series foundation models. arXiv preprint arXiv:2402.03885. 2024 Feb 6.\n[2] Ni J, Wang S, Liu Z, Shi X, Zhong X, Ye Z, Jin W. U-Cast: Learning Hierarchical Structures for High-Dimensional Time Series Forecasting. arXiv preprint arXiv:2507.15119. 2025 Jul 20."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FmivrzCSk0", "forum": "tgnXCCjKE3", "replyto": "tgnXCCjKE3", "signatures": ["ICLR.cc/2026/Conference/Submission8562/Reviewer_Vucs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8562/Reviewer_Vucs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938866985, "cdate": 1761938866985, "tmdate": 1762920418140, "mdate": 1762920418140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CPiRi, a channel permutation-invariant framework for multivariate time series forecasting. It combines a frozen pretrained temporal encoder with a lightweight spatial interaction module trained under random channel shuffling. This design makes the model rely on content-based relations instead of channel order, achieving strong generalization and state-of-the-art performance on multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Interesting motivation.\n2. Clear and easy-to-follow writing.\n3. Comprehensive theoretical analysis provides support for the proposed method."}, "weaknesses": {"value": "1. Since this paper only uses Sundial as the temporal feature extractor, it lacks an explanation of why Sundial was chosen over other foundation models. Can this framework generalize to other pretrained models such as Chronos [1] or Moment [2]?\n2. I appreciate that the paper uses high-dimensional datasets with channel heterogeneity. This is an interesting attempt for scalability analysis. However, could you also test the model on Time-HD [3]?\n3. The main concern lies in efficiency (which the authors did not discuss in the main text). The framework invokes two large pretrained models and employs multi-head attention for high-dimensional inputs, which could lead to significant computational overhead.\n4. iTransformer [4] also applies multi-head attention along the channel dimension and is permutation-equivariant. How does this work differ from iTransformer in terms of channel modeling?\n\n[1] \"Chronos: Learning the language of time series.\" arXiv preprint arXiv:2403.07815 (2024).\n[2] \"Moment: A family of open time-series foundation models.\" arXiv preprint arXiv:2402.03885 (2024).\n[3] \"U-Cast: Learning Hierarchical Structures for High-Dimensional Time Series Forecasting.\" arXiv preprint arXiv:2507.15119 (2025).\n[4] \"iTransformer: Inverted Transformers Are Effective for Time Series Forecasting.\" arXiv preprint arXiv:2310.06625 (2023)."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yv6nZnbRfi", "forum": "tgnXCCjKE3", "replyto": "tgnXCCjKE3", "signatures": ["ICLR.cc/2026/Conference/Submission8562/Reviewer_AJUX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8562/Reviewer_AJUX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967072749, "cdate": 1761967072749, "tmdate": 1762920417671, "mdate": 1762920417671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles multivariate time series forecasting and points out that many models inadvertently entangle cross-channel interactions with the specific channel order seen in training. The authors introduce a three-stage framework: a frozen, pretrained univariate backbone to extract temporal features, a lightweight spatial module that views these features as an unordered set and models their relations with self-attention, and a frozen per-channel decoder. To enforce order agnosticism, training is done with random channel shuffling, discouraging any dependence on positional cues. On traffic style benchmarks, the approach proves effective, and the paper further shows it can generalize even when trained on only a subset of channels."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem framing is timely and interesting. The paper not only says “permutations matter” but builds an explicit diagnostic: train with fixed order, test with shuffled order, show catastrophic failure for several competitive models. \n2. The proposed framework is reasonable and coherent, with the per channel frozen temporal encoder feeding a permutation aware spatial block, and the permutation based training strategy reinforcing the intended behavior.\n3. The reported improvements indicate that the method actually delivers robustness rather than just matching accuracy in the standard setting."}, "weaknesses": {"value": "1. My main concern is the reliance on a large pre-trained backbone. Table 4 shows that removing the pre-trained weights leads to a substantial drop in accuracy, which suggests that much of the gain comes from the foundation model rather than from the proposed permutation invariant interaction itself. However, in Table 1 the competing CD baselines are trained from scratch and do not benefit from comparable pre-training. This raises a fairness question: to what extent are the improvements due to the architectural idea, and to what extent are they due to access to stronger prior knowledge? It would help to include baselines equipped with similar pre-trained features, or to report results for CPiRi without pre-training in Table 1.\n2. All benchmarks are traffic datasets. It is unclear whether the same channel ordering effect appears in other multivariate settings (e.g., energy, industrial telemetry, climate) or in higher-dimensional public datasets such as [1]. Showing at least one non-traffic dataset would clarify how general the phenomenon is.\n3. The paper shows that changing channel order can hurt performance, but it is not clear how this effect scales as the number of channels grows. Do larger channel sets make models more brittle to reordering, or does the effect plateau? A controlled study where channel count is progressively increased would make the robustness claim stronger.\n\n\n[1] U-Cast: Learning Hierarchical Structures for High-Dimensional Time Series Forecasting. 2025"}, "questions": {"value": "The questions are included in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PWwbwnPuON", "forum": "tgnXCCjKE3", "replyto": "tgnXCCjKE3", "signatures": ["ICLR.cc/2026/Conference/Submission8562/Reviewer_DSFJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8562/Reviewer_DSFJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762109143715, "cdate": 1762109143715, "tmdate": 1762920417304, "mdate": 1762920417304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to All Reviewers"}, "comment": {"value": "We extend our sincere gratitude to all reviewers for their thorough analysis and constructive feedback. Your insightful comments have been valuable in strengthening our paper. We have carefully revised the manuscript to address every point raised. The changes are **highlighted**."}}, "id": "gwx81p2VOi", "forum": "tgnXCCjKE3", "replyto": "tgnXCCjKE3", "signatures": ["ICLR.cc/2026/Conference/Submission8562/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8562/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission8562/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763454598792, "cdate": 1763454598792, "tmdate": 1763454598792, "mdate": 1763454598792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}