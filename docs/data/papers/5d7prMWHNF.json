{"id": "5d7prMWHNF", "number": 19397, "cdate": 1758295909848, "mdate": 1759897041472, "content": {"title": "Learning Robust Intervention Representations with Delta Embeddings", "abstract": "Causal representation learning has attracted significant research interest during the past few years, as a means for improving model generalization and robustness. Causal representations of interventional image pairs, have the property that only variables corresponding to scene elements affected by the intervention / action are changed between the start state and the end state. While most work in this area has focused on identifying and representing the variables of the scene under a causal model, fewer efforts have focused on representations of the interventions themselves. In this work, we show that an effective strategy for improving out of distribution (OOD) robustness is to focus on the representation of interventions in the latent space.  Specifically, we propose that an intervention can be represented by a Causal Delta Embedding that is invariant to the visual scene and sparse in terms of the causal variables it affects. Leveraging this insight, we propose a method for learning causal representations from image pairs, without any additional supervision. Experiments in the Causal Triplet challenge demonstrate that Causal Delta Embeddings are highly effective in OOD settings, significantly exceeding baseline performance in both synthetic and real-world benchmarks.", "tldr": "", "keywords": ["action representation", "causal representation learning", "interventions"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b867a4476053568941b880c38c6a407a52795d2.pdf", "supplementary_material": "/attachment/4e3d30faad53b994480d847f0d82637e29b76cbe.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Causal Delta Embeddings (CDE), a method to learn robust representations of interventions from image pairs. The key idea is to represent an intervention as the difference between pre- and post-intervention latent embeddings. The method combines cross-entropy, supervised contrastive, and sparsity losses to enforce independence, sparsity, and invariance. Results on the Causal Triplet benchmark show strong out-of-distribution generalization and meaningful representation structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation and solid theoretical grounding.\n- Simple but elegant framework using delta embeddings to capture interventions.\n- Strong experimental performance.\n- Well-written and well-structured paper, with convincing ablations and visualizations."}, "weaknesses": {"value": "- The method is trained with three losses. It is unclear how sensitive the performance is to the weighting between them. Although the appendix provides implementation details, it is not clear how one would set these weights in practice."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VRk3rHXRe4", "forum": "5d7prMWHNF", "replyto": "5d7prMWHNF", "signatures": ["ICLR.cc/2026/Conference/Submission19397/Reviewer_jtzb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19397/Reviewer_jtzb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857791772, "cdate": 1761857791772, "tmdate": 1762931317326, "mdate": 1762931317326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to learn representations corresponding to interventions, in contrast to learning representations for the underlying variables. The motivation behind this is to learn reusable representations/embeddings for interventions invariant of the object on which the intervention acts. From the paper, I gather that such representations are useful in Visual Language Action (VLA) models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea of learning representations for actions is interesting, and it is a reasonable choice to learn these representations using interventional data. I believe Causal Delta Embeddings (CDEs) will have practical applications in robotics and other interactive domains."}, "weaknesses": {"value": "I include my major concerns under \"weaknesses\" and minor concerns (mainly related to writing) under \"questions.\" My most important concerns are W1, W2, and W5 (d, e). Most weaknesses/questions can be answered without experiments. I will raise my score if my concerns are addressed.\n\n**W1. Interventional vs counterfactual**: A major confusion I have is whether CDE requires interventional or counterfactual data, the latter being a stricter requirement. In lines 194-197, the goal is stated to learn some function using a dataset of pre- and post-interventional data. If these data samples are obtained with the same exogenous noise, they are counterfactual samples. The equation in lines 157-158 mentions the exogenous noise $\\epsilon$, but its role in the training dataset is unclear. \"Identical noise\" is assumed in lines 211, which implies the requirement is counterfactual data. To satisfy eq. (1) using interventional data, the encoder $\\phi$ must disregard the exogenous noise completely. Can the authors clarify what the real requirements are?\n\n**W2. Should the underlying representation be identifiable?** Even if counterfactual data is provided, eq. (1) can be satisfied only using an identifiable $\\phi$. Is my understanding correct? If that's the case, will such an identifiable encoder $\\phi$ be automatically learned while learning CDE, or is it a starting requirement? If my understanding is wrong, please provide a counterexample and include it in the text. I have a related concern on line 216. It is written \"an action's representation is independent of the causally irrelevant...\" Does \"independent\" here mean statistically independent, or that it is not affected by the \"causally irrelevant elements?\"\n\n**W3. Will the aggregation module allow patch-level CDE?** In Sec. 5.2.1, the loss function is applied only over the aggregated embeddings. It is possible for patch-level embeddings to not follow Def. 2 even when the aggregated embedding satisfies Def. 2. Is that true? Are patch-level CDEs not required to follow Def. 2?\n\n**W4. Comparison to BISCUIT**: BISCUIT (Lippe et al., 2023) also encodes the action variable responsible for interventions. See Fig. 7 in (Lippe et al., 2023). Is it possible to compare CDE against BISCUIT?\n\n**W5. Questions on experiments**:\n\n**W5. (a)** What are the baseline models trained on? Lines 257-259 say that the encoder is a DINO-pretrained vision backbone. Are ResNets and ViT in Tables 1 and 2 also DINO-trained? Why is it compared against a CLIP in Table 2, instead of another DINO? What exactly is the oracle-mask approach?\n\n**W5. (b)** How are object-centric models, such as Slot Attention, adapted to predict actions?\n\n**W5. (c)** Why is OOD Comp. accuracy smaller than OOD Syst. accuracy for all baselines, except CDE in Table 1? OOD Syst. seems to be a more difficult task.\n\n**W5. (d)** How is CDE able to achieve that much accuracy in OOD Syst. in Table 1? The manifestations of actions in images are linked to the object on which it acts. So how can the model foresee what the action will look like on an unseen object? I can think of a possibility: CDE works only in scenarios where the action manifestation on the unseen object was seen during training, and that can also maybe explain the difference between OOD Comp. and OOD Syst. accuracies. Can the authors explain why CDE works on OOD Syst.?\n\n**W5. (e)** The CE-only model in Table 3 beats all baselines in Table 1. I thought vanilla-R and vanilla-V were also trained with just CE. So does CDE benefit from something beyond its loss functions?\n\n**W5. (f)** What is Fig. 10 supposed to convey? If the point was to show that CDEs are suitable for k-NN, then it must be compared with other baselines or other variants of CDE with fewer losses (like in Table 3). Although I would appreciate answering this question with experiments, I will not reduce my score if the experiments are not provided, as it is not a main experiment."}, "questions": {"value": "**Q1. Related works on CRL from interventional data**: In related works, some works on causal representation learning (CRL) that use interventional data are mentioned. However, these works are not mentioned in the introduction when the story is built around the existing state of CRL. I think it is important to highlight the current advances in CRL using interventional data, and how this work is different from them. Also, I would suggest adding more recent works in CRL using interventional data. See [A1-3] and the references therein. [A1] is a contemporary work. The authors need not include it, but I shared it here as a source for recent works.\n\n**Q2. Related works on contrastive learning**: There are two sentences on contrastive learning -- one listing two general contrastive learning works, and another providing a slightly unfair comparison w.r.t. this work. While contrastive learning only compares individual samples, it also does not require any pre- and post-intervention sample pairs, like this work. Another thing is that several works that link contrastive learning to CRL are missing. Some of them are [A4-5] and related works in [A6].\n\n**Q3. Related works on SMS**: Again, a few important works on SMS are missing from your literature survey. See [A7-9].\n\n**Q4. Minor writing comments**:\n\n**Q4. (a)** I suggest rewording the sentence in line 121, starting with \"While previous methods...\" to clarify what part of the interventional mechanism is captured by CDEs and what invariance (across contexts) is targeted by CDE.\n\n**Q4. (b)** Line 154: \"a set of causal variables $Z\\in\\mathcal{Z}\\subset\\mathbb{R}^l$...\" Which variable is the set here?\n\n**Q4. (c)** Line 161: \"a complex, **non-invertible** generative function...\" How can you learn actions if the variables on which the action works are not retrievable?\n\n**Q4. (d)** How did eq. (3) come about? Is $f$ in eq. (3) same as $f$ in line 157? How did the second equality in eq. (3) come about? Are the non-zero indices of $\\delta_a$ aligned with the changes in $z_a$ due to the action?\n\n**Q4. (e)** Can you clarify what the sentence in line 323 starting with \"If, however, these...\" means?\n\n**Q4. (f)** I find lines 425-427 to be slightly misleading. In Fig. 8, there are indeed some anti-parallel representations learned. But there are also some nearly-anti-parallel representations for action pairs that do not make sense. For example, cut-break, move-pull and move-eat have around -0.75 in Fig. 8.\n\n**Q4. (g)** Do any of the numbers in Tables 5 and 6 appear anywhere in the main results?\n\n**References**\n\n[A1] Pranamya Kulkarni, Puranjay Datta, Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Ali Tajer, \"ROPES: Robotic Pose Estimation via Score-Based Causal Representation Learning\", ArXiv 2025.\n\n[A2] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Ali Tajer, \"Linear Causal Representation Learning from Unknown Multi-node Interventions\", NeurIPS 2024.\n\n[A3] Dingling Yao, Dario Rancati, Riccardo Cadei, Marco Fumero, Francesco Locatello, \"Unifying Causal Representation Learning with the Invariance Principle\", ICLR 2025.\n\n[A4] Julius von Kügelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Schölkopf, Michel Besserve, Francesco Locatello, \"Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style\", NeurIPS 2021.\n\n[A5] Roland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, Wieland Brendel, \"Contrastive Learning Inverts the Data Generating Process\", ICML 2021.\n\n[A6] Dingling Yao, Danru Xu, Sébastien Lachapelle, Sara Magliacane, Perouz Taslakian, Georg Martius, Julius von Kügelgen, Francesco Locatello, \"Multi-View Causal Representation Learning with Partial Observability\", ICLR 2024.\n\n[A7] Elliot Layne, Jason Hartford, Sébastien Lachapelle, Mathieu Blanchette, Dhanya Sridhar, \"Sparsity regularization via tree-structured environments for disentangled representations\", ArXiv 2024.\n\n[A8] Sébastien Lachapelle, Pau Rodríguez López, Yash Sharma, Katie Everett, Rémi Le Priol, Alexandre Lacoste, Simon Lacoste-Julien, \"Nonparametric partial disentanglement via mechanism sparsity: Sparse actions, interventions and sparse temporal dependencies\", ArXiv 2024\n\n[A9] Danru Xu, Dingling Yao, Sébastien Lachapelle, Perouz Taslakian, Julius von Kügelgen, Francesco Locatello, Sara Magliacane, \"A Sparsity Principle for Partially Observable Causal Representation Learning\", ICML 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hw9sPf6b2C", "forum": "5d7prMWHNF", "replyto": "5d7prMWHNF", "signatures": ["ICLR.cc/2026/Conference/Submission19397/Reviewer_AZZ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19397/Reviewer_AZZ2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932140032, "cdate": 1761932140032, "tmdate": 1762931316884, "mdate": 1762931316884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the concept of delta embeddings to represent atomic causal interventions between two images (e.g. open or close a drawer on the image) and proposes a simple method for classifying intervention actions. The proposed method first separately generates embeddings of a pair of images distant by a single intervention using a backbone Vit coupled with a MLP head tasked to disentangle the embeddings into delta embeddings. Then, the difference between the two delta embeddings is computed and sent as an input to an action classifier. The pipeline is trained end-to-end and regularized with contrastive and sparsity losses to improve the creation of robust representations that hold out-of-distribution. The method achieves improved o.o.d performance compared to baselines and generates embeddings with meaningful relationships between classes of actions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a challenging problem in causal representation learning, namely the disentanglement of interventions, using a an original approach. The geometry of Delta embeddings could potentially convey very meaningful information, as hinted by the experiments and visualizations in the appendix.\n2. The paper is well-written and easy to understand. The theoretical section complements well the description of the approach, justifying it accurately.\n3. The experiments on out-of-distribution splits are particularly useful for assessing the generalization of the proposed delta embeddings."}, "weaknesses": {"value": "Experiments are conducted on a single benchmark (causal Triplet), which limits the generalizability of the findings (although the experiments in o.o.d settings mitigate this issue). Using larger backbone models on more datasets would further strenghten the contributions."}, "questions": {"value": "1. How realistic is the assumption of independence of latent factors in the data generative process? Indeed, the presence or absence of an object can have an effect on the lighting of the scene or on objects on top of it.\n2. Must the set of possible actions $\\mathcal{A}$ be known in advance or can the approach generalize to new actions, e.g. compositions of actions?\n3. Have you conducted experiments with additional backbone models, e.g. ResNet-18, for a fair comparison with other baselines?\n4. How do you interpret the drop in performance in o.o.d in Table 2 for the procTHOR multi-object setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7ttLyAYd9g", "forum": "5d7prMWHNF", "replyto": "5d7prMWHNF", "signatures": ["ICLR.cc/2026/Conference/Submission19397/Reviewer_RYXt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19397/Reviewer_RYXt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952457276, "cdate": 1761952457276, "tmdate": 1762931316308, "mdate": 1762931316308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the task of learning robust representations via interventional data. It proposes Causal Delta Embeddings (CDE) for representing interventions as latent-space deltas between pre/post-states. Results on synthetic and real benchmarks show gains in triplet evaluations and effective identification of changed semantics."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* tackles a very important problem of learning robust and interpretable representations \n* leverages pretrained vision encoders in a good way and moves away from toy-dataset-only evaluations. \n* clear conceptual framing of intervention representation problem\n* strong quantitative OOD gains; well-executed ablations\n* visualization & semantic structure analysis support claims"}, "weaknesses": {"value": "* only evaluates one vit backbone\n* requires heavy supervision that is only possible with synthetic data\n* empirical gains limited in real-world \n* lacks exploration of confounding effects or imperfect interventions"}, "questions": {"value": "* how robust is CDE to noise or partial observability in interventions?\n* does sparsity regularization risk collapsing subtle but real effects?\n* how does the performance change when using different pretrained models? MAE, VQ-VAE's encoder, CLIP  would be very interesting"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y2c6X2fWQZ", "forum": "5d7prMWHNF", "replyto": "5d7prMWHNF", "signatures": ["ICLR.cc/2026/Conference/Submission19397/Reviewer_Xc8e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19397/Reviewer_Xc8e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762276948454, "cdate": 1762276948454, "tmdate": 1762931315891, "mdate": 1762931315891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}