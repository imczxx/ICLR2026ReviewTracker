{"id": "QmgEUx2Cd7", "number": 22409, "cdate": 1758330666150, "mdate": 1759896867704, "content": {"title": "LORE: Jointly Learning The Intrinsic Dimensionality and Relative Similarity Structure from Ordinal Data", "abstract": "Learning the intrinsic dimensionality of subjective perceptual spaces such as taste, smell, or aesthetics from ordinal data is a challenging problem. We introduce LORE (Low Rank Ordinal Embedding), a scalable framework that jointly learns both the intrinsic dimensionality and an ordinal embedding from noisy triplet comparisons of the form, \"Is A more similar to B than C?\". Unlike existing methods that require the embedding dimension to be set apriori, LORE regularizes the solution using the nonconvex Schatten-p quasi norm, enabling automatic joint recovery of both the ordinal embedding and its dimensionality. We optimize this joint objective via an iteratively reweighted algorithm and establish convergence guarantees. Extensive experiments on synthetic datasets, simulated perceptual spaces, and real world crowdsourced ordinal judgements show that LORE learns compact, interpretable and highly accurate low dimensional embeddings that recover the latent geometry of subjective percepts. By simultaneously inferring both the intrinsic dimensionality and ordinal embeddings, LORE enables more interpretable and data efficient perceptual modeling in psychophysics and opens new directions for scalable discovery of low dimensional structure from ordinal data in machine learning.", "tldr": "LORE jointly infers both the intrinsic dimensionality and an ordinal embedding from noisy triplet comparisons, enabling compact, interpretable perceptual representations from purely ordinal data.", "keywords": ["Ordinal Embedding", "Intrinsic Dimensionality", "Psychophysics", "Subjective Perceptual Learning", "Relative Similarity Embedding"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9369124f2536ea9a9bf78e51e79d7bac581b71ed.pdf", "supplementary_material": "/attachment/c521f70070b352eab562379922f3518834e95769.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents LORE, a framework that jointly learns ordinal embeddings and their intrinsic dimensionality using a nonconvex Schatten-p regularization and an iteratively reweighted optimization algorithm. Experiments on synthetic and real perceptual datasets show that LORE can automatically recover low-rank, interpretable embeddings with competitive triplet accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents an interesting framework (LORE) that aims to jointly learn ordinal embeddings and their intrinsic dimensionality, addressing a recognized limitation of existing approaches.\n\n2. The proposed optimization procedure is clearly described and includes a convergence argument, suggesting technical soundness.\n\n3. Experimental results across synthetic and real perceptual datasets provide encouraging evidence that LORE can recover compact and interpretable embeddings."}, "weaknesses": {"value": "1. The paper lacks a theoretical analysis explaining under what conditions the Schatten-p regularization can correctly recover the intrinsic rank, which limits the strength of its main claim.\n\n2. The iteratively reweighted optimization is presented formally but lacks practical insight; for example, the paper does not show convergence curves, runtime comparisons, or how initialization influences final embeddings.\n\n3. The figures lack sufficient information for interpretation. Several plots (e.g., Figure 2 and Figure 3) omit axis labels or error ranges, and some results do not specify experimental settings or data sources, reducing the clarity and comparability of the findings.\n\n4. The study provides limited discussion of hyperparameter sensitivity, particularly the effects of λ and p on performance and rank estimation."}, "questions": {"value": "1. Under what data or noise conditions can the Schatten-p regularization reliably recover the intrinsic rank?\n\n2. How sensitive is LORE to the choices of \\lambda and p?\n\n3. Could the authors show more evidence of the optimization’s empirical behavior, such as convergence stability or runtime?\n\n4. All experiments are based on psychophysical or human-judgment data (food, music, cars). Have the authors tested, or plan to test, the method on non-perceptual ordinal datasets (e.g., image or text similarity)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xXm1PILwIx", "forum": "QmgEUx2Cd7", "replyto": "QmgEUx2Cd7", "signatures": ["ICLR.cc/2026/Conference/Submission22409/Reviewer_kWHX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22409/Reviewer_kWHX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979634360, "cdate": 1761979634360, "tmdate": 1762942207594, "mdate": 1762942207594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LORE (LOw Rank Embedding), a novel and scalable framework designed to jointly learn the intrinsic dimensionality ($d$) and the optimal relative structure of perceptual spaces from noisy ordinal data (triplet comparisons of the form \"A is more similar to B than C\"). Addressing a fundamental limitation of existing Ordinal Embedding (OE) methods that rely on pre-defined or estimated dimensions, LORE leverages a low-rank constraint on the embedding matrix $Z$ and employs a highly non-convex Schatten quasi-norm as a regularizer to promote the discovery of the true intrinsic dimensionality. The optimization is handled by an effective iteratively reweighted algorithm with provable convergence guarantees. Extensive experiments on synthetic data, simulated perceptual spaces, and real-world crowdsourced datasets demonstrate that LORE successfully recovers the true intrinsic rank, achieves competitive triplet accuracy, and yields semantically interpretable embedding axes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses the critical and underexplored problem of jointly discovering the intrinsic dimensionality and relative structure in perceptual spaces, which is a key limitation of prior Ordinal Embedding (OE) methods. The introduction of the low-rank constraint via the non-convex Schatten quasi-norm is highly novel within the OE literature.\n\n2. Unlike many empirical approaches, LORE provides a convergence theorem (Theorem 1, page 5) for its optimization objective and the proposed iterative reweighted algorithm. This rigorous theoretical foundation significantly strengthens the paper's contribution.\n\n3. The experiments are thorough and persuasive. LORE successfully recovers the true intrinsic rank in synthetic and simulated LLM perceptual spaces (Figure 4) where other baselines fail. On real crowdsourced data (Food-100, Musicians, Cars), it maintains high triplet accuracy while achieving significantly lower rank embeddings compared to SOTA OE methods (Table 5).\n\n4. The learned embedding axes (Figure 5) are shown to be semantically interpretable (e.g., \"Sweet to Savory,\" \"Learned Axis 1\"), offering valuable insights into the underlying perceptual characteristics of the data, which is highly beneficial for discovery tasks."}, "weaknesses": {"value": "1. While the paper provides a convergence theorem, the optimization objective $\\min \\Psi(Z)$ remains highly non-convex. The analysis primarily focuses on convergence to a stationary point, which may not always be the globally optimal solution. A more in-depth discussion on the practical robustness to initialization and the likelihood of escaping poor local minima would be beneficial.\n\n2. The LORE objective function includes several regularization parameters ($\\lambda, \\tau, \\mu$). Although Figure 2 demonstrates stability across a range of $\\lambda$ values for a fixed $\\tau$, a full exploration of the joint sensitivity of $\\lambda$ and $\\tau$ is absent. These parameters are crucial for balancing triplet accuracy and rank recovery, and their interplay needs more detailed investigation.\n\n3. The paper should explicitly discuss the cases where the intrinsic rank $d$ may not be an integer (e.g., fractional dimensionality in complex manifold structures) and whether LORE's reliance on a rank constraint limits its ability to fully capture these more intricate data structures."}, "questions": {"value": "1. The optimization relies on the non-convex Schatten quasi-norm. Could the authors provide a more detailed analysis or empirical evidence (e.g., through multiple restarts with different random initializations) showing the consistency and quality of the stationary points reached by the algorithm?\n\n2. The paper uses a simulated perceptual space derived from a large language model (LLM) embedding. Can the authors provide more intuition or validation for why the LLM's embedding space represents a \"true perceptual $d$-dimensional space\" that LORE is attempting to recover, and how the inherent noise was modeled in this specific experiment?\n\n3. Regarding the runtime complexity, how does the convergence speed (number of iterations for Algorithm 1) of LORE change as the total number of triplets ($T$) and the true intrinsic dimension ($d$) scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qboVUjx1Z0", "forum": "QmgEUx2Cd7", "replyto": "QmgEUx2Cd7", "signatures": ["ICLR.cc/2026/Conference/Submission22409/Reviewer_inQi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22409/Reviewer_inQi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012825464, "cdate": 1762012825464, "tmdate": 1762942207379, "mdate": 1762942207379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LORE (Low Rank Ordinal Embedding), an ordinal-embedding framework that jointly learns (i) an embedding that satisfies triplet comparisons and (ii) the intrinsic dimensionality (rank) of the latent perceptual space. The key idea is to regularize the embedding matrix with a nonconvex Schatten-p quasi-norm (with smoothing of the triplet loss), optimized via an iteratively reweighted scheme that is shown to converge to a stationary point. Experiments indicate that LORE achieves comparable triplet accuracy while discovering substantially lower-rank solutions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1 Tackles a long-standing limitation of ordinal embedding, i.e., choosing the dimensionality, by jointly inferring rank and coordinates, rather than grid-searching over dimensions. \n\n2 Uses Schatten-p regularization (p∈(0,1)) to promote low rank, with a softplus-smoothed triplet loss and an iteratively reweighted algorithm; provides a convergence-to-stationary-point guarantee and implementation details. \n\n3 Experiments indicate that LORE achieves comparable triplet accuracy while discovering substantially lower-rank solutions."}, "weaknesses": {"value": "1 The theory ensures convergence to a stationary point, but not global minima or exact rank identification; this is acknowledged as a limitation.\n\n2 The paper argues that LORE uncovers the intrinsic dimensionality without under- or over-estimating it. As stated, this reads as a subjective claim. Please provide stronger evidence to demonstrate that the method does not “mask” latent structure or inflate rank.\n\n3 This paper claims that Künstle et al. (2022) require specifying plausible dimensionalities, risking misspecification and loss of power if the true rank lies outside those bounds. Do you have experiments showing this failure mode and quantifying how often it occurs under realistic sampling/noise? \n\n4 The paper states that training separate embeddings per hypothesized rank (as in Künstle et al., 2022) is computationally prohibitive. Please report the total cost to reach the same triplet accuracy for both methods.\n\n5 Missing direct comparison to Künstle et al. (2022).\n\n6 Literature coverage is dated. The citations lean heavily on pre-2022 works and omit several recent, directly relevant works.\n\n[1] Künstle D E. Machine Learning for Psychophysical Scaling with Ordinal Comparisons[D]. Eberhard Karls Universität Tübingen, 2024.\n[2] Huber L S, Künstle D E, Reuter K. Tracing truth through conceptual scaling: Mapping people’s understanding of abstract concepts[J]. 2024.\n[3] Sauer Y, Künstle D E, Wichmann F A, et al. An objective measurement approach to quantify the perceived distortions of spectacle lenses[J]. Scientific Reports, 2024, 14(1): 3967.\n[4] Huber L S, Künstle D E, Reuter K. Tracing truth through conceptual scaling[J]. Cognition, 2026, 266: 106321."}, "questions": {"value": "1 Can you provide calibration evidence to show that the method neither hides structure nor inflates rank?\n\n2 Do you have experiments where the true rank lies outside the candidate set used by Künstle et al. (2022)? How often does this occur, and what is the performance degradation?\n\n3 For equal target triplet accuracy, what is the cost for LORE vs. training multiple embeddings as in Künstle et al. (2022)? \n\n4 Why is there no quantitative comparison to Künstle et al. (2022)? \n\n5 How does LORE differ conceptually and empirically from recent works?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SIY4AD16D9", "forum": "QmgEUx2Cd7", "replyto": "QmgEUx2Cd7", "signatures": ["ICLR.cc/2026/Conference/Submission22409/Reviewer_86F2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22409/Reviewer_86F2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762229377683, "cdate": 1762229377683, "tmdate": 1762942207193, "mdate": 1762942207193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper claims that all existing OE approaches are based on pre-specified embedding dimensions, which may lead to some problems. Meanwhile, the paper emphasizes the advantages of low intrinsic dimensional embedding--easier to interpret, less computationally intensive, while existing OE approaches based on pre-specified embedding dimensions often result in high-dimensional embeddings. \nBased on this, the paper introduces LORE (Low Rank Ordinal Embedding), a novel method for ordinal embedding that jointly learns both the low-dimensional embedding and **its intrinsic dimensionality** from noisy triplet comparisons.\nFurthermore, the paper establishes an efficient optimization strategy based on iteratively reweighted minimization and provides a scalable algorithm suitable for large-scale perceptual similarity data.\nAnd the paper validates the effectiveness and efficiency of LOPE through an extensive evaluation"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The explanation of the background and significance of the problem is very clear, and the problem to be solved is very meaningful.\n2. LORE is effective in reliably overlooking the intrinsic dimensionality and demonstrates the interpretability of low dimensional representations in semantics, which is helpful for solving problems in psychology, neuroscience, and social science.\n3. The theoretical explanation is very rigorous."}, "weaknesses": {"value": "1. More new methods should be compared, and more datasets should be compared, especially considering that SOE and t-STE are both methods from 2014. This may lead to doubts about the performance of LORE.\n2. On the accuracy metric, which may be the most important metric, LOPE is not always optimal or even suboptimal.\n3. A low rank does not necessarily mean an improvement in method performance, so more explanation is need.\n4. For the metric of computational efficiency(time), low dimensional embedding is not the only solution(eg: FORTE vs. LORE), so the advantages of LORE should be further explained.\n5. Following 3, if the embeddings of other OE methods are not interpretable, the differences between other methods and LORE should be compared in Figure 5."}, "questions": {"value": "1. If the embeddings of other OE methods are not interpretable, the differences between other methods and LORE should be compared in Figure 5.\n2. The factors that affect computational efficiency (time metric) may need to be explained in order for readers to understand why the time differences of methods such as SOE/t-STE can be so significant at the same rank."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qQ8nA5pHod", "forum": "QmgEUx2Cd7", "replyto": "QmgEUx2Cd7", "signatures": ["ICLR.cc/2026/Conference/Submission22409/Reviewer_sNLD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22409/Reviewer_sNLD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762704243678, "cdate": 1762704243678, "tmdate": 1762942206883, "mdate": 1762942206883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}