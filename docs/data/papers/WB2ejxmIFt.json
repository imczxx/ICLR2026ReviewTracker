{"id": "WB2ejxmIFt", "number": 13481, "cdate": 1758218456407, "mdate": 1759897434012, "content": {"title": "Scale-time Equivalence in Neural Network Training", "abstract": "Neural networks have demonstrated remarkable performance improvements as model size, training time, and data volume have increased, but the relationships among these factors remain poorly understood. We develop a theoretical framework to investigate the interplay between model size and training time, introducing the concept of scale-time equivalence: the idea that in certain training regimes, a small model trained for a long time can achieve similar performance to a larger model trained for a shorter time. To analyze this, we model neural network training as a gradient flow in random subspaces of larger non-linear models. In this setting, we show theoretically that neural network scale and training time can be traded off with each other. Empirically, we validate scale-time equivalence on MLPs and CNNs trained with gradient descent across standard vision benchmarks, CIFAR-10, SVHN, and MNIST.\n\nWe then investigate the consequences of scale-time equivalence on double descent, the phenomenon where model performance changes non-monotonically with respect to training data volume, model scale, and training time. In regimes where scale-time equivalence holds, we show that double descent with respect to training time and model scale may share a common cause: namely, overfitting to noise early during training. Through this, we provide potential explanations for several previously unexplained empirical phenomena: reduced data requirements for generalization in larger models, heightened sensitivity to label noise in overparameterized models, and instances where increasing model scale does not necessarily enhance performance.", "tldr": "We show that scaling the size of a network is equivalent to increasing training time; using this, we provide a novel explanation for scale-wise double descent.", "keywords": ["generalization", "neural network", "scaling law", "double descent"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f6dc0da991dc5a589870336ee6cf200a7cb4309.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper theoretically and empirically investigates the possibility of having training time as a variable for scaling law. It claims an equivalence between the model size and training time."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "n/a"}, "weaknesses": {"value": "1: This paper is based on a few strong, likely unrealistic, assumptions. Particularly, it assumes that (a) only a **linear subspace** affects the network output: $\\alpha = K\\beta$ with $K$ being a linear projection. I don’t see any reason or evidence that this linear assumption is reasonable; in practice, it could be highly non-linear. Another assumption made is that $K$ is fixed over training. As the authors also admit, it is more realistic to treat $K$ (if it at all exists, although I doubt) as time-varying. As all the results of the paper are based on these assumptions, if they are not true, the claims of the paper would not hold.\n\n2: I don’t understand the point of introducing a large $P$ dimensional space. Combining Eq. 1 and 2, one gets $\\alpha = KR \\theta + K\\beta_0 = R’\\theta + \\beta_0’$, where $KR$ is a linear projection and $K\\beta_0$ is a vector. What is the necessity of the complication of involving an additional large space?\n\n3: Another major concern of mine is that the theory, Theorem 3.1, does not really provide a meaningful bound. In Eq. 4, the bound is actually quite big: look at the exponential term, and think about the large value of both $p$ and $t$. I don’t think “this theorem implies closeness …” (Line 162) at all. With such a loose bound, discussions that follow become meaningless.\n\n4: concerns about technical statements: \n\n(a): Why the matrix $R$ of Eq. 1 is a random matrix with elements iid drawn from unit Gaussian(Line 135)? I don’t see a freedom of the choice of $R$.\n\n(b): (starting Line 139), what is the rationale behind the logic: (i) flat minima → redundant dimensions in the network; (ii) redundant dimensions in the network → linear projection?\n\n5: Empirically, it is commonly known that a small model, no matter how long it is training, can not achieve comparable performance of a large model. How do you reconcile this conflict with the “scale-time equivalence”?"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OqFz4hE6zd", "forum": "WB2ejxmIFt", "replyto": "WB2ejxmIFt", "signatures": ["ICLR.cc/2026/Conference/Submission13481/Reviewer_zSqk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13481/Reviewer_zSqk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589777492, "cdate": 1761589777492, "tmdate": 1762924099545, "mdate": 1762924099545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the generalization during neural network training in terms of model size, training size, and data volume. Interestingly, the authors introduce a concept called \"scale-time equivalence\", which means that a small model with long training achieves the same test/training performance as a large model with short training. In the paper, this equivalence is analytically shown by considering that the parameters of the small model lie in a random subspace of the parameter space of the large model. Then, the equivalence is illustrated in experimental settings involving MLPs & CNNs as models and MNIST, CIFAR10 & SVHN as datasets. Finally, the equivalence is used to explain the double descent phenomenon, for which the authors first analytically study a linear regression scenario to come up with a hypothesis that the double descent is related to the label noise, and then experimentally demonstrate the noise effects in the aforementioned experimental settings."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces an interesting and useful concept (scale-time equivalence). Although a relationship between model size and training time has been observed in prior works (Nakkiran et al., 2019;2021), theoretical analysis of such a relationship and finding an equivalence in terms of the test/generalization performance is the original aspect of this work, to the best of my knowledge. If this equivalence could be extended to real-world scenarios (beyond the image classification cases in the paper), it could be a valuable tool to study the learning behaviour of large models using smaller models. \n2. The authors utilize the introduced \"scale-time equivalence\" concept to explain the double descent phenomenon by linking it to the effect of label noise during training, indicating the usefulness of the concept."}, "weaknesses": {"value": "1. The presentation in the paper lacks the care required for a publication in a top venue. Specifically, the descriptions of the theoretical and experimental results are hard to follow since the authors tend to jump between arguments without explaining the connection properly (e.g., see section 3.1). Similarly, the quality of the figures is not good, and the captions are not descriptive enough. \n\n2. The motivations/explanations of some significant assumptions are lacking in the paper. For details, see my questions below.\n\n3. Although the paper is submitted under learning theory, it is light on theory. The only significant theoretical result in the paper is Theorem 3.1 (scale-time equivalence), but its proof boils down to bounding some simple terms while dealing with the randomness resulting from a random projection matrix with iid standard Gaussian entries. The reason for this is that the authors consider a set of artificial but significantly simplifying assumptions.\n\n4. While the authors discuss the equivalence in terms of test/training performance, the proven theorem (Theorem 3.1) just states the closeness of a low-dimensional projection of the parameters of the neural network to some quantity that depends on the product of model size and training time. To make this result significant, it should be analytically connected to the test/training performances of small and large neural networks considered in the paper. In its current state, the theoretical result is not persuasive enough for the claims.\n\n5. Experimental results in Figures 1 and 2 are not convincing enough, since although the trends observed in the figures can be related to the claimed equivalence, there is a significant gap between the predictions and experimental results beyond the standard deviation margins. \n\n6. Experimental settings are limited to image classification on simple datasets (MNIST, CIFAR10 & SVHN) with the performance measure being mean squared error (MSE)."}, "questions": {"value": "1. Regarding the assumptions:\n- a) How realistic is the assumption that the parameter vector of the small model lies in a random subspace of the parameter space of the large model?\n- b) Related to (a), the authors assumed the random projection matrix $R$ (between the parameter vectors) to be formed by i.i.d. standard Gaussian entries, which seems quite unrealistic while it significantly simplifies the proof. Could the authors relax this assumption?\n- c) Again, related to (a), by design, the parameters of the two models are tied to each other (Eq. 1), which sounds like the theoretical result stems from the design of this simplified setting, not from the nature of a general machine learning setting. What do the authors think about this?\n- d) Why do the authors need to introduce a third subspace ($r$-dimensional, smaller than the dimension of the small model) and assume that the output of the small model is a function of $\\alpha$ (projection of the parameters of the large model to the third subspace)?\n- e) What happens if the initialization (initial parameter vector) $\\theta = 0$ is relaxed?\n- f) In line 177, it is stated that the equivalence holds if the model size $p$ is large and $pt$ (the product of the model size and training time $t$) is small. This implies the equivalence is shown only for a small training time $t$, making the equivalence valid only under very limited cases. Is it possible to extend the equivalence to high $t$ cases? \n2. Regarding experiments:\n- a) What exactly do the authors mean by \"achieve non-zero generalization\" in line 197 and the Figure 1 caption?\n- b) For the experiments, the authors switch the definition of the scale: model size to the effective number of network parameters, defined\nas the maximum number of training points that can be fit by the network. What is the reason for this switch? How do the authors enforce/set the maximum number of training points that can be fit by the network?\n- c) For the experiments, the authors set the effective parameter count as the cube root of the number of parameters (based on a heuristic argument in Appendix B). Is it possible that this specific choice of model scale interferes with the experimental confirmation of the scale-time equivalence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TnnOFQhujn", "forum": "WB2ejxmIFt", "replyto": "WB2ejxmIFt", "signatures": ["ICLR.cc/2026/Conference/Submission13481/Reviewer_j6aS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13481/Reviewer_j6aS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758976829, "cdate": 1761758976829, "tmdate": 1762924098779, "mdate": 1762924098779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the concept of \"Scale-Time Equivalence\" (STE), proposing that increasing a neural network's size (scale) is functionally equivalent to extending its training duration (time). The authors develop a \"random subspace model\" to theoretically ground this equivalence, showing that under gradient flow, a model's training trajectory is primarily governed by the product of its effective scale ($p$) and time ($t$). Empirically, they demonstrate this trade-off in MLPs and CNNs on vision benchmarks. The primary application of this framework is a unified explanation for the double descent phenomenon. The paper argues that both scale-wise and time-wise double descent are manifestations of the same underlying mechanism: the rapid initial learning of noisy features, a process that smaller models traverse more slowly. This dynamics-focused perspective is contrasted with the conventional, static view of an interpolation threshold, and the authors present several experiments to adjudicate between these competing hypotheses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength is its conceptual novelty. The introduction of Scale-Time Equivalence (STE) offers an interesting and elegant attempt to provide a unified perspective on the distinct phenomena of scale-wise and time-wise double descent. A significant merit of this work is that it goes beyond proposing a new theory by empirically testing its predictions against the conventional wisdom. The authors design a series of experiments to directly adjudicate between their dynamics-based \"early noise acquisition\" hypothesis and the established \"interpolation threshold\" hypothesis, which provides a novel perspective."}, "weaknesses": {"value": "The main weakness of this paper is that its empirical claims are too weak to convincingly support whether scale-time equivalence is actually observed in deep learning. \n1. **Limited Scale:** The experiments are confined to small MLPs and CNNs on MNIST, CIFAR-10, and SVHN. This small-scale validation is insufficient to demonstrate that the observed equivalence would hold for the large-scale models where scaling laws are most relevant.\n2. **Failure with Adam Optimizer:** The STE phenomenon breaks down when using the Adam optimizer (Appendix D, Fig. 7). Given that Adam and its variants are the standard for training most modern networks, this finding severely restricts the practical applicability of the paper's core thesis.\n3. **Improper Hyperparameter Tuning:** The experiments use a fixed learning rate across all model scales. This is a methodological flaw, as the optimal learning rate often changes with model size.\n4. **Theory-Experiment Mismatch:** The theory, specifically the error bound in Theorem 3.1, is only non-vacuous in the limit of large model size ($p$) and small training progress ($pt$).1 The exponential term $(e^{\\eta pth...}-1)$ makes the bound loose for large pt, and the $1/\\sqrt{p}$ pre-factor requires large $p$ for the approximation to be tight. However, the empirical claims in Figure 2 explicitly test the predictive power in the opposite regime: using \"small models trained for long times\" to predict the performance of large models. This scenario (small $p$, large $t$) is precisely where the theoretical bound becomes vacuous.\n5. **Reliance on Ad-Hoc Heuristics:** The trade-off curves in Figure 1, which form the primary empirical evidence for STE, are critically dependent on defining \"effective scale\" as the cube root of the total parameter count ($p \\propto P_{abs}^{1/3}$).1 The justification for this is a non-rigorous heuristic argument (Appendix B) that appears tailored to fit the data. Furthermore, the visual fit of the data to the 1:1 proportionality lines lacks statistical quantification (e.g., goodness-of-fit), making the claim of a systematic trade-off less rigorous.\n6. **No Model Capacity Limits:** The STE framework does not account for a model's intrinsic capacity. The equivalence principle incorrectly implies a small model can match a large model's performance on any task if trained long enough, which is false for tasks that exceed the smaller model's representational capacity."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5L40f9vSu9", "forum": "WB2ejxmIFt", "replyto": "WB2ejxmIFt", "signatures": ["ICLR.cc/2026/Conference/Submission13481/Reviewer_Nweq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13481/Reviewer_Nweq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809473096, "cdate": 1761809473096, "tmdate": 1762924098092, "mdate": 1762924098092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the relationship between neural network performance, model size and training time. For certain regimes, they argue there is a tradeoff between training a large model for few steps and a small model for many steps. They propose a random subspace model and provide a bound in Thm 3.1 between the dynamics of $A_t$ (which is independent of model size) and $\\alpha_t$ which is the evolution induced by gradient descent on $p$ parameter model scales like $\\propto e^{ \\eta t p} - 1$. The authors argue that this indicates an equivalence between training time $t$ and model size $p$.  They provide several experiments on computer vision datasets (MNIST, CIFAR) which show an approximate linear tradeoff between $p$ and $t$. They then discuss double descent where scale-time invariance breaks and argue that it is driven by overfitting noise early in training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper provides an interesting idea, which is the approximate tradeoff between model size and training time in neural networks. They conduct many experiments to attempt to validate the hypothesis that the loss depends on $p \\times t$. They also provide many interesting observations about the source of double descent in linear models."}, "weaknesses": {"value": "While this work has many strengths, it also has major limitations in my opinion. Below I describe some of my concerns. That said, I am open to increasing my score provided these can be adequately addressed.\n\n**Novelty of training time vs model size tradeoff** Many works on scaling laws point out a tradeoff between model size and training time. In fact, the idea of compute optimal neural scaling laws (Hoffman et al) is based on choosing a joint scaling of $p$ and $t$ to follow the Pareto frontier in performance.\n\n**How does Bound Connect to Loss**  The authors provide a bound which connects the dynamics of the small $p$ dimensional model to the unprojected dynamics. However, they do not provide a direct formula for how the loss itself depends on $t$ and $p$. To make claims about scale time equivalence (at the level of loss), they need to connect their bound to performance.\n \n**Limit Behavior** The performance of the neural network should behave sensibly if $p \\to \\infty$ at fixed $t$ or $t \\to \\infty$ at fixed $p$. The bound provided is too loose to capture the behavior of the model in either of these regimes. However, the common chinchilla scaling law\n\n$$\\mathcal L = t^{- \\alpha} + p^{-\\beta}$$ \n\nadmits one to analyze either the infinite time or infinite parameter limit. \n\n**Connection To Double Descent Section Unclear** The authors provide an analysis of a linear model with signal and noise directions to explain non-monotonic behaviors in training. It is unclear what this has to do with the bound in Thm 3.1."}, "questions": {"value": "1. Can the authors use their Thm 3.1 bound to describe the time and parameter dependence of the loss itself?\n2. In chinchilla scaling laws $\\mathcal L = t^{-\\alpha} + p^{-\\beta}$ there is also an approximate scale time equivalence at the compute optimal joint scaling if $\\alpha=\\beta$. In that case the optimal parameter size $p_\\star(t) \\propto t$ and then the loss depends only on the product of $t p$. Could something like this be explaining the experiments?\n3. Could the derived bound be very loose? It does not give a sensible result if $t \\to \\infty$ with fixed $p$ or if $p \\to \\infty$ at fixed $t$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5XlsurUu4T", "forum": "WB2ejxmIFt", "replyto": "WB2ejxmIFt", "signatures": ["ICLR.cc/2026/Conference/Submission13481/Reviewer_RVis"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13481/Reviewer_RVis"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837173058, "cdate": 1761837173058, "tmdate": 1762924097579, "mdate": 1762924097579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}