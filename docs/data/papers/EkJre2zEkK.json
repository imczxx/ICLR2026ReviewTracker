{"id": "EkJre2zEkK", "number": 8853, "cdate": 1758099963756, "mdate": 1759897759543, "content": {"title": "A Fast and Scalable Extented Hoyer Projection for Structured Neural Network Sparsity", "abstract": "This paper introduces a novel and efficient projection method based on the extended Hoyer score, designed to induce sparsity in neural networks. While the classical $\\ell_1$ regularizer is almost everywhere differentiable and admits efficient linear-time projections, it lacks scale invariance. In contrast, the Hoyer score combines the advantages of differentiability and scale invariance, but its associated non-convex optimization problem is challenging.\n\nTo address this issue, we define the extended Hoyer score and analyze the properties of the corresponding space. \nBuilding on a first lemma showing that the extended Hoyer surface forms a hypercone, we propose a naïve projection algorithm. \nThen, leveraging a second lemma that provides an efficient thresholding rule, we derive a fast projection algorithm requiring only a single hypercone projection. \nFurthermore, we extend the classical bi-level $\\ell_{1,\\infty}$ projection to the more general $\\ell_{H,\\infty}$ projection, enabling improved column-wise structured sparsity.\n\nExperimental results demonstrate that our extended Hoyer projection achieves linear complexity. We apply the proposed bi-level Hoyer projection to classification tasks using autoencoder and transformer architectures. The experiments show that our method not only directly determines the number of selected features but also improves the accuracy-sparsity trade-off.", "tldr": "This paper introduces a novel and efficient projection method based on the extented Hoyer score, designed to induce sparsity in neural networks.", "keywords": ["Optimisation of Neural networks sparsity", "Structured  Sparsity", "Extent Hoyer score", "Extent Hoyer score geometry", "Fast Hoyer Projection"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba08c81977f8690d90b4f537f82fc23f676b9586.pdf", "supplementary_material": "/attachment/bc9affee34e29c06d01ae00a7aa656efeb7dad58.zip"}, "replies": [{"content": {"summary": {"value": "This paper resolves the non-convex optimization issue associated with achieving structural sparsity with Hoyer-based regularizations. The paper makes theoreitical analysis on the space of Hoyer score and proposes a fast projection algorithm with a single hypercone projection onto the Hoyer score surface. The proposed Hoyer projection leads to improved efficiency-performance tradeoff on autoencoder and transformer tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper makes novel observation and analysis on the surface of Hoyer score. This leads to the first projection-based optimization method with the Hoyer measurement, which is a significant contribution\n2. The paper makes solid theoreitical derivations to show the correctness and effectiveness of the proposed projection. \n3. The overall presentation is good. The paper is easy to follow and proposed method is well-motivated."}, "weaknesses": {"value": "The major weakness of this work comes with the emperical evaluations. Though the paper claims to resolve the non-convex optimization issue of the ogirinal Hoyer regualrization, whether this is a practically meaningful in deep learning remains doubtful. Deep learning itself is known to be non-convex in most cases, where having a non-convex regualrization added to the loss may not bring much addiitonal difficulty to the optimization. The paper should make a thorough comparison between the Hoyer regularization optimization and the proposed Hoyer projection to show if the proposed method is more effective and/or easier to optimize.\n\nFurthermore, there lacks adequate comparison against related work on sparisity-inducing regualrizations, projections, and neural network pruning methods. The only baseline used in the experiments is L1-based method, which is very basic and has shown to be performing worse than more recent projection and regualrization methods. Adding more baselines, and potentially adding more neural network models into the experiment setting can better justify the method emerically."}, "questions": {"value": "1. Please do a direct coparison of the cost and performance between the proposed Hoyer projection and directly performing gradient descent with Hoyer regularization\n2. Please add more baselines, especially traditional projection-based sparisification methods like SCAD and MDP"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "192bMKMMVH", "forum": "EkJre2zEkK", "replyto": "EkJre2zEkK", "signatures": ["ICLR.cc/2026/Conference/Submission8853/Reviewer_fkhc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8853/Reviewer_fkhc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761685839906, "cdate": 1761685839906, "tmdate": 1762920617409, "mdate": 1762920617409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a projection method for neural network sparsity based on an \"extended Hoyer score.\" The authors describe the geometric properties of this score, showing that its level sets form a hypercone, and leverage this to derive a \"one-shot,\" linear-time projection algorithm. This single-vector method is further extended to a bi-level $\\ell_{H,\\infty}$ projection to induce structured, column-wise sparsity. The approach is evaluated on autoencoder and transformer architectures, where the authors report improvements in the accuracy–sparsity trade-off."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Clear Motivation and Problem Framing: The paper does a good job motivating the need for a scale-invariant, differentiable sparsity-inducing method."}, "weaknesses": {"value": "**Weakness:**\n\n**1. Fundamental lack of novelty leading to re-solving previously solved problems:**\n    The major weakness of this paper is a fundamental lack of novelty. Both the Hoyer score and sparsity criterion have been well-studied [1, 2, 3, 4] and in better depth. Specifically, [2, 3] (following from [1] and Hoyer's original work [5]) solve the exact problems this paper addresses  (single-vector and grouped projection, respectively) with far more rigor, theoretical depth, and experimental coverage. The failure to cite or compare against this seminal works fundamentally fails to represent state of the problem the paper is trying to solve.\n    \n**2. Algorithms are unproven heuristics with significant theoretical gaps:**\n    The paper’s algorithmic framing, based on an \"extended Hoyer\" score and \"hypercone geometry,\" is a reframing of the standard orthant-projection trick already used in prior work [3]. The algorithms derived from this framing lack theoretical justification: the \"naive\" alternating projection (Algorithm 1) has no convergence guarantee, and the \"fast\" algorithms are unproven heuristics for problems that have already been solved *exactly* with rigor.\n\n**2.1 For the single-vector projection (Algorithm 2):**  \nThe paper proposes an iterative hard-thresholding/active-set heuristic without proofs of uniqueness or global optimality. The claimed $O(n)$ complexity is also unsupported by a formal bound on the *while* loop's iterations. Closely related subproblems were already handled by a well-established line of work the authors do not cite: *Potluru et al.* (2013) [1] provided an exact $O(n\\log n)$ routine (\"Sparse-opt\") for the fixed $\\ell_1/\\ell_2$ constraint, and *Thom et al.* (2015) [2] subsequently developed a provably exact $O(n)$ Euclidean projector via a soft-thresholding Representation Theorem.\n\n**2.2 For the structured-sparsity projection (Algorithm 3):** The proposed bi-level $l_{H,\\infty}$ extension (Algorithm 3) is presented as a novel method for structured sparsity. However, it appears to be a simple heuristic composition of a single-vector Hoyer projection and an $l_{\\infty}$ projection. The paper provides no proof of feasibility or equivalence to a well-defined constrained optimization problem and fails to compare against rigorous, jointly-optimized group Hoyer projectors like GSP [3].\n    \n**Weak Empirical Validation:** The experiments are insufficient to support the paper's claims. The datasets used are either very small (like the 779-sample HIF2 dataset) or are not the most representative benchmarks for Transformer sparsification (like the tabular Otto dataset). They are small-scale, lack statistical rigor (e.g., confidence intervals), and most importantly, omit a direct runtime and accuracy comparison to the most relevant baselines [2, 3]. The reported runtime improvements over the old Hoyer (2004) [5] baseline are modest, and the accuracy differences are not shown to be statistically significant. \n\nTypo/error: \n1. Main title on openreview says \"extented\", do the authors mean extended?\n2. Pdf title makes the typo: \"EXTENDEDED\",  instead of \"Extended\" again?\n\n**References:**\n1. Potluru, Vamsi K., Sergey M. Plis, Jonathan Le Roux, Barak A. Pearlmutter, Vince D. Calhoun, and Thomas P. Hayes. \"Block coordinate descent for sparse NMF.\" _arXiv preprint arXiv:1301.3527_ (2013).\n2. Thom, Markus, Matthias Rapp, and Günther Palm. \"Efficient dictionary learning with sparseness-enforcing projections.\" _International Journal of Computer Vision_ 114, no. 2 (2015): 168-194.\n3. Ohib, Riyasat, Nicolas Gillis, Niccolo Dalmasso, Sameena Shah, Vamsi K. Potluru, and Sergey Plis. \"Explicit Group Sparse Projection with Applications to Deep Learning and NMF.\" _Transactions on Machine Learning Research_.\n4. Yang, Huanrui, Wei Wen, and Hai Li. \"Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures.\" _arXiv preprint arXiv:1908.09979_ (2019).\n5. Hoyer, P. O. (2004). Non-negative matrix factorization with sparse\u0002ness constraints. Journal of Machine Learning Research, 5, 1457– 1469."}, "questions": {"value": "1. Could the authors clarify the theoretical or empirical advantages of their proposed iterative hard-thresholding heuristic (Algorithm 2) over the existing, provably-exact soft-thresholding $O(n)$ algorithm from [1, 2]?\n1.1 Specifically, is this heuristic guaranteed to converge to the same optimal solution?\n2. This is not an experimental benchmark paper and the paper introduces three new algorithms (Algorithms 1, 2, and 3) as mathematical optimization methods. Could the authors provide the corresponding formal guarantees for these algorithms, such as proofs of convergence, solution uniqueness, and global optimality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ru3lMzhHX3", "forum": "EkJre2zEkK", "replyto": "EkJre2zEkK", "signatures": ["ICLR.cc/2026/Conference/Submission8853/Reviewer_KKLj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8853/Reviewer_KKLj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885478194, "cdate": 1761885478194, "tmdate": 1762971942117, "mdate": 1762971942117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a fast and scalable projection method for inducing structured sparsity in neural networks. It introduces an extended Hoyer score that transforms the projection space into a hypercone, allowing efficient computation. Experiments on synthetic data, autoencoders, and transformer models show faster convergence and better accuracy–sparsity trade-offs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces an extended Hoyer score that turns the projection space into a hypercone, enabling an efficient one-shot linear-time projection. It further extends to structured sparsity via a novel bi-level projection."}, "weaknesses": {"value": "1. Only a few datasets (HIF2, Otto, SST-2) are tested; results could be strengthened by larger and more diverse benchmarks (e.g., ImageNet-scale, vision transformers).\n2. Training setup lacks details (e.g., number of epochs, batch size, sparsity target l, and hyperparameter tuning).\n3. The paper focuses on geometric intuition but lacks convergence guarantees or formal proof.\n4. The paper does not compare with other sparse training methods.\n5. The paper does not present computational advantage, like speed."}, "questions": {"value": "1. Can the authors provide wall-clock runtime comparisons or FLOPs analysis on real GPUs to support the claimed linear complexity improvement?\n2. Since the bilevel projection involves nested optimization, how stable is the gradient flow during end-to-end training? Any divergence observed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZzsSm6b9Tx", "forum": "EkJre2zEkK", "replyto": "EkJre2zEkK", "signatures": ["ICLR.cc/2026/Conference/Submission8853/Reviewer_bMyb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8853/Reviewer_bMyb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951811038, "cdate": 1761951811038, "tmdate": 1762920616696, "mdate": 1762920616696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the question of inducing sparsity in nonconvex optimization problems using variants of the Hoyer sparsity measure. Efficient algorithms are proposed and validated on real-world datasets and deep learning architectures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(i) Considers the Hoyer sparsity criterion in which the l1 norm is relaxed to a sum and the resulting score is analyzed to provide an efficient algorithm.\n(ii) A structured sparsity setting is also briefly considered in the main paper and an algorithm is proposed to optimize for it.\n(ii) Experiments are provided to show the benefits of the proposed approach on the sparse attention matrices for transformers on the OTTO group classification dataset and GLUE."}, "weaknesses": {"value": "(a) It is unclear how the new score that is proposed satisfies desirable properties of sparsity (*)\n(b) Prior works on analyzing and proposing efficient algorithms for Hoyer sparsity are not considered (*)\n(c) Experiment results do not compare to other pruning/distillation approaches in the literature. \n\n* Comparing measures of sparsity. https://arxiv.org/abs/0811.4706\n* First results  on uniqueness of SPARSE NMF https://www.eurasip.org/Proceedings/Eusipco/Eusipco2005/defevent/papers/cr1658.pdf\n* Block Coordinate descent for Sparse NMF https://arxiv.org/abs/1301.3527\n* Sparse Activity and Sparse Connectivity in Supervised Learning. JMLR \n* Explicit Group Sparse Projection with Applications to Deep Learning and NMF  https://arxiv.org/abs/1912.03896"}, "questions": {"value": "(1) What is the stated novelty of the proposed approach over the prior literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nYvAGwFIE6", "forum": "EkJre2zEkK", "replyto": "EkJre2zEkK", "signatures": ["ICLR.cc/2026/Conference/Submission8853/Reviewer_GnD1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8853/Reviewer_GnD1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031224069, "cdate": 1762031224069, "tmdate": 1762920616390, "mdate": 1762920616390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}