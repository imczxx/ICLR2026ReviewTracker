{"id": "Ha075JDMZR", "number": 10250, "cdate": 1758165094816, "mdate": 1759897663197, "content": {"title": "MotionGPT3: Human Motion as a Second Modality", "abstract": "With the rapid progress of large language models (LLMs), multimodal frameworks that unify understanding and generation have become promising, yet they face increasing complexity as the number of modalities and tasks grows. We observe that motion quantization introduces approximation errors that cap motion quality, and that unifying discrete text and continuous motion within a single-stream backbone amplifies cross-modal interference. Motivated by recent multi-branch Transformer designs that separate signals from different modalities, we propose MotionGPT3, a bimodal motion–language model for both understanding and generation. MotionGPT3 encodes raw motion into a continuous latent space using a variational autoencoder (VAE), thereby avoiding quantization-induced artifacts, while leveraging the semantic prior of pretrained language models. A dual-stream Transformer with shared attention preserves modality-specific routes while enabling controlled, bidirectional information flow, which reduces interference, stabilizing optimization, and empirically accelerates convergence without degrading fidelity. For multimodal joint training, a generate-then-align three-stage schedule further improves stability and limits cross-task interference. Experiments show that MotionGPT3  achieves 2× faster convergence in training loss and up to 4× faster convergence in validation, while maintaining state-of-the-art performance on standard motion understanding and motion generation benchmarks.", "tldr": "We propose MotioinGPT3, a bimodal motion-language framework designed to address the challenges of unified motion understanding and generation.", "keywords": ["3d motion", "text-driven motion generation", "text-to-motion", "human motion synthesis", "motion caption"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/581f5315bee66966b2c16d5fedd95c5380f42de9.pdf", "supplementary_material": "/attachment/b1503895d23368eeae1cfe8ad8f568715cfe0607.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents MotionGPT3, a motion–language model designed to jointly handle motion understanding and generation while addressing key limitations of existing multimodal frameworks. To avoid the motion quantization issue, it encodes raw motion into a continuous latent space with a lightweight diffusion head, eliminating quantization-induced artifacts and enabling higher-fidelity synthesis. The architecture employs a dual-stream Transformer with shared attention, which maintains modality-specific information while allowing cross-modal exchange. Besides, a three-stage generate-then-align training schedule is proposed to further enhance the  convergence  efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well written, easy to follow, and clearly organized. The figures are self-explanatory.\n* The motivation is concrete and reasonable, and the method design is aligned well with the corresponding motivations, presenting sound performance improvements.\n* This paper offers a well-reasoned perspective on the gap between discrete language token sequences and continuous motion latent representations, which can provide valuable insights to the community."}, "weaknesses": {"value": "* Lack of detailed comparisons of inference latency or FLOPs against discrete-token baselines. Authors should provide the computational overheads induced by each proposed component to demonstrate the method's efficiency.\n* The evaluation of the method is constrained to a single dataset (HumanML3D), which can not sufficiently demonstrate the generalization of the proposed framework to other motion domains. Providing more results on other benchmarks with diverse features would better illustrate the method's versatility. Besides, it is expected to offer more results with more recent language models other than GPT-2.\n* As shown in Table 1, why does the proposed method significantly lower on the MModality metric compared to other models? Does this indicate that the model can only fits a specific distribution, thereby sacrificing the diversity of generated outputs."}, "questions": {"value": "* typos: line 32, citepacross"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JjMgDGE3gu", "forum": "Ha075JDMZR", "replyto": "Ha075JDMZR", "signatures": ["ICLR.cc/2026/Conference/Submission10250/Reviewer_9qBu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10250/Reviewer_9qBu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831044031, "cdate": 1761831044031, "tmdate": 1762921607126, "mdate": 1762921607126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MotionGPT3, a unified model that addresses motion quantization errors and cross-modal interference in motion-language tasks. Its key innovations include: 1) a VAE-based continuous motion latent space, 2) a dual-stream Transformer for controlled cross-modal interaction, and 3) a diffusion head to connect discrete text and continuous motion. Trained in three stages, MotionGPT3 sets new state-of-the-art performance on HumanML3D for both text-to-motion and motion-to-text generation, while converging 2-4 times faster than baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The technical design in this paper is highly targeted, with each core component—a continuous VAE, a dual-stream architecture, and three-stage training—precisely solving a specific problem, and its necessity is rigorously validated through ablation studies, resulting in a lean and non-redundant overall architecture.\n\n2. The experimental validation is comprehensive, encompassing quantitative comparisons, qualitative examples, and thorough ablation studies.\n\n3. The study ensures high reproducibility by detailing implementation specifics—including data preprocessing, training protocols, and evaluation tools—and adheres to open-source standards with released code and materials."}, "weaknesses": {"value": "1. The bimodal branch architecture proposed in this paper to address cross-modal interference in motion-language modeling is not a particularly novel approach, as similar frameworks have been proposed in existing unified text-image understanding and generation work, such as BAGEL [1]. However, the paper lacks discussion on how the proposed method differs from these existing approaches.\n\n2. Baseline comparisons are outdated, lacking recent models (e.g., MotionGPT-2 2024 [2]，MG-MotionLLM 2025 [3]), which may conceal performance gaps in key metrics like M2T BERTScore.\n\n3. The paper provides analysis on training convergence speed but lacks evaluation of inference latency, which remains a critical metric for motion generation applications.\n\n[1] Emerging Properties in Unified Multimodal Pretraining, 2025, arxiv, 27 July\n[2] MotionGPT-2: A General-Purpose Motion-Language Model for Motion Generation and Understanding, 2024 arxiv, 29 Oct\n[3] MG-MotionLLM: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities, 2025 arxiv, 3 April"}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jBvx57BGLQ", "forum": "Ha075JDMZR", "replyto": "Ha075JDMZR", "signatures": ["ICLR.cc/2026/Conference/Submission10250/Reviewer_cYVn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10250/Reviewer_cYVn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882754919, "cdate": 1761882754919, "tmdate": 1762921606598, "mdate": 1762921606598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose MotionGPT3, a bimodal motion-language framework to unify the motion understanding and generation. It introduces two different modality branches to endow the model with motion-to-text and text-to-motion abilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper contains numerous figures and tables, as well as abundant visualization results, with a relatively clear overall structure.\n\n2. Video demos are provided, demonstrating excellent performance.\n\n3. Motion generation and motion understanding tasks are realized through two different branches and fine-tuning of the LLM.\n\n4. The experimental results have achieved significant improvements."}, "weaknesses": {"value": "1. The description of Figure 2 and the method section is not clear enough, making it difficult to intuitively grasp the authors' entire training process design and the detailed reasoning procedure.\n\n2. Although the paper achieves good results, the method feels relatively incremental and highly hierarchical, lacking overall simplicity. Compared with MotionGPT and MotionGPT2, it does not bring a strong sense of novelty.\n\n3. The autoregressive continuous token proposed by the authors has been used in many motion generation papers, which makes the contribution seem insufficient.\n\n4. There are some typos. For example, is \"L40\" supposed to be \"LLM\"? The content from Line 82 to Line 89 is better presented in bullet points. Rarely seen in introductions is the writing style from Line 57 to Line 80, which fails to convey the necessity and practicality of the method proposed by the authors."}, "questions": {"value": "1. It is hoped that the authors can elaborate on the training details of the three stages, including the inputs and outputs. In particular, they should clarify why such inputs are used in the second stage and what advantages they bring. Additionally, what is the main purpose of keeping the text branch frozen in the second stage? Could the authors provide an input example for Stage 2?\n\n2. What exactly are the advantages of MotionGPT3 compared to MotionGPT and MotionGPT2, and which problems that the latter two failed to solve have been addressed? Relevant experiments would be highly appreciated.\n\n3. The authors are expected to clarify the points mentioned in the \"Weakness\" section.\n\nIf the authors can address the questions I raised, I may consider increasing the score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bpY8yljEke", "forum": "Ha075JDMZR", "replyto": "Ha075JDMZR", "signatures": ["ICLR.cc/2026/Conference/Submission10250/Reviewer_VYUy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10250/Reviewer_VYUy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995761027, "cdate": 1761995761027, "tmdate": 1762921606059, "mdate": 1762921606059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a bimodal motion–language model for text-to-motion and motion-to-text generation. The key designs include: a continuos latent modtion space with a latent diffusion header to bridge its gap between the next-token predition framework, a dual branch framework with shared attention to bridge the gap of the two modalities, and a three stage training strategy for more stable optimization of the proposed framework. Experimental results show the effectiveness of the proposed framework on both the text-to-motion and motion-to-text tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation of utilizing the continuous motion latent space for lossless motion encoding and the diffusion header to bridge the gap between the next-token generation framework is reasonable. \n- The dual-branch framework to preserve modality-specific information and the shared attention for cross-modal communication is well motivated, and the three-stage training schemes stabilize the optimization of the proposed framework. \n- Experimental results on benchmarks of the two tasks are strong, and the effect of different design choices is validated with ablation studies. \n- The writing is good and the paper is easy to understand."}, "weaknesses": {"value": "- The paper claims continuous VAE for motion encoding is better, but lacks an experimental comparison on motion encoding and decoding quality with previous schemes. Specifically, how is the improvement of the continuous VAE compared to the recently stronger motion quantization methods, e.g., the residual VQ proposed by MoMask (CVPR 2024) and the 2D motion quantization in MoGenTS (NeurIPS 2024)?\n- Experiments are only conducted on the HumanML3D datasets. Adding more diverse datasets, e.g., Motion-X and  KIT-ML, will better illustrate the generalizability of the proposed framework."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WUXcBHHgoc", "forum": "Ha075JDMZR", "replyto": "Ha075JDMZR", "signatures": ["ICLR.cc/2026/Conference/Submission10250/Reviewer_Hcm9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10250/Reviewer_Hcm9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762316189812, "cdate": 1762316189812, "tmdate": 1762921605654, "mdate": 1762921605654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}