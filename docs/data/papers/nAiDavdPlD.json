{"id": "nAiDavdPlD", "number": 16651, "cdate": 1758267323640, "mdate": 1759897227369, "content": {"title": "Eliciting Human-like Social Reasoning in Large Language Models", "abstract": "Large language models (LLMs) have gained significant attention for their potential to replicate human participants in social science simulations. \nHowever, previous works on LLM reasoning focus on enhancing the capabilities for math and logical problems, overlooking the reasoning process behind social behavior, such as controversial social attitudes, moral dilemmas, and economic games.\nIn this study, we explore the limitations of current models and propose a new approach to improve their human-likeness in social behavioral reasoning tasks. \nWe introduce the Social-Behavioral-Reasoning (SBR) dataset, comprising 1,560 quadruples of human profiles, social questions, reasoning processes, and final choices. \nUtilizing this dataset, we evaluate large reasoning models (LRMs), revealing a contradiction: while LRMs increase society-level diversity, they fail to maintain individual-level accuracy.\nOur findings further indicate that the observed increase in diversity is primarily attributed to random variation introduced by longer reasoning durations, rather than improved understanding of human diversity.\nTo address these issues, we propose the Reasoning-Enhanced-SFT method, which explicitly aligns both the reasoning and final choices with human data. \nOur experimental results demonstrate that our method significantly improves both in-domain and out-of-domain performance, enhancing the generalization ability across diverse social contexts.\nOur user study results confirm the model's ability to produce a reasoning process more closely aligned with specific human reasoning patterns.\nOur work offers a new pathway to overcome the challenges that limit the use of LLMs in social simulations. Aligning model outputs with human reasoning boosts LLMs' credibility and applicability in social science, enabling more precise and insightful simulations of human behavior.", "tldr": "", "keywords": ["Large Language Model", "Social Simulation", "LLM Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b680f7d20d21fb045088ba64063d54f6b126f2a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Social-Behavioral-Reasoning (SBR) dataset and a Reasoning-Enhanced Supervised Fine-Tuning (SFT) framework that jointly trains models on both reasoning processes and final decisions, improving the human-likeness and interpretability of large language models in social reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clearly identifies a meaningful gap, the lack of social reasoning capability in current LLMs, and frames it as a concrete research problem.\n\n2. Proposes a straightforward yet effective Reasoning-Enhanced SFT approach that aligns both reasoning processes and behavioral outputs.\n\n3. Constructs and releases the SBR dataset that explicitly links human profiles, social questions, reasoning texts, and behavioral choices, providing a valuable foundation for future studies."}, "weaknesses": {"value": "1. The approach mainly involves building a dataset and applying standard SFT/LoRA fine-tuning; the improvements largely stem from data rather than algorithmic innovation.\n\n2. Experiments are confined to quantitative comparisons on Collective Diversity (CD) and Individual Realism (IR), without richer qualitative or interpretive insights.\n\n3. While well-structured, the dataset is relatively small (1,560 samples) and lacks systematic comparison with prior social or moral reasoning datasets."}, "questions": {"value": "How would you utilize the remaining space for more analysis with depth?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3uxXdK3LrY", "forum": "nAiDavdPlD", "replyto": "nAiDavdPlD", "signatures": ["ICLR.cc/2026/Conference/Submission16651/Reviewer_TdxY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16651/Reviewer_TdxY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760771555360, "cdate": 1760771555360, "tmdate": 1762926712183, "mdate": 1762926712183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Social-Behavioral-Reasoning (SBR) dataset and a fine-tuning method called Reasoning-Enhanced-SFT, aiming to improve LLMs’ ability to perform human-like social reasoning. The authors analyze reasoning models and find that longer reasoning increases collective diversity but decreases individual realism. The proposed method jointly aligns reasoning traces and behavioral outcomes, leading to improvements on the SBR dataset and limited out-of-domain generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  Provides a novel dataset linking human profiles, reasoning processes, and behavioral choices.\n\n2. Offers empirical insight into the diversity–realism trade-off in reasoning models."}, "weaknesses": {"value": "1. Lack of novelty. The proposed Reasoning-Enhanced-SFT is a straightforward extension of existing SFT with reasoning supervision, and the improvements may stem mainly from additional data rather than methodological innovation.\n\n2. Limited contribution. The SBR dataset is small and simple, which restricts both the generality and the depth of the findings. The analysis remains mostly descriptive without deeper insight.\n\n3. The claimed gains in diversity and realism are largely in-domain, and the external evaluation is narrow in scope."}, "questions": {"value": "1. How was the reasoning quality and consistency in SBR verified?\n\n2. How would this approach generalize to other domains of social reasoning?\n\n3. Could the observed gains be explained by longer reasoning sequences rather than true alignment with human reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ljTbSXvfoW", "forum": "nAiDavdPlD", "replyto": "nAiDavdPlD", "signatures": ["ICLR.cc/2026/Conference/Submission16651/Reviewer_foVw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16651/Reviewer_foVw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875259323, "cdate": 1761875259323, "tmdate": 1762926711770, "mdate": 1762926711770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether large reasoning models (LRMs) can exhibit human-like social behavioral reasoning. The authors first construct a small Social-Behavioral-Reasoning (SBR) dataset consisting of demographic profiles, social questions, free-form human rationales, and final choices. They then evaluate a set of existing LRMs and observe that test-time reasoning tends to increase “collective diversity” while reducing “individual realism.” To address this, the paper proposes a “Reasoning-Enhanced SFT” approach that trains models on both human rationales and decisions. Experiments on the SBR benchmark and a moral-dilemma dataset, together with a small user study, suggest modest improvements in perceived human-likeness of reasoning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Clear motivation: The paper highlights an important challenge of simulating human social behavior and reasoning with LLMs, a topic of growing interest in both machine learning and computational social science.\n\nData collection effort: Constructing a dataset with paired profiles, open-ended rationales, and decisions reflects a meaningful annotation effort.\n\nProblem space relevance: The work touches on value alignment, human-agent interaction, and social simulation — areas with potential downstream societal impact."}, "weaknesses": {"value": "1. Severe Limitations in Problem Formulation and Scope\nWhile the paper aims to study \"social reasoning,\" its operationalization is overly simplistic. The task is confined to answering single-turn, context-free survey questions, which is more akin to preference elicitation than complex social reasoning. The chosen questions (e.g., on household chores or early retirement) lack the elements of strategic interaction, dynamic belief updating, or rich social context that are central to genuine social cognition. Consequently, the problem formulation is not sufficiently complex to support broad claims about eliciting \"human-like social reasoning\" in LLMs.\n\n2. Methodological Simplicity Compounded by Critical Data Scarcity\nThe core methodological proposal, \"Reasoning-Enhanced-SFT,\" is a standard application of supervised fine-tuning on rationale + answer pairs. While methodologically simple, its primary weakness lies in the dataset's extremely small scale. Fine-tuning on merely 1,560 training examples from 104 individuals is highly unlikely to impart generalizable social reasoning abilities. Instead, it is far more plausible that the model is overfitting to the specific linguistic styles and response patterns present in this small sample. The reported improvements, including the out-of-domain results, may stem from learning superficial stylistic cues rather than genuine cognitive processes. The authors do not provide sufficient evidence (e.g., rigorous analysis of failure cases or generalization across more diverse tasks) to disentangle true reasoning from stylistic imitation.\n\n3. Inability to Validate Key Claims Due to Data Scale and Accessibility\nThe paper's central claims—particularly the \"diversity-realism trade-off\" and the effectiveness of the proposed solution—rest entirely on a dataset that is too small to yield statistically meaningful conclusions. Social attitudes and reasoning are incredibly diverse; findings from 104 participants cannot be reliably generalized. Metrics like \"Individual Realism\" lose their meaning when the ground truth for a given demographic profile is based on a single data point. This small sample size makes the entire quantitative evaluation fragile and potentially misleading.\n\nFurthermore, the lack of access to the full human rationales during the review process prevents an independent assessment of their quality and complexity, which is crucial for a dataset-centric contribution. Without this, it is impossible to verify whether the human data provides a sufficiently strong and nuanced signal for training."}, "questions": {"value": "Your central claim is that Reasoning-Enhanced-SFT improves social reasoning. However, given the extremely small dataset (104 participants), a more parsimonious explanation is that the model is overfitting to the specific phrasing and stylistic patterns of this sample. How can you definitively distinguish genuine improvement in social cognition from mere stylistic imitation? For instance, did you perform any qualitative analysis on the generated reasoning to show it goes beyond surface-level mimicry? What steps were taken to validate that the out-of-domain performance is not also an artifact of stylistic generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pBZFjXQt33", "forum": "nAiDavdPlD", "replyto": "nAiDavdPlD", "signatures": ["ICLR.cc/2026/Conference/Submission16651/Reviewer_gLzv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16651/Reviewer_gLzv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979991037, "cdate": 1761979991037, "tmdate": 1762926711277, "mdate": 1762926711277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper asks whether large “reasoning” LLMs (LRMs) actually think like humans in social decision-making. It builds a new Social-Behavioral-Reasoning (SBR) dataset pairing demographic personas, socially contested questions, human written reasoning, and final choices. Two complementary metrics are introduced: Collective Diversity (distributional entropy over choices across personas) and Individual Realism (closeness to a specific persona’s choice). Across many LRM–baseline pairs, LRMs raise diversity but often degrade persona-level realism, and granting more test-time reasoning tokens amplifies this divergence—suggesting added “thinking” increases stochastic branching rather than faithful persona modeling. To address this, the authors propose Reasoning-Enhanced SFT that supervises both human rationales and outcomes. The resulting model (LHBRM) improves both diversity and realism, shows out-of-domain gains on Moral Machine, and is preferred by human raters for realism of reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 Problem framing + evaluation lens are spot-on.\nThe diversity–realism two-axis evaluation directly captures the core tension in using LLMs as human proxies (population heterogeneity vs. persona fidelity), moving beyond one-number “accuracy.”\n\n2 Actionable negative finding that challenges assumptions.\nThe paper shows that giving LRMs more test-time reasoning increases distributional diversity but hurts persona-level realism—an impactful, counter-intuitive result that reframes how “thinking tokens” should be interpreted in social reasoning.\n\n3 Simple, generalizable training recipe with measurable gains.\nReasoning-Enhanced SFT (supervising human rationales + outcomes) is architecture-agnostic and empirically improves both axes, making it easy for the community to adopt and build upon."}, "weaknesses": {"value": "1 Dataset scope and representativeness are too narrow.\nThe SBR corpus has ~hundreds of subjects and 15 scenarios, with limited demographic, cultural, and ideological coverage. Online-recruitment selection effects are not quantified, weakening external validity.\n\n2 Construct validity of the metrics is questionable.\nThe diversity metric (entropy) can reward stochastic noise rather than faithful recovery of group structure; the realism metric assumes an ordinal-to-interval mapping on [0,1] that may distort semantic distances. Sensitivity to binning, option count, and mapping choices is not reported.\n\n3 The mechanism behind “more reasoning tokens ↓ realism” is under-substantiated.\nThe paper attributes the effect to stochastic branching, but does not adequately control for temperature/top-p, sampling count, deterministic decoding, reflect-and-revise loops, or chain-length constraints; thus causal interpretation remains weak.\n\n4 Persona prompting is fragile and may leak stereotypes.\nPrompt template, attribute order/density, and conflicting attributes likely drive outputs. There is no systematic robustness analysis (prompt variants, attribute permutations) nor checks for prompt-induced stereotype leakage.\n\n5 Comparison fairness and supervision budget confounds.\nLRMs and baselines are not clearly matched for compute (context window, token budget, generation time). Reasoning-Enhanced-SFT supervises both process and outcome, potentially increasing total supervised tokens vs. “outcome-only” baselines, confounding attribution of gains.\n\n6 Process supervision may learn post-hoc rationalizations and amplify bias.\nHuman “reasons” can be justificatory rather than causal; the model may imitate stylistic rationales, not decision mechanisms. Safety auditing for harmful or discriminatory patterns in learned rationales is limited.\n\n7 Evaluation breadth and rigor are insufficient.\nOOD testing is limited (Moral Machine, realism only); human preference studies are small-N with sparse statistical reporting (CIs, multiple-comparison control, inter-rater reliability). Error analysis by persona/issue is thin, and reproducibility details (code, seeds, exact configs) are incomplete."}, "questions": {"value": "The same as the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rNcBBt7lBL", "forum": "nAiDavdPlD", "replyto": "nAiDavdPlD", "signatures": ["ICLR.cc/2026/Conference/Submission16651/Reviewer_wp3H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16651/Reviewer_wp3H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762115004104, "cdate": 1762115004104, "tmdate": 1762926710764, "mdate": 1762926710764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}