{"id": "NJUmKny4ZI", "number": 22043, "cdate": 1758325260792, "mdate": 1759896889306, "content": {"title": "It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents", "abstract": "Web-based agents powered by Large Language Models  are increasingly used for tasks such as email management or professional networking. However, their reliance on web content makes them vulnerable to hijacking attacks: adversarial instructions hidden in ordinary interface elements that divert the agent from its assigned task. To effectively measure the risks of such attacks, we introduce the Task-Redirecting Agent Persuasion Benchmark (TRAP). TRAP makes three contributions. First, it provides a flexible framework for generating adversarial injections, combining five modular dimensions. Second, it delivers a benchmark of 630 task suites on realistic website clones to measure agent susceptibility. Third, it introduces an objective one-click hijack evaluation method that avoids reliance on  LLM judges and reduces ambiguity from agent skill gaps. We evaluate six frontier models on TRAP and find that agents are hijacked in 25\\% of cases on average, with hijack success rates ranging from 13\\% on GPT-5 to 43\\% on DeepSeek-R1. We find that small design choices, such as using buttons instead of hyperlinks or lightly tailoring attacks to the environment, can multiply success rates. Moreover, effective hijacks often transfer across models, revealing systemic vulnerabilities. By releasing TRAP, we provide a reproducible, modular and extensible benchmark for systematically evaluating hijacking risks in web-based agents.", "tldr": "", "keywords": ["web agents", "browser agents", "agent safety", "agent hijacks", "agent benchmark", "prompt injections", "text injections"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/89f4c41e9672bb88a2f37639a4761a135e954bdd.pdf", "supplementary_material": "/attachment/56f0c4ace0d40e02648a3736bec2fbacf7df47af.zip"}, "replies": [{"content": {"summary": {"value": "In this work, the authors propose TRAP, a benchmark designed to evaluate vulnerabilities arising from Adversarial Injections. The benchmark defines five modular dimensions and includes 630 agent susceptibility tests using realistic website clones. The authors report the average hijacking success rate across six evaluated models and the transferability rate between model pairs, demonstrating the presence of systemic security vulnerabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses a more diverse set of vulnerability cases compared to prior work.\n\n- Provides the interesting observation that a vulnerability detected in one model can transfer to other models.\n\n- The LLM-manipulation category based on Cialdini’s principles is an interesting approach, rather than framing the problem purely as jailbreaking.\n\n- Reports transfer rates between models through an extensive evaluation."}, "weaknesses": {"value": "- Considers a hijack successful the moment a click occurs, but in the real world hijacking may unfold over multiple turns.\n\n- Although the set of vulnerability cases is large, it is still limited to elements like buttons, links, and user-editable areas."}, "questions": {"value": "- Is there a way to infer or estimate the risk/severity level for each vulnerability? (how large cost each vunlerability takes?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6Wo82KeVf3", "forum": "NJUmKny4ZI", "replyto": "NJUmKny4ZI", "signatures": ["ICLR.cc/2026/Conference/Submission22043/Reviewer_4gAY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22043/Reviewer_4gAY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955486509, "cdate": 1761955486509, "tmdate": 1762942031770, "mdate": 1762942031770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Disclosure: Claude is used to refine this review.\n\nThis paper introduces TRAP, a benchmark for evaluating hijacking vulnerabilities in LLM-based web agents. The benchmark constructs 630 task suites on 6 cloned websites by combining 5 modular components: injection interface, human persuasion principles, LLM manipulation methods, injection location, and tailoring. Using a one-click success criterion, the authors evaluate 6 models and find an average 25% hijack success rate, ranging from 13% (GPT-5) to 43% (DeepSeek-R1). Results show buttons are 3× more effective than hyperlinks, attacks transfer across models, and light tailoring substantially increases success."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Web agent hijacking is a real and growing threat. The focus on systematic evaluation is timely and valuable.\n- The 5-component framework (interface, persuasion, manipulation, location, tailoring) enables systematic ablation studies and is extensible to new attack types.\n- The paper systematically examines transferability, component effectiveness, interface types, location, and tailoring, which provides actionable insights."}, "weaknesses": {"value": "- Only buttons and hyperlinks are tested; no images, pop-ups, audio, forms, or other realistic attack vectors. Also, one-click criterion is too simplistic, as in practice agent scaffolding can detect recover from errors (e.g., [1] discussed this in detail). Further discussion and justification of the one-click criterion are needed.\n- No defenses/controls are evaluated. This limits practical applicability. It would be interesting to see if input filters / output monitors can mitigate the problem.\n\n[1] Wu et al. Dissecting Adversarial Robustness of Multimodal LM Agents. ICLR 2025. https://arxiv.org/abs/2406.12814"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L41ulF6Ixr", "forum": "NJUmKny4ZI", "replyto": "NJUmKny4ZI", "signatures": ["ICLR.cc/2026/Conference/Submission22043/Reviewer_22Ey"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22043/Reviewer_22Ey"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964102808, "cdate": 1761964102808, "tmdate": 1762942031550, "mdate": 1762942031550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TRAP, a modular benchmark for task-redirecting hijacks of web agents. TRAP composes injections from five dimensions (interface, persuasion principle, LLM manipulation method, location, tailoring), yielding 630 task–injection suites across six cloned websites built on REAL. Success is defined objectively as the agent clicking an injected hyperlink or button, avoiding LLM judges and skills-gap ambiguity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Frames persuasion-driven hijacks as modular components that can be recombined and extended.\n\n2. Clear threat setup; fixed observation modality (AXTree) to control confounders; broad, factorized analysis across persuasion and manipulation methods with transferability measurements.\n\n3. The five-component decomposition, location diagrams, and controlled comparisons (button vs hyperlink; targeted vs non-targeted prompts; tailored vs non-tailored) make the overall story legible."}, "weaknesses": {"value": "1. TRAP’s modularity is compelling, but the paper does not empirically contrast against prior agent-security benchmarks (e.g., AgentDojo, AgentHarm, InjecAgent, etc) on overlapping attack types to show what conclusions change when using the one-click criterion.\n\n2. Only text injections via buttons and hyperlinks are used in the core dataset; pop-ups, banners, multimedia, and richer UI elements are out of scope.\n\n3. Using only accessibility trees improves control, but many deployed agents rely on screenshots/DOM blends. The gap between AXTree-only susceptibility and multi-modal observation remains unquantified.\n\n4. No evaluation of simple, possible mitigations defense on the attack."}, "questions": {"value": "1. Could the authors further clarify the contribution over prior agent-security benchmarks such as AgentDojo, AgentHarm, and InjecAgent? I suspect hijack performance may be similar between systems that use simulated environments (as in this paper) and those that use simulated observations for email or shopping (as in AgentDojo or InjecAgent).\n\n2. I am concerned about the assumption that current LLM agents rely on the AXTree to take actions. Has this been verified empirically? My expectation is that many agents rely on screenshots or DOM representations instead.\n\n3. Your metric stops at a click. How does the HSR translate to downstream harm in realistic workflows (e.g., redirects, data exfiltration, unintended transactions)?\n\n4. Some hijacks transfer broadly while others do not. What features distinguish globally transferable injections from model-specific ones?\n\n5. As a benchmark, an initial average HSR of 25% seems low, which suggests the benchmark may not be sufficiently challenging. Consider strengthening tasks or injections to better discriminate model robustness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "or6SseAFYD", "forum": "NJUmKny4ZI", "replyto": "NJUmKny4ZI", "signatures": ["ICLR.cc/2026/Conference/Submission22043/Reviewer_LTYp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22043/Reviewer_LTYp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974449261, "cdate": 1761974449261, "tmdate": 1762942031289, "mdate": 1762942031289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TRAP (Task-Redirecting Agent Persuasion Benchmark), a benchmark for evaluating hijacking vulnerabilities in web-based LLM agents. The authors construct 630 task suites by combining 18 benign tasks with 35 injection templates built from five modular components: injection interface (button/hyperlink), human persuasion principles (7 Cialdini principles), LLM manipulation methods (5 types), injection location, and tailoring. The benchmark is built on REAL's cloned websites (Amazon, Gmail, Calendar, LinkedIn, DoorDash, Upwork). The key innovation is a one-click evaluation metric: hijacking success is determined when the agent clicks the injected element, avoiding ambiguity from multi-step outcomes and LLM judge bias. Evaluating six frontier models, they find an average 25% hijack success rate (ranging from 13% on GPT-5 to 43% on DeepSeek-R1), with buttons 3× more effective than hyperlinks and light tailoring increasing success by up to 5.6×."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- 630 task suites on realistic website clones to measure agent susceptibility.\n- Verifiable evaluation without reliance on LLM judges.\n- Interesting findings regarding the vulnerability of different models (hijack success rates ranging from 13% on GPT-5 to 43% on DeepSeek-R1).\n- Using realistic clones of popular websites from REAL (Garg et al., 2025). This is important as prompt injections are a major threat for agents, and prompt injection benchmarks in realistic environments are highly needed.\n- Accuracy on benign tasks is also measured and provides a baseline for agents’ capabilities.\n- Showing that the considered injection templates are transferable between different LLMs is important and is a nice side contribution.\n- Valuable ablation studies (e.g., hyperlinks vs. buttons)."}, "weaknesses": {"value": "- My major concern is that the benchmark has a very low number of unique tasks (only 18). A total of 630 tasks are created by using 35 injection templates (7 persuasion principles × 5 LLM manipulation methods). The benchmark would be more useful with a larger number of unique tasks (say, at least 50 or better 100). Varying injection templates is less interesting, since they shouldn’t be assumed fixed (see the discussion on adaptive attacks in [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151) and [The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections](https://arxiv.org/abs/2510.09023)).\n- *“We introduce a single, unambiguous success criterion: whether the agent clicks the injected element.”* - This makes the tasks easy to grade, but it’s also a weakness of the benchmark. Realistic hijacks typically require the agent to perform multiple steps.\n\nMinor points:\n- “Prompt injections” seems to be a much more established name compared to “hijacks”. I wonder why the authors seem to strongly prefer “hijacks”. When reading the paper, it was not clear to me if there is some substantial difference between them, but it seems like they refer to the same behavior.\n- *“This creates ambiguity: if an agent starts to follow a malicious instruction but fails to complete it, is that a skill gap or a true refusal?”* - Note that these two things are easy to disentangle since one can measure refusals directly (e.g., as done in AgentHarm), which is a straightforward task for an LLM judge."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Eu2LuuKM7p", "forum": "NJUmKny4ZI", "replyto": "NJUmKny4ZI", "signatures": ["ICLR.cc/2026/Conference/Submission22043/Reviewer_Dpz7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22043/Reviewer_Dpz7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762086481582, "cdate": 1762086481582, "tmdate": 1762942030968, "mdate": 1762942030968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}