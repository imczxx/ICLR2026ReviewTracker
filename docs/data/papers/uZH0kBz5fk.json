{"id": "uZH0kBz5fk", "number": 562, "cdate": 1756746841375, "mdate": 1759898253780, "content": {"title": "AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting", "abstract": "We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction. \nUnlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation to the problem of animating humans in scenes. By representing humans and scenes as Gaussians, our approach allows for geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. \nOur key insight is that the rendering can be decoupled from the motion synthesis and each sub-problem can be addressed independently, without the need for paired human-scene data. \nCentral to our method is a Gaussian-aligned motion module that synthesizes motion without explicit scene geometry, using opacity-based cues and projected Gaussian structures to guide human placement and pose alignment. To ensure natural interactions, we further propose a human–scene Gaussian refinement optimization that enforces realistic contact and navigation. We evaluate our approach on scenes from Scannet++ and the SuperSplat library, and on avatars reconstructed from sparse and dense multi-view human capture. Finally, we demonstrate that our framework allows for novel applications such as geometry-consistent free-viewpoint rendering of edited monocular RGB videos with new animated humans, showcasing the unique advantage of 3DGS for monocular video-based human animation. To fully gauge the quality of our results, we urge the reader to watch the supp video.", "tldr": "", "keywords": ["gaussian splatting", "neural rendering", "human animation", "human-scene interaction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8607d568207dcbc263396d835d9dfb91878f35a3.pdf", "supplementary_material": "/attachment/f08e932418e68b9dc6ec1338dc6e081845a33072.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents the successful integration of a reconstructed, drivable 3D Gaussian human avatar into a 3D Gaussian Splatting (3DGS) scene, enabling fundamental actions like walking and sitting.\nThe core contribution is the effective migration and adaptation of traditional scene-mesh human interaction techniques (including RL-based locomotion and motion transitions) to the 3DGS. This required specific technical adjustments, notably: Gaussian point simplification (via PCA) and floating point filtering for rendering a cleaner Gaussian obstacle map needed for path planning. Furthermore, the work includes contact optimization to address foot-ground interaction issues.\nOverall, this work is a meaningful contribution to the community, expanding the potential application scenarios for 3DGS avatars."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Achieved a novel and effective integration of a human avatar into a realistic 3DGS environment, successfully demonstrating basic locomotion (walking) and sitting actions.\n2.The paper tackles key challenges inherent in porting RL locomotion from structured mesh scenes to  3DGS. The use of PCA for simplification and floating point filtering effectively reduces noise in the obstacle perception, which is crucial for path planning.\n3.The inclusion of contact optimization is valuable, as it helps mitigate common artifacts like foot sliding, floating, or unnatural foot interaction with the ground plane, thus enhancing overall realism."}, "weaknesses": {"value": "1.The experimental section lacks comprehensive visual validation. The paper only showcases the results of the proposed method without providing visual comparisons corresponding to Baseline A and Baseline B.\n2.The work appears to overlook a crucial aspect of realism when integrating an avatar: lighting and shadows. An important requirement for placing a human into a varying scene environment is that the avatar's appearance (including self-shadows and scene-induced illumination changes) should react naturally to the ambient light, which is currently a strength of mesh-based approaches. This omission significantly limits the potential for higher evaluation.\n3.Minor foot clipping briefly observable in the accompanying video around the 1:20 mark, suggesting the contact optimization could still benefit from further refinement.\n4.The method's pipeline diagram does not clearly highlight the core technical contribution or the novel, adapted aspects of the proposed approach."}, "questions": {"value": "1.Could the authors clarify why the video only shows the comparison between Baseline A vs. Ours, and not include the visual comparison for Baseline B vs. Ours?\n2.It would be highly beneficial to include a visual comparison showing the achieved result side-by-side with a simple real-world reference video (e.g., the same person performing the same walking or sitting actions in the same scene) to better gauge the fidelity and realism.\n3.Please elaborate on the specific method or process used to accurately handle the scale and proportional alignment of the avatar when placing it into the reconstructed 3DGS scene.\n4.After the initial scene and avatar reconstruction, what is the approximate training or optimization time required for the proposed method to successfully enable path-planned walking and sitting within the scene?\n5.Beyond simple walking, can the proposed method support more dynamic and large-scale motions, such as running, jumping, or dancing?\n6.The paper mentions the \"picking up\" action. Based on the description and lack of scene object segmentation, achieving this action seems highly challenging, if not currently infeasible. We suggest removing or clarifying this claim to prevent potential misunderstanding regarding the system's capabilities"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YmtNY7YGb9", "forum": "uZH0kBz5fk", "replyto": "uZH0kBz5fk", "signatures": ["ICLR.cc/2026/Conference/Submission562/Reviewer_2xxi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission562/Reviewer_2xxi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761038995916, "cdate": 1761038995916, "tmdate": 1762915547699, "mdate": 1762915547699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the task of generating human motion within 3D environments while enabling photorealistic rendering of the resulting animation. To this end, the work proposes to utilize 3D Gaussian Splatting (3DGS) in contrast with meshes that are traditional for the motion generation task. The method decouples animation generation from 3DGS rendering and introduces a \"refinement stage\" to correct the Gaussians' placement in accordance with the animation. Experiments show visual improvements compared to two baselines: Baseline A - no \"refinement stage\"; Baseline B - mesh-based rendering."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* To the best of my knowledge, the work for the first time proposed to simultaneously solve the task of motion generation in the environment and photorealistic rendering. \n* The work raises an interesting question: is it possible to generate human animation in the scene when both are presented with 3D Gaussians in contrast with meshes in previous works.\n* The approach demonstrates convincing qualitative results with sufficient visual realism."}, "weaknesses": {"value": "* Limited novelty. While the results are good, the presented method is mostly an engineering pipeline based on the existing methods with \"refinement stage\" as the only novel part. The refinement stage is presented as a set of heuristics to polish the Gaussians' positions: the frames and Gaussians are selected based on predefined rules and thresholds, as well as the Gaussians' offset directions.\n* Continued reliance on meshes. While the work investigates the applicability of 3D Gaussians for motion in the scene generation, it still relies heavily on meshes. The animation of a person is generated utilizing SMPL parametric meshes in the \"approximated navigation meshes\" (line 296) as the environment. Thus, the work doesn't demonstrate the principal benefits of using Gaussians compared to meshes.\n* Unclear method description. From the text, it is hard to tell what parts of the method are novel, because the work describes the utilized methods in too much detail. Describing only the necessary information on the existing methods in the \"Preliminaries\" subsection would help better separate the method from prior works.\n* The experiments are not clearly presented. Table 1 is difficult to interpret, and the baselines are complex pipelines rather than previous works."}, "questions": {"value": "* I would like to have more clarification on the choice of predicting offsets for each Gaussian instead of learning SMPL pose correction. Intuitively, it seems that adjusting the SMPL pose would result in fewer artifacts than the independent transformation of Gaussians."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mzgoZzntUh", "forum": "uZH0kBz5fk", "replyto": "uZH0kBz5fk", "signatures": ["ICLR.cc/2026/Conference/Submission562/Reviewer_CKuQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission562/Reviewer_CKuQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761290616492, "cdate": 1761290616492, "tmdate": 1762915547594, "mdate": 1762915547594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles a new task that animates and renders a digital avatar in a 3D scene. While most of the previous works focus on one of animating digital avatars with existing poses, pose generation and 3D scene generation, this work combines them all as a unified pipeline.\n\nThe method starts with the reconstructions of digital avatars and 3D scenes seperately. Then it synthesizes the motions to plug the digital avatar into the scene. The motion synthesis incorporates a coarse module (an RL locomotion policy that predicts the waypoints) and a fine module (a deterministic latent-space optimizer that performs fine-grained motions when approaching the target). It follows with an optimization-based contact refinement to avoid penetration and enforce proper contact between the digital avatar and the scene.\n\nIt showcases the rendering results, particularly in the motions of walking and sitting. The motions are physically plausible with little penetration with the scenes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The task of animating digital avatars in 3D scenes is important in various AR/VR applications, e.g., game engines.\n* Modeling 3D scenes and digital avatars seperately then combining them together in the test time is a reasonable choice which could be the use case in games where the digital avatars/3D scenes are replacable.\n* The writing and the entire pipeline are easy to follow."}, "weaknesses": {"value": "* The interactions demonstrated in the paper are limited to two simple motions—walking in open space and sitting on a chair. Other common daily activities, such as running or walking at different speeds, reaching for objects, or lying on a bed, are not explored.\n* The motions shown in the supplementary videos, while physically plausible, still appear somewhat unnatural. For instance, the arms do not swing naturally or fluidly during walking sequences.\n* Since the motion synthesis module is optimization-based, computational efficiency is a concern for real-world applications. For example, when controlling a digital avatar in a game, the system should be capable of planning and rendering motions in real time. It would be helpful if the paper reported the execution time of each module in the pipeline.\n* The paper is largely a systematic study and lacks clear technical contributions. The individual components—such as 3D reconstruction of scenes and avatars—are well studied in prior work. Similarly, the RL-based locomotion policy and latent-space optimizer are established techniques in motion planning. The main technical novelty appears to lie in the contact refinement module, which extends ideas previously explored in SMPL-scene interactions to 3D Gaussian-scene interactions."}, "questions": {"value": "* There is still cloudy artifact in the contact regions of feet and the ground. See 1:27-1:28 of the supplementary video, the contact of the feet and the mattress. In 1:36 of the supplementary video, when approaching the seats, the avatar performs a unnatural “leaning forward\" motion. What is the cause of such artifacts in rendering and motion synthesis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "18V04jGb7t", "forum": "uZH0kBz5fk", "replyto": "uZH0kBz5fk", "signatures": ["ICLR.cc/2026/Conference/Submission562/Reviewer_b3rV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission562/Reviewer_b3rV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761502353091, "cdate": 1761502353091, "tmdate": 1762915547295, "mdate": 1762915547295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method that enables virtual humans, represented using Gaussian splats, to navigate and interact in complex environments reconstructed as 3D Gaussian scenes. To achieve this, novel Gaussian-aligned motion synthesis and contact refinements are proposed."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "As far as I know, this is the first work that models human-environment interactions in 3D Gaussian space. Although the interaction is quite basic (it cannot model intricate object manipulation), it is still impressive that the model can handle occlusion and collision. The video result quality is good and can inspire future works."}, "weaknesses": {"value": "The critical weakness, in my opinion, is that there is no visual comparison (especially video comparison) with baselines. The authors attempted to design naive baselines (which I appreciate, since there were no previous 3D Gaussian works that enabled human-environment interaction), and the design is reasonable. However, they did not show any visual comparisons with them (i.e., baseline A and baseline B presented in the experiment section), and I think this is a significant weakness. Although readers might expect that the baselines would fail, to fairly validate the necessity of this work, the paper should have presented proper visual comparisons. I felt that the paper is a bit unpolished for submission. I would like to see other reviewers’ opinions on this and vote again. For now, I am leaning toward rejection and would encourage the authors to resubmit after including proper baseline visual comparisons and polishing the submission. Again, the results and the idea itself are very good."}, "questions": {"value": "**Suggestions**\n\nAuthors don’t have to show this in the rebuttal, but for the revision, visual examples of handling collisions can be highlighted. My first impression was that this work simply places the human in the 3D scene. \n\nAlso, the overview figure does not emphasize the paper’s main contributions, such as contact refinement, and therefore is not very helpful for understanding the paper. I suggest adding the two main contributions in the overview figure (Gaussian-aligned motion synthesis and contact refinement)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZqhSsSiEDm", "forum": "uZH0kBz5fk", "replyto": "uZH0kBz5fk", "signatures": ["ICLR.cc/2026/Conference/Submission562/Reviewer_gJYF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission562/Reviewer_gJYF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971666030, "cdate": 1761971666030, "tmdate": 1762915547069, "mdate": 1762915547069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}