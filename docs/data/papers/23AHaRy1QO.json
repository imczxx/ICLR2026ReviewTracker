{"id": "23AHaRy1QO", "number": 24949, "cdate": 1758362310159, "mdate": 1763560424037, "content": {"title": "Efficient Fine-tuning with Decomposed Foundation Model", "abstract": "Fine-tuning billion-scale large language models (LLMs) is challenging due to the extremely large model size, particularly in memory-constrained scenarios, even with parameter-efficient fine-tuning (PEFT) and quantization. To address this challenge, we propose a novel method based on the decomposition then fine-tuning (DeFT) paradigm, which effectively decomposes the foundation model and reduces the number of model parameters during fine-tuning, while retaining model quality. DeFT introduces a highly efficient layer importance aware search algorithm for fine-grained model decomposition and successfully repurposes model decomposition for fine-tuning. Additionally, DeFT can seamlessly integrate with PEFT and quantization methods to enhance fine-tuning efficiency further. Extensive experiments on various LLM backbones demonstrate that DeFT achieves comparable or even better performance than the baseline PEFT and quantization methods, while improving both memory efficiency and computation efficiency for fine-tuning. Remarkably, DeFT enables fine-tuning of a 65B model on a consumer GPU with just 24GB of memory, all without relying on offloading strategies, saving significant expenses for purchasing or renting high-end GPUs.", "tldr": "", "keywords": ["Large Language Model Fine-tuning", "Foundation Model Decomposition"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/85e91524fc03334440be8b4e2c50c452c52673da.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The experimental results are a major strength. DeFT not only achieves comparable or even slightly better performance than standard QLoRA/LoRA baselines, but it does so with a significantly smaller model. The headline claim, backed by experiments (Table 4), is that DeFT enables the fine-tuning of a 65B parameter model on a single 24GB consumer GPU (e.g., RTX 4090) without any offloading strategies, reducing memory usage by ~36% and total training time by ~24-33%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The primary strength of this paper is its immense practical value. The ability to fine-tune a 65B (or 70B) model on a single 24GB consumer GPU is a game-changer. It breaks down a major hardware barrier and democratizes access to large-scale fine-tuning for a much wider range of researchers and developers.\n\nDeFT is not just \"SVD + LoRA.\" The core novelty lies in the fine-grained decomposition search algorithm. This algorithm (Eq. 6) is principled, intelligently optimizing a clear trade-off (performance loss vs. memory gain) and weighting it by layer sensitivity. The ablation study (Table 3) compellingly proves that this search is the most critical component of DeFT, as a naive uniform compression (w/o Search) fails dramatically."}, "weaknesses": {"value": "The paper's most surprising finding is that the smaller, compressed +DeFT models often outperform the larger, baseline QLoRA models (e.g., LLaMA-65B, 80.84 vs. 78.71 avg). While the ablation shows what causes this (the search algorithm), it doesn't fully explain why. Is this a regularization effect? Does the SVD process \"clean\" noisy, over-parameterized weights? A deeper discussion of this \"compression-for-a-gain\" phenomenon would strengthen the paper.\n\nThe method relies on a small set of calibration data (256 samples from the downstream task) to calculate activation statistics for both the SVD whitening and the layer importance ($\\alpha_l$). Appendix C.3 shows performance is sensitive to the size of this data. More analysis on its sensitivity to the content (e.g., different random 256-sample sets) would be valuable."}, "questions": {"value": "the finding that +DeFT models (which are smaller) can outperform their baseline QLoRA counterparts is fascinating. What is the authors' primary hypothesis for this? Is it a regularization effect from the low-rank constraint, or does the activation-aware SVD process somehow \"clean up\" the pre-trained weights in a way that is beneficial for fine-tuning?\n\nWhat is the one-time pre-processing cost (SVD decomposition + search) for the largest models, LLaMA-65B and LLaMA-3 70B? The 10-minute cost for 7B is excellent, but how does this cost scale with model size?\n\nThe use of the SVD-tail to initialize the LoRA adapter is a very clever idea, and Table 3 shows it provides a small but real benefit (62.90 vs. 62.52). Does this imply that the truncated singular vectors (the \"discarded\" information) are somehow intrinsically well-suited for adaptation, which is exactly what LoRA does? This seems like a powerful insight if true."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V3Bq585SfE", "forum": "23AHaRy1QO", "replyto": "23AHaRy1QO", "signatures": ["ICLR.cc/2026/Conference/Submission24949/Reviewer_ZfCd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24949/Reviewer_ZfCd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604115281, "cdate": 1761604115281, "tmdate": 1762943258257, "mdate": 1762943258257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "What really jumps out from the experiments is how well DeFT performs. It holds its own—or even does a bit better—compared to standard QLoRA and LoRA approaches, but manages to do so with a smaller model. The coolest part, according to Table 4, is that you can actually fine-tune a massive 65 billion parameter model on a regular 24GB graphics card (like an RTX 4090) without any fancy workarounds like offloading. This means you use about a third less memory and can train the model 24–33% faster."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "What really makes this paper stand out is how useful it is in practice. Being able to fine-tune a massive 65B (or 70B) model on just a regular 24GB consumer GPU is a big deal. It tears down a huge hardware barrier and opens the door for many more researchers and developers to work with large models, not just those with expensive gear.\n\nDeFT isn’t just another “SVD plus LoRA” approach. The real innovation is in its detailed decomposition search algorithm. This part of the method (see Equation 6) is smart about balancing how much performance you might lose against how much memory you can save, and it takes into account how sensitive each layer of the model is. Table 3 in the ablation study makes it clear that this search step is crucial—if you try to compress everything the same way without it, the results are much worse."}, "weaknesses": {"value": "The paper's most surprising finding is that the smaller, compressed +DeFT models often outperform the larger, baseline QLoRA models (e.g., LLaMA-65B, 80.84 vs. 78.71 avg). While the ablation shows what causes this (the search algorithm), it doesn't fully explain why. Is this a regularization effect? Does the SVD process \"clean\" noisy, over-parameterized weights? A deeper discussion of this \"compression-for-a-gain\" phenomenon would strengthen the paper.\n\nThe method relies on a small set of calibration data (256 samples from the downstream task) to calculate activation statistics for both the SVD whitening and the layer importance ($\\alpha_l$). Appendix C.3 shows performance is sensitive to the size of this data. More analysis on its sensitivity to the content (e.g., different random 256-sample sets) would be valuable."}, "questions": {"value": "It's really interesting that the smaller +DeFT models sometimes do better than the larger QLoRA baselines. Do the authors have a sense of why this happens? Could it be that limiting the model with a low-rank constraint acts like a regularizer, or maybe the activation-aware SVD step helps tidy up the pre-trained weights so they're actually more useful for fine-tuning?\n\nHow much time does it actually take to run the one-time setup—like SVD decomposition and the search algorithm—on really big models, like LLaMA-65B or LLaMA-3 70B? I saw it only takes about 10 minutes for the 7B model, which is great, but does this time go up a lot as the models get bigger?\n\nStarting the LoRA adapter with the SVD-tail is a really smart move, and Table 3 shows it actually helps a bit (62.90 vs. 62.52). Does this mean that the part of the model you usually throw away—the truncated singular vectors—are especially good for adaptation, which is what LoRA is all about? If so, that feels like a pretty important insight."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V3Bq585SfE", "forum": "23AHaRy1QO", "replyto": "23AHaRy1QO", "signatures": ["ICLR.cc/2026/Conference/Submission24949/Reviewer_ZfCd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24949/Reviewer_ZfCd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604115281, "cdate": 1761604115281, "tmdate": 1763354955727, "mdate": 1763354955727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DeFT (Decomposition-then-Fine-Tuning), a framework that leverages model decomposition to improve the efficiency of fine-tuning large language models (LLMs). The core idea is to decompose the foundation model before fine-tuning, thereby reducing parameter count and memory usage while maintaining strong performance. Specifically, DeFT applies activation-aware singular value decomposition (SVD) with whitening to capture input-distribution information and then performs a layer-importance-aware search to determine optimal truncation points based on outlier-weighted sensitivity scores. The truncated singular values initialize the LoRA modules, while the remaining components reconstruct the frozen base model. During training, only the lightweight LoRA parameters are updated."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed approach is novel compared with existing low-rank adaptation methods.\n- DeFT demonstrates strong empirical performance across multiple LLMs (e.g., LLaMA, Qwen) and task domains.\n- Results are evaluated with multiple metrics such as memory usage and accuracy, providing a comprehensive empirical assessment."}, "weaknesses": {"value": "- The method relies heavily on matrix decomposition and search, which may reduce conceptual clarity.\n- The paper’s overall presentation and writing quality could be significantly improved; it currently reads as if prepared under time pressure and would benefit from further polishing.\n- Some technical details are insufficiently explained, making it difficult for others to reproduce the results accurately."}, "questions": {"value": "- Definition of WS (L98/L135, Figure 2):\nWhat exactly does “WS” denote? How does the method determine whether a pre-trained weight qualifies as WS and should be decomposed using SVD?\n\n- Computational Overhead:\nThe activation-aware compression loss involves truncating singular values per weight matrix and position. Does this introduce additional computational overhead, especially when evaluated multiple times for each layer?\n\n- Layer Position (Figure 3b):\nWhat does “Layer position” specifically represent in the figure? Is it the depth within the transformer stack or an abstract ordering metric?\n\n- Cache Mechanism (L261):\nCould the authors elaborate on the “cache mechanism” mentioned in L261? How does it function to reduce the computational cost or memory overhead during decomposition or fine-tuning?\n\n- Notation of A, B in Figure 2:\nWhat do A* and B* represent? Are they LoRA adapters, or do they differ from the A and B in the same figure? Clarifying their relationship would help readers follow the architecture more easily. Similar to this, the paper should identify all notations and make them easier to be followed by readers.\n\n- Source of Memory Savings:\nSince DeFT still initializes LoRA modules using truncated singular values, it is unclear where the training-time memory reduction primarily comes from. The paper should explicitly describe whether the savings stem from reduced activation storage, frozen base layers, or smaller gradient tensors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZVJf8zKez4", "forum": "23AHaRy1QO", "replyto": "23AHaRy1QO", "signatures": ["ICLR.cc/2026/Conference/Submission24949/Reviewer_mCwb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24949/Reviewer_mCwb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155025656, "cdate": 1762155025656, "tmdate": 1762943258046, "mdate": 1762943258046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce decomposition then fine-tuning (DeFT) paradigm, which decomposes the foundation model and then reduces the number of model parameters during fine-tuning, thus enhancing both memory-efficiency and computation-efficiency for\nfine-tuning. In addition, DeFT can be seamlessly combined with PEFT and quantization methods. As a result, DeFT allows for fine-tuning of a 65B model on a commercial off-the-shelf GPU with 24GB of memory."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors empirically show that DeFT can be integrated with PEFT and quantization methods (e.g., LoRA, QLoRA).\n- The authors demonstrate that DeFT can save GPU memory usage by up to 40%, thereby allowing for fine-tuning of a 65B model on a commercial off-the-shelf GPU with 24GB memory.\n- The authors conducted extensive experiments for various LLMs including recent LLMs (e.g., Qwen3)."}, "weaknesses": {"value": "- The baselines (LoRA and QLoRA) are too limited to verify the effectiveness of DeFT. There are so many variants of LoRA and QLoRA (e.g., DoRA, LoftQ), but it is hard to determine whether DeFT is also effective for those variants or not at this moment.\n- Given that the tasks ranging from AddSub to GSM8k are relatively easy, it is difficult to justify the efficacy of DeFT. It would be more beneficial if the authors use more challenging tasks such as MATH."}, "questions": {"value": "N.A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q8Q7ZUlEDI", "forum": "23AHaRy1QO", "replyto": "23AHaRy1QO", "signatures": ["ICLR.cc/2026/Conference/Submission24949/Reviewer_LY5k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24949/Reviewer_LY5k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762597340734, "cdate": 1762597340734, "tmdate": 1762943257860, "mdate": 1762943257860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DeFT for memory efficiency of finetuning. The core component is (1) SVD based weight decomposition on $W S$ where $S = X X^\\top$ is the empirical covariance matrix built from the calibration data (2) a constrained importance score optimization problem that aims to adaptively select different SVD selection threshold per layer as Eqn 5. It aims to optimize the performance while satisfying the memory efficiency and highest output reconstruction error at the same time. The experiments are conducted on Llama series 1 2 3, QWen 2 and 3, with main baselines as full FT, zeroshot, and QLoRA. The main benchmarks are arithmetic reasoning tasks, text summarization, and wiki perplexity. DeFT matches the performance of QLoRA with 20% memory efficiency savings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper flow is clear and easy to follow even upon the first time of read. Figure 2 provides a clear high-level illustration of DeFT.\n\n- The experiment setup follows the standard approach and well illustrated."}, "weaknesses": {"value": "- **The novelty is limited.** The SVD decomposition is mainly based on Wang et al. [1] and the main technical novelty besides Wang et al. is on the layerwise important weight search process. The iterative search process is a simple alternating optimization and cannot be considered as main novelty. From my perspective, DeFT is a simple if not incremental extension of SVD-LLM at best. \n\n- **The baselines are insufficient**. **DeFT as a weight decomposition method should be compared against alternative pruning/quantization methods also applied to the model weights. There is no such comparison on the main paper besides QLoRA**. I could perceive an alternative better int4 quantization / pruning method also equipped with LoRA finetuning matches DeFT performance and memory efficiency. \n\n- **The memory efficiency besides QLoRA is marginal compared to the extra overhead.** If I understand correctly, DeFT will not save *any* activation memory during finetuning as the decomposed weights are also kept frozen, and the DeFT only saves 20% parameters. From Table 1 and 4, DeFT only save <20% memory besides QLoRA, which I believe should mainly come from loading 20% less master weights per layer. **This is not a major saving if we also consider the overhead of SVD reconstruction.** Specifically, I could perceive offloading 20% more weights to CPU and load twice with 2 reduction can have the same memory efficiency and *much* higher throughput during inference than SVD reconstruction. If the whole memory savings from SVD is less than 30-50%, there will be plentiful alternatives that achieve better tradeoff between memory and compute efficiency during inference. \n\nThe second and third weakness are critical (in my opinion this is even more critical than technical novelty as we cannot position DeFT among related works without clear comparison) and I would vote for reject. \n\nReference:\n[1] Xin Wang, Yu Zheng, Zhongwei Wan, and Mi Zhang. SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression, April 2024."}, "questions": {"value": "Although the high-level illustration of DeFT is generally clear, the iterative search in sec 2.5 is not clear/detailed enough to show the entire search process. An algorithmic pseudocode illustrating such search formally is needed. \n\nOther questions are listed in weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Not needed"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n8gBdYQgyS", "forum": "23AHaRy1QO", "replyto": "23AHaRy1QO", "signatures": ["ICLR.cc/2026/Conference/Submission24949/Reviewer_QxzS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24949/Reviewer_QxzS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762630304719, "cdate": 1762630304719, "tmdate": 1762943257610, "mdate": 1762943257610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to all reviewers"}, "comment": {"value": "Dear Area Chair, SPC, and Reviewers,\n\nWe are grateful for the reviewers' thoughtful comments and the time dedicated to evaluating our submission. Their insights have greatly strengthened our work. We have now revised the manuscript accordingly and submitted the updated PDF, along with this point-by-point summary of revisions. Key modifications within the manuscript are highlighted in blue for your convenience.\n\n- Experiments: We have added additional experiments including: (1) combining DeFT with another quantization method LoftQ; (2) evaluations on more challenging task MATH; (3) impact of different calibration data subsets on the final performance. \n\n- Presentation & Discussion: We have revised the notations and technical details, and added an algorithmic pseudocode for the truncation position search algorithm to improve the clarity of our method. Besides, we have followed the reviewer’s suggestion to add more deeper discussion on the \"compression-for-a-gain\" phenomenon of our experimental results.\n\n- Methodology: We have clarified the novelty, baseline comparison and fine-tuning efficiency of our method.\n\nSincerely,\n\nAuthors of submission #24949"}}, "id": "4SJV0CMIcl", "forum": "23AHaRy1QO", "replyto": "23AHaRy1QO", "signatures": ["ICLR.cc/2026/Conference/Submission24949/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24949/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission24949/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763561617269, "cdate": 1763561617269, "tmdate": 1763561617269, "mdate": 1763561617269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Action Needed: Review Rebuttal and Update Evaluation"}, "comment": {"value": "Dear Reviewers,\n\nThank you, as always, for your valuable contributions and efforts. The authors have now submitted their rebuttal. Please take a moment to review it and provide any necessary follow-up actions, such as additional questions, clarification requests, or updates to your review.\n\nSince the initial ratings ranged from 2 to 6, I kindly ask you to pay close attention to the perspectives of the other reviewers when preparing your final response.\n\nThank you again for your support."}}, "id": "zXKG9NwxBQ", "forum": "23AHaRy1QO", "replyto": "23AHaRy1QO", "signatures": ["ICLR.cc/2026/Conference/Submission24949/Area_Chair_VeRe"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24949/Area_Chair_VeRe"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission24949/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763604996010, "cdate": 1763604996010, "tmdate": 1763604996010, "mdate": 1763604996010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}