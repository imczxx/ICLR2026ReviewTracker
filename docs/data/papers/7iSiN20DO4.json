{"id": "7iSiN20DO4", "number": 15992, "cdate": 1758258245974, "mdate": 1759897268455, "content": {"title": "SeWA: Selective Weight Average via Probabilistic Masking", "abstract": "Weight averaging has become a standard technique for enhancing model performance. However, methods such as Stochastic Weight Averaging (SWA) and Latest Weight Averaging (LAWA) rely on manually designed checkpoint selection rules, which struggle under unstable training dynamics. To minimize human bias, this paper proposes Selective Weight Averaging (SeWA), which adaptively selects checkpoints during the final stages of training for averaging. Both theoretically and empirically, we show that SeWA achieves a better generalization. From an algorithm implementation perspective, SeWA can be formulated as a discrete subset selection problem, which is inherently challenging to solve. To address this, we transform it into a continuous probabilistic optimization framework and employ the Gumbel-Softmax estimator to learn the non-differentiable mask for each checkpoint. Theoretically, we first prove that SeWA converges to a critical point with flatter curvature, thereby explaining its underlying mechanism. We further derive stability-based generalization bounds for SeWA, which are sharper than those of SGD under both convex and non-convex assumptions, thus providing formal guarantees of improved generalization. Finally, extensive empirical evaluations across diverse domains, including behavior cloning, image classification, and text classification, demonstrate the robustness and effectiveness of our approach.", "tldr": "", "keywords": ["generalization; optimization; stability; mask learning"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4125bb2f35fb97efffe7c09c0c9478a0291a1e09.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Weight averaging is known to stabilize the training and gives smoother weights. This has reportedly improved model generalization. \nThis paper proposes selective weight averaging to minimize the bias introduced by existing state-of-the-art methods that rely on manually designed checkpoint selection rules for stochastic weight averaging and latent weight averaging. \nThe idea is to adaptive selects checkpoints during the final stage of training for averaging."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n- Targetting a difficult problem\n- Avoids bias introduced due to manually selecting checkpoints\n- Reduces the need for extensive hyper parameter tuning\n- Mitigates performance degradation caused by redundant weight selection\n- Formulated adaptive selection of checkpoints as a discrete subset selection problem by transforming the problem into a continuous probabilistic optimization framework. \n- Derive stability-based generalization bounds which are sharper than SGD in both convex and non-convex optimization"}, "weaknesses": {"value": "Oveall, I find the idea quite interesting. I have few comments:\n\n- I see the idea behind Figure 1, but it did not help me  understand the fact that SeWA reaches a flatter curvature (stable minima)\n- Improvements reported in Figure 3 & 4 are showing marginal improvements even compared to Random\n\nThe experimentation overall seems not super convincing. But maybe theoretical guarantees claimed in the paper might be more promising. Unfortunately, this work is not in my expertise which is why I could not closely check the mathematical details. \n\nI would need to rely on the fellow reviewers to verify the math. Experimentally, I am not fully convinced of the benefits of the proposed approach."}, "questions": {"value": "- The difference between SeWA vs SWA is not clear in the introduction. If my understanding is correct, SWA is using previous K  epochs as well. How is SeWA different in that aspect?\n- Is the analysis still valid if a model is fine-tuned after being trained with SeWA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "4YkCvd56zS", "forum": "7iSiN20DO4", "replyto": "7iSiN20DO4", "signatures": ["ICLR.cc/2026/Conference/Submission15992/Reviewer_XUU5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15992/Reviewer_XUU5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740076798, "cdate": 1761740076798, "tmdate": 1762926201414, "mdate": 1762926201414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the SeWA algorithm, which is a post-training checkpoint averaging method that learns which $K>0$ of the last $k$ checkpoints to include in the final average. It relaxes the discrete selection with Gumbel-Softmax and optimizes it using Monte Carlo (iteratively). The paper provides theoretical results for evaluating generalization error in both convex and non-convex settings. It also provides empirical results over diverse domains, including behavior cloning (D4RL), CIFAR-100, and AG News."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper provides tighter theoretical results for convex and non-convex settings (the proofs were not checked).\n* The paper uses standard assumptions of smoothness and Lipschitz. \n* The experiments were tested on diverse domains: image, text, and locomotion trajectories."}, "weaknesses": {"value": "* In practice, if you select the final model on validation at multiple checkpoints, you would need to run SeWA at each validation point (or at least repeatedly near the end), which can be expensive compared to the other approaches that do not require this procedure and can be used directly. The paper would benefit from reporting wall-clock time per SeWA run, relative to one training epoch, for the chosen $K$, $k$, $M$, and max_iterations in each experiment. Also, the paper should specify the number of SeWA optimization iterations (max_iteration) and the value of $M$ used per experiment. Currently, only a D4RL ablation in the appendix touches on this, but it's still hard to capture the full cost of the proposed approach.\n* The paper also should discuss the storage overhead of additional $k$ checkpoints versus the other approaches, which keep a single running aggregate\n* In Figures 3 and 4, the results for SeWA and LAWA appear identical across different values of $K$. Also, SGD takes longer to converge on CIFAR-100; the authors may consider more complex architectures better suited to this task. For example, in [1] (TWA), they used ResNet and VGG and achieved more stable convergence and better accuracy on TWA than SGD (around 80%).\n* The results in Figures 3 and 4 do not show a significant improvement. Given the costly SWeA optimization process, averaging $k$ checkpoints at random yields results similar to those from the complete optimization and requires neither extensive hyperparameter tuning nor a costly optimization procedure. Also, EMA and SWA should be added to these CIFAR-100/AG News experiments, not only D4RL.\n* The authors did not provide any supplementary material. Since no code for the empirical evaluation is available, the community cannot verify or assess the proposed approach's contribution.\n\n\n[1] Li, Tao, et al. \"Trainable weight averaging: Efficient training by optimizing historical solutions.\" The Eleventh International Conference on Learning Representations. 2022."}, "questions": {"value": "1. Regarding the experiments in Figure 2, wouldn’t a fairer comparison be to average the SWA with $K$ checkpoints over the last $k$ iterations, rather than every $K$ iterations? The same for EMA: a fairer comparison is to use a weight decay of $1-K$, which effectively averages the last $K$ checkpoints.\n2. Did the authors try a trainable approach as done with TWA [1]? Why not add TWA to the comparison? \n\n[1] Li, Tao, et al. \"Trainable weight averaging: Efficient training by optimizing historical solutions.\" The Eleventh International Conference on Learning Representations. 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PPlRgjoHM7", "forum": "7iSiN20DO4", "replyto": "7iSiN20DO4", "signatures": ["ICLR.cc/2026/Conference/Submission15992/Reviewer_yKEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15992/Reviewer_yKEf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832077545, "cdate": 1761832077545, "tmdate": 1762926200790, "mdate": 1762926200790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses an adaptive weight selecting method for weight averaging method of training DNNs at the training tail stage. Specifically, it formulates a discrete scheme of determining the binary variables for choosing the checkpoints, and then leverage GS estimator to continualize the optimization to fit the training framework. Then, some analytical results are presented, so are the experimental evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The continuation of the weight averaging is a good idea on the class of weight averaging methods for improving the generalization ability of DNNs."}, "weaknesses": {"value": "1. The results are insufficient: except Table 2, the results (e.g.g, Figure 3 and 4 for image class-action and text classification) do not show distinctive improvements so  the competitiveness  lacks convincing supports; otherwise, the performance improvement look increment in general setups.  Besides, the evaluated network architectures are datasets can be extended for comprehensiveness.\n2. As it claims its particular effectiveness on RL that may have more unstable training. Would it be possible to have particular theoretical analysis focusing on such task and settings?\n3. Similar to the insight in the above bullet, at the early training stage “more unstable training” can be expected, what if SeWA is applied in such scenario? (TWA claims its advantage also at the early training stage, as it also somehow optimizes the weights. By the way, why not compare with TWA in the experiments?)."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aZatVr2owH", "forum": "7iSiN20DO4", "replyto": "7iSiN20DO4", "signatures": ["ICLR.cc/2026/Conference/Submission15992/Reviewer_h3s3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15992/Reviewer_h3s3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840495441, "cdate": 1761840495441, "tmdate": 1762926200107, "mdate": 1762926200107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SeWA, Selective Weight Averaging. Instead of averaging the last k checkpoints by rule, SeWA learns which checkpoints to average by casting selection as a probabilistic masking problem.\nIn addition, this paper uses large text space for theory analysis.\nSeWA is evaluated on D4RL behavior cloning, CIFAR-100, and AG News."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well motivated and has strong theoretical backing. The paper provides both flatness convergence arguments and stability-based generalization bounds, comparing favorably with SGD, SWA, and LAWA under convex and non-convex assumptions.\n2. The experiments span three domains (RL, vision, and text), showing consistent improvements across varied architectures and data distributions."}, "weaknesses": {"value": "1. while the paper emphasizes SeWA’s simplicity, its sample-based optimization introduces additional forward passes and probability updates that are not accounted for in the baseline comparisons. A fair comparison should including additional computational consumption on these.\n2. Except for the RL experiments, most performance curves show nearly overlapping trajectories in the figures. The visual and quantitative margins are subtle in the curve plot only form.\n3. The sample-based optimization uses the training objective F as the criterion (as stated in sec. 2) for selecting checkpoints. This implicitly assumes that the training and test loss landscapes are sufficiently aligned, which rarely holds in realistic non-convex regimes. Consequently, SeWA might overfit to training dynamics when the learned selection probabilities are tuned purely on training losses.\n4. Since SeWA relies on sampling and averaging across multiple candidate checkpoints, it requires storing a substantial portion (or even all) of the recent checkpoints in storage consumption. This can become expensive for large-scale or long-horizon training (e.g., vision transformers, LLMs), where checkpoints are heavy and I/O-bound."}, "questions": {"value": "All my concerns/questions are listed in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CzI9vR35Ux", "forum": "7iSiN20DO4", "replyto": "7iSiN20DO4", "signatures": ["ICLR.cc/2026/Conference/Submission15992/Reviewer_VxcC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15992/Reviewer_VxcC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922937017, "cdate": 1761922937017, "tmdate": 1762926198719, "mdate": 1762926198719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}