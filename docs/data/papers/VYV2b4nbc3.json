{"id": "VYV2b4nbc3", "number": 12490, "cdate": 1758208179895, "mdate": 1763551182259, "content": {"title": "Stochastic Order Learning: An Approach to Rank Estimation Using Noisy Data", "abstract": "A novel algorithm, called stochastic order learning (SOL), for reliable rank estimation in the presence of label noise is proposed in this paper. For noise-robust rank estimation, we first represent label errors as random variables. We then formulate a desideratum that encourages reducing the dissimilarity of an instance from its stochastically related centroids. Based on this desideratum, we develop two loss functions: discriminative loss and stochastic order loss. Employing these two losses, we train a network to construct an embedding space in which instances are arranged according to their ranks. Also, after teaching the network, we identify outliers likely to have extreme label errors and relabel them for data refinement. Extensive experiments on various datasets show that the proposed SOL algorithm yields decent rank estimation results even when labels are corrupted by noise.", "tldr": "", "keywords": ["rank estimation", "label noise", "order learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abe0aec0918c81d48cd32b248a4b9000dda8396f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Stochastic Order Learning (SOL), a novel algorithm for robust rank estimation when data labels are corrupted by noise. The key contributions include modeling label errors as random variables, formulating a desideratum based on minimizing stochastic dissimilarity from centroids, and introducing two new loss functions: discriminative loss and stochastic order loss. The method also incorporates an outlier detection and relabeling scheme to refine noisy training data. Extensive experiments on various datasets (facial age estimation, aesthetic score regression, medical assessment, and textual regression) demonstrate that SOL achieves state-of-the-art performance and exhibits strong robustness to label noise."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of treating label errors as random variables and formulating the objective based on minimizing stochastic dissimilarity is a theoretically sound approach to handling noise in ordinal data.\n2. The paper introduces two complementary loss functions: the discriminative loss and the stochastic order loss. The former is for embedding space construction by attracting to neighboring centroids and repelling from distant ones, and the latter enforces pairwise ordering relationships in a stochastic manner.\n3. The outlier detection and relabeling scheme provides a practical way to refine noisy ranks, improving the overall reliability of the training data.\n4. The algorithm is tested extensively across a variety of rank estimation tasks, including facial age estimation (MORPH II, CLAP2015), aesthetic score regression (AADB), medical assessment (RSNA), and textual regression (WMT2020), under synthetic (Gaussian, Laplacian, Uniform) and real-world noise settings."}, "weaknesses": {"value": "1. While experiments demonstrate that the label refinement generally reduces MAE and standard deviation of noise, the relabeling scheme (Equation 20) uses a heuristic approach for the magnitude of label error correction (half of the mean absolute difference over all training instances). A stronger theoretical justification or a more adaptive mechanism for this step could enhance its reliability.\n2. Some Missing related works[1,2,3]. Expecially, [1] used the normal distribution and the Gaussian kernel to model label ambiguity, which is similar to the noise modeling in this work.\n\n[1] Gao, Bin-Bin, et al. \"Deep label distribution learning with label ambiguity.\" TIP 2017\n\n[2] Li, Shikun, et al. \"Selective-supervised contrastive learning with noisy labels.\" CVPR 2022\n\n[3] Liu, Yang, and Hongyi Guo. \"Peer loss functions: Learning from noisy labels without knowing noise rates.\"  ICML 2020"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EGOymK3KYr", "forum": "VYV2b4nbc3", "replyto": "VYV2b4nbc3", "signatures": ["ICLR.cc/2026/Conference/Submission12490/Reviewer_9juu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12490/Reviewer_9juu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761368471718, "cdate": 1761368471718, "tmdate": 1762923364474, "mdate": 1762923364474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We would like to thank all reviewers for their time and effort in providing valuable feedback. We will upload our responses to each question or comment as soon as possible."}}, "id": "sGkl4iR2sF", "forum": "VYV2b4nbc3", "replyto": "VYV2b4nbc3", "signatures": ["ICLR.cc/2026/Conference/Submission12490/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12490/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12490/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762989726657, "cdate": 1762989726657, "tmdate": 1762989726657, "mdate": 1762989726657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an algorithm called Stochastic Order Learning (SOL) for ordinal data rank estimation under label noise. The authors model label errors as random variables and, based on this idea, introduce two loss functions ‚Äî discriminative loss and stochastic order loss ‚Äî to learn an embedding space that preserves rank order despite noisy annotations. Additionally, they design an outlier detection and relabeling mechanism based on the learned embeddings to reduce the effect of noisy labels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors correctly identify the limitation of existing ordinal regression and order-learning methods in handling label noise. The stochastic reformulation is an interesting conceptual step forward.\n- Combining stochastic modeling of label errors with metric learning objectives is novel within the ordinal regression field.\n- The experiments span diverse domains ‚Äî facial age estimation, aesthetic score prediction, medical image assessment, and text regression ‚Äî providing broad empirical context."}, "weaknesses": {"value": "- The algorithm repeatedly relies on the exact probability values $ùëù_ùë†$ of the assumed noise distribution in equations (3), (9), and (12)‚Äì(17). However, in real-world tasks the noise variance œÉ is unknown. The authors only fix a constant ‚Äútest value‚Äù ùúé test\nduring inference (see Eq. (22) on page 7 and Appendix C.3). This assumption undermines the theoretical validity of the method: since the noise distribution cannot be known or verified, the resulting stochastic weighting is arbitrary and not grounded in data.\n- In equations (8)‚Äì(10), the discriminative loss aggregates weighted squared distances between each sample and multiple centroids.\nHowever, this formulation does not ensure that the monotonicity constraint (Eq. (5)) holds.\nAppendix A only shows that monotonicity is a sufficient condition, not a necessary one. The authors incorrectly reverse the logic ‚Äî assuming that minimizing the proposed loss would imply monotonicity.\nThis is a clear logical inversion error, meaning the optimization process does not guarantee the intended ordered embedding structure.\n- All experiments are conducted using synthetic noise (Gaussian, Laplacian, Uniform), despite the paper claiming to handle real-world noisy labels.\nThe only real-noise experiment (on WMT2020) shows merely about a 2% improvement, which is negligible given the additional model complexity. Although the paper claims the training cost is ‚Äúacceptable,‚Äù Tables 19‚Äì21 show that SOL‚Äôs training time roughly doubles (87‚Äì100% slower) compared to the baseline GOL.\nOn larger datasets such as RSNA, a single epoch exceeds 1000 seconds ‚Äî an impractical runtime for real-world use."}, "questions": {"value": "see the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7haQdwthf7", "forum": "VYV2b4nbc3", "replyto": "VYV2b4nbc3", "signatures": ["ICLR.cc/2026/Conference/Submission12490/Reviewer_nqaZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12490/Reviewer_nqaZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735925025, "cdate": 1761735925025, "tmdate": 1762923364063, "mdate": 1762923364063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method, Stochastic Order Learning (SOL), for robust rank estimation on label-noisy data. The method models label errors as random variables and learns an embedding space where each instance is encouraged to approach its stochastically related rank centroids. To achieve this, the authors design two loss functions, the discriminative loss and the stochastic order loss. After training, the method further improves data quality by detecting and relabeling outliers (instances with extreme label errors). Experiments on various datasets (facial age estimation, aesthetic score regression, medical imaging, and textual regression) demonstrate high accuracy and strong robustness to label noise."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "his paper's strengths are as follows.\n\n(1) This paper is the first study to address rank estimation with label noise, a setting that is pervasive in real-world scenarios, as noted in the paper. The contribution is thus highly significant for practical applications.\n\n(2) The paper introduces a natural probabilistic model of label errors for rank estimation and proposes an appropriate learning framework based on this probabilistic formulation.\n\n(3) The method demonstrates robust performance under both artificially generated and naturally occurring label noise."}, "weaknesses": {"value": "This paper's weakness is as a follow.\n\n(1)  The paper assumes label noise as formulated in Equation (2), but it remains unclear how the noise parameter œÉ is determined in real-world problems. During training, œÉ controls the amount of label corruption and is therefore crucial, yet in practice, this parameter is typically unknown. How is œÉ selected or estimated in real-world settings?"}, "questions": {"value": "(1) In real-world scenarios, does label noise actually follow the distribution assumed in Equation (2)? Moreover, how do the authors verify that such a type of label noise occurs in real data?\n\n(2) From the quantitative results in Appendix D.3, the impact of outlier detection and relabeling appears very small. Why does removing outliers not lead to a more noticeable quantitative improvement?\n\n(3) It is recommended to include visualizations of outlier detection on real-noise datasets (e.g., WMT2020).  Since detecting real noisy labels would be highly beneficial in practice, this would better showcase the potential of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UivAdVnnGs", "forum": "VYV2b4nbc3", "replyto": "VYV2b4nbc3", "signatures": ["ICLR.cc/2026/Conference/Submission12490/Reviewer_Bpfh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12490/Reviewer_Bpfh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778754143, "cdate": 1761778754143, "tmdate": 1762923363696, "mdate": 1762923363696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an algorithm called Stochastic Order Learning (SOL) for robust and reliable rank estimation in the presence of label noise. The key idea is to model label errors as random variables, following a discrete Gaussian distribution, and to learn an embedding space where instances are arranged according to their true ranks despite noisy labels. The method introduces two loss functions‚Äîdiscriminative loss and stochastic order loss, which are to enforce geometric constraints in the embedding space. Additionally, SOL includes an outlier detection and relabeling mechanism to refine the training data. Extensive experiments on facial age estimation, aesthetic score regression, medical assessment, and textual regression datasets demonstrate that SOL outperforms existing noise-robust classification, regression, and rank estimation methods under various synthetic and real-world noise settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper tackles an important problem - label noise in ordinal regression.\n2.\tExtensive experiments across multiple domains (computer vision and natural language processing) and various noise types (Gaussian, Laplacian, and Uniform) demonstrate the effectiveness of the proposed approach."}, "weaknesses": {"value": "1.\tThe method's performance is tied to the assumption of a symmetric, unimodal noise distribution, which is a key limitation in practical applications.\n2.\tAblation studies and hyperparameter analysis are heavily focused on CLAP2015. It is unclear if the same settings and component importance hold for datasets with the largest gains (e.g., GDELT is not mentioned in this context, but the principle applies to datasets where SOL shines).\n3.\tThe method introduces non-trivial computational cost compared to non-stochastic baselines, which could be a constraint."}, "questions": {"value": "1.\tThe method assumes a symmetric noise model (Eq. 2). How would SOL perform if the real-world label noise is asymmetric (e.g., annotators consistently over-estimate ages)?\n2.\tThe hyperparameter œÉtest is fixed during inference. Could the performance be further improved by making it adaptive or by estimating it from the data?\n3.\tThe outlier relabeling uses a global average correction (Eq. 20). Have you explored instance-specific relabeling strategies, and why was a uniform correction chosen?\n4.\tThe computational cost is higher than GOL. Are there strategies to improve the efficiency of the stochastic distance computations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "82UeQeYcsS", "forum": "VYV2b4nbc3", "replyto": "VYV2b4nbc3", "signatures": ["ICLR.cc/2026/Conference/Submission12490/Reviewer_8PLT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12490/Reviewer_8PLT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906558271, "cdate": 1761906558271, "tmdate": 1762923363402, "mdate": 1762923363402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed stochastic order learning method frames orders as random variables, and develops discriminative loss and stochastic order loss to optimize network parameters. Experiments are conducted on benchmark facial age estimation datasets. Results show its superiority over baselines under different noise distributions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-The proposed method models label errors as random variables and provides a solid theoretical basis.\n\n-Stochastic order learning method is not sensitive to the prior noise distribution, shown in Table 1~4. Different noise distributions, such as Gaussian, Laplacian and uniform distribution, lead to similar performance.\n\n-Extensive experiments are conducted on various datasets and results show its effectiveness for the age estimation task."}, "weaknesses": {"value": "-Baseline methods are not comprehensive. A na√Øve method is to utilize these mature ranking loss functions in Learning to Rank methods, like RankNet and SoftRank. Similar idea has been implemented in SoftRank. These kinds of methods should be compared in the experiments.\n\n-Compared with GOL in Table 1~4, the performance improvement of stochastic order learning method is marginal. \n\n-Compared with those benchmark loss functions, the computation complexity of the proposed stochastic order loss is higher. Moreover, the time complexity of the proposed loss function should be provided."}, "questions": {"value": "Ranking loss functions, like RankNet and SoftRank, are not compared in the experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y6bBigvvea", "forum": "VYV2b4nbc3", "replyto": "VYV2b4nbc3", "signatures": ["ICLR.cc/2026/Conference/Submission12490/Reviewer_BwZ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12490/Reviewer_BwZ6"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094359075, "cdate": 1762094359075, "tmdate": 1762923362983, "mdate": 1762923362983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}