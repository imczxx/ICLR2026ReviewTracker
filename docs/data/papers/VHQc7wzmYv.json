{"id": "VHQc7wzmYv", "number": 15744, "cdate": 1758254769456, "mdate": 1759897284986, "content": {"title": "TokUR: Token-Level Uncertainty Estimation for Large Language Model Reasoning", "abstract": "While Large Language Models (LLMs) have demonstrated impressive capabilities, their output quality remains inconsistent across various application scenarios, making it difficult to identify trustworthy responses, especially in complex tasks requiring multi-step reasoning.\nIn this paper, we propose a **Tok**en-level **U**ncertainty estimation framework for **R**easoning (**TokUR**) that enables LLMs to self-assess and self-improve their responses in mathematical reasoning.\nSpecifically, we introduce low-rank random weight perturbation during LLM decoding to generate predictive distributions for token-level uncertainty estimation, and we aggregate these uncertainty quantities to capture the semantic uncertainty of generated responses.\nExperiments on mathematical reasoning datasets of varying difficulty demonstrate that TokUR exhibits a strong correlation with answer correctness and model robustness, and the uncertainty signals produced by TokUR can be leveraged to enhance the model’s reasoning performance at test time.\nThese results highlight the effectiveness of TokUR as a principled and scalable approach for improving the reliability and interpretability of LLMs in challenging reasoning tasks.", "tldr": "We study the dynamics between token-level uncertainty estimation and mathematical reasoning.", "keywords": ["Uncertainty Estimation", "Large Language Models", "LLM Reasoning"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6deae95a031a7fbff2370cb926cc54ae4176403.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the TokUR method, which injects low-rank weights to approximate the weight posterior distribution in Bayesian modeling of LLMs. During the decoding stage, it captures uncertainty in the model, leveraging quantized uncertainty to simultaneously estimate the semantic uncertainty of generated responses. Experimental results demonstrate that the proposed method produces uncertainty estimates that correlate well with task difficulty, and further show its effectiveness in detecting erroneous reasoning paths and improving answer quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces a novel approach for estimating uncertainty in long-form reasoning and demonstrates its effectiveness.\n2. The paper is clearly written and easy to follow.\n3. The experimental design is fairly comprehensive."}, "weaknesses": {"value": "1. The authors decompose uncertainty into Aleatoric Uncertainty (AU) and Epistemic Uncertainty (EU). However, the definition of AU appears to correspond more to uncertainty in the input, while EU is more related to uncertainty in the model output. Since EU focuses on the model’s prediction behavior, it should theoretically better reflect the model’s capability issues. Nevertheless, the experimental results do not support this, and the paper does not provide analysis or interpretation of the three uncertainty estimates. I believe such analysis should be included.\n\n2. The expressions for Query-Level Uncertainty and Response-Level Uncertainty both involve (y), and the definitions appear highly similar, which may cause confusion or misunderstanding. I suggest the authors explicitly clarify the difference and explain the rationale behind these definitions to avoid ambiguity for readers.\n\n3. Assumption 3.1 ignores temporal correlations among parameters. The impact of this approximation needs to be justified either qualitatively or quantitatively to demonstrate the validity of the assumption.\n\n4. Some important related work is missing in the discussion. Many existing studies on uncertainty estimation and LLM reasoning theory are not referenced. For example:\n    *  *Position: Uncertainty Quantification Needs Reassessment for Large Language Model Agents* (ICML 2025), which discusses the relevance of uncertainty estimation.\n    * *A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning* (NeurIPS 2025), which provides theoretical insights into reasoning errors in LLMs to improve reasoning accuracy.\n    * *From Calibration to Collaboration: LLM Uncertainty Quantification Should Be More Human-Centered* (arXiv:2506.07461), which analyzes the limitations of existing LLM uncertainty estimation approaches."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "epgmjrtAtV", "forum": "VHQc7wzmYv", "replyto": "VHQc7wzmYv", "signatures": ["ICLR.cc/2026/Conference/Submission15744/Reviewer_YhsK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15744/Reviewer_YhsK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896215814, "cdate": 1761896215814, "tmdate": 1762925980881, "mdate": 1762925980881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the TokUR framework for uncertainty quantification in large language models (LLMs), targeting complex, multi-step mathematical reasoning tasks. TokUR estimates uncertainty at the token level by introducing low-rank random weight perturbations during decoding, enabling principled decompositions into aleatoric and epistemic uncertainty for each token and entire responses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Most technical concepts are well-explained and visualized, aiding algorithmic understanding.\n\n- The approach itself is elegant.\n\n- It addresses a key practical need for safe and reliable LLM deployment, especially where confidence calibration is critical."}, "weaknesses": {"value": "- Experiments focus on mathematical reasoning; application to verbal reasoning, open-ended tasks, or black-box LLMs is not shown.\n\n- Repeated sampling during inference, while compatible with vLLM, is still non-trivial and could limit deployment in latency-sensitive use cases."}, "questions": {"value": "- How does TokUR perform on open-domain, verbal, or dialogue tasks?\n\n- How much cost does in incur to get perturbed weights?\n\n- How do these perturbed weights affect the performance of the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YFXOBZPS7x", "forum": "VHQc7wzmYv", "replyto": "VHQc7wzmYv", "signatures": ["ICLR.cc/2026/Conference/Submission15744/Reviewer_ZNDU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15744/Reviewer_ZNDU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997864004, "cdate": 1761997864004, "tmdate": 1762925980536, "mdate": 1762925980536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TokUR, a framework that lets LLMs self-assess and self-improve on multi-step mathematical reasoning by estimating token-level uncertainty during decoding. TokUR applies low-rank random weight perturbations, producing predictive distributions for each generated token. These per-token signals are decomposed into aleatoric and epistemic uncertainty and then aggregated to the sequence level to capture a response’s semantic confidence. Compared to prior query-level methods and response-level methods, TokUR offers a more fine-grained and accurate uncertainty estiomation. Experiments show that TokUR’s uncertainty is strongly correlated with correctness and robustness, flags incorrect reasoning paths, selects higher-quality solutions among candidates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong theoretical ground. \n\nThe paper’s theoretical backgrounds appear solid and thoughtfully developed. Assumptions are stated clearly.\n\n2. Clear positioning vs. prior work. \n\nThe manuscript does an excellent job for introducing the limitations of prior methods (query-level and response-level approaches), making it easy to understand the gap this work fills. \n\n3. Readable and well organized. \n\nThe paper is clearly written and easy to follow, with clear notations."}, "weaknesses": {"value": "1. On task scope\n\nIt seems that the framework is “not limited to mathematical reasoning”. Is there a reason that the paper should focus especially on mathematical reasoning? A few non-math settings would support the generalizability of the framework.\n\n2. On cost analysis\n\nWhile the cost is the common challenge of uncertainty estimation, it seems that there is no thorough cost analysis in the paper, compared to prior methods like query-level uncertainty estimation. Also, how does the estimation technique used in Section 3.3 help reduce cost? \n\n3. On model diversity\n\nEvaluating two models from the same Llama family seems insufficient to support the correlation between accuracy and token-level uncertainty estimation. Including a few more models from different family and sizes is encouraged to reduce bias in experiments and to strengthen the claim."}, "questions": {"value": "1. It would be helpful to analyze the length-robustness of your token-level uncertainty estimates. As responses grow longer, how do the estimation errors (or bounds) evolve?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "itCzz4U2Rg", "forum": "VHQc7wzmYv", "replyto": "VHQc7wzmYv", "signatures": ["ICLR.cc/2026/Conference/Submission15744/Reviewer_9t1g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15744/Reviewer_9t1g"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998655252, "cdate": 1761998655252, "tmdate": 1762925980200, "mdate": 1762925980200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}