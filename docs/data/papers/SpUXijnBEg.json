{"id": "SpUXijnBEg", "number": 22606, "cdate": 1758333438030, "mdate": 1759896857101, "content": {"title": "Direct Optimal Action Learning", "abstract": "Recent advancements in offline reinforcement learning have leveraged two key innovations: policy extraction from behavior-regularized actor-critic (BRAC) objective and the use of expressive policy architectures, such as diffusion and flow models. However,  backpropagation through iterative sampling chains is computationally tricky and often requires policy-specific solutions and careful hyperparameter tuning. We observe that the reparameterized policy gradient of the BRAC objective trains the policy to clone an ``optimal'' action.  Building on this insight, we introduce   \\textbf{Direct Optimal Action Learning (DOAL)}, a novel framework that directly learns this ``optimal'' action. Then, efficient behavior losses native to the policy's distribution (e.g., flow matching loss) can be used for efficient learning. Furthermore, we demonstrate that the traditional balancing factor between Q-loss and behavior-loss can be reinterpreted as a mechanism for selecting a trust region for the optimal action. The trust region reinterpretation yields a \\textbf{Batch-Normalizing Optimizer}. This facilitates the hyperparameter search and makes it shareable across policy distributions. Our DOAL framework can be easily integrated with any existing Q-value-based offline RL methods. To control the impact of value estimation, our baseline models use simple behavior clone loss and implicit q-learning. We apply DOAL to Gaussian, Diffusion, and Flow policies. In particular, for Diffusion and Flow policies, we obtained strong baseline models by improving the \\textbf{MaxQ Action Sampling}. Our results on 15 tasks from the OGBench and D4RL adroit datasets show that DOAL consistently improves performance compared against strong baseline models while simplifying hyperparameter search. Our best models achieved very strong performance.  The code is available through \\href{https://anonymous.4open.science/r/iclr2026-7144}{Anonymous Github}.", "tldr": "Learn the optimized action, do not learn to optimize action.", "keywords": ["Offline Reinforcement Learning", "Diffusion Model", "Flow Matching", "Reparameterized Policy Gradient"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4b5d79f751f4c0622a1e4b2a54b25540e6ffcea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Direct Optimal Action Learning (DOAL) simplifies offline RL policy extraction by decoupling optimal action computation from policy training. It achieves this by generating a synthetic \"optimal\" action via Q-function ascent, then uses efficient behavior cloning to train the policy to match it. This framework, which also introduces a more stable trust region hyperparameter, consistently outperforms strong baselines across various policy types and benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Policy Extraction. DOAL elegantly decouples optimal action computation from policy training, sidestepping complex backpropagation through generative models by reframing optimization as simple behavior cloning.\n2. Improved Hyperparameter Stability. Reinterpreting the BRAC trade-off coefficient $\\alpha$ as a normalized trust region $\\delta$, backed by Proposition 2 and the Batch-Normalizing Optimizer, offers a more interpretable and stable hyperparameter. Empirical evidence shows $\\delta$ can be shared across algorithms, simplifying tuning.\n3. Strong Empirical Results. DOAL achieves consistent and significant performance gains across 15 challenging offline RL tasks, notably improving over meticulously tuned baselines, which enhances the credibility of its effectiveness."}, "weaknesses": {"value": "1. Inconsistency in Theorems and practical implementation: Proposition 1 establishes the gradient equivalence by defining $a^{\\text {target }}$ using the Q-gradient evaluated at the policy's output, $\\pi_\\theta(s)$. However, the paper then argues this is conceptually inconsistent and, in the final DOAL objective (Eq. 16), defines the target using the Q-gradient evaluated at the data action, $a$. While this change is key to decoupling, the original equivalence from Proposition 1 no longer strictly holds, leaving a gap in the theoretical justification for the final objective.\n2. The paper introduces the Batch-Normalizing Optimizer and the trust-region parameter $\\delta$ as a more stable replacement for the sensitive hyperparameter $\\alpha$ from the BRAC objective. Despite this, the final DOAL actor loss in Equation 15 still contains an $\\alpha$ parameter, which is described as a controller for the \"learning rate of actor\" and is copied from a prior work. This is confusing. It is unclear how this $\\alpha$ interacts with $\\delta$ and why it is still necessary if the optimization trade-off is now managed by the trust region. This ambiguity detracts from the otherwise clean narrative of simplifying hyperparameter tuning.\n3. In Table 1, some experimental results with D- still behind the vanilla algorithms. Also, the reviewer suggests that the authors to better present this table such as showing the final averaged performance."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dwH036y0xs", "forum": "SpUXijnBEg", "replyto": "SpUXijnBEg", "signatures": ["ICLR.cc/2026/Conference/Submission22606/Reviewer_2U5f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22606/Reviewer_2U5f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761493312609, "cdate": 1761493312609, "tmdate": 1762942300910, "mdate": 1762942300910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Direct Optimal Action Learning (DOAL), a framework for offline RL that reframes behavior-regularized actor‚Äìcritic training as directly learning an ‚Äúoptimal‚Äù action target for each data point and then imitation-learning that target using losses native to the policy family (Gaussian Policies, flow policies and diffusion policies). This avoids costly backpropagation through iterative sampling chains in expressive policies (diffusion/flow), while keeping value guidance from the learned Q-function. The authors also introduce a Batch-Normalizing Optimizer that sets a dataset-level ‚Äútrust region‚Äù (Œ¥) to control how far targets move from behavior actions, aiming to make tuning more interpretable and consistent across policy distributions. Empirically, across 15 tasks from OGBench and D4RL Adroit, DOAL variants match the performance of strong baselines (IQL/FQL/TrigFlow) with shared, environment-level hyperparameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a novel and elegant reformulation of behavior-regularized actor‚Äìcritic training by directly learning an optimal per-sample action target and then imitating that target with policy-native losses, which removes the need for costly backpropagation through iterative sampling chains in flow and diffusion-based policies. This idea is conceptually clean, improves computational practicality for expressive policy classes, and introduces a trust-region‚Äìstyle batch normalization that provides a more interpretable and stable hyperparameter compared with traditional BRAC scaling. Empirically, across OGBench and D4RL Adroit, the method achieves performance comparable to strong baselines (IQL/FQL/TrigFlow) while using shared environment-level hyperparameters, suggesting that the proposed reformulation maintains effectiveness without extensive tuning"}, "weaknesses": {"value": "1. Writing & clarity: The paper has noticeable grammatical issues, inconsistent notation, and several typos, which make the narrative harder to follow; The main results table is dense and difficult to scan, hindering quick cross-method comparisons across policy families and environments; consider adding per-method aggregates (e.g., mean/median across tasks) to improve readability.\n\n2. Shallow experimentation: The evaluation covers only 15 tasks total‚Äî6 from D4RL Adroit and 9 from OGBench. Moreover, results are averaged over just 4 seeds (D4RL) and 3 seeds (OGBench), which weakens statistical confidence and robustness of the findings.\n\n3. No compute analysis: A central claimed benefit is reduced computational cost by avoiding backpropagation through iterative diffusion/flow sampling chains, yet the paper provides no wall-clock or memory measurements to substantiate this. Even a simple per-epoch time or training-step latency comparison against BRAC-style training would clarify the practical value.\n\n4. Missing ablations: There are no targeted ablations to disentangle contributions from (i) direct optimal-action targets, (ii) the Batch-Normalizing optimizer; the current analysis discusses hyperparameters but doesn‚Äôt isolate causal impact on performance."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tYh4FTdkvC", "forum": "SpUXijnBEg", "replyto": "SpUXijnBEg", "signatures": ["ICLR.cc/2026/Conference/Submission22606/Reviewer_5mtx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22606/Reviewer_5mtx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880137408, "cdate": 1761880137408, "tmdate": 1762942300634, "mdate": 1762942300634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Direct Optimal Action Learning (DOAL), a framework that replaces end-to-end BRAC-style policy gradients through the value network with a two-step policy extraction:\n(1) compute an ‚Äúoptimized‚Äù target action per data point via a first-order step on the Q-function;\n(2) train the policy to imitate this target with a behavior-native loss (e.g., Gaussian MSE, flow matching, diffusion reconstruction).\nThis yields distribution-agnostic policy extraction without backpropagating through iterative samplers used by diffusion/flow policies. The authors reinterpret the BRAC trade-off as a trust region and introduce a Batch-Normalizing Optimizer that scales the action step by the batch norm of |\\‚àá‚ÇêQ|, controlled by a single hyperparameter Œ¥. Experiments on OGBench and D4RL Adroit show consistent improvements of DOAL variants (Gaussian, flow, diffusion) over their respective baselines, and analyze Max-Q sampling and the role of candidate count \nùëõ\nsample\nn\nsample\n\t‚Äã\n\n."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Simple, general recipe for policy extraction. Shows BRAC‚Äôs policy gradient can be reframed as target-matching against an action updated by ‚àá‚ÇêQ, enabling training with any policy family‚Äôs native loss (Gaussian/flow/diffusion) and avoiding gradients through multi-step samplers.\n\nInterpretable trust region. The Batch-Normalizing Optimizer controls expected step size via Œ¥, normalizing by batch statistics of |\\‚àá‚ÇêQ|¬≤‚Äîcleaner than tuning BRAC‚Äôs ùõº.\n\nCovers expressive policies. Instantiates DOAL for Gaussian, Flow Matching, and Diffusion policies with concrete objectives (e.g., DIQL, DIFQL, DTrigFlow)."}, "weaknesses": {"value": "Novelty is mainly a reinterpretation plus a practical recipe.\nThe equivalence that turns BRAC into target-matching is neat but technically light. Stronger differentiation from value-guided diffusion/flow, energy guidance, and behavior-regularized objectives would clarify what DOAL achieves beyond engineering convenience.\n\nValue-quality dependence and limited uncertainty handling.\nDOAL targets rely on ‚àáùëéùëÑ at data actions; there‚Äôs no integrated uncertainty-aware step control (ensembles, variance-based scaling) for OOD safety, which matters in offline regimes.\n\nBenchmark scope and stability.\nWhile OGBench and Adroit are covered, Adroit volatility and late-training collapse are reported without a full diagnosis, weakening stability claims."}, "questions": {"value": "1. Target evaluation point. You compute ‚àáùëéùëÑ at data actions to avoid a mismatch with ùúãùúÉ(ùë†). Have you compared evaluating at ùúãùúÉ(ùë†Ôºâ (or hybrids) and quantified the difference?\n\n2. Œ¥ scheduling. Does adaptive or annealed Œ¥ improve late-training stability on Adroit where collapses occur?\n\n3. Value backbones. What happens when pairing DOAL with ReBRAC-style critics‚Äîdo gains persist?\n\n4. High-D actions. For diffusion/flow in high-D action spaces, does anisotropy in ‚àáùëéùëÑ hurt target-matching? Any whitening or per-dimension scaling beyond batch normalization? \n\n5. Max-Q proposals. Beyond sampling from ùúãùúÉ, can proposal refinement (e.g., short Langevin steps guided by Q) reduce noise sensitivity for large ùëõ samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xh7oywkrRc", "forum": "SpUXijnBEg", "replyto": "SpUXijnBEg", "signatures": ["ICLR.cc/2026/Conference/Submission22606/Reviewer_GcHL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22606/Reviewer_GcHL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897553015, "cdate": 1761897553015, "tmdate": 1762942300338, "mdate": 1762942300338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Direct Optimal Action Learning (DOAL), a framework for offline RL that avoids backpropagating through multi‚Äëstep generative policies (diffusion/flow) when using BRAC‚Äëstyle actor objectives. The key observation is that the BRAC policy gradient is (approximately) equivalent to minimizing the distance between the policy output and a single ‚Äúoptimal action‚Äù target derived via a first‚Äëorder update from the dataset action. DOAL computes that target directly from \\nabla_a Q_\\phi(s,a) (evaluated at the dataset action) and then trains the policy to imitate the target using a loss native to the policy family (e.g., flow matching or a diffusion loss), thus decoupling target computation from the policy‚Äôs sampling chain. The paper further replaces the sensitive BRAC coefficient \\alpha with a Batch‚ÄëNormalizing Optimizer that scales the update so that the expected squared step size equals a user‚Äëchosen trust‚Äëregion parameter, and presents Algorithm‚ÄØ1 showing integration with IQL for value learning. Experiments on 9 OGBench tasks and 6 D4RL Adroit tasks compare Gaussian, flow, and diffusion policies show mixed results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear, unifying insight and simple objective. This paper makes explicit that BRAC‚Äôs policy gradient equals the gradient of a squared‚Äëerror to a target action. This clarifies what end‚Äëto‚Äëend training is doing and motivates learning the target directly, which is easy to implement and compatible with any policy class.\n- DOAL avoids backprop through iterative sampling chains, which is particularly important for diffusion/flow actors by supervising with native behavior losses. This makes the approach broadly plug‚Äëand‚Äëplay with flow/diffusion policies.\n- Hyperparameter reinterpretation via a trust region. The Batch‚ÄëNormalizing Optimizer replaces a brittle \\alpha search with an interpretable \\delta that sets an expected squared update magnitude; the denominator‚Äôs batch statistic stabilizes scale across tasks."}, "weaknesses": {"value": "- To me, it's unprincipled to replace a‚Äô that which should be sampled from the policy with the dataset action a, the paper does not provide a solid justification for this change (cf. Prop. 1/Eq. 13). In addition, the derivation appears to hinge on an L2 BC loss‚Äîhow does the argument extend to other losses (e.g., log-likelihood, flow-matching, diffusion objectives)? \n- Results are not uniformly stronger. The claim that DOAL ‚Äúconsistently improves performance‚Äù over strong baselines is only partially supported. For example, DTrigFlow is dramatically worse on cube‚Äësingle‚Äëplay, and DIQL is broadly weaker than IQL on many tasks. \n- Efficiency claims lack measurement. A central motivation is avoiding BPTT through iterative samplers, but there are no wall‚Äëclock time, memory, or gradient‚Äëcall counts comparing DOAL to TrigFlowQL/FQL under matched hardware.\n- The observation of Max-Q sampling does not constitute a novel contribution. Prior work (e.g., Q-chunking) already formalizes that the candidate count N in Max-Q sampling implicitly sets the level of KL regularization between the induced policy and the proposal/sampling distribution. From this perspective, increasing N weakens the effective regularization and can degrade performance, so it is unsurprising that ‚Äúthe bigger N, the better‚Äù does not hold. Please cite these results.. \n- Scope of value learners. Most experiments couple DOAL with IQL for a controlled study, but since BRAC‚Äëstyle methods remain strong (e.g., ReBRAC in Table‚ÄØ1). Is this policy extraction approach still improve on other value learners?"}, "questions": {"value": "see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5sVU6q0jjQ", "forum": "SpUXijnBEg", "replyto": "SpUXijnBEg", "signatures": ["ICLR.cc/2026/Conference/Submission22606/Reviewer_bqLn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22606/Reviewer_bqLn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160041925, "cdate": 1762160041925, "tmdate": 1762942299942, "mdate": 1762942299942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}