{"id": "7B9mTg7z25", "number": 12994, "cdate": 1758212546541, "mdate": 1759897471966, "content": {"title": "The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against LLM Jailbreaks and Prompt Injections", "abstract": "How should we evaluate the robustness of language model defenses? Current defenses against jailbreaks and prompt injections (which aim to prevent an attacker from eliciting harmful knowledge or remotely triggering malicious actions, respectively) are typically evaluated either against a *static* set of harmful attack strings, or against *computationally weak optimization methods* that were not designed with\nthe defense in mind. We argue that this evaluation process is flawed.\n\nInstead, we should evaluate defenses against *adaptive attackers* who explicitly modify their attack strategy to counter a defense's design while spending *considerable resources* to optimize their objective. By systematically tuning and scaling general optimization techniques—gradient descent, reinforcement learning, random search, and human-guided exploration—we bypass 12 recent defenses (based on a diverse set of techniques) with attack success rate above 90% for most; importantly, the majority of defenses originally reported near-zero attack success rates. We believe that future defense work must consider stronger attacks, such as the ones we describe, in order to make reliable and convincing claims of robustness.", "tldr": "Jailbreak and prompt injection defenses must stop evaluating with datasets of static harmful strings and fixed attack algorithms; by evaluating against \"adaptive attacks\", we break 12 defenses with simple attack techniques.", "keywords": ["prompt injection defense", "adaptive evaluation", "jailbreaks", "adversarial examples"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7855aef1ec24a6c34096532b64736e455004920c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The Authors critique the standard evaluation practices for LLM jailbreak and prompt injection defenses, arguing that testing against static attack datasets provides a false and misleading sense of security. The authors posit that defenses must instead be evaluated against strong, adaptive attackers who are aware of the defense's design (mirroring Kerckhoffs's Principle from cryptography). This paper introduces a general, four-step adaptive attack framework (Propose, Score, Select, Update, or \"PSSU\") and instantiate it using four powerful attack families: gradient-based, reinforcement learning, search-based, and large-scale human red-teaming. By applying this rigorous methodology, the authors systematically \"break\" 12 recent and diverse LLM defenses, achieving over 90% attack success rates on systems that originally reported near-zero vulnerability. The work concludes that the field must adopt this more adversarial and adaptive evaluation standard to make credible claims of robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's primary strength is its comprehensive and rigorous experimental takedown of 12 recent LLM defenses.\n2. The \"PSSU\" (Propose, Score, Select, Update) loop  is a clear and effective generalization that unifies disparate attack methods (gradient, RL, search, human) into a single, understandable conceptual framework.\n3. The unique and impressive strength is the execution of a large-scale ($20,000 prize pool, 500+ participants) human red-teaming competition."}, "weaknesses": {"value": "1. The paper's automated attacks, particularly the search-based method, are extremely powerful, relying on a SOTA LLM (Gemini-2.5 Pro) as the \"LmMutator\" and another LLM as a \"critic\". While this is a fair choice under their \"large compute\" threat model, it somewhat obscures the \"algorithmic\" contribution of the PSSU framework itself. It is difficult to disentangle whether the high ASR is due to the novel search/RL framework or the emergent creative-reasoning capabilities of the SOTA models powering the attack.\n\n2. The paper correctly identifies reward hacking and unreliable auto-raters as a key weakness in automated evaluation ( Appendix C.1 ). However, the search-based attack's \"Scorer\" component uses a \"separate critic LLM\" to generate numerical scores from qualitative feedback. This appears to re-introduce the very problem the paper warns about. The authors do not sufficiently discuss how they validated this critic LLM or ensured it wasn't providing misleading or \"hackable\" scores, which could in turn lead the LmMutator down trivial, non-generalizable paths."}, "questions": {"value": "Question-1: Your search-based attack's 'Scorer' relies on a 'separate critic LLM' to convert textual feedback into a numerical score. How did you validate this critic LLM to ensure it wasn't susceptible to its own form of reward hacking (i.e., the LmMutator learning to \"trick\" the critic) and that its scores meaningfully guided the search toward genuinely effective attacks?\n\nQuestion-2: how much of the automated attack's success hinges on using a SOTA model (Gemini-2.5 Pro) as the LmMutator? Have you tested whether the PSSU framework's effectiveness persists if a weaker, open-source model is used as the mutator? This would help clarify the specific contribution of your framework versus the raw capability of the powerful model you used to drive it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Not applicable"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iSu665WL6Q", "forum": "7B9mTg7z25", "replyto": "7B9mTg7z25", "signatures": ["ICLR.cc/2026/Conference/Submission12994/Reviewer_kWyh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12994/Reviewer_kWyh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761282331441, "cdate": 1761282331441, "tmdate": 1762923743158, "mdate": 1762923743158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges current evaluation practices for LLM defenses, arguing that robustness claims based on static attack sets or weak optimization methods are unreliable. The authors propose evaluating defenses against adaptive adversaries that tailor their strategies to the defense design and use scalable optimization methods such as gradient descent, RL, random search, and human-guided exploration. Applying this framework, they successfully bypass 12 prominent LLM defenses, each originally reporting near-zero attack success, now with success rates exceeding 90%. Their findings highlight that robustness requires testing against adaptive, defense-aware attackers, calling for stronger and more realistic evaluation protocols."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a highly relevant and pressing issue in LLM safety, evaluating the true robustness of jailbreaking and prompt-injection defenses.\n2. The authors conduct an extensive and systematic analysis of 12 diverse defense mechanisms, providing a thorough assessment."}, "weaknesses": {"value": "1. The proposed attack framework requires more clarification for different threat models. For example, under white-box access, what exactly is the attacker optimizing? Are they generating a suffix, a full prompt template, or token-level perturbations, and which optimization strategies are available or suitable in each case? Conversely, under black-box access, what are the precise inputs and outputs for each attack family (gradient-based, RL, search, human red-teaming), and how do those strategies interact with limited-query, rate-limited environments? Detailing these distinctions would improve the paper’s quality.\n2. Lack of methodological details. Although the paper introduces a generalized PSSU framework, the description remains largely conceptual. Critical implementation details, such as how prompts are represented, how the scoring function is operationalized, and how the update step is concretely performed across different attack types, are missing. This abstraction limits reproducibility and makes it difficult to assess the actual novelty or technical rigor of the proposed adaptive framework.\n3. Some claims are too strong but insufficiently justified. To substantiate such a claim, it would be better to provide a theoretical formulation of robustness in the context of jailbreak attacks, including what constitutes a robust defense, how robustness should be measured under adaptive adversaries, and what characteristics a defense must satisfy to meet that standard."}, "questions": {"value": "1. How are the four steps of the PSSU loop concretely instantiated under each threat model setup? Does the framework support cross-family adaptation (e.g., applying reinforcement learning fine-tuning after a search-based exploration), or are the four attack families evaluated independently?\n2. What objective function is used to quantify success, detectability, and cost in the Score step? In the Update step, how are gradient or policy updates performed in the highly discrete prompt space? Are gradients approximated in embedding space or estimated through REINFORCE-like techniques? What are the termination criteria for the optimization loop (e.g., fixed iterations, convergence threshold, or resource budget)?\n3. What is the performance of the proposed PSSU loop on the 12 evaluated defenses? Are any of these defenses considered robust under this framework? Additionally, the results presentation appears unclear; if detailed outcomes are shown in Figure 1, this figure is never referenced or discussed in the main text.\n4. Evaluation metrics: What metrics are used to measure the success of jailbreak attacks under different threat model setups? The paper mentions using an LLM judge. Does this judge produce a binary success/failure label or a graded score along a numerical range?\n5. It would strengthen the paper to include a more comprehensive discussion of existing jailbreak attack methodologies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QypQ77drS8", "forum": "7B9mTg7z25", "replyto": "7B9mTg7z25", "signatures": ["ICLR.cc/2026/Conference/Submission12994/Reviewer_XpmW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12994/Reviewer_XpmW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761522805857, "cdate": 1761522805857, "tmdate": 1762923742760, "mdate": 1762923742760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper critiques current LLM jailbreak and prompt injection defenses by showing that most are evaluated against static or weak attack sets, leading to a false sense of robustness. The authors propose a general adaptive attack framework that integrates gradient-based, reinforcement learning, search-based, and human red-teaming attacks. Using these methods, they systematically break 12 state-of-the-art defenses and achieve over 90% attack success rate in most cases."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong empirical contribution. The authors conduct a comprehensive empirical study, evaluating 12 well-known LLM defenses under multiple adaptive attack paradigms. The scale and depth of this evaluation are impressive and provide much-needed realism in jailbreak defense assessment.\n\n2. Methodological framework. The generalized “PSSU” adaptive attack loop unifies existing attack types under a single conceptual lens. This provides a reusable structure for future evaluation tools.\n\n3. High practical relevance. The results have direct implications for practitioners and researchers designing LLM defenses. The finding that most defenses fail under realistic adaptive evaluation is an important wake-up call for the field.\n\n4. Contribution to jailbreak research. The study serves as a critical meta-evaluation for the entire jailbreak-defense literature and offers clear guidelines (e.g., including human red-teaming, adaptive tuning) that can inspire more rigorous evaluation pipelines."}, "weaknesses": {"value": "1. Cross-method comparability and interpretability. Because the 12 defenses come from heterogeneous benchmarks and threat settings, the reported attack success rates are not directly comparable. The paper explicitly states this limitation, but it weakens the quantitative conclusions — making it hard to decide which defense class performs better. As a result, the framework cannot currently guide defense selection.\n\n2. Limited extensibility and sustainability. The framework lacks an explicit plan for continual integration of new attacks and defenses. If the ecosystem is not maintained or modularized, future defenses or new attack paradigms (e.g., agentic adaptive attacks or multimodal prompts) may again reveal unaddressed vulnerabilities. Without an open, extensible benchmark infrastructure, the community risks repeating the same evaluation pitfalls.\n\n3. Lack of actionable guidance. While the paper convincingly breaks defenses, it provides limited insight into how to design better ones. Beyond “evaluate adaptively,” readers are left without systematic principles for constructing more resilient defenses.\n\n4. Evaluation cost realism. Some adaptive attacks assume substantial compute and fine-tuning budgets. It remains unclear how such evaluations can be standardized or made reproducible across different resource regimes."}, "questions": {"value": "1. After running adaptive evaluations across multiple defenses, how should practitioners select among defenses? Is there a metric or qualitative signal that indicates partial robustness under your framework?\n\n2. Can your adaptive evaluation framework also provide guidance for improving defenses (e.g., by analyzing failure patterns or sensitivity to attack families)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qXkDTVYbLM", "forum": "7B9mTg7z25", "replyto": "7B9mTg7z25", "signatures": ["ICLR.cc/2026/Conference/Submission12994/Reviewer_8Azb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12994/Reviewer_8Azb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851607649, "cdate": 1761851607649, "tmdate": 1762923742440, "mdate": 1762923742440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a brand-new framework to reliably evaluate the robustness of defense against jailbreak and prompt injection attacks. With the solid evaluation, the authors bypass 12 recent SOTA defenses with ASR higher than 90% for most of the methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1 This paper is well-written.\n\n2 The soundness of this method is good.\n\n3 The experiments are quite solid.\n\n4 The findings proposed in this paper are interesting to the community."}, "weaknesses": {"value": "From my view, this paper has no obvious weakness, and I think the solid evaluation made by the authors and the insights proposed in this paper should be highlighted to the adversarial community."}, "questions": {"value": "1 How does the proposed framework perform on the Claude-family models, such as Claude 3.5,  Claude 4 and Claude 4.5 Sonnet?\n\n\n2 In reality, the defenders usually apply multiple defenses instead of a single one. Does combining multiple strong defense methods enhance the empirical robustness of the system?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uS1fWKEJKt", "forum": "7B9mTg7z25", "replyto": "7B9mTg7z25", "signatures": ["ICLR.cc/2026/Conference/Submission12994/Reviewer_BBuB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12994/Reviewer_BBuB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990500054, "cdate": 1761990500054, "tmdate": 1762923741945, "mdate": 1762923741945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}