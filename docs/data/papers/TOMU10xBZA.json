{"id": "TOMU10xBZA", "number": 15375, "cdate": 1758250737107, "mdate": 1759897310847, "content": {"title": "AlphaFormer: End-to-End Symbolic Regression of Alpha Factors with Transformers", "abstract": "Identifying predictive patterns for stock market trends, known as alpha factors, is a critical challenge in quantitative finance. Symbolic regression (SR) methods can discover these factors as interpretable mathematical expressions, offering advantages over ``black-box'' machine learning approaches and manual methods that rely heavily on human expertise. However, existing SR methods typically restart the discovery process for each new dataset, failing to leverage prior knowledge. To address this limitation, we propose AlphaFormer, an encoder-decoder Transformer model designed for the end-to-end generation of synergistic alpha factors from raw stock market data. AlphaFormer leverages pre-training on synthetic datasets to efficiently uncover synergistic alpha factors for new datasets, capitalizing on acquired prior knowledge. To overcome the challenge of generating synthetic stock datasets with temporal dependencies, we introduce a novel generative framework that integrates multiple time-series generative models to generate synthetic stock data and dynamically select the highest quality samples, ensuring the creation of high-fidelity datasets crucial for pre-training. Extensive evaluations on real-world stock market datasets demonstrate that AlphaFormer outperforms existing methods across widely used metrics, achieving superior performance with significantly reduced inference computation---generating only 33\\% as many factors as the best baseline and requiring no further training during inference. Backtests further show that AlphaFormer delivers the highest annual return among all methods, highlighting its practical potential for superior investment performance.", "tldr": "We propose AlphaFormer, an encoder-decoder Transformer model specifically designed for the end-to-end generation of synergistic alpha factors from raw stock market data.", "keywords": ["Symbolic Regression", "Alpha Mining", "Time Series Generative Modeling"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd934e6d454debb520e20356ad2e1205a04d2ac1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AlphaFormer to learn alpha factors from raw stock market data, lying in the symbolic regression scheme. It also incorporates with a pre-training process on synthetic data to transfer prior knowledge. AlphaFormer is claimed to outperform alternative baselines in terms of both evaluation metrics and simulated trading performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The use of pre-training to transfer knowledge for symbolic alpha discovery is interesting.\n    \n- The preliminaries about alpha factor, alpha mining, factor pools, and evaluation are clearly formalized, making the paper more readable to a broader ML audience."}, "weaknesses": {"value": "- The synthetic data generation may not provide sufficient information to learn prior knowledge for transformation into downstream applications. The generative models used are not specifically designed for financial data, which probably fail to capture useful temporal dynamics but output some noise. Moreover, the likelihood-based selection of high-quality synthetic data cannot guarantee diversity.\n    \n- This paper claims that the proposed approach reduces inference computation, but does not provide its complexity (or runtime) and the comparison against existing methods.\n\n- [Minor] Citation formatting appears incorrect throughout the paper."}, "questions": {"value": "- Beyond CSDI used in this paper, there are some other representative deep generative models particularly designed for time series, such as TimeGAN [1] and Diffusion-TS [2]. Have they been explored for the synthetic data generation in this paper?\n    \n- As far as I know, vanilla GRU and Transformer without specific designs are not the common choice for effective time series generation. Why are they chosen for the synthetic data generation?\n    \n- What are the computational complexity and runtime of the proposed approach?\n    \n- Until 2021, the results in Figure 4 do not show that AlphaFormer outperforms the other baselines. Does it mean that the effectiveness of the proposed approach may be limited in some specific periods?\n\n\n\n[1] Yoon et al. \"Time-series generative adversarial networks.\" *Neurips* 2019.\n\n[2] Yuan et al. “Diffusion-TS: Interpretable Diffusion for General Time Series Generation.“ *ICLR* 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ctHbvGchDF", "forum": "TOMU10xBZA", "replyto": "TOMU10xBZA", "signatures": ["ICLR.cc/2026/Conference/Submission15375/Reviewer_kFA2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15375/Reviewer_kFA2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839537943, "cdate": 1761839537943, "tmdate": 1762925657478, "mdate": 1762925657478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AlphaFormer that performs symbolic regression to generate synergistic alpha factors. The model is pre-trained on synthetic time-series datasets produced by multiple generative models and then iteratively constructs an alpha pool on real data. Empirically, AlphaFormer achieves higher IC, Rank IC, Sharpe ratio, and annual return than several baselines, and delivers higher cumulative returns in backtests on CSI 300 and S&P 500."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Deep learning for symbolic regression of alpha factors is well-motivated.\n\n- AlphaFormer provides interpretable alpha factors while enabling efficient end-to-end generation.\n\n- Heavy lifting in pre-training with a light, fixed-parameter inference loop is operationally attractive."}, "weaknesses": {"value": "- The synthetic data generation part is somewhat unreliable. Alpha factors learned from synthetic data cannot generalize to the real stock market as long as the synthetic data do not fully resemble real data.\n\n- The backtest curve in Figure 4 shows that until roughly 2021 the cumulative excess return of AlphaFormer is not strictly superior. The effectiveness of AlphaFormer in the simulated trading is weak."}, "questions": {"value": "- How do you confirm that the prior knowledge learned from synthetic data is economically meaningful across regimes? Have you compared pre-training on synthetic data with pre-training on real data?\n\n- Statistical methods, such as ARMA-GARCH and the bootstrap, are considered generative models in finance. Why not include these methods to generate synthetic data?\n\n- The evaluation of synthetic data quality is model-centric. Have you considered statistical criteria?\n\n- Why do you use an LSTM for stock embedding? It seems that transformers are more suitable for this task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zrMfm1f9g5", "forum": "TOMU10xBZA", "replyto": "TOMU10xBZA", "signatures": ["ICLR.cc/2026/Conference/Submission15375/Reviewer_boST"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15375/Reviewer_boST"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918577932, "cdate": 1761918577932, "tmdate": 1762925656899, "mdate": 1762925656899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key limitation in alpha factor discovery: existing symbolic regression methods restart from scratch for each new dataset without leveraging prior knowledge. AlphaFormer proposes an encoder-decoder Transformer for end-to-end alpha factor generation from raw stock data, enabled by pre-training on synthetic datasets to learn transferable patterns. To generate high-fidelity synthetic stock data with temporal dependencies, the authors introduce a novel framework that integrates multiple time-series generative models (GRU, Transformer, Diffusion) with dynamic quality-based selection via an LSTM evaluator. The model architecture embeds both the dataset (using LSTM and Transformer encoders to capture cross-stock relationships) and the existing factor pool, then autoregressively generates new factors in Reverse Polish Notation. Experiments on CSI300 and CSI500 show AlphaFormer achieves superior IC (6.01% vs 5.19% for the best baseline) while generating only 33% as many factors and requiring no retraining during inference. Backtesting demonstrates the highest annual returns, highlighting practical potential through efficient pre-training and inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) Novel approach to synthetic data generation for financial time series\nTraditional symbolic regression pre-training uses simple distributions (e.g., sampling independent points). However, AlphaFormer did:\n(i) Temporal dependencies (ii) Cross-stock correlations: Dependencies between multiple stocks. The proposed multi-model generation framework with dynamic selection addresses this challenge effectively, representing a non-trivial contribution to generating realistic financial data for pre-training.\n\n(2) Paradigm shift from search to conditional generation. This paper demonstrates substantial originality by reconceptualizing alpha factor discovery. While prior methods (GP-based and RL-based like AlphaGen) frame this as a search/optimization problem requiring extensive exploration per dataset, AlphaFormer transforms it into a conditional generation problem with pre-training. This enables knowledge transfer across datasets and significantly improves inference efficiency.\n\n(3) Strong empirical results with practical implications. The method achieves state-of-the-art performance across multiple metrics (IC, Rank IC, Sharpe Ratio, CAGR) while generating 67% fewer factors than baselines. The cross-market generalisation and superior backtest returns demonstrate real-world applicability."}, "weaknesses": {"value": "(1) Compared to AlphaGen, the innovation is not so significant. The main contributions are adding pre-training and multi-stock modelling. The primary improvements are replacing the RL training paradigm with pre-training and swapping LSTM for Transformer. While this brings significant efficiency gains, the core mechanisms remain similar to AlphaGen, including RPN representation, alpha pool management, and L1-regularised linear combination. The work feels more like an incremental extension (\"AlphaGen + pre-training\") rather than a fundamental methodological breakthrough.\n\n(2) The paper claims \"high-fidelity synthetic datasets,\" but provides insufficient validation: No distribution distance metrics (e.g., Wasserstein distance, MMD) between synthetic and real data are reported. No analysis of what temporal properties the generative models capture (autocorrelation, volatility clustering, etc.). Using likelihood for sample selection does not guarantee the synthetic data captures true market dynamics; high likelihood does not equal realistic market behaviour."}, "questions": {"value": "(1) Can you add error bars or other measures to account for randomness in the cumulative return plots? Statistical significance testing (e.g., bootstrap confidence intervals, performance across different random seeds) would help distinguish genuine improvements from noise.\n\n(2) I do not have sufficient information to reproduce your results. I could not find the code or a GitHub link (unless I missed it). If reproducibility issues are addressed, particularly by releasing code and detailed implementation details, I would consider raising my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "goHa3oSEgo", "forum": "TOMU10xBZA", "replyto": "TOMU10xBZA", "signatures": ["ICLR.cc/2026/Conference/Submission15375/Reviewer_JWzm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15375/Reviewer_JWzm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15375/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995845279, "cdate": 1761995845279, "tmdate": 1762925656389, "mdate": 1762925656389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}