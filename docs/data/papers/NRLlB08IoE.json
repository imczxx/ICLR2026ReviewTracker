{"id": "NRLlB08IoE", "number": 20806, "cdate": 1758310414835, "mdate": 1759896957669, "content": {"title": "DL-Bench: Deep learning specific code generation benchmark", "abstract": "Deep learning (DL) has revolutionized  areas such as computer vision, natural language processing, and more. However, developing DL systems is challenging due to the complexity of DL workflows. Large Language Models (LLMs), such as GPT, Deepseek, Claude, Llama, Mistral, Qwen, etc., have emerged as promising tools to assist in DL code generation, offering potential solutions to these challenges. Despite this, existing benchmarks like DS-1000 are limited, as they primarily focus on small DL code snippets related to pre/post-processing tasks and lack comprehensive coverage of the full DL pipeline, including different DL phases and input data types. Similarly, MLE-bench focuses more on Machine Learning Engineering (MLE) tasks and broader ML workflows, without leveraging test cases.\n\nTo address this, we introduce DL-Bench, a novel benchmark dataset designed for function-level DL code generation. DL-Bench categorizes DL problems based on three key aspects: phases such as pre-processing, model construction, and training; tasks, including classification, regression, and recommendation; and input data types such as tabular, image, and text. DL-Bench diverges from related benchmarks, DS-1000 and AICoderEval, across four dimensions: it occupies a semantically distinct region for both prompts and code embedding, emphasizes DL constructs with a higher DL/ML token ratio, and requires more complex code solutions. State-of-the-art LLMs (e.g., O3-Mini, DeepSeak-V3) achieve, on average, significantly lower 28.5\\% pass@1 score on DL-Bench than on DS-1000 (53.3\\%).  This result underscores DL-Bench`s greater challenging problems set.  Our taxonomy of bugs found in LLM-generated DL code highlights the distinct challenges that LLMs face when generating DL code compared to general code.\nFurthermore, our analysis reveals substantial performance variations across categories which emphasizes valuable insights that DL-Bench offers for potential improvement in the DL-specific generation. Our preliminary result shows that\nDL-Bench can enhance LLM performance as a categorization training dataset, achieving an average 4.2\\% improvement on DS-1000 with guided three-shot learning.\n\nOverall, our empirical results demonstrate the utility of DL-Bench as a comprehensive benchmark while offering insights for future improvements across diverse functional categories.", "tldr": "DL-Bench is a 520-function benchmark showing LLMs struggle with deep-learning code", "keywords": ["Benchmark", "Code Generation", "Deep Learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d04e7f7c793ae889d498e7504de06eee7800a3a9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new benchmark, DL-Bench, that evaluates LLMs’ ability to generate deep-learning-specific functions, covering deep-learning (DL) problems among three aspects: different phases, different tasks and different input data types. The main contribution is the benchmark itself.\nThis benchmark is novel (different from previous benchmarks) on multiple dimensions: (1) it occupies a semantically distinct region in the embedding space, (2) it contains more DL/ML specific keywords, (3) the oracle solutions are more complex, and (4) the pass@1 on DL-Bench of the SOTA LLMs is significantly lower than the pass@1 on previous benchmarks.\nAnalysis on DL-Bench provides new insights: (1) Analysis on LLMs’ generated code on DL-Bench find different bug distribution from that on general code. (2) Performance difference across different categories indicates insights for potential improvement. (3) When used as examples in few-shot prompting, DL-Bench can improve the performance of the problems in the same category."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper provides the various experiments from different aspects on its differences from previous benchmarks, and performs more analysis on LLMs’ generated code on DL-Bench. The paper clearly describes the data collection process and all the prerequisites of the analysis experiments. DL-Bench covers 520 samples from different DL libraries, constituting a broad context to examine LLMs’ ability to generate DL-specific functions."}, "weaknesses": {"value": "The evidence in the paper only shows the difficulty (pass@1 on this benchmark is lower, the oracle solution is more complex) and the semantic difference (semantically distinct in embedding space), but none of them shows the quality of this benchmark. A more difficult and different benchmark is not automatically a better benchmark.\n\nMajor questions/limitations:\nThe quality of DL-Bench is questionable. Many experiments only show that DL-Bench is different from/more difficult than existing DL benchmarks, but they do not guarantee the quality of DL-Bench. On the other hand, from many evidences, I believe the quality of DL-Bench is questionable.\n  1. Prompt: One of the bug categories in this benchmark is “prompt missing information” (Section 5.2, Finding 6), but this category only means the quality of the prompt provided by DL-Bench is not good enough.\n  2. Function signature: The prompt does not always include the signature of the function to be generated. For example, the `load_model` function in the dataset does not contain the exact function signature, but only a rough description. In Python, missing this information (e.g. not including the name of each parameter) can cause the function failed to be called in the test (e.g. when arguments are passed by corresponding parameter names).\n  3. Data leakage: In Section 3, the authors claimed to crawl the code only from GitHub repositories updated after the training cutoff of GPT-4o, October 2023. But this only limits the repository update time, but not the update time of each function. For example, the `make_grid` function in the dataset was last updated at May 12, 2022, before the cutoff date of GPT-4o, causing a potential data leakage. Even worse, some source repositories of the benchmark are not updated since long ago. For example, `IntelLabs/nlp-architect` was archived on Nov 8, 2022, and the code of `itdxer/neupy` was lastly updated on Apr 5, 2019, `DeepRegNet/DeepReg` was updated on Jun 19, 2021. Overall, the dataset might contain serious data leakage.\n  4. Context: This benchmark examines the quality of the generated functions by replacing the original functions. However, the necessary context of the target function is not passed to the LLM.\n  5. Executive environment: This benchmark contains calls to external libraries. Different versions of these libraries have different APIs. However, the list of available libraries and their versions are never told to the LLMs. If the LLMs import a library not in the environment, the generated code accidentally fails.\n\nMinor questions/limitations:\n1. Section 5.2 Finding 6: The comparison between bug distributions of general code generation and Deep Learning code generation is invalid: the authors compare the failures of different LLMs on different benchmarks. There are two variables to explain the difference in bug distribution, and we do not know how much of the difference comes from the difference in model capabilities, and other portions come from the difference in the coding task.\n2. Errors in dataset: the columns ` test_suites` and ` ground_truth_function_code` do not match other columns in the dataset. For example, for the test sample `to_image` in Figure 9, these columns are probably for the function ` distort_points_kannala_brandt`.\n3. The file `src/LLM/call.py` in the repository is not runnable, because of the text `W` on line 30.\n4. Environment: although there are many dockerfiles, the testing frameworks in the benchmark rely on local environments, and there is no mention of these dockerfiles in project otherwhere. This means there is no environment separation, and the library version conflicts are very easy to happen.\n5. The repository is full of unprofessional code, and whether the benchmark actually works on another computer is very questionable."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fth2BSurmA", "forum": "NRLlB08IoE", "replyto": "NRLlB08IoE", "signatures": ["ICLR.cc/2026/Conference/Submission20806/Reviewer_GnjX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20806/Reviewer_GnjX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706897289, "cdate": 1761706897289, "tmdate": 1762935591114, "mdate": 1762935591114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DL-Bench, a benchmark dataset of 520 function-level deep learning code generation problems sourced from 30 GitHub repositories. The dataset categorizes problems along three dimensions: DL pipeline stages (pre/post-processing, model construction, training, inference, evaluation), ML tasks (like classification, regression, etc), and input data types (image, text, structured array). The authors evaluate seven state-of-the-art LLMs on DL-Bench, finding significantly lower performance (28.5% average pass@1) compared to the existing DS-1000 benchmark (53.3%). The paper includes quantitative analysis showing DL-Bench occupies a semantically distinct space from related benchmarks, a bug taxonomy specific to DL code generation, and demonstrates potential applications for few-shot prompting improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-  The benchmark construction is rigorous with multiple validation steps. The filtering from 2,000+ raw data points to 520 high-quality instances through both automated and manual review (involving four co-authors) ensures quality. The evaluation is comprehensive, testing seven diverse LLMs. Enough points were made to show difference between earlier ML/DL benchmarks like DS 1000 and DL Bench.\n- The three-dimensional categorization scheme (pipeline stage × ML task × input data type) is novel and provides actionable insights for targeted improvements. The bug taxonomy specifically for DL-generated code extends prior work meaningfully with new categories like \"arithmetic and logical errors\" and \"performance issues.\"\n- The motivation is clear—existing benchmarks lack coverage of complete DL pipelines. The paper systematically demonstrates four dimensions of divergence from existing benchmarks (semantic space, DL-relevance, solution complexity, and LLM performance). \n- The work addresses a real gap. The finding that state-of-the-art models achieve only 28.5% average pass@1 on DL-Bench versus 53.3% on DS-1000 is striking and highlights the challenge and introduces a solid benchmark in the field of DL based code generation problems."}, "weaknesses": {"value": "- There is inconsistency in reporting evaluations with other prior work. For showing that DL bench has no overlap with prior benchmarks, it compared itself with DS-1000 and AICoderEval, whereas for all other ablations (especially to show difficulty and categories), it didn't include AICoderEval. \n\n- With only 520 instances from 30 repositories, the dataset is relatively small, compared to its former dataset DS-1000, it is almost half its size. \n\n- The manual labeling process is critical but under-documented, the claim of \"strong inter-rater reliability\" in Section 7 lacks supporting evidence. The same can be said about labels in the classification task. \n\n- There is no comparison with human evaluations to know how well do humans do on this task. That will genuinely tell more about the diversity and difficulty of the dataset."}, "questions": {"value": "- Figure 1 can be more clearer with proper annotations showing where a LLM is involved and where humans are involved. \n- There has been very less discussion about test cases / test case quality. \n- Why does O3-Mini show lower performance with three-shot prompting on DS-1000 (61.0% to 50.2%)? This seems counterintuitive.\n- What specific criteria led to the reduction from 2,000 raw data points to 520 final instances? It was not clear while reading the paper. \n- The abstract can be made shorter and clearer, especially while discussing the effectiveness of the benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DmbZ7QSxoN", "forum": "NRLlB08IoE", "replyto": "NRLlB08IoE", "signatures": ["ICLR.cc/2026/Conference/Submission20806/Reviewer_eoPa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20806/Reviewer_eoPa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967513124, "cdate": 1761967513124, "tmdate": 1762935579569, "mdate": 1762935579569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DL-Bench, a benchmark designed to address the inadequacies of existing benchmarks in evaluating the DL code generation capabilities of LLMs. Current benchmarks focus primarily on fragmented data science code snippets and fail to comprehensively cover the entire DL workflow. DL-Bench fills this gap by providing a function-level code generation benchmark that spans various DL stages, multiple task types, and different data types. The core contributions are: 1) The construction and release of a more challenging, DL-focused code generation benchmark; 2) A fine-grained analysis of LLM performance across different DL sub-tasks, enabled by a multi-dimensional taxonomy; 3) The proposal of a bug taxonomy specifically for LLM-generated DL code; 4) A demonstration of how the benchmark can be used to improve code generation performance on other datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear and Significant Problem Statement. The paper identifies the limitations of existing data-science-oriented benchmarks in assessing DL code generation, addressing a clear gap in the current field.\n2. Well-Reasoned Benchmark Design. The three-dimensional taxonomy (DL pipeline stages, ML task types, input data types) is a commendable design choice. It enables a more in-depth and fine-grained analysis of model capabilities than previously possible.\n3. Comprehensive Experimental Evaluation. The benchmark is validated across seven SOTA LLMs under controlled conditions, with statistical rigor. The results demonstrate that DL-Bench is semantically distinct and requires more complex solutions than prior benchmarks."}, "weaknesses": {"value": "1. Support for Some Conclusions is Weak.\n\t1) \"Live\" Version Evaluation: The results in Table 2 are inconclusive, as the cutoff date becomes more recent, the performance degradation for some LLMs is not significant, and Qwen Coder 2.5's performance even increases. This weakens the conclusion that newer data is inherently more challenging.\n\t2) Classifier Performance Discrepancy: The classifier used to predict pipeline stages reportedly achieves high accuracy on manually labeled instances but a low weighted F1-score of only 0.56 in five-fold cross-validation. This discrepancy is not adequately explained and impacts the reliability of the Stage-Predicted Three-Shot experiments.\n2. The paper suffers from several writing and formatting issues. For instance, there are missing spaces after commas, and punctuation is sometimes absent where required (e.g., a missing period before \"Finally, we\" in Section 4.1)."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0NgSoC8ZE8", "forum": "NRLlB08IoE", "replyto": "NRLlB08IoE", "signatures": ["ICLR.cc/2026/Conference/Submission20806/Reviewer_eF5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20806/Reviewer_eF5M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987586074, "cdate": 1761987586074, "tmdate": 1762935519836, "mdate": 1762935519836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript presents DL-Bench, a benchmark to evaluate LLMs on DL function code generation. The benchmark is sourced from GitHub repositories, includes generated prompts for extracted DL functions, and is categorized based on sample pipeline step, task type, and input type. The authors show that DL-Bench differentiates itself from related datasets through a semantic comparison, token analysis, and code complexity analysis. When evaluating state-of-the-art LLMs on DL-Bench, the overall performance is worse than related datasets and unique types of failures were identified. Furthermore, the authors show that DL-Bench can be used for purposes other than benchmarking, such as DL code classification and few-shot prompting."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Overall, the manuscript is very well structured and written. The findings are substantiated with significant results from relevant experiments and presented nicely in figures and tables."}, "weaknesses": {"value": "In my opinion, there are a few minor weaknesses in the manuscript. The first is that the text is incredibly dense and still many details had to be left out of the main text and added as an appendix. For example, the data categories and labels are only listed in Table 3 and in Appendix F but not in the main text. This has the effect that I sometimes didn't feel I grasped all the important aspects of the benchmark as I was reading the text. I am not sure how this can be best remedied within the page limit, but I did feel sometimes that the gray \"Findings\" text bubbles were somewhat repetitive in regards to the text that come right before them. Additionally, I spotted a few small language errors in the text. These include inconsistency between how figures are referenced in the text (sometimes \"Figure X\" is used while \"Fig X\" is used elsewhere), repeated author names in text references (e.g., \"Shin et al.Shin et al\" on line 98), and other miscellaneous errors (e.g., \"LLM\" -> \"LLMs\" on line 179, \"a commonly used *metric* pass@k\" on lines 244-245, \"Also, even if\" -> \"Also, even though\" on line 466)."}, "questions": {"value": "These weaknesses do not significantly detract from the manuscript's merit. \nTherefore, I would recommend accepting this manuscript. I hope the authors can make minor adjustments to their manuscript to address the weaknesses I explained above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k0nsgpjjDq", "forum": "NRLlB08IoE", "replyto": "NRLlB08IoE", "signatures": ["ICLR.cc/2026/Conference/Submission20806/Reviewer_F6LC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20806/Reviewer_F6LC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988297825, "cdate": 1761988297825, "tmdate": 1762935494215, "mdate": 1762935494215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}