{"id": "u1RjV99DPu", "number": 14883, "cdate": 1758245031022, "mdate": 1759897343639, "content": {"title": "Multi-Level Multi-Turn RL Outperforms GRPO: Reasoning with Textual Feedback", "abstract": "Reinforcement learning with verifiable rewards has become the standard for training reasoning models, with Group Relative Policy Optimization (GRPO) achieving remarkable performance across mathematical, coding, and scientific domains. However, these approaches suffer from severe sample inefficiency due to sparse binary rewards, where even partially correct responses receive zero reward, providing no learning signal and causing extremely slow convergence. We propose Multi-Level Multi-Turn Reinforcement Learning (MLMT-RL), a novel framework that addresses this limitation by leveraging textual feedback to provide dense, interpretable learning signals. MLMT-RL decomposes reasoning into two synergistic levels: a higher-level policy generates task-specific contextual feedback, while a lower-level policy produces refined responses conditioned on this feedback. To ensure effective coordination between guidance generation and execution, we formulate a principled bi-level optimization framework where the higher-level policy is regularized by the lower-level value function. Additionally, we introduce novel metrics to evaluate feedback quality and utilization effectiveness. Our results demonstrate superior parameter efficiency: MLMT-RL with 2B parameters outperforms 3B GRPO models by 3.13% on MATH500, 5.18% on MBPP, and 4.77% on GPQA. Similarly, our 6B model surpasses 7B GRPO models by 3.0%, 2.8%, and 5.7% respectively. MLMT-RL thus establishes a highly efficient paradigm that delivers superior reasoning performance with significantly fewer parameters.", "tldr": "MLMT-RL is a multi-level multi-turn approach that decomposes reasoning into higher-level feedback generation and lower-level response refinement, outperforming GRPO-based models on three benchmarks.", "keywords": ["hierarchical reinforcement learning", "LLM reasoning", "self-correction in LLMs", "multi-turn RL"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/401113a9512a1f1012498b3238e332869b652974.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Multi-Level Multi-Turn Reinforcement Learning (MLMT-RL), a novel hierarchical reinforcement learning framework for reasoning tasks. The core idea is to mitigate the sparse reward problem in GRPO (Group Relative Policy Optimization) by using a higher-level policy to generate task-specific textual feedback, which guides a lower-level policy to refine responses. The method features:\nA bi-level optimization structure with value regularization. New evaluation metrics (feedback optimality and compatibility). Significant empirical gains over GRPO and other strong baselines (e.g., SCoRe, ArCHer)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses the sparse reward problem in GRPO-style RL by introducing a multi-level, multi-turn framework that leverages textual feedback for improved learning efficiency.\n\nThe bi-level optimization formulation with value function regularization is reasonable and aligns the feedback generation with the refinement process.\n\nEmpirical results show solid gains across multiple reasoning benchmarks, with smaller models outperforming larger GRPO baselines.\n\nNovel metrics for feedback quality and compatibility provide a useful way to evaluate intermediate learning signals"}, "weaknesses": {"value": "The proposed MLMT-RL framework presents a reasonable yet mostly compositional innovation. While the idea of multi-turn feedback-based refinement has been explored in various prior works, this paper organizes it into a bi-level reinforcement learning framework with value function regularization, and introduces evaluation metrics to quantify feedback quality. The contribution is more in the systematization and empirical demonstration than in the introduction of novel algorithmic paradigms.\n\nThe overall architecture is a fairly straightforward composition of known ideas (multi-turn feedback, hierarchical RL), with limited algorithmic novelty.\n\nThe textual feedback mechanism relies heavily on LLM heuristics, and the feedback generation process is not end-to-end learnable.\n\nExperiments are restricted to structured reasoning tasks (math/code/QA), and generalization to open-ended or real-world scenarios is unclear."}, "questions": {"value": "Generalization / Task Scope\n\nHave the authors considered applying the method to open-ended or real-world tasks beyond structured reasoning benchmarks?\n\nRobustness of Feedback\n\nHow robust is the approach to noisy or hallucinated feedback? Would incorrect guidance from the higher-level model degrade performance?\n\nAblation / Design Choice\n\nIs there any analysis on how much the value regularization term contributes to the performance, compared to standard two-level training?\n\nModel Efficiency\n\nWould the method still hold advantages in settings where training two separate models is computationally prohibitive?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sJuzEW3FWT", "forum": "u1RjV99DPu", "replyto": "u1RjV99DPu", "signatures": ["ICLR.cc/2026/Conference/Submission14883/Reviewer_wXhB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14883/Reviewer_wXhB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760501094951, "cdate": 1760501094951, "tmdate": 1762925232146, "mdate": 1762925232146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Multi-Level Multi-Turn Reinforcement Learning (MLMT-RL), a hierarchical framework designed to address the sample inefficiency problem in GRPO caused by sparse binary rewards. The key idea is to decompose reasoning into two synergistic levels: a higher-level policy generates task-specific textual feedback, while a lower-level policy produces refined responses conditioned on this feedback. The authors formulate this as a bi-level optimization problem where the higher-level policy is regularized by the lower-level value function. Experiments on MATH-500, MBPP, and GPQA demonstrate that MLMT-RL achieves superior parameter efficiency, with 2B models outperforming 3B GRPO models and 6B models surpassing 7B GRPO models across all benchmarks. The paper also introduces novel metrics to evaluate feedback quality and utilization effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper provides compelling empirical evidence of GRPO's limitations (74% advantage collapse, 83% zero rewards on MBPP), making a strong case for the proposed approach\n2.\tThe bi-level optimization formulation (Equation 3) provides a principled way to coordinate the higher-level and lower-level policies, with mathematical derivation in the appendix\n3.\tThe experiments span multiple domains (math, coding, science), model scales (1B-8B), and include carefully designed baselines that ablate different components (ZSCoT, SCoRe-FF, TF-LBM)\n4.\tThe single-model variant (Table 3) shows comparable performance, improving deployability"}, "weaknesses": {"value": "1.\tLimited theoretical analysis. For example, no convergence guarantees for the bi-level optimization. This paper also miss analysis of why textual feedback fundamentally addresses sparse rewards beyond providing \"dense signals\".\n2.\tEvaluation limited to reasoning tasks; generalizability to open-ended generation, dialogue, or multimodal tasks is unclear.\n3.\tThe heterogeneous model pairing analysis (Section 7.6) is limited to 1B scale\n4.\tThe choice of k in V^k_L approximation lacks ablation studies\n5.\tNo comparison with recent strong baselines (e.g., DeepSeek-R1 mentioned in references but not compared)\n6.\tMLMT-RL requires 16.5 hours vs. BC's 1.2 hours—the trade-off isn't thoroughly discussed\n7.\tSome hyperparameter choices (e.g., λ value, k value) lack sensitivity analysis"}, "questions": {"value": "1.\tCan you provide convergence analysis or sample complexity bounds for the bi-level optimization? Under what conditions does the approximation V*_L ≈ V^k_L hold?\n2.\tHow sensitive is the method to the regularization parameter λ in Equation 3? What happens with different values of k for the value function approximation?\n3.\tGiven the 13.75× increase in training time vs. BC (16.5h vs. 1.2h), can you provide a cost-benefit analysis? At what performance level does MLMT-RL become preferable?\n4.\tThe FO metric uses an LLM judge. Have you validated this against human judgments? What's the inter-rater agreement?\n5.\tHave you tested MLMT-RL on tasks beyond reasoning (e.g., summarization, creative writing)? Why should we expect the approach to work there?\n6.\tHow does MLMT-RL compare to the latest models like DeepSeek-R1(Distill) or o1-preview that also use RL for reasoning?\n7.\tIn what situations does the higher-level policy generate unhelpful or misleading feedback? Can you characterize failure modes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2dMtcVZTXO", "forum": "u1RjV99DPu", "replyto": "u1RjV99DPu", "signatures": ["ICLR.cc/2026/Conference/Submission14883/Reviewer_gugG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14883/Reviewer_gugG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761409045974, "cdate": 1761409045974, "tmdate": 1762925231799, "mdate": 1762925231799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The core contribution of this paper is to address the problems of insufficient learning signals and slow convergence in reinforcement learning inference tasks due to sparse binary rewards. The authors propose the MLMT-RL framework, which decomposes the complex inference process into a hierarchical learning framework by introducing a text feedback mechanism: The higher-level policy is responsible for generating guiding corrections, providing dense text feedback. The lower-level policy is responsible for revising the initial answer based on the feedback provided by the higher-level policy, generating a better answer. These two models are updated through bi-level optimization with additional value function regularization. Furthermore, to quantify the effectiveness of the feedback, the authors design new evaluation metrics, including feedback optimality and feedback compatibility. Experimental results show that MLMT-RL not only converges faster on multiple inference benchmarks with parameter efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper accurately identifies the core pain points of methods such as GRPO in sparse reward environments—\"advantage collapse\" and low sample efficiency. Introducing dense textual feedback to replace sparse binary rewards is a very intuitive and effective solution. \n\nThe problem is broken down into two hierarchical structures: \"generating feedback\" and \"utilizing feedback.\" A two-level optimization model is used, and a theoretical derivation is provided to guarantee the optimal strategy, making the entire learning process more stable and efficient.\n\nThis paper establishes a reasonable baseline, covering different types of baselines such as BC, ZSCoT, ArCHer, and SCoRe, clearly demonstrating its advantages in multi-level, multi-round, and dynamic learning feedback. In addition to the final acc, convergence speed and parameter efficiency are compared, proving the framework's superiority in improving the performance of small models. Furthermore, LLM-as-a-judge is introduced to evaluate feedback quality, and new metrics (FO, $\\Delta$acc, etc.) and corresponding ablation experiments are designed to verify the necessity of each component in the framework (high/low-level fine-tuning, value regularization)."}, "weaknesses": {"value": "W1: The central motivation of this work rests on the \"advantage collapse\" issue in GRPO. While the authors provide initial statistics (74% collapse frequency), this claim could be significantly strengthened. The current analysis does not fully disentangle whether this is an inherent flaw of sparse-reward RL or a byproduct of confounding factors.\n\n- Could this issue be mitigated by using stronger base models (e.g., Llama-3.2-3B-instruct, Qwen2.5 and Qwen3 series), different hyperparameter settings (e.g., higher sampling temperature for), or a larger rollout size such as 16? A more rigorous analysis is needed to rule out these possibilities.\n\n- The provided statistics are static. It would be more compelling to show how the collapse rate evolves during GRPO's training process. Visualizing this trend would more dynamically illustrate the learning stagnation and underscore the necessity of the proposed MLMT-RL framework.\n\nW2: There is a wealth of work in the field currently on \"self-reflection,\" \"self-correction,\" and \"self-evolving.\" The paper's contribution needs to be more clearly situated within the landscape of related research.\n\n- The proposed two-layer \"generate-then-refine\" process is structurally similar to many \"self-reflection\" and \"self-correction\" paradigms. The authors should explicitly articulate the unique advantages of formalizing this as a multi-turn, bi-level RL problem, compared to simpler iterative refinement schemes.\n-  The high-level policy acts as a \"teacher\" providing feedback to a \"student\" low-level policy. This paper would benefit from a discussion on how MLMT-RL relates to or advances traditional Teacher-Student learning. Is it a new form of dynamic knowledge transfer where the teacher's guidance is itself learned via RL?  The two-layer framework of MLMT-RL is formally very similar to the \"generate-reflect-correct\" process. The authors need to more clearly articulate the essential differences between MLMT-RL and these paradigms. What are the unique advantages of modeling in a multi-turn form?\n\nW3: There are critical unresolved questions regarding the learning process of the low-level policy. The lower policy $\\pi_L$ is used for two functionally distinct tasks: generating an initial answer (turn 1) and revising it based on feedback (turn 2). Using a single network for both could lead to task interference or conflicting gradient updates. The authors should justify this design choice or discuss potential alternatives (e.g., using separate policy heads). Besides, the pseudocode suggests that learning signals originate from the final reward after Turn 2. It is unclear if or how the policy $\\pi_L$ for the initial generation is updated. If it is not updated, it functions merely as a fixed sampler, which diminishes the \"interactive learning\" aspect of the framework. If it is updated, the mechanism for credit assignment and backpropagation to this first step needs to be explicitly detailed.\n\nW4: Several key implementation details are missing, hindering reproducibility and a full understanding of the results.\n\n- The update rule for the value function $V_L$ appears to be based on TD learning, involving a $V_L(x_{next}, g_{next})$ term. However, in a two-turn episodic setting, the task terminates after the second turn, meaning there is no \"next state.\" This suggests the update should be a Monte Carlo one, based on the final reward $R$. The authors must clarify the exact update mechanism and how to sample x_next and g_next. Furthermore, the specific architecture for encoding the (x, g) pair into the DistilRoBERTa-based value network needs to be described.\n\n- For the final evaluation, is the reported performance based on the single-turn generation of the fine-tuned $\\pi_L$, or does it involve the full two-turn generation-and-correction pipeline? This is crucial for understanding the practical utility and computational cost.\n\n- The data source and training algorithm for the BC baseline are not specified. Was it trained on expert trajectories? What specific SFT algorithm was used? This information is essential for a fair comparison.\n\nMinor Problems: In line 205, the 79% acc of MBPP should be Figure 1 Left. Besides, if the authors consider open-sourcing the code, it will be helpful to enhance academic reproducibility."}, "questions": {"value": "Q1: The framework in this paper is set at two rounds. Can it naturally scale to more rounds (L > 2)? Compared to true multi-round inference frameworks such as REMA[1] and RAGEN[2], what are the potential and limitations of MLMT-RL in handling longer inference chains?\n\nOther problems mentioned in Weaknesses.\n\n[1] Wan, Ziyu, et al. \"Rema: Learning to meta-think for llms with multi-agent reinforcement learning.\" arXiv preprint arXiv:2503.09501 (2025).\n[2] Wang, Zihan, et al. \"Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning.\" arXiv preprint arXiv:2504.20073 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dDTsksmQJu", "forum": "u1RjV99DPu", "replyto": "u1RjV99DPu", "signatures": ["ICLR.cc/2026/Conference/Submission14883/Reviewer_65SX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14883/Reviewer_65SX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919844019, "cdate": 1761919844019, "tmdate": 1762925231399, "mdate": 1762925231399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Multi-Level Multi-Turn Reinforcement Learning (MLMT-RL), a novel reinforcement learning framework designed to improve reasoning models trained with verifiable rewards. Considering the sparse binary rewards in GRPO, MLMT-RL overcomes this limitation by incorporating textual feedback from a higher-level policy model and optimizing the whole framework through a bi-level optimization process. The higher-level optimization aims to improve feedback optimality, and the lower-level optimization aims to optimize the reasoning ability. By combining multi-level, multi-turn feedback, and verifiable rewards, it achieves robust reasoning performance, dense learning signals, and improved sample efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a comprehensive experimental and theoretical analysis of the limitations of GRPO, with a specific focus on the issue of sparse binary rewards. By demonstrating how the lack of intermediate feedback signals hampers efficient learning in reasoning tasks, the authors establish a strong motivation for developing a more informative RL framework.\n2. The proposed MLMT-RL introduces a well-structured bi-level optimization paradigm that coordinates a higher-level feedback generation policy with a lower-level reasoning policy.\n3. Experimental results across multiple reasoning benchmarks (e.g., MATH500, MBPP, GPQA) show consistent and substantial improvements over GRPO, demonstrating the efficacy of the proposed approach."}, "weaknesses": {"value": "1. The paper does not clearly define the optimization goal of the higher-level. Although a loss function is provided, the direction and purpose of this optimization are not well explained. This makes it difficult to interpret how the higher-level objective contributes to overall model improvement\n2. While the proposed bi-level optimization framework is conceptually sound, its computational complexity is not clearly discussed. Because the higher- and lower-level objectives are nested, optimizing the higher-level policy often requires an additional development set and may involve second-order computations such as Hessian approximations [1,2]. The paper does not specify the time or computational cost of this process, nor does it provide details on how the gradients are practically updated. Instead, it only presents an approximate objective, leaving the actual optimization procedure somewhat unclear. Moreover, in typical bi-level optimization, multiple lower-level updates are performed before each upper-level update. The paper does not clarify this relationship or provide details about how often the two levels of optimization are alternated during training.\n3. All compatibility metrics focus on improvements from the first to the second turn, with little analysis of benefits beyond two turns. As a result, the paper provides limited evidence for true multi-turn scalability.\n4. Since GRPO is an outcome-driven optimization method, a natural baseline would be to introduce a reward model that scores rollouts. Both this approach and MLMT-RL introduce an additional model component, yet the authors do not discuss or compare the two. Including such a comparison would strengthen the justification for MLMT-RL.\n5. The necessity of the proposed bi-level structure is not fully justified. If the higher-level model is already fine-tuned (e.g., via SFT) to generate adaptive feedback, one could update only a single model while treating higher-level responses as prompts. The paper does not discuss why maintaining two separately optimized policies is preferable to this simpler approach.\n6. In Appendix 7.6, both the higher-level and lower-level models are approximately 1B in size. It would be valuable to explore asymmetric configurations (e.g., a 7B higher-level model with a 1.5B lower-level model) to test whether larger higher-level models can provide stronger guidance. When both models are of similar size, their reasoning capabilities may overlap, and the higher-level model could introduce redundant or even misleading feedback during rollouts.\n7. In the introduction, the authors mention three types of feedback and claim that task-specific trained feedback significantly outperforms the alternatives across all benchmarks. However, no corresponding figures or tables are provided to substantiate this claim.\n8. It is unclear whether the higher-level policy continues to provide feedback when the lower-level model already produces a correct answer. In many cases, unnecessary feedback could add noise and inefficiency. If such cases are frequent, the higher-level model may receive even sparser reward signals, undermining the motivation for introducing dense feedback in the first place.\n9. The paper lacks a clear description of the training datasets used during optimization, which makes it difficult to assess the reproducibility and fairness of the experimental results.\n10. Figure 4 (right) compares model sizes between GRPO and MLMT-RL. Still, given that MLMT-RL involves two models, it would be more informative to list the base models and sizes of both the higher-level and lower-level components explicitly, ensuring a fair comparison.\n11. Typo: line 304, $s$ is introduced without any explanation. Moreover, a concise notation table (covering all policies, value functions, optimal variants, and regularization terms) would substantially improve clarity.\n\n---\n[1] Chen H, Wang X, Guan C, et al. Auxiliary learning with joint task and data scheduling[C]//International conference on machine learning. PMLR, 2022: 3634-3647.\n\n[2] Liu Z, Chai H, Li C, et al. Adaptive Data and Task Joint Scheduling for Multi-Task Learning[C]//2025 IEEE 41st International Conference on Data Engineering (ICDE). IEEE, 2025: 2038-2051."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OFXwPdVk17", "forum": "u1RjV99DPu", "replyto": "u1RjV99DPu", "signatures": ["ICLR.cc/2026/Conference/Submission14883/Reviewer_RGJa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14883/Reviewer_RGJa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921802160, "cdate": 1761921802160, "tmdate": 1762925230943, "mdate": 1762925230943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}