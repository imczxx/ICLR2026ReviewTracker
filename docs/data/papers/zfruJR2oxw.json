{"id": "zfruJR2oxw", "number": 9312, "cdate": 1758118463041, "mdate": 1759897732085, "content": {"title": "CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning", "abstract": "Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing methods typically treat all training samples uniformly, overlook the vast differences in problem difficulty relative to the model's current capabilities. This uniform training strategy leads to inefficient exploration of problems the model has already mastered, while lacking effective guidance on the problems that are challenging its abilities the most, limiting both learning efficiency and the performance upper-bound. To address this, we propose \\textbf{CLPO (Curriculum-guided Learning for Policy Optimization)}, a novel algorithm that creates a dynamic pedagogical feedback loop within the policy optimization process. The core of CLPO is to leverage the model's own rollout performance to conduct real-time difficulty assessment, thereby constructing an \\textbf{Online Curriculum}. This curriculum then guides an \\textbf{Adaptive Problem Restructuring} mechanism, where the model acts as its own teacher: it diversifies medium-difficulty problems to promote generalization and simplifies hard problems to make them more accessible. Our approach transforms the static training procedure into a dynamic process that co-evolves with the model's capabilities. Experiments show that CLPO achieves \\textbf{state-of-the-art (SOTA)} performance across eight challenging mathematical and general reasoning benchmarks, with an average \\textbf{pass@1} improvement of \\textbf{6.96\\%} over ohter methods, demonstrating its potential for more efficiently training more capable reasoning models.", "tldr": "", "keywords": ["Large Language Models; LLM Reasoning;Curriculum Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d16215ce9ee920609220e4bb805213c1dd4feacf.pdf", "supplementary_material": "/attachment/12d713cd2e786fe0bb0998e7ce49ed6875bfcb66.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes CLPO, a novel framework for improving LLM reasoning via reinforcement learning. CLPO creates a dynamic curriculum by using the model's own performance to assess problem difficulty in real-time. It then restructures problems, diversifying medium ones and simplifying hard ones, to create an optimal training batch, leading to a more efficient and effective learning process that co-evolves with the model's capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe core idea of \"Guided Self-Evolution\" is innovative and directly addresses the limitations of uniform data sampling in RLVR. It creates an adaptive, self-improving loop that is both intuitive and powerful.\n2.\tThe paper provides strong empirical validation through extensive experiments, including detailed ablation studies for each component and a pass@k analysis that demonstrates improved solution diversity."}, "weaknesses": {"value": "The CLPO framework introduces several steps that seem computationally expensive compared to baselines like GRPO. Specifically, for each batch, CLPO requires (1) generating N rollouts to assess the difficulty of each original problem, and (2) using the LLM itself to rewrite the \"medium\" and \"hard\" problems, and then (3) generating another N rollouts for these newly restructured problems to filter them. This overhead is non-trivial and is a critical factor for practical applications. The paper lacks any analysis or discussion of the computational cost (in terms of time or FLOPs) compared to the baseline methods."}, "questions": {"value": "1.\tWhich specific Qwen3-8B was used—the base or the post-training?\n2.\tDid you reproduce all the baseline results in Table 1 yourselves?\n3.\tThe core components like adaptive restructuring and dynamic KL seem similar to prior work [1,2,3]. You seem to have achieved this merely by assembling or tuning parameters. What is the key novelty beyond combining these ideas?\n4.\tHow do you guarantee that \"Adaptive Problem Restructuring\" strictly preserves the original answer?\n5.\tHow does CLPO compare to DAPO when evaluated under an equal computational budget (e.g., fixed training time)?\n\n[1] Xu C, Sun Q, Zheng K, et al. WizardLM: Empowering large pre-trained language models to follow complex instructions[C]//The Twelfth International Conference on Learning Representations. 2024.\n\n[2] Wang Y, Kordi Y, Mishra S, et al. Self-instruct: Aligning language models with self-generated instructions[J]. arXiv preprint arXiv:2212.10560, 2022.\n\n[3] Yu Q, Zhang Z, Zhu R, et al. Dapo: An open-source llm reinforcement learning system at scale[J]. arXiv preprint arXiv:2503.14476, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ep0ksgJUQQ", "forum": "zfruJR2oxw", "replyto": "zfruJR2oxw", "signatures": ["ICLR.cc/2026/Conference/Submission9312/Reviewer_syXH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9312/Reviewer_syXH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799714739, "cdate": 1761799714739, "tmdate": 1762920947533, "mdate": 1762920947533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CLPO (Curriculum-guided Learning for Policy Optimization) is proposed to create a dynamic pedagogical feedback loop within the policy optimization process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The proposed RL paradigm could run automatically.\n2.The experiments show improvements across a variety of datasets."}, "weaknesses": {"value": "1.It's better to show improvements for some datasets such as AIME25, LCV-V6.\n2.The MATH-500 performance is not improved.\n3.The improvements is not obvious for some datasets such as AMC23, MMLM-Pro\n4.Qwen-3B is in the experiments, there are not other base-model be evaluated. \n5.There is limited novielty in the proposed algorithm."}, "questions": {"value": "1.Why are improvements demonstrated only on specific datasets like AIME25 and LCV-V6, and not across a broader range of benchmarks?\n2.Why has performance on the MATH-500 dataset remained unchanged despite the proposed method?\n3.Why are the reported improvements minimal or inconsistent on datasets such as AMC23 and MMLM-Pro?\n4.Why is Qwen-3B the only base model evaluated in the experiments, with no comparison to other foundational models?\n5.What novel contributions does the proposed algorithm introduce beyond existing methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZOGhg3P36f", "forum": "zfruJR2oxw", "replyto": "zfruJR2oxw", "signatures": ["ICLR.cc/2026/Conference/Submission9312/Reviewer_rCGW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9312/Reviewer_rCGW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920817827, "cdate": 1761920817827, "tmdate": 1762920947269, "mdate": 1762920947269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CLPO (Curriculum-guided Learning for Policy Optimization), a novel algorithm to improve training efficiency for Large Language Models (LLMs) in reasoning tasks using Reinforcement Learning with Verifiable Rewards (RLVR). Unlike existing methods that treat all training samples equally, CLPO dynamically assesses problem difficulty based on the model's performance and constructs an Online Curriculum. It restructures problems by simplifying hard ones and diversifying medium-difficulty ones, enabling the model to act as its own teacher. Experiments on eight benchmarks show CLPO achieves state-of-the-art (SOTA) results, improving pass@1 by 6.96%, demonstrating its effectiveness in training reasoning models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and timely topic—leveraging reinforcement learning (RL) to enhance the reasoning capabilities of large language models (LLMs). RL plays a crucial role in improving LLMs, and this work contributes meaningfully to advancing RL-based training methods.\n2. The experimental evaluation is thorough, covering eight challenging mathematical and reasoning benchmarks. The results clearly demonstrate the effectiveness of the proposed method, with improvements over existing approaches.\n3. The authors provide code, which enhances reproducibility and facilitates further research, making the work accessible and impactful for the broader community."}, "weaknesses": {"value": "1. While the topic is significant, the idea of integrating curriculum learning into reinforcement learning is not entirely new. The paper's innovation is somewhat incremental, and it could benefit from a deeper discussion on how CLPO differs from and improves upon existing similar methods. Highlighting unique contributions more explicitly would strengthen the paper's originality.\n2. Although the authors provide code, its usability is limited due to poor readability and the lack of a basic README file or documentation to guide users on how to reproduce results or run experiments. This makes replication challenging and reduces accessibility. Providing well-documented and user-friendly code would greatly enhance the paper's impact and utility for the research community."}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xjfihbGe3y", "forum": "zfruJR2oxw", "replyto": "zfruJR2oxw", "signatures": ["ICLR.cc/2026/Conference/Submission9312/Reviewer_h1ux"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9312/Reviewer_h1ux"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971407419, "cdate": 1761971407419, "tmdate": 1762920946863, "mdate": 1762920946863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Curriculum-guided Learning for Policy Optimization, a reinforcement learning framework that integrates curriculum learning into RL with verifiable rewards to enhance reasoning in LLMs. Unlike conventional RLVR methods that uniformly sample problems regardless of difficulty, CLPO dynamically constructs an online curriculum by evaluating the model’s rollout performance. This curriculum drives an adaptive problem restructuring mechanism that diversifies medium-difficulty problems to promote generalization and simplifies overly hard ones for accessibility. Furthermore, a difficulty-aware policy optimization strategy introduces dynamic KL regularization to balance exploration and exploitation. Experiments show improvement over several baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Proposes CLPO, a novel integration of curriculum learning and RLVR, enabling dynamic, self-adaptive policy optimization.\n\n- Introduces an adaptive problem restructuring mechanism, where the model acts as its own teacher by adjusting problem difficulty.\n\n- Employs difficulty-aware policy optimization via dynamic KL regularization to effectively balance exploration and exploitation.\n\n- Demonstrates better results across eight reasoning benchmarks."}, "weaknesses": {"value": "- The evaluation dataset selection appears curated, focusing on benchmarks (e.g., AIME2024, GPQA Diamond) that may favor mathematical reasoning. It remains unclear why more challenging or recently adopted datasets such as AIME25, GPQA, ACPBench, or HeadQA were not included for broader validation.\n\n- Experiments are conducted only on the Qwen3-8B base model, limiting insights into scalability or robustness across different model sizes and architectures.\n\n- The dynamic KL regularization analysis (varying scaling factors α) lacks clear trends—results appear noisy or unstable, suggesting the method’s sensitivity to hyperparameter tuning.\n\n- The adaptive restructuring mechanism likely incurs non-trivial computational overhead, but the paper does not provide analysis on training efficiency or cost.\n\n- The discussion could be better contextualized with respect to related self-play, auto-curriculum, or competence-based RL literature to highlight conceptual novelty."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fhu2j6V6vQ", "forum": "zfruJR2oxw", "replyto": "zfruJR2oxw", "signatures": ["ICLR.cc/2026/Conference/Submission9312/Reviewer_s6og"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9312/Reviewer_s6og"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177960589, "cdate": 1762177960589, "tmdate": 1762920946608, "mdate": 1762920946608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}