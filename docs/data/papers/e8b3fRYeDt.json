{"id": "e8b3fRYeDt", "number": 7888, "cdate": 1758041078563, "mdate": 1759897824254, "content": {"title": "Reprogramming LLM Semantics: A Symbolic Attack Using Emoji-Based Context Mutation", "abstract": "In this paper, we introduce a novel black-box jailbreak method against\nLLMs based on symbolic role inversion and semantic redefinition, which we\ncall the Emoji Game. Our method establishes a fictional, non-executable\nemoji-based simulation wherein harmful requests are symbolically reinterpreted,\nbypassing the model’s internal safety filters. We empirically demonstrate\nthe effectiveness of our attack against state-of-the-art models, achieving\nnotably higher attack success rates compared to established jailbreak\nstrategies.", "tldr": "We introduce a novel jailbreak framework that reprograms black-box LLM semantics through structured symbolic abstraction", "keywords": ["Large Language Models", "Jailbreak Attacks", "Semantic Reprogramming", "Symbolic Abstraction", "Model Alignment and Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3e14d1d9917893ec3fc1d5adb4490c774781c24.pdf", "supplementary_material": "/attachment/332c80e8699cc8536ff8da8054b37041db7b42d7.zip"}, "replies": [{"content": {"summary": {"value": "This paper describes a text-to-text  _jailbreaking_ technique that uses an attacker LLM to transform a user prompt intended to elicit a harmful response so that it is less likely to be refused or deflected. Specifically, the technique instructs the attacker LLM to recast an instruction to the target LLM as an instruction to role-play as a player in a word game, substituting certain terms (nouns, verbs, URLs) by colon-enclosed emoji shortcodes. When the transformed prompts are not refused, the responses produced are not directly harmful but could be post-processed to attempt to reverse the transformation and recover an intelligible harmful response to the original prompt.\n\nThe authors evaluate the proposed _Emoji Game_ jailbreak technique on 100 prompts from the HarmBench dataset and compare the results against various black-box jailbreak techniques from the recent literature, measuring $\\text{ASR}\\_\\text{ref}$, the proportion of non-refusal responses, judged by the absence of certain keywords, and $\\text{ASR}\\_\\text{harm}$, the proportion of responses classified as harmful by an LLM-as-judge (gemma-3-12b-it)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The evaluation setup includes a good coverage of models: reasoning, non-reasoning, open-weights, proprietary, and models from different providers.\n- The evaluation of the refusal rate in responses (based on the occurrence of certain keywords) shows that responses to prompts using the proposed strategy are refused less often.\n- The evaluation results using an LLM-as-judge harmfulness classifier are marginally better on one of the models under evaluation."}, "weaknesses": {"value": "This paper proposes _Emoji Game_, a jailbreak technique that incorporates many ingredients from existing published or folklore attack strategies such as role-playing and encodings. When evaluated using an LLM-as-judge to measure the harmfulness of responses, it generally underperforms existing attacks (see Table 2) except on one of the models evaluated, gpt-oss-20b. As such, the contributions of the paper are iterative, limited to another variant of well documented and widely discussed jailbreak strategies, without advancing the understanding of the mechanisms why such strategies work, and without proposing any robust defense against them.\n\nI encourage the authors to read [Rando2025], which eloquently articulates the characteristics that an academic paper on jailbreak techniques should possess to genuinely contribute towards the community understanding of safety vulnerabilities in LLMs and their mitigations.\n\n[Rando2025] Javier Rando _Do not write that jailbreak paper_. In the Fourth Blogpost Track at ICLR 2025. Online: https://openreview.net/forum?id=TbN25IjHyC.\n\n- Evaluation done exclusively on single-turn scenarios.\n- Evaluation only includes black-box attack strategies as baselines, even on open-weight models or for techniques that are known to transfer to proprietary models like GCG.\n- Missing key details to allow reproducibility (e.g. the system prompt of the harmfulness classifier).\n- The paper often uses grandiose language without properly explaining its meaning (e.g. \"[O]ur Emoji Game defines a precise symbolic grammar and a stable abstraction mechanism, allowing systematic reprogramming of the model interpretive layer.\", \"[W]e redefine the model’s understanding of meaning itself.\", \"[T]his is the first work to introduce a jailbreak framework based on formally structured symbolic grammar constructed through recursive emoji abstraction.\")"}, "questions": {"value": "- What system prompt do you use to judge harmful responses using gemma-3-12b-it to report $\\text{ASR}_\\text{harm}$? Since you acknowledge on page 8 that \"evaluating harm in this context requires a dedicated judge model that is specifically trained to recognize symbolic patterns and semantic camouflage used in the attack.\", how can you be confident that gemma-3-12b-it is accurately judging the harmfulness of the responses your attack produces?\n\n- On page 6, you say \"Each strategy is executed at least 100 times per target model, though some run up to 300 trials depending on attack variability\". Without any more details, my understanding is that you try at least each attack strategy (each baseline and yours) at least once on each of the 100 HarmBench prompts that you selected, but that sometimes you try each attack more than once based on the sample *variance* you observe (up to 3 times per prompt?). If this is correct, you could be sometimes comparing pass@3 with pass@1 results. Can you clarify what you meant and why you did not just run each attack on each prompt consistently 3 times reporting the variance?\n\n- What system prompt do you use to post-process the responses of the target model using GPT-5? Did you try to evaluate the harmfulness of post-processed responses using the same method you use to evaluate the original responses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zTyKkhlFyC", "forum": "e8b3fRYeDt", "replyto": "e8b3fRYeDt", "signatures": ["ICLR.cc/2026/Conference/Submission7888/Reviewer_QHZ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7888/Reviewer_QHZ1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761065501331, "cdate": 1761065501331, "tmdate": 1762919921543, "mdate": 1762919921543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a jailbreaking method that encodes a malicious prompt as compositions of symbolic primitives in a fictional “Emoji” language, while explicitly instructing the LLM that these primitives are non-executable. The target model is then asked to produce its (malicious) response in this same language. An attacker LLM defines the language at runtime for each query, using three primitive types. On evaluation, the method generally outperforms baseline attacks on refusal-based attack success rate (ASR) metrics and is competitive on a HarmBench-derived ASR metric."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper is original in proposing a novel obfuscation strategy to encode malicious queries as primitives in a fictional game language. Based on the provided examples, the resulting model outputs are detailed and, when mapped back to natural language, unambiguously harmful - showing the attack’s practical potency. Empirically, the method generally outperforms or remains competitive with the baselines reported in the paper. The work is timely given increasing misuse of LLMs (e.g., misinformation)."}, "weaknesses": {"value": "- The approach presented in the paper is conceptually not very novel - similar methods have been proposed in papers such as [1][2][3]. Emoji Game can be construed as another instantiation of a Cipher attack, and does not use these other highly-related attacks as baselines.\n- The manuscript itself appears heavily AI-written, leading to degraded clarity - non-standard terminology, formatting issues (e.g., the citation on line 85), and redundancy (the last two paragraphs of Related Work convey essentially the same content).\n- While the paper contrasts itself with prior work in terms of methodology, it does not convincingly motivate why Emoji Game is needed as a distinct approach.\n- The work is outperformed by existing methods when evaluated using the more standard HarmBench-based metric. It is only more performant than existing methods when using the refusal-based ASR metric, which is a weaker metric in my opinion. The paper also does not mention whether this metric aligns with human safety preferences.\n- The paper does not benchmark itself against any standard defense procedures. In particular, since the output can be interpreted by an LLM to retrieve harmful content, I suspect that output filtering will prove to be quite effective against Emoji Game.\n- I feel that there is a lot of information missing in the paper, which I will enumerate in the “Questions” section below.\n\n[1] GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher. Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, Zhaopeng Tu. \n[2] ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs. Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran.\n[3] Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction. Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, Kai Chen."}, "questions": {"value": "- What is motivation for the specific refusal words chosen by the authors for the refusal-based ASR? Are they taken from prior work?\n- Are the primitives for the Emoji Game regenerated for each query? What is the token cost per attack?\n- What does ‘symbolic plausibility’ mean in line 255?\n- How are the ‘interpretations’ generated in line 263?\n- ‘Appendix 5’ isn’t in the paper in line 340.\n- The paragraph starting at line 322 is unclear - what is ‘trial size’?\n- The terminology used in the tables is not properly defined and explained.\n- The paper mentions improvements in multi-turn settings at multiple places - are there any experiments backing this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "566ROXQXuI", "forum": "e8b3fRYeDt", "replyto": "e8b3fRYeDt", "signatures": ["ICLR.cc/2026/Conference/Submission7888/Reviewer_YcTZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7888/Reviewer_YcTZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761588429675, "cdate": 1761588429675, "tmdate": 1762919920982, "mdate": 1762919920982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies jailbreak attacks that encode malicious instructions as emoji. The attacker converts prompts into an emoji language, performs Q&A with the model in that emoji environment, and then decodes the model’s emoji outputs back into harmful content. While the idea is interesting and the authors report high attack success rates, the manuscript reads more like an experimental report than a conference paper and, in my opinion, does not meet ICLR standards for acceptance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The idea is interesting and uncovers a new safety risk: emojis can be used as a covert channel to convey malicious instructions. The authors demonstrate the feasibility of this approach and report reasonably strong attack success rates."}, "weaknesses": {"value": "1. The writing is verbose and unfocused. The background, core idea, and experiments could be presented concisely — I estimate the content could be reduced to 4–6 pages rather than the current 9.\n\n2. Limited novelty. Conceptually, this work differs from prior work mainly in the choice of a different proxy “language” (emoji). Prior papers have used alternate encodings; switching to emojis feels like an incremental variation rather than a substantive advance.\n\n3. A major technical concern: I remain unconvinced that arbitrary text can reliably be encoded as emojis, or that all models handle emojis well. If either assumption fails — that is, if some texts cannot be faithfully represented as emoji sequences, or if target models poorly understand emoji — then the proposed attack would break. The paper does not sufficiently address this fragility across different models and inputs."}, "questions": {"value": "Refer to the proposed weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "l0fP5YKwcN", "forum": "e8b3fRYeDt", "replyto": "e8b3fRYeDt", "signatures": ["ICLR.cc/2026/Conference/Submission7888/Reviewer_fhPM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7888/Reviewer_fhPM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843695383, "cdate": 1761843695383, "tmdate": 1762919920484, "mdate": 1762919920484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}