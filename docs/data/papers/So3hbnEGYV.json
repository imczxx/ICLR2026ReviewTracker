{"id": "So3hbnEGYV", "number": 17461, "cdate": 1758276318153, "mdate": 1759897173772, "content": {"title": "Quantization Meets Reasoning: Exploring and Mitigating Degradation of Low-Bit LLMs in Mathematical Reasoning", "abstract": "Low-bit post-training quantization (PTQ) is a practical route to deploy reasoning-capable LLMs under tight memory and latency budgets, yet it can markedly impair mathematical reasoning (drops up to 69.81% in our harder settings). We address two deployment-critical questions with process-level precision: Where along a step-structured solution does degradation first arise? How to mitigate it while staying in the low-bit regime? Across widely used PTQ methods (AWQ, GPTQ, SmoothQuant), open-source model families (Qwen, LLaMA; 0.5--7B), and math reasoning benchmarks (GSM8K, MATH, AIME), we perform format-aligned chain-of-thought with step-aligned attribution and uncover two robust regularities: (i) PTQ disproportionately elevates method and execution errors relative to high-level conceptual mistakes; and (ii) failures emerge early, with the first vulnerable step flipping and cascading to the final answer. These regularities suggest a general intervention principle: restore local token-level margins exactly at the earliest failure frontier. We instantiate this principle as a lightweight measure → locate → restore loop that operates directly on the quantized model: detect the first faulty step, construct our \"Silver Bullet\" datasets, and apply small-scale supervised/preference tuning. In our settings, as few as 332 curated examples and 3--5 minutes of compute on a single GPU recover 4-bit weight math reasoning toward the full-precision baseline while preserving PTQ efficiency. Our framework is quantizer- and architecture-agnostic within the evaluated regimes, and turns low-bit degradation from a global accuracy problem into a local, reproducible process intervention.", "tldr": "", "keywords": ["Quantization", "Reasoning", "Low-bit Training", "Large Language Models"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a73acd970cff7186fa3e2ba0ef7bfe117736510.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper analyzes and mitigates performance drops due to quantization in math reasoning. The paper first determines which points in a reasoning chain suffer disproportionately from quantization, finding that quantization results in execution errors as well as in errors in method choice, and that these errors tend to occur early in the reasoning chain.  They then propose a method that involves locating errors and training the model to avoid these errors. The model is taught to follow a particular format to make step identification easier. The authors analyze model errors according to a taxonomy of math errors and use an LLM as judge protocol to automate annotation, voting across multiple models. The quantized model is improved by obtaining datasets of correct/incorrect solutions for finetuning and fine-tuning the model on this data using DPO. \n\nThe results indicate that their training helps models recover their performance while maintaining quantized efficiency, and that the training does not negatively impact performance on non-math data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Qualitative analysis which indicates which kinds of capabilities suffer more/less in math after quantization\n- Results show an improvement to quantization after training re-training according to the Silver Bullet method.\n- Performance is maintained on MMLU-Pro, indicating that math training doesn't destroy the model's other abilities"}, "weaknesses": {"value": "- Ablation results: From Table 2, it's not at all clear that the method is really better than choosing random steps. There are slight improvements on some columns but drops on others. This seems to indicate that, while training generally can improve downstream performance, there's no special gain due to the supervision according to error cases/classifications, and choosing random steps would result in very similar performance. This hurts claims like 4.5 (i). \n- Limited experimentation: the authors only look at 4-bit and not lower-bit quantization (2 or 3-bit). The intro sets the scope to smaller models (0.5-7B) but the same method could be demonstrated on 14 or 32B models. \n- Important details about finetuning missing. Appendix B does not provide any details on the format alignment training (linked on L211). My worry here is based on the line 199 saying that quantized and full-precision models are finetuned separately. This formatting data (while designed to teach formatting) also does include a lot of math information (regardless of what the objective stated in L195 is, at the end of the day if you train on math data you'll improve the model's math ability). Depending on when and how the data is used, I'm concerned the comparisons are not exactly fair. If the data were used to train a single model that is then quantized in different ways, I think that would be a fair comparison. But if the model is first quantized and then trained, it could be an unfair comparison depending on which method the qLora parameters were tuned for.\n- The majority voting pipeline has a number of design decisions that seem unvalidated. This is fine if they result in lower precision, but worrying if they result in low recall (i.e. things are agreed on which shouldn't be)\n- Writing/organization: the intro description of the method is fairly vague, it's not clear from the intro what \"silver bullet\" is or how it addresses the problem identified.  Table 1 is unclear (what does Van. mean?) and introduced several pages before it's discussed. Many \\paragraph headings are missing spaces before the text."}, "questions": {"value": "- The finding that earlier errors hurt performance more should be qualified: do these earlier errors hurt disproportionately? I.e. any error that happens earlier is likely to lead to more incorrect answers because of compounding errors, but does this have more of an impact on quantized models? Are non-quantized models able to better recover from early errors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AL3t3tIgRa", "forum": "So3hbnEGYV", "replyto": "So3hbnEGYV", "signatures": ["ICLR.cc/2026/Conference/Submission17461/Reviewer_bsjF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17461/Reviewer_bsjF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761598815958, "cdate": 1761598815958, "tmdate": 1762927346163, "mdate": 1762927346163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript focuses on analyzing the errors induced by LLM quantization for math reasoning. To this end, the authors design and conduct a fine-grained error analysis to study the error modes induced by quantization and propose a training method to mitigate such errors. The experiments provide an in-depth analysis of these errors and demonstrate the effectiveness of the proposed training method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The studied problem, the performance degradation from quantization in math reasoning, is important. The finding that such degradations are non-trivial, especially for smaller LLMs, is interesting.\n\n2. A fine-grained error analysis is designed and conducted, which sheds light on why and how quantization can lead to performance degradation. It shows that quantization predominantly impairs the model’s ability to perform procedural operations and arithmetic execution.\n\n3. The proposed training method can effectively mitigate the performance degradation from quantization, as validated by empirical results."}, "weaknesses": {"value": "1. It would be helpful to present the (potential) performance degradation from quantization on tasks other than math reasoning (apart from MMLU), to provide a comparison regarding whether and how much larger performance degradation is observed on math reasoning.\n\n2. The paper only experimented with models of sizes up to 7B parameters. Stronger and more larger models should be included to strengthen the comprehensiveness of the empirical evaluations of the proposed methods."}, "questions": {"value": "1. Have you evaluated larger LLMs (more than 7B) regarding the degradation from quantization? It might be interesting to observe whether the degradation is even smaller.\n\n2. The MATH dataset has annotations of question difficulty and subject. It might be interesting to further analyze the correlation between degradation from quantization and the question difficulty or subject using these annotations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LmOAAofrC2", "forum": "So3hbnEGYV", "replyto": "So3hbnEGYV", "signatures": ["ICLR.cc/2026/Conference/Submission17461/Reviewer_fcs2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17461/Reviewer_fcs2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799897277, "cdate": 1761799897277, "tmdate": 1762927345276, "mdate": 1762927345276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic investigation into the degradation of mathematical reasoning abilities in low-bit LLMs due to post-training quantization. The authors propose a \"measure-locate-restore\" loop, which involves a step-aligned error analysis to identify early-stage failures and then uses targeted \"Silver Bullet\" datasets with DPO to fine-tune quantized models. The study demonstrates that this approach can effectively restore mathematical reasoning performance to near full-precision baselines with minimal computational resources, while preserving the efficiency benefits of low-bit models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the highly relevant challenge of deploying powerful LLMs in resource-constrained environments by making them efficient through quantization, while simultaneously preserving their reasoning capabilities.\n2. The paper widely used PTQ methods (AWQ, GPTQ, SmoothQuant) and mathematical benchmarks (GSM8K, MATH, AIME).\n3. The paper shows that significant recovery of mathematical reasoning accuracy can be achieved with as few as 332 curated examples and 3-5 minutes of GPU compute, which is a compelling result for practical deployment."}, "weaknesses": {"value": "1. While the paper claims its framework is quantizer- and architecture-agnostic and applicable to \"broader domains,\" the experiments are exclusively focused on mathematical reasoning tasks and a specific set of LLM families and PTQ methods. Further evidence would strengthen the broader generalizability claim.\n2. The paper studies a limited variety of models, and for mathematical reasoning tasks, long CoT models are more mainstream but were not investigated. Furthermore, there was no analysis of larger models and moe models."}, "questions": {"value": "1. In your method, are the LoRA weights also quantized in the same way during inference?\n2. The study primarily focuses on mathematical reasoning. What specific modifications or considerations would be necessary to effectively apply the \"measure-locate-restore\" loop and \"Silver Bullet\" concept to other complex reasoning domains, such as logical inference or code generation?\n3. The authors need to further validate the effectiveness of their method on mainstream thinking models (Qwen3), moe models (Mistral/Qwen3), and larger models (>=32B)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UJvnnzRSjW", "forum": "So3hbnEGYV", "replyto": "So3hbnEGYV", "signatures": ["ICLR.cc/2026/Conference/Submission17461/Reviewer_51Nr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17461/Reviewer_51Nr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968747873, "cdate": 1761968747873, "tmdate": 1762927344402, "mdate": 1762927344402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic investigation into the degradation of mathematical reasoning in low-bit quantized LLMs. The authors introduce a \"measure-locate-restore\" pipeline to first diagnose and then mitigate this performance loss. Through an error analysis across various models, quantization methods, and benchmarks, they find that post-training quantization predominantly introduces early-stage method and execution errors rather than high-level conceptual mistakes. Based on this diagnosis, they propose a lightweight mitigation strategy. They automatically construct compact, targeted \"Silver Bullet\" datasets of failure cases and use DPO to fine-tune the quantized models. Their experiments demonstrate that this approach can recover reasoning performance to near full-precision levels with minimal data (as few as 332 examples) and compute (3-5 minutes on a single GPU), without compromising the efficiency benefits of quantization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The work addresses a highly practical problem. The deployment of LLMs on resource-constrained hardware is a significant bottleneck, and understanding and rectifying performance degradation from essential compression techniques like quantization is of great importance to the field.\n3. A  key contribution is the detailed, step-aligned error analysis. The finding that quantization disproportionately increase method errors and execution errors, and that these errors manifest early in the reasoning chain.\n4. The proposed method demonstrates ability to restore performance with a very small number of examples of fine-tuning."}, "weaknesses": {"value": "1. The definitions for the four high-level error types (Conceptual, Method, Execution, Reasoning) appear to have some overlap, which could lead to subjective classification. For example, misusing a formula in an unsuitable context could be interpreted as a Conceptual Error (misunderstanding the problem's constraints) or a Method Error (choosing an inappropriate method)."}, "questions": {"value": "1. In your discussion of \"Why Reasoning Errors seem to vanish\" (line 412), you explain that earlier procedural errors mask subsequent logical flaws. Does this imply that after your restoration process fixes these initial errors, \"Reasoning Errors\" might re-emerge as the next performance bottleneck in the now-stronger models?\n2. The paper highlights that quantization primarily increases \"Method\" and \"Execution\" errors. Your ablation study (Table 2) shows that fine-tuning on Conceptual Error also yields the very strong results. This seems slightly counter-intuitive; Do you have an intuition for why this is the case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yeJkRX6QrW", "forum": "So3hbnEGYV", "replyto": "So3hbnEGYV", "signatures": ["ICLR.cc/2026/Conference/Submission17461/Reviewer_xpx2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17461/Reviewer_xpx2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762273664470, "cdate": 1762273664470, "tmdate": 1762927343490, "mdate": 1762927343490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}