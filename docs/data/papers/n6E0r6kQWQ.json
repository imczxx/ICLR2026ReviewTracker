{"id": "n6E0r6kQWQ", "number": 1626, "cdate": 1756898720413, "mdate": 1763693215978, "content": {"title": "SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning", "abstract": "Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet optimally integrating Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains a fundamental challenge. Through a comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from an entropy-based perspective, we reveal key differences between these paradigms: SFT induces coarse-grained, global shifts to policy distributions, while RL performs fine-grained, selective optimizations.\nOur analysis further establishes entropy as a critical indicator of training efficacy. \nBuilding on these observations, we introduce **S**upervised **R**einforcement **F**ine-**T**uning (**SRFT**), a single-stage framework that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. \nSRFT simultaneously applies SFT and RL to directly optimize LLMs using demonstrations and self-exploration rollouts rather than through two-stage sequential methods.\nExtensive experiments show that SRFT outperforms zero-RL baselines by **9.0%** on five mathematical reasoning benchmarks and by **10.9%** on three out-of-distribution benchmarks.\nMoreover, by leveraging demonstration data, SRFT maintains a more stable policy entropy, facilitating sustained policy improvement.", "tldr": "We propose SRFT, an entropy-aware single-stage framework that unifies SFT and RL for LLM reasoning.", "keywords": ["LLM Reasoning", "Reinforcement Learning", "Supervised Reinforcement Fine-Tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c8736f1f9b30e79b437da9f1fa34b723bdd238f4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SRFT, a single-stage framework that unifies SFT and RL via an entropy-aware weighting scheme: SFT provides coarse global shifts while RL delivers fine selective refinements, coordinated by policy entropy; SRFT applies both SFT and RL on demonstrations, decomposes the RL loss on rollouts into positive/negative parts; on Qwen2.5-Math-7B across iid and ood benchmarks, SRFT improves over zero-RL baselines by 9.0% and 10.9% and maintains more stable policy entropy during training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Grounded in an entropy-based view of SFT–RL integration, the paper frames SFT as a coarse, global shift and RL as a fine, selective refinement, then unifies them in a single-stage training scheme that balances both with entropy-aware weighting. \n2. Methodologically clean and well-motivated: on demonstrations, SRFT applies SFT and RL jointly with an entropy-decayed SFT weight; on rollouts, it decomposes the RL loss into positive/negative components; the three parts are combined into a single, explicit loss. \n3. The evaluation is comprehensive: based on Qwen2.5-Math-7B, SRFT is tested on five competition-level math benchmarks and three OOD sets, using standard metrics (avg@32 / pass@1). Table 2 shows that SRFT achieves the best in-distribution and OOD averages. \n4. Training dynamics further support the design: SRFT maintains more stable policy entropy and smoother reward/length curves than RL-only training, consistent with the paper’s motivation."}, "weaknesses": {"value": "1. Under-justified entropy weights. The entropy-aware coefficients are hard-coded to $w_{\\text{SFT}}=0.5$ and $w_{\\text{RL}}=0.1$. The paper notes they “can be fine-tuned” but offers no rationale or analysis for these choices. \n2. Unclear compute parity in training-dynamics comparisons. SRFT optimizes three losses—demo-SFT, demo-RL, and self-rollout RL—over two data streams, trained for 500 steps with 8 rollouts per prompt. Figure 6 compares reward, response length, and entropy to RL-only, but it is not stated whether tokens, FLOPs/GPU-hours, or memory were matched, making it hard to attribute gains to the algorithm rather than extra compute."}, "questions": {"value": "1. Justification and robustness of entropy coefficients. What motivated $w_{\\text{SFT}}=0.5$ and $w_{\\text{RL}}=0.1$ beyond heuristic tuning? Please include sensitivity analyses spanning orders of magnitude, or consider replacing fixed constants with a learned/monotone mapping from entropy to weights.\n2. Balancing the three objectives. Since $L_{\\text{SRFT}} = L_{\\text{SFT}}^{\\text{demo}} + L_{\\text{RL}}^{\\text{demo}} + L_{\\text{RL}}^{\\text{self-rollout}}$, have you tried introducing explicit coefficients (e.g., $\\alpha$,$\\beta$) and ablating their ratios to clarify relative contributions?\n3. Interpreting the response-length gap in Figure 6. RL-only produces notably shorter outputs. Is this primarily because RL-only lacks a demonstration warm start while SRFT leverages expert traces (e.g., DeepSeek-R1)? If RL-only were initialized on the same demonstrations, would the reward and length curves approach SRFT?\n4. Compute accounting and scalability. Please report compute and memory for SRFT vs. RL-only under the stated setup (500 steps, 8 rollouts/prompt): total and per-step FLOPs, peak memory, effective batch sizes, sequence lengths, and tokens processed. This would clarify fairness and inform scalability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KD7b2rCelc", "forum": "n6E0r6kQWQ", "replyto": "n6E0r6kQWQ", "signatures": ["ICLR.cc/2026/Conference/Submission1626/Reviewer_KnF6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1626/Reviewer_KnF6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761471808002, "cdate": 1761471808002, "tmdate": 1762915836132, "mdate": 1762915836132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SRFT, a single-stage fine-tuning paradigm that combines supervised fine-tuning (SFT) and Reinforcement Learning (RL) through an entropy-aware weighting mechanism. Instead of applying SFT and RL sequentially, SRFT jointly optimizes both objectives. The approach introduces a combined Loss function that integrates SFT, off-policy RL and on-policy RL components. The authors analyse the resulting learning dynamic and compare singe-stage versus two-stage training. The experiments are based on Qwen2.5-Math-7B, with ablations on two additional Qwen variants and show that SRFT outperforms standard SFT->RL pipelines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Evaluation methodology is thoroughly reported and appears to follow best practices (e.g. multi-seed evaluation on high-variance benchmarks, ablation across prompt variants) which increases confidence in the robustness of the results.\n- Evaluation on both in-distribution and out-of-distribution benchmarks is good.\n- The authors include a thorough analysis of learning dynamics.\n- Multiple ablations are presented, including on entropy weighting factors and training on different Qwen-based model variants\n- Figures and Tables are well made\n- Motivation is clear"}, "weaknesses": {"value": "- The method is only tested on the Qwen model family.\n- The claims in section 3.1 seem to lack quantitative validation. (It is the result of one training run per category on Qwen2.5-Math-7B)"}, "questions": {"value": "- **Q1:** Training stability\n   - (a) Reinforcement learning on smaller models (1.5B/7B) is often brittle and sensitive to hyperparameters. Could the authors comment on the training stability of SRFT compared to standard RL (e.g., GRPO)?\n   - (b) Introducing additional loss components can sometimes destabilize optimization. How does SRFT’s stability compare to pure SFT training in practice?\n- **Q2:** It is mentioned that math-verify is used for dataset filtering. Is it also used for the answer matching in the evaluation? If not, could the authors clarify which evaluation framework or procedure is used?\n- **Q3:** Generalization across models\n   - (a) Does SRFT also improve performance when starting from an RL tuned model (like Qwen2.5-Math-7B-Instruct)?\n   - (b) Were other model architectures tested (e.g. Gemma, LLaMa) to asses the whether SRFT benefits generalize beyond the Qwen family?\n- **Q4:** How does the computational cost of SRFT compare to the standard SFT->RL pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sxn8Frc4Fl", "forum": "n6E0r6kQWQ", "replyto": "n6E0r6kQWQ", "signatures": ["ICLR.cc/2026/Conference/Submission1626/Reviewer_5rC4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1626/Reviewer_5rC4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827626701, "cdate": 1761827626701, "tmdate": 1762915835976, "mdate": 1762915835976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts an analysis on the learning dynamics (as evidenced by induced token probability distributions, entropy-based analysis and training dynamics) of SFT and RL approaches for fine-tuning LLMs for improved reasoning. The paper shows that SFT induces coarse-grained and large changes in the base policy distribution of the LM whereas RL induces finer-grained and localized policy changes. Using these observations, the paper introduces a joint single-stage method called SRFT that couples three different loss objectives including self-rollouts and standard SFT + RL. Results showcase that the proposed method improves results across several benchmarks like AIME'24, AMC'23 and even OOD benchmarks like GPQA-D and MMLU-Pro."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and easy to follow.\n- The analyses conducted are presented in a compact form making it easily digestible.\n- The motivation to introduce the SRFT method is well-grounded based on the empirical analyses conducted in the paper."}, "weaknesses": {"value": "- Currently in line 160, there is a claim that states that the fig 2a results reveal a fundamental difference between SFT and RL regarding their reshaped probability distributions. Could there be some details provided regarding how exactly the heat-map is computed. Currently, it is unclear and a lot of details are omitted.\n    - Is the token output sequence generated by the base model or the SFT/RLd model? Further, are the log-probs simply computed token-wise with the base and the SFT/RLd model? The appendix mentions teacher forcing, but it is unclear what that means in this context, is the base model the teacher whose rollouts are used?\n    - This experiment is also only done with the Qwen model series, since recent works have shown that Qwen models might have some unintended biases (https://arxiv.org/abs/2506.10947, https://arxiv.org/pdf/2507.10532), it would be great to also show the result on non-Qwen models.\n    - It would be interesting to further establish this trend by plotting KL-divergence / JSD plots wrt to the base and SFT/RL models.\n    - Further, this heat-map is for one particular problem instance. It would be good to show some more examples, even though fig 2b is a more a quantitative treatment of the same.\n- The numbers shown in tab 1 don’t seem fully accurate for Qwen-2.5-Math-7B, AMC and Minerva. Reference the Sober reasoning paper (https://arxiv.org/pdf/2504.07086). What hyper parameters were used for evaluation? Would the same takeaways hold If the \"optimal\" evaluation hyper parameters were used as suggested in the sober reasoning paper?\n- In lines 202-203, it is stated: “This result suggests that SFT may overfit to demonstrations, and the subsequent RL attempts to correct this deviation, thereby increasing the learning tax of the two-stage paradigm.” Is this result globally true or does this depend on the number of epochs of SFT training (currently the paper seems to be doing 3 epochs). It would be interesting to ensure that this result globally holds true regardless of the number of optimization steps (since this is important for final performance as noted in OpenThoughts (https://arxiv.org/abs/2506.04178))\n- Is the SRFT method better than simple on-policy distillation (as shown in methods like GKD: https://arxiv.org/abs/2306.13649). For a fair comparison in the current setting, the teacher used for on-policy distillation should be DeepSeek-R1.\n- All experiments are only conducted with Qwen models. As mentioned earlier in the review, prior works have shown biases and data contamination in Qwen models with regards to the benchmarks used for testing. Would the results of SRFT also hold for non-Qwen models?\n- Why do the performance numbers between tab 1 and tab 2 differ for qwen2.5-math-7B, shouldn’t they be the same numbers?\n- There should be an ablation with different weighting factors for each of the loss terms in eq 12. For example, it might be plausible that only the self-rollout RL loss term might be the most beneficial (similar to an on-policy distillation loss), hence a leave-one-out ablation experiment (where each time the training is done only with two loss terms in the mix is used) should be conducted for more conclusive results.\n- Do the results also improve on AIME’25? This is an important test of generalisation as shown in Sober reasoning paper."}, "questions": {"value": "All my questions are also raised in the weakness section so that its easy for the authors to answer all concerns / queries jointly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "NA"}}, "id": "TfcR6tw6RK", "forum": "n6E0r6kQWQ", "replyto": "n6E0r6kQWQ", "signatures": ["ICLR.cc/2026/Conference/Submission1626/Reviewer_svi3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1626/Reviewer_svi3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986757076, "cdate": 1761986757076, "tmdate": 1762915835850, "mdate": 1762915835850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SRFT (Supervised Reinforcement Fine-Tuning), a single-stage approach that unifies SFT and RL for LLM reasoning. The key idea is to balance demonstration distillation (SFT) and online exploration (RL) in one loop, using policy entropy as a dynamic indicator to weight the two objectives and avoid either overfitting to demos or premature entropy collapse. Conceptually, SFT makes coarse, global shifts in the policy, while RL makes fine, selective adjustments; combining them directly improves efficiency and stability. Empirically (Qwen-2.5-Math-7B), SRFT achieves improvement across five math-reasoning benchmarks over baseline methods like RL-only and SFT+RL. Training curves show faster reward growth, longer solutions, and stable entropy versus RL-only."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposal to unify SFT and RL into a single stage is interesting and enables finer control over the trade-off between the two.\n- The paper offers a comprehensive analysis of learning dynamics and the respective effects of SFT and RL for language models, improving our understanding of both paradigms.\n- The manuscript is clearly written and easy to follow."}, "weaknesses": {"value": "- The experiments are not yet fully convincing; more evidence is needed to demonstrate the effectiveness of the proposed SRFT method. In particular, the SFT data use DeepSeek-R1 responses (Line 360), which likely exceed the quality of the Qwen2.5-7B policy’s rollouts. Figure A2 suggests the model learns primarily from SFT—the SFT loss dominates—implying that distillation, rather than RL, drives most gains. For a fair comparison, both RL and SRFT should start from the same fine-tuned initialization, or the SFT data should be generated offline by the policy model itself to remove teacher-distillation effects.\n\n- Prior work (e.g., OpenAI-o1, DeepSeek-R1) has demonstrated the importance of RL scaling for boosting LLM capabilities. Always incorporating SFT during training may hinder RL scalability and waste compute; the paper should consider when SRFT scales favorably versus when pure RL is preferable or how SRFT affects the scalability of RL.\n\n- In the method part, the paper proposes various strategies, like entropy-guided reweighting and mixing different losses for SRFT. It would be better to provide detailed ablations to demonstrate the effectiveness of these designs."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "35FMmBS0PP", "forum": "n6E0r6kQWQ", "replyto": "n6E0r6kQWQ", "signatures": ["ICLR.cc/2026/Conference/Submission1626/Reviewer_bDfv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1626/Reviewer_bDfv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994323198, "cdate": 1761994323198, "tmdate": 1762915835667, "mdate": 1762915835667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}