{"id": "7CyRV4EIjs", "number": 20883, "cdate": 1758311331266, "mdate": 1759896953838, "content": {"title": "PAPER: Privacy-Preserving ResNet Models using Low-Degree Polynomial Approximations and Structural Optimizations on Leveled FHE", "abstract": "Recent work has made *non-interactive privacy-preserving inference* more practical by running deep Convolution Neural Network (CNN) with Fully Homomorphic Encryption (FHE). However, these methods remain limited by their reliance on *bootstrapping*, a costly FHE operation applied across multiple layers, severely slowing inference. They also depend on *high-degree polynomial approximations* of non-linear activations, which increase multiplicative depth and reduce accuracy by 2–5% compared to plaintext ReLU models. In this work, we focus on ResNets, a widely adopted benchmark architecture in privacy-preserving inference, and close the accuracy gap between their FHE-based non-interactive models and plaintext counterparts, while also achieving faster inference than existing methods. We use a *quadratic polynomial approximation* of ReLU, which achieves the theoretical minimum multiplicative depth for non-linear activations, along with a penalty-based training strategy. We further introduce *structural optimizations* such as node fusing, weight redistribution, and tower reuse. These optimizations reduce the required FHE levels in CNNs by nearly a factor of five compared to prior work, allowing us to *run ResNet models under leveled FHE without bootstrapping*. To further accelerate inference and recover accuracy typically lost with polynomial approximations, we introduce parameter clustering along with a joint strategy of data encoding layout and ensemble techniques. Experiments with ResNet-18, ResNet-20, and ResNet-32 on CIFAR-10 and CIFAR-100 show that our approach achieves up to $4\\times$ faster private inference than prior work with comparable accuracy to plaintext ReLU models.", "tldr": "", "keywords": ["Privacy-Preserving Machine Learning", "Leveled Fully Homomorphic Encryption", "Polynomial Approximations"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a1ee3b599e8d3553aff6437b499e5d77c8352b1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method for privacy-preserving inference of deep convolutional neural networks under Fully Homomorphic Encryption (FHE). The authors focus on the ResNet architecture and present the first method that enables large models like ResNet-18, ResNet-20, and ResNet-32 to run entirely under Leveled FHE (LFHE) without bootstrapping. Their key idea is to use a degree-2 polynomial approximation of ReLU combined with a penalty-based training strategy to stabilize training. Furthermore, they introduce three structural optimization techniques (node fusing, weight redistribution, and tower reuse) to reduce the multiplicative depth, as well as co-design techniques like parameter clustering and ensemble inference that utilize unused ciphertext slots. Experiments on CIFAR-10 and CIFAR-100 show that their accuracy is comparable to plaintext ReLU models, and their inference speed is up to 4x faster than existing FHE-based methods (like AutoFHE and MPCNN)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It is the first work to demonstrate deep ResNet models (ResNet-18/20/32) running on Leveled FHE *without* requiring any bootstrapping. This is a highly significant result, as bootstrapping is the single largest performance bottleneck in the field. The authors achieve this by systematically attacking the total multiplicative depth, starting with a novel training strategy (§2) that successfully stabilizes a degree-2 polynomial activation—achieving the theoretical minimum depth for that component.\n2. The minimal-depth activation is complemented by a suite of three novel structural optimizations (§3). The \"node fusing\" and \"weight redistribution\" are clever techniques for collapsing linear operations, but the \"Tower Reuse\" concept is particularly insightful, as it cleverly manipulates the CKKS scheme's parameters to reduce FHE levels even further.\n3. The co-design strategy presented in §4 is highly innovative. The idea of exploiting \"unused slots\" in the HW layout to pack an ensemble of models is an excellent example of turning a layout inefficiency into a powerful feature. It provides a near \"zero-cost\" mechanism (in terms of inference time) to claw back the accuracy lost from the low-degree polynomial approximation. The subsequent introduction of \"slice ensemble clustering\" to mitigate the new memory/encoding overhead created by this ensemble demonstrates a deep, holistic understanding of the FHE-ML trade-offs.\n4. All these claims are well-supported by both solid theoretical derivations and credible experimental validation."}, "weaknesses": {"value": "Please refer to my questions below."}, "questions": {"value": "Comment 1: Query regarding the Numerical Stability of Weight Redistribution (Sec. 3.2) under Fixed-Point Precision Constraints\n\nThe \"Weight Redistribution\" introduced in Section 3.2 is a very clever structural optimization, aiming to reduce the multiplicative depth of operations like polynomial activations to their theoretical minimum. This is significant for reducing the required FHE levels $L$.\n\nA core claim of the paper is that this technique can be applied \"while maintaining functional equivalence.\" Appendix E.2 provides detailed mathematical derivations proving this algebraic equivalence in the ideal domain of real numbers ($\\\\mathbb{R}$).\n\nHowever, my question concerns how this ideal algebraic equivalence holds up when applied within the finite fixed-point precision framework set by the paper (i.e., $b$-bit precision as described in Sec. 2.2 and specified as $b=10$ in Sec. 5.1).\n\nSpecifically, in the \"Update Forward\" $\\\\rightarrow$ \"Receivers\" part (Sec. E.2.1), the coefficient update rule for a polynomial receiver is given by $\\\\bar{c}\\i = c\\i \\\\upsilon^i$, where $\\\\upsilon$ is a coefficient from the \"donor\" layer (e.g., $\\\\upsilon = c\\d$).\n\nI have the following questions:\n\n1.  Regarding Exponential Coefficient Scaling: The update rule $\\\\bar{c}\\i = c\\i \\\\upsilon^i$ is exponential in nature. Could the authors clarify the impact of this? For example, if a \"donor\" has $\\\\upsilon = 1.5$ (a very reasonable value), the $c\\2$ term of a 2nd-degree polynomial \"receiver\" will become $\\\\bar{c}\\2 = c\\2 \\\\cdot (1.5)^2 = 2.25 \\\\cdot c\\2$, an amplification of 2.25x.\n    \n2.  Potential Conflict with Fixed-Point Precision: As shown in the example, in a deep network like ResNet, this redistribution operation may occur multiple times in sequence. If this operation, with $\\\\upsilon=1.5$, occurs just twice in a row, the same coefficient would grow by $2.25^2 \\\\approx 5.06$ times. Does this exponential amplification of coefficients (or exponential shrinking if $\\\\upsilon < 1.0$) not risk pushing coefficients to rapidly exceed the representable range of $b=10$ (causing Overflow), or fall below the precision of $b=10$ (causing Underflow or truncation to zero)?\n    \n3.  Guarantee of \"Functional Equivalence\": Given this potential risk of overflow or precision loss, have the authors conducted numerical range analyses or experiments to ensure that all redistributed coefficients ($\\\\bar{c}\\i$ and $\\\\bar{w}\\i$) remain strictly within the $b=10$ fixed-point precision range in all cases (or in the post-training models)?\n    \n\nCould the authors please clarify how \"functional equivalence\" is rigorously maintained in the practical, fixed-point FHE environment?\n\nComment 2: Query on the Generality of the \"Zero-Overhead\" Assumption for Ensemble Inference in Sec. 4.3\n\nThe authors propose a core co-design technique in Sec. 4.1 and 4.3: leveraging \"unused slots\" in the HW layout to pack $M$ ensemble models, thereby achieving ensemble inference with \"no extra computation or memory overhead.\" This is critical for recovering the accuracy lost to polynomial approximation.\n\nMy query concerns the generality of this \"zero-overhead\" assumption.\n\n1.  Dependence on Precondition: This technique appears to be entirely dependent on the precondition that the input data's spatial dimensions ($h \\\\times w$) are significantly smaller than the FHE ciphertext's slot capacity (typically $N/2$ in CKKS).\n    \n2.  Evidence in Paper: In the paper's experiments (Sec. 5.1, H.1), $N = 2^{15}$ (i.e., $N/2 = 16,384$ available slots), while the CIFAR-10 input is $h \\\\times w = 32 \\\\times 32 = 1024$. Clearly, $1024 \\\\ll 16,384$, which leaves 94% of \"unused slots\" available for packing.\n    \n3.  Limitation on Generality: The authors present this as a \"co-design technique\" in Sec. 4. However, for many non-\"toy\" computer vision tasks, it is common for the input size $h \\\\times w$ to approach or even exceed the slot capacity $N/2$. For example, on ImageNet (a standard benchmark), the input is $h \\\\times w = 224 \\\\times 224 = 50,176$. This size ($50,176$) is far larger than the $16,384$ available slots. In this scenario, not only are there no \"unused slots\" for packing, but even a single input channel cannot fit into one ciphertext. Does this mean the core advantage of this \"zero-cost\" ensemble fails completely in this standard scenario?\n    \n\nCould the authors please clarify the boundary conditions for this key technique more explicitly in the paper? Specifically, to acknowledge that its \"zero-overhead\" nature is highly contingent on the $h \\\\times w \\\\ll N/2$ condition.\n\nComment 3: Query on the Stability and Reproducibility of the Training Strategy (Sec 2.2)\n\nA key achievement of this paper is the ability to train deep ResNets with a simple degree-2 polynomial, achieving an accuracy drop of only 1.3% (Sec 5.2). This result relies heavily on the \"Activation Regularization\" strategy (Sec 2.2) to prevent \"escaping activations.\"\n\nMy query is regarding the stability and reproducibility of this training strategy.\n\n1.  A \"Three-Piece\" Strategy: At the end of Sec 2.2, the authors state that the main $\\\\zeta$-penalty (Eq. 1) alone may still lead to instability. They then introduce two \"additional strategies\": (i) clipping pre-activations during training and (ii) introducing a warm-up schedule for $\\\\zeta$. This suggests that the final stable result depends on a \"three-piece combination\" of these components.\n    \n2.  Lack of Ablation: This combination may be sensitive to tuning and potentially difficult to reproduce. However, the paper does not seem to provide an ablation study that isolates the importance of the two \"additional strategies.\"\n    \n\nTo help assess the robustness of this method and the ease of its reproducibility, could the authors please clarify:\n\n   Are all three components (the $\\\\zeta$-penalty, the hard-clipping, and the $\\\\zeta$-warm-up) necessary to achieve the reported stability and accuracy?\n   What happens to the training stability and final accuracy if the \"additional\" hard-clipping strategy is disabled?\n   Similarly, what happens if the $\\\\zeta$-warm-up schedule is disabled and the full penalty $\\\\zeta = 0.001$ is used from the start?\n\nUnderstanding the necessity of each component is crucial for evaluating the overall robustness of this novel training approach.\n\nComment 4: Query on the Storage Cost of \"Slice Ensemble Clustering\" (Sec 4.2 & 4.3)\n\nMy query concerns the storage overhead of the proposed clustering techniques, particularly the 'Slice Ensemble Clustering' that is necessary for the final high-accuracy results.\n\n1.  The Initial Claim: In Sec. 4.2, when \"Slice Clustering\" (for a single model) is introduced, the paper states that the \"additional storage from slice-specific codebooks is modest.\" This is a key claim.\n    \n2.  The Cost Multiplier: However, in Sec. 4.3, this method is extended to \"Slice Ensemble Clustering\" (SEC) to support $M$ models. As described, this changes each codebook entry from a scalar to an $M$-dimensional vector. This logically implies that the total storage cost for all codebooks is multiplied by a factor of $M$ (e.g., $M=2$ or $M=4$).\n    \n3.  The Contradiction in Appendix H.3: This multiplied cost seems to be far from \"modest.\" My concern is strongly supported by the authors' own statement in Appendix H.3:\n    \n    > \"We did not extend this configuration \\[Slice Clustering with M ≥ 2\\] to ResNet-32 due to system memory limitations.\"\n    \n4.  The Question: This suggests that the actual storage cost of the final, high-accuracy \"Slice Ensemble Clustering\" technique is not \"modest\" at all, but is, in fact, a critical bottleneck that prevents the method from scaling even from ResNet-20 to ResNet-32.\n    \n\nCould the authors please clarify this apparent contradiction? How should one reconcile the claim of \"modest\" overhead in Sec. 4.2 with the \"system memory limitations\" encountered for ResNet-32 in Appendix H.3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gC1j5d5DOT", "forum": "7CyRV4EIjs", "replyto": "7CyRV4EIjs", "signatures": ["ICLR.cc/2026/Conference/Submission20883/Reviewer_xdM9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20883/Reviewer_xdM9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934416231, "cdate": 1761934416231, "tmdate": 1763000003333, "mdate": 1763000003333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the slow inference of HE-based CNNs, which traditionally rely on costly bootstrapping due to high-degree polynomial approximations of ReLU. This work is the first to execute deep CNNs like ResNet-18, -20, and -32 entirely under leveled FHE (LFHE) without bootstrapping. To achieve this, the authors propose a quadratic (degree-2) polynomial approximation of ReLU stabilized by a novel penalty-based training strategy. This is combined with structural optimizations—node fusing, weight redistribution, and tower reuse—to minimize the required multiplicative depth. Furthermore, parameter clustering and an ensemble technique are jointly applied to accelerate inference and recover accuracy. Experiments on CIFAR-10/100 demonstrate that this approach achieves up to 4x faster inference while matching the accuracy of plaintext ReLU models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work addresses and resolves the fundamental challenges of polynomial approximation in FHE, namely coefficient truncation and escaping activations . The proposed **regularization with a clip-range penalty**, in particular, appears to be a generalizable technique applicable to the approximation of various functions in HE-based PPML, not just ReLU.\n- The **Tower Reuse** and **Parameter Clustering**  techniques seem broadly applicable beyond this specific paper and could aid in reducing levels and complexity in other PPML models.\n- It successfully implements relatively deep models (ResNet-20/32) using LFHE, achieving competitive performance in terms of both **accuracy and inference time**."}, "weaknesses": {"value": "- The paper presents low-degree polynomial approximations and structural optimizations as its primary contributions. However, the polynomial approximation method itself directly adopts the 'quantization-aware polynomial fitting' from PILLAR (Diaa et al., 2024). Furthermore, the **training strategy** shares similarities with PILLAR, as both pursue the same objective (stabilizing activations) and utilize a clip function during training. Additionally, idea of introducing a regularization term to constrain the range of intermediate values—while effective—is a well-established concept, particularly in fields like model quantization (e.g., [1]), which learns clipping parameters via regularization). This existing body of work weakens the novelty claim of the training strategy.\n- The \"novel structural optimizations\" of **Node Fusing** and **Weight Redistribution** might be considered minor or standard techniques in FHE-based PPML. For example, prior works like MPCNN have presented the fusing of Convolution and Batch Normalization (ConvBN), and AutoFHE also presents techniques for fusing consecutive operations (such as two convolutions) into a single one, treating them as implementation details rather than major, standalone contributions. Thus, these optimizations could be viewed more as clear specifications of implementation rather than truly novel techniques.\n\n[1] Choi, Jungwook, et al. \"Pact: Parameterized clipping activation for quantized neural networks.\" *arXiv preprint arXiv:1805.06085* (2018)."}, "questions": {"value": "- The \"Tower Reuse\" technique, which introduces sublevels to decouple the RNS modulus size from the scaling factor, bears a strong conceptual resemblance to prior work, notably **\"**Grafting: Decoupled Scale Factors and Modulus in RNS-CKKS\" [2]. What are the specific technical advantages or novel contributions that differentiate the proposed Tower Reuse method from the established Grafting technique?\n- The paper posits that pre-encoding all weight combinations leads to \"prohibitive memory usage\" and \"on-demand\" encoding causes \"heavy computational overhead\". This is a critical premise for the utility of parameter clustering. However, is this overhead truly prohibitive? Is it not feasible to store pre-encoded plaintexts organized by their required level and layout? The paper states this is a common issue, yet the claim of \"heavy\" overhead lacks external citation (e.g., from a survey paper) to quantify this bottleneck.\n\n[2] Cheon, Jung Hee, et al. \"Grafting: Decoupled Scale Factors and Modulus in RNS-CKKS.\" *Cryptology ePrint Archive* (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nde9Rs6jZk", "forum": "7CyRV4EIjs", "replyto": "7CyRV4EIjs", "signatures": ["ICLR.cc/2026/Conference/Submission20883/Reviewer_fn4b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20883/Reviewer_fn4b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961231817, "cdate": 1761961231817, "tmdate": 1763000003138, "mdate": 1763000003138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a series of technical optimizations for privacy-preserving ResNet inference under leveled FHE, successfully eliminating bootstrapping and achieving up to 4× speedups on CIFAR-10/100 datasets. While the individual techniques---including low-degree polynomial activations, node fusing, weight redistribution, and parameter clustering---are well-executed and mathematically sound, the work suffers from significant limitations that undermine its broader impact. The evaluation is restricted to outdated CNN architectures (ResNet-18/20/32) on toy datasets, with no discussion of whether these methods generalize to modern architectures like Transformers or Vision Transformers, which dominate current research. More critically, the lack of bootstrapping is not a feature but rather a symptom: it indicates the approach fundamentally relies on bounded multiplicative depth, which severely constrains model scale and type, making it unsuitable for contemporary deep learning. The paper essentially optimizes within a narrow, legacy setting without addressing whether the proposed techniques can be repurposed for the increasingly diverse and complex models that the PPML community actually needs to deploy. Unless the authors provide compelling evidence that these methods extend meaningfully to Transformers or other modern architectures, or propose a fundamentally new framework rather than incremental optimization of ResNets, the work remains a well-polished but ultimately narrow contribution to a shifting landscape."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is technically solid and well-executed, successfully combining multiple complementary optimization strategies (low-degree polynomials, structural fusing, weight redistribution, and clever clustering) to achieve impressive quantitative gains—eliminating bootstrapping, reducing RNS levels by ~5×, and delivering 4× speedups on ResNets without sacrificing accuracy. The theoretical contributions are rigorous with proper mathematical proofs, and the ablation studies are thorough, making this a valuable reference for anyone working on FHE-based neural network inference within the specific constraints of leveled homomorphic encryption."}, "weaknesses": {"value": "The fundamental limitation is that the absence of bootstrapping is not an achievement but rather a constraint—the method is fundamentally bounded by fixed multiplicative depth, which severely restricts the scale and type of models that can be supported, and there is absolutely no evidence or discussion that these techniques generalize beyond ResNets to modern architectures like Transformers or ViTs. The experimental evaluation is confined to outdated CNN architectures on toy datasets (CIFAR-10/100), which represent a narrow slice of what contemporary deep learning actually looks like, and the authors do not even acknowledge this limitation as a concern. The paper essentially performs impressive local optimization within a legacy setting without proposing any principled path forward for the diverse, evolving landscape of modern models—making it a well-crafted but ultimately myopic contribution to a field that has already moved well beyond ResNets."}, "questions": {"value": "Can this approach actually scale to ImageNet-level datasets with the current setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o1fF7lyTti", "forum": "7CyRV4EIjs", "replyto": "7CyRV4EIjs", "signatures": ["ICLR.cc/2026/Conference/Submission20883/Reviewer_P459"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20883/Reviewer_P459"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995783322, "cdate": 1761995783322, "tmdate": 1762937478896, "mdate": 1762937478896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a technique for optimizing a ResNet model to enable secure AI inference under leveled homomorphic encryption (HE) without requiring bootstrapping. In particular, the authors design a training strategy that allows the ReLU activation function to be replaced with a quadratic polynomial, and they effectively apply an ensemble method to maintain high accuracy on the CIFAR-10 and CIFAR-100 datasets while keeping the activation function as a low-degree polynomial."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "They reduce the multiplicative depth by combining layers, enabling the implementation of ResNet under leveled homomorphic encryption (LHE). In addition, they introduce a slice ensemble clustering method, which leverages unused SIMD slots to perform parallel computations across multiple models simultaneously, thereby improving training stability.\n\nThis is a clear and meaningful contribution, as implementing a full architecture under LHE is by no means an easy task. Moreover, simplifying the model while employing an ensemble-based approach is a valid and effective strategy that could generalize well to other datasets and models—especially since LHE/FHE schemes such as CKKS support SIMD operations, making ensemble-based methods particularly advantageous.\n\nFrom a machine learning perspective, demonstrating that good performance can still be achieved using a quadratic activation function is significant. The introduction of a novel training method to make this possible is also intriguing. If the proposed approach is validated further, it could provide valuable insights for the privacy-preserving machine learning (PPML) community."}, "weaknesses": {"value": "Implementing ResNet under LHE is certainly a remarkable achievement. However, it is difficult to assess how well the proposed method would generalize to models targeting larger datasets, as the expressiveness of the architecture may be reduced once layers are merged. In this regard, it would be interesting to see how the approach performs on ImageNet. Of course, implementing ImageNet-scale models under LHE is known to be extremely challenging. Nonetheless, even if a fully bootstrapped FHE model or a plaintext simulation were used, it would be valuable to experimentally demonstrate that replacing the activation function with a quadratic function leads to meaningful performance improvements.\n\nWhen applying the ensemble technique to more complex architectures, the inherent limitations of LHE imply that the ensemble complexity must increase to achieve deeper expressiveness. This, in turn, could reduce efficiency if the number of models exceeds the maximum SIMD slot capacity, as additional inference steps would be required. For instance, it would be helpful to discuss how the proposed ensemble approach might behave on ImageNet-scale tasks—not necessarily by performing an actual homomorphic implementation, but at least by providing a conceptual or experimental discussion of its scalability and expected runtime performance trends.\n\nIt seems that the discussion of tower reuse in the paper is based on outdated techniques. The claim that ‘accuracy degrades as the gap between RNS primes and the scaling factor increases’ only applies when the scaling factor is fixed and maintained at its initial value throughout the computation. However, this issue has been fully resolved in the following paper. Please refer to it. It would be advisable to rewrite the section on tower reuse to reflect the implications of this newer method.\n\nKim, Andrey, Antonis Papadimitriou, and Yuriy Polyakov. \"Approximate homomorphic encryption with reduced approximation error.\" Cryptographers’ Track at the RSA Conference. Cham: Springer International Publishing, 2022.\n\nMoreover, the statement that the security level is proportional to N/Q appears to be inaccurate. Security in homomorphic encryption is not determined by such a simple proportional relationship. For example, please refer to the following paper for a more accurate treatment of this issue. It would also be beneficial to consult a wider range of recent papers on homomorphic encryption to ensure the discussion reflects the current understanding in the field.\n\nKirshanova, Elena, Chiara Marcolla, and Sergi Rovira. \"Guidance for efficient selection of secure parameters for fully homomorphic encryption.\" International Conference on Cryptology in Africa. Cham: Springer Nature Switzerland, 2024."}, "questions": {"value": "1. Even at the plaintext level, could you verify whether the quadratic activation function can effectively replace the original one when applied to ImageNet?\n2. Could you discuss the potential advantages and disadvantages of applying the ensemble method to large-scale datasets such as ImageNet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Slt9ZzhpAk", "forum": "7CyRV4EIjs", "replyto": "7CyRV4EIjs", "signatures": ["ICLR.cc/2026/Conference/Submission20883/Reviewer_gvwz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20883/Reviewer_gvwz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762414300114, "cdate": 1762414300114, "tmdate": 1762937478025, "mdate": 1762937478025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}