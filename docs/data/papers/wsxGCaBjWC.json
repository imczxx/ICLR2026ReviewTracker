{"id": "wsxGCaBjWC", "number": 20680, "cdate": 1758308927820, "mdate": 1759896964342, "content": {"title": "Gradient Descent with Large Step Sizes: Chaos and Fractal Convergence Region", "abstract": "We examine gradient descent in matrix factorization and show that under large step sizes the parameter space develops a fractal structure. We derive the exact critical step size for convergence in scalar-vector factorization and show that near criticality the selected minimizer depends sensitively on the initialization. Moreover, we show that adding regularization amplifies this sensitivity, generating a fractal boundary between initializations that converge and those that diverge. The analysis extends to general matrix factorization with orthogonal initialization. Our findings reveal that near-critical step sizes induce a chaotic regime of gradient descent where the long-term dynamics are unpredictable and there are no simple implicit biases, such as towards balancedness, minimum norm, or flatness.", "tldr": "Gradient descent with near-critical step sizes enters a chaotic regime, characterized by sensitivity to initialization, fractal convergence regions, and absence of simple implicit biases.", "keywords": ["large step size", "gradient descent", "matrix factorization", "convergence", "implicit bias", "chaos", "fractal basin boundary"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a0578c3467f1d76fc38815dcbcb1807b31dc598.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors theoretically examine the dynamics of full-batch gradient descent (GD) with large step-sizes on a shallow matrix factorization problem\n\n$$ 1/2 \\Vert UV^T - Y \\Vert_F^2 $$\n\nIn contrast to previous work, they particularly focus on critical learning rates, i.e., learning rates for which GD converges just about and exhibits chaotic behavior. Their main results are as follows:\n\nFor the simplified setting of scalar factorization (the matrix $Y$ is a scalar $y$ which is decomposed into the inner product of two vectors $u,v$) and fixed learning rate $\\eta$, Theorem 1\n\n- characterizes the region of initializations $D_\\eta$ for which GD converges, \n- shows that around any point on the boundary of D one finds initializations such that GD converges to arbitrarily large/unbalanced solutions or to the saddle $(u,v) = (0,0)$, and\n- lower bounds the topological entropy of the dynamical system defined by GD characterizing it to be chaotic\n\nIn the setting of Theorem 1, but with additional $\\ell_2$-regularization of $u$ and $v$, and fixed learning rate $\\eta$, Theorem 3 shows that \n\n- the outer boundary of $D_\\eta$ has fractal structure,\n- for $y = 0$, the set of initializations converging to global optimality is unbounded (although the set of global minimizers is bounded), and\n- on the boundary of $D_\\eta$ the limit is highly sensitive to the choice of initialization.\n\nFinally, in Proposition 5 they show that Theorem 1 and 3 can be lifted to matrix valued $y$ by restricting to initializations in a particular slice of the high dimensional space.\n\nEmpirical simulations support their results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Examining this extreme region of initialization/learning rate leads to beautiful connections between GD and fractals/chaotic systems"}, "weaknesses": {"value": "- The rigorous analysis is strongly restricted to shallow scalar factorization (Proposition 5 only extends the insights to a null set of the matrix factorization space)\n- The results are only relevant for a (quasi-)null set of those initializations for which GD converges"}, "questions": {"value": "On the one hand, I think it is an interesting line of study to understand the connection of GD with large learning rates and chaotic systems. On the other hand, the results of the paper are strongly limited to scalar factorization and even in this case, most of the effects can only be observed on a (quasi)-null set of the successful initializations which is apparently confirmed by parts of the discussion (see points below). Furthermore, parts of the results raise doubts on whether topological entropy is the right measure to identify chaotic behavior (see points below). Consequently, I lean towards not accepting the submission. Due to time constraints, I could not check the proofs in detail.\n\nThe following points might be considered to improve the draft:\n\n- l. 092: „the map GD sends a region C containing multiple minimizers onto a larger region that contains C“ -> Is this a general statement or restricted to a specific objective function? It seems to me that this does not hold in general. Consider, e.g., a suitably scaled version of the objective function $f(x) = 1/4 x^4 - 1/2 x^2$ with derivative $f’(x) = x^3 - x = (x-1)(x+1)x$, and C = [-2,2]. Then the map GD should map C to a subset instead of a superset.\n\n- ll. 184/185: „…the supremum of the step sizes that allow convergence, where…“ -> shouldn’t it be „…supremum of the step sizes that allow convergence to global optimizers…“? At least this is the formal definition of $\\eta(U,V)$. Furthermore, I would suggest to formally define $\\mathcal{M}$ at least once in clear math notation.\n\n- ll. 238/239: The explicit form of $D_{\\eta}'$ seems to be incorrect for $y = 0$. In this case according to (3), convergence should hold for a.e. initialization and every $\\eta$ such that $D_{\\eta}'$ should be equal to the whole space a.e.\n\n- l. 247: The statement claims that for $y = 0$ and any learning rate (or $y \\neq 0$ and arbitrarily small learning rates) GD is a chaotic system. This appears counter-intuitive to me. Shouldn’t GD for extremely small learning rates be absolutely non-chaotic? Either there is a mistake in the statement or the use of topological entropy to measure chaos of GD seems inappropriate.\n\n- ll. 293-311: The discussion about instability of points with norm larger than $2/\\eta$ and the empirical observation that converging GD realizations always reach stationary points with norm less than $2/\\eta$ aligns with existing work on sharpness regularization due to large step-sizes (since in this toy setting sharpness and parameter norm coincide). It thus considerably weakens the relevance of the theoretical statement in Theorem 1 that one can reach limit points with arbitrary high norm/sharpness. \n\n- l. 298: „…the basin of the saddle…“ \n\n- l. 325: „…the presence of a basin…“\n\n- l. 340: The notation {z = y} is very imprecise\n\n- l. 358: In the definition of $p^-$ and $p^+$, the argmin/argmax is not always a singleton, or is it? If not, it would be cleaner to write $p^- \\in$ argmin …\n\n- l. 415: „…when the step size…“\n\n- l. 436: „…applied to the scalar factorization…“\n\n- l. 480: „..development of such a program.“"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zeq5VPFoJd", "forum": "wsxGCaBjWC", "replyto": "wsxGCaBjWC", "signatures": ["ICLR.cc/2026/Conference/Submission20680/Reviewer_F5Q8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20680/Reviewer_F5Q8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719535072, "cdate": 1761719535072, "tmdate": 1762934064336, "mdate": 1762934064336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors examine the dynamics of gradient descent with step sizes at (or near) the critical value associated with a given initialization. In particular, they analyze the scalar factorization problem in both its regularized and unregularized forms. \nIn the unregularized case, they rigorously determine the critical step size for any given initialization and show that the behavior near the convergence boundary is chaotic, both in the norm of the minimizer, the imbalance between the two factorization vectors, and the nature of the final point (minimum or strict saddle). They also derive a lower bound for the topological entropy of gradient descent in this setting. \nIn the regularized case, they rigorously show that the convergence boundary is fractal, implying that convergence itself is unpredictable near the boundary, and provide an intuitive explanation of the mechanism, based on a partition of the parameter space. Here too, they demonstrate that the dynamics initialized near the boundary are chaotic, showing in particular that gradient descent converges either to the nearest or the farthest global minimizer, and that infinitesimal perturbations can flip this selection. \nFinally, they show that in the general matrix factorization problem, there exists a non-trivial set of initializations for which the problem decomposes into parallel sub-dynamics of scalar factorizations, allowing the extension of the previous results to this case."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The setting studied in this work allows for a rigorous analysis of gradient descent when critical step sizes are used, with a thorough characterization of the chaotic nature of the dynamics and of the unpredictability of both convergence itself and the final point. \nAs stated by the authors, this can be connected with broader lines of research in realistic machine learning settings, such as the edge\nof stability, and to the best of my knowledge, this is the first rigorous work on the characterization of chaos for gradient descent."}, "weaknesses": {"value": "The setting studied in this paper is very specific, namely the scalar version of the matrix factorization problem. As stated by the authors, this can be extended to the general matrix factorization only with orthogonal initializations, which correspond to the case where the problem actually decomposes into many parallel scalar ones. \nIt would strengthen the impact to clarify how these mechanisms extend in more general settings, or even how the intuition provided by this simple setting is useful when moving to more realistic scenarios where gradient descent with critical step sizes is used."}, "questions": {"value": "Beyond the numerical experiments for general initializations in the appendix, can you provide some results or intuition on whether the qualitative mechanisms and intuitions developed in this paper for a simple optimization landscape persist in more complex settings, where, for example, many local minima are present (either in theoretical settings or in experimental findings in realistic machine learning settings)?\n\nDo you have any insights on whether similar chaotic or fractal behaviors might appear for adaptive or momentum-based optimizers (e.g., Adam) near critical hyperparameters?\n\nIn Theorem 3, unboundedness is shown only in the case of $y = 0$, but it is then stated in the main text that, in general, the convergence region has an unbounded interior. Even though the caption of Figure 3 states that this behavior is qualitatively observed in general, I think it should be made clearer in the main text that this property, in the general $y \\neq 0$ setting, is only observed numerically.\n\nIn the figure captions, the parameter space is referred to as $(x, y)$ in the unbounded case. I think this causes confusion, since in the main text $(u, v)$ is used in both cases, and $y$ is used for the target of the factorization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eSWzkNzxQl", "forum": "wsxGCaBjWC", "replyto": "wsxGCaBjWC", "signatures": ["ICLR.cc/2026/Conference/Submission20680/Reviewer_V6Q9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20680/Reviewer_V6Q9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820625499, "cdate": 1761820625499, "tmdate": 1762934063785, "mdate": 1762934063785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes gradient descent (GD) for both regularized and unregularized (shallow) matrix factorization, which simplifies to a scalar factorization problem under some orthogonal conditions. For the unregularized case, the authors prove the critical step size needed to converge to a global minimizer, as well as the conditions under which GD fails to converge to a minimizer. They state that this new result also tightens existing ones found in the literature.\nFurthermore, they show how GD is sensitive to initialization for this problem, and how at the critical step size, infinitesimal perturbations of the initialization can send the trajectory to a minimizer with arbitrarily large norm, sharpness, or imbalance, or to a saddle point. For the regularized problem, they show that the global dynamics of GD become even more unpredictable than for the unregularized problem. This unpredictability depends on the geometry of the boundary of the convergence region, which they show is self-similar and hence has a fractal structure."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents new insights into the geometry of the convergence boundary for matrix factorization, which simplifies to a scalar factorization problem. It also presents a dichotomy in the geometry between regularized and unregularized scalar factorization: for the regularized problem, the convergence region (up to a measure-zero set) has an unbounded interior, whereas for the unregularized case, it coincides almost everywhere with a bounded domain. To my knowledge, this is novel.\n\nFurthermore, the authors present critical step size regions that determine convergence or failure to converge. I am not sure exactly how novel this part is, however (which I discuss in the following section). Nonetheless, I believe that the mathematical tools used to derive the results could be useful for the broader research community."}, "weaknesses": {"value": "- I am somewhat unsure about the novelty of the step size bound for convergence in the unregularized case. Looking at the first term in Equation (3), I believe it comes from the inverse of the sharpness. If we then consider the results from Ghosh et al. (2025), specifically the case where $L=2$ and the target matrix is rank-1, this seems to reduce to the scalar factorization case. For this, they then prove that using a learning rate of $\\eta > 1 / |y|$ induces oscillations (and thus prevents convergence). Could the authors verify if this is the case? I do think that the second term in Equation (3) could be new, and it warrants more discussion than simply stating it depends on the convergence region. There should also be more discussion on what is done differently to improve the Wang et al. (2022) results, as mentioned in Line 255.\n\n- The paper's language is sometimes overly strong; for example, in line 257, the phrase \"precise description of chaos in gradient descent.\" This is a description of GD on a specific problem, not a general one.\n\n- I found the mathematical language to be overly complicated at times. For example, if I understand correctly, Proposition 5 seems to state that if we start from an orthogonal initialization, GD is invariant. This allows a simplification to the loss function in lines 162-163, which then reduces to the scalar factorization case. If so, this section could be simplified to improve readability. I also found lines 363-373 difficult to follow, though this may just be my interpretation.\n\n- Finally, a limitation is the simplicity of the studied model. While I do not expect derivations for the settings in lines 478-480, experimental results would be beneficial. For example, does adding regularization to deep matrix factorization alter the results of Ghosh et al. (2025), and do any of the results from this paper translate to that setting?"}, "questions": {"value": "I have listed a few questions in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TKNq7QZ2xf", "forum": "wsxGCaBjWC", "replyto": "wsxGCaBjWC", "signatures": ["ICLR.cc/2026/Conference/Submission20680/Reviewer_t5Rh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20680/Reviewer_t5Rh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936418367, "cdate": 1761936418367, "tmdate": 1762934063147, "mdate": 1762934063147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies regularized matrix factorization problems with gradient descent (GD). The authors investigate what happens when GD is run with large step sizes, a regime often associated with improved generalization in deep learning, yet poorly understood theoretically. \n\nThey first start from a simplified matrix factorization, in which the target matrix is reduced to a scalar. In particcular, for the unregularized scalar matrix factorization the authors characterize the critical step size below which the GD converges almost surely, sensitivity to initialization, and topological entropy of the GD system. For the regularized case the GD training dynamics exhibit fractal convergence boundadry.\n\nThen they discuss the possibility to extend their results to general matrix factorization problems. The paper is concluded with some discussions on limitations and future directions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors provide a solid theoretical analysis for matrix factorization with gradient descent. The main results reveal that the training dynamics can exhibit chaos and fractal geometry when stepsizes vary."}, "weaknesses": {"value": "1. The main results are limited to gradient descent on matrix factorization problems. In practice there is still a gap between this type of optimizatin problem and large-scale machine learning problems with stocahstic gradient methods.\n\n2. For the general matrix factorization problems, the authors only provide partial results -- the initialization of the matrices should satisfy a special orthogonality condition, under which the trajectories of the column vectors can be characterized using the scalar factorization case. This type of condition might be a bit strong and not covering all cases. This may also indicate the intrinsic difficulties of extending the scalar case to the high dimensional case.\n\n3. The experiments provided are limited to synthetic simulations, and there seems no detailed discussions between the theoretical results on chaos, fractal geometry and empirical observations from real-world problems."}, "questions": {"value": "1. Although Section 4 covers the case when initialization matrices are rescaling of identities, there are still many cases not covered and the orthogonality conditions seems to be strong. Could the authors could provide some comments on the main challenges of extending the one-dimensional case to the high-dimensional case.\n\n2. Could the authors provide some discussions on whether or not the chaos or factal geometry can be observed in real-world machine learning problems?\n\n3. Any challenges of extending the GD dynamics to SGD dynamics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bFmAhb7s22", "forum": "wsxGCaBjWC", "replyto": "wsxGCaBjWC", "signatures": ["ICLR.cc/2026/Conference/Submission20680/Reviewer_Nc47"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20680/Reviewer_Nc47"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984120216, "cdate": 1761984120216, "tmdate": 1762934062758, "mdate": 1762934062758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}