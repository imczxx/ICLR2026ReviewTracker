{"id": "RAaYqK9dk9", "number": 11816, "cdate": 1758204032724, "mdate": 1763322361926, "content": {"title": "Diffuse and Disperse: Image Generation with Representation Regularization", "abstract": "The development of diffusion-based generative models over the past decade has largely proceeded independently of progress in representation learning. These diffusion models typically rely on regression-based objectives and generally lack explicit regularization. In this work, we propose $\\textit{Dispersive Loss}$, a simple plug-and-play regularizer that effectively improves diffusion-based generative models. \nOur loss function encourages internal representations to disperse in the hidden space, analogous to contrastive self-supervised learning, with the key distinction that it requires no positive sample pairs and therefore does not interfere with the sampling process used for regression.\nCompared to the recent method of representation alignment (REPA), our approach is self-contained and minimalist, requiring no pre-training, no additional parameters, and no external data. We evaluate Dispersive Loss on the ImageNet dataset across a range of models and report consistent improvements over widely used and strong baselines. We hope our work will help bridge the gap between generative modeling and representation learning.", "tldr": "Plug-and-play representation regularizer that improves generative modeling with minimal overhead.", "keywords": ["Diffusion", "Flow", "Generative Model", "Representation Learning", "Contrastive Learning", "Self-Supervised Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6492bf96812212c6af841758c543b3e2da235db.pdf", "supplementary_material": "/attachment/ea50ae97e767aae00ec843938c8ce7d58b946c9c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the Dispersive Loss as a plug-and-play regularization term for diffusion-based generative models. The Dispersive Loss encourages the hidden feature representations to disperse (or spread out) in the latent space, analogous to the non-alignment term of contrastive learning, but uniquely requires no positive sample pairs. The authors claim this enhances the quality of generated images and improves training efficiency. They report consistent improvements over strong baselines across various models on the ImageNet dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Simplicity and Plug-and-Play Design: The proposed Dispersive Loss is highly appealing due to its self-contained and minimalist nature. It is a simple regularization term that can be added to the standard diffusion loss without requiring pre-training, external data, or additional visual encoders. This makes it easy to integrate into existing diffusion pipelines.\n\n2. Demonstrated Training Acceleration: The paper shows an intriguing result of training acceleration without the need for external data or modules (unlike methods like REPA). While the gain is marginal, the fact that a simple internal regularization can accelerate convergence is a valuable finding."}, "weaknesses": {"value": "## 1. Fundamental Gaps in Theoretical Justification and Representation Analysis\n\nThe paper's core weakness lies in the disconnect between its SSL-inspired motivation and its analysis.\n\n- Lack of Deep Theoretical Insight: The paper does not provide sufficient theoretical insights into why the Dispersive Loss leads to improvement, nor does it explore what kind of high-level representations generative models fundamentally require. The mechanism of dispersion is asserted to be beneficial without a rigorous explanation of its effect on the learned noise-prediction function or the resulting generative manifold.\n\n- Missing Representation Utility Analysis: Since the method is explicitly motivated by SSL, it should verify the \"rich semantic features\" from SSL methods. Analysis techniques like linear probing on the average-pooled features (at the intermediate or final layer) for tasks like classification or retrieval are essential to validate the core hypothesis that the regularization improves semantic content.\n\n- Unclear Design Rationale: It is asserted that the alignment (positive pair) term of contrastive loss is detrimental to the generation task, leading to its exclusion, as in Table 2. This is a strong claim that is not clearly supported. Intuitively, the alignment term would, at worst, be redundant; the paper must theoretically or empirically demonstrate why it would be actively harmful.\n\n## 2. Lack of Experimental Rigor and Insufficient Evidence\n\nThe experimental section suffers from issues of completeness, methodology, and evidence that undermine the paper's claims.\n\n- Missing Empirical Evidence. The proposed method has not been thoroughly investigated. While the paper evaluates various settings of the regularizers, e.g., different models and depths, it does not sufficiently analyze why this method is effective. \n\n- Missing Scalability and High-Resolution Tests: The paper lacks experiments in high-resolution settings, such as ImageNet-512x512 or text-to-image generation. This omission makes it impossible to assess the method's scalability and effectiveness on datasets where modern generative models typically face their greatest architectural challenges.\n\n## 3. Marginal Performance and Missing Training Dynamics Analysis\n\nThe reported performance gains are modest, and the paper fails to investigate the interaction between its components.\n\n- Marginal Performance Gains: The overall performance gains achieved by Dispersive Loss are marginal. The resulting models neither consistently match SOTA generative performance (e.g., REPA) nor demonstrate effectiveness in any downstream context.\n\n- Missing Integration with Faster Frameworks: While the paper notes its potential for acceleration, it is unclear whether the proposed framework can be combined with existing faster training frameworks like REPA. An experiment or intuition on this synergy is needed.\n\n- Unanalyzed Training Dynamics: The paper completely lacks an in-depth analysis of the training dynamics involving the new loss term. The authors should show the evolution of the Dispersive Loss over the course of training to see if it plateaus or exhibits different behavior from the standard denoising loss. An analysis of the conflict or synergy between the denoising loss and the Dispersive Loss (e.g., by analyzing the direction of their respective gradients) is necessary to clarify the training process. Furthermore, an investigation into how the dispersive loss varies across diffusion timesteps is crucial, as its impact is intuitively expected to be different for high-noise versus low-noise regimes.\n\n- Unfair Comparisons: The paper lacks comparison with fairer versions of SOTA models. For example, when comparing against REPA (800 epochs), the comparison should be normalized for total training time or resources consumed to provide a true measure of efficiency (Disperse loss uses >=1200 epochs in Table 6).\n\n- Incomplete Results. The paper only compares with SiT and SiT-REPA (except for one-step generation) and uses FID as their single evaluation metric. Authors should conduct comprehensive comparisons with current state-of-the-art methods that share the same idea, such as Lighting-DiT [`1], DC-AE1.5 [2], DDT [3], etc. Besides, more evaluation metrics, such as Inception-Score, should be included in the paper.\n\n\n\n\n**I recommend that the authors conduct a deeper investigation into the learned representations and strictly refine their experimental validation and theoretical justification during rebuttal period.**\n\n\n[1] Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models\n\n[2] DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space\n\n[3] DDT: Decoupled Diffusion Transformer"}, "questions": {"value": "1. Could the authors provide theoretical or empirical evidence to clarify what specific kinds of high-level semantic features are required by image generation models, particularly diffusion models?\n\n2. how does the proposed dispersive loss specifically facilitate the learning of such required semantic features?\n\n3. Given that contrastive learning frameworks typically rely on heavy image augmentations to learn high-level semantics, how is the Dispersive Loss able to learn similar semantic information without relying on these image augmentations?\n\n4. To support the main claim, shouldn't experiments be conducted to explicitly demonstrate the model's ability to learn rich semantic features, perhaps including evaluations on downstream tasks to provide valuable support?\n\n5. Does a high representation norm truly indicate the capture of semantic information, and could the authors validate whether the model has learned semantic features, similar to the REPA method, by using linear probing?\n\n6. Could the authors include feature similarity comparisons, such as Centered Kernel Alignment (CKA), with other feature extractors to better demonstrate the improved semantics?\n\n7. To further validate the effectiveness of dispersive loss, would it be beneficial to test it on more complex generative tasks, such as text-to-image generation or text-to-video generation?\n\n8. Since the method claims removing alignment term in line 231, shouldn't there be some investigation into why the alignment term deteriorates performance?\n\n9. What happens when augmented images are used for positive pairs (i.e., should an ablation study compare the use of the same image versus an augmented image)?\n\n10. How does down-weighting the alignment term in the contrastive loss affect the overall performance (i.e., how does performance change when moving from a standard contrastive loss toward the dispersive loss)?\n\n11. Does this additional regularization lead to features that are usable for other discriminative tasks (even though this is not the main focus, it could demonstrate a strength of the learned features)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UShcyk73Mo", "forum": "RAaYqK9dk9", "replyto": "RAaYqK9dk9", "signatures": ["ICLR.cc/2026/Conference/Submission11816/Reviewer_D22i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11816/Reviewer_D22i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653772630, "cdate": 1761653772630, "tmdate": 1762922838511, "mdate": 1762922838511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This authors try to address a key limitation of generative diffusion models with representation learning: diffusion models trained with regression-based denoising objectives often lack explicit regularization for the feature space, and therefore are limited in performance. To tackle this, the authors introduce a Dispersive Loss, which eliminates the need for explicit positive sample pairs, compared to the contrastive learning counterpart."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors aim to address a fundamental problem, namely that the generation task should not be left to stand alone with representation learning, and offer insightful perspectives.\n\n2. The paper features a clear structure and coherent logic.\n\n3. The proposed method does not rely on a pretrained encoder."}, "weaknesses": {"value": "1. The authors used limited evaluation metrics. As they don't rely on a pretrained encoder and claim the importance of representation learning in the generative task, the authors should evaluate how the method performs with metrics like linear probing.\n\n2. It's not clear why the major improvements were made in the case without CFG, while the performance with CFG only achieves very limited improvements. The authors should provide deeper analyses to explain this discrepancy and not rely only on FID (which is not in favor of the proposed method when evaluated with CFG).\n\n3. It's not clear how the proposed method scales with the diversity and the size of datasets. Some empirical verification or theoretical motivation would be useful.\n\n4. There is a slight redundancy in the mathematical derivation process of Section 3.2."}, "questions": {"value": "Please see the weakness part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "edkKexrtwQ", "forum": "RAaYqK9dk9", "replyto": "RAaYqK9dk9", "signatures": ["ICLR.cc/2026/Conference/Submission11816/Reviewer_btvs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11816/Reviewer_btvs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964468051, "cdate": 1761964468051, "tmdate": 1762922838027, "mdate": 1762922838027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a plug-in regularizer designed to enhance diffusion-based models by promoting the dispersion of internal representations in the hidden space. To demonstrate its effectiveness, the authors apply the proposed regularizer to two representative diffusion-based models, DiT (Peebles & Xie, 2023) and SiT (Ma et al., 2024). Experiments conducted on the ImageNet dataset show that the regularizer effectively improves model performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed regularizer can be directly integrated into diffusion models with intermediate representations and requires little additional computational effort. \n2. It elegantly incorporates concepts from self-supervised learning into diffusion model training in a straightforward and theoretically sound manner. \n3. Experiments on a real-world image dataset demonstrate that adding the regularizer significantly improves performance, and the experimental results are comprehensive."}, "weaknesses": {"value": "1. It would be helpful to clarify the scope of applicability. Can the proposed regularizer be applied to all diffusion models with intermediate representations? \n2. Qualitative comparisons between images generated with and without the proposed regularizer would make the improvements more intuitive and visually convincing. \n3. Although experiments explore different blocks, loss weights, and temperatures, it would be beneficial to provide systematic guidance or heuristics for selecting these hyperparameters."}, "questions": {"value": "See \"Weaknesses\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tS6CoTvZVW", "forum": "RAaYqK9dk9", "replyto": "RAaYqK9dk9", "signatures": ["ICLR.cc/2026/Conference/Submission11816/Reviewer_5V25"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11816/Reviewer_5V25"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976052476, "cdate": 1761976052476, "tmdate": 1762922837640, "mdate": 1762922837640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dispersive Loss, a simple yet effective plug-and-play regularizer designed to improve diffusion-based generative models by encouraging dispersion of internal representations in hidden space. Unlike prior work such as REPA [1], which requires pre-trained external encoders and additional parameters, the proposed method is self-contained, requiring no pre-training, no extra parameters, and no external data.\n\nConceptually, Dispersive Loss can be interpreted as a “contrastive loss without positive pairs.” It regularizes the hidden representations by penalizing excessive clustering and promoting diversity, inspired by the repulsive component of contrastive learning. The authors provide theoretical motivation, multiple instantiations (InfoNCE-, Hinge-, and Covariance-based), and efficient implementations requiring only a few lines of code.\n\nThe paper provides extensive empirical evaluation across several diffusion backbones, including SiT [2], DiT [3], and MeanFlow [4]. Experiments on ImageNet 256×256 consistently show significant FID improvements (up to ~11–13%) over strong baselines and even outperform contrastive variants that require two-view sampling. The method is also shown to generalize well to one-step generation and smaller datasets like CIFAR-10, demonstrating broad applicability.\n\n[1] Yu, Sihyun, et al. \"Representation alignment for generation: Training diffusion transformers is easier than you think.\" arXiv preprint arXiv:2410.06940 (2024).\n\n[2] Ma, Nanye, et al. \"Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.\n\n[3] Peebles, William, and Saining Xie. \"Scalable diffusion models with transformers.\" Proceedings of the IEEE/CVF international conference on computer vision. 2023.\n\n[4] Geng, Zhengyang, et al. \"Mean flows for one-step generative modeling.\" arXiv preprint arXiv:2505.13447 (2025)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of removing positive pairs while retaining the repulsive regularization aspect is conceptually appealing and practically justified by diffusion models’ intrinsic alignment objective.\n- Comprehensive experiments across multiple architectures (DiT, SiT, MeanFlow) and scales (S/B/L/XL) show consistent improvements in FID and Inception Scores.\n- The improvement trend scales with model size, indicating the loss acts as an effective regularizer for large-capacity models prone to overfitting.\n- Dispersive Loss outperforms all contrastive baselines even with careful tuning of noise schedules.\n- The plug-and-play simplicity (no multi-view augmentation or external encoders) is convincingly demonstrated.\n- The authors provide ablations for hyperparameters ($\\lambda$, $\\tau$), layer placement, and different loss variants (Table 2–4), showing robustness across configurations.\n- Implementation details are transparent (Algorithm 1–2, Table 8).\n- The inclusion of MeanFlow and CIFAR-10 experiments supports generality across diffusion and flow-matching paradigms.\n- Figures (e.g., Fig. 1–4) clearly illustrate how Dispersive Loss integrates into existing architectures with negligible computational overhead.\n- Comparisons with REPA (Table 6) highlight the system-level efficiency advantage (no 1.1B-parameter pre-trained model, no 142M external images).\n- The paper adheres to reproducibility standards (code, README included) and presents results with careful quantitative analysis.\n- The method provides a bridge between generative and representation learning, a frontier that is conceptually and practically valuable for the field."}, "weaknesses": {"value": "- The method is motivated intuitively but lacks a formal analysis of why dispersion improves generation quality. A deeper information-theoretic or geometric argument (e.g., on latent coverage or mutual information bounds) would strengthen the theoretical grounding.\n- While FID and Inception Scores are strong indicators, evaluation on semantic diversity, perceptual similarity, or representation quality (e.g., CLIP-based metrics) could better reveal what aspects of representation regularization improve."}, "questions": {"value": "- Although CIFAR-10 is included, other generative domains (text-to-image or high-res synthesis) could further establish generality. It would be interesting to see whether Dispersive Loss also benefits conditional or multimodal diffusion models.\n- While Figure 3 shows increased representation norms, further qualitative or visualization-based analyses (e.g., embedding t-SNEs) would make the mechanism more intuitive.\n- Some missing works on representation learning using/within diffusion models should be included [5, 6, 7, 8, 9] in related works.\n\nOverall, I think this is a good paper and I would be happy to raise the score if my comments are addressed properly.\n\n[5] Wang, Yingheng, et al. \"Infodiffusion: Representation learning using information maximizing diffusion models.\" International conference on machine learning. PMLR, 2023.\n\n[6] Mittal, Sarthak, et al. \"Diffusion based representation learning.\" International conference on machine learning. PMLR, 2023.\n\n[7] Hudson, Drew A., et al. \"Soda: Bottleneck diffusion models for representation learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[8] Zhang, Zijian, Zhou Zhao, and Zhijie Lin. \"Unsupervised representation learning from pre-trained diffusion probabilistic models.\" Advances in neural information processing systems 35 (2022): 22117-22130.\n\n[9] Yang, Xingyi, and Xinchao Wang. \"Diffusion model as representation learner.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sDsyECUdYd", "forum": "RAaYqK9dk9", "replyto": "RAaYqK9dk9", "signatures": ["ICLR.cc/2026/Conference/Submission11816/Reviewer_i7Q5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11816/Reviewer_i7Q5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117864324, "cdate": 1762117864324, "tmdate": 1762922837325, "mdate": 1762922837325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}