{"id": "nfOnCngWtp", "number": 6862, "cdate": 1757998749833, "mdate": 1759897887594, "content": {"title": "Perceptual Neural Video Compression with Video Variational AutoEncoder at Low Bitrates", "abstract": "Existing neural video compression methods typically rely on frame-wise coding frameworks. Motion estimation and compensation are used to eliminate inter-frame redundancy, and compression performance is further enhanced through explicit residual or implicit conditional coding. However, these methods are primarily optimized for distortion, leading to significant degradation in perceptual quality at low bitrates. In this paper, we propose a novel learning-based video compression framework that leverages the compression and generative capabilities of video variational autoencoders. Unlike traditional frame-wise processing, our method operates on groups of frames, effectively improving perceptual quality at low bitrates. Specifically, we utilize video variational autoencoders to eliminate both temporal and spatial redundancy, encoding video clips into a perception-oriented latent space. Then, transform coding is applied to further capture spatial dependencies, yielding a more compressible latent representation. Finally, entropy coding is used to compress the quantized latent representation of the group of frames. Since each group of pictures is treated independently, our method can naturally be processed in parallel for acceleration. To incorporate information from adjacent frame groups and maintain temporal consistency across groups, we introduce an overlapping processing strategy, ensuring smooth transitions between adjacent groups. Extensive experimental results on benchmark datasets such as HEVC, UVG, and MCL-JCV demonstrate that our framework outperforms existing methods in terms of perceptual metrics.", "tldr": "Learned Video Compression", "keywords": ["video compression", "image compression", "neural network", "end-to-end optimization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c575c262fa2344f35564e1c3967d77b6aa3555a.pdf", "supplementary_material": "/attachment/01ffbef24f2a7c593c55d703860c14510ea14f69.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a perceptual video codec that encodes entire GoPs with a pretrained Video-VAE (CV-VAE or LeanVAE) to get a perceptually friendly latent, then apply a hyperprior entropy model in latent space, with no explicit motion estimation/compensation modules. A training-free GoP-overlap (one shared transition frame with weighted fusion) improves inter-GoP continuity at tiny bitrate overhead. Results show perceptual gains vs HEVC/VVC/DCVC-FM on LPIPS/FID/KID/BRISQUE, and a LeanVAE variant reaches ~24 fps decode, while keeping a sizable FID-BDRate improvement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The system is clearly presented, empirically supported, and well-motivated for the low-bitrate perceptual regime. The overlap trick is simple and practical, and the ablations are thorough. The authors include ablations on latent channel width (c = 64, 128, 192), overlap weight $\\theta$, and different perceptual losses, showing how each impacts perceptual and temporal metrics. These details demonstrate careful experimentation and help readers reproduce the setup.\n- Quantitatively, the codec achieves consistent gains in FID-BDRate and LPIPS compared to HEVC, VVC, and DCVC-FM, particularly below 0.05 bpp. Qualitatively, reconstructions exhibit fewer blocking and temporal flicker artifacts, suggesting that the VAE prior effectively captures smooth temporal evolution.\n- The proposed training-free overlap, which blends one frame between neighboring GoPs, helps mitigate the temporal discontinuities that often plague GoP-based codecs. The idea is lightweight, easy to implement, and incurs almost no bitrate overhead while yielding measurable improvements on tOF/tLP."}, "weaknesses": {"value": "- **Limited novelty.**: In my opinion, neither the idea of non-explicit motion estimation nor the use of video generation-style (3D) autoencoders / tokenizers are novel in the video compression community. For example, MAGVIT-v2 [1] (though not specifically designed for video compression but did validate for perceptual compression performance) and the more recent DHVC [2] and GiVIC [3] have both adopted 3D autoencoder (and 3D autoencoder only) and patchification to consume a group/chunk of frames for joint spatiotemporal redundancy reduction. The same goes for hierarchical entropy coding or the utilization of spatial-channel context [4]. Though I think the training-free temporal continuity strategy is an interesting addition, the differences and novelties for the other two major contributions should be more clearly discussed in the paper.\n- **Evaluation scope**. While the results convincingly show perceptual gains at very low bitrates (and the model is designed targeting low bitrates more), the paper does not explore how the proposed method performs at moderate or high bitrates, where pixel fidelity becomes more relevant. It would strengthen the work if the authors could report whether the video VAE remains competitive in those regimes or if its advantage diminishes once distortion metrics dominate.\n- **Complexity profiling.** The claimed real-time decoding is achieved only with LeanVAE; the heavier CV-VAE version remains far from real-time. A more detailed latency breakdown (tokenization, I/O, entropy, AR context, upsampling) would help clarify bottlenecks and show whether the method scales to higher resolutions.\n\nOverall, I think the paper is intuitive and of good quality, but the claim \"eliminating the need for motion estimation and compensation\" seems incremental or even not entirely novel to me. It would be helpful if the authors are able to demonstrate why such a video VAE is particularly preferable in the case of perceptual-oriented compression, significantly different from some existing methods, or is able to yield superior compression performance when optimized for distortion-oriented losses. \n\n[1] Language Model Beats Diffusion - Tokenizer is key to visual generation, ICLR'24\n\n[2] Deep Hierarchical Video Compression, AAAI'24\n\n[3] GIViC: Generative Implicit Video Compression, ICCV'25\n\n[4] Neural Video Compression With Diverse Contexts, CVPR'23"}, "questions": {"value": "Please refer to the **Weaknesses** section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "j1X8wOv2kn", "forum": "nfOnCngWtp", "replyto": "nfOnCngWtp", "signatures": ["ICLR.cc/2026/Conference/Submission6862/Reviewer_8vdS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6862/Reviewer_8vdS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777500223, "cdate": 1761777500223, "tmdate": 1762919117569, "mdate": 1762919117569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a neural video compression framework that leverages a Video Variational AutoEncoder (Video VAE) to jointly encode an entire Group of Pictures (GoP) into a perceptual latent space, rather than relying on the conventional motion estimation and compensation pipeline used in traditional or neural codecs. To encourage temporal smoothness across GoPs, the method introduces a training-free frame-overlapping strategy between adjacent GoPs. Experiments on the HEVC, UVG, and MCL-JCV datasets show improvements in perceptual metrics such as FID, KID, and BRISQUE at low bitrates (<0.03 bpp), compared with HEVC, VVC, and DCVC-FM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper leverages the strong spatio-temporal joint modeling capability of Video VAE. By jointly encoding entire GoPs, the method in principle avoids the need for explicit motion estimation and motion compensation, and claims to effectively eliminate both spatial and temporal redundancy.\n2 .The proposed training-free overlapping coding strategy that smooths transitions between GoPs is easy to implement and incurs only a small bitrate overhead under low-bitrate scenarios. The paper also provides quantified temporal continuity metrics (tOF and tLP) comparing results before and after overlapping."}, "weaknesses": {"value": "1. Although the paper claims to address perceptual degradation at lower bitrates (below 0.03 bpp), the experimental section lacks results at bitrates near 0.03bpp. Moreover, evaluations on the UVG, HEVC, and MCL-JCV datasets indicate that the proposed method does not exhibit clear advantages over other methods in terms of LPIPS.\n2. Some of the baseline methods used for comparison (e.g., VVC and DCVC-FM) are relatively outdated and were primarily optimized for rate–distortion (RD) performance rather than perceptual quality. This makes the comparison potentially less fair or conclusive.\n3. The paper lacks an ablation study on the blending factor θ and the number of overlapping frames, which would help evaluate the sensitivity of the method to these parameters."}, "questions": {"value": "1. How sensitive is the method to the choice of the pretrained Video VAE, and have alternative VAE backbones been tested for generalization? To what extent does the chosen VAE architecture contribute to the final compression performance?\n2. Does the training strategy significantly impact both convergence speed and perceptual performance? How was the final training strategy selected and experimentally validated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YyEKrxHfgw", "forum": "nfOnCngWtp", "replyto": "nfOnCngWtp", "signatures": ["ICLR.cc/2026/Conference/Submission6862/Reviewer_FWxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6862/Reviewer_FWxT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790195152, "cdate": 1761790195152, "tmdate": 1762919117041, "mdate": 1762919117041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for perceptual neural video compression based on a video variational autoencoder (VAE) operating at low bitrates. The proposed system consists of three main components: a generative video VAE encoder, a transform block that compresses the encoder’s output, and a hyperprior module for entropy coding. The paper is clearly written and the experimental results are promising; however, the approach offers limited technical novelty. Additionally, some aspects remain unclear and should be clarified by the authors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and clearly presented.\n\n2. The experimental results are solid and support the proposed approach.\n\n3. The work can serve as a valuable reference point for future research in the field."}, "weaknesses": {"value": "1. Some points in the paper are unclear (see questions).\n\n2. A few formulas appear to be incorrect (see questions).\n\n3. The paper lacks a PSNR comparison with state-of-the-art methods."}, "questions": {"value": "1. The size of $L_t$ is unclear. The authors mention $t$ as the first dimension size, but this cannot be correct, since $t$ is simply the index of the clip. Later in the paper (Section 3.1), a definition of $t$ is provided, but it remains ambiguous, as $t$ was previously defined as the clip index. Do the authors actually mean that the size of the latent representation depends on the clip number? This point requires clarification.\n\n2. Is the size of $Y_t$ really the same as that of $L_t$? This seems rather unusual. Similarly, why are $g_a$ and $g_s$ models so much smaller than the hyperprior models? This choice is not very intuitive.\n\n3. Regarding Fig. 7, I do not understand why the same number of channels is maintained for $L$ and $Y$. From the dashed lines, it appears that the number of channels of $L$ has little impact on performance, whereas the number of channels of $Y$ seems to be the crucial factor.\n\n4. The authors should report the performance in terms of PSNR, not just perceptual metrics. This is important to assess how accurately the reconstruction can diverge from the original uncompressed video.\n\n5. You wrote, \"where $P$ represents a perceptual metric such as LPIPS.\" Does this mean that the results in Figure 5 were obtained by training a separate model for each metric? In other words, did you train one model for LPIPS, another for FID, and so on?\n\n6. (Minor) I would suggest testing more than 3 rate points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ytcvM0gbrH", "forum": "nfOnCngWtp", "replyto": "nfOnCngWtp", "signatures": ["ICLR.cc/2026/Conference/Submission6862/Reviewer_zWCo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6862/Reviewer_zWCo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822775295, "cdate": 1761822775295, "tmdate": 1762919116108, "mdate": 1762919116108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a 3D Video VAE-based video compression framework, which eliminates the need for motion estimation and compensation. Instead, the model leverages video VAE to jointly capture spatial and temporal dependencies. To ensure temporal consistency between different GoPs, the authors further propose a simple overlapping processing strategy, allowing smooth transitions between adjacent frame groups"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of using 3D VAE for direct video compression is interesting."}, "weaknesses": {"value": "- This work appears somewhat incremental, representing a straightforward extension of 2D variational autoencoder–based image codecs to 3D variational autoencoder–based video codecs, without introducing any novel modules. Fundamental concepts such as hierarchical coding and the entropy model are not new. Therefore, this should not be considered a primary contribution of the paper.\n\n- This work lacks a comprehensive comparison of perceptual quality, and additional metrics should be included for a fair evaluation. Moreover, since LPIPS is a strong indicator of perceptual quality, the method fails to outperform the baselines.\n\n- The proposed group-level processing may function similarly to B-frame coding, potentially offering an advantage not present in other baseline methods. \n\n- Furthermore, the applicability of this work is limited, as the model processes groups of nine frames at a time, potentially introducing significant delay and restricting its use in real-time or low-latency scenarios.\n\n- BD-rate evaluation is performed using only three bitrate points, which may not yield reliable or smooth curves for comparison."}, "questions": {"value": "- Although fidelity is not the primary focus of this work, a PSNR comparison with the baselines should be provided to ensure the method maintains high reconstruction quality.\n\n- Additionally, results using perceptual quality metrics, such as DISTS, should be reported."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vn3FLtw1Zw", "forum": "nfOnCngWtp", "replyto": "nfOnCngWtp", "signatures": ["ICLR.cc/2026/Conference/Submission6862/Reviewer_apEs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6862/Reviewer_apEs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994335655, "cdate": 1761994335655, "tmdate": 1762919115794, "mdate": 1762919115794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a perceptual neural video compression framework leveraging video Variational AutoEncoders (VAEs) to achieve high perceptual quality at low bitrates. Instead of operating on a frame-by-frame basis, the approach encodes entire groups of frames (GoPs) to jointly eliminate spatial and temporal redundancies. The compressed latent representation is further optimized using transform and entropy coding, while an overlapping GoP strategy maintains temporal consistency between video groups. Extensive experiments on HEVC, UVG, and MCL-JCV benchmarks demonstrate improvement over both classical (HEVC, VVC) and state-of-the-art neural codecs in perceptual metrics, particularly at low bitrates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Core conceptual originality:** The shift from frame-wise, motion-compensation paradigms to a holistic, group-of-pictures encoding grounded in video VAEs is meaningful and opens new design space for neural video compression (see Figure 2, which visually contrasts this group-based paradigm against prior methods).\n\n**Strong empirical results:** The approach consistently outperforms both traditional and advanced learning-based codecs (HEVC, VVC, DCVC-FM, PLVC, HVFVC) on several perceptual quality benchmarks. For instance, Figure 5 and Table 2 exhibit clear quantitative improvements in FID, LPIPS, KID, and BRISQUE at low bitrates across multiple data sets.\n\n**Solid ablation and hyperparameter tuning:** The impact of the latent channel size is presented clearly in Figure 7, with a thoughtful analysis on compression ratio and perceptual quality."}, "weaknesses": {"value": "**Ablation, but incomplete sensitivity analysis:**  While the effect of the channel size $c$ is analyzed in Figure 7, other key hyperparameters (e.g., the GoP size, the overlapping window length, or the blending factor $\\theta$) are only mentioned as fixed values. A more systematic ablation—e.g., does increasing the overlap length or adjusting the blending factor enhance temporal quality further, or does it merely increase redundancy? How robust is the method to varying GoP length?\n\n**Perception-distortion tradeoff is underdeveloped:** Although perceptual quality is prioritized (as per FID, LPIPS, KID curves in Figure 5), the paper does not provide a deep exploration of the tradeoff behavior. For example, how does the perceptual gain compare to distortion loss in PSNR? And at what point does visual coherence break down (Section 4, Figure 5)? This is especially important given the negative impact on pixel-level fidelity for critical applications.\n\n**Potential overstatement of generality:** Although the method is shown to outperform other codecs on standard datasets, its robustness to domains with rare events, streaming scenarios, or videos with complex, long-term motion is not explored. The parallelizable GoP-level coding could expose the system to unique artifacts in these cases."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "keXxDPASNe", "forum": "nfOnCngWtp", "replyto": "nfOnCngWtp", "signatures": ["ICLR.cc/2026/Conference/Submission6862/Reviewer_R79P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6862/Reviewer_R79P"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050147673, "cdate": 1762050147673, "tmdate": 1762919115463, "mdate": 1762919115463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}