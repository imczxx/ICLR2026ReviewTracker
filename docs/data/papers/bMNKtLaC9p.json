{"id": "bMNKtLaC9p", "number": 13171, "cdate": 1758214602916, "mdate": 1759897459426, "content": {"title": "Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets", "abstract": "Achieving both accuracy and diverse reasoning remains challenging for Large Language Models (LLMs) in complex domains like mathematics. A key bottleneck is evaluating intermediate reasoning steps to guide generation without costly human annotations. To address this, we first introduce a novel Process Reward Model (PRM) trained automatically using Monte Carlo Tree Search coupled with a similarity-based data augmentation technique, effectively capturing step-level reasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks (GFlowNets) to operate at the reasoning step level. Unlike traditional reinforcement learning focused on maximizing a single reward, GFlowNets naturally sample diverse, high-quality solutions proportional to their rewards, as measured by our PRM. Empirical evaluation shows strong improvements in both accuracy and solution diversity on challenging mathematical benchmarks (e.g., +2.59\\% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective generalization to unseen datasets (+9.4\\% absolute on SAT MATH). Furthermore, we benchmark our PRM against existing open-source reward models, demonstrating superior alignment with reasoning quality and more consistent guidance for downstream generation. Our work demonstrates the potential of PRM-guided, step-level GFlowNets for developing more robust and versatile mathematical reasoning in LLMs.", "tldr": "", "keywords": ["GflowNets", "SubTrajectory Balance", "GFlowNet-Finetuning", "LLM Reasoning", "PPO"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8148aa618b5ce3dd51ee96648373175eb71cd8a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper trains an automated Process Reward Model (PRM) using MCTS + similarity grouping, then uses the PRM as the step-level reward to fine-tune LLMs with GFlowNets (SubTB) at step granularity/ Reported gains: small but positive on in-domain MATH Level 5 and larger on SAT MATH; authors also claim increased diversity via lower semantic similarity of generated solutions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear step-level formulation (actions = reasoning steps) and an explicit termination probability π(sf|s) within the GFlowNet policy. \n2. Self-contained PRM training pipeline (MCTS, continuous scores, rollout reuse + similarity grouping) with some validation diagnostics"}, "weaknesses": {"value": "1. PRM reliability and calibration are under-substantiated.\nThe core premise hinges on PRM accuracy (U(s′|s) ∈ [0,1]) guiding both PPO and GFlowNets. But the paper offers limited, small-scale PRM validation and mostly heuristic similarity grouping. This makes it hard to trust the PRM as an absolute reliable signal.\n\n2. Overlap/Difference with prior work (“Flow of Reasoning”) is unclear.\nThe paper positions step-level GFlowNets for diverse trajectories, but related work already targets divergent reasoning with minimal examples (Flow of Reasoning, 2024/2025). What is materially new? The comparison is relegated to related-work mentions rather than a head-to-head study, and the conceptual delta is not crisply argued. \n\n3. Gains are modest and may be within variance.\nOn in-domain MATH Level 5, the 3B model improves ~+2.6pts over baseline and 8B is +0.7–0.9. GSM8K barely moves. There are no confidence intervals, no multi-seed repeats, and limited ablations teasing apart PRM vs. GFlowNet vs. decoding heuristics. It’s difficult to conclude statistical significance rather than randomness. (Table 2). \n\n4. Termination probability π(sf|s): underspecified in practice.\nWhile π(sf|s) is mentioned (sink state), the actual parameterization/training signals for termination are not detailed: how is π(sf|s) learned/stabilized under SubTB at step level; what is the impact of termination on reward estimates? (Sec. 4.1–4.2). \n\n5. Chosen tasks don’t stress true solution multiplicity.\nMATH/GSM8K often admit stylistic variation rather than structurally distinct paths. If “diversity” is central, it'd be more convincing to evaluate on domains with genuinely multiple optimal plans (e.g., Blocksworld, program synthesis with multiple implementations, theorem-proving with lemmas reorderings). The current “diversity” metric is a weak proxy and might just reward rephrasing, not distinct strategies. (Sec. 5.2)."}, "questions": {"value": "1. Termination behavior and reasoning depth:\nYou mention the use of a termination probability π(sf|s) to model when reasoning should stop. Could you discuss what qualitative patterns you observed — for instance, do longer reasoning chains correlate with higher accuracy, or does early termination sometimes produce more concise correct reasoning?\n2. On PRM design and training dynamics:\nHow sensitive is your overall training process to the quality of the PRM? For example, if the PRM is slightly miscalibrated or trained on fewer MCTS rollouts, does the downstream GFlowNet policy still converge reliably, or do you observe instability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g0ymdPPQdI", "forum": "bMNKtLaC9p", "replyto": "bMNKtLaC9p", "signatures": ["ICLR.cc/2026/Conference/Submission13171/Reviewer_G497"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13171/Reviewer_G497"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560174150, "cdate": 1761560174150, "tmdate": 1762923875303, "mdate": 1762923875303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework to improve both accuracy and diversity in LLM mathematical reasoning. The authors first develop an automated PRM using MCTS and a similarity-based data augmentation technique to capture step-level reasoning quality without human annotation. This PRM is then used for a step-level GFlowNet, which empirically demonstrates significant gains in accuracy and diversity over PPO baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles the highly important and challenging goal of improving both accuracy and diversity in LLM reasoning.\n2. The paper is clearly presented and easy to follow.\n3. The method shows strong empirical gains over the PPO baseline."}, "weaknesses": {"value": "1.  The paper lacks sufficient ablation studies on the key techniques used in the proposed PRM, such as similarity-based data augmentation and continuous scoring. This makes it difficult to judge the actual effectiveness and individual contribution of each proposed component.\n\n2.  The performance evaluation is limited to a comparison with PPO, while lacking comparisons against other methods like DPO or GRPO. Although the method demonstrates superior performance over PPO, it remains unclear whether it holds any advantage over these other techniques.\n\n3.  The paper lacks a direct experimental comparison between the token-level GFlowNet and the proposed step-level GFlowNet. The authors only argue conceptually for the superiority of the step-level approach. To validate the step-level GFlowNet as a significant contribution, it is essential to demonstrate this superiority empirically."}, "questions": {"value": "Please refer to the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DnEkgGtDb6", "forum": "bMNKtLaC9p", "replyto": "bMNKtLaC9p", "signatures": ["ICLR.cc/2026/Conference/Submission13171/Reviewer_e3c4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13171/Reviewer_e3c4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630824949, "cdate": 1761630824949, "tmdate": 1762923874971, "mdate": 1762923874971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework to enhance the accuracy and diversity of Large Language Models (LLMs) in mathematical reasoning. The method combines two key components: an automatically trained Process Reward Model (PRM) and step-level fine-tuning using Generative Flow Networks (GFlowNets). Empirical results on math benchmarks show that this approach improves accuracy and significantly enhances the diversity of generated solution strategies, with particularly better generalization performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is generally well written and easy to understand.\n\n2. The paper provides thorough experimentation, demonstrating improvements in both accuracy and a quantitatively measured diversity metric across multiple benchmarks."}, "weaknesses": {"value": "1. The baseline of using GFlowNets with only final rewards seems missing, which would clarify the specific contribution of the sophisticated step-level PRM versus the GFlowNet objective itself. Also see Q1.\n\n2. The overall framework involves complex components (MCTS, similarity-based augmentation, step-level GFlowNets with SubTB loss), which might make it sensitive to hyperparameters and potentially difficult to reproduce. The paper would be strengthened by a sensitivity analysis of key parameters (e.g., the similarity threshold)."}, "questions": {"value": "1. The interplay between PRM and GFLowNet is still somehow confusing. Is the GFLowNet objective generally better than PPO, or does it have specific strengths when optimizing the PRM? Could you comment on the results of using GFlowNets with a simple, binary terminal reward and compare it with PPO? This would help isolate the benefit of the step-level reward signal from the benefit of the GFlowNet's diversity-seeking objective.\n\n2. Besides, could you elaborate on the PPO baseline? What’s the exact reward used here? Only from PRM or also combined with the final correctness? If combined, how are they combined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9MvkZtf6aB", "forum": "bMNKtLaC9p", "replyto": "bMNKtLaC9p", "signatures": ["ICLR.cc/2026/Conference/Submission13171/Reviewer_rCR8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13171/Reviewer_rCR8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996925672, "cdate": 1761996925672, "tmdate": 1762923874371, "mdate": 1762923874371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}