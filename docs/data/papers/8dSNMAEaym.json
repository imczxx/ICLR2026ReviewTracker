{"id": "8dSNMAEaym", "number": 11789, "cdate": 1758203842950, "mdate": 1759897554795, "content": {"title": "Concept Concentration for Faithful Representation Intervention", "abstract": "Representation intervention aims to locate and modify the representations that encode the underlying concepts in Large Language Models (LLMs) to elicit the aligned and expected behaviors. Despite the empirical success, it has never been examined whether one could locate the faithful concepts for intervention. In this work, we explore the question in safety alignment. If the interventions are faithful, the intervened LLMs should erase the harmful concepts and be robust to both in-distribution adversarial prompts and the \\textit{out-of-distribution} (OOD) jailbreaks. While it is feasible to erase harmful concepts without degrading the benign functionalities of LLMs in linear settings, we show that it is \\textit{infeasible} in the general non-linear setting. To tackle the issue, we propose \\texttt{Concept Concentration} (\\texttt{COCA}). Instead of identifying the faithful locations to intervene, \\texttt{COCA} refactors the training data with an explicit reasoning process, which first identifies the potential unsafe concepts and then decides the responses. Essentially, \\texttt{COCA} simplifies the decision boundary between harmful and benign representations, enabling more effective linear erasure. Extensive experiments with multiple representation intervention methods and model architectures demonstrate that \\texttt{COCA} significantly reduces both in-distribution and OOD jailbreak success rates, and meanwhile maintaining strong performance on regular tasks such as math and code generation.", "tldr": "", "keywords": ["LLMs", "Representation Intervention", "Safety Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abd3d8af2ecc21d17723039efefecc480145f76e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the limitations of existing representation intervention methods for safety alignment in LLMs, showing that perfect harmful concept erasure is theoretically impossible in non-linear representation spaces. To address this, it proposes COCA (COncept ConcentrAtion), which introduces structured reasoning annotations to linearize harmful concepts, enabling effective and faithful erasure. Experiments across four LLMs demonstrate that COCA improves robustness against out-of-distribution jailbreaks while preserving benign capabilities."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. **Clear Theoretical Contribution.** The paper rigorously identifies the root cause of representation intervention failure as non-linear entanglement between harmful and benign concepts and mathematically proves the impossibility of perfect erasure under this regime (Theorem 2.2).\n2. **Novel Conceptual Shift.** Instead of seeking ideal intervention points in a complex space, it introduces Concept Concentration (COCA), a new paradigm that simplifies the representation space at the data level.\n3. **Strong Empirical Results.** COCA substantially reduces OOD jailbreak success rates across four base models (LLaMA-3.1-8B, Qwen-2.5-7B, etc.) while preserving performance on math and code reasoning tasks, demonstrating a balance between safety and utility"}, "weaknesses": {"value": "1. **Annotator Bias.** The method depends on fine-grained concept annotations, so residual bias or subjectivity in defining “unsafe” content may remain.\n2. **Absence of Large Reasoning Model Evaluation.** The paper does not include experiments on Large Reasoning Models (LRMs), which are increasingly important for assessing safety and concept alignment. Because the proposed method relies on explicit structural annotations, its applicability to LRMs such as DeepSeek-R1-Qwen-7B or DeepSeek-R1-LLaMA-8B remains unclear. Without such validation, it is difficult to confirm whether the approach generalizes beyond standard instruction-tuned LLMs. Comparison with recent LRM baselines [1,2] would strengthen the evaluation.\n\n[1] Jeung, Wonje, et al. \"SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment.\" NeurIPS (2025).\\\n[2] Wang, Zijun, et al. \"Star-1: Safer alignment of reasoning llms with 1k data.\" arXiv preprint arXiv:2504.01903 (2025)."}, "questions": {"value": "I don't have additional questions. Solid work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0i4zKYLoTr", "forum": "8dSNMAEaym", "replyto": "8dSNMAEaym", "signatures": ["ICLR.cc/2026/Conference/Submission11789/Reviewer_3MTC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11789/Reviewer_3MTC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760699468219, "cdate": 1760699468219, "tmdate": 1763016710527, "mdate": 1763016710527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes COCA, a presentation intervention approach to improve the safety alignment robustness against out-of-distribution jailbreaks. It first conducts a theoretical analysis to reveal the infeasibility of existing representation intervention techniques to erase harmful concepts without degrading utility under non-linear setup. It then proposes a data refinement approach to explicitly decompose the safety data into reasoning steps and prove that such approach is able to simplify the decision boundary and make linear erase feasible. Evaluation results on several jailbreaks and comparison with SOTA concept erase and safety alignment baselines demonstrate the effectiveness of the COCA."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important problem in LLM safety.\n- The theoretical analysis is insightful and valuable to the safety community, as it exposes fundamental limitations of existing methods and motivates the proposed approach.\n- The proposed data refinement method is intuitive and easy to follow."}, "weaknesses": {"value": "- The empirical improvement over existing methods is relatively small.\n- The presentation could be improved for better clarity and readability."}, "questions": {"value": "- The presentation, especially in Tables 1 and 2, could be improved. The current layout is hard to read and makes it difficult to match results with the proposed approach.\n- When compared with state-of-the-art alignment methods such as STAIR, the improvement achieved by COCA-structured data appears marginal and, in some cases, even falls below baseline performance.  For example, STAIR achieves 4.3% ASR on PAIR when applied to LLaMA-3.1-8B, while COCA+RR yields 7.8%.\n- How are ID and OOD attacks refined? What is the rationale for treating all jailbreak attacks as OOD? Some, like PAIR, do not involve unreadable tokens, what characteristic makes them OOD?\n- It would be helpful to include evaluations against gradient-based jailbreaks, such as GCG [1], to further demonstrate robustness.\n\n---\nReference \n----\n\n[1] Zou A, Wang Z, Carlini N, et al. Universal and transferable adversarial attacks on aligned language models[J]. arXiv preprint arXiv:2307.15043, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BOsSITbe6J", "forum": "8dSNMAEaym", "replyto": "8dSNMAEaym", "signatures": ["ICLR.cc/2026/Conference/Submission11789/Reviewer_eUou"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11789/Reviewer_eUou"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860258694, "cdate": 1761860258694, "tmdate": 1762922812350, "mdate": 1762922812350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the safety alignment problem of LLMs. The authors propose COCA, which first indentifies the potential unsafe concepts and then decides the response. The anthor introduce a well-designed system prompt annotated by teacher model (GPT-4o) into the training data. It aims to induce the student LLMs to reflect and indentify on the potential unsafe the concepts before the standard response. The authors validate the effectiveness of the proposed approach in multiple experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The problem studied in this paper is valuable. \n2. The paper is well written. \n3. The proposed approach is effective."}, "weaknesses": {"value": "1. This paper tells a good story, from the standard safety alignment, concept-centric alignment to the linear representation hypothesis. After reading that, I expect some amazing ideas to solve the non-linear representation alignment problem. However, the method is just like a prompt or CoT engineer with well-designed prompts, and fine-tune the model to generate safety thinking before the standard response. We believe this is a very normal and robust method in real applications. \n2. The evidence of my first point can be found in Table 3. The \"self-generated\" method can also significantly improve the safety performance. And I believe that the LLMs can also achieve a good safety performance by adding this \"concept prompt\" in the LLMs without any training."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dbRCWSeP02", "forum": "8dSNMAEaym", "replyto": "8dSNMAEaym", "signatures": ["ICLR.cc/2026/Conference/Submission11789/Reviewer_J5S5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11789/Reviewer_J5S5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918400215, "cdate": 1761918400215, "tmdate": 1762922812005, "mdate": 1762922812005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes COCA, a data pre-processing framework designed to make safety reasoning explicit in fine-tuning data. COCA reformats training examples into a structured reasoning sequence using five tags, aiming to encourage the model to reason about and remove harmful content before producing final outputs. The authors provide theoretical analysis suggesting that such structured reasoning can concentrate harmful concepts into a more linearly separable subspace in the model’s internal representation space. Comprehensive experiments on several open-weight LLMs show that COCA enhances model's resistance to both in-distribution and out-of-distribution attacks while maintaining utility."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is easy-to-integrate in practice. Since it requires no system changes and only modifies the training data, COCA could be easily applied to existing fine-tuning framework.\n2. The paper provides a formal analysis demonstrating that COCA encourages concentration of harmful concepts into a linearly separable subspace.\n3. The authors present visualization result to illustrate the better internal separation after fine-tuning with COCA's data."}, "weaknesses": {"value": "1. The authors evaluate COCA primarily on medium-sized models (7B-9B). Including smaller (<=3B) and larger (>=14B) models would help assess COCA's generalizability.\n2. The paper claims that COCA is orthogonal to SRG and STAIR and could be combined with them, yet no experiments demonstrate this. Moreover, COCA’s reported performance in Table 1 does not show clear improvement over these baselines.\n3. COCA relies on five hardcoded tags. The paper does not clarify how these tags were defined or whether performance depends on their semantics or order. It remains unclear if changing, removing, or reordering tags would affect performance.\n4. The theoretical analysis models COCA’s data as a generic structured reasoning sequence and does not differentiate the role of individual tags. As a result, the theory does not explain how specific tags contribute to concept concentration. It is unknown that whether the effectiveness of COCA comes from its five-tag design or only from its longer reasoning context provided during the fine-tuning.\n5. COCA's main contribution is related to train the model to follow a specific reasoning pattern through fine-tuning. Therefore, comparisons with similar reasoning-based alignment methods, such as Deliberative Alignment [1], are necessary.\n6. The paper presents internal-state separability results only for COCA, while ignoring the baselines. Without this comparison, it is unclear whether the observed separation is unique to COCA or a general outcome of fine-tuning.\n7. COCA evaluates internal-state separability but does not use these states to classify or guide safety decisions. Higher separability in internal representations does not necessarily imply improved output safety and quality of the model.\n\n[1] Guan, Melody Y., et al. \"Deliberative alignment: Reasoning enables safer language models.\" arXiv preprint arXiv:2412.16339 (2024)."}, "questions": {"value": "Please check my questions in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ooG7O7ys1B", "forum": "8dSNMAEaym", "replyto": "8dSNMAEaym", "signatures": ["ICLR.cc/2026/Conference/Submission11789/Reviewer_Z6N3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11789/Reviewer_Z6N3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942105185, "cdate": 1761942105185, "tmdate": 1762922811602, "mdate": 1762922811602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}