{"id": "1yXsMYyZVj", "number": 5710, "cdate": 1757928310065, "mdate": 1763292570622, "content": {"title": "Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning", "abstract": "Supervised fine-tuning (SFT) plays a critical role for pretrained large language models (LLMs), notably enhancing their capacity to acquire domain-specific knowledge while preserving or potentially augmenting their general-purpose capabilities. However, the efficacy of SFT hinges on data quality as well as data volume, otherwise it may result in limited performance gains or even degradation relative to the associated baselines. To mitigate such reliance, we suggest categorizing tokens within each corpus into two parts---**positive** and **negative** tokens---based on whether they are useful to improve model performance. Positive tokens can be trained in common ways, whereas negative tokens, which may lack essential semantics or be misleading, should be explicitly forgotten. Overall, the token categorization facilitate the model to learn less informative message, and the forgetting process shapes a knowledge boundary to guide the model on what information to learn more precisely. We conduct experiments on well-established benchmarks, finding that this forgetting mechanism not only improves overall model performance and also facilitate more diverse model responses.", "tldr": "Supervised fine-tuning via forgetting enhances your LLM's generalization", "keywords": ["Large Language Models", "Supervised Fine-tuning", "Preference Fine-tuning", "Machine Unlearning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3b9620b625028cb4192fd8509f279e30f692dd3c.pdf", "supplementary_material": "/attachment/84214be995228bb531522a286720f22cce450ead.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a simple yet effective token-level forgetting mechanism for supervised fine-tuning (SFT) of large language models. The method identifies “positive” and “negative” tokens based on loss differences between a base model and a lightly fine-tuned reference model, and applies a modified loss function: maximizing likelihood for positive tokens while decreasing likelihood for negative ones, with an adaptive coefficient λ(step). Experiments on five benchmarks using several LLaMA variants (1B, 3B, 8B) demonstrate consistent improvements over standard SFT and sequence-level filtering approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Simplicity and clarity: The method is conceptually simple, easy to implement, and requires no additional labeling or complex architecture changes.\n\nInnovative data generation: Automatically deriving token quality scores via model-to-model loss differences is a clever and generalizable idea, potentially applicable to diverse SFT or preference-optimization tasks.\n\nEmpirical improvements: Consistent performance gains across multiple LLaMA sizes and benchmarks, showing robustness to model scale.\n\nPractical benefit: The approach can serve as a lightweight alternative to preference optimization, offering better noise control during fine-tuning without needing pairwise preference data."}, "weaknesses": {"value": "[Limited novelty relative to prior work]\nWhile the paper presents a clean formulation, its conceptual link to existing negative preference optimization (NPO) and token-level preference learning methods is not deeply analyzed.\nFor instance, TNPO (Token-level Negative Preference Optimization, Xu et al., 2024) also applies token-level weighting to downscale harmful or low-quality regions in text, and Unlearning approaches (e.g., Thakkar et al., 2024) similarly reduce log-likelihood for specific token spans.\nThe paper should clearly articulate how its influence-based token labeling differs fundamentally—is the improvement from the loss design itself, or from the way token labels are generated?\n\n[Model diversity limitation]\nAll experiments use only LLaMA-based models. This narrow scope limits confidence in the method’s generality; it remains unclear whether the forgetting mechanism would generalize to different architectures (e.g., Qwen, Mistral, GPT-OSS) or even decoder–encoder models (T5, UL2).\n\n[Lack of qualitative or diagnostic analysis]\nThe paper does not provide examples or visualization of which tokens are labeled negative, nor does it discuss possible biases introduced by the loss-based scoring."}, "questions": {"value": "Clarify the conceptual delta from NPO/TNPO/unlearning-based fine-tuning and include controlled comparisons using matched compute and data.\n\nExtend experiments to non-LLaMA architectures to test generality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9TAwf80LmW", "forum": "1yXsMYyZVj", "replyto": "1yXsMYyZVj", "signatures": ["ICLR.cc/2026/Conference/Submission5710/Reviewer_zLGE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5710/Reviewer_zLGE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656208648, "cdate": 1761656208648, "tmdate": 1762918210292, "mdate": 1762918210292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "pAmQF9TrD5", "forum": "1yXsMYyZVj", "replyto": "1yXsMYyZVj", "signatures": ["ICLR.cc/2026/Conference/Submission5710/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5710/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763292569617, "cdate": 1763292569617, "tmdate": 1763292569617, "mdate": 1763292569617, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel token-level \"forgetting\" mechanism for SFT, where tokens are classified as \"positive\" or \"negative\" based on their influence scores derived from a reference model. While positive tokens are learned via standard gradient descent, negative tokens are actively \"unlearned\" through gradient ascent on their loss."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is easy to follow and read."}, "weaknesses": {"value": "1. The method applies gradient ascent on negative tokens, which may distort contextual representations and disrupt the internal consistency of sequential predictions. This could degrade fluency, coherence, or generalization, even if downstream task metrics improve. It needs more experiments to analyze the impact of the loss designed in the paper not only on the final metrics but also the training dynamics.\n\n2. The influence-based token scoring relies on a reference model trained on a potentially noisy subset of data. Errors or biases in this reference model may misclassify high-quality tokens as negative, leading to harmful unlearning of useful information.\n\n3. The evaluation is limited to only three LLaMA-3 variants and a narrow set of baselines (e.g., full SFT and a simple token masking baseline). It omits comparisons with established data filtering or other related baselines, weakening the claimed advantage. While, there are also various benchmarks to evaluate the general-purpose datasets (such as instruction-following, AlpacaEval, Arena-Hard) instead of these QA benchmarks.\n\n4. The proportion of tokens classified as positive vs. negative (which depends on the thresholding parameter ρ and the influence score distribution) is not disclosed. Without this, it is difficult to assess the scale of the \"forgetting\" effect, reproduce the method, or understand whether performance gains stem from aggressive data reduction rather than the forgetting mechanism itself."}, "questions": {"value": "I still have concerns about the token-level gradient ascent and would appreciate additional analytical experiments to better understand its specific effects."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lRH5EbOOOt", "forum": "1yXsMYyZVj", "replyto": "1yXsMYyZVj", "signatures": ["ICLR.cc/2026/Conference/Submission5710/Reviewer_A1AE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5710/Reviewer_A1AE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817791239, "cdate": 1761817791239, "tmdate": 1762918209722, "mdate": 1762918209722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a token-level “forgetting” mechanism for supervised fine-tuning (SFT) of LLMs. Each token is classified as positive or negative via a small proxy model; negative tokens are down-weighted with a reversed loss term. Experiments on two commonsense QA benchmarks show +1.3 % accuracy and +5.8 % diversity vs. standard SFT. The idea is intuitive and timely, but the submission suffers from incomplete evaluation, and weak baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The research problem is interesting, and mitigating noisy SFT data is relevant to the community.\n2.  Experiments on well-established benchmarks, show that this forgetting mechanism is promising."}, "weaknesses": {"value": "1. The empirical experiment is not enough, lacking baseline methods mentioned in related works. Only one table shows the main result in the whole paper, which makes the experiment insufficient. Can you use some exploratory figures to show the effectiveness of your method?\n\n2. The presentation is poor and the method part is hard to follow. For instance, Eq. (5) and Eq. (8) are hard to understand. Why do you set lambda(step) in that way?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "599n8b5rUz", "forum": "1yXsMYyZVj", "replyto": "1yXsMYyZVj", "signatures": ["ICLR.cc/2026/Conference/Submission5710/Reviewer_Y6Si"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5710/Reviewer_Y6Si"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837647448, "cdate": 1761837647448, "tmdate": 1762918209427, "mdate": 1762918209427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}