{"id": "qQuca6fbFX", "number": 11529, "cdate": 1758200954799, "mdate": 1763178204567, "content": {"title": "FDVLA: A Flow-Diffusion Vision-Language-Action Framework with Dual Reasoning Modulation", "abstract": "Recent advances in vision-language models (VLMs) have empowered robots to interpret natural language and perform complex manipulation tasks. Existing vision-language-action (VLA) frameworks typically adopt autoregressive decoding or diffusion-based strategies. While the former may lead to fragmented or less smooth trajectories, the latter often lacks explicit injection of reasoning semantics into the action generation process, which can affect the quality of generated actions. In this paper, we propose FDVLA, a unified framework integrating semantic reasoning with smooth and physically coherent action generation. We introduce a flow-diffusion mechanism that unifies global trajectory planning (via flow fields) and fine-grained action refinement (via diffusion) in a dual-headed policy, enabling physically coherent and stable action generation. Additionally, we design DualMod, a lightweight module that injects semantic signals into both velocity and noise prediction branches, thus integrating high-level reasoning into action generation. Extensive experiments across diverse simulated and real-world robotic tasks, demonstrate that FDVLA achieves solid performance, efficient inference, and shows robust generalization under a variety of task conditions.", "tldr": "FDVLA unifies flow-guided planning and diffusion-based correction with semantic reasoning to enable coherent and controllable robotic action generation.", "keywords": ["Vision-Language-Action", "Robotic Manipulation", "Diffusion Policy", "Flow Matching", "Reasoning Modulation", "Trajectory Generation", "Multimodal Learning", "End-to-End Policy"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/153a32c6ada1be98b3f170b8ee54a96578b42da1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a flow-diffusion VLA framework for robotic manipulation, aimed at achieving both physically coherent and stable action generation. Additionally, a lightweight DualMod module is proposed to inject semantic signals into both the velocity and noise prediction branches, facilitating the integration of high-level reasoning into action generation. Experimental results demonstrate that the proposed method significantly outperforms state-of-the-art (SoTA) methods in both simulated and real-world robotic tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method exhibits excellent performance compared to state-of-the-art (SoTA) methods in both simulation and real-world tasks.\n\n2. The ablation study effectively validates the design of each component within the framework."}, "weaknesses": {"value": "1. Lack of Experimental Support for High-Level Reasoning: The authors claim that DualMod integrates high-level reasoning into action generation; however, there is insufficient experimental evidence to support this assertion. There is also a lack of discussion regarding the reasoning vector ( r ) and its impact on action generation.\n\n2. Limited Novelty: The novelty of the approach appears limited, as the combination of flow matching with diffusion policy is not new and has been extensively explored, such as in [a]. According to [a] and the original diffusion policy paper, the performance of the diffusion policy on the Push-T task has achieved over 0.9 in success rate (SR). In contrast, this paper reports a performance of 0.788 on the same task, raising questions about the validity of the results.\n\n[a] Jung, Chanhyuk, Sangwon Kim, Kwang-Ju Kim, Dasom Ahn, Joonki Baek, Sungkeun Yoo, and Byoung Chul Ko. \"Flow-Guided Policies: Overcoming Diffusion Limitations for Robust Robot Imitation Learning.\" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2486-2491. 2025.\n\n3. Table 4 indicates that the effects of DualMod on the flow branch and the denoising branch are identical. Could the authors provide a more insightful analysis regarding this observation?\n\n4. In Equation 1, it appears that ( z ) in the first term should be ( e_{\\text{gt}} ), as this term calculates the noise prediction loss. Typically, z denotes the latent code."}, "questions": {"value": "1. Does the proposed method finetune the model for each tasks?\n\n2. Are FDVLA-3B, 7B, 32B trained with the same size of data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZogUrAfvRj", "forum": "qQuca6fbFX", "replyto": "qQuca6fbFX", "signatures": ["ICLR.cc/2026/Conference/Submission11529/Reviewer_PYef"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11529/Reviewer_PYef"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657083217, "cdate": 1761657083217, "tmdate": 1762922624511, "mdate": 1762922624511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper seeks to improve VLAs via two innovations. First, they use both a flow-matching head that produces smooth trajectory for global planning and a diffusion head that further refines the trajectory for more fine-grained control. This leads to smoother trajectories that also achieve higher success rates. Secondly, they propose to inject language conditioning via FiLM (feature-wise linear modulation) to both of the action heads, leading to stronger grounding and higher success rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem studied is important, and the proposed solution is interesting and well-grounded. \n- The paper is well-written and logical.\n- Quantitative results are fairly comprehensive, including results also on real-world, ablation and inference speeds. \n- Performance gain is significant."}, "weaknesses": {"value": "- There's no qualitative results (e.g., videos) that corroborate the claim that the combination of flow-matching and diffusion heads lead to smoother trajectories. Similarly, there's no such video for the stronger semantic grounding (e.g., a video showing a cluttered environment where the baseline fails to retrieve the correct object would be nice). \n- The baseline comparison might not be strong enough. Please consider comparing against stronger models such as Pi0-Fast, Pi05. In fact, the discussion of this paper reminds me of these baselines. In those works, they also found that flow-matching on top of a VLM can lead to weaker semantic grounding and weaker performance so they co-train the flow-matching with the discrete autoregressive prediction in Pi05."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xEeL5KQ1uH", "forum": "qQuca6fbFX", "replyto": "qQuca6fbFX", "signatures": ["ICLR.cc/2026/Conference/Submission11529/Reviewer_rhYF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11529/Reviewer_rhYF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934933583, "cdate": 1761934933583, "tmdate": 1762922624098, "mdate": 1762922624098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FDVLA, a unified Flow–Diffusion Vision-Language-Action framework that tightly couples high-level semantic reasoning from large vision-language models with low-level continuous action generation. The key innovation lies in the integration of flow matching and diffusion denoising within a single policy architecture—where the flow branch models global, physically consistent velocity fields, and the diffusion branch refines actions via denoising. A dedicated DualMod module further injects reasoning signals from language semantics into both branches, aligning coarse trajectory planning and fine-grained corrections under shared semantic guidance. Extensive experiments across  simulation and real-world single-/dual-arm manipulation tasks demonstrate that FDVLA consistently outperforms leading VLA and policy-head baselines in both success rate and trajectory smoothness, while maintaining efficient inference."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a mathematically coherent unification of flow matching and diffusion denoising within a dual-headed policy. The flow branch captures global, physically consistent motion fields, while the diffusion branch refines local details through stochastic denoising. The introduction of a “flow-consistency” constraint effectively couples the two processes, ensuring stable dynamics and complementarity between deterministic and stochastic generation.\n2. Comprehensive Empirical Validation across Simulation and Real-World Benchmarks: FDVLA demonstrates consistent and notable improvements over mainstream VLA frameworks (e.g., OpenVLA, pi_0, ) and policy-head （Diffusion Policy, ACT, ARP) baselines across a variety of simulation  and real-world single-/dual-arm manipulation tasks. These results substantiate the framework contribution across diverse control conditions.\n3. Improved Motion Smoothness through Flow–Diffusion Coupling: As evidenced by Table 3, the integration of flow matching and diffusion denoising yields measurable gains in trajectory smoothness, quantified by lower jerk metrics without sacrificing task success. This indicates that the hybrid formulation  produces more stable, physically coherent motions that is critical for real-world manipulation."}, "weaknesses": {"value": "While the paper demonstrates clear improvements on Push-T, ALOHA, and RLBench, these simulation environments are somewhat dated and less representative of current VLA evaluation standards. Incorporating more recent and widely adopted benchmarks—such as LIBERO or CALVIN—and comparing against a broader range of modern VLA frameworks would further substantiate the generality of the proposed approach.\n\nIn the real-world experiments, the paper lacks supplementary materials showing continuous multi-rollout executions. Providing longer, uncut video sequences or detailed rollout logs would better validate the reported stability and smoothness.\n\n3 Many diffusion-based action heads improve temporal smoothness through intra-chunk ensembling. The paper does not explicitly compare FDVLA’s continuity advantages against these approaches.\n\nThe visualizations presented in the paper could be improved. Many of the current figures appear somewhat coarse or low in visual fidelity"}, "questions": {"value": "In Table 2, the results for RVT-2 and ARP are missing without explanation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TPd108MfEI", "forum": "qQuca6fbFX", "replyto": "qQuca6fbFX", "signatures": ["ICLR.cc/2026/Conference/Submission11529/Reviewer_hqxi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11529/Reviewer_hqxi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965486993, "cdate": 1761965486993, "tmdate": 1762922623656, "mdate": 1762922623656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FDVLA, a unified Flow–Diffusion VLA framework designed to integrate global motion planning and fine-grained action refinement within a single policy. The approach aims to combine the complementary strengths of flow-based and diffusion-based modeling.\n\nThe flow component models global, deterministic action trajectories as a velocity field that evolves actions toward target states. It captures coarse, long-horizon structure and is intended to stabilize trajectory generation. The diffusion component provides fine-scale, stochastic corrections to the flow predictions through a denoising process, improving action smoothness and variability. During inference, both components are integrated in a DDIM-style update procedure that combines deterministic flow guidance with residual diffusion refinement.\n\nTo incorporate semantic context, FDVLA introduces Dual Reasoning Modulation (DualMod), which injects reasoning embeddings from a pretrained Qwen 2.5-VL vision-language model into both the flow and diffusion branches. This modulation is implemented through FiLM-style feature scaling and shifting, allowing the policy to adapt its action generation to task-level textual and visual cues.\n\nThe architecture includes a SigLIP, Qwen 2.5-VL, and a transformer-based policy head with dual output heads for flow and diffusion predictions. The training procedure consists of two stages:\n- Pretraining on large-scale multimodal robot datasets: Droid and Open X Embodiment.\n- Finetuning on downstream datasets, with frozen visual features and partially frozen language features.\n\nFDVLA is compared against several prior baselines: pi-0 and OpenVLA (finetuned similarly), and Diffusion Policy, ACT, RVT-2, and ARP (trained from scratch). Evaluation covers both simulation and real-world manipulation tasks: FDVLA achieves higher performance across all settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "### Ambitious unification of flow and diffusion modeling.\nThe paper attempts to merge two influential paradigms, flow matching and diffusion denoising, into a single unified architecture for VLA action head. The motivation is that flow-based models offer deterministic, globally consistent trajectories, while diffusion models provide stochastic robustness and fine-grained correction. Bridging continuous ODE-based and noise-based policy families is a meaningful direction that could inspire future hybrid models.\n\n---\n\n### Extensive experimental evaluation.\nThe empirical evaluation covers a wide range of domains: simulation (Push-T, RLBench, ALOHA) and several real-world robotic manipulation tasks. The reported performance consistently improves upon both pretrained and from-scratch baselines, suggesting that the proposed system design provides practical benefits.\n\n---\n\n### Ablation and component analysis.\nThe paper includes ablation studies isolating the impact of the flow term, the diffusion term, and the DualMod module. Removing either modulation reduces success rates, indicating that the injected semantic embeddings indeed contribute to performance. Similarly, turning off the flow component or the diffusion refinement degrades model performance. While these ablations do not fully validate the design choice, they do show that each major module has a measurable functional contribution to empirical performance.\n\n---\n\nThe implementation details are relatively thorough. The authors specify model scales, training data composition, optimization parameters, and hardware setups."}, "weaknesses": {"value": "### Inconsistent temporal variables\nThe paper uses three time-like symbols, $T$, $t$, and $\\tau$, without explicit definitions or consistent usage. For example, \"Given a noisy action input $A_t^{\\tau}$ at timestep t\": what timestep is this? The index in an action chunk? I don't fully understand the exact temporal dynamics or how the flow and diffusion components are coupled in either training or inference.\n\n---\n\n### Flow-matching formulation\nI have some doubts in the paper’s formulation of the flow-matching objective. Equation (1) defines the \"flow term\" as a squared error between the predicted velocity field and $(A_0 - A_t)/(T - t)$, which the authors describe as the ground-truth velocity field. Why is this the case? In standard flow matching, the reference velocity is derived from the time derivative of an interpolation between endpoints, typically $s’(\\tau)(A_0 - A_T)$, where $s(\\tau)$ is a smooth scheduling function. It depends on the endpoints and the schedule derivative, not on the current state $A_t$. The proposed formulation instead introduces a state-dependent term, and the denominator $(T - t)$ introduces a singularity as $t \\to T$.\n\nIn equation 6, The gradient $\\nabla_{A_t}\\epsilon_\\theta(A_t, t)$ should be a Jacobian matrix. How can it be equated to a velocity vector?\n\n---\n\n### Ambiguity in architectural design\nFDVLA uses both SigLIP and Qwen 2.5-VL as vision encoders, but the paper does not explicitly clarify this dual-encoder setup.\nSigLIP is described as the observation encoder producing visual tokens, while Qwen 2.5-VL internally includes its own vision backbone. It is unclear how the model functions with 2 vision heads."}, "questions": {"value": "See above in the weaknesses. Please clarify:\n- What exactly do the time variables represent?\n- How is the velocity field defined?\n- What vision heads does the model actually use?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "61x65nhvYv", "forum": "qQuca6fbFX", "replyto": "qQuca6fbFX", "signatures": ["ICLR.cc/2026/Conference/Submission11529/Reviewer_wZMw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11529/Reviewer_wZMw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762657356898, "cdate": 1762657356898, "tmdate": 1762922623340, "mdate": 1762922623340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}