{"id": "m2rgUNmnDI", "number": 24794, "cdate": 1758360387694, "mdate": 1763016295843, "content": {"title": "Depth-consistent Motion Blur Augmentation", "abstract": "Motion blur is a ubiquitous phenomenon commonly encountered in lightweight, handheld cameras. Addressing this degradation is essential for preserving visual fidelity and ensuring the robustness of vision models for scene understanding tasks. In the literature, robustness to motion blur has been generally treated like other degradations; this despite the complex space-variant nature of motion blur due to scene dynamics and its inherent dependence on scene geometry and depth. While some recent works addressing this issue have introduced space-variant blur due to scene dynamics, they fall back on space-invariant blurring to model camera egomotion which is imperfect. This work proposes an efficient methodology to generate space-variant depth-consistent blur to model camera egomotion by leveraging depth foundation models. We refer to our approach as Depth-consistent Motion Blur Augmentation (DMBA). To demonstrate the effectiveness of DMBA in improving robustness to realistic motion blur, we provide experiments for the tasks of semantic segmentation and self-supervised monocular depth estimation. We include results for standard networks on the Cityscapes dataset for semantic segmentation and the KITTI dataset for monocular depth estimation. We also illustrate the improved generalizability of our method to complex real-world scenes by evaluating on commonly used datasets GoPro and REDS that contain real motion blur.", "tldr": "", "keywords": ["Motion Blur", "Augmentation", "Segmentation", "Depth estimation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/8089e34778f12802a3d6a0ee95a9146db64b70ed.pdf", "supplementary_material": "/attachment/943bdd292d0f330a459954fd797e392ec6d95de2.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Depth-consistent Motion Blur Augmentation (DMBA) to model the motion blur. It consists of two parts: the 3D motion trajectory generation and the depth-consistent motion blur. Experiments on two tasks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a 3D trajectory generation pipeline guided by depth to model the motion blur, which involves 6 DoF to simulate camera motion. The experiments show the superiority of the proposed method."}, "weaknesses": {"value": "1. The key ideas of the proposed method lie in the 3D motion trajectory and the depth estimated by the Depth Anything model. However, the 3D motion is not new and is widely discussed in traditional computer graphics. Besides, the depth is predicted by the existing pretrained model. Thus, the paper seems to lack novelty. A clarification of novelty is encouraged.\n2. The introduced \"depth-consistent\" is confusing. What is the role of depth in the method? And, in Eq. 2, the \"proj(.)\" involves depth as an input. Both the detail and the motivation are missing.\n3. Sec. 3.2.2 notes that the existing depth groundtruth maps are sparse and the Depth Anything model provides better results. An ablation study is supposed to validate this point, yet it is not included in the current paper."}, "questions": {"value": "1. The method seems to overlook the local motion of objects. How can it handle such scenes with local motions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aqkZMQpWVj", "forum": "m2rgUNmnDI", "replyto": "m2rgUNmnDI", "signatures": ["ICLR.cc/2026/Conference/Submission24794/Reviewer_YMgh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24794/Reviewer_YMgh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761221324366, "cdate": 1761221324366, "tmdate": 1762943199840, "mdate": 1762943199840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "lqTLeZjWZg", "forum": "m2rgUNmnDI", "replyto": "m2rgUNmnDI", "signatures": ["ICLR.cc/2026/Conference/Submission24794/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24794/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763016295054, "cdate": 1763016295054, "tmdate": 1763016295054, "mdate": 1763016295054, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to generate realistic motion blurred images by considering the different depth of different pixels, thus the generated images are with realistic camera motion and depth consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The reported experiment results are marginally improved."}, "weaknesses": {"value": "1. The contribution is very limited. The conventional methods generate the motion blurred images by averaging a sequence of images in a video, which is efficient, motion realistic, and depth consistent. The proposed method actually perform the same thing.\n\n2. The explanation of Eq.(1) is missed and confusing. What is the physical meaning for multiplying jerk and velocity? How about the subtraction between the acceleration and pose?\n\n3. There is no technique contribution from this work."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yJ8Ob8Z8cX", "forum": "m2rgUNmnDI", "replyto": "m2rgUNmnDI", "signatures": ["ICLR.cc/2026/Conference/Submission24794/Reviewer_9jTg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24794/Reviewer_9jTg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673991055, "cdate": 1761673991055, "tmdate": 1762943199489, "mdate": 1762943199489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Depth-consistent Motion Blur Augmentation (DMBA), that leverages depth foundation models to generate realistic, space-variant, depth-aware motion blur for modeling camera egomotion. Unlike prior space-invariant approaches, DMBA produces motion blur that aligns with scene geometry, thereby enhancing robustness in semantic segmentation and monocular depth estimation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Performance: The proposed DMBA is not only effective on its own but also demonstrates strong complementarity with existing augmentation methods such as CCMBA, leading to further performance improvements when the two are combined. The approach is conceptually clear, computationally efficient, and broadly applicable across vision tasks requiring robustness to motion blur."}, "weaknesses": {"value": "1. Deformation Blur: The method primarily handles motion blur induced by camera or rigid-object motion, without accounting for non-rigid deformations (e.g., folding fingers, facial movements) where object shape changes over time. As a result, DMBA may fail to model blur from deformable or articulated motion.\n\n2. Reference: Some related works are missing from the discussion. Since the main contribution involves using 3D trajectories informed by depth, references such as * should be cited and compared.\n\n*Controllable Blur Data Augmentation Using 3D-Aware Motion Estimation (ICLR 2025) \nGS-Blur: A 3D Scene-Based Dataset for Realistic Image Deblurring (NeurIPS 2025)"}, "questions": {"value": "1. The number of sharp frames (n) used to synthesize motion blur may play a crucial role in realism and robustness. Could the authors include an ablation study exploring how varying n affects performance and the fidelity of generated blur?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UC9ygjkiX8", "forum": "m2rgUNmnDI", "replyto": "m2rgUNmnDI", "signatures": ["ICLR.cc/2026/Conference/Submission24794/Reviewer_eTca"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24794/Reviewer_eTca"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812621597, "cdate": 1761812621597, "tmdate": 1762943199227, "mdate": 1762943199227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to augment standard vision datasets (e.g. for segmentation or monocular depth estimation) to include synthetic camera blur, The standard training datasets are augmented using the proposed strategy, and the results show that the achieved scores outperform standard settings, Improved performance is also shown on real-world datasets, such as GoPro and REDS."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-written and the ideas are sound.\n* The proposed strategy indeed shows slightly improved scores on several benchmark datasets."}, "weaknesses": {"value": "* Motion blur is considered only for camera motion, but not for object motion or for changing intrinsics blur.\n* It's not specified how the depth maps are scaled to metric depth.\n* Some works that are mentioned as recent are from 2021, which is in terms of deep learning work not so recent."}, "questions": {"value": "Would it be possible to also add other types of motion blur?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oH2OqIoJkw", "forum": "m2rgUNmnDI", "replyto": "m2rgUNmnDI", "signatures": ["ICLR.cc/2026/Conference/Submission24794/Reviewer_pw8S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24794/Reviewer_pw8S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986348939, "cdate": 1761986348939, "tmdate": 1762943198966, "mdate": 1762943198966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}