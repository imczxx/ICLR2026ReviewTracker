{"id": "3lKAoqNXle", "number": 11139, "cdate": 1758190519365, "mdate": 1758954973424, "content": {"title": "O²-CritiCuRL: Offline-Online Step-aware Curriculum Reinforcement Learning for Visual Reasoning", "abstract": "Large language models demonstrate strong capabilities in reasoning tasks, yet they frequently generate flawed intermediate reasoning steps while still arriving at correct final answers. \nSuch behavior raises concerns about interpretability and reliability, as it suggests reliance on spurious shortcuts rather than faithful reasoning. \nExisting attempts to incorporate step-level supervision are limited by long, redundant trajectories that burden optimization and obscure decisive reasoning steps.\nWe propose O²-CritiCuRL, a novel curriculum reinforcement learning framework that explicitly models critical-step awareness through an iterative offline–online training paradigm.\nIn the offline stage, O²-CritiCuRL decomposes chain-of-thought trajectories and employs a step-level reward to automatically identify decisive steps while down-weighting redundant ones, followed by restructuring trajectories into difficulty tiers for curriculum learning. \nIn the online stage, we introduce a progressive step-level reinforcement learning strategy, where truncated reasoning chains encourage the model to infer missing steps and refine its reasoning process. \nThese two stages are coupled through an iterative offline–online mechanism, enabling the model to progressively improve its focus on critical steps and overcome the limitations of static supervision.\nExtensive experiments across multiple reasoning benchmarks demonstrate that our method achieves the state-of-the-art performance.", "tldr": "", "keywords": ["Large Language Model", "Reasoning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/27472e0b061cb67a726824b067366a688043206c.pdf", "supplementary_material": ""}, "replies": [], "withdrawn": true}