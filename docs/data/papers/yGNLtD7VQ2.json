{"id": "yGNLtD7VQ2", "number": 8982, "cdate": 1758105631824, "mdate": 1759897750108, "content": {"title": "Exposing Vulnerabilities in Latent-Noise Diffusion Watermarks", "abstract": "Watermarking techniques are vital for protecting intellectual property and preventing fraudulent use of media.\nRecently, a prominent approach to watermarking diffusion models relies on embedding a secret key in the initial noise. The resulting pattern is often considered hard to forge into unrelated images and remove. In this paper, we make a key observation that there is an inherent many-to-one mapping between images and initial noises. Therefore, there are regions in the clean image latent space pertaining to each watermark that get mapped to the same initial noise when inverted. We expose this as a vulnerability by proposing a black-box adversarial attack using only a single watermarked image and without presuming access to any diffusion model. Our forgery attack simply adds perturbations to unrelated, potentially harmful images so that they would enter the region of watermarked images and get falsely labeled as watermarked. We show that a similar approach can also be applied to watermark removal by learning perturbations to exit this region. We report results on multiple watermarking schemes (Tree-Ring, RingID, WIND, and Gaussian Shading). Our results demonstrate the effectiveness of the attack and expose vulnerabilities in current watermarking methods, motivating future research on improving them.", "tldr": "", "keywords": ["Diffusion Watermarks", "Adversarial Attack", "Watermark Forgery Attack", "Watermark Removal Attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ca3ff7ba581ab7bab6667a544ff6903e644ae836.pdf", "supplementary_material": "/attachment/4526466c04c4b10692c5bc20983db89547bf90a7.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents an elegant method for watermark forgery and removal that only requires access to a single watermarked image and a VAE trained on a dataset similar to the diffusion model used to generate the images. To forge a watermark, their method optimizes the noise added to the clean image, minimizing the distance between the clean image and a watermarked image in the VAE’s latent space. This process works because of a many-to-one mapping between images and initial noise latents. The procedure for removing a watermark is the same, except that noise is added to the watermarked image instead of the clean image."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is simple and elegant.\n2. Compared to previous work, it only needs access to a VAE, rather than the full diffusion model.\n3. The motivating result is analyzed well and provides a strong basis for the paper.\n4. The figures that explain and describe the idea are well put together."}, "weaknesses": {"value": "1. With a simple, principled idea, the focus should be on demonstrating that it works well in a broad range of scenarios. The experiments are too shallow and lack analysis and reflection.\n2. The writing quality could be improved\n3. Too much in the appendix.\n  3.a. All of the RingIID and WIND results are there; at least some of them should be in the main part of the paper. \n  3.b. There is no comparison against previous work on these two defenses either.\n4. The results in Tables 1 and 2 are not discussed enough. Yang et al (2024a) having a 0% ASR for all results in Table 1 needs to be addressed. Likewise, the poor performance of Yang et al. (2024a) and Zhao et al. (2025) in Table 2 also needs to be addressed.\n5. You cannot put all the results related to RingIID-type methods in the appendix, but then discuss them in the discussion section. Without the appendix, this part of the discussion does not make sense."}, "questions": {"value": "Questions:\n1. Following some of my comments in the weaknesses section: Can you explain why Yang et al (2024a) and Zhao et al. (2025) perform so poorly? Is it due to the experiment setup you are using? Could you also provide results where they perform well? If not, how are they a good basis for comparison?\n2. Your method is $1.46\\times$ to $2.62\\times$ worse than Muller et al.’s work in the LPIPS metric. Why is lowering the $l_2$ distance at the cost of the LPIPS distance (and some ASR) beneficial?\n3. For the “Results - Computational Time” section, which attack do you evaluate, the forgery or the removal attack? Why are you only comparing against Muller et al.’s work and not any of the other attacks you include?\n4. When describing Lukas et al.’s work, you say they assume access to a copy of the generator, but that’s not true. Similarly to your experiments, they assume access to an older version of Stable Diffusion (v 1.1) to attack a newer version (v 2.0).\n\nSuggestions:\n1. Including the p-values for the examples of your attacks provided in Figures 5 and 6 would be interesting. \n2. Figure 3 has a low resolution.\n3. In Section 3.1, you explain that the inversion process does not use a text prompt because “the model owner typically does not keep track of the generated images or the prompts used”. A better justification could be that images found in the wild, for which we want to establish ownership, do not necessarily come with the prompt used to generate them, especially if they are clean, non-generated images.\n4. In your abstract, you claim your method does not need access to any diffusion model; that is not true since you need access to the VAE part of the diffusion model. A clearer claim is that you do not need any denoising model.\n\nWriting:\n1. Excessive usage of “This” to start sentences without specifying what “This” refers to.\n2. Inconsistent terminology: using either “clean” image or “non-watermarked” image, pick one and stick to it.\n3. When describing the threat model in Section 4.1, it is unclear what is being formalized: the parties or the various phases? It appears to be the various phases, but if that’s the case, it should be described as such, and the phases should be the focus, rather than in parentheses.\n4. In the attacker’s threat model bullet point in Section 4.1, it should be included that they have access to either the same VAE as the defender or one trained on similar data.\n5. You switch between different times, one example is the end of Section 4.2.\n6. The bolding and underscoring in Tables 1 and 2 are not consistent.\n7. Space missing between “keys.” and “We” in the “Evaluation metrics.” paragraph in Section 5."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Awhz595Lpv", "forum": "yGNLtD7VQ2", "replyto": "yGNLtD7VQ2", "signatures": ["ICLR.cc/2026/Conference/Submission8982/Reviewer_PQqk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8982/Reviewer_PQqk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760646626172, "cdate": 1760646626172, "tmdate": 1762920713332, "mdate": 1762920713332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores vulnerabilities in diffusion latent-domain watermarks under adversarial removal/forgery. The adversarial attack objective is to let the VAE-encoded latent of an image to be as close as possible to the VAE latent of another image, so that subsequent DDIM inversion-based watermark detection process would be obfuscated and cannot reliably detect watermarks. The authors demonstrated effectiveness of this attack on certain methods in terms of both removal and forgery, and demonstrated superiority compared to existing watermark removal/forgery methods in terms of effectiveness and certain perceptual quality metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe authors successfully demonstrated vulnerabilities on certain diffusion latent-domain watermarks by showing the effectiveness of the attack method.\n-\tThe method achieves watermark removal performance comparable to some baseline methods without requiring access to the complete watermark detection pipeline. It only requires a proxy of the VAE involved. \n-\tThe method does not require a collection of multiple images before initiating watermark removal/forgery.\n-\tThe watermark removal method causes smaller modifications (PSNR) to the original image compared to baseline approaches."}, "weaknesses": {"value": "-\tThe proposed attack would only be effective with the presence of a VAE encoder in the watermark detection pipeline.\n-\tThe watermark removal fails for RingID and WIND watermarks. Furthermore, the authors’ explanation for such failure lacks sufficient rigor. The authors attributed this to RingID embedding “a watermark into the entire initial latent noise space”, which is not true, as RingID’s region of modification should be consistent with Tree-Ring’s approach. Given the drastically different removal performance on Tree-Ring and RingID, the authors should provide a more thorough investigation into the fundamental causes underlying these performance differences, so as to demonstrate that this removal method generalizes across latent-noise diffusion watermarks."}, "questions": {"value": "-\tFor multi-key watermarking methods: are experiments conducted using a single consistent watermark key, or are different keys used across different images?\n-\tAre the perturbations $\\delta$ bounded within an $l_\\infty$ epsilon-ball? If not, would it be possible that some pixels would be significantly altered?\n-\tIn Tables 1 and 2, the ASR for Yang et al. (2024a) is consistently near zero, constrasting sharply with the results reported in the original paper. What factors might be causing this discrepancy? \n-\tWhat are the mathematical definitions for the $l_2$ and $l_\\infty$ distances used in Tables 1 and 2? Additionally, please specify the pixel value ranges used in these computations (e.g., [0, 255], [0, 1], etc.)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1PzZEGRYcO", "forum": "yGNLtD7VQ2", "replyto": "yGNLtD7VQ2", "signatures": ["ICLR.cc/2026/Conference/Submission8982/Reviewer_E7z2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8982/Reviewer_E7z2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761372697647, "cdate": 1761372697647, "tmdate": 1762920713009, "mdate": 1762920713009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies black-box adversarial attacks against latent-noise watermarking for diffusion models by exploiting the many-to-one mapping induced by empty-prompt DDIM inversion. The core idea is to learn imperceptible perturbations that align a target image's VAE latent with that of a watermarked (or non-watermarked) reference, thereby inducing forgery or removal attacks. Experiments on Stable Diffusion v1.4 and v2.0 across multiple watermarking schemes report high attack success rates with modest distortion and lower runtime than baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The \"many-to-one\" latent-space mapping intuition is clear and insightful.\n- The proposed method is simple, sound and effective.\n- The paper addresses both forgery and removal within a unified framework"}, "weaknesses": {"value": "- Robustness and scope need clarification and strengthening. The evaluation is limited to two closely related models (Stable Diffusion v1.4 and v2.0). Please broaden the experiments to include more diverse backbones.\n\n- Assumptions about watermark type and attacker knowledge should be made explicit. The method targets latent-noise watermarks, which implicitly assumes the attacker knows the watermarking mechanism. This capability is not clearly stated in the threat model and limits the method's generalizability to other watermark types. Please clarify the attacker's knowledge and access assumptions and evaluate or discuss applicability beyond latent-noise watermarks.\n\n- It is unclear how the the attacking baselines in Table 2 are selected for comparison. They should also compare against some other advanced attacks such as UnMarker [1], VAE Attack [2], Diffusion Attack [3], etc. \n\nSome issues that need merited:\n\n- The transition from motivation to method is not tight. Perhaps visualizing the changes in image features before and after optimization can improve clarity.\n  \n- The definition of \"Watermark Region\" is introduced but not used later.\n  \n- The statement \"we do not assume any access to a denoising diffusion model or a proxy version of it\" is too strong. The approach requires a surrogate VAE derived from a diffusion model. Please revise the claim and explicitly state this requirement.\n\n[1] http://arxiv.org/abs/2405.08363\n\n[2] M. Saberi, V. S. Sadasivan, K. Rezaei, A. Kumar, A. Chegini, W. Wang, and S. Feizi, “Robustness of AI-image detectors: Fundamental limits and practical attacks,” in ICLR, 2024.\n\n[3] B. An, M. Ding, T. Rabbani, A. Agrawal, Y. Xu, C. Deng, S. Zhu, A. Mohamed, Y. Wen, T. Goldstein, and F. Huang, “WAVES: benchmarking the robustness of image watermarks,” in ICML, 2024."}, "questions": {"value": "- Can you expand the evaluation to include more diverse generative backbones?\n\n- Your proxy model SD 1.4 is from the same family of the watermarking model SD 2.0. Could you explain transferability of proxy models from other families? Otherwise, you have to put this \"same-family model\" assumption in your threat model, since you haven't tested other cases.\n\n- Can you clarify your selection of attacking baselines?\n\n- How does your method generalize beyond latent-noise watermarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HKJ1EG1siG", "forum": "yGNLtD7VQ2", "replyto": "yGNLtD7VQ2", "signatures": ["ICLR.cc/2026/Conference/Submission8982/Reviewer_rFDg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8982/Reviewer_rFDg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793239496, "cdate": 1761793239496, "tmdate": 1762920712655, "mdate": 1762920712655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper identifies a fundamental vulnerability in the class of latent noise based watermarking methods for diffusion models. \n- The main observation is that when the DDIM inversion process is performed with an empty prompt for detection, it creates a many to one \n   mapping from the clean image's latent space to the initial noise space.\n    - This causes structured and predictable watermarked regions within the VAE's latent space -  any image whose latent representation \n       falls in this region will be identified as containing the watermark.\n- A blackbox adversarial attack for both forgery and removal is proposed which exploits the above vulnerability.\n    - The formulated of the attack is an optimization problem whose goal is to add an imperceptible perturbation to a target image such that \n       its VAE latent representation shifts either into a watermarked region or out of it.\n- Experiments are done on four SOTA watermarking methods (Tree Ring, RingID, WIND, and Gaussian Shading). This attack is shown to be effective especially for forgery as it achieves almost perfect success rates while having much lesser visual distortion than previous works."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The discovered vulnerability in this paper is novel i.e. the identification of the watermarked region in the VAE latent space as a vulnerability and this discovery yields a new class of attacks which were ignored by previous works.\n- The proposed attack is practical and has minimal requirements. The attacker needs only a single watermarked image and does not require access to the diffusion model's proprietary components.\n     - This is further validated in the results in Table 8 where a proxy VAE from a very different model family FLUX1 is used.\n     - The proposed attack therefore is more threatening and relevant for real world systems than previous methods that need large datasets \n        of watermarked images or privileged access to model weights.\n- The paper contains extensive experiments in Section 5 and the Appendix 1 covering evaluations across 4 SOTA watermarking schemes and 2 SD versions that show how general the vulnerability is. \n     - This method has a stronger tradeoff between efficacy and stealth than the baselines. Ex: In Table 1 the forgery attack on Tree Ring SD achieves a 91.06% ASR with L2 of 63.22 whereas the baseline from Muller et al achieves 100% ASR but at more perceptible L2 of 15.22"}, "weaknesses": {"value": "- The proposed method is unable to remove watermarks from schemes like RingID and WIND where the ASR is 0% in all the experiments (Table 4). The authors provide the likely reason (Section 6) that these methods embed a very strong signal across the entire latent space making them difficult to push out of the watermarked region. This should be framed more explicitly as a weakness of the proposed attack method. The forgery attack works well but the paper's claims about a 'general' method for both forgery and removal are only partially supported by the experiments/evidence.\t\n- The SVM experiment (Section 4.2) is used to motivate the existence of a separable watermarked region. The experiment uses a dataset of 1000 images generated with the same key which is a strong assumption given the final attack's threat model of a single watermarked image. It would be good to clarify that this experiment is a conceptual proof of existence under idealized conditions which is different from the more practical single image attack case which is the main focus of the paper."}, "questions": {"value": "- The results in Table 4 show a 0% ASR for removing RingID and WIND watermarks which is attributed to the strength of their embedded signal. Does this suggest a fundamental limitation of latent space perturbation attacks against such methods? Or could the attack probably succeed with a different objective?\n- In the proxy VAE experiment in Table 8 the ASR for forging the Tree Ring watermark using the FLUX1 VAE is significantly lower 2.63% versus SD VAE 78.65% at a similar perturbation level. Could you explain the the potential reasons for this large discrepancy? Could a potential defense against this attack involve training a proprietary VAE whose latent geometry is dissimilar to those of popular open source models?\n- For the removal attack the perturbed image is guided towards the latent representation of a mean pixel image. Appendix D shows this is more effective than using a real image. Could you provide more intuition as to why this simple target is so effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "advh9BEoj7", "forum": "yGNLtD7VQ2", "replyto": "yGNLtD7VQ2", "signatures": ["ICLR.cc/2026/Conference/Submission8982/Reviewer_xCat"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8982/Reviewer_xCat"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216854881, "cdate": 1762216854881, "tmdate": 1762920712281, "mdate": 1762920712281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}