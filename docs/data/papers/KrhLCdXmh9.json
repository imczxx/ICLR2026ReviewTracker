{"id": "KrhLCdXmh9", "number": 12453, "cdate": 1758207940936, "mdate": 1762967623981, "content": {"title": "ObjLoc: Indoor Camera Relocalization based on Open-Vocabulary Object-Level Mapping", "abstract": "Indoor visual relocalization plays a key role in emerging spatial and embodied AI applications. However, prior research has predominantly focused on methods based on low-level vision. Despite notable progress, these methods inherently struggle to capture scene semantics and compositions, limiting their interpretability and interactivity. To address this limitation, we propose ObjLoc, a camera relocalization system designed to provide an intuition of scene object compositions and accurate pose estimation, which can be seamlessly reused in high-level tasks. Specifically, leveraging recent foundation models, we first introduce a multi-modal strategy to integrate open-vocabulary semantic knowledge for effective 2D-3D object matching. Additionally, we design an object-oriented reference frame and a corresponding retrieval strategy for pose priors, enabling extension to scalable scenes. To ensure robust and accurate pose optimization, we also propose a novel dual-path 2D Iterative Closest Pixel loss guided by object geometry. Experimental results demonstrate that ObjLoc achieves superior relocalization performance across various datasets. Our source code will be released upon acceptance.", "tldr": "", "keywords": ["visual camera relocalization", "object-level mapping", "open vocabulary"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7a3eebf2e86a8695b9e56bb11324e14708eff3e3.pdf", "supplementary_material": "/attachment/d75761277b63ba26279db42345d1e0a9b33d6c4f.zip"}, "replies": [{"content": {"summary": {"value": "ObjLoc is an open-vocabulary, object-level indoor camera relocalization system that integrates semantic object understanding, scene graph reasoning, and 2D–3D pose optimization to achieve scalable and interpretable localization. The system leverages open-vocabulary models (Florence2, CLIP) to detect and encode objects, and enhances semantic comprehension through text descriptions generated by GPT-4o. Meanwhile, it constructs a scene graph to model spatial relationships between objects. Localization is performed in a coarse-to-fine manner, where a DIoU-based strategy and a newly proposed dual-path 2D ICP loss are employed to align observed and projected object pixel regions, enabling accurate pose estimation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: ObjLoc proposes an open-vocabulary, object-level camera relocalization framework that integrates semantic understanding, geometric consistency, and scalable map construction at the system level. The introduced object-level map suite and dual-path 2D ICP loss represent a clear departure from traditional keypoint- or patch-based localization paradigms, demonstrating strong originality.\n\nQuality: The paper replaces full images with object-based reference frames, extracts features using CLIP, applies a DIoU-based strategy for coarse pose filtering, and employs a dual-path ICP for fine-grained pose optimization. These components are rigorously validated through comprehensive experiments, showing strong robustness and reliability.\n\nClarity: The paper is well-structured with clear motivation and a modular system design. The entire pipeline—from map construction and object association to pose refinement—is logically organized and easy to follow. Illustrations and qualitative results visually demonstrate the workflow and innovations, making the presentation clear and coherent.\n\nSignificance: The paper effectively addresses key challenges in object-level camera relocalization, such as low descriptor discriminability, lack of reliable pose priors, and ambiguity from center-point alignment. Its proposed solutions achieve superior performance over existing methods, offering both strong research significance and practical application potential."}, "weaknesses": {"value": "１.While the system demonstrates strong performance in terms of accuracy and scalability, the paper does not discuss computational efficiency in the experiments. The inference time of the open-vocabulary, object-level relocalization method is not reported.\n\n２.The paper employs object-level feature descriptors based on CLIP, but it does not compare against traditional feature-based or dense matching methods, which raises concerns about the credibility of the reported performance. \n\n３.The paper proposes using object-level reference frames as a replacement for RGB images to reduce memory consumption. However, it does not provide a comparative experiment to demonstrate whether this substitution leads to improved performance."}, "questions": {"value": "１.The system seems to rely heavily on the detection and encoding of objects via CLIP. How does ObjLoc perform when applied to scenes with a diverse set of object types, especially in environments with uncommon or rare objects that might not be well-represented in the model's training data?\n\n２.Compared with dense feature-based methods, does the object-level representation limit the localization accuracy?\n\n３.How does ObjLoc perform across different indoor benchmarks, such as 7Scenes? A comparison with existing state-of-the-art methods on these datasets would help validate the claims of superior performance.\n\n４.Can ObjLoc be extended to outdoor or larger-scale environments? Have any related tests or improvements been conducted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SGTIdqpkzu", "forum": "KrhLCdXmh9", "replyto": "KrhLCdXmh9", "signatures": ["ICLR.cc/2026/Conference/Submission12453/Reviewer_nktE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12453/Reviewer_nktE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546613100, "cdate": 1761546613100, "tmdate": 1762923332840, "mdate": 1762923332840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "7lsygVzQJ9", "forum": "KrhLCdXmh9", "replyto": "KrhLCdXmh9", "signatures": ["ICLR.cc/2026/Conference/Submission12453/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12453/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762967622854, "cdate": 1762967622854, "tmdate": 1762967622854, "mdate": 1762967622854, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed method builds a system to query the camera pose of an RGB image with a given 3D segmentation map. Given a sequence of posed RGB-D images, the author first clusters the 2D segmentation into 3D space to form a scene graph containing point cloud, bounding box and clip feature for each object. During camera relocalization, the Florence-2 and GPT-4o will be used to extract the visual cue and language cue from the query image. Then the camera will be relocalized by sub-graph matching between the query image and the 3D scene graph. The final pose is further refined by ICP."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The results look good."}, "weaknesses": {"value": "The paper is hard to follow. Furthermore, the novelty of the method is limited. It seems more suitable for a robotics conference like IROS and ICRA rather than ICLR. The author uses SAM, CLIP and Cropform to build the scene graph, while during relocalization, the author only uses Florence-2 and GPT-4o to extract features from the query image. Why not use SAM, CLIP and Cropform as well?\nComparisons with some baselines are missing, like DSAC, DSAC++, ACE, ACE0.\n\nMore detailed questions:\nLine 195, the description of the strategy of mask nodes is missing. The author should clarify whether it is the same as mask clustering.\nFig 3, The explanation of subgraph similarity calculation is missing. Furthermore, the term used in this section is ambiguous. If the subgraph similarity is calculated as Fig 3(a), it cannot be represented as \"compute the similarity of all possible neighbor pairs\". Especially in line 266, the author claims the subgraph has a length n. Also, the object candidate of $G_q$ is not defined, which indicates Fig 3(a) might be wrong. If  $G_q$  is the graph on the query image, the objects in Fig 3(a) would be outside the query images.\nSec 3.3, the coarse pose prior is derived from the pose of the reference frame with respect to the 2D box, which is sensitive to outliers.\nEqu 7b, the definition of $p_i$ is missing. The operation of ψis missing.\nIn experiments, the details of reference frame selection are missing.\nThe iteration number/optimizer of 2D ICP is missing.\nThe visualization of segmentation, Fig 10, is irrelevant to the proposed method."}, "questions": {"value": "Due the to limitations mentioned in the weaknesses, I tend to give negative recommendation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uTOLIB0Qxv", "forum": "KrhLCdXmh9", "replyto": "KrhLCdXmh9", "signatures": ["ICLR.cc/2026/Conference/Submission12453/Reviewer_JSej"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12453/Reviewer_JSej"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905435850, "cdate": 1761905435850, "tmdate": 1762923332470, "mdate": 1762923332470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ObjLoc, an object-level indoor camera relocalization framework that builds a compact, semantic 3D map using open-vocabulary object descriptors (CLIP), scene graphs, and object reference frames. At inference, ObjLoc performs vision–language matching for 2D–3D correspondence, retrieves a coarse pose via DIOU-based matching (to the reference frames), and refines it using a dual-path 2D ICP loss. Experiments on multiple indoor datasets demonstrate significant improvements in accuracy and recall while using smaller maps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Overall, the paper is well-written and clearly structured.\n\n2. Integrates vision–language matching, scene-graph context, and geometric optimization into a unified pipeline. The proposed dual-path 2D ICP loss and DIOU-based retrieval are elegant and well-motivated.\n\n3. Demonstrates strong robustness and scalability across multi-room and multi-floor indoor scenes.\n\n4. The object-oriented reference frame design leads to highly compact and storage-efficient maps.\n\n5. The authors include thorough ablation studies that help clarify the impact of key design choices."}, "weaknesses": {"value": "1. The use of multiple large models (CLIP + SAM + Florence2 + GPT-4o) results in slow runtime (6.0 seconds per frame) and large memory requirements, making real-time operation unrealistic.\n\n2. The authors did not compare their method to strong modern visual localization systems such as ACE or GLACE. Given their performance and relevance, such a comparison is necessary to provide context for the contribution.\n\n3. Since the method relies exclusively on object-level cues for localization, it is unlikely to perform well in dynamic environments (as briefly acknowledged in the limitations). This severely limits its practicality.\n\n4. The pipeline depends heavily on accurate instance segmentation and object detection; any segmentation or detection error can propagate through the entire system, leading to incorrect associations or failed relocalization. A robustness analysis against such errors is missing.\n\n- ACE: Learning to Relocalize in Minutes using RGB and Poses, CVPR 2023.\n- GLACE: Global Local Accelerated Coordinate Encoding, CVPR 2024."}, "questions": {"value": "1. I would highly appreciate a comparison to more modern and stronger visual localization baselines such as ACE and GLACE, as well as the addition of median rotation and translation errors as evaluation metrics (computed over all samples, not only those under the threshold)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8UydHaRKCf", "forum": "KrhLCdXmh9", "replyto": "KrhLCdXmh9", "signatures": ["ICLR.cc/2026/Conference/Submission12453/Reviewer_KRu6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12453/Reviewer_KRu6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995067734, "cdate": 1761995067734, "tmdate": 1762923332161, "mdate": 1762923332161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a visual localization pipeline using an object-level map. The mapping stage using RGB-D images creates a graph map, where each node is an open-vocabulary object defined by its 3D point cloud, corresponding keyframes, and CLIP-based descriptor. The localization stage first establishes correspondences between objects in the query and in the map, which are then used to estimate the query camera pose in a coarse-to-fine manner - starting from the pose of the best-matching keyframe and optimizing the object projection alignment using ICP. The main contribution of the paper is the overall design of the pipeline and a number of partial design choices (descriptor, retrieval, ICP), which might be interesting for the design of future object-level localization pipelines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an interesting problem. In particular, I appreciate the idea of building abstract (in terms of using semantics / segmentations rather than appearance information) and thus compact scene representations.\n\nThe paper builds on ideas presented in previous works (GOReloc, CLIP-Loc), but the overall design is interesting and novel in terms of using language information and DIOU-based retrieval. The ideas presented in the paper (combined language and image-based CLIP descriptor, DIOU retrieval, and ICP alignment based on object point clouds) could be interesting to the community and for future works on object-level camera localization.\n\nThe proposed approach is technically sound. The paper is well-written and easy to follow. The presented figures help understand the design."}, "weaknesses": {"value": "**Baselines selection and SotA claims.**\n\nThe paper claims that \"Experiments on various datasets demonstrate that our object-level system consistently achieves state-of-the-art performance in indoor camera relocalization\" (L107). Yet, this is not true for multiple reasons: \n\n1. The paper does not contain a comparison to any current state-of-the-art localization methods. In fact, it does not even cite the two probably gold-standard methods for accurate visual localization: hloc [Sarlin et al., From Coarse to Fine: Robust Hierarchical Localization at Large Scale, CVPR 2019] [Sarlin et al., SuperGlue: Learning Feature Matching with Graph Neural Networks, CVPR 2020] (a method based on matching local features) and ACE [Brachmann et al., Accelerated Coordinate Encoding: Learning to Relocalize in Minutes Using RGB and Poses, CVPR 2023] (a learning-based scene coordinate regressor). ACE (and other similar scene coordinate regressors) easily achieve similar map sizes as the proposed method. hloc by default produces a large map representation, but recent work [Wang, MAD-DR: Map Compression for Visual Localization with Matchness Aware Descriptor Dimension Reduction, ECCV 2024] [Laskar et al., Differentiable Product Quantization for Memory Efficient Camera Relocalization, ECCV 2024] shows that these representations can be reduced to a few MB without too large a loss in accuracy. Both hloc and ACE have publicly available source code.\n\nTwo of the baselines (Moreu et al., and Shavit et al.) belong to the family of absolute camera pose regressors. It is well-known by now that approaches from this family of localization algorithms perform significantly worse than methods based on 3D models. In fact, it has been shown that such approaches might not even outperform simple pose approximation strategies (see [Sattler et al., Understanding the Limitations of CNN-based Absolute Camera Pose Regression, CVPR 2019]).\n\n2. The paper evaluates the proposed approach on datasets that are not commonly used in the localization literature, making it virtually impossible to compare its performance with state-of-the-art approaches. It is unclear why more commonly used datasets such as 7Scenes, 12Scenes, and RIO10 (all room-level), Indoor-6 (multiple rooms) InLoc (two floors of a large university building), or the Gangnam Station and Hyundai Department Store datasets (both multiple floors of larger indoor scenes) were not used. Compared to ScanNet and ScanNet++, they offer the advantage that mapping and query images were obtained from different trajectories, making the localization problem harder. RIO10, InLoc, and Indoor-6 further offer query images taken under different viewing and illumination conditions, again making the visual localization problem harder.\n\n3. There are multiple works based on refining an initial pose estimate. The supplementary material compares to PixLoc, which was one of the first such methods based on learned features. In particular, the work by Pietrantoni et al. seems to be highly relevant to this work under review, In the context of privacy-preserving visual localization, they aim to estimate the pose of a query image wrt. an abstract map. The map they are using is represented by a set of segmentation labels and corresponding 3D points (e.g., from sparse structure-from-motion points in [Pietrantoni et al., Segloc: Learning segmentation-based representations for privacy-preserving visual localization, CVPR 2023] or dense Gaussian Splats in [Pietrantoni et al., Gaussian Splatting Feature Fields for (Privacy-Preserving) Visual Localization, CVPR 2025]). After obtaining an initial pose estimate via image retrieval, they refine the pose estimate by iteratively updating the camera pose such that the segment labels detected in the query image and a projection of the segment labels stored in the map match as good as possible (a similar cost function was used in [Lianos et al., VSO: Visual Semantic Odometry, ECCV 2018] in the context of visual SLAM). Since they do not need to store colors or features, these methods produce compact representations. \n\nIn general, the idea of localizing wrt. a semantic map (storing semantic labels together with 3D geometry) was already explored in [Stenborg et al., Long-term visual localization using semantically segmented images, ICRA 2018], [Toft et al., Semantic Match Consistency for Long-Term Visual Localization, ECCV 2018], and [Larsson et al., A Cross-Season Correspondence Dataset for Robust Semantic Segmentation, CVPR 2019].\n\nThese methods are all highly relevant, not cited in the work under submission, and would make natural baselines for a family of methods highly related to the proposed approach.\n\nIn summary, the current experimental evaluation is clearly insufficient for understanding the performance of the proposed approach compared to related methods and the current state-of-the-art.\n\n**Assumption on static scene.** \n\nThe paper mention that the method \"cannot capture and locally update dynamic changes in a scene\" (L484). It is not clear if this applies only to the mapping phase or even if the localization does not work in the presence of scene changes. If this is the second case, the limitation is very significant, and it should be mentioned prominently right at the beginning of the paper. In particular, most of the approaches discussed above are robust against such changes at query time. \n\n**Lighting Variance Analysis.**\n\nSec. C of the supplementary claims to test localization under different lighting conditions. However, the example in Fig. 7 shows three images that seem to have adjusted brightness, not changed lighting in the scene. The authors should clarify if they approximate the lighting changes by adjusting image brightness. If this is the case, using existing dataset that contains real-world changes would be preferable (see list of datasets above)."}, "questions": {"value": "In its current form, without comparisons to state-of-the-art methods and discussions and comparisons to highly related methods, I do not see how the paper can be accepted. In order to change my recommendation, I would want to see such comparisons (at least with hloc and ACE) on established datasets (the Indoor-6 dataset (https://github.com/microsoft/SceneLandmarkLocalization) seems small enough to allow evaluation in limited time but is challenging enough that results would be meaningful (as opposed to the 7Scenes and 12Scenes datasets which are essentially solved))."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "J5Av2ucx25", "forum": "KrhLCdXmh9", "replyto": "KrhLCdXmh9", "signatures": ["ICLR.cc/2026/Conference/Submission12453/Reviewer_5fUS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12453/Reviewer_5fUS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762294953931, "cdate": 1762294953931, "tmdate": 1762923331807, "mdate": 1762923331807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}