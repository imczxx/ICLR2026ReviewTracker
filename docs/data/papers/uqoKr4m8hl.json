{"id": "uqoKr4m8hl", "number": 2280, "cdate": 1757048019514, "mdate": 1759898158849, "content": {"title": "Adaptive Rank, Reduced Forgetting: Knowledge Retention in Continual Learning Vision-Language Models with Dynamic Rank-Selective LoRA", "abstract": "Continual learning (CL) aims to accumulate knowledge from sequential tasks without catastrophic forgetting. Vision–language models like CLIP, with strong generalization, are widely used for CL. Existing methods often adapt isolated PTM components, adding inference complexity and limiting PTM improvement, or rely on replay, stored information, or assumptions, incurring high costs and limited applicability. To advance models as continual learners, we explore CL via natural, efficient PTM updates instead of complex task-specific additions. We thus study continual low-rank learning and systematically analyze how LoRA ranks and placements affect *learning* and *forgetting*. We find that a relatively *higher-rank* LoRA improves task learning (*i.e.*, *plasticity*) but increases forgetting, while a relatively *lower-rank* LoRA reduces forgetting (*i.e.*, *stability*) but limits adaptation. Crucially, we find a *plasticity–stability balance* tied to rank across parameters and tasks, with *moderately small ranks* maximizing CL benefits. Motivated by this, we propose **Co**ntinual **Dy**namic **R**ank-Selective LoR**A** (**CoDyRA**), which continually updates PTMs with LoRA adapters of adaptively optimized rank. While the new-task objective drives learning, CoDyRA adaptively minimizes ranks with *sparsity-promoting regularization* to reduce interference and forgetting, achieving a plasticity–stability balance tailored to different parameters and tasks. Adaptively selected and minimized LoRA ranks keep the updated model closer to its previous state while learning new tasks. CoDyRA enables efficient CL as a sequence of LoRA-based tasks without storing past data, task information, or relying on assumptions. It preserves the original model architecture and deployment pipeline, adding no inference overhead. Extensive experiments show CoDyRA improves new representations while retaining old knowledge, achieving state-of-the-art results.", "tldr": "", "keywords": ["Continual Learning", "CLIP", "Low-Rank Adaptation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1402a9dd8dce215f432b455738d432f66f20a65f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of catastrophic forgetting in continual learning (CL) for pre-trained vision-language models (VLMs) like CLIP. The authors note that existing methods have significant drawbacks: approaches relying on task-specific modules add inference complexity and require task-ID prediction, while replay-based methods incur high storage and computational costs.\nTo overcome these limitations, this paper explores performing CL through Low-Rank Adaptation (LoRA), a natural and efficient update mechanism. The authors first systematically analyze how LoRA's rank and placement affect the trade-off between \"plasticity\" (the ability to learn new tasks) and \"stability\" (the ability to retain old knowledge). Their key finding is that a relatively higher rank promotes plasticity but exacerbates forgetting, while a relatively lower rank enhances stability but limits the model's adaptation. They find that an optimal balance exists at a moderately small rank, although this balance point varies across different parameter locations and tasks.\nBased on this analysis, the paper proposes CoDyRA (Continual Dynamic Rank-Selective LoRA). This method continually updates the VLM by adaptively optimizing the rank of each LoRA adapter. CoDyRA achieves this by jointly optimizing two objectives: (1) the standard new-task learning objective, which drives plasticity; and (2) a sparsity-promoting $l_1$ regularization applied to a set of learnable \"importance weights\" for each rank. This regularization dynamically minimizes the number of active ranks, forcing the model update to remain closer to its previous state, thereby reducing interference and forgetting.\nThe main contributions of this paper include:\nA systematic study of the impact of LoRA's rank and placement on the plasticity-stability trade-off in VLM continual learning.\nThe proposal of the CoDyRA method, which uses sparsity-promoting regularization to adaptively select and minimize the rank of LoRA. It operates without storing past data, requiring task information, or adding task-specific components."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Solid Analysis and Clear Motivation: A core strength of this paper is the systematic analysis provided in Section 3.2. The authors delve into the impact of the rank and placement of LoRA on the tradeoff between plasticity (learning new knowledge) and stability (retaining old knowledge). This analysis clearly reveals why a fixed-rank LoRA strategy is not optimal and provides strong motivation and design guidance for the subsequently proposed adaptive-rank method.\n\n2. Ingenious Parameter Update Mechanism: CoDyRA does not directly optimize the discrete \"rank,\" but instead introduces a set of learnable \"importance weights\" ($w^{t,m}$) for each LoRA module and innovatively combines them with $l_1$ sparse regularization. This design cleverly transforms the difficult discrete rank selection problem into a solvable continuous sparse optimization problem. Furthermore, the authors employ the proximal gradient method and its soft thresholding operation to effectively solve the non-differentiability optimization problem caused by the $l_1$ norm.\n\n3. The paper provides robust ablation experiments and parameter sensitivity analysis (Section 4.4) 9. The authors not only verified the influence of LoRA insertion position and initial rank 10, but also conducted in-depth analysis of key hyperparameters, such as the maximum pruning threshold ($\\kappa_{max}$) and dense-iteration ratio 11, fully demonstrating the rationality and robustness of the model design."}, "weaknesses": {"value": "The paper's core assumption is that \"minimizing LoRA rank\" can serve as an effective proxy for \"reducing catastrophic forgetting\". However, the analysis in Section 3.2 (Fig. 3) primarily demonstrates a correlation between rank and forgetting, but fails to deeply investigate the causality. The root cause of forgetting is the interference of the parameter update direction with old tasks, not just the rank of the update. For instance, a high-rank update might not cause forgetting if its update direction is orthogonal to the parameter subspace of old tasks. Conversely, a low-rank (or even rank-1) update could be catastrophic if its direction is incorrect (e.g., directly opposes a critical gradient direction for a previous task). The paper lacks a deeper parameter-space analysis to substantiate this assumption. For example, does the low-rank increment $\\Delta W$ generated by CoDyRA's $l_1$ sparsification truly interfere less with old knowledge in terms of its update direction compared to standard LoRA? A specific analysis of this direction's orthogonality or interference is missing. Currently, the validity of \"low rank\" as a robust proxy for \"low forgetting\" has not been sufficiently theoretically or empirically demonstrated."}, "questions": {"value": "### Potential Typo in Core Equations (Eq. 4 & 7)\n\nThere appears to be a typo in the soft-thresholding operator used for the \\( l_1 \\) regularization.\n\n1. **The Goal:**  \n   The paper aims to use \\( l_1 \\) regularization to promote sparsity by pushing the importance weights \\( w \\) **toward 0**.\n\n2. **The Formula:**  \n   However, Eq. (4) and Eq. (7) define the operator with a **plus sign**:\n\n   \\[\n   w_{i} := \\mathbb{I}(|\\hat{w}_{i}| > \\kappa) \\cdot (\\hat{w}_{i} + \\operatorname{sign}(\\hat{w}_{i}) \\cdot \\kappa)\n   \\]\n\n3. **The Contradiction:**  \n   This formula would actually *amplify* the weights (e.g., 5 becomes 6), moving them **away from 0**.  \n   This is the opposite of sparsity.\n\n---\n\nShould this formula use a **minus sign**  \n(\\( \\ldots - \\operatorname{sign}(\\hat{w}_{i}) \\cdot \\kappa \\))  \nto correctly implement the soft-thresholding (shrinkage) operation?  \nPlease clarify if this is a typo in the manuscript and if the correct operator was used in the implementation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CMALaFgyJI", "forum": "uqoKr4m8hl", "replyto": "uqoKr4m8hl", "signatures": ["ICLR.cc/2026/Conference/Submission2280/Reviewer_P5E9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2280/Reviewer_P5E9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805345492, "cdate": 1761805345492, "tmdate": 1762916173896, "mdate": 1762916173896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Continual Dynamic Rank-Selective LoRA (CoDyRA) for continual learning, mainly for cross-domain continual learning. The proposed method has been developed based on CLIP VLM and LoRA approach. The paper is well-presented and comprehensive. The paper is easy to follow with helpful highlights for the reader. Despite its positive values, this paper has a few fundamental issues; please see the weaknesses."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Strength:\n(1). This paper discusses an interesting up-to-date sub-problem of CL, i.e, cross-domain CL.\n\n(2). The motivation of the proposed method is clear.\n\n(3). The writing is well presented with helpful highlights, charts, and diagrams.\n\n(4). The experiment results are comprehensively delivered and conducted on many datasets."}, "weaknesses": {"value": "(1). The main idea of the proposed method is adding new trainable weight w^{t,m}_i associated with B^{t,m}_{:i}. The proposed trainable weights are supposed to make \\delta W^{t,m} more adaptive. From the methodology perspective, the idea is arguably not novel as the idea of trainable weights was already applied in previous methods, e.g, ConvPrompt [CVPR 2024].\n\n(2). No theoretical and numerical proofs showing that the trainable weights idea improves model adaptation and reduces model forgetting significantly.\n\n(3). Performance issue: Even though the proposed method achieves the highest performance on average, it is significantly outperformed by the previous method (RAIL-Primal), i.e, 11\\% on Caltect101 dataset and 3.9\\% on Flowers dataset.\n\n(4). Continual learning is the art of defying catastrophic forgetting (CF). But, I do not see a comprehensive forgetting analysis.\n\n(5). The paper misses a comparison of the proposed method to the newest CLIP and LoRA-based CL methods, i.e, CLAP4CLIP (NeurIPS 2024), C-CLIP (ICLR 2025), Mind-the-Gap (ICCV 2025), CL-LoRA(CVPR-2025), InfLORA (CVPR 2024).\n\n\nReferences:\n\n[1]. CLAP4CLIP: Continual learning with probabilistic finetuning for vision-language models.\n\n[2]. C-CLIP: Multimodal continual learning for vision-language model.\n\n[3]. Mind the gap: Preserving and compensating for the modality gap in clip-based continual learning.\n\n[4]. CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free Class-Incremental Learning\n\n[5]. Inflora: Interference-free low-rank adaptation for continual learning.\n\n[6] Convolutional prompting meets language models for continual learning (ConvPrompt)"}, "questions": {"value": "Please address the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OI8Q3J6dxE", "forum": "uqoKr4m8hl", "replyto": "uqoKr4m8hl", "signatures": ["ICLR.cc/2026/Conference/Submission2280/Reviewer_oHrv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2280/Reviewer_oHrv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941662647, "cdate": 1761941662647, "tmdate": 1762916173631, "mdate": 1762916173631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets continual learning (CL) for vision-language models like CLIP by introducing CoDyRA. This method dynamically optimizes the rank of LoRA adapters during sequential task learning to balance the plasticity and stability. The authors systematically analyze the impact of LoRA rank and placement on learning-forgetting trade-offs and propose an adaptive rank-selection mechanism driven by sparsity-promoting regularization. Extensive experiments on benchmarks such as MTIL and X-TAIL demonstrate improved performance over SOTA methods in retaining pre-trained capabilities while improving generalization, with no inference overhead. Though very intriguing and promising, this work could benefit from a more in-depth theoretical analysis and a more structured presentation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It introduces a fine-grained, adaptive approach to LoRA-based CL, offering a novel perspective supported by convincing experimental validation.\n2. The method is straightforward to implement and exhibits potential for scalability due to its simplicity.\n3. Comprehensive experiments across diverse benchmarks and model configurations, including visualizations, robustly substantiate the core claim that rank manipulation addresses the learning-forgetting trade-off effectively."}, "weaknesses": {"value": "1. Though the authors have provided a preliminary empirical analysis of the impact of the lora location and rank (sec 3.2), the study lacks direct theoretical derivation or proof, necessitating deeper analytical foundations beyond empirical results.\n2. Marginal improvements in Tables 1 and 2 (often fractions of a percent, less than 1%) raise concerns about the method’s effectiveness and generality compared to state-of-the-art approaches.\n3. Most experiments use the ViT-B/16 backbone of CLIP. More tests on a larger or different model architecture and different pre-trained parameters could give a broader impact assessment."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rgc6jgo2UM", "forum": "uqoKr4m8hl", "replyto": "uqoKr4m8hl", "signatures": ["ICLR.cc/2026/Conference/Submission2280/Reviewer_VY2p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2280/Reviewer_VY2p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991809136, "cdate": 1761991809136, "tmdate": 1762916173407, "mdate": 1762916173407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CoDyRA (Continual Dynamic Rank-Selective LoRA), a continual learning method for vision–language models like CLIP that updates pre-trained models using LoRA adapters with adaptively optimized ranks. By balancing plasticity and stability through dynamic rank selection and sparsity regularization, CoDyRA enables efficient continual updates without replay, task-specific modules, or added inference cost, achieving state-of-the-art performance while preserving prior knowledge."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured and clearly written, making it easy to follow.\n\n2. The study tackles an important problem in continual learning by employing low-rank adaptation.\n\n3. The experimental evaluation is comprehensive, incorporating a wide range of baselines and datasets, which enhances the credibility of the paper’s conclusions."}, "weaknesses": {"value": "1. Some related works appear to have been overlooked. There are also several recent studies that attempt to adjust their architectures dynamically in continual learning, such as TreeLoRA.\n\n    TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree.\n\n2. Can the authors extend the proposed method to large language models to further validate its scalability?"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uxmX7YYvof", "forum": "uqoKr4m8hl", "replyto": "uqoKr4m8hl", "signatures": ["ICLR.cc/2026/Conference/Submission2280/Reviewer_TRej"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2280/Reviewer_TRej"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762410452830, "cdate": 1762410452830, "tmdate": 1762916173215, "mdate": 1762916173215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}