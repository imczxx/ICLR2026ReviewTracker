{"id": "ArkQbqV2Km", "number": 1933, "cdate": 1756968247065, "mdate": 1759898177644, "content": {"title": "GarmentPainter: Efficient 3D Garment Texture Synthesis with Character-Guided Diffusion Model", "abstract": "Generating high-fidelity, 3D-consistent garment textures remains a challenging problem due to the inherent complexities of garment structures and the stringent requirement for detailed, globally consistent texture synthesis. Existing approaches either rely on 2D-based diffusion models, which inherently struggle with 3D consistency, require expensive multi-step optimization or depend on strict spatial alignment between 2D reference images and 3D meshes, which limits their flexibility and scalability. In this work, we introduce GarmentPainter, a simple yet efficient framework for synthesizing high-quality, 3D-aware garment textures in UV space. Our method leverages a UV position map as the 3D structural guidance, ensuring texture consistency across the garment surface during texture generation. To enhance control and adaptability, we introduce a type selection module, enabling fine-grained texture generation for specific garment components based on a character reference image, without requiring alignment between the reference image and the 3D mesh. GarmentPainter efficiently integrates all guidance signals into the input of a diffusion model in a spatially aligned manner, without modifying the underlying UNet architecture. Extensive experiments demonstrate that GarmentPainter achieves state-of-the-art performance in terms of visual fidelity, 3D consistency, and computational efficiency, outperforming existing methods in both qualitative and quantitative evaluations.", "tldr": "", "keywords": ["3D Generation; Texture Generation; Diffusion"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/01c89c676562678fe972d0dd7fcdde0e1255ba77.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents GarmentPainter, a framework for synthesizing garment textures in UV space. It leverages a UV position map as a 3D structural guide to ensure texture consistency across the garment surface. Experimental results show that GarmentPainter achieves state-of-the-art performance in terms of visual fidelity, 3D consistency, and computational efficiency.\n\nHowever, the overall framework lacks substantial novelty, as employing a UV position map for 3D structural guidance is an established practice. In addition, the motivation for introducing the type selection module is not clearly justified."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. An application-driven study that delivers convincing results.\n2. Construct a high-quality garment dataset tailored for texture generation."}, "weaknesses": {"value": "1. The paper shows limited novelty. Using a UV position map is not a new idea, and Paint3D also support direct generation in UV space.\n\n2. The introduction of the type selection module is not well-motivated. Why not directly use the cloth type as a text prompt instead of introducing an additional type encoder? It would be helpful to report the performance when using cloth type as a textual condition. Moreover, the claim that the type selection module works without alignment between the reference image and the 3D mesh seems somewhat overstated.\n\n3. The discussion of 2D virtual try-on in the related work section appears unnecessary, since the paper’s focus is on 3D texture generation.\n\n4. The discussion of limitations is insufficient. How does the proposed method perform on fine or automatically generated UV maps, such as those produced by atlas-based methods?\n\n5. Minor issue: There are a few typos in Figure 3."}, "questions": {"value": "1. The paper shows limited novelty. Using a UV position map is not a new idea, and Paint3D also support direct generation in UV space.\n\n2. The introduction of the type selection module is not well-motivated. Why not directly use the cloth type as a text prompt instead of introducing an additional type encoder? It would be helpful to report the performance when using cloth type as a textual condition. Moreover, the claim that the type selection module works without alignment between the reference image and the 3D mesh seems somewhat overstated.\n\n3. The discussion of limitations is insufficient. How does the proposed method perform on fine or automatically generated UV maps, such as those produced by atlas-based methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "i9a403LucA", "forum": "ArkQbqV2Km", "replyto": "ArkQbqV2Km", "signatures": ["ICLR.cc/2026/Conference/Submission1933/Reviewer_4FyR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1933/Reviewer_4FyR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659424457, "cdate": 1761659424457, "tmdate": 1762915962125, "mdate": 1762915962125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **GarmentPainter**, a diffusion-based framework for generating 3D garment textures directly in UV space. The method modifies Stable Diffusion 1.5 by removing text cross-attention and injecting multiple VAE-encoded latent modalities (reference image, UV position map, UV texture/mask) to guide generation. A small type-selection module is added to control generation across top/bottom/one-piece garments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Data Contribution**: The authors curate a garment-specific dataset with UV maps, reference images, and mask/position data, which is valuable for this niche area of 3D garment texturing.\n- **Structural Innovation on SD1.5**: The way the authors adapt SD1.5 — particularly replacing text cross-attention with multi-modal VAE latent conditioning — is a novel and neat architectural modification that simplifies conditioning without heavy architectural changes.\n- **Experimental Soundness**: The ablation studies are well-designed and convincingly demonstrate the necessity of each component (UV position map, type selection), showing clear performance degradation when removed."}, "weaknesses": {"value": "**Concerns on Generalization**\n> *[Sec.3 L206-209]* “Ultimately, we curate a dataset comprising 7,579 clothing items, including 3,703 tops, 2,114 bottoms, and 1,762 one-piece garments.”\n- Although the dataset creation is commendable, the total scale (~7.6k) appears small relative to the architectural modifications made to SD1.5 (multi-modal VAE inputs, removal of text cross-attention). It raises the question of whether such a limited dataset is sufficient to grant **true generalization** rather than overfitting.\n- The paper does not specify the **train/validation/test split**, nor clarify whether evaluation is on the same data source or on external data. This omission further amplifies concerns regarding generalization.\n- UV robustness is under-explored. All data are processed in Blender with a consistent UV unwrapping workflow. It remains unclear whether the method can generalize to **other UV layouts**, especially auto-generated or platform-specific UVs, which often introduce discontinuities.\n\n**Limited Novelty**\n- While the architectural attempt is appreciated, using a UV coordinate/position map to maintain spatial continuity is not new — both **Paint3D** and **TEXGen** adopt similar strategies.\n- Furthermore, directly generating UV maps inherently struggles to guarantee 3D spatial consistency due to UV seam discontinuities and varying unwrapping conventions. This is why many recent works still rely on multi-view synthesis[1,2,3] for texture painting. That said, for **garment textures**, where UVs tend to be more regular, the approach is acceptable and practical.\n\n**Relatively Outdated Foundation**\n- From a generative model standpoint, the field has rapidly evolved beyond SD1.5 (e.g., SDXL, FLUX, Qwen-Image), with substantially improved image quality, resolution, and visual priors. Operating on SD1.5 limits output resolution to 512px and inherently lags behind current capabilities.\n- The paper neither discusses nor compares against more recent texture-oriented or multi-view-consistent generation approaches such as: Mv-Adapter[1], FlexiTex[3], which would provide a more up-to-date benchmark of competitiveness.\n\n--- \n\n**Reference**\n\n[1] \"Mv-adapter: Multi-view consistent image generation made easy.\" *in ICCV 2025*.\n\n[2] \"Mvpaint: Synchronized multi-view diffusion for painting anything 3d.\" *in CVPR 2025*.\n\n[3] \"FlexiTex: Enhancing Texture Generation via Visual Guidance.\" *in AAAI 2025*."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vmlNzVuGIi", "forum": "ArkQbqV2Km", "replyto": "ArkQbqV2Km", "signatures": ["ICLR.cc/2026/Conference/Submission1933/Reviewer_4r63"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1933/Reviewer_4r63"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722775891, "cdate": 1761722775891, "tmdate": 1762915961805, "mdate": 1762915961805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets the intesting problem of 3D garment texture synthesis which is of great importance to the industry applications like games and animations. The main idea of the proposed GarmentPainter is to leverage the UV map to align 2D images to enable the 2D models to generate the 3D UV textures. To train the model, a new dataset is introduced with high-quality garments which is helpful for the garment texture generation problem. Experimental results validate the effectiveness and efficiency of the proposed algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The problem of 3D garment generation is an important problem for industry applicatioons. \n* The proposed garment dataset with high-quality garments should be useful to the 3D community.\n* The proposed algorithm obtain promising results with sufficient ablation studies."}, "weaknesses": {"value": "* More implementation details should be provided to facilitate the reproduction of the paper. Or it would be better to provide the code for reproduction. \n\n* For the experimental results in Table 1, I would suggest to provide more comparisons against the papers published in the recent two years or in the year of 2025.\n\n* In the experiments, the evaluation metric is based on FID and KID, which may not be consistent with human subjective evaluations. Thus, is it possible to provide a user study to verify the effectiveness of the proposed algorithm against the baselines. \n\n* The algorithm is based on the inpainting model trained from SD v1.5. As there are rapid developement of the text-to-image community, how about the performance of the proposed algorithm if better inpainting models, like flux-kontext, is utilized?"}, "questions": {"value": "Please mainly address the questions in the weakness section. More specifically, the questions related with the experiments should be well addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6ysF4ww3kG", "forum": "ArkQbqV2Km", "replyto": "ArkQbqV2Km", "signatures": ["ICLR.cc/2026/Conference/Submission1933/Reviewer_GpsL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1933/Reviewer_GpsL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727958673, "cdate": 1761727958673, "tmdate": 1762915960675, "mdate": 1762915960675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GarmentPainter, a method that generates garment textures directly in UV space from a reference person image. The approach encodes the reference image and a UV position map into latents, concatenates them (with a masked UV texture channel) as UNet inputs, and injects a garment-type embedding (top/bottom/one-piece) into the diffusion timestep embedding for better control. The authors also describe a dataset of ~7.6k garments with reference images, UVs, type labels, and position maps, and report strong speed and competitive FID/KID against several baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple and practical design: Minimal modifications to a standard inpainting diffusion backbone (channel concatenation + type embedding) make the method easy to implement and deploy.\n\n2. Fast inference: Reported end-to-end UV generation is notably fast (single forward path), which is attractive for production pipelines compared with multi-view/iterative methods.\n\n3. Workflow alignment: Accepting a person-in-context reference image maps well to real authoring scenarios, reducing pre- and post-processing overhead.\n\n4. Clear data description: The paper gives a concrete breakdown of categories (top/bottom/one-piece), rendering protocol (front/back), and labeling procedure, which improves readability and reuse."}, "weaknesses": {"value": "1. Fairness & reproducibility: Different baselines appear to be run under different input protocols (e.g., prompts, masks, background handling, illumination). This can bias comparisons. A single unified evaluation protocol (resolution, masking, prompts, backgrounds/lighting) and a reproducible package would strengthen claims.\n\n2. 3D consistency metrics are thin: The evaluation focuses on image-space metrics (e.g., FID/KID) and runtime. It lacks direct measures of UV seam continuity, cross-view consistency, and geometric adjacency color differences, which are crucial for textures that must look coherent on a mesh.\n\n3. Generalization beyond the training domain: The dataset leans toward a specific visual domain. Claims of robustness to real photos, complex materials, heavy patterns, and challenging poses would be more convincing with systematic out-of-domain tests and a failure-mode analysis."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pamUgpNFsG", "forum": "ArkQbqV2Km", "replyto": "ArkQbqV2Km", "signatures": ["ICLR.cc/2026/Conference/Submission1933/Reviewer_ZtVi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1933/Reviewer_ZtVi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149546051, "cdate": 1762149546051, "tmdate": 1762915960064, "mdate": 1762915960064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}