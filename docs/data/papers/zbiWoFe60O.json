{"id": "zbiWoFe60O", "number": 8046, "cdate": 1758054822406, "mdate": 1759897811978, "content": {"title": "Topological Invariance and Breakdown in Learning", "abstract": "We prove that for a broad class of permutation-equivariant learning rules (including SGD, Adam, and others), the training process induces a bi-Lipschitz mapping of neurons and preserves key topological properties of the neuron distribution. This result reveals a qualitative difference between small and large learning rates. Below a critical topological threshold $\\eta^\\*$, the training is constrained to preserve the topological structure of the neurons, whereas above $\\eta^\\*$ the process allows topological simplification, making the neuron manifold progressively coarser and reducing the model's expressivity. An important feature of our theory is that it's independent of specific architectures or loss functions, enabling universal applications of topological methods to the study of deep learning.", "tldr": "We prove that permutation-equivariant learning rules (e.g. SGD, Adam) preserve the topological structure of neurons at small learning rates, but break it and simplify models at large learning rates.", "keywords": ["learning dynamics", "topology"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0afbb032b51db530c407bb0676ea4b5c5d143b8d.pdf", "supplementary_material": "/attachment/6128c94135c184e8fe673c39ad64e10c4307eb70.zip"}, "replies": [{"content": {"summary": {"value": "Permutation symmetry allows us to view the neurons inside a neural network as points belonging to a set that we can interpret as a topological space. Permutation symmetry and \"continuity\" of the training algorithm can be used to prove that each training step is a homeomorphism (a topology-preserving map) for all learning rates lower than a certain *topological* threshold. When the learning rate is higher than this threshold, the training algorithm can collapse different neurons and change the neurons' topological structure, simplifying it. The results also show that, below the threshold, the probability density of neurons is also preserved. Simple numerical experiments on two-layer neural networks validate this when the neurons are initialized on toy manifolds."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- I think the approach proposed in the paper is novel and promising. The idea that there is a topological threshold below which topology of the neurons, in some sense, preserved is quite interesting. \n- The paper employs topology, an underexplored tool in the neural network literature, to study the joint effect of symmetry and finite learning rates, a significant and well-studied problem in current research.\n- The paper is well explained and clearly readable, even in its more technical aspects."}, "weaknesses": {"value": "As I wrote above, I think the ideas presented in the paper are quite interesting. However, I see two main issues with the theoretical contributions that, at the moment, prevent me from recommending acceptance.\n\nThe main weakness of the paper, in my view, is that in all real scenarios, the number of neurons is, of course, finite. This means that the set of neurons $S^{(t)}$ will be a discrete set of different (with prob 1 if we initialize randomly) points in Euclidean space. If we make $S^{(t)}$ inherit the Euclidean topology of $\\mathbb{R}^d$ (as prescribed in L222), it will inherit the *discrete topology*, i.e. the \"finest\" possible one generated by all subsets of $S^{(t)}$. \nTherefore, in all real cases where neurons are neither continuous nor infinite, the neurons set can't be as structured as a topological or differentiable manifold. \n\nThe main result thus reduces to saying that, below the topological threshold, the training algorithm will never merge two points into the same one. This comes from the fact that any bijective function between spaces with the discrete topology is a homeomorphism, as any function will be continuous. Put in this way, I think, the result is not particularly interesting or surprising as we are talking about points into a continuous space and the fact that two of them will exactly coincide is, intuitively, a \"measure zero event\" which will therefore virtually never happen. \n\nThe result would be more interesting if the authors were able to prove that a \"coarser\" notion of topology, intended in the sense of topological data analysis, is preserved. This would mean intuitively that we see the neuron as \"approximating\" a topological space or a manifold and the topological features are defined, e.g. by persistent homology. The authors already seems to suggest that this is the case in the numerical experiments, but I don't think that the theoretical result implies it in any way.\n\nMoreover, usually weights are randomly sampled, and it seems unlikely that neurons will fall in such a pattern that can resemble the approximation of a manifold, like a Gaussian blob. It is not clear what preservation/non-preservation of topology would entail in that scenario.\n\nThe second, somewhat less important issue I see, is that in a standard MLP neural network, only neurons belonging to the same layer can be swapped, that is permutation symmetry only holds layerwise and the result of the paper only hold in each layer separately. I believe that this further reduces the impact of this set of results.\n\n### Minor weaknesses\n- As I mentioned above, the main topology-preservation result tells us that neurons points cannot merge (as it is also recognized in the paper at line L245). This is discussed in *Simsek et al. \"Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances.\" International Conference on Machine Learning. PMLR, 2021*, where it is proven that gradient flow cannot reach a symmetry subspace (equivalent to merging neurons states) in finite time. I recognized that the authors deal with a more general setting, but I think this should be acknowledged.\n- L097: shouldn't \"row\" and \"column\" be swapped?\n- L119: \"the collection of neurons at a given time step can be viewed as a manifold embedded in the Euclidean space, whose topology reflects its connectivity and shape.\" I don't think this is a correct characterization of the works referenced after this sentence. They each study the topology of neural networks in different senses, all of which are different from the one explored in this work."}, "questions": {"value": "- Referring to my comments above, do the authors believe that their results extend to a notion of topology besides the \"trivial\" homeomorphism between discrete finite spaces, like it is suggested in the experiments?\n- If this is the case, how can we interpret the result when we initialize weights randomly and not on a well-behaved manifold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lk2gLH4Rz7", "forum": "zbiWoFe60O", "replyto": "zbiWoFe60O", "signatures": ["ICLR.cc/2026/Conference/Submission8046/Reviewer_9kDK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8046/Reviewer_9kDK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814890271, "cdate": 1761814890271, "tmdate": 1762920040198, "mdate": 1762920040198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how permutation-equivariant learning rules (e.g., SGD, Adam) affect the topological structure of neurons during training. The authors prove that for sufficiently small learning rates (below a critical threshold $\\eta^*$), training acts as a bi-Lipschitz mapping and preserves the topological features of the neuron manifold. Once the learning rate exceeds this threshold, training enables topological simplification: neuron manifolds become coarser, expressive capacity may reduce, and the model structure simplifies. The proposed theory is claimed to be architecture- and loss-function-agnostic, offering a general framework for understanding the interplay between learning rate and the topology of neural representations."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of analyzing training dynamics through the topology of neuron configurations is original and conceptually appealing. Connecting learning-rate regimes with structural changes in neural representations could offer new perspectives on optimization stability and expressivity."}, "weaknesses": {"value": "While the framework is intriguing, its topological interpretation remains unclear. In practical settings, the number of neurons is finite, so the natural topology on this set is discrete, where every bijection is trivially a homeomorphism. Intuitively, small learning rates cause small parameter updates, so the point cloud of neurons should change smoothly. However, it is not evident in what sense nontrivial topological properties arise or how they meaningfully relate to the learning dynamics.\n\nThe paper further suggests that a coarser topology corresponds to lower expressivity and that stable optimization requires topological preservation. These claims are intriguing but not well justified. Clarification on what ``preserving topology'' means in the finite or discrete setting, and how this affects training dynamics and expressivity, is crucial to assess the paper’s relevance."}, "questions": {"value": "- Which topological properties are considered, and how do they relate to training dynamics?\n- Why does a coarser topology of the neuron manifold imply lower expressivity? How does the topology of weights constrain the representable function class?\n- Why should stable optimization require the updating map to be a homeomorphism (or bijection in the discrete case)?\n- For a finite neuron set, one could induce nontrivial topologies using tools from topological data analysis (e.g., Vietoris--Rips complexes). Could the authors comment on why they did not adopt such constructions and instead worked with the discrete topology?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cE3yFGOfLf", "forum": "zbiWoFe60O", "replyto": "zbiWoFe60O", "signatures": ["ICLR.cc/2026/Conference/Submission8046/Reviewer_Ws8X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8046/Reviewer_Ws8X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820086480, "cdate": 1761820086480, "tmdate": 1762920039775, "mdate": 1762920039775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a class of permutation-equivariant learning rules (including SGD, Adam, and others).\nAuthors show that training process induces a bi-Lipschitz mapping of neurons and preserves key topological properties of the neuron distribution. This result reveals a qualitative difference between small and large learning rates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper theoretically studies a \"topology\" of sets of neurons. Authors prove that \"topology\" changes smoothly for small learning rates\n2. The theory is universal across architectures (MLP, CNN)\n3. The paper grounds its claims in well-established concepts from topology (homeomorphisms, quotient maps, Betti numbers) and measure theory, offering a formal language to describe neural network evolution."}, "weaknesses": {"value": "1. The situation when neurons (weights) lie on a genus-2 manifold is artificial. Typically, neurons are initialized randomly.\n2. Experiment are limited to small MLPs and MNIST. Large models, like Transformers are not studied.\n3. I am not sure that the definition of \"neuron\" and equivariance (eq. (2)) are applicable for transformers. \n4. The generalization error, which is of most interest in ML/AI is not studied.\n5. Despite of the theoretical insights, the paper doesn’t propose new training algorithms, regularizers, or learning rate schedules that exploit the topological critical point."}, "questions": {"value": "1) In Equation 4, instead of X must be X^{t} ?\n2) How your paper is related to:\nNaitzat, G., Zhitnikov, A., & Lim, L. H. (2020). Topology of deep neural networks. Journal of Machine Learning Research, 21(184), 1-40.\n3) Fig. 3, 4. When you write \"neurons are initialized on genus-2 surface\" it is not clear what you mean by \"neurons\".\nYou mean activations of weights of a network? Because \"X\" in previous equations mean weights, for which evolution rule (4) applies.\n4) How discrete set of neurons can evolve by a homeomorphism?\n5) How your results are related to Edge of Stability (EOS) ?\nZhu, X., Wang, Z., Wang, X., Zhou, M., & Ge, R. (2022). Understanding edge-of-stability training dynamics with a minimalist example. arXiv preprint arXiv:2210.03294.\n6) Can we compute or estimate K empirically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "No87KtfRIA", "forum": "zbiWoFe60O", "replyto": "zbiWoFe60O", "signatures": ["ICLR.cc/2026/Conference/Submission8046/Reviewer_6rk3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8046/Reviewer_6rk3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821670063, "cdate": 1761821670063, "tmdate": 1762920039464, "mdate": 1762920039464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyses how the topology of neurons changes under learning rules (SGD, Adam). Model parameters are defined as a set of neurons, i.e. subset of parameters that are permutation-equivariant to the learning rule. First, the authors show that if the update rule is permutation-equivariant and continuous, then it provides a bi-Lipschitz mapping between the neurons at subsequent optimization steps which preserves topological properties. With the same assumptions, the main theorem establishes that the learning algorithm corresponds to a homeomorphism for sufficiently small learning rates. This implies that the evolution of neurons during training is governed by a homeomorphisms which preserve the topological properties of the manifolds. On the contrary, when the learning rate is large, the topology of neurons can change up to some degree. These results imply that there is a certain value of learning rate that governs the two phases of training dynamics. Moreover, the authors show that in the topology-preserving scenario, the learning rule corresponds to a measure preserving bijection. The work demonstrates that the proposed theory holds for common optimization methods GD and Adam. The authors verify the proposed theoretical framework with experiments with regression and MNIST classification problems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a theoretical framework characterizing the neurons’ topology under learning algorithms which does not rely on specific architectures or loss functions. The authors show that the proposed theory holds for common optimization methods such as GD and Adam.\n- The experimental results demonstrate that large learning rates lead to change in topological characteristics while small learning rates preserve the topological properties as expected from the proposed theory."}, "weaknesses": {"value": "- While theoretical results consider large and small learning rates separately, more complex learning rate schedulers are commonly used in practice. Can the proposed theoretical framework characterize the evolution of neurons' topology when the learning rate decreases and/or increases during training? \n- The benefits for the practical application (for example, relation to model performance, choice of learning rate schedule, initialization choice, etc.) are not clear. How the proposed methodology can guide the training process in practice?\n- The experimental part is limited. As far as I understand, the provided experiments focus only on fully connected layers and small neural networks.\n- Some relevant works were not mentioned: arXiv:2401.03824v1, arXiv:2012.15834v2, arXiv:2102.00485v2"}, "questions": {"value": "Please, see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2HZsdQqRAK", "forum": "zbiWoFe60O", "replyto": "zbiWoFe60O", "signatures": ["ICLR.cc/2026/Conference/Submission8046/Reviewer_s1Q7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8046/Reviewer_s1Q7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848674591, "cdate": 1761848674591, "tmdate": 1762920039170, "mdate": 1762920039170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This theoretical paper investigates the implications of permutation equivariance in learning algorithms under continuity and smoothness assumptions.\nIt focuses on a *neuron manifold*: a geometric object formed by representing each neuron as a particle with its input and output weights as coordinates (akin to a point cloud for a finite number of neurons, and a manifold in the infinite limit).\nThe core contribution of the paper is to derives conditions under which the evolution frome one timestep to the next of the neuron manifold is a surjective or homeomorphic/diffeomorphic mapping.\nFurthermore, the authors identify a so-called *topological critical point* which is a specific learning rate inversely proportional to a Lipschitz-like constant and below which the neuron manifold evolves homeomorphically, preserving its topology.\n\nThis critical point can actually be envisoned as a local value as it depends on the local geometry of the loss landscape and therefore also as a dynamical value changing with the parameter during the training process.\nThis perspective allows for different regimes (namely: topological invariance and *topological breakdown regimes*) to occur during a single training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a theorem relating the learning rate and the topological nature of the update rule which describe when the topological properties of the neuron manifold is preserved.\nThe results on no-merging or splitting could be interesting on its own to understand redundancy in the neuron's computations.\nThe paper also establishes a connection between the introduced *topological critical point* with the progressive sharpening phase and edge of stability regime, which implies the topological breakdown regime.\n\nThe paper focuses on discrete updates, which is what is used in practice while theory often relies on continuous formalism.\n\nOverall, the paper seems technically sound, with detailed proofs in the appendix, and is well presented: assumptions and definitions are clearly stated (notably in Section 3).\nThe main conclusion, while not so surprising, is expressed cleanly and under well-defined conditions.\nThe experiments should be reproducible based on information provided in the paper."}, "weaknesses": {"value": "The scope could be a bit narrow as it is not clear whether if (or how) the framework can be used with multiple layers. The paper does not provide an explicit mapping between network parameters and the neuron point clouds in the multilayer case and experiments are also restricted to shallow architectures.\n\nThere is limited motivation for why preserving or breaking topological invariance could be useful or the motivation is not clearly justified.\nRepeated claims about topological simplification, coarsening, and reduced expressivity and model capabilities in the topological breakdown regime, while theoretically valid, do not translate into actionable insights: the paper does not demonstrate such merging happen or that it affects model performance hence claims that topological simplification reduces expressivity seems overstated.\nIt would be convincing to see a \"failure mode\" that only happens in the topological breakdown regime and prevents learning on some task.\nThe link with grokking (line 279) is not clear and therefore seems a bit too speculative, and the application to learning rate scheduling (line 464) is not justified: why would one want to fix the topology of the neuron manifold ?\n\n**Concern on experimental validation**: the theory predicts that under P1 and P2-K, the update $\\hat U^{(t)}$ is continuous (theorem 1). Since connectedness is preserved by continuous maps, one would expect the number of connected components to remain constant.\nHowever, Figure 4 (c) and the Betti numbers in Figure 5 show an increase in apparent connected components, which the authors interpret as evidence of topological breakdown at large learning rates.\nThis interpretation seems questionable: topological breakdown should involve merging (quotienting) of neurons, and if I understood correctly not creating more components (as explained the upper bound of eq 7 always holds).\nMy understanding is that the apparent increase in connected components could instead result from discretization artifacts: after stretching between discrete points, they can appear as disconnected clusters.\nThus, Figures 4 (c) and 5 appear inconsistent with the theory, while Figure 3 (c) and its additional intersections aligns more closely with expected behavior (merging only).\n\n## Minor and additional feedback\n- line 247: confusing sentence \"once merged\" should be changed to something like \"whereas if two neurons are merged\"\n- figure 12 in appendix mentions that smaller stepsize models are trained for longer. I imagine you are reffering to the number of steps but the plots shows less epochs so it can be confusing. Moreover it is stated that small stepsize models are trained for longer to ensure convergence but the left plot does not seem to have converged yet. Finally the values of 1/K, a flatness measure (sharpness inversion) are suprising as larger stepsize end up with values an order of magnitude higher than a small stepsize (that is larger stepsize leads to sharper parameter), do you confirm these results ?\n- line 355: the claim \"The loss can be stably optimized only when the topology of the neurons is preserved.\" is unclear, are you referring to the interval $\\eta \\in [\\frac{1}{K}, \\frac{2}{K}]$ and if yes is the claim no inverted ?"}, "questions": {"value": "1. **Scope of the neuron manifold concept**\nIs the notion of a neuron manifold still meaningful for MLPs with multiple hidden layers? Could each layer be viewed as an independent, partially redundant manifold, or does weight sharing across layers make this interpretation invalid?\n\n1. **Theory–experiment alignment**\nCould the authors clarify how Theorem 1 relates to the observations in Figures 3(c), 4(c), and 5, especially given the apparent inconsistency between the predicted topological continuity and the increase in connected components discussed above?\n\n1. **Motivation**\nFrom a practical standpoint, what is the benefit of constraining or preserving the topology of the neuron manifold? Could this have implications for interpretability or do you envision a failure mode where model expressivity could actually be significantly reduced ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JEi4SRZmh5", "forum": "zbiWoFe60O", "replyto": "zbiWoFe60O", "signatures": ["ICLR.cc/2026/Conference/Submission8046/Reviewer_WTqQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8046/Reviewer_WTqQ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965897542, "cdate": 1761965897542, "tmdate": 1762920038724, "mdate": 1762920038724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}