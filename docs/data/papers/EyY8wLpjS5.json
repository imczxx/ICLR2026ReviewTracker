{"id": "EyY8wLpjS5", "number": 1818, "cdate": 1756944084772, "mdate": 1759898184073, "content": {"title": "Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers?", "abstract": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation that consists of learnable activation functions, with the potential to capture more complex relationships from data. Presently, KANs are deployed by replacing multilayer perceptrons (MLPs) in deep networks, including advanced architectures such as vision Transformers (ViTs). Given the success of replacing MLP with KAN, this work asks whether KAN could learn token interactions. In this paper, we design the first learnable attention called **K**olmogorov-**Ar**nold **At**tention (KArAt) for ViTs that can operate on any basis, ranging from Fourier, Wavelets, Splines, to Rational Functions. However, learnable activations in the attention cause a memory explosion. To remedy this, we propose a modular version of KArAt that uses a low-rank approximation. By adopting the Fourier basis into this, Fourier-KArAt and its variants, in some cases, outperform their traditional softmax counterparts, or show comparable performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We also deploy Fourier KArAt to ConViT and Swin-Transformer, and use it in detection and segmentation with ViT-Det. We dissect the performance of these architectures on the classification task by analyzing their loss landscapes, weight distributions, optimizer paths, attention visualizations, and transferability to other datasets, and contrast them with vanilla ViTs. KArAt's learnable activation yields a better attention score across all ViTs, indicating improved token-to-token interactions and contributing to enhanced inference. Still, its generalizability does not scale with larger ViTs. However, many factors, including the present computing interface, affect the relative performance of parameter- and memory-heavy KArAts. We note that the goal of this paper is not to produce efficient attention or challenge the traditional activations; by designing KArAt, we are the first to show that attention can be learned and encourage researchers to explore KArAt in conjunction with more advanced architectures that require a careful understanding of learnable activations.", "tldr": "", "keywords": ["Multi-Head Self-Attention", "Vision Transformers", "Kolmogorov-Arnold Network", "Kolmogorov-Arnold Transformers", "Kolmogorov-Arnold Attention"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff4b2c26e72cc118ebe9dc9c1a7d1a2ab0e57c7f.pdf", "supplementary_material": "/attachment/09884fa1e99e3f8b4b7560e3425caaddc637f8cc.zip"}, "replies": [{"content": {"summary": {"value": "This work investigates whether substituting the conventional softmax operation in ViTs with a learnable activation function will improve the performance or not. The proposed method draws inspiration from the new KAN architectures. A key implementation challenge is how to mitigate the computational cost when replacing it with a learnable activation function. The authors addressed this with a low-rank approximation strategy, where the learnable function could project the input to a lower-dimensional space. To improve the performance of the low-rank approximated method, the authors have also proposed using the Fourier basis encodings to replace the conventional B-splines. The authors have provided performance of the proposed method on ViT structures with various datasets, along with the intuitive analysis of the performance, and the qualitative result to show how the learnable activation is helping during the attention."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method further explored the potential of using KAN-based structure in the widely used ViTs, which might make a good contribution to the vision community to push forward with the architecture update and exploration. In fact, to make the ViTs more “explainable” is an interesting and important research topic.\n- The authors have made relatively thorough and rigorous experiments on various benchmarks with different model settings, including parameter comparison, ablation studies, and efficiency analysis, though some of the analysis could be moved to the main paper."}, "weaknesses": {"value": "- I understand that making the activation function in transformers learnable and making it a KAN-based architecture is interesting and could have potential benefits to allow the attention mechanism to better learn the complex data. However, based on what has been shown in the main paper, the proposed architecture cannot be applied to larger-scale models and is limited to ViT-Tiny. I think this is a big disadvantage, and I do think having real-world applications on those larger-scale---or not even larger-scale, but rather “conventional scale” we would use in modern days---is very important for the general motivation of this paper. I do think the authors need to revisit the proposed method and reassure that this big discrepancy in performance on larger-scale models is rooted in the limitations of the KAN-based architecture or not. On the other hand, if the authors really want to focus only on the ViT-Tiny architecture, it will change the entire story of this paper. The motivation, the focus, and the theoretical explanation need to be updated.\n- I think the loss landscape and the attention visualization should have provided more intuitions on why the proposed method is more explainable than the conventional self-attention, and the authors should have provided more theoretical links to the intuitive results. However, the main analysis in the paper is not presented theoretically or intuitively strong. I hope the authors could revise the paper and update it with a stronger analysis."}, "questions": {"value": "I think the authors should focus first on the major weaknesses of the paper and truly update the manuscript based on reviewers’ suggestions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "flHrN7u7Mh", "forum": "EyY8wLpjS5", "replyto": "EyY8wLpjS5", "signatures": ["ICLR.cc/2026/Conference/Submission1818/Reviewer_eokU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1818/Reviewer_eokU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727167744, "cdate": 1761727167744, "tmdate": 1762915899014, "mdate": 1762915899014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Kolmogorov-Arnold Attention (KArAt), a learnable attention mechanism inspired by Kolmogorov-Arnold Networks (KANs). Unlike standard Transformers that use a fixed softmax operation to compute attention weights, KArAt replaces it with learnable nonlinear bases such as Fourier, B-spline, wavelet, and rational functions. To alleviate the huge memory overhead, the authors employ a low-rank approximation and evaluate Fourier-KArAt across multiple Vision Transformer (ViT) variants and datasets (CIFAR-10/100, ImageNet-1K). Results show noticeable gains on small models (ViT-Tiny) but consistent degradation on larger models and transfer learning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "KArAt represents one of the first systematic attempts to reparameterize attention as a learnable functional operator, rather than a fixed probabilistic normalization. This perspective is intellectually stimulating and could influence future research in both theoretical deep learning and architectural design.\n\nAnother strong aspect is the experimental comprehensiveness.\nThe authors do not merely demonstrate their method. They conduct cross-scale evaluations (Tiny → Base), transfer learning tests, and multiple ablation studies on basis functions.\n\nThe writing and presentation quality are outstanding. The figures are informative, the mathematical notations are consistent, and the discussion sections are honest about both strengths and limitations."}, "weaknesses": {"value": "While conceptually elegant, the proposed approach does not deliver consistent empirical gains. Performance drops substantially on larger ViT models, revealing poor scalability and unstable optimization dynamics. Although the learnable operator Φ enriches the model’s nonlinearity, it also sharpens the loss landscape, leading to unstable gradients and weaker generalization.\n\nMoreover, the transfer learning results (Table 2) show clear degradation across datasets. Indicating that the method’s performance is also extremely sensitive to hyper-parameters and whether the operator is blockwise or universal. This sensitivity further limits the model’s practicality and reproducibility.\n\nFinally, the paper does not clearly define the intended application domain—it remains unclear when or why one should prefer KArAt over standard or efficient attention variants (e.g., Performer [1], Linformer [2], cosFormer [3]).\n\n[1] Choromanski, K.M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J.Q., Mohiuddin, A., Kaiser, L. and Belanger, D.B., Rethinking Attention with Performers. In International Conference on Learning Representations.\n[2] Wang, S., Li, B.Z., Khabsa, M., Fang, H. and Ma, H., 2020. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768.\n[3] Qin, Z., Sun, W., Deng, H., Li, D., Wei, Y., Lv, B., Yan, J., Kong, L. and Zhong, Y., cosFormer: Rethinking Softmax In Attention. In International Conference on Learning Representations."}, "questions": {"value": "1. The authors attribute performance drops in larger ViTs to “spiky loss landscapes.”\nHave they empirically verified this claim or could other regularization techniques help stabilize training?\n\n2. Since many recent attention variants also modify the attention for efficiency or stability, how does KArAt compare in both computational cost and performance under comparable parameter budgets?\n\n3. Can the authors identify specific domains where KArAt might offer concrete advantages over standard softmax?\n\n4. Given that the Q, K, and V projections are already learnable and flexible, what motivates the replacement of the softmax operator with another learnable function? Could the authors provide concrete evidence that this additional nonlinearity offers expressive benefits beyond what the QKV transformations already achieve?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kaWsghJuLS", "forum": "EyY8wLpjS5", "replyto": "EyY8wLpjS5", "signatures": ["ICLR.cc/2026/Conference/Submission1818/Reviewer_aECS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1818/Reviewer_aECS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792503113, "cdate": 1761792503113, "tmdate": 1762915898833, "mdate": 1762915898833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a learnable attention mechanism called KArAt, claimed to be the first learnable attention mechanism that replaces the traditional softmax function. KArAt uses learnable activation functions based on Fourier, Wavelets and B-splines. The paper uses a low-rank approximation to overcome the memory constraints. The authors' experiment on standard vision datasets like CIFAR-10, ImageNet-1K shows 5-7% improvements over the baseline, but fails to adapt to larger ViTs, showing poorer performance than the baselines. The method is also more computationally expensive than the baseline, requiring more memory and training time. The authors claim with extensive analysis that the KArAt produces better attention maps and transfers well to various downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) First of its kind analysis of learnable attention on ViT transformers through thorough analysis and ablation on various model sizes, various downstream tasks visualizing the loss landscape, attention map and spectral analysis. The authors are honest with the flaws in the approach (doesn’t scale well and is computationally expensive). \n2) KArAt produces better attention maps when compared to the traditional MHSA."}, "weaknesses": {"value": "1) Although the authors claim KArAt to produce better attention maps through attention heatmap and spectral analysis, there is no numerical evidence that shows this result. Maybe curating a small dataset derived from an existing one that doesn’t work well with MHSA but with KArAt would have been great addition. \n2) Gains concentrate on Tiny; Small/Base underperform MHSA, and training costs are significantly higher. If KArAt doesn’t stabilize for larger backbones, the usability case is weak."}, "questions": {"value": "1) What aspects of KArAt most directly cause degradation on Small/Base? Any signs that per‑layer/ per‑head G or r can stabilize training at scale?\n2) How does having a few KArAt layers mixed with MHSA affect accuracy and stability on ViT-Tiny/Small/Base? Does placement (early/mid/late) matter under compute-matched training"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nZf4XXtCnd", "forum": "EyY8wLpjS5", "replyto": "EyY8wLpjS5", "signatures": ["ICLR.cc/2026/Conference/Submission1818/Reviewer_ouZw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1818/Reviewer_ouZw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971007132, "cdate": 1761971007132, "tmdate": 1762915898700, "mdate": 1762915898700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present  a method to incorporate Kolmogorov-Arnold network into Vision Transformer. The prosed method is to replace softmax operation in self-attention with learnable operator based on KA network. The authors clain that the learnable operation may capture the token-to-token relationship better.\n\nSince the full rank KAN operator would lead to memory-explosion, the authors propose a low rank approximation with Fourier basis.\n\nThe evaluation shows that the ViT tiny could achieve significant gain by changin the softmax with KAN."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper raises a question about fixed softmax operation in self-attention.\n2. The results in ViT tiny model is promising."}, "weaknesses": {"value": "1. First of all, this method does not scale properly. Even the performance with ViT-Base is disappointing.\n2. Memory overhead even with low rank approximation is not justified.\n3. As shown in Figure 3, the loss landscape becomes less smooth, which may results in training instability."}, "questions": {"value": "1. How to overcome the scalability problem?\n2. In case of standard ViT, the token-to-token relationship is captured by softmax and projection of learnable heads. In that sense, standard self-attention also  learns how to capture the relationship. Is KAN really better approach?\n3. How to justify the memory overhead."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7zJM8cimz6", "forum": "EyY8wLpjS5", "replyto": "EyY8wLpjS5", "signatures": ["ICLR.cc/2026/Conference/Submission1818/Reviewer_TBHR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1818/Reviewer_TBHR"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991225185, "cdate": 1761991225185, "tmdate": 1762915898541, "mdate": 1762915898541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}