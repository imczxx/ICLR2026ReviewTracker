{"id": "2H2aNzDeYX", "number": 7839, "cdate": 1758038332906, "mdate": 1759897828255, "content": {"title": "Accelerating Regression Tasks with Quantum Algorithms", "abstract": "Regression is a cornerstone of statistics and machine learning, with\n  applications spanning science, engineering, and economics.\n  While quantum algorithms for regression have attracted considerable attention,\n  most existing work has focused on linear regression, leaving many more complex\n  yet practically important variants unexplored.\n  In this work, we present a unified quantum framework for accelerating a broad\n  class of regression tasks---including linear and multiple regression, Lasso,\n  Ridge, Huber, $\\ell_p$-, and $\\delta_p$-type regressions---achieving up to a\n  quadratic improvement in the number of samples $m$ over the best classical\n  algorithms.\n  This speedup is achieved by extending the recent classical breakthrough of\n Jambulapati et al. (STOC'24) using several quantum techniques, including\n  quantum leverage score approximation (Apers &Gribling, 2024) and the\n  preparation of many copies of a quantum state (Hamoudi, 2022).\n  For problems of dimension $n$, sparsity $r < n$, and error parameter\n  $\\epsilon$, our algorithm solves the problem in\n  $\\widetilde{O}(r\\sqrt{mn}/\\epsilon + \\mathrm{poly}(n,1/\\epsilon))$\n  quantum time, demonstrating both the applicability and the efficiency of\n  quantum computing in accelerating regression tasks.", "tldr": "We develop a unified quantum framework that accelerates a broad range of regression tasks (linear, Lasso, Ridge, Huber, $\\ell_p$, etc.), achieving quadratic runtime improvements in data size.", "keywords": ["Quantum Algorithms", "Regression"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2aefd53694746faab93bc0adbb50c479e3266f1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a unified quantum framework for accelerating sparsification in generalized linear models (GLMs), yielding quadratic speedups over classical methods across linear, multiple, Lasso, Ridge, Huber, $\\ell_p$, and $\\gamma_p$ regression. The authors extend the recent classical breakthrough of Jambulapati et al. (2024) by developing a Quantum Multiscale Leverage Score Overestimates (QMLSO) algorithm that leverages quantum leverage score approximation and quantum state preparation techniques. For problem dimension $n$, sparsity $r<n$, sample size $m$, and error parameter $\\varepsilon$, their algorithm achieves $\\widetilde{\\mathcal{O}}!\\left(\\frac{r\\sqrt{mn}}{\\varepsilon}+\\mathrm{poly}!\\left(n,\\frac{1}{\\varepsilon}\\right)\\right)$ quantum time complexity, demonstrating quadratic speedup in m when $\\varepsilon$ is constant and $m \\gg n$."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive theoretical framework with broad applicability. The paper successfully unifies quantum speedups for a diverse family of regression problems under a single GLM sparsification framework, extending beyond previous quantum work that focused primarily on linear regression. The introduction of proper loss functions and multiscale leverage score overestimates provides elegant abstractions that capture the essential structure needed for quantum acceleration across multiple problem variants.\n2. Rigorous technical development with novel quantum subroutines. The QMLSO algorithm (Algorithm 1) and the quantum weight initialization procedure (Theorem 9) represent substantial technical contributions that carefully adapt classical contractive algorithms to the quantum setting. The complexity analysis is thorough, properly accounting for query complexity to different oracles and demonstrating clear quadratic speedups with explicit leading-order terms that dominate in the regime where sparsification is beneficial $\\varepsilon=\\Omega!\\left(\\sqrt{\\tfrac{n}{m}}\\right)$."}, "weaknesses": {"value": "1. Limited discussion of QRAM requirements. While the paper acknowledges on page 5 (lines 217-220) that \"QRAM serves as a natural quantum analogue of the classical RAM model\" and mentions that \"practical realization of scalable QRAM remains highly uncertain,\" there is insufficient critical analysis of how QRAM requirements scale with problem size and what this means for near-term quantum advantage.\n\n2. Gap between stated contributions and actual novelty over prior quantum work. The paper claims on page 1 (lines 17-18) to address \"broader and both theoretically and practically important regression tasks such as ℓp regression and Huber regression\" beyond linear regression, but the relationship to Song et al. (2023) is understated. As acknowledged in Table 1, Song et al. already achieved $\\widetilde{\\mathcal{O}}!\\left(\\frac{\\sqrt{m},n^{1.5}}{\\varepsilon}\\right)$ time for linear, multiple, and ridge regression, where the improvement here is from $n^{1.5}$ to $rn$, which is significant only when $r \\ll n^{0.5}$."}, "questions": {"value": "1. Can you provide explicit bounds on the QRAM size, access time, and circuit depth required for representative problem instances? \n\n2. Can you provide a revised complexity analysis that explicitly accounts for QRAM access time as a function of the data size, and identify at what point (if any) QRAM overhead eliminates the quantum advantage?\n\n3. What exactly is the time complexity of your algorithms without QRAM? Does the quantum advantage survive this modification, and if so, under what parameter regimes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "ztR8b4rxN2", "forum": "2H2aNzDeYX", "replyto": "2H2aNzDeYX", "signatures": ["ICLR.cc/2026/Conference/Submission7839/Reviewer_aYYA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7839/Reviewer_aYYA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883704364, "cdate": 1761883704364, "tmdate": 1762919883635, "mdate": 1762919883635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified quantum framework for accelerating sparsification in generalized linear models (GLMs), yielding quadratic speedups over classical methods across linear, multiple, Lasso, Ridge, Huber, $\\ell_p$, and $\\gamma_p$ regression. The authors extend the recent classical breakthrough of Jambulapati et al. (2024) by developing a Quantum Multiscale Leverage Score Overestimates (QMLSO) algorithm that leverages quantum leverage score approximation and quantum state preparation techniques. For problem dimension $n$, sparsity $r<n$, sample size $m$, and error parameter $\\varepsilon$, their algorithm achieves $\\widetilde{\\mathcal{O}} (\\frac{r\\sqrt{mn}}{\\varepsilon}+\\mathrm{poly} (n,\\frac{1}{\\varepsilon} ) )$ quantum time complexity, demonstrating quadratic speedup in m when $\\varepsilon$ is constant and $m \\gg n$."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive theoretical framework with broad applicability. The paper successfully unifies quantum speedups for a diverse family of regression problems under a single GLM sparsification framework, extending beyond previous quantum work that focused primarily on linear regression. The introduction of proper loss functions and multiscale leverage score overestimates provides elegant abstractions that capture the essential structure needed for quantum acceleration across multiple problem variants.\n2. Rigorous technical development with novel quantum subroutines. The QMLSO algorithm (Algorithm 1) and the quantum weight initialization procedure (Theorem 9) represent substantial technical contributions that carefully adapt classical contractive algorithms to the quantum setting. The complexity analysis is thorough, properly accounting for query complexity to different oracles and demonstrating clear quadratic speedups with explicit leading-order terms that dominate in the regime where sparsification is beneficial $\\varepsilon=\\Omega (\\sqrt{\\frac{n}{m}} )$."}, "weaknesses": {"value": "1. Limited discussion of QRAM requirements. While the paper acknowledges on page 5 (lines 217-220) that \"QRAM serves as a natural quantum analogue of the classical RAM model\" and mentions that \"practical realization of scalable QRAM remains highly uncertain,\" there is insufficient critical analysis of how QRAM requirements scale with problem size and what this means for near-term quantum advantage.\n\n2. Gap between stated contributions and actual novelty over prior quantum work. The paper claims on page 1 (lines 17-18) to address \"broader and both theoretically and practically important regression tasks such as ℓp regression and Huber regression\" beyond linear regression, but the relationship to Song et al. (2023) is understated. As acknowledged in Table 1, Song et al. already achieved $\\widetilde{\\mathcal{O}} (\\frac{\\sqrt{m} n^{1.5}}{\\epsilon} )$ time for linear, multiple, and ridge regression, where the improvement here is from $n^{1.5}$ to $rn$, which is significant only when $r \\ll n^{0.5}$."}, "questions": {"value": "1. Can you provide explicit bounds on the QRAM size, access time, and circuit depth required for representative problem instances? \n\n2. Can you provide a revised complexity analysis that explicitly accounts for QRAM access time as a function of the data size, and identify at what point (if any) QRAM overhead eliminates the quantum advantage?\n\n3. What exactly is the time complexity of your algorithms without QRAM? Does the quantum advantage survive this modification, and if so, under what parameter regimes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "ztR8b4rxN2", "forum": "2H2aNzDeYX", "replyto": "2H2aNzDeYX", "signatures": ["ICLR.cc/2026/Conference/Submission7839/Reviewer_aYYA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7839/Reviewer_aYYA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883704364, "cdate": 1761883704364, "tmdate": 1763523207623, "mdate": 1763523207623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified quantum framework for accelerating sparsification in generalized linear models (GLMs). It yields quadratic speedups over classical methods across linear, multiple, Lasso, Ridge, Huber, $\\ell_p$, and $\\gamma_p$ regression. It extends the recent classical breakthrough of Jambulapati et al. (2024) by developing a Quantum Multiscale Leverage Score Overestimates (QMLSO) algorithm. QMLSO utilizes quantum leverage score approximation and quantum state preparation techniques. To be precise, for problem dimension $n$, sparsity $r<n$, sample size $m$, and error parameter $\\varepsilon$, their algorithm achieves $\\widetilde{\\mathcal{O}} (\\frac{r\\sqrt{mn}}{\\varepsilon}+\\mathrm{poly}(n,\\frac{1}{\\varepsilon} ))$ quantum time complexity. This gives quadratic speedup in $m$ when $\\varepsilon$ is constant and $m \\gg n$."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper successfully unifies quantum speedups for a diverse family of regression problems under a single GLM sparsification framework. It goes beyond previous quantum work that focused primarily on linear regression. The introduction of proper loss functions and multiscale leverage score overestimates provides elegant abstractions of essential structure needed for quantum acceleration across multiple problem variants.\n2. The QMLSO algorithm (Algo 1) and the quantum weight initialization procedure (Thm 9) represent some technical contributions. They carefully adapt classical contractive algorithms to the quantum setting. The complexity analysis is thorough. It properly accounts for query complexity to different oracles. It shows clear quadratic speedups with explicit leading-order terms. These terms dominate in the regime where sparsification is useful."}, "weaknesses": {"value": "1. Limited discussion of QRAM requirements. I know some quantum algorithms and speedup techniques (theory). I do not understand QRAM. this paper mentioned QRAM many times but never precise enough for me to know. For example, on page 5 (lines 217-220): \"QRAM serves as a natural quantum analogue of the classical RAM model\" and mentions that \"practical realization of scalable QRAM remains highly uncertain.\" This is too vague to build much scientific arguments or understanding. There is insufficient analysis of how QRAM requirements scale with problem size and what this means for near-term quantum advantage. (my understanding is, we don’t have QRAM now. And we probably won’t have it soon. But to weigh the practicality and relevance of this paper, I think some details are needed. Please see my questions.)\n\n2. Gap between stated contributions and actual novelty over prior quantum work. I am wondering, the improvements presented in the paper are results of new techniques or mirages from the choice of setting. The paper claims on page 1 (lines 17-18) to address \"broader and both theoretically and practically important regression tasks such as $\\ell_p$ regression and Huber regression\" beyond linear regression, but the relationship to Song et al. (2023) is understated. Also in Table 1, Song et al. already achieved $\\widetilde{\\mathcal{O}} (\\frac{\\sqrt{m}n^{1.5}}{\\varepsilon} )$ time for linear regression, multiple regression, and ridge regression, where the improvement here is from $n^{1.5}$ to $rn^{0.5}$, which is significant only when $r \\ll n$. This needs further clarifications."}, "questions": {"value": "1. Can you provide explicit bounds on the QRAM size, access time, and circuit depth required for representative problem instances? \n\n2. Can you provide a revised complexity analysis that explicitly accounts for QRAM access time as a function of the data size, and identify at what point (if any) QRAM overhead eliminates the quantum advantage?\n\n3. What exactly is the time complexity of your algorithms without QRAM? Does the quantum advantage survive this modification, and if so, under what parameter regimes?\n\n\nTypos:\n\nLine 17 and line 64: I guess the $\\delta_p$ in the abstract of the paper, should be $\\gamma_p$?\n\nLine 75: ''This technique have been'' should be ''This technique has been''.\n\nLine 111: Word ''constructing'' has repeated twice.\n\nLine 375: Word ''the'' has repeated twice.\n\nLine 443: ''this can be reduces'' should be ''this can be reduced''.\n\nLine 489: In reference section, the title of the paper didn't compile correctly.\n\nLine 788: Word ''makeing'' is a typo.\n\nLine 830: change ``update,(Jambulapati'' -> ''update, (Jambulapati''. a space is missing in the writing\n\nLine 1068: ''we obtain we desired'' should be ''we obtain what we desired''\n\nLLM disclaimer: I used LLMs to check (a few of) my claims, to understand some refs mentioned in the submission, and to polish my language."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "ztR8b4rxN2", "forum": "2H2aNzDeYX", "replyto": "2H2aNzDeYX", "signatures": ["ICLR.cc/2026/Conference/Submission7839/Reviewer_aYYA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7839/Reviewer_aYYA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883704364, "cdate": 1761883704364, "tmdate": 1763609987189, "mdate": 1763609987189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a quantum framework for efficiently solving various regression and optimization problems by adapting Jambulapati et. al. into a quantum algorithm and noticing that many of the regression problems can be formulated as sparse generalized linear models. It achieves quadratic speedup comparing to previously best-known algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper provides a unified optimization framework for many of the regression problems in optimization, which provides a convenient way for further quantum algorithm research.\n- The paper itself is presented well, with easy-to-follow narratives."}, "weaknesses": {"value": "- The paper does not highlight clearly the source of the quantum speedup, which might be hard to understand. Also the paper, although citing the Jambulapati et. al., does not discuss in detail how the classic algorithm is related to the quantum version.\n- The contribution of the paper is a bit lacking. Although a unified speedup across different regression problems is nice, the contribution of the paper is limited to implementing the original algorithm with several quantum tricks (like Hamoudi's copy preparation trick, which is well-known and widely applied, for example in https://arxiv.org/abs/2402.12745)"}, "questions": {"value": "- Is it possible to prove a quantum lower bound on the regression problems in the paper?\n- Is it possible to incorporate also infinite-norm regression (i.e., max) into the framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OLW0OD2cIW", "forum": "2H2aNzDeYX", "replyto": "2H2aNzDeYX", "signatures": ["ICLR.cc/2026/Conference/Submission7839/Reviewer_C66o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7839/Reviewer_C66o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904020713, "cdate": 1761904020713, "tmdate": 1762919883242, "mdate": 1762919883242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a quantum algorithm for improving the speed of finding sparsifiers (most commonly, weighted subset of training samples) that approximately preserve the value of the loss function over the parameter domain, allowing for faster training."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper applies recent work on quantum leverage score approximation by Apers and Gribling in the context of recent result showing existence of sparsifiers for a wide class of losses in Generalized Linear Models\\ by Jambulapati et al. The resulting approach improves the complexity of finding the sparsifier: it shares the $\\sqrt{m}$ term with existing approaches, but improves the dependence on $n$ from $n^{1.5}$ to $rn^{0.5}$, an advantage in sparse ($r<<n$) scenarios."}, "weaknesses": {"value": "The result is an incremental advancement combining existing classical work (Jambulapati et al.) with prior quantum algorithms (Hamoudi, Li et al., Apers and Gribling). The presentation in the manuscript makes it unclear which results are prior work and which are novel (e.g. Def. 5. and Def. 6 should include reference to Jambulapati et al.). The main manuscript provides no clear description of the algorithms beyond listing the steps in Alg 1. and Alg. 2, and many of the key elements in these two algorithms are not properly explained. For example, in Alg. 1, ModLevApprox and WeightCompute are jointly described using one, very high-level sentence in the main manuscript; description in the appendix B.1. indicates these are not original contributions, but recapitulation of Apers and Gribling’s Theorem 4. \n\nThe algorithm is aimed at a very practical problem of speeding up GLM, however, by working in a QRAM-based framework, it’s practical applicability on near-to-medium term hardware is heavily constrained."}, "questions": {"value": "Are there application scenarios (e.g. data characteristics) where the constraint related to QRAM can be circumvented?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZaUoSzKQ7f", "forum": "2H2aNzDeYX", "replyto": "2H2aNzDeYX", "signatures": ["ICLR.cc/2026/Conference/Submission7839/Reviewer_Ewfu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7839/Reviewer_Ewfu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941163999, "cdate": 1761941163999, "tmdate": 1762919882842, "mdate": 1762919882842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript presents quantum algorithms for many regression tasks, including linear, multiple, Lasso, Ridge, Huber, $\\ell_p$, and $\\delta_p$ regressions. These algorithms are based on a unified framework, where the core quantum technique is a fast algorithm for approximating leverage scores due to Apres & Gribling (2024).  On top of this, a generalized linear model sparsifier is constructed using the techniques from Cohen & Peng (2015) and Jambulapati et al. (2024).\n\nBased on this GLM sparsification framework, the authors derived a quantum algorithm for various regression problems. These quantum algorithms are faster than best-known classical algorithms in the not-too-precise regime, and are faster than previous quantum algorithms by Song et al. (2023) when the linear model is sparse."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The manuscript is very well written. The statements are supported with rigorous proofs. Although this work relies on existing quantum subroutines, it incorporates quantum subroutines in a nontrivial way to develop multiscale leverage score overestimation and quantum importance sampling, which are key components of this framework. Overall, I think this submission makes a solid theoretical contribution to solving regression problems."}, "weaknesses": {"value": "For linear, multiple, and Ridge-regressions, the quantum algorithms proposed in this work are faster than previous quantum algorithms by Song et al. (2023), mainly due to the sparsity $r \\leq n$. It looks like this improvement is merely due to the fact that the authors considered sparse regression models. Is it the case, or is the improvement truly a consequence of the new techniques?"}, "questions": {"value": "This is related to my question in the previous section. If we consider sparse regression models with sparsity $r$ and use the quantum algorithm by Song et al. (2023) to solve it, what would the dependence on $r$ look like?\n\nA minor typographical issue in lines 111-112: repeated word \"constructing\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rq9IiMKcr3", "forum": "2H2aNzDeYX", "replyto": "2H2aNzDeYX", "signatures": ["ICLR.cc/2026/Conference/Submission7839/Reviewer_QN5x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7839/Reviewer_QN5x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964121199, "cdate": 1761964121199, "tmdate": 1762919882513, "mdate": 1762919882513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}