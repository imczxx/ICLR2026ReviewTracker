{"id": "E9heQLPNGh", "number": 5126, "cdate": 1757851962168, "mdate": 1759897993105, "content": {"title": "Zero-Shot Model Search via Text-to-Logit Matching", "abstract": "With the increasing number of publicly available models, there are pre-trained, online models for many tasks that users require. In practice, users cannot find the relevant models as current search methods are text-based using the documentation which most models lack of. This paper presents ProbeLog, a method for retrieving classification models that can recognize a target concept, such as \"Dog\", without access to model metadata or training data. Specifically, ProbeLog computes a descriptor for each output dimension (logit) of each model, by observing its responses to a fixed set of inputs (probes). Similarly, we compute how the target concept is related to each probe. By measuring the distance between the probe responses of logits and concepts, we can identify logits that recognize the target concept. This enables zero-shot, text-based model retrieval (\"find all logits corresponding to dogs\"). To prevent hubbing, we calibrate the distances of each logit, according to other closely related concepts. We demonstrate that ProbeLog achieves high retrieval accuracy, both in ImageNet and real-world fine-grained search tasks, while being scalable to full-size repositories.  Importantly, further analysis reveals that the retrieval order is highly correlated with model and logit accuracies, thus allowing ProbeLog to find suitable and accurate models for users tasks in a zero-shot manner.", "tldr": "", "keywords": ["weight-space learning", "interpretability"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f15bf4d2bf35d2037049baa34c94b52ac3af3784.pdf", "supplementary_material": "/attachment/da7ed303744364933da1a0605f436f18b243e811.zip"}, "replies": [{"content": {"summary": {"value": "The paper considers zero-shot model search with a logit-matching based method. The authors argue that many models in repositories such as Hugging Face lack textual metadata, making conventional text-based search insufficient. ProbeLog aims to address this by probing models with a fixed set of images, constructing descriptors for each logit, and comparing them with CLIP-derived concept descriptors. However,retrieving models that lack textual documentation is uncommon and not particularly useful in practice. Moreover, the method involves straightforward application of existing techniques (probing, CLIP similarity, and distance calibration) with no significant theoretical or algorithmic innovation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper tries to solve a fairly new problem that is retrieving models in large model repositories without textual information.\n- The paper creates two model zoos for this task\n- The writing is clear."}, "weaknesses": {"value": "- The motivation for the problem itself is weak. In practice, models that lack texts in both metadata and comment section are often incomplete, unverified, or low-quality. Such models are rarely useful for end users and typically should not appear in search results. Consequently, the proposed setting addresses an unrealistic scenario with limited practical or research value.\n\n- The paper argues that “most models are undocumented” based on counting Hugging Face model cards, but this ignores that most undocumented models are toy experiments, checkpoints, or duplicates. The paper does not demonstrate use cases that users actually need to retrieve such models.\n\n- The method essentially reuses existing CLIP embeddings and probing techniques with minor heuristic modifications. There is little conceptual innovation or theoretical insight."}, "questions": {"value": "- what are the concrete use cases for this method? how often do user want to retrieve models without any textual metadata?\n- what are the performance of the retrieved models? although the retrieval accuracy is high, do the retrieved model actually help the users with their tasks, given that most undocumented models are of low quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5RhwDRNOJc", "forum": "E9heQLPNGh", "replyto": "E9heQLPNGh", "signatures": ["ICLR.cc/2026/Conference/Submission5126/Reviewer_zazE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5126/Reviewer_zazE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625069936, "cdate": 1761625069936, "tmdate": 1762917895254, "mdate": 1762917895254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Model zoos like Hugginface contain a ton of models and a sizable portion (~60%) of them are not properly documented or annotated. This paper tackles the problem of searching for relevant classification model for a textual given concept e.g., dog or panda. \n\n- The proposed approach (ProbeLog) works in zero-shot setting. \n- It computes Euclidean distance between probe responses for logits and textual descriptions. \n- The paper proposes two new datasets (INet-Hub and HF-Hub) for the setup.\n- The proposed method yields superior results as compared to baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method outperforms the baseline CLIP-Dissect on Top-1 and Top-5 retrieval accuracies for text-based retrievals and logit classification. \n2. The paper targets the hubbness problem faced by earlier approaches. In particular, they choose 500 classes from ImageNet-21K to calibrate the distances of each logit. Table 4 presents an ablation study over othe datasets.\n3. Ablations studies are perfomed over: (i) Selecting probes: access to probes from target concept’s distribution is useful; (ii) Number of probes to select: ~4000 probes provides a balance between accuracy and efficiency.\n4. The paper presents results with a 1500 models. However, it presents a compelling case of scaling to millions of models. For instance, a one time investment of ∼3300 GPU hours (RTX-A5000 GPU) would be required to proble million classification models. Thereafter, a single dedicated GPU would do the job."}, "weaknesses": {"value": "1. Section 4 could be better written. \n\n(i) Please define $n$ explicitly (assumed it to be number of probes).  \n(ii) Better notation could be used for cosine similarity. $CLIP(x_1, c)$ can be confusing.  \n(iii) Creation of our zero-shot text-based logit descriptors should be properly discussed in the text as well.  \n\n2. The paper should include more baselines since the datasets are proposed in the paper itself. How about comparing against the model retrieved by existing hugginface search or a web search query? \n3. The paper seem to be proposed for a single concept (e.g., dog, pandas) at a time. However, real usage is typically about a specific domain (say animal classification) rather than a single concept. Can the propsal be extended to union or intersaction or multiple concepts? or can it work out of the box?"}, "questions": {"value": "1. Can the model be extended to say text classification methods?\n2. Have you experimented with methods other than CLIP? How do the recent VLM methods perform in this domain?\n3. Please fix small things like: $j^th$ and $I.e.$.\n4. What do you mean by \"We then train the model on the selected data\" (in #312)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H4WiuQDXgE", "forum": "E9heQLPNGh", "replyto": "E9heQLPNGh", "signatures": ["ICLR.cc/2026/Conference/Submission5126/Reviewer_SCNZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5126/Reviewer_SCNZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939762911, "cdate": 1761939762911, "tmdate": 1762917894789, "mdate": 1762917894789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for searching for classification models within large model repositories. To be the best of my knowledge, the proposed application is novel and interesting. The proposed solution, although simple, seems to outperform existing alternatives that were not devised for this application in particular. The similarities between CLIP scores and classifier responses are used to assess the classifier's attunement with the query concept."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The application is useful in practice and removes the burden of having highly technical knowledge to select classifiers from a repository.\n- The proposed solution is simple and yields reasonable results, although far from optimal.\n- There is value in the datasets, although it is not clear from the description how accurate the labeling is."}, "weaknesses": {"value": "- Conceptually, we want the retrieved classifier(s) to be discriminative. It seems like it would be interesting to select logits that report high responses for high CLIP scores and low responses for low CLIP scores. However, this does not seem considered or even discussed by the authors. Clearly stating the difference between the proposed appication os model selection needs to be addressed.\n- I would appreciate a discussion of the model search application within the context of model selection. This traditional topic in machine learning is very related to the proposed model search. How are the two different? What learnings can be used from model selection for model search?\n- Why do the de-meaning and normalization in Equation (5) based on the mean and standard deviation of the entire logit? Why these statistics in particular? A detailed discussion leading to this choice would strengthen the work. Otherwise, it appears to be an ad hoc solution.\n- What happens is the concept provided by the user is not well represented in the set of probes? For example, it would have been interesting to use a query concept and to remove the probes that actually belong to the query concept and see how performance varies. Ablation studies of this kind would add clarity for designing the probe set.\n- Although the authors claim in Appendix F that the approach is unsupervised and the label mapping is only for numerical evaluations, the mapping may be used in the future to create new methods (and it surely was used by the authors to design their approach and select its hyperparameters). From that perspective, the approach is not unsupervised. It is like claiming that a classifier is unsupervised because one is providing a pre-trained version: one surely does not needs labels to use it in practice, but this fact does not make the classifier unsupervised.\n- I think that the authors should add a section/paragraph on the ethical implications of their work. Automating model selection may produce biased or unfair results, in particular if the probe set has inherent biases for example. I'm not concerned by the choices of the authors but think a discussion might be useful for practitioners.\n- How do the results change when using different (e.g., more recent) multimodal models?"}, "questions": {"value": "- How do the results change when using different (e.g., more recent) multimodal models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ez0a4g1Zdt", "forum": "E9heQLPNGh", "replyto": "E9heQLPNGh", "signatures": ["ICLR.cc/2026/Conference/Submission5126/Reviewer_2d7A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5126/Reviewer_2d7A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951831965, "cdate": 1761951831965, "tmdate": 1762917894439, "mdate": 1762917894439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The core problem this paper addresses is: how to find a model capable of recognizing a specific concept (e.g., \"dog\") within a massive (e.g., million-scale) model repository where documentation is severely lacking. The paper proposes a zero-shot model search method called ProbeLog, with the following technical process: The system first uses a fixed set of input images (called \"probes\") to \"probe\" each classification model in the repository. For each output dimension (logit) of a model, its responses (activation values) to all probes are recorded to form a vector. By calculating the distance between a text descriptor and all logit descriptors, the method can identify the logit (and its corresponding model) that best matches the text concept."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The online overhead is not that large.\n\nThe paper is well-written and flows smoothly."}, "weaknesses": {"value": "The probe set is too large. When the model repository is very large, this method will also incur significant offline overhead.\n\nThe logit output is limited to classification models.\n\nIt should be compared with more methods in the field of model routing.\n\nThe method relies heavily on the samples in the probe set. If new images are significantly different (OOD) from the images in the probe set, the method is likely to fail."}, "questions": {"value": "If time permits, could you add more experiments to explain why the performance is not good in specific domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kf10DwP6Ii", "forum": "E9heQLPNGh", "replyto": "E9heQLPNGh", "signatures": ["ICLR.cc/2026/Conference/Submission5126/Reviewer_1n89"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5126/Reviewer_1n89"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003342158, "cdate": 1762003342158, "tmdate": 1762917894182, "mdate": 1762917894182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}