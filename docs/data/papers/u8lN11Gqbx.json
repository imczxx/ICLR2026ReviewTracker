{"id": "u8lN11Gqbx", "number": 22585, "cdate": 1758333103158, "mdate": 1759896858127, "content": {"title": "Diffusion-based dynamics as a cognitive model of human speech production", "abstract": "Human language production requires transforming abstract communicative intent into fluent speech, yet the algorithmic nature of this transformation remains less understood. Most studies aligning large language models (LLMs) with brain activity have focused on autoregressive LLMs (aLLMs), which generate text left-to-right by committing to the next token. While effective at predicting neural and behavioral signatures of comprehension, this paradigm assumes incremental generation. In contrast, diffusion LLMs (dLLMs) construct sentences by iteratively denoising global representations. Despite their distinct generative dynamics, dLLMs now rival aLLMs on standard NLP benchmarks, prompting the question of whether the brain likewise engages in global, iterative refinement—especially during pre-articulatory planning when sentence structure remains flexible. To test this hypothesis, we correlated intermediate denoising steps of a dLLM with electrocorticography (ECoG) activity during naturalistic speech production. dLLM representations explained significant neural variance from pre- to post-production, with especially strong encoding in middle/inferior temporal and motor-related regions. These results support iterative refinement as a plausible neural mechanism of human speech planning.", "tldr": "Diffusion LLM embeddings explained ECoG signal variance in specific cortical regions more effectively than autoregressive LLMs.", "keywords": ["model-brain alignment", "diffusion large language model", "speech production", "ECoG"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/801de89d89c44daaa9f0459dff5cc27ee46ccdfe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tests whether diffusion-based large language models (dLLMs) better align with human brain activity during speech production than traditional autoregressive LLMs (aLLMs), which generate text sequentially. dLLMs iteratively refine entire sentence representation. This process may more closely mirror how the human brain processes linguistic information.\n\nThe study uses electrocorticography (ECoG) recordings from four patients producing natural speech. The authors compare intermediate denoising steps of dLLMs (LLaDA-8B and Dream-7B) with their autoregressive counterparts (LLaMA3-8B and Qwen2.5-7B).\n\nThey find that dLLM embeddings explain more neural variance during both pre- and post-articulatory phases, particularly in middle and inferior temporal as well as motor regions. Early diffusion steps correlate with pre-articulatory activity, while later steps align with articulation and monitoring processes.\n\nOverall, their findings support iterative refinement as a plausible mechanism for speech planning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Quality of writing: The paper is very well written. It clearly states its contributions and situates them in the literature with precision (explaining what dLLMs are, context of psycholinguistics, model–brain alignment). The figures are clear and informative. \n\n- Novel hypothesis and framing: The idea that diffusion dynamics could map onto human speech planning is original and makes sense in the context of model-brain alignment. It challenges the dominance of autoregressive models in brain–language alignment. To my knowledge, this is the first paper attempting such an experiment.\n\n- Data quality: The use of naturalistic ECoG speech production data (4 participants, 50 hours) is extremely valuable. The temporal precision of ECoG allows for fine-grained mapping of pre- and post-articulatory processes.\n\n- Careful model design: The authors implement a psychologically interpretable “greedy confidence” revelation algorithm for diffusion steps and ensure comparability by matching autoregressive steps within the same model. Autoregressive versions of the same architectures are used as controls.\n\n- Comprehensive analysis and discussion: The study includes both lexical-level statistics (frequency, POS) and representational-level analyses (PCA trajectories, JS divergence, encoding correlations). The authors remain cautious in their interpretation, clearly noting that their results are based on a high-quality but limited dataset and emphasizing the need for replication with other modalities such as fMRI and MEG."}, "weaknesses": {"value": "- Data quality: What is a strength is also a limitation. The dataset includes only four participants. While the data are of exceptional quality, this raises concerns about generalizability to other modalities such as MEG or fMRI. The small sample size also limits the statistical rigor of some analyses.\n\n- Limited baselines: The study tests only two diffusion models and their autoregressive counterparts at a medium scale (7–8B parameters). Results might differ with larger or multimodal models, which makes the conclusions somewhat model-specific.\n\n- Potential confound: The dLLM and aLLM comparison involves algorithmic differences: confidence-based token revelation versus prefix growth. Neural alignment differences may therefore reflect these distinct operational mechanisms rather than the diffusion process itself.\n\n- Fixed analysis parameters: The authors use a fixed layer (layer 20) and a fixed number of diffusion samples (5) for their analyses. It would be important to verify that these choices are indeed optimal or robust across settings.\n\n- Minor issue: Typo at line 782: “Brian Encoding.”"}, "questions": {"value": "1. Could the authors test whether their results generalize beyond ECoG? For instance, by evaluating model–brain alignment using fMRI or MEG data with publicly available datasets for comparison?\n\n2. How sensitive are the results to the choice of diffusion layer (layer 20) and the number of denoising samples (5)? Would varying these parameters change the alignment patterns observed?\n\n3. To what extent could the observed alignment differences between dLLMs and aLLMs be explained by the algorithmic disparity between confidence-based revelation and prefix growth? It's unclear whether it changes entirely the way of analyzing the results.  Would random or frequency-based token revelation produce similar neural alignment patterns? This would test if the greedy-confidence rule is critical.\n\n4. Have the authors explored whether larger or multimodal diffusion models reproduce the same alignment advantage, or whether this effect is specific to mid-sized architectures (7–8B)?\n\n5. Given the limited number of participants, can the authors provide effect-size estimates or subject-level analyses to assess the robustness and inter-individual consistency of their findings? How stable are the diffusion–brain correlations across the four patients?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZNWx0RQ1Fo", "forum": "u8lN11Gqbx", "replyto": "u8lN11Gqbx", "signatures": ["ICLR.cc/2026/Conference/Submission22585/Reviewer_UqtJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22585/Reviewer_UqtJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761563708381, "cdate": 1761563708381, "tmdate": 1762942291772, "mdate": 1762942291772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work is part of a broader research effort to explore the relationship between internal representations of large language models (LLMs) and human brain activity recorded during speech production. Specifically, the authors focus on comparing diffusion LLMs (dLLMs) (construct sentences iteratively) with autoregressive LLMs (aLLMs) (follow next-word prediction with a left-to-right sequence) to examine which LLMs better mirrors human-like speech production. The evaluation focuses on comparing the sentence representations across steps in two types of LLMs and demonstrates how the embeddings extracted from the stepwise representations of the target sentences are used in a neural encoding model for predicting ECoG brain activity in speech production. The authors compare brain encoding performance across two types of LLMs to examine neural dynamics in both language and motor regions. Overall, diffusion LLMs provide a new way to iteratively generate sentences and reorganize lexical categories across the course of generation differently than autoregressive LLMs.\n\n**Contributions:**\n\n* *Alignment between diffusion LLMs and ECoG activity during naturalistic speech prediction:* The study extract the internal embeddings from diffusion LLMs across denoising steps and use these representations in neural encoding model to predict ECoG brain activity during speech production, which is methodologically novel. This framing provides a new comparison for language production, whereas autoregressive LLMs are frequently used in language comprehension.\n* *Comprehensive evaluation:* The study compares diffusion LLMs and their autoregressive versions, assessing iteratively denoising global representations from dLLMs and left-to-right sequence representations from aLLMs and relates these representations with brain recordings during speech production. For each encoding model, the authors measure the correlation between actual and predicted brain activity. Further, they compare encoding performance across both types of LLMs, and quantify correlation performance across generative steps in sentence construction and six cortical ROIs.\n\n**Technical summary:**\nThis is primarily an empirical study, and its methodology involves the following components:\n* *Embeddings from diffusion and autoregressive LLMs:* The authors use both diffusion LLMs and autoregressive LLMs to extract the embeddings during speech production. For the autoregressive approach, they follow standard left-to-right generation with five progressive prefixes (20-100%) and extract sentence embeddings at each step. For diffusion LLMs, an iterative denoising approach reveals the best possible word at any position in the sentence; fix the revealed words, and next iterations fill the remaining masks to complete the sentence. Embeddings from each iteration step are then used for encoding.\n* *Neural encoding model:* To train neural encoding model, the authors use banded ridge model, where multiple embedding sources can be combined and passed as input to predict target ECoG recordings. The model is minimised using MSE; ridge parameters are independent per kernel and selected via randomized search over the precomputed kernels. Using the split=true option provides each embedding specific predictions. \n* *ECoG conversation dataset:* The authors use an available ECoG dataset, where four patients engaged in natural conversation with family, friends, and doctors, yielding 50 hours of comprehension and 50 hours of production data. The authors use production data, considering utterance between 5-25 words in length. \n\n**Experimental design/evaluation:**\n\n* *Word-generation dynamics across diffusion steps:* The authors evaluate the word generation across diffusion steps in diffusion LLMs and how these temporal trajectory changes differ from autoregressive LLMs. To quantify the distributional differences, they compute Jensen–Shannon divergence between model families across which shows how generation happens between steps.\n* *Neural encoding performance:* This analysis evaluates whether the step-wise representations extracted from diffusion LLMs better predict ECoG than from autoregressive LLMs. The primary questions are (i) how the encoding performance varies across five generative steps and six cortical ROIs, and further check if there are any differences in activation patterns for early vs. late diffusion steps.\n\n**Main findings:**\nAccording to the authors’ interpretation, the main findings are as follows:\n* dLLMs (25.2%) show stronger step-wise differentiations than aLLMs (9.7%).\n* dLLMs rely mostly on high-frequency words at sentence boundaries (especially at start of sentence) and low-frequency words at mid-sentence, whereas aLLMs show opposite tendency at middle steps. \n* In dLLMs, early diffusion steps are strongly correlated with temporal regions (STG, MTL+ITL), while later diffusion steps show high correlation in STG, IFG, AG."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I found this work to have the following strengths:\n* *Clarity:* The manuscript is well written and well structured. The pipeline in Figure 2 is easy to follow, and clearly presents the differences in generation steps between dLLMs and aLLMs. Later, the greedy-based token revelation algorithm is described clearly.  The banded (multi-kernel) ridge encoding model is explained well, including how model-specific predictions are obtained. The results section clearly reports step-wise differences in neural encoding across the two model families and relates denoising-step representations to activity across cortical ROIs as step index increases.\n* *Originality:* The idea of using diffusion LLMs during speech production in a neural encoding model to predict ECoG is a simple but methodologically novel contribution.. Prior brain encoding studies typically use autoregressive LLMs for both comprehension or production and learn a ridge-regression model. In contrast, dLLMs provide global, iterative refinement of the whole sentence, whereas aLLMs generate left-to-right based only on preceding tokens, offering a complementary approach for pre-articulatory planning.\n* *Significance:* This work is significant in that it contributes to a better understanding of the parallels between language-\t models dynamics and language processing in the human brain over time during speech production. It shows that diffusion LLMs capture neural dynamics qualitatively differ with autoregressive LLMs, and suggest that diffusion LLMs offers global approach that might be a better parallel for how human plan utterances than left-to-right generation in autoregressive LLMs."}, "weaknesses": {"value": "From my perspective, the primary weaknesses of this study arise from the lack of comparison with prior literature, and limited evaluation:\n* *Limited model evaluation:*\n    * The prior work on ECoG data considered speech-to-text language model (Whisper) and  the autoregressive LLM (GPT-2) to perform brain encoding on both speech comprehension and speech production [Goldstein et al. 2025]. In particular, they show that the encoding model accurately predicts neural activity at each level of the language processing in both comprehension and production. However, the current study focuses only on speech production and considers only text-based representations from dLLMs, concluding that dLLMs outperform aLLMs on production.\n    * Speech production relies on both acoustic and auditory-pathways alongside language production. However, this study is limited to text representations across denoising steps. which misses the low-level speech dynamics that are crucial for motor/sensory regions. As a result, the claim that the diffusion LLMs encoder better than autoregressive LLMs for production is underdetermined and may reflect missing pathways rather than diffusion\n\n* *Small sample size and low temporal resolution:* The most significant limitation of current study is that they restrict  utterances to 5–25 words words in length by not considering both short and long utterances. This may be the fact that the current study reports very low encoding performance for both diffusion LLMs ($\\sim$0.05) and autoregressive LLMs ($\\sim$0.03) across six cortical ROIs. In particular, the performance of autoregressive LLMs is significantly lower than prior work that analyzes full comprehension and production datasets and reports higher correlations ($\\sim$r > 0.15, Goldstein et al., 2025) across language and motor regions. As a result, the claims made in the paper are undermined by limited dataset and temporal resolution rather than model choice is unclear.\n     * Sampling ECoG activity at evenly spaced bins is not ideal, as prior study considered full comprehension and production dataset using 25 ms windows from −2000 to +2000 ms around word onset. Ignoring full ECoG activity by considering only evenly spaced bins may limit language-related activity. \n* *Lack of baseline results:* The paper compares only two categories of LLMs without reporting any baseline results. Without a strong baseline model, it is hard to make claims about whether these LLMs are strong at predicting ECoG and have better encoding performance. A strong baseline would establish what these  LLMs predict beyond simpler models, and prior work.\n* *No standard error bars across subjects:* The results reported in Figure 4 are  mean values only for diffusion LLMs and autoregressive LLMs, without accompanying measures of variability such as standard error across subjects. These metrics are critical for evaluating the robustness of the findings, as this may explain each subject’s specific results in production.\nFor a complete and detailed account of both major and minor issues, please refer to the “Questions” section."}, "questions": {"value": "I would like to thank the authors for the interesting comparison of dLLMs and aLLMs in ECoG encoding during speech production in this work. However, there are several points that I believe require further attention/work. I have divided these into major issues, which should be prioritized, and minor ones, which should be addressed for a strong version of current work.\n\n**Major Comments/Questions:**\n* *Small sample size:* While I fully understand the complexity associated with empirical research involving brain datasets, the sample size in this study appears to be very limited. Similar to Goldstein et al. 2025 study, I strongly encourage the authors to consider a full 50hours production ECoG dataset that includes both short and long utterances, and compare the findings of dLLMs and aLLMs with prior approaches. If aLLMs replicate prior performance to Goldstein et al. 2025] work, and dLLMs still result in superior encoding performance than aLLMs across six cortical ROIs, then the authors provide a clear justification for the claims made in the paper and its sufficiency for the conclusions drawn.\n* *Comparison with baselines:* I recommend authors to compute strong baselines such as acoustic features (log-mel or MFCC), mid-level speech embeddings (Whisper encoder/decoder) as these are prior pathways for language production. Then compare two types of LLMs against baselines to establish (i) absolute performance and (ii) whether dLLMs/aLLMs explain unique variance beyond the baselines.  \n* *Variance partitioning on two types of LLMs:* Since authors used the banded ridge model to train encoding models across feature spaces, each kernel may learn unique explainable variance, and shared between them during speech production. I strongly recommend authors to perform following analysis\n     * Perform variance partitioning on both dLLMs and aLLMs to analyze the unique and shared variance of these LLMs across six cortical ROIs.\n     * Using aLLMs, Goldstein et al. 2025 report higher encoding performance in IFG during speech production. I recommend authors to compare this analysis with prior approaches.\n     * I recommend authors to present a ROI × (Unique dLLM, Unique aLLM, Shared) table/heatmap.\n     * This analysis can be conducted at each diffusion step as well.\n* *Reporting variability:* Please report standard errors across four participants alongside mean values to provide a clearer picture of variability in Figure 4 for both dLLMs and aLLMs. Even with a small dataset, such measures are crucial for assessing the robustness and reliability of the results. \n* *Overclaims and Scope:*  With limited sample size, constrained utterance lengths, only production data selection, the claim that dLLMs are more “cognitively plausible” or “human-like” than aLLMs is overstated. Further, the authors only considered five diffusion steps with a greedy-approach, target-aware revelation approach, while aLLMs are evaluated with fixed word distribution with left-to-right prefixes making these are directly comparable. Please soften the language and add matched comparisons (e.g., target-agnostic dLLM steps and/or a rightward-only, non-bidirectional mask-fill variant ) to isolate the effect of diffusion from bidirectionality.\n* *Layer selection rationale:* The choice of layer 20 is justified by prior work on language comprehension, but those studies (e.g., Caucheteux et al., 2022) did not use LLaMA-2 or Qwen-2.5. Selecting “layer 20” across different architectures and tasks lacks grounding. The best layer selection should be based on prior work using the same architecture or, preferably, on empirical validation. Follow Antonello et al. 2023 for best layer selection for LLaMA-2. I recommended authors to empirically select the best layer for Qwen-2.5.\n\n**Minor Comments/Typos:**\nWhile addressing the following points may not be critical to the paper’s core contributions, doing so would enhance the overall quality. \n* Figure 3: Please fix the Y-axis for Figure 3b and 3c.\n* Figure 3: Clarify how the percentage measures are computed.\n* Line 165: Across both types LLMs, are there any differences in training data? initialization, or architecture? Whether the underlying backbone and number of layers are the same across models? Please report a table with model training details, parameters, #layers.\n* Please justify the choice of (steps =5 ) diffusion steps. Does increasing the number of steps change contextual representations or brain-prediction performance? Authors can provide examples by considering steps=10 (generating sentences by considering 10 denoising steps).\n* Goldstein et al. (2025) show clear POS clusters in language embeddings, but current study using aLLMs step representations look cluttered with no obvious POS structure. Please clarify why. Concretely.\n\n**General Advice:**\nThe manuscript presents a comparison study of two types of LLMs in ECoG encoding during speech production and a range of experimental design choices for stepwise feature extraction during denoising and building encoding models. However, the current version lacks a clear comparison with previous work that evaluates both comprehension and production for the same dataset, relies on small sample size and limited experimental evaluation. Adding explicit implications and addressing the above mentioned weaknesses and major comments would make the work stronger."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UXfMRfwlQo", "forum": "u8lN11Gqbx", "replyto": "u8lN11Gqbx", "signatures": ["ICLR.cc/2026/Conference/Submission22585/Reviewer_Gw42"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22585/Reviewer_Gw42"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608232132, "cdate": 1761608232132, "tmdate": 1762942291439, "mdate": 1762942291439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an investigation into the cognitive plausibility of diffusion-based large language models (dLLMs) as a model for human speech production. The authors propose that the iterative, global refinement process of dLLMs may better capture the pre-articulatory planning phase of speech. To test this hypothesis, the authors correlate the internal representations from dLLMs (LLaDA-8B, Dream-7B) and their aLLM counterparts (LLaMA3-8B, Qwen2.5-7B) with electrocorticography (ECoG) data from four patients engaged in naturalistic conversation. They extract embeddings from five progressive \"steps\" for both model classes. For dLLMs, these steps are defined by a novel \"greedy confidence-based token revelation\" algorithm, which iteratively unmasks the tokens of a known target sentence in order of the model's confidence. The authors conclude that these results support iterative refinement as a plausible neural mechanism for human speech planning"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of investigating the connection between diffusion language models and human brain response is novel and interesting."}, "weaknesses": {"value": "1. The biggest problem of this paper is that the authors fail to provide any neurological basis & insight of why dLLMs can align to human brain responses and the connection & differences between aLLMs and dLLMs in aligning to brain response.\nThe research method is common and has been widely used in previous work. So only the analysis of dLLMs-brain alignment becomes an interesting point.\nHowever, this authors just provide brain encoding performance and do not conduct any further analysis.\nAlthough the authors mentioned \"These results suggest that the brain’s production system may function more like an iterative refinement process than a strictly left-to-right generator.\" Such claim is arbitrary and lacks supporting material.\nThis weakness greatly reduces the contribution of this paper.\n\n2. The authors fall short in investigating the alignment between dLLMs and brain response in the context of other cognitive signals (e.g. fmri). Besides, the study's experiment on ECoG data only involves four patients. Given the known variability in electrode placement and individual pathology, N=4 is an insufficient sample size to make broad claims. Previous studies used fmri dataset with hundreds of subjects."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P6q7ZuItWr", "forum": "u8lN11Gqbx", "replyto": "u8lN11Gqbx", "signatures": ["ICLR.cc/2026/Conference/Submission22585/Reviewer_rYyJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22585/Reviewer_rYyJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726958908, "cdate": 1761726958908, "tmdate": 1762942291130, "mdate": 1762942291130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work compares large language models based on diffusion vs. autoregressive processes to predict human brain activity during speech production, as recorded by electrocorticography. The authors show that, with their settings, diffusion LLMs yield better brain scores than autoregressive LLMs, and conclude that the iterative refinement strategy is thus a better neural model of speech planning in humans."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- To my knowledge, this is the first study that investigates the brain alignment of diffusion large language models. \n- While many papers study natural language processing during listening or reading, there are not many works on the production side, which is an interesting complementary research question.\n- The authors compare aLLM and dLLM with similar versions of the same architecture, making the comparison on comparable grounds."}, "weaknesses": {"value": "The work is interesting and novel but not entirely convincing due to some methodological issues.\n\n- An important potential issue is related to how the sentence embeddings are created. \nThis is a technical point that is important to assess the comparison between the aLLM and the dLLM. Here, as far as I understood, the sentence embeddings are created by averaging over all tokens of the sentence (the one corresponding to the production data). In non-autoregressive LLMs, each token at the output has the knowledge of the whole sentence, whereas for an autoregressive LLM, for a given token, only past words are seen. Averaging over all tokens in a non-autoregressive setting feels intuitive, but for an autoregressive LLM, averaging all the tokens, although understandable, is not necessarily the obvious choice. As the sentence gets built, cumulatively averaging the tokens might lead to blurring the embedding. Instead, one might want to use the last token in the sentence. This is an empirical question in the end, but it needs to be tested, as it might result in an unfair comparison between aLLM and dLLM, favoring the non-autoregressive representations. Relatedly, this might explain the very low brain scores for aLLM (Fig. 4a and 6; see notably IFG), which are very weak, and seem inconsistent with the existing literature. Likewise, this crucial point about how the sentence embedding is created could explain the negative results on Fig. 3a, with more blurring leading to less separated dimensions.\n\n- Although partially acknowledged as a limitation in the conclusion, the a priori choice of a layer (layer 20 here) could be hindering some differences between aLLM and dLLM, as the best layer is not necessarily the same for both variants. Although the rationale behind the choice of the best layer makes sense, this choice is questionable. First, previous work has shown that there is large variations in the relative depth of the best layer between model families (see e.g., Antonello et al., 2023, Fig 1c), and also as a function of the performance/model size: the larger the model, the earlier the layer in terms of its relative depth (Hong et al., 2024). \nIt would be interesting to compare layer by layer the brain alignment of both variants (autoregressive and diffusion), which would provide a better ground for a fair comparison between the two aLLMs and the dLLMs. This seems straightforward to investigate, without much more work (albeit more compute).\n\n- Throughout the paper, there are several claims that the results concern specifically the production process (eg \"iterative refinement as a plausible neural mechanism of human speech planning\", \"these results highlight diffusion models as a cognitively plausible framework for capturing the dynamics of human speech production\", \"dLLMs capture neural dynamics of human speech production in ways that qualitatively differ from aLLMs\"). It is not obvious from the results that this is the case. It would be interesting to apply the same analysis to the \"comprehension\" part of the prompt, which would serve as a baseline to compare the production results with.\n\n- Concerning the analyses presented in Fig. 3c and 3d, as well as lines 336 and following, I am not sure I have fully understood. The aLLM results do not seem to depend on any model, simply on the sequential left-to-right nature of the words, contrary to other results, such as the one involving embeddings. If this is true, then the reporting is misleading: it is just a comparison with a baseline, that compares with natural left-to-right ordering in English, not a property of the language models themselves.\n\nMinor comments/typos.\n\n- The Introduction and Related Work might leave an unfamiliar reader with the feeling that only autoregressive LLMs have been used to predict brain activity, whereas many works have used non-autoregressive LLMs (typically BERT) to model natural language processing in the human brain (see e.g., Toneva & Wehbe, 2019; Sat et al., 2019; Schrimpf et al., 2021; Caucheteux et al., 2021; Lamarre et al., 2022). Those references focus on language comprehension, but even for production, Goldstein et al. (2025) use the representation obtained by the encoder of the Whisper model, which is non-autoregressive. \nAlthough this is probably known by the authors, it might be worth discussing these works a bit better.\n\n- typos: Fig. 2 average vs. averaged"}, "questions": {"value": "- How exactly are the sentence embeddings created? And how the results depend on this choice? In particular, using the last token for the sentence representation might be a better choice for the autoregressive LLMs (see Weaknesses section). \n\n- How was the threshold for sentence length chosen? A look at the distribution of sentence length, Fig. 5a, does not provide an obvious answer such as a natural boundary. It seems that the specific range that is chosen by the authors might favor the dLLM compared to the aLLM. Imagine a very long sentence; it feels like at some point the left-to-right sequential order should be important for the alignment with brain activity. It would be fairer to use a larger range of the distribution, and then study how the two approaches compare as a function of sentence length. This should result in a cleaner and more insightful way to compare the two approaches, and might provide insights on how the two processes might complement each other on different time scales.\n\n- What are the positions of the tokens at each step during token revelation in dLLMs?\nAs a supplementary figure, it would be useful to see the successive positions of the tokens that are chosen. The case of a left-to-right reveal would amount to the strategy provided by the aLLM. Such an analysis would be useful to see if there are specific biases in the working of the dLLM, and the departure from the autoregressive processes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dhi7RFjpxj", "forum": "u8lN11Gqbx", "replyto": "u8lN11Gqbx", "signatures": ["ICLR.cc/2026/Conference/Submission22585/Reviewer_sif2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22585/Reviewer_sif2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820066528, "cdate": 1761820066528, "tmdate": 1762942289094, "mdate": 1762942289094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to test a hypothesis on whether the human brain conducts iterative refinement of language production during pre-articulatory planning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is the first to test, via ecog analysis using diffusion modeling, whether the human brain may conduct iterative refinement of language production during pre-articulatory planning.  I found the paper’s goal to be interesting, and supported by  experimentation."}, "weaknesses": {"value": "- I have a general hesitation toward papers that take one particular modeling approach and then compare its alignment to another model with the brain data. The authors should be even more careful in stating the implications of these findings. If a model fits better with neural data, it does not yet mean that this model has anything to do with the brain representation or processes — it simply means that the model has a better fit in the studied aspect than some other model, but there is no mechanistic evidence of the brain functionality.\n\n- The outputs of the models with are different than standard autoregressive models, but that could also be simply because autoregressive are *autoregressive* and output the most likely outcomes given the t-0..k observations. Diffusion has a different approach based on denoising, and it is natural that the inference steps treat the likelihoods differently. Why would that be strongly linked to the brain function? Couldn't this simply be because the model works differently? \n\n- Some key recent papers on (semantic) language reconstruction (see e.g. Nature and Nature Communications / Communications Biology, but also CS venues) are missing and should be discussed."}, "questions": {"value": "- I would have liked to see control conditions with permutated data that breaks the ties between the neural recordings and the language, i.e., retains the distribution of data, but has random connections to the language tokens. Sometimes a model follows a kind of mode collapse average outputs, and this explains part of the results. Also, significance testing should be done for this according to the keep-it-maximal strategy.\n\n- I would have liked to see an analysis on generative performance with more controlled scenarios, like predictions on \"unlikely\" outcomes for which either autoregressive nor other model relying strongly on sequential information would not give high likelihoods. It may well be that this would have led to different conclusions. In summary, the present evidence is sufficient, but not comprehensive, for the claims on brain functionality."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Brain data, but with public dataset"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vS5EbTOOtd", "forum": "u8lN11Gqbx", "replyto": "u8lN11Gqbx", "signatures": ["ICLR.cc/2026/Conference/Submission22585/Reviewer_V6ig"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22585/Reviewer_V6ig"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851697408, "cdate": 1761851697408, "tmdate": 1762942288448, "mdate": 1762942288448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}