{"id": "jVKhAfg0LS", "number": 24993, "cdate": 1758362898705, "mdate": 1759896738927, "content": {"title": "Adversarial Attacks on Medical Hyperspectral Imaging Exploiting Spectral-Spatial Dependencies and Multiscale Features", "abstract": "Medical hyperspectral imaging (HSI) represents a transformative innovation in diagnosing diseases and planning treatments by capturing detailed spectral and spatial features of tissues. However, the integration of deep learning into medical HSI classification has unveiled critical vulnerabilities to adversarial attacks. These attacks compromise the reliability of clinical applications, potentially leading to diagnostic inaccuracies and jeopardizing patient outcomes. This study identifies two fundamental reasons for the susceptibility of medical HSI models to adversarial manipulation: their reliance on local pixel dependencies, which are essential for preserving tissue structures, and their dependence on multiscale spectral-spatial features, which encode hierarchical tissue information. To address these vulnerabilities, we propose a novel adversarial attack framework specifically tailored to medical HSI. Our approach introduces the Local Pixel Dependency Attack, which exploits spatial relationships between neighboring pixels, and the MultiScale Information Attack, which perturbs spectral and spatial features across hierarchical scales. Experiments on the Brain and MDC datasets reveal that our method significantly reduces classification accuracy, particularly for critical tumor regions, while maintaining imperceptible perturbations. Compared to existing methods, the proposed framework highlights the unique fragility of medical HSI models and underscores the urgent need for robust defenses. This work highlights critical vulnerabilities in medical HSI models and demonstrates how leveraging local pixel dependencies and multiscale spectral-spatial features can guide the development of targeted defenses to enhance model robustness and clinical reliability.", "tldr": "", "keywords": ["medical hyperspectral", "adversarial attack", "spectral-spatial dependencie", "multiscale features"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1bcd63b2d0180bd22985f592f2fae55e79f04754.pdf", "supplementary_material": "/attachment/2e2a81b0195e7b50f576bb6e744f113dcd14babb.zip"}, "replies": [{"content": {"summary": {"value": "The paper targets adversarial robustness in medical hyperspectral imaging (MHSI) classification. It introduces two training-free attacks: Local Pixel Dependency Attack (LPDA), which averages gradients in local windows to preserve spatial coherence, and Multiscale Information Attack (MIA), which injects perturbations across down-/up-sampled scales and aggregates them. Experiments on the Brain and MDC datasets, with several classifiers and defense networks, show that these attacks sharply degrade tumor-class accuracy while keeping global OA/AA/Kappa nearly unchanged. Appendix studies window and scale sensitivity and component ablation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Addresses a practically significant yet under-explored topic: robustness of MHSI models.\n\nExploits spatial and multiscale dependencies that align with medical-image classification characteristics. Addresses a practically significant yet under-explored topic: robustness of MHSI models."}, "weaknesses": {"value": "Lacks quantitative and visual assessment of perturbation imperceptibility.\n\nNo adaptive-attack evaluation against defenses; robustness conclusions remain incomplete.\n\nSeveral symbols and implementation details are ambiguous (local-gradient computation, scale-set definition).\n\nMethodological innovation and theoretical explanation are limited."}, "questions": {"value": "Provide quantitative metrics (PSNR/SSIM/spectral-curve shift) and ROI visualizations of perturbation imperceptibility.\n\nDescribe and evaluate adaptive attacks tailored to RCCA, WFSS, AIAF, and S3ANet.\n\nSpecify the position-dependent gradient computation and parameter settings (window, stride, iterations).\n\nDefine the scale set S and interpolation operators precisely.\n\nAdd mechanism-level evidence (feature-response or gradient heatmaps, cross-scale disruption visualization)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2LPyK5ms2T", "forum": "jVKhAfg0LS", "replyto": "jVKhAfg0LS", "signatures": ["ICLR.cc/2026/Conference/Submission24993/Reviewer_KRRQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24993/Reviewer_KRRQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674056713, "cdate": 1761674056713, "tmdate": 1762943276856, "mdate": 1762943276856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores the adversarial threat against deep learning-based Medical Hyperspectral Imaging (HSI). Specifically, the paper studies two fundamental reasons for the medical HSI models' vulnerability: dependence on local pixel dependencies and reliance on multiscale spectral-spatial features. To this end, the paper introduces an adversarial attack scheme specifically against medical HSI by exploiting the spatial connection between neighborhood pixels associated with multiscale information. Thus, the attack perturbs both spectral and spatial features across hierarchical scales. Extensive experiments across the brain and the multi-dimensional choledoch database have demonstrated the efficacy of the proposed medical adversarial attack scheme. Furthermore, systematic analyses have also justificated the effectiveness of the method design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is generally well-written. The motivation and the medical attack topic are interesting. It's also significant to highlight the adversarial security threat of the modern deep learning-based diagnosis system.\n2. The experimental results are promising. The proposed method achieves superior attack efficacy across diverse benchmarks and scenarios.\n3. Detailed discussion of the importance of adversarial threats in the context of computer-aided medical image analysis is concrete and insightful.\n4. The experimental setup and evaluation metric introduction are comprehensive."}, "weaknesses": {"value": "1. The proposed method seems to focus solely on the white-box scenarios. However, most of the deep diagnosis systems/models are in a black box in practice. Thus, the paper should additionally discuss the black-box extension.\n2. The paper primarily focuses on the empirical justification of the spatial-spectral features, yet the theoretical analyses are limited. This somehow restricted the efficacy justification of the proposed method.\n3. Visualizations of adversarial attacks for medical images are missing. The paper primarily focuses on the quantitative results. More insights from the qualitative results should be given.\n4. It seems that the proposed method is also applicable to natural images, yet few discussions about the specificity of the proposed method in the medical image analysis area are included."}, "questions": {"value": "1. Can the proposed method be extended to a black-box scenario for a real-world attack simulation?\n2. Is it possible to include error bar tests to evaluate the stability of the proposed adversarial attack method?\n3. Can the proposed method be extended to the natural image domain to conduct adversarial attacks? If so, then what's the unique contribution to the medical domain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MIYxIz2SHn", "forum": "jVKhAfg0LS", "replyto": "jVKhAfg0LS", "signatures": ["ICLR.cc/2026/Conference/Submission24993/Reviewer_12ny"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24993/Reviewer_12ny"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900095179, "cdate": 1761900095179, "tmdate": 1762943276582, "mdate": 1762943276582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the vulnerabilities of deep learning models used in medical hyperspectral imaging (HSI), revealing that their reliance on local pixel dependencies and multiscale spectral-spatial features makes them highly susceptible to adversarial attacks. The authors introduce two novel attack methods—Local Pixel Dependency Attack and Multiscale Information Attack—which exploit spatial relationships and hierarchical features to generate imperceptible perturbations that drastically reduce classification accuracy. Experiments on brain and cancer datasets demonstrate that these attacks outperform existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+) The paper is well-organized and easy to follow\n\n+) An important topic and raising awareness of the risks posed by adversarial attacks in sensitive medical applications"}, "weaknesses": {"value": "-) The discussion is limited in the digital domain, and may not be feasible and practical to perform these attacks in real-world hospital environments\n\n-) The proposed attacks focus primarily on classification tasks; their impact on other medical imaging tasks (e.g., segmentation, detection) is not explored."}, "questions": {"value": "a) What defense mechanisms can we perform to mitigate these specific attacks in clinical settings?\n\nb) How feasible is it for an attacker to deploy these methods in real-world hospital environments?\n\nc) Do the proposed adversarial attacks perform on other medical imaging tasks, such as segmentation or detection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FElAoVx9sj", "forum": "jVKhAfg0LS", "replyto": "jVKhAfg0LS", "signatures": ["ICLR.cc/2026/Conference/Submission24993/Reviewer_L9wW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24993/Reviewer_L9wW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070218845, "cdate": 1762070218845, "tmdate": 1762943276064, "mdate": 1762943276064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel adversarial attack framework for medical hyperspectral imaging (MHSI), focusing on vulnerabilities arising from local pixel dependencies and multiscale spectral-spatial features. It proposes two attack methods: the Local Pixel Dependency Attack, which exploits spatial relationships between neighboring pixels, and the Multiscale Information Attack, which introduces perturbations across multiple resolutions to target hierarchical spectral and spatial features. The authors demonstrate the effectiveness of these attacks through experiments on the Brain and MDC datasets, showing that their methods significantly reduce classification accuracy in clinically relevant areas, such as tumor detection, while keeping perturbations imperceptible. The paper also includes an ablation study, confirming the enhanced effectiveness of the combined attack strategies. While the paper addresses important issues in MHSI, the methods proposed are not sufficiently novel, as they largely build on existing adversarial attack techniques without offering a breakthrough specific to medical hyperspectral imaging."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper focus on an interesting research area.\n2. The motivation of this work is clear and convince."}, "weaknesses": {"value": "1. While the paper introduces the Local Pixel Dependency Attack and Multiscale Information Attack, these techniques are not significantly novel in the broader context of adversarial attacks. Similar methods, such as Spectral-Spatial FGSM (SS-FGSM) and Spectral-Spatial Attack (SSA), already explore pixel-level and spectral-spatial perturbations in hyperspectral imagery. \n2. The proposed attack methods, such as the Local Pixel Dependency Attack and Multiscale Information Attack, rely heavily on gradient-based perturbations, which are commonly used in many adversarial attack frameworks. \n3. While the Local Pixel Dependency Attack exploits local pixel relationships, the method could benefit from a more sophisticated model of spatial dependencies. The attack assumes that the spatial context can be effectively modeled with a simple local window (e.g., 3x3, 5x5). However, in medical hyperspectral images, spatial dependencies may be more complex and non-linear, especially in tissue boundaries, tumor regions, or vascular networks.\n4. The current framework focuses on attacking a single model at a time, but adversarial attacks are often evaluated for their transferability across different models. In real-world scenarios, medical hyperspectral imaging models may vary in architecture and training methodology, making it crucial to assess whether the proposed attacks are model-agnostic."}, "questions": {"value": "1. Have the authors considered testing the transferability of the attacks across different model architectures (e.g., CNNs, transformers, etc.)? If the attacks work primarily on one model but fail on others, how generalizable do the authors believe their framework is for real-world clinical systems?\n2. Did the authors consider prioritizing specific spectral bands that might have higher diagnostic value, particularly in medical applications like tumor detection?\n3. How do the authors balance the attack's strength with the need for imperceptibility, especially in clinical settings where the perturbations must be indistinguishable from real images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZvqmLZ3OvX", "forum": "jVKhAfg0LS", "replyto": "jVKhAfg0LS", "signatures": ["ICLR.cc/2026/Conference/Submission24993/Reviewer_WyoT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24993/Reviewer_WyoT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762395390716, "cdate": 1762395390716, "tmdate": 1762943274883, "mdate": 1762943274883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}