{"id": "eprVJ7NRYk", "number": 24563, "cdate": 1758358002364, "mdate": 1759896760279, "content": {"title": "Neural Implementations of Rational Approximation: CauchyNet and XNet", "abstract": "Rational approximants often outperform polynomials, especially near nonsmooth structure. On bounded 1D domains, they attain optimal rates (exponential for analytic targets; root-exponential for analytic functions with finitely many singularities). Yet scalable neural parameterizations with classical rates are limited.\nWe propose \\textbf{CauchyNet}, a rational parameterization from the Cauchy integral formula that we implemented in a neural network.\nFor scalability, we employ \\textbf{XNet}, a ridge-projected Cauchy layer with linear $\\mathcal{O}(MN)$ complexity. \n\nAcross parameter-matched approximation and PDE tests, Cauchy-based models show the expected rate diagnostics and strong accuracyâ€“compute trade-offs.", "tldr": "We introduce CauchyNet and XNet, neural architectures leveraging Cauchy integral formula for scalable rational function approximation in solving PDEs.", "keywords": ["Rational Approximation", "Complex Analysis", "Function Approximation", "Partial Differential Equations"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1639c946cbe36789db0a8e0e511f182e2aaed29.pdf", "supplementary_material": "/attachment/815f90d9d8599a33690e056c5e1b10df0465707f.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces CauchyNet, a neural network architecture based on multivariate partial fractions. After introducing the architecture, the authors apply it within the context of physics-informed neural networks and demonstrate its effectiveness on several benchmark problems."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The idea of using rational activation functions has been proposed in the literature and is motivated by the gain of expressivity. The authors build on this idea and propose a novel architecture based on multivariate partial fractions.\n- The comparison of the authors between their architecture and a PINN with a fixed activation function shows promising results in terms of final accuracy."}, "weaknesses": {"value": "- Section 2 is very hard to follow since the architecture is mostly defined in one dimension and the definition through the tensor product to high dimensions is not very well explained.\n- After simplification in Section 2.3, it seems that the architecture boils down to using a low degree adaptive rational activation function in each neuron. If this is the case, the novelty of the architecture is limited over Boulle et al. and Molina et al.\n- The authors mention that existing rational neural networks lack connection to approximation theory. I strongly disagree with this statement, the approximation rates from Boulle et al. are quasi optimal (to approximate functions with $k$ derivatives in dimension $d$ and Telgarsky also provides approximation results for rational neural networks). One can simply extend these results to analytic functions to show exponential convergence.\n- The authors mention that one of their main contributions is to prove convergence rate for their architecture. However, there is no theory in the paper besides the citation of the known rational approximation result from Newman and Stahl. It is not clear that the proposed architecture achieve this rate given the softplus regularization.\n- There is a lack of core references from the literature on the use of different activation functions (including adaptive ones in PINNs by Jagtap et al.), rational functions and neural networks (e.g. by Telgarsky), and approximation theory for neural networks (e.g. by Yarotsky)."}, "questions": {"value": "- Could the authors report the computational cost of training CauchyNet compared to a standard PINN?\n- Could the authors clarify the novelty of their architecture compared to existing rational neural networks with adaptive activation functions?\n- The numerical experiments should be compared with neural networks using adaptive activation functions (see Jagtap et al.) and previous rational neural networks architectures. At the moment, the proposed architecture is larger than the baselines due to the rational weights."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uiHoP0HwHm", "forum": "eprVJ7NRYk", "replyto": "eprVJ7NRYk", "signatures": ["ICLR.cc/2026/Conference/Submission24563/Reviewer_NnQh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24563/Reviewer_NnQh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760708791084, "cdate": 1760708791084, "tmdate": 1762943123335, "mdate": 1762943123335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents two neural architectures, CauchyNet and XNet, which are based on the principle of replacing standard activation functions with learnable rational approximations. CauchyNet is presented as a direct realization of the multivariate Cauchy integral formula, summing a series of atoms. The XNet is a more efficient implementation which uses ridge-projections of 1D Cauchy atoms to scale to high-dimensional applications. The authors claim these methods preserve exponential or root-exponential approximation rates for analytic and piecewise-analytic functions, and show several experiments including function approximation, solving PDEs, and generative modeling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The authors provide code. I have run this and the results are consistent with those reported in the paper.\n- The authors test across several domains and demonstrate that the Cauchy/XNet layers can be trained stably with good results. \n- The idea of integrating rational function theory into neural architectures is conceptually appealing and might inspire new directions in neural approximation theory."}, "weaknesses": {"value": "1. Limited Novelty. The main activation, as well as much of the detail on the CauchyNet and XNet, has already been introduced in \"Cauchy activation function and XNet,\" reference [6] in the original work. This paper appears to just implement these within a neural network. While the connection to classical rational approximation is clearly emphasized, it is hard to say that this constitutes a novel contribution beyond the prior work in this field.\n2. Weak experimental baselines. The baselines are limited to shallow MLPs with standard activation functions. PINNs have seen many improvements from the naive implementation. Moreover, there are many PDEs which exhibit \"sharp local features\" which the authors suggest would be modeled more accurately with rational approximation. Demonstrating a drop-in replacement of an MLP by a CauchyNet in a challenging problem where even modern alternatives to PINNs (FBPINNs, PIKANs) would be more convincing. Likewise, the RationalNet (ref. 1 in the original work) or Kolmogorov-Arnold networks [arXiv 2404.19756] also use learnable activations (whether rational or otherwise) and see little-to-no comparison in this work. \n3. Limited literature context. Related to the previous point, RationalNet and KANs are not only two examples of alternative baselines, but also serve as two examples of related works which are not adequately discussed in this work. I encourage the authors to review related literature and include a broader set of sources, as well as sufficient descriptions of their work in this broader context.\n4. Scalability not convincingly demonstrated. Although XNet is claimed to scale linearly, there are no experiments which demonstrate this. I attempted to reproduce large-scale tests of an autoencoder with XNet or CauchyNet replacements for some layers. I found a lot of difficulty in scaling these up to the order of millions of parameters and maintaining stable gradients. I used several tricks from the paper to ensure stability, but in the end I still found that the model did not do well to model functions with sharp gradients and detailed features. The paper would benefit from a true large-scale implementation to substantiate the \"scalable\" claim."}, "questions": {"value": "1. How does this work differ conceptually and technically from \"Cauchy activation function and XNet\"? Are CauchyNet and XNet an extension or merely an implementation of that prior architecture?\n2. Can the authors provide results on more competitive baselines (mentioned above)?\n3. Could the authors provide experiments which demonstrate the ability to scale to realistic scenarios, beyond toy problems?\n4. How sensitive is the approach to the choice of the denominator offset? Is there any analysis on the stability landscape or conditioning for varying this hyperparameter?\n5. Can the authors comment in technical detail on how their proposed approaches compare to rational activation functions with trainable poles? For example, PAU [arXiv 1907.06732] and rational ReLU [arXiv 2502.06283].\n6. Is it possible to ensure that the pole-separation margin remains controlled during training deep networks, beyond the preservation rates presented in the paper? Would this introduce something like an implicit regularization constraint, and would this have the potential to limit expressivity at the cost of stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sBLaQ96EWd", "forum": "eprVJ7NRYk", "replyto": "eprVJ7NRYk", "signatures": ["ICLR.cc/2026/Conference/Submission24563/Reviewer_WpTv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24563/Reviewer_WpTv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850403064, "cdate": 1761850403064, "tmdate": 1762943123128, "mdate": 1762943123128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors aim to construct an architecture that inherits the optimal convergence rates of rational approximants while avoiding the curse of dimensionality which follows from tensor product constructions to reach high dimensions. They do this by using XNets to construct a modular layer that builds ridge-projected Cauchy layers. In 1D the method reduces to the classical theory, and empirical measurement are used to gauge accuracy/capacity in higher dimensions. \n\nThis is a mathematically sound and nice paper with a clear tie to classical approximation theory. The authors demonstrate substantial accuracy gains for a variety of benchmarks. \n\nOverall this is nice but straightforward. The results outperform MLPs for simple regression and PINN tasks. It would have been nice to see some more challenging comparisons (e.g. to deep resnets or transformers)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The problem is well motivated and provides clear advantages over mlps."}, "weaknesses": {"value": "The evidence is primarily experimental, and improvement is demonstrated on simple problems against weak competiting architectures. Additional comparisons to more challenging benchmarks might strengthen the impact, otherwise this strikes me as a \"make an applied math idea trainable and see if it does good regression\" paper, which I personally find interesting but may be of less interest to the ICLR community."}, "questions": {"value": "no questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5rSruNKXaU", "forum": "eprVJ7NRYk", "replyto": "eprVJ7NRYk", "signatures": ["ICLR.cc/2026/Conference/Submission24563/Reviewer_48b8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24563/Reviewer_48b8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857205288, "cdate": 1761857205288, "tmdate": 1762943122864, "mdate": 1762943122864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CauchyNet and XNet, which are model architectures that are similar to rational function interpolants, based on the Cauchy integral formula. The architecture is obtained by discretizing the integral formula using quadratures. In one dimension, this inherits theoretical rates from classical rational approximant theory. Directly scaling the quadrature with dimension would result in exponentially many poles, so the paper proposes an alternative high dimensional model called XNet. This is obtained by projecting the higher dimensional Cauchy layer along a few ridge directions, leading to linear scaling with dimension. Numerical experiments showcase improved performance of these architectures compared to baselines in common PDE problems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well written and easy to follow.\n* Theory seems to be sound and relevant.\n* The architecture seems to be novel, at least in the higher dimensional case (XNet). \n* Numerical results are strong and convincing."}, "weaknesses": {"value": "* It seems like the theory for the high dimensional model is a bit narrow, in that in needs separability of the ridges. \n* Improvements over classical rational and spectral methods is a bit unclear, and the gap to prior work is also not that clear to me. \n* The main benefit of the ridge-projected CauchyNet is the dimension scaling. However, as per my understanding, all experiments are done in 3 dimensions or less."}, "questions": {"value": "1. What is the scope for theory in non-ridge settings?\n2. Could the authors elaborate on the improvement of this approach to classical spectral methods for solving PDEs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J6SIJVSEMw", "forum": "eprVJ7NRYk", "replyto": "eprVJ7NRYk", "signatures": ["ICLR.cc/2026/Conference/Submission24563/Reviewer_WAEZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24563/Reviewer_WAEZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154775460, "cdate": 1762154775460, "tmdate": 1762943122623, "mdate": 1762943122623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}