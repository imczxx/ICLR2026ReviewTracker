{"id": "dF4gObeMCu", "number": 7701, "cdate": 1758032745913, "mdate": 1759897838493, "content": {"title": "Motion Score Matching in Video Generation", "abstract": "Subtle motions in videos are critical for overall realism for video generation but are overlooked by current score distillation methods like distribution matching distillation. Current distillation methods prefer to match the style first, %because that will\nsince it takes up most of the numerical significance. Such a distillation scheme will only create poorly generated motions, severely degrading the overall realism after distillation. \nTo address this, we propose motion score and enforce the matching of motion distribution in distillation. \nWe show that matching motion distribution is vital for the quality of generated videos.", "tldr": "Explicit score matching for motions in video generation to enhance the overall realism.", "keywords": ["Diffusion", "Score Matching", "Video Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/25370044bba933b643d47e7b4528b16a31e41adb.pdf", "supplementary_material": "/attachment/0965a30f5b73ff49e70e3ec2683ec3c6be477bdb.pdf"}, "replies": [{"content": {"summary": {"value": "The paper shows that current video diffusion training may overlook temporal motion, harming video quality. It proposes Motion Score Matching to better model motion and improve generation. This approach also aims for broader applications in 3D, physics-aware, and graphics research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem is clearly motivated, targeting a specific motion-related challenge in the popular DMD framework."}, "weaknesses": {"value": "**Fundamental difference:**  \n  While the problem is clearly motivated, the proposed MSM approach mainly adds a regularization loss (eqs. 10 and 11); it is unclear what fundamental difference this introduces compared to the vanilla DMD loss.  \n\n**Comparison to DMD variants:**  \n  DMD has newer variants such as DMD2, and it would be helpful to clarify what aspects of motion modeling the MSM loss improves beyond these existing methods.  \n\n**Hyperparameter:**  \n  The weight $\\\\lambda$ in eq. 11 is not specified, and its impact on the results should be discussed.  \n\n**Evaluation metrics:**  \n  Including motion-specific metrics, such as FVMD [1], could better highlight the effect of the proposed method on motion quality.\n\n\nRef:\\\n[1] Liu, J., Qu, Y., Yan, Q., Zeng, X., Wang, L. and Liao, R., 2024. Fr'echet Video Motion Distance: A Metric for Evaluating Motion Consistency in Videos. arXiv preprint arXiv:2407.16124."}, "questions": {"value": "Please see the [Weakness] section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xj6EtUhcmq", "forum": "dF4gObeMCu", "replyto": "dF4gObeMCu", "signatures": ["ICLR.cc/2026/Conference/Submission7701/Reviewer_oerc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7701/Reviewer_oerc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642643506, "cdate": 1761642643506, "tmdate": 1762919761428, "mdate": 1762919761428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that in Distribution Matching Distillation (DMD) for video diffusion, motion cues are buried because adjacent frames differ only slightly and forward diffusion further shrinks those differences. It proposes Motion Score Matching (MSM): define a motion score as the temporal difference between the teacher’s score outputs of adjacent frames, and add a KL-matching term so the student aligns with the teacher in this motion-score space. MSM is combined with DMD (weighted by λ) and evaluated by distilling WAN2.1-T2V on MixKit. The paper reports CD-FVD improvements (and a re-interpretation of TRAJAN under mostly-static videos) in Stage-1 and smaller gains in Stage-3, while MSM is not applicable to Stage-2 (sequential distillation) because it requires identical timesteps for all frames."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear, plausible motivation for “motion burial.” Two contributing factors are articulated: (i) inter-frame signal is small relative to appearance; and (ii) forward diffusion scales inter-frame differences by \\alpha_t, further diminishing motion. The accompanying derivations are straightforward and convincing.\n\n2. Simple, low-overhead formulation. MSM amounts to a temporal difference over score outputs plus a KL term, combined with DMD via a scalar weight. It is easy to implement and essentially free at inference time.\n\n3. Some quantitative signal. On MixKit, CD-FVD improves. The paper also discusses why its TRAJAN numbers are interpreted as lower-is-better in mostly-static settings."}, "weaknesses": {"value": "## major\n1. Insufficient qualitative evidence and visualization protocol\n\n\t•\tCurrent presentation relies on only a couple of examples, which is not enough to substantiate claims of noticeably improved motion.\n\n\t•\tAdopt a systematic side-by-side protocol: same prompts, same seeds, same frame counts; frame-aligned comparisons; zoom-ins / slow-motion snippets to expose motion details.\n\n\t•\tInclude an intuitive “motion burial” visualization (e.g., illustrating per-frame score/flow magnitude over time) to make the motivation directly observable.\n\n\t•\tConsider a human preference study (pairwise votes/CMOS) to complement metric-based evidence.\n\n2. Limited quantitative scope and metric framing\n\n\t•\tExperiments are only on MixKit (~6.5k clips); add at least one more public dataset (e.g., WebVid, UCF-101, MSR-VTT) to test generality.\n\n\t•\tThe TRAJAN lower-is-better re-interpretation under mostly static videos introduces ambiguity; pair it with motion-oriented diagnostics (e.g., optical-flow consistency, speed/acceleration distributions, trajectory smoothness) and standard CD-FVD/FVD to reduce reliance on any single interpretation.\n\n\t•\tReport results consistently across multiple metrics and summarize trade-offs (appearance vs. motion) to clarify the operating regime.\n\n3. Missing ablations, robustness, and comparative baselines\n\n\t•\tProvide sensitivity analyses: MSM weight \\lambda, timestep schedules, sampling steps/video length.\n\n\t•\tTest different teachers (e.g., CogVideoX) and vary video duration/frame rate to probe robustness.\n\n\t•\tAdd comparisons to alternative motion-enhancing distillation or motion/appearance-disentangling methods to position MSM within the landscape.\n\n\t•\tIf feasible, include cross-dataset generalization as part of robustness (train on one, evaluate on another).\n\n## minor\n\n4. Reproducibility details are thin. While the WAN2.1-T2V teacher and the student setup are outlined, the implementation appears to rely on external code with a promise of future release. Practical details—compute budget for training to 14k iters, data cleaning/curation, and other training specifics—are not fully documented.\n\n5. Scope/structure feels imbalanced. A sizeable portion of the paper is devoted to Discussion/Future Work, while central empirical questions remain under-explored. The contribution reads as an incremental, practical add-on that would benefit from broader experiments and stronger visual evidence to close the empirical loop.\n\n6. Ablations and robustness (if not already covered above). It would be helpful to report sensitivities to \\lambda, timestep sampling schedules, frame rate/video length, and different teachers (e.g., CogVideoX), and to show generalization on at least one additional public dataset. This would clarify robustness and the method’s operating envelope."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "e3A0qSFEaV", "forum": "dF4gObeMCu", "replyto": "dF4gObeMCu", "signatures": ["ICLR.cc/2026/Conference/Submission7701/Reviewer_E2iG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7701/Reviewer_E2iG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649251643, "cdate": 1761649251643, "tmdate": 1762919760591, "mdate": 1762919760591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "this paper is trying to tackle the problem of video diffusion models producing weird or janky motion. The authors argue that current distillation methods (DMD) tend to gloss over subtle movements, messing up the final video.\n\nThe main contribution here is \"Motion Score Matching\" (MSM). It's a new loss function that, during distillation, explicitly tells the student model to copy the motion distribution of the teacher model, not just the final look of the frames. It's calculated by looking at the difference in score outputs between adjacent frames. The paper gives a bit of theory for why this should work and claims it adds almost no extra compute cost.\n\nYou've tested this using metrics like CD-FVD and TRAJAN, and included some visual examples."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Good Experimental Proof: The numbers, especially in Table 1 (p. 7), look pretty good. It shows MSM beating standard DMD on motion-specific stuff (JACCARD, occlusion) and also on the general (CD-FVD) metric. This seems most helpful in the early stages of distillation.\n\n- Honesty About Limits: I appreciate that you were upfront about what MSM can't do, like how it's currently not usable for sequential/unequal-timestep distillation (Section 6.2).\n\n- Clear Setup: You did a good job laying out the training schedule, optimizer, dataset, etc. (Section 5.1). It seems like someone could reasonably reproduce your work."}, "weaknesses": {"value": "- The \"Why\" is a Bit Weak: \n\nYou introduce the motion score as just the difference between adjacent frames (Eq. 10), but the paper doesn't really give a rock-solid, formal proof for why this is the optimal way to do it, especially when you're dealing with noisy frames. The argument feels more intuitive than rigorous (Section 4.2). I was also looking for some ablations here—like, did you try different ways of defining \"motion\"? What if you weighted the loss differently?\n\n- Doesn't Work on SOTA Pipelines: \n\nThis is a big one. You state (in 5.2 and 6.2) that MSM can't be used when different noise timesteps are applied to different frames. Since that's how a lot of modern, state-of-the-art video models work, this severely limits the impact and real-world usefulness of your paper. Just pointing it out as a limitation isn't quite enough; it's a critical weakness.\n\n- Are the CD-FVD Gains Just from Longer Training? \n\nLooking at Table 1, the improvement on CD-FVD is... okay, it's moderate. But more importantly, you trained your MSM model for 14,000 iterations versus the baseline's 1,000. That's 14x longer. It's impossible to tell if the improvement is from your method or just from the massive difference in training time. We need an apples-to-apples comparison.\n\n- Evaluation is on Simple Stuff: \n\nThe figures you show (Figs 1, 2) are all pretty short, simple, single actions. There's no evidence here that MSM helps with more complex, long-term video challenges, like tracking multiple objects, maintaining consistent motion over dozens of frames, or handling complex trajectories."}, "questions": {"value": "- Other Motion Operators? Did you experiment with any other ways to formulate the motion score, beyond just a simple frame difference? Maybe something like higher-order time derivatives, or using optical flow (which you hint at in 6.1)? Does the choice of operator make a big difference?\n\n- Longer or Busier Videos? You focused on short, single-action clips. Have you looked at how MSM performs on longer sequences, or in scenes with multiple moving objects and occlusions? Does the motion realism gain still hold up, or does it fade?\n\n- Any Fixes for the Unequal-Timestep Problem? In 6.2 you mention MSM can't be used in sequential distillation. Can you suggest any research directions or ideas on how to get around this? This seems crucial for the method to be widely adopted.\n\n- Ablation on Training Time? Given the 14,000 vs. 1,000 iteration difference, could you run a controlled experiment to isolate the effect of MSM from the effect of just training longer?\n\n- Can your provide more video generation demonstration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d9fQ5mK7Po", "forum": "dF4gObeMCu", "replyto": "dF4gObeMCu", "signatures": ["ICLR.cc/2026/Conference/Submission7701/Reviewer_eue4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7701/Reviewer_eue4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793317946, "cdate": 1761793317946, "tmdate": 1762919760113, "mdate": 1762919760113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to introduce a motion score diffusion for video generation. The paper demonstrates that preserving the motion signal is vital for video generation quality."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The motivation of introducing motion clues for video generation is interesting. \n\nIdentifying two causes of buried motion signal is interesting."}, "weaknesses": {"value": "1. In the abstract, “Current distillation methods prefer to match the style first, since it takes up most of the numerical significance. Such a distillation scheme will only create poorly generated motions,”, please add high-level intuition or explanation. \n\n2. Line 34, what is PF-ODE? The term appears without explanations. ODE is a common terminology, but PF-ODE (presumably,  Probability Flow (PF) ODE) is not. \n\n3. Eq. 7 uses implementation-specific notation (‘torch.mean’). Please replace framework code with standard mathematical notation.\n\n4. Motion score” is essentially a finite-difference along the temporal index of the teacher’s score (Eq. 10), i.e., a simple frame-to-frame subtraction of the score output. Beyond the intuition that motion signals are small (Eqs. 7–8), there’s little new theory or a principled derivation of why this particular differencing is the right statistic to distill—or how it relates to the teacher’s underlying SDE/PF-ODE. Claims like “prove that preserving motion is vital” are stated but not accompanied by a formal theorem/proof.\n\n5. Also, prior works (e.g., [1,2]) have leveraged explicit optical flow for motion control/consistency with clear interpretability, whereas “motion score” is a more implicit proxy and should be positioned relative to these flow-based lines.\n\n6. The method requires identical timesteps across frames; authors explicitly say it “cannot be applied” to modern autoregressive/unequal-timestep pipelines (Stage 2 and many current T2V systems) (Sec. 5 & Sec. 6.2). That sharply narrows the scope to a subset of bidirectional, equal-noise settings.\n\n7. The paper adopts TRAJAN’s JACCARD & Occlusion Accuracy but then inverts the interpretation, arguing that lower is better because still videos score “artificially high.” This runs counter to metric semantics and risks confusion and cherry-picking. Human studies or motion realism judgments are absent; temporal consistency or optical-flow stability metrics are not reported as primary evidence. Only CD-FVD is added, with mixed gains.\n\n8. Adding MSM could harm spatial fidelity or style. There’s no ablation over the weight $\\lambda$ in Eq. 12, no FID/LPIPS-like appearance trade-off curves, and no user preference study quantifying motion realism vs. appearance degradation.\n\n9. Baselines are too narrow. Comparisons focus mainly on DMD with/without MSM. Other motion-aware distillation/regularization baselines (including those the paper itself discusses in related work) aren’t discussed under the same setup, so it’s hard to tell if MSM is better than existing motion-appearance disentangling strategies.\n\n\n\n[1]. Nam H, Kim J, Lee D, et al. Optical-flow guided prompt optimization for coherent video generation[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 7837-7846.\n\n[2]. Geng D, Herrmann C, Hur J, et al. Motion prompting: Controlling video generation with motion trajectories[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 1-12."}, "questions": {"value": "See my weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "buazt7aBDw", "forum": "dF4gObeMCu", "replyto": "dF4gObeMCu", "signatures": ["ICLR.cc/2026/Conference/Submission7701/Reviewer_476N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7701/Reviewer_476N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893792372, "cdate": 1761893792372, "tmdate": 1762919759211, "mdate": 1762919759211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}