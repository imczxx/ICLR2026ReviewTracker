{"id": "QteJJF57yG", "number": 1590, "cdate": 1756894982384, "mdate": 1763565631457, "content": {"title": "WeTok: Powerful Discrete Tokenization for High-Fidelity Visual Reconstruction", "abstract": "Visual tokenizer is a critical component for vision generation. However, the existing tokenizers often face unsatisfactory trade-off between compression ratios and reconstruction fidelity. To fill this gap, we introduce a powerful and concise WeTok tokenizer, which surpasses the previous leading tokenizers via two core innovations. (1) Group-wise lookup-free Quantization (GQ). We partition the latent features into groups, and perform lookup-free quantization for each group. As a result, GQ can efficiently overcome memory and computation limitations of prior tokenizers, while achieving a reconstruction breakthrough with more scalable codebooks. (2) Generative Decoding (GD). Different from prior tokenizers, we introduce a generative decoder with a prior of extra noise variable. In this case, GD can probabilistically model the distribution of visual data conditioned on discrete tokens, allowing WeTok to reconstruct visual details, especially at high compression ratios. On the ImageNet 50k validation set, at a high-fidelity setting, WeTok achieves a record-low zero-shot rFID of 0.12, outperforming leading continuous tokenizers like FLUX-VAE (0.18) and SD-VAE 3.5 (0.19) with 400% compression ratio. Furthermore, in a high-compression regime, WeTok achieves a zero-shot rFID of 3.49 at a 768× compression ratio, substantially surpassing Cosmos, which scores 4.57 at only 50% our compression ratio.", "tldr": "A family of powerful discrete visual tokenizers designed to resolve the long-standing conflict between compression efficiency and reconstruction fidelity.", "keywords": ["Image Reconstruction", "Image Generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c8fb27844f00d0816d11594a55d4bedc37efa77d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose Group-wise Lookup-Free Quantization (GQ) to reduce computational and memory costs while maintaining high-fidelity image reconstruction. This design enables flexible scaling of the codebook to an effectively unlimited size. Furthermore, WeTok introduces a generative decoder that integrates adversarial training, using Gaussian noise as input and quantized tokens as conditional guidance, to mitigate the reconstruction loss typically caused by high compression ratios. Experimental results demonstrate that WeTok achieves superior reconstruction and generation performance even under extremely high compression settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose Group-wise Lookup-Free Quantization (GQ) to overcome the computational bottleneck, thereby achieving higher image reconstruction fidelity.\n\n2. The experiments yield promising results, and the ablation studies are comprehensive."}, "weaknesses": {"value": "1. I am mainly concerned about the compression ratio. As shown in Table 4, when the compression ratio reaches 768×, the number of image tokens is only 8 × 8 = 64. However, the resulting rFID of 8.94 is considerably higher than that reported by TiTok [1], indicating a notable degradation in reconstruction quality under such high compression.\n\n2. The main contribution, Group-wise Quantization (GQ), appears to primarily alleviate computational overhead. However, the paper does not provide experiments demonstrating how this design directly facilitates higher compression ratios. For instance, conducting experiments with a downsampling ratio of 32×32 and a hidden channel size of 64 (8×8) could better illustrate how GQ contributes to improving compression efficiency.\n\n3. The Generative Decoder (GD) introduces Gaussian noise as the generator’s input, which may lead to training instability. To address this issue, the authors propose a two-stage training scheme; however, this approach appears overly complex and potentially difficult to reproduce. It would be helpful to report the performance when training all losses jointly from scratch, to evaluate whether the two-stage procedure is truly necessary.\n\n[1] An image is worth 32 tokens for reconstruction and generation. NIPS 2024"}, "questions": {"value": "1. There is a minor concern regarding GQ. The approximation of the entropy loss is theoretically lower-bounded by BSQ, which performs worse than LFQ. However, Figure 4 in the ablation study shows a consistent improvement as the number of groups increases. It remains unclear where the performance begins to degrade — what is the optimal balance point for the number of groups?\n\n2. There are several existing 1D tokenizer models, such as TiTok [1] and SweetTok [2]. The authors are encouraged to include comparisons with these methods to strengthen the experimental validation and make the results more comprehensive.\n\n[1] An image is worth 32 tokens for reconstruction and generation. NIPS 2024\n\n[2] SweetTok: Semantic-Aware Spatial-Temporal Tokenizer for Compact Video Discretization. ICCV 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jI0jR6rHq2", "forum": "QteJJF57yG", "replyto": "QteJJF57yG", "signatures": ["ICLR.cc/2026/Conference/Submission1590/Reviewer_2k14"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1590/Reviewer_2k14"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760865551975, "cdate": 1760865551975, "tmdate": 1762915827812, "mdate": 1762915827812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Reply to All Reviewers"}, "comment": {"value": "Dear Reviewers,\n\nWe sincerely thank all reviewers for their insightful feedback and constructive suggestions. We are encouraged that the reviewers found WeTok to exhibit **\"relatively strong\"** (Reviewer Dztj), **\"strong performance\"** (Reviewer YMro), and **\"SOTA performance\"** (Reviewer bHBm) in reconstruction. We are also glad that the **\"mathematically grounded\"** formulation of GQ (Reviewer YMro) and the **\"well written\"** nature of the paper (Reviewer bHBm) were recognized.\n\nIn this rebuttal, we have addressed all concerns by:\n1.  **Clarifying Novelty:** Highlighting the theoretical guarantees of GQ and the unique single-step nature of our Generative Decoder.\n2.  **New Comparisons:** Adding comparisons with **Infinity**, **TiTok**, **SweetTok**, and **Epsilon-VAE** to benchmark against the latest SOTA.\n3.  **New Ablations:** Providing experiments to justify the two-stage training strategy and high-compression generation quality.\n4.  **Revised Manuscript:** We have updated the paper to include these discussions and citations.\n\nWe promise that we will open source WeTok, which we trained in the paper and achieved state-of-the-art reconstruction performance, to promote the technological progress of the community. We hope our responses and additional experiments satisfactorily address your questions.\n\nSincerely,\n\nThe Authors"}}, "id": "alzauIExl0", "forum": "QteJJF57yG", "replyto": "QteJJF57yG", "signatures": ["ICLR.cc/2026/Conference/Submission1590/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1590/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1590/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763549184232, "cdate": 1763549184232, "tmdate": 1763549184232, "mdate": 1763549184232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents WeTok, which is a discrete tokenizer for improving the tradeoff between compression ratio and reconstruction fidelity. Discrete tokenizer has the benefit of having a higher compression ratio, but long has the problem of low reconstruction fidelity.  \n\nWeTok proposes two key techniques, Group-wise lookup-free Quantization (GQ) and Generative Decoder (GD). \n\nGQ tries to address the memory and computational cost of Lookup-free-quantization (LFP). It partitions the codebook into groups and performs quantization on each group independently to eliminate token entropy loss as the memory bottleneck.\n\nGD is a generative decoder, instead of the traditional one-step deterministic decoder used in prior methods. \n\nQualitative results in Table 3 and Table 4 are relatively strong comparing to the prior state-of-the-art methods"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(+) The results presented in the tables are relatively strong, achieving a much larger codebook size and good rFID and PSNR\n\n(+) The proposed two methods (GQ and GD) make sense and are well motivated. GQ seems to be a practical and effective solution for solving the bottleneck of the CE loss\n\n(+) The evaluation compared against many methods in Tables 3 and 4"}, "weaknesses": {"value": "(-) the GD method is not very novel. It is known to the community such diffusion decoder can work, dating back to OpenAI's \"Consistency Decoder\". In addition, such generative decoder does not come with no cost. First, the decoding time increases, which can limit some of the real-time or latency-sensitive applications. Second, as it is a generative model, the decoder could also hallucinate \n\n(-) lack of comparison with more state-of-the-art autoencoders. For example, infinity tokenizer (https://cvpr.thecvf.com/virtual/2025/poster/34414). Both work claim to increase codebook size and claim to be SotA in the field, though taking pretty different approaches. Therefore a comparison is worthy for the community. However the paper primarily focuses on older VAEs such as VQGAN and SD-VAE\n\n(-) For generation tasks, such as the ones presented in Supp Mat Table 6, shows the proposed method is only marginally improving against SotA Open-MAGVIT2-AR-XL (Luo et al., 2024) 2.33 vs WeTok-AR-XL (Ours) 2.31 in FID\n\n(-) The model is trained in two stages: \"In first stage, we train our WeTok with the reconstruction loss, i.e., Eq. 2, 6 and 8. In the second stage, we adapt the model for generative tasks\". This is atypically and lacks of explanation on why this is needed. Traditional VAE are trained with a single stage and its generation task quality is on-par with the proposed method (see the point above)\n\n(-) seems datasets play a role in terms of the model quality (which is expected), as evidenced in Figure 6. However, in results section, all models are trained with different datasets, which complicates the analysis of whether the proposed method is effective or the 400M GD data is more suitable for Coco and ImageNet"}, "questions": {"value": "- \"We surprisingly find that while reconstructions from leading models like FLUX-VAE and SD-VAE 3.5 collapse after iterations, WeTok’s\noutputs are remarkably robust and converge to a fixed value.\" Is there any explanation on why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yG6V8COjsI", "forum": "QteJJF57yG", "replyto": "QteJJF57yG", "signatures": ["ICLR.cc/2026/Conference/Submission1590/Reviewer_Dztj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1590/Reviewer_Dztj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761263050160, "cdate": 1761263050160, "tmdate": 1762915827576, "mdate": 1762915827576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents WeTok, a novel tokenizer that effectively balances high compression efficiency with high-fidelity image reconstruction, achieving state-of-the-art reconstruction performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written.\n\nThe reconstruction results achieve SOTA performance among existing discrete tokenizer, demonstrating the effectiveness of the proposed framework."}, "weaknesses": {"value": "## Fairness of Comparison\n\nThe comparison in Table 3 appears unfair.  The strong baseline MGVQ is a VQ-based tokenizer, whereas WeTok adopts LSQ, which has already been shown to be more efficient than VQ.  To ensure a fair evaluation, the authors should compare WeTok with an LSQ-based version of MGVQ. \nFurthermore, the MGVQ codebook size is only 8192 × 4, but its effective capacity is actually $2^{52}$, not limited by the nominal codebook size.\n\n## Lack of Novelty\nThe proposed method shows limited novelty.  Overall, the approach seems like a combination of existing components rather than a fundamentally new idea.  Specifically:\n- LSQ has been widely explored in prior works.  \n- Group-wise quantization is not new.  \n- The Generative Decoder design has already been introduced in previous studies.\n\nThe authors should better clarify what unique contribution or new insight WeTok introduces beyond these known elements.\n\n## Missing Discussion on Semantic Tokenizers\n\nThe paper lacks discussion and comparison with **semantic tokenizers**, which have proven to be powerful for visual understanding and generation.  Several recent works are relevant and should be considered:\n\n[1] ImageFolder: Autoregressive Image Generation with Folded Tokens. https://arxiv.org/pdf/2410.01756\n\n[2] Factorized Visual Tokenization and Generation. https://arxiv.org/pdf/2411.16681\n\n[3] DualToken: Towards Unifying Visual Understanding and Generation with Dual Visual Vocabularies. https://arxiv.org/pdf/2503.14324\n\n[4] TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation. https://arxiv.org/pdf/2412.03069\n\n## Missing High-Compression Evaluation\n\nThe authors claim that **WeTok** achieves a **768× compression ratio**, which is indeed a significant advantage. However, there is no **quantitative or qualitative evidence** showing the generation quality under this extreme compression rate. It is recommended that the authors provide **generation results** and **gFID metrics** at the claimed **768× compression level** to substantiate this important claim."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U7GW0wBFGL", "forum": "QteJJF57yG", "replyto": "QteJJF57yG", "signatures": ["ICLR.cc/2026/Conference/Submission1590/Reviewer_bHBm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1590/Reviewer_bHBm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765025854, "cdate": 1761765025854, "tmdate": 1762915827314, "mdate": 1762915827314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WeTok, a new family of discrete visual tokenizers that aims to resolve the long-standing trade-off between compression ratio and reconstruction fidelity in latent-space visual generation. WeTok combines two main innovations: Group-wise Lookup-Free Quantization (GQ) and Generative Decoder (GD). Extensive experiments on ImageNet-50k and MS-COCO show that WeTok achieves state-of-the-art (SOTA) reconstruction metrics and strong performance in zero-shot and class-conditional image generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed GQ formulation provides a mathematically grounded way to reduce the entropy-loss memory bottleneck in LFQ and BSQ, with a provably smaller approximation error.\n2. The paper includes large-scale ablations (quantization types, group numbers, architectures, learning schedules) and comparisons across both high-fidelity and high-compression regimes.\n3. The proposed method achieves strong performance on both image reconstruction and AR-based generation results, even surpassing continuous tokenizers at similar compression ratios."}, "weaknesses": {"value": "1. Diffusion-based decoder for visual reconstruction has been studied in previous literatures[1][2], it would be better to cite these work and further discuss the differences with them.\n2. In the ablation study section, it's interesting to see that after converting the decoder to a generative model, the reconstructed images are more realistic. It would be better to include some further discussion or analysis.\n\n[1] Epsilon-VAE: Denoising as Visual Decoding\n[2] Diffusion Autoencoders are Scalable Image Tokenizers"}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0CFGnkvRa2", "forum": "QteJJF57yG", "replyto": "QteJJF57yG", "signatures": ["ICLR.cc/2026/Conference/Submission1590/Reviewer_YMro"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1590/Reviewer_YMro"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812789758, "cdate": 1761812789758, "tmdate": 1762915827122, "mdate": 1762915827122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}