{"id": "F20XWCnZdk", "number": 5866, "cdate": 1757942341934, "mdate": 1763394953299, "content": {"title": "NorSA: Accelerate LLM Decoding via Normalized Sparse Activation", "abstract": "Sparse activation accelerates  the decoding of large language models by eliminating redundant computations and reducing memory access during matrix multiplications. Current approaches have potential limitations as they rely on the strong assumption that \"values across different dimensions of hidden states are drawn from independent and identically distributed random variables.\" Our research challenges this assumption by analyzing how causal dependencies exist between tokens and correlations exist between different dimensions of hidden states. Building on this insight, we introduce Normalized Sparse Activation (NorSA), a method that accounts for inter-dimensional relationships and integrates contextual information through rotation and norm-based thresholding. NorSA achieves superior performance while maintaining computational efficiency. Experiments across LLaMA, Mistral, and Qwen model series show that NorSA consistently outperforms existing methods. For LLaMA3-8B with 50% activation sparsity, NorSA narrows the perplexity gap to only 0.44 points relative to the dense model, while restricting the zero-shot accuracy decline to a mere 1.23%, surpassing La RoSA by 1.63% and TEAL by 3.9%.", "tldr": "", "keywords": ["Sparse Activation", "Large Language Model", "Inference Speedup"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44bdbbd796b212c430ff8fd469d891c32ad7dabb.pdf", "supplementary_material": "/attachment/caaed69cf971869985fc36d2294fb06cc02f9d78.zip"}, "replies": [{"content": {"summary": {"value": "The study introduces Normalized Sparse Activation (NorSA), a method to speed up large language model decoding by reducing some of the unnecessary computations. The key insight of the method, different to others, is that it correctly does not assume that the generated tokens are IID, but they are casual (a token is conditioned in all the previous generated tokens). Therefore, the method does a simple adjustement when choosing the tokens to threshold. Furthermore, it also applies normalization, rotation, and thresholding to maintain meaningful activations. The method is tested in several benchmarks and using 3 family of LLMs (LLaMA, Mistral and Qwen) reaching results that outperform the other two methods it compares with."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I think these are the main strengths of the paper:\n\n1) The main part of the method (normalized sparse attention) is well-motivated, makes very much sense, and is very simple to understand. This is actually quite nice, considering that most of the papers try to over-complicate things in order to make them sound novel.\n\n2) The results are quite good. The method decisively outperforms TEAL and La RoSA in several benchmarks.\n\n3) There are some interesting ablation studies shown in the method.\n\n4) Very nice to see that the authors also do a hardware aware kernel for the method."}, "weaknesses": {"value": "I think these parts of the paper can be further improved:\n\n1) It is unclear to me the connection between sections 4.2 and 4.3. In particular, while I really like the motivation of 4.2, to some degree, 4.3 looks to me a bit forced, almost like trying to increase the complexity of the method. Furthermore, I think there must be an ablation that shows the performance of both 4.2 and 4.3 in isolation, without them being combined.\n\n2) Presentation\n\n2a) Related work can be further improved. Right now, it just mentions some papers, but without clarifying how do they work, why they are important and how do they connect with this work.\n\n2b) Figures 1 and 2 can be massively improved. There is a lot of trivial information there (the attention mechanism) that can be collapsed.\n\n2c) Table 2 comes before Table 1 in the paper, this should be rearranged."}, "questions": {"value": "I would appreciate if the authors can clarify these potential issues:\n\n1) Why the timing performance has been done in A100 GPUs? Is it just because that is what the authors have or some other reasons? It would be ideal if we could see some results in more modern H100 (or B-series) GPUs.\n\n2) What does 100% sparsity even mean? \n\n3) Are all the results shown while also having standard speedup mechanisms such as FlashAttention and KV caching?\n\nI am quite willing to increase my score based on the answers for Weaknesses and Questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hEbEpoknfw", "forum": "F20XWCnZdk", "replyto": "F20XWCnZdk", "signatures": ["ICLR.cc/2026/Conference/Submission5866/Reviewer_pMJ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5866/Reviewer_pMJ8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760710302765, "cdate": 1760710302765, "tmdate": 1762918311851, "mdate": 1762918311851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Main Rebuttal"}, "comment": {"value": "Hello reviewers, we would like to express our heartfelt gratitude to all of you for your time and effort in reviewing our paper! \n\nThe acknowledgment of the empirical results achieved by our method and the recognition of the challenge to the independent and identically distributed (i.i.d.) assumptions in past work are greatly appreciated.\n\nAfter carefully reading through each and every comment and suggestion, we address some common concerns below:"}}, "id": "cVJwwBXaAB", "forum": "F20XWCnZdk", "replyto": "F20XWCnZdk", "signatures": ["ICLR.cc/2026/Conference/Submission5866/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5866/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5866/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763393279553, "cdate": 1763393279553, "tmdate": 1763393279553, "mdate": 1763393279553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present that existing activation sparsification techniques (e.g., TEAL, LaRoSA) rely on the flawed assumption of i.i.d. activations. To address this, NorSA introduces norm-based thresholding (to incorporate contextual scale information) and rotation matrices (to decorrelate activation dimensions). Experiments across LLaMA, Mistral, and Qwen families show the efficacy of the approach."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation of iid assumption not holding is reasonable.\n- Having numerical results across different model families and kernel implementation is good."}, "weaknesses": {"value": "- Lack of novelty: The rotation idea is adopted from SliceGPT paper. \n\n- Clarity needs to be improved. The context-aware selection seems equivalent to the layer-wise sparsity allocation. In other words, though other works select activated neurons upon some threshold, the thresholds could be assigned in the global cross-layer information, making them context-aware as well. \n\n- The writing needs to be improved. There are many inconsistencies of writing format throughout the paper."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CxELNMdlt9", "forum": "F20XWCnZdk", "replyto": "F20XWCnZdk", "signatures": ["ICLR.cc/2026/Conference/Submission5866/Reviewer_mCoY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5866/Reviewer_mCoY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891143479, "cdate": 1761891143479, "tmdate": 1762933842816, "mdate": 1762933842816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Normalized Sparse Activation (NorSA) and enhances prior approaches by considering the correlations between the dimensions of hidden states."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work challenges the assumption that different activation dimensions are independent and suggests a way to incorporate that.\n2. Combining the norm calculation with other operations is an effective way to reduce overhead.\n3. The author invested considerable effort into the implementation and achieved impressive empirical results across different scales."}, "weaknesses": {"value": "Some of the technical and experimental designs might need additional motivation; please see \"Questions\"."}, "questions": {"value": "1. I have some reservations about Equation (3). The authors compare activation entries to their norm, but these seem like different quantities, making the comparison questionable. Why not use $\\tau$ standard deviations from the mean instead?\n\n2. Regarding equations (5) and (6), is the PCA-based rotation matrix an approximate solution? Also, I'm unsure why a rotation matrix would be useful here, as I usually think of it for smoothing outliers.\n\n3. In Section 5.2, how do the authors \"learn\" the rotational matrix?\n\n4. Is there a typo after Equation (4)? It references `sparsify` twice.\n\n5. I haven't seen any speed comparisons with other methods. Is the main focus that the improvements are mainly in quality with minimal latency impact? If so, that seems reasonable to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0jlSe73AmH", "forum": "F20XWCnZdk", "replyto": "F20XWCnZdk", "signatures": ["ICLR.cc/2026/Conference/Submission5866/Reviewer_L5Vk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5866/Reviewer_L5Vk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968059127, "cdate": 1761968059127, "tmdate": 1762918311119, "mdate": 1762918311119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "NorSA is a training-free activation sparsity method for LLMs that relaxes the i.i.d. activation assumption used by TEAL / La RoSA. It uses a norm-normalized threshold (|xᵢ| > τ‖x‖) so sparsity decisions depend on the overall scale of each token’s hidden state, and introduces PCA-based rotation matrices to reduce linear correlations between dimensions before sparsification. Experiments on LLaMA-2/3, Mistral, and Qwen-2.5 show better perplexity and zero-shot / MMLU performance than TEAL and La RoSA at the same sparsity, plus real decoding speedups with Triton kernels."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The normalized per-hidden-state thresholding rule is simple, intuitive, and easy to integrate into existing models.\n\n- Strong empirical results across multiple model families and sparsity levels, consistently surpassing TEAL and La RoSA, especially at higher sparsity.\n\n- Hardware-aware implementation (fused SwiGLU+norm, sparse GEMV) and ablations on rotations / covariance make the method feel practically usable."}, "weaknesses": {"value": "- Conceptual novelty is moderate: norm-normalized thresholds + PCA rotations sit close to prior rotation-based sparsity work; the paper could be clearer about what is genuinely new.\n\n- There is no clean, large-scale ablation of “NorSA without rotations” vs “NorSA with rotations”.\n\n- The choice of PCA as the rotation mechanism is under-motivated: no comparison to learned rotations (e.g., via distillation), and limited discussion of calibration cost and scaling."}, "questions": {"value": "- PCA vs learned rotations: Why did you choose PCA-based rotations over learning rotations with a small teacher–student distillation objective or gradient-based optimization? \n- TEAL and the i.i.d. assumption: You argue that prior work (e.g., TEAL) implicitly assumes i.i.d. activations across dimensions. Could you make this more explicit? What part of TEAL’s design depends on that assumption, and can you show empirical deviations from i.i.d. in real hidden states?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iEouB8EjrV", "forum": "F20XWCnZdk", "replyto": "F20XWCnZdk", "signatures": ["ICLR.cc/2026/Conference/Submission5866/Reviewer_DQe5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5866/Reviewer_DQe5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5866/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763376733757, "cdate": 1763376733757, "tmdate": 1763376733757, "mdate": 1763376733757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}