{"id": "ys2psnXTJL", "number": 3283, "cdate": 1757393690472, "mdate": 1763726752253, "content": {"title": "FlowBypass: General Training-Free Rectified Flow Image Editing via Trajectory Bypass", "abstract": "Training-free image editing has attracted increasing attention for its efficiency and independence from training data.  However, existing approaches predominantly rely on inversion–reconstruction trajectories, which impose an inherent trade-off: longer trajectories accumulate errors and compromise fidelity, while shorter ones fail to ensure sufficient alignment with the edit prompt.  Previous attempts to address this issue typically employ backbone-specific feature manipulations, limiting general applicability.  To address these challenges, we propose FlowBypass, a novel and analytical framework grounded in Rectified Flow that constructs a bypass directly connecting inversion and reconstruction trajectories, thereby mitigating error accumulation without relying on feature manipulations.  We provide a formal derivation of two trajectories, from which we obtain an approximate bypass formulation and its numerical solution, enabling seamless trajectory transitions.  Extensive experiments demonstrate that FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.", "tldr": "", "keywords": ["Training-free", "Image Editing", "Rectified Flow"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83c672792402b8b1d5fff8dd107e230d4798baf2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FlowBypass, a novel, general, and training-free image editing framework built upon the Rectified Flow (RF) model. The core motivation is to resolve the inherent trade-off in existing inversion-reconstruction editing pipelines: long trajectories preserve prompt alignment but accumulate discretization errors, compromising fidelity; short trajectories maintain fidelity but lack editing alignment.\n\nFlowBypass addresses this by analytically deriving a \"trajectory bypass\" ($b_t$) as the solution to a first-order linear Ordinary Differential Equation (ODE). This bypass directly connects an intermediate state of the inversion trajectory ($x_{t_B}$) to the reconstruction trajectory ($y_{t_B} = x_{t_B} + b_{t_B}$), circumventing the error-prone terminal noise state. The framework achieves editing without relying on backbone-specific feature manipulation, which enhances its generality. The authors employ Euler discretization and practical approximations (e.g., finite-difference approximation for the partial derivative of the velocity field) to implement the analytical solution.\n\nExtensive experiments on the EditEvalv2 benchmark, utilizing various RF backbones (SD3.5M, SD3.5L, FLUX.1-dev), demonstrate that FlowBypass consistently achieves a superior balance between prompt alignment and image fidelity compared to state-of-the-art methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strong and Novel Theoretical Foundation: The central contribution, the \"trajectory bypass\" ($b_t$), is rigorously derived as the analytical solution to a linear ODE (Equation 10). This principled approach provides a solid, non-heuristic framework for error mitigation in inversion-based editing, which is a significant theoretical advance over prior empirical or feature-manipulation-based methods.\n\nExcellent Generalizability: By abstaining from backbone-specific Feature Manipulation (FM), the method successfully demonstrates robust performance across diverse Rectified Flow architectures (SD3.5M/L and FLUX.1-dev). This general applicability makes FlowBypass a potentially influential technique in the broader field of diffusion model editing.\n\nCompetitive State-of-the-Art Performance: The quantitative results clearly position FlowBypass favorably on the fidelity-alignment Pareto front (Figure 5). It achieves superior perceptual fidelity (lowest LPIPS on FLUX.1-dev) and strong text alignment (best T.Sim. on SD3.5L), confirming its effectiveness in achieving the intended balance.\n\nThorough Ablation Studies: The authors provide comprehensive ablation studies on critical design choices, including the impact of the bypass step $t_B$ (Figure 10, Table 6), the choice of prompt conditionings (Figure 6, Table 4), and the necessity of the analytical approximations (Table 3, Figure 8). These experiments solidify the robustness and rationale behind the proposed framework's final configuration."}, "weaknesses": {"value": "High Computational Overhead due to Gradient Approximation: The core mechanism relies on approximating the partial derivative of the velocity field, $\\frac{\\partial}{\\partial x_{t}}\\hat{v}_{\\theta}$, using finite differences (Equation 12). This typically requires two forward passes of the entire generative model per step to calculate the derivative, which can be computationally prohibitive, especially for large models like SD3.5L and FLUX.1-dev. The paper lacks a detailed analysis or comparison of the runtime and computational cost overhead introduced by this specific approximation compared to standard inference methods.\n\nImpact of Simplistic Approximations: While the approximations (especially the first-order Taylor expansion for the exponential term in Equation 13) are crucial for numerical stability, relying on such simplistic methods can potentially limit the accuracy of the bypass calculation, particularly in highly non-linear regions of the velocity field. The trade-off between stability/efficiency and precision should be discussed more critically.\n\nSensitivity to Hyperparameters ($t_B$ and CFG Scale): The ablation studies (Figure 9, Figure 10) show that performance is highly sensitive to the choice of the bypass timestep ($t_B$) and the CFG scale ($\\omega$). While $t_B=30$ and $\\omega=2.0$ are selected as optimal, the need for extensive search over these parameters for optimal performance suggests that FlowBypass may require careful tuning for new models or editing tasks. A more adaptive or automated way to select these parameters would significantly improve the practical utility of the method."}, "questions": {"value": "Above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sY0E6TxTLu", "forum": "ys2psnXTJL", "replyto": "ys2psnXTJL", "signatures": ["ICLR.cc/2026/Conference/Submission3283/Reviewer_bETU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3283/Reviewer_bETU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555513870, "cdate": 1761555513870, "tmdate": 1762916646306, "mdate": 1762916646306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "## General Response\nWe sincerely thank all reviewers for their thoughtful, detailed, and constructive feedback!\n\nWe are pleased that all reviewers (7EG6, K56Z, z99e, bETU) recognize the significance and generalizability of our training-free trajectory bypass framework for inversion-based image editing:\n\n* *“The training-free image editing framework … seems shows strong generalizability across Rectified Flow models.”* — **7EG6**\n* *“The method effectively addresses the fidelity-alignment trade-off without relying on model-specific feature manipulations, ensuring broader applicability.”* — **K56Z**\n* *“The proposed method is not tied to specific backbone architectures, which can be applied on various base models such as FLUX and SD3.”* — **z99e**\n* *“By abstaining from backbone-specific Feature Manipulation (FM), the method successfully demonstrates robust performance across diverse Rectified Flow architectures (SD3.5M/L and FLUX.1-dev). This general applicability makes FlowBypass a potentially influential technique in the broader field of diffusion model editing.”* — **bETU**\n\nWe are also glad that the reviewers (7EG6, K56Z, bETU) point out our theoretical formulation rigorous and principled:\n\n* *“The core idea appears theoretically sound, ...”* — **7EG6**\n* *“The core idea of constructing a bypass between inversion and reconstruction trajectories is well-motivated. The theoretical derivation of the bypass term from first principles is rigorous and elegant.”* — **K56Z**\n* *“Strong and Novel Theoretical Foundation: ... This principled approach provides a solid, non-heuristic framework for error mitigation in inversion-based editing, which is a significant theoretical advance over prior empirical or feature-manipulation-based methods.”* — **bETU**\n\nWe are further pleased that all reviewers (7EG6, K56Z, z99e, bETU) highlight the strong performance and thorough analysis in our work:\n\n* *“The ..., and the experimental results demonstrate promising performance.”* — **7EG6**\n* *“The method effectively addresses the fidelity-alignment trade-off without relying on model-specific feature manipulations, ensuring broader applicability.”* — **K56Z**\n* *“Quantitative and qualitative results in paper demonstrate that the outstanding performance of the proposed method.”* — **z99e**\n* *“Competitive State-of-the-Art Performance: ...  It achieves superior perceptual fidelity (lowest LPIPS on FLUX.1-dev) and strong text alignment (best T.Sim. on SD3.5L), confirming its effectiveness in achieving the intended balance.”* — **bETU**\n* *“Thorough Ablation Studies: The authors provide comprehensive ablation studies on critical design choices, ... These experiments solidify the robustness and rationale behind the proposed framework's final configuration.”* — **bETU**\n\n\n## Updates and New Experiments\nWe summarize the additional analyses and experiments inspired by the reviewers’ comments. These updates will be incorporated into the revised PDF after further discussion with reviewers.\n\n* **More compared baselines:**\nWe introduce four additional baselines for a broader and more comprehensive comparison, including **FlowEdit**, **FluxSpace**, **Prompt-to-Prompt**, and **FireFlow**. The quantitative results have been updated in **Table 1** of the revision, and the qualitative comparisons have been updated in **Figure 2** and **Figure 11**. Besides, we conduct a user study to evaluate how well FlowBypass aligns with human preference, as presented in **Sec. A.3.2**.\n\n* **Runtime comparison with SOTA methods and runtime breakdown:**\nWe provide a detailed runtime comparison in **Table 7** of the revision, along with a breakdown and analysis of the time cost at different stages in **Table 8**.\n\n* **Additional ablation studies and visualizations:**\nWe report further ablation studies, including the impact of the effect of different prompt choices (**Sec. 4.4.3**), the hyperparameter $\\zeta$ (**Sec. A.3.6**), and edit performance when editing starts from various intermediate timesteps without the bypass (**Sec. A.3.8**). In addition, we provide more visualizations, including extra editing results beyond the EditEvalv2 dataset in **Figure 13** and visualizations of the bypass under different $t_B$ in **Figure 16**.\n\n* **More analysis and expanded discussion of limitations:**\nWe refine the analysis and interpretation of our results, and we add a dedicated **Sec. 5** to discuss the limitations of FlowBypass.\n\nWe sincerely thank all reviewers again for their insightful suggestions and look forward to further discussion any time."}}, "id": "XnpWMWVrsK", "forum": "ys2psnXTJL", "replyto": "ys2psnXTJL", "signatures": ["ICLR.cc/2026/Conference/Submission3283/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3283/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3283/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763725554424, "cdate": 1763725554424, "tmdate": 1763725554424, "mdate": 1763725554424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FlowBypass, a training-free image editing framework leveraging Rectified Flow models. By analytically constructing a “bypass” trajectory between inversion and reconstruction, FlowBypass aims to retain high-fidelity details in unedited regions while improving prompt alignment. Extensive quantitative and qualitative experiments demonstrate the superior performance compared to existing training-free methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation of this paper is clear and meaningful: Most inversion-based image editing methods suffer from extensive hyper-parameter tuning to strike a balance between image fidelity and prompt alignment, which is a long-lasting problem for training-free image editing methods.\n- The proposed method is not tied to specific backbone architectures, which can be applied on various base models such as FLUX and SD3.\n- Quantitative and qualitative results in paper demonstrate that the outstanding performance of the proposed method.\n- The paper is clear and well-written."}, "weaknesses": {"value": "- Missing several baselines: FlowEdit [1], QK-Edit [2], FluxSpace [3], FireFlow [4].\n- Intuitively, given a source image, one could invert it to a specific time step $t$ and then start denoising from that step to obtain the edited image. This may also achieve high-fidelity image editing. However, in Table 2, authors only provide the results of $t=30$ and $t=50$, which is not enough to illustrate the advantages of the proposed method. Authors should provide more comprehensive comparisons between their method and this approach in ablation study (such as $t=40$).\n- As illustrated in Line 256, FlowBypass constructs the reconstruction trajectory starting from an intermediate state rather than from the inverted noise. Can this make FlowBypass more efficient than previous methods? Authors are expected to provide a comparison of runtimes.\n\n\n[1]. FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models\n\n[2]. QK-Edit: Revisiting Attention-based Injection in MM-DiT for Image and Video Editing\n\n[3]. FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers\n\n[4]. FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hPlwFvIzzM", "forum": "ys2psnXTJL", "replyto": "ys2psnXTJL", "signatures": ["ICLR.cc/2026/Conference/Submission3283/Reviewer_z99e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3283/Reviewer_z99e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567224437, "cdate": 1761567224437, "tmdate": 1762916645929, "mdate": 1762916645929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FlowBypass, a training-free image editing framework based on Rectified Flow. The method introduces a \"trajectory bypass\" to mitigate error accumulation in inversion-reconstruction pipelines, striking a balance between semantic alignment and fidelity preservation in irrelevant regions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core idea of constructing a bypass between inversion and reconstruction trajectories is well-motivated. The theoretical derivation of the bypass term from first principles is rigorous and elegant. The method effectively addresses the fidelity-alignment trade-off without relying on model-specific feature manipulations, ensuring broader applicability."}, "weaknesses": {"value": "The paper emphasizes superior performance in \"challenging editing scenarios\" but relies solely on the EditEvalv2 benchmark (150 images). While the appendix includes external examples, it lacks systematic evaluation across diverse domains. Experiments neglect complex edits like multi-object coordination or motion changes. Testing on dynamic edits (e.g., \"walking\" to \"dancing\") would better validate temporal consistency and generalization. This narrow validation weakens claims of robustness, particularly for complex edits like structural transformations. \n\nNo dedicated section discusses failure cases or boundaries of the method. For instance, Figure 8 reveals artifacts from approximation failures, but the paper does not analyze root causes. Explicitly outlining scenarios where FlowBypass fails would provide a more balanced perspective.\n\nThe reliance on automated metrics overlooks human perceptual factors critical for image editing. A user study assessing edit naturalness and consistency would complement quantitative results and align with real-world application needs.\n\nThe paper omits runtime comparisons with state-of-the-art methods.\n\nThe paper highlights independence from feature manipulations but does not rigorously compare with methods like HeadRouter or Prompt-to-Prompt. Such comparisons are essential to demonstrate advantages in localized editing precision (e.g., texture replacement) and scenario applicability."}, "questions": {"value": "See weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The ethics statement mentions adding meta-information to edited images. What specific techniques (e.g., watermarking) are used? How robust are they against removal?"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WLa984ron3", "forum": "ys2psnXTJL", "replyto": "ys2psnXTJL", "signatures": ["ICLR.cc/2026/Conference/Submission3283/Reviewer_K56Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3283/Reviewer_K56Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809200812, "cdate": 1761809200812, "tmdate": 1762916645573, "mdate": 1762916645573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present the idea that the standard inversion process may not identify the optimal reversed noise for subsequent editing tasks. Instead, they propose initiating the reconstruction process from an alternative starting point, determined by computing an offset that bridges the inversion and image generation processes. This offset is formulated as a linear differential equation with an analytical solution. Through appropriate approximation, they demonstrate that this new starting point contributes to preserving high-fidelity details during prompt-guided image editing. Results on benchmark datasets appear to support the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The training-free image editing framework, that leverages a trajectory bypass mechanism to achieve superior fidelity-alignment trade-offs, seems show strong generalizability across Rectified Flow models.\n- The core idea appears theoretically sound, and the experimental results demonstrate promising performance."}, "weaknesses": {"value": "1. The derivation from Equation (5) to Equation (6) appears to suggest that $x_1 = y_1$. However, since $y_t$ corresponds to the reconstruction process and $x_t$ corresponds to the inversion process, it seems more natural that $x_1$ refers to the original image while $y_1$ represents the reversed noise. If the authors intend for both $x_1$ and $y_1$ to represent the original image, could they clarify why the integration bounds and velocity direction (should it be $-\\hat{v}_\\theta$?) are set as shown? Please correct me if I have misunderstood this derivation.\n\n2. The formulation of FlowBypass from Equation (5) to Equation (7) appears very similar to FlowEdit [1] when differentiating Equation (5). Specifically, the key replacement in Equation (7), where $y_t = x_t + b_t$, corresponds to Equation (7) in [1]. While the authors clearly draw inspiration from related work, FlowEdit is not cited in the manuscript. Additionally, while the introduction of $dZ_t^{inv}$ (equivalent to $\\frac{d}{dt}b_t$) in [1] is quite intuitive, the integration-based derivation in this paper makes the subtraction of $y_t$ and $x_t$ less clear. Could the authors clarify the relationship to FlowEdit and explain the conceptual advantage of their formulation?\n\n3. The derivation of the bypass term (Equation 8) assumes that the offset $b_t$ is small enough to permit first-order Taylor expansion. This assumption may not hold for significant edits (e.g., drastic content changes), potentially affecting the accuracy of the bypass calculation. The paper would benefit from discussing when this assumption is valid and how the method performs when it breaks down.\n\n4. Given the similarity to FlowEdit [1], it should be included as a baseline for comparison. Additionally, considering the extra computational cost of calculating $b_t$, comparing with efficient methods such as [2] would provide a more complete picture of the speed-performance trade-off.\n\n5. The paper would benefit from a more thorough discussion of the method's limitations, particularly its performance in challenging scenarios such as extreme content modifications or style transfer tasks.\n\n6. The authors claim that prompt choice has a crucial impact on the final results. This raises the question of whether baseline methods could achieve better performance if the same prompt optimization strategy were applied to them, since prompt selection is independent of the method itself. Without this ablation study on baseline methods, the comparison may not be entirely fair.\n\n## Reference:\n- [1] Flowedit: Inversion-free text-based editing using pre-trained flow models. ICCV2025\n- [2] Fireflow: Fast inversion of rectified flow for image semantic editing. ICML2025"}, "questions": {"value": "- Ablation Study on Prompt Configuration (Lines 228-230): The use of $C^p_{rec}=C_y$ and $C^n_{rec}=C_x$ in the reconstruction setting appears to be an important design choice. Could the authors conduct an ablation study to evaluate this configuration? Specifically, what happens if the negative prompt is left as null text instead?\n\n- The paper lacks ablation studies on two key design choices in Equation (13): (1) the selection of the hyperparameter $\\xi$, and (2) the decision to replace the exponential with its first-order Taylor approximation in the positive domain. It would be valuable to understand how sensitive the method is to these choices and what motivated this particular approximation.\n\n-  According to Section 3.4, the key step relies on the construction of $b*_{t_i}$. It would be helpful to include visualizations showing the intermediate results of $b*_{t_i}$ or $y_{t_B}$ to provide better intuition about how the bypass mechanism affects the trajectory.\n\n- How many model forward passes are required for computing $b^*_{t_i}$ and completing the entire image editing process? A detailed breakdown of the computational overhead would help readers assess the practical efficiency of the method, especially compared to baseline approaches."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KdfNeqNPCf", "forum": "ys2psnXTJL", "replyto": "ys2psnXTJL", "signatures": ["ICLR.cc/2026/Conference/Submission3283/Reviewer_7EG6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3283/Reviewer_7EG6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928285264, "cdate": 1761928285264, "tmdate": 1762916645308, "mdate": 1762916645308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}