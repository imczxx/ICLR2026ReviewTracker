{"id": "IAw56L4WjG", "number": 7725, "cdate": 1758033828387, "mdate": 1759897837025, "content": {"title": "Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs", "abstract": "Group Relative Policy Optimization (GRPO) has proven to be an effective tool for post-training language models (LMs). However, AI systems are increasingly expressed as modular programs that mix together multiple LM calls with distinct prompt templates and other tools, and it is not clear how best to leverage GRPO to improve these systems. We begin to address this challenge by defining mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by module across rollouts and handles variable-length and interrupted trajectories.We find that mmGRPO, composed with automatic prompt optimization, improves accuracy by 11% on average across classification, many-hop search, and privacy-preserving delegation tasks against the post-trained LM—and by 5% against prompt optimization on its own. mmGRPO is released as an open-source optimizer for compound-AI systems.", "tldr": "Extends GRPO to modular LM programs; shows that composing GRPO with prompt optimization works the best", "keywords": ["chain of thought", "online reinforcement learning", "in-context learning", "multi-hop reasoning", "prompting techniques", "bettertogether"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dbce6b069b4bef59cad397841c1fefa057862f6a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MMGRPO, a practical extension of Group Relative Policy Optimization (GRPO) designed for post-training multi-module language model programs. The core contribution is a module-level grouping strategy that enables policy gradient updates across structurally diverse trajectories, and the authors demonstrate that combining this method with prompt optimization yields strong performance improvements on a variety of tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses the timely and relevant problem of optimizing compound AI systems, which is a clear departure from traditional single-call LM fine-tuning.\n- The proposed MMGRPO method is an intuitive and elegant extension of the existing GRPO framework, making it straightforward to understand and implement.\n- The empirical evaluation is solid, demonstrating the method's effectiveness across three distinct tasks and showing the complementary benefits of combining weight optimization with prompt optimization through the BetterTogether framework."}, "weaknesses": {"value": "- My main concerns lie in the uniform credit assignment strategy. Assigning the final trajectory's reward to every intermediate module call is a considerable *simplification*. In multi-step reasoning or tool-use scenarios, the importance of each module's contribution can vary greatly; an early-stage error might derail the entire process, while a late-stage refinement might only offer marginal improvement. This approach masks the complexity of temporal credit assignment and may not be robust for tasks with longer horizons or more intricate module dependencies, where the signal-to-noise ratio for early modules would be low.\n- The method's reliance on a deterministic program structure limits its applicability to more dynamic agentic systems where the control flow itself might be learned or stochastic.\n- The experiments are conducted on 8-billion parameter models, and it is unclear how these findings on policy optimization will translate to much larger foundational models."}, "questions": {"value": "- Instead of grouping module calls by their relative invocation order, have the authors considered alternative alignment strategies, such as grouping based on the *semantic similarity* of module inputs, which might create more coherent groups for policy learning?\n- Given the challenge of sparse rewards in long-horizon tasks, was any form of *reward shaping* or intermediate reward mechanism explored? \n- How does the framework differentiate between a low-reward trajectory resulting from a single catastrophic failure versus one from an accumulation of minor imperfections?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j26H9G4kXi", "forum": "IAw56L4WjG", "replyto": "IAw56L4WjG", "signatures": ["ICLR.cc/2026/Conference/Submission7725/Reviewer_eeBe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7725/Reviewer_eeBe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761024772306, "cdate": 1761024772306, "tmdate": 1762919782001, "mdate": 1762919782001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an extension of Group Relative Policy Optimization (GRPO) to a setting where multiple module LM programs are invoked, each with their own prompts/contexts. This is challenging for regular GRPO because of varying number of steps, differing control flows etc. The key idea is to generate full program trajectories, align module calls across trajectories and apply GRPO to each module separately. Results are shown on three tasks - intent classification (Banking 77), privacy conscious delegation (PAPILLON), and multi-hop claim verification (HoVer). Two baseline approaches are compared: a chain of thought (CoT) baseline and prompt optimization (MIPROv2). The proposed method  improves upon CoT and sometimes performs better than MIPROv2. When used in combination with prompt optimization using the better together framework, it consistently outperforms both baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Proposes a novel framework to extend GRPO to a setting where multiple module LMs are called. \n* Demonstrates that the method performs better than a chain of thought baseline.\n* Shows that the method in combination with prompt optimization performs better than using prompt optimization alone."}, "weaknesses": {"value": "* The method does not outperform the prompt optimization baseline on its own.\n* It would be good to report a baseline where GRPO can be applied to the multi-module LM setting in some adhoc manner for a specific setting, e.g. by requiring that rollouts have the same number of steps. Even if such a setting is artificial, it would allow us to measure the benefit of the proposed approach which does not impose such requirements.\n* Though the approach is general and allows for multiple teachers, it seems like all evaluations are reported with a single teacher.\n* L227: The Markovian assumption is a limitation but is not mentioned in the Limitations section. \n* L285: Though the approach allows for different LMs for each module, results are presented only for the scenario where the same LM is used for each module."}, "questions": {"value": "* Sec 5.1: Do any of these setups use more than 1 teacher? i.e. does the list of teacher programs in L666 consist of any teacher other than the student program?  \n* Sec 5.1 : It would be good to present examples of rollouts for each task, showing the trajectory of module calls, and some examples where these might lead to differing lengths.\n* Appendix A3: It would be good to present some examples (inputs/outputs) showing how Algorithm 2 (FormingModuleLevelGroups) works since that is a critical component of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3M4HUqZL2H", "forum": "IAw56L4WjG", "replyto": "IAw56L4WjG", "signatures": ["ICLR.cc/2026/Conference/Submission7725/Reviewer_LPF6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7725/Reviewer_LPF6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761598684636, "cdate": 1761598684636, "tmdate": 1762919781265, "mdate": 1762919781265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies reinforcement learning for multi-module LM programs by moving GRPO updates from full trajectories to module-level groups, and articulates when this decomposition is preferable. Trajectory-level GRPO is feasible for fixed, single-shape pipelines, but becomes awkward when control flow varies (skips, repeats, early stops) or when module boundaries are administered separately. The proposed MMGRPO samples full runs, then groups tokens by (module id, k-th invocation) and applies a GRPO loss to that module’s tokens using the final program reward. The method rests on a Markov-given-message assumption: once routed to module M, next-token probabilities depend only on M’s input message q (which is expected to summarize upstream state) and its own prefix; routing and tools are treated as environment. Under three conditions, (one shared LM being optimized, equal or linearizable structure, and determinism given the same generations), the paper shows the sum of per-module gradients has the same direction as the trajectory-level gradient up to scale, which justifies the decomposition in fixed-shape settings. In practice, full-run sampling lets each run contribute to every module that fired and allows reuse of teacher traces by aligning their module calls; with LoRA updates and an optional prompt-optimization stage, results are comparable to trajectory-level training on fixed structures and remain usable when control flow varies, subject to how well q captures the necessary context."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It's relative easy, practical implementation. Sampling full runs does not require architectural changes; Integrates with LoRA, optional prompt optimization, and teacher traces.\n2. Though many assumptions, they are stated properly. The main result is scoped correctly: directional equivalence of trajectory- vs module-level gradients under explicit conditions."}, "weaknesses": {"value": "1. Narrow utility window. Benefits seem tied to variable control flow or administrative boundaries; outside that, a single trajectory loss is simpler and likely stronger.\n2. Arbitrary partitioning risk. Module boundaries are designer choices; outcomes can depend on where you cut. The paper offers no merge/split robustness or guidance.\n3. Strong Markov-given-message assumption. It is assumed, not validated, that q fully summarizes upstream state; if serialization is lossy, gradients are biased.\n4. Heterogeneous groups. Grouping by the k-th call mixes unlike contexts/prompts; apples-to-oranges grouping can inflate variance and misdirect updates."}, "questions": {"value": "1. merge/split: what's the design criteria for choosing the boundary? also include ablation of merge/split of modules.\n2. specify the message q scheme. what information about all prefix goes into the message that by right is sufficient to encode everything?\n3. following (2), curious to see a state-Incorporated module (learned latent / prompt embedding). Try a learned state appended to q: a small encoder s = f(upstream traces) whose vector is serialized (text or special tokens) or fused via a prompt embedding adapter.\n4. would grouping benefit from context awareness, not simply same call, same module name? Add a variant grouping by (module, k, cluster(q \\oplus s)). If this outperforms (module, k) at the same keep-rate/compute, it indicates heterogeneity at fixed k was hurting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7oN28ggb4R", "forum": "IAw56L4WjG", "replyto": "IAw56L4WjG", "signatures": ["ICLR.cc/2026/Conference/Submission7725/Reviewer_5Lbj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7725/Reviewer_5Lbj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802869385, "cdate": 1761802869385, "tmdate": 1762919779626, "mdate": 1762919779626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces MMGRPO, a multi-module extension of  GRPO designed for modular AI systems that combine multiple language model calls and tools. MMGRPO groups calls by module and manages variable or interrupted trajectories. When combined with automatic prompt optimization, it improves accuracy over post-trained LMs and over prompt optimization alone. The optimizer is released as an open-source tool."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper appears solid overall, providing a convincing answer to the research question it poses.\n\n- Although it does not make a particularly strong claim, the motivation and reasoning are coherent.\n\n- The proposed approach is conceptually simple, as it mainly applies GRPO to a multistep system rather than introducing a new or overly complex algorithm."}, "weaknesses": {"value": "- The ablation study and experimental analysis, from a quantitative point of view, are limited. Reducing the empirical depth of the analysis.\n\n- The evaluation relies exclusively on accuracy, which is inadequate for large language model assessment.\nThe authors should adopt standard LLM evaluation metrics, such as pass@k (and ideally maj@k, as used in DeepSeek), to provide a more comprehensive and reliable performance evaluation."}, "questions": {"value": "Please address the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Xt5uBl4cXv", "forum": "IAw56L4WjG", "replyto": "IAw56L4WjG", "signatures": ["ICLR.cc/2026/Conference/Submission7725/Reviewer_ZN5E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7725/Reviewer_ZN5E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820204250, "cdate": 1761820204250, "tmdate": 1762919779275, "mdate": 1762919779275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed Multi-module GRPO, an extension to GRPO algorithm/implementation which allows grouping LM calls by module and weight optimization for each module separately. They motivate this through the need for optimizing compound AI systems that consist of multiple modules (e.g., deep research) and allow privacy-preserving applications. They also showed combining mmGRPO with prompt optimization techniques consistently improve performance across 3 tasks of single-stage intent-classification, two-stage privacy-conscious delegation program, and four-stage claim verification."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is addressing an increasingly relevant challenge in modern AI systems—optimizing multi-component or modular architectures rather than isolated models"}, "weaknesses": {"value": "- First, the paper is really poorly written, and suffers from significant clarity and presentation issues. It lacks a clear scientific structure and concrete running examples that could help readers understand the core problem and motivation. Introducing a concrete example early in the introduction would greatly improve readability. Certain terms get used for the first time without prior definition or explanation (see details in question section). Overall, the paper, in its current state, is not ready for publication and would benefit from substantial rewriting to improve presentation and clarity.\n\n- The results section is also underdeveloped. The empirical evaluation is limited to a single table (Table 1) with no additional analysis, and ablation studies to justify the design choices or isolate the contributions of different components. The comparison baselines are also quite narrow.\n\n- Finally, the advantages of mmGRPO over simpler methods such as prompt optimization, which require substantially less computational overhead, are not convincingly demonstrated (and are even acknowledged by the authors)"}, "questions": {"value": "1- [suggestion] The paper can benefit from a teaser/motivating figure that readily explains the idea/motivation behind the method.\n\n2- [suggestion] A concrete example on LM program would have been helpful for the reader to understand what you mean by Control Flow, trajectory of module calls, etc.\n\n3- [Line 191] certain terms get used for the first time without properly being introduced before. What does the author refer to student/teacher program In Section 3? \n\n4- Do authors consider a separate module for the last step in each tasks? like label generation for classification, final response generation for PUPA etc.\n\n5- why not including the trajectory-level GRPO as another baseline to better understand the contribution of grouping the trajectories (module-level)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mr8XmpUprH", "forum": "IAw56L4WjG", "replyto": "IAw56L4WjG", "signatures": ["ICLR.cc/2026/Conference/Submission7725/Reviewer_qhaK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7725/Reviewer_qhaK"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866796068, "cdate": 1761866796068, "tmdate": 1762919778633, "mdate": 1762919778633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}