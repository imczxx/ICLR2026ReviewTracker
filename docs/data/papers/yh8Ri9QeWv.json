{"id": "yh8Ri9QeWv", "number": 23590, "cdate": 1758346046029, "mdate": 1759896806111, "content": {"title": "When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models", "abstract": "Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.", "tldr": "We show that BT-loss in reward modeling leads to spurious learning signals due to representation distance, thus propose a normalization to rescale update focusing on prediction error.", "keywords": ["LLM Alignment", "RLHF", "Reward Model", "Bradley-Terry"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/81f7e39e9d82119f2087589446ee79f9ca2674ab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors observed that the gradient update under BT loss involving representation distance and proposed an alternative loss that normalize over representation distance so that mean training signal is from prediction accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The presentation is good and derivations sound. Empirical results showed improvement over existing baselines in at least some of the experiments."}, "weaknesses": {"value": "The paper builds on the premise that representation distance ***should not*** be treated as part of the signal and its apparance in gradient is a problem that we should solve. This key premise is not justified to a satisfactory level. In fact from statistical literature on logistic regressions (where a linear BT is a special case), the asymptotic variance of parameter should depends on representation distance (e.g., through [classic asymptotic theory](https://stats.stackexchange.com/questions/231329/fisher-information-in-logit-model)). This is the key that the model can do prediction on unseen pairs. I acknowledge the empirical results showed improvements in some cases but it is not consistent in all experiments and the conceptual connection is unclear. If the premise is true, following the authors' argument, for any classification problem use cross entropy loss the last layer representation vector (in place of difference of representation) should not appear in the gradient as BT can be seen as a classification problem with features being representation *difference* before last layer.  \n\nAlso the experimental results in tables is hard to interpret by themselves. I noticed that in the tables the authors highlight the proposed method without explaining, while the proposed method is neither highest nor lowest in the numerical results, e.g., in Table 1, with gemma-2b-it, in task **Chat** BT + margin out is largest and BT+ label smooth is the smallest. Same thing happens in every individual tasks beside **Reasoning** (and average due to Reasoning)."}, "questions": {"value": "- Intuition on representation distance and prediction error entanglement: \n    - The authors argued in Q1 that it supports intuition from (7) and argued that representation distance is entangled with prediction error by the product structure in the gradient. However, it can be deeper than that product structure. If we are willing to assume the reward model is smooth enough (as the authors assumed), pairs close in representation space should also have similar reward value and small $|d|$. In my understanding these are pairs that are *not* informative for the model: in the extreme case that the two responses are identical so $d=0$, preference annotation is at best a coin toss, the labeling error $\\sigma(d)-1$, in such data, is going to be large either way, and the reward distance **should** kill that data in update as preference between identical pairs are pure noise. This argument should holds for small distance pairs --- model (and us) should not be surprised that we cannot predict results of close-to-fair coin flips well as there is no signal. In this case should the representation distance be used to reduce the weight on these unsurprising pairs? The authors proposal would up weight these kind of high noise pairs. \n\n    - Aren't representation part of the signal? For reward modeling, getting the ranking of existing pairs is not the end of the story, one needs to be able to predict unseen new pairs. These can be done leveraging smoothness in representation space (Sun et al. 2025). For reward model to generalize to unseen pairs we would need to see things large enough to cover most of the representations (difference) space so that we are doing interpolation rather than extrapolation. What is the intuition that we should kill the signal from representation space? \n\n- What is the mechanism the proposed normalization on **loss** works?\n    - It is indeed true that for a data point that 1) the model gets wrong and 2) the representation distance is large. The loss gradient can even diverge. Is the normalization avoiding this only? Will a gradient clipping achieve the same goal? What is the percentage of data point these two things happen at the same time? \n        - This is close to ask that whether the issue of having distance in gradient is 1) it just should not be there or 2) some of them can be too big that harm optimization. \n    - A layer norm before last linear layer. How does the authors' proposal compare to this implementation tweak? From the statistical literature it is know that it can be beneficial to normalize features before fitting logistic regression. \n\n- How to reconcile with past active learning results?\n    - A line of work on active learning like Feng et al. 2025 and Shen et al. 2025 actually treated representation distance as part of the signal and look for points that the model struggles and representation is far. They showed doing this is beneficial. My intuition is that these design helped to cover large representation space for generalization. I wonder how these line of results can be reconciled with what the authors observed that normalizing using representation distance being helpful? \n\n\n\n\nSun, Hao, Yunyi Shen, and Jean-Francois Ton. \"Rethinking reward modeling in preference-based large language model alignment.\" In The Thirteenth International Conference on Learning Representations. 2025.\n\nFeng, Y., Kwiatkowski, A., Zheng, K., Kempe, J. and Duan, Y., 2025. Pilaf: Optimal human preference sampling for reward modeling. arXiv preprint arXiv:2502.04270.\n\nShen, Y., Sun, H. and Ton, J.F., Active Reward Modeling: Adaptive Preference Labeling for Large Language Model Alignment. In Forty-second International Conference on Machine Learning. 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WY4vA4bONP", "forum": "yh8Ri9QeWv", "replyto": "yh8Ri9QeWv", "signatures": ["ICLR.cc/2026/Conference/Submission23590/Reviewer_31NW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23590/Reviewer_31NW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760903458814, "cdate": 1760903458814, "tmdate": 1762942725213, "mdate": 1762942725213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the update dynamics of the Bradley–Terry (BT) loss commonly used in reward modeling for RLHF.\nThe authors identify that the gradient norm of BT-loss depends not only on the prediction error (i.e., reward difference between chosen and rejected responses) but also on the representation distance between their hidden states (Eq. 7).\nThis coupling leads to what the paper calls representation distance bias—pairs with large embedding distances receive disproportionately large updates even when correctly ranked, while small-distance pairs (often in reasoning tasks) receive vanishingly small gradients.\nTo address this, the paper proposes NormBT, a pair-wise normalization scheme that rescales gradient contributions by the inverse of the representation distance (Eq. 10–13).\nEmpirical results on RewardBench show consistent improvements over the vanilla BT baseline, particularly in the Reasoning category (+5 % absolute accuracy in Table 1), without major regressions in other categories."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear identification of a structural bias in BT-loss: The decomposition of gradient norm (Eq. 7) elegantly shows that update magnitude scales with both prediction error and representation distance. This theoretical insight provides a solid foundation for understanding how BT-based reward models may fail to learn from fine-grained preference pairs, especially in reasoning-oriented data.\n2. Simple, lightweight correction: NormBT is a “drop-in” modification requiring no architectural change.\nBy reweighting each pair’s contribution with $w_i = 1 / \\|h_w - h_l\\|$, it effectively balances updates between small- and large-distance pairs (Sec. 2.2). The EMA-based normalization (Eq. 11–12) further ensures numerical stability without noticeable computational cost.\n3. Empirical consistency without global degradation: Across four experiment settings (Table 1a–1b), NormBT consistently improves performance, with the largest gain in Reasoning pairs—those identified as suffering most from gradient underflow in Figure 2.\nThe model does not experience major losses in other domains, suggesting a targeted improvement rather than a crude reweighting.\n4. Strong conceptual clarity and visualization:  Figures 1–3 compellingly illustrate the problem: reasoning pairs lie close in representation space and thus yield weak gradients under BT-loss.\nThe analysis connects qualitative intuition with quantitative evidence."}, "weaknesses": {"value": "1. Limited generality of theoretical analysis: The derivation in Eq. 7 assumes a linear score head $r(x, y) = w_s^T h_\\phi(x, y)$ and a Lipschitz-smooth embedding map.\nThis simplification ignores the nonlinear components of modern reward models (layer normalization, activation scaling, or residual mixing).\nThe paper does not test whether the same coupling holds under non-linear heads or multi-layer scoring networks.\nTherefore, the claimed “representation distance bias” may be an artifact of this specific linear assumption rather than a universal property of BT-loss.\n2. Gradient magnitude only—no directional analysis: The study focuses purely on gradient norms (Figure 2, Eq. 7) without considering gradient directions or correlations across pairs.\nRepresentation similarity might also induce gradient alignment bias (e.g., correlated update directions causing slow convergence), but this aspect is not examined.\nThis omission limits the understanding of whether the issue is truly about update strength or about geometric interference in optimization.\n3. Narrow experimental scope:  The evaluation relies solely on RewardBench and two backbones (Gemma-2B-it and Llama-3.2-3B).\nWhile Table 1 shows improvement on Reasoning tasks, Safety and Chat-Hard categories exhibit negligible or even slightly negative changes (e.g., Chat-Hard: 40.35 → 39.80).\nNo standard deviation or significance testing is reported, making the claimed “5 % improvement” potentially within noise range.\n4. Lack of downstream validation in RLHF: The paper focuses exclusively on reward model accuracy without verifying whether these improvements translate to better policy alignment. Unlike On the Robustness of Reward Models for LM Alignment (Hong et al., 2025), which analyzes propagation of reward robustness to RLHF training (Figure 5–7 in that paper), NormBT stops short of such experiments.\nThis leaves open whether reduced representation bias actually yields more stable or less verbose RLHF outcomes.\n5. Scalability and numerical stability concerns: While the paper claims “negligible overhead,” pair-wise distance computation and EMA tracking may introduce instability for large batches or larger backbones (7B–13B). The authors provide no training dynamics such as gradient norm evolution or variance plots to demonstrate convergence stability under normalization. Table 2 shows performance drops when EMA is removed (67.78 avg vs 73.57 with EMA), but does not clarify whether EMA stabilizes or merely rescales.\n6. Insufficient ablation and alternative metrics:  The proxy $\\|h_w - h_l\\|$ is justified through correlation with full gradient distance (r = 0.928 in Appendix C), but no comparison with alternative similarity measures (cosine distance, Mahalanobis, etc.) is provided.\nConsequently, it is unclear whether “norm distance” is the optimal normalization factor or merely one convenient proxy.\n7. Missing analysis of dataset-dependent behavior: Figure 2 shows that reasoning pairs have smaller distances, but the paper never explains why. Are these due to shorter responses, higher lexical overlap, or semantic similarity? Without analyzing dataset-level statistics, the conclusion risks attributing data-specific phenomena to universal model bias."}, "questions": {"value": "1. Eq. 7 assumes a linear reward head. Would the same distance coupling persist if r(x, y) were computed through a multi-layer MLP or a mixture-of-experts head?\n2. Have the authors tested whether using a NormBT-trained RM leads to better alignment during PPO or RLOO training, similar to how BSR (Hong et al., 2025) demonstrated downstream effects?\nIf not, could this method unintentionally bias generation length or safety preference?\n3. Since normalization depends on $∥h₍w₎−h₍l₎∥$, how does NormBT behave if the backbone representation scale changes due to LayerNorm configuration or mixed-precision training?\n4. Are there cases where $1/∥h₍w₎−h₍l₎∥$ causes gradient explosion for extremely similar pairs?\nHow is this mitigated beyond the small constant ϵ introduced in Eq. 11?\n5. Could the authors provide standard deviations or confidence intervals for Table 1 results to confirm that the observed gains are statistically significant?\n6. How would NormBT perform on other preference datasets (e.g., UltraFeedback, Skywork-Reward-Preference-80K) or under OOD settings with domain shift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lFuaJApfBg", "forum": "yh8Ri9QeWv", "replyto": "yh8Ri9QeWv", "signatures": ["ICLR.cc/2026/Conference/Submission23590/Reviewer_HY6G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23590/Reviewer_HY6G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761229491593, "cdate": 1761229491593, "tmdate": 1762942724955, "mdate": 1762942724955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses reward model training via the Bradley-Terry (BT) objective and discusses how the gradient per-sample shows that representation distance between chosen and rejected responses can be quite influential on training. Thus, for problems like reasoning, smaller distances lead to updates that are less effective. The paper proposes NormBT - a method to mitigate this issue for RM training"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important problem of the impact of representation distance on alignment, Figure 1 is an important illustration.\n2. The gradient norm analysis is crisp and the norm of the gradient is neatly depicted to be dependent on prediction error and representation distance.\n3. The final objective for NormBT is quite intuitive and easy to understand."}, "weaknesses": {"value": "1. Most direct alignment methods (DAAs) like DPO [1], SimPO [2] and AlphaPO [3] skip the reward modeling stage. DAAs are the most popular ways to align LLMs these days, making the paper a bit limited in its impact.\n2. Reward models can easily get over optimized. The paper lacks ablations and experiments discussing the careful optimization of RMs during training.\n3. The baselines are not explained in detail\n4. The experiments are not trustworthy because there are no error bars, very small models were used and the major improvement is in reasoning. Why does changing the loss function improve reasoning but not other categories? I dont think the smaller representation distance explanation is sufficient."}, "questions": {"value": "See weaknesses. The experiments are quite limited and lacking any kind of significance testing. Significant overhaul is needed before the paper is ready for ICLR"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "e9Ux8i63ew", "forum": "yh8Ri9QeWv", "replyto": "yh8Ri9QeWv", "signatures": ["ICLR.cc/2026/Conference/Submission23590/Reviewer_GbNk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23590/Reviewer_GbNk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972985949, "cdate": 1761972985949, "tmdate": 1762942724056, "mdate": 1762942724056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a modified reward modeling objective, NormBT, motivated by the learning dynamics of the conventional Bradley-Terry (BT) model. Through gradient analysis on the BT loss, the paper observes that the magnitude of gradient updates varies by the samples, which is represented by different topics in practice. Mainly coming from the hidden representation discrepancy while learning to rank the responses, the proposed method thereby regularizes the BT loss by sample-level hidden representation distance norm. NormBT was evaluated on the well-known datasets and models, outperforming several variations of BT models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a mathematical decomposition of the Bradley-Terry model in the reward modeling context.\n2. While occasionally underperforming compared to the baselines, NormBT generally demonstrates strong performance on the benchmark.\n3. The post-hoc analysis on small-margin items in Figure 4 aligns the motivation and empirical consequences, making the method more convincing."}, "weaknesses": {"value": "Despite the strengths, the paper can be improved with more empirical support to demonstrate the practical advantage of NormBT.\n\n1. **Reward modeling baselines**: The authors compare NormBT against several variants of BT loss in Section 3. Given that the main objective of this paper is to develop a reward modeling algorithm that effectively captures true preferences in the data, the baselines need not be limited to BT loss variants. Specifically, a few points on GRM [1] need to be clarified by the authors, which are listed in the Questions section below. Other than GRM, the authors can address several additional methods that have been tried to improve the performance of reward models given fixed preference data.\n\n\n2. **Benchmark analysis**: While RewardBench is a great benchmark to assess the reward models, there are a lot of works that follow RewardBench by introducing more challenging tasks, as recent RMs obtain very high scores on RewardBench. Thus, cross-validation with one or two more reward modeling benchmarks could strengthen the authors’ claim that NormBT is a performant reward modeling objective, e.g., RM-Bench [2] and RewardBench 2 [3]. Especially given that NormBT sometimes underperforms, cross-validating across multiple benchmarks can strengthen the paper.\n\n3. **Additional references on systematic analysis of BT model’s learning dynamics**: As a minor comment, there were a few previous works that studied the learning dynamics of the BT model, such as [4] that also spot hidden representation margin to be the source of the reward model over-optimization issue, which is one of the core motivation of the paper. To claim methodological novelty, previous works should be more precisely addressed and contrasted.\n\n\n&nbsp;\n\n**References**\n\n[1] Yang et al., 2025, “Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs.” (NeurIPS 2024)\n\n[2] Liu et al., 2024, “RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style.” (ICLR 2025)\n\n[3] Malik et al., 2025 “RewardBench 2: Advancing Reward Model Evaluation.”  (Preprint)\n\n[4] Hong et al., 2025, “On the Robustness of Reward Models for Language Model Alignment.” (ICML 2025)"}, "questions": {"value": "- Why is GRM-Gemma-2B reported but not GRM-Llama-3.2-3B? – In Table 1(b), the authors report GRM-Gemma-2B, but not GRM-Llama-3.2-3B. [GRM trained on Llama-3.2-3B](https://huggingface.co/Ray2333/GRM-Llama3.2-3B-rewardmodel-ft) is directly comparable to the last rows of Table 1(b), so it can be easily added to the results. And GRM-Llama-3.2-3B on the official RewardBench leaderboard scores 90.9 on average.\n\n- Is GRM-Gemma-2B trained on Skywork-Reward-Preference-80k-v0.2? – The [official model card](https://huggingface.co/Ray2333/GRM-Gemma-2B-sftreg) for GRM-Gemma-2B reports that it was trained on [a different mixture](https://huggingface.co/datasets/weqweasdas/preference_dataset_mixture2_and_safe_pku). However, it is listed as a reward model trained from Skywork-Reward-Preference-80k-v0.2 in Table 1(b). If the caption holds for the last four rows and not the top four models, including GRM-Gemma-2B-sftreg, it should be clearly explained."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rqQJ38iNGy", "forum": "yh8Ri9QeWv", "replyto": "yh8Ri9QeWv", "signatures": ["ICLR.cc/2026/Conference/Submission23590/Reviewer_R4Sp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23590/Reviewer_R4Sp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975847641, "cdate": 1761975847641, "tmdate": 1762942723506, "mdate": 1762942723506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}