{"id": "EYUG4Su6ZU", "number": 20638, "cdate": 1758308464852, "mdate": 1759896966630, "content": {"title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports", "abstract": "Artificial intelligence is undergoing the paradigm shift from closed language models to interconnected agent systems capable of external perception and information integration. As a representative embodiment, Deep Research Agents (DRAs) systematically exhibit the capabilities for task decomposition, cross-source retrieval, multi-stage reasoning, and structured output, which markedly enhance performance on complex and open-ended tasks. However, existing benchmarks remain deficient in evaluation dimensions, response formatting, and scoring mechanisms, limiting their capacity to assess such systems effectively. This paper introduces a rigorous benchmark and a multidimensional evaluation framework tailored to DRAs and report-style responses. The benchmark comprises 214 expert-curated challenging queries distributed across 10 broad thematic domains, each accompanied by manually constructed reference bundles to support composite evaluation. The framework enables comprehensive evaluation of long-form reports generated by DRAs, incorporating integrated scoring metrics for semantic quality, topical focus, and retrieval trustworthiness. Extensive experimentation confirms the superior performance of mainstream DRAs over web-search-tool-augmented reasoning models, yet reveals considerable scope for further improvement. This study provides a robust foundation for capability assessment, architectural refinement, and paradigm advancement in DRA systems.", "tldr": "", "keywords": ["Deep Research", "Agents", "Benchmarking"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4dd72d82801bd9c677c4a941d47be0ab104bf0d4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Rigorous Bench, a benchmark specifically designed to evaluate Deep Research Agents on report-style tasks. \nIt contributes 1) a dataset of 214 expert-curated, high-complexity queries across diverse domains, each paired with rich reference bundles,  and 2) a multidimensional evaluation framework capturing semantic quality, topical focus, and retrieval trustworthiness. \nExperiments with 13 models show that DRAs outperform tool-augmented baselines, while highlighting systemic limitations such as efficiency–quality and decomposition–coherence trade-offs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Some prior benchmarks are limited to short-form answers, while this work targets long-form, report-style outputs.\n- The dataset design includes expert involvement, multi-stage validation, and inclusion of trustworthy sources.\n- Clear evaluation framework with interpretable integrated scoring, not just black-box similarity metrics.\n- Experimental results are extensive, covering a variety of strong baselines and providing clear differentiation."}, "weaknesses": {"value": "- Heavy reliance on expert-designed rubrics and curated sources may introduce biases and limit scalability.\n- Experiments focus mainly on current DRAs vs tool-augmented LLMs. It is unclear how robust the benchmark is for future, more diverse systems."}, "questions": {"value": "- Fig. 2 contains too many abbreviations. They should be explained in the caption at least."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PNY9wb5LUz", "forum": "EYUG4Su6ZU", "replyto": "EYUG4Su6ZU", "signatures": ["ICLR.cc/2026/Conference/Submission20638/Reviewer_a7RE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20638/Reviewer_a7RE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760889642641, "cdate": 1760889642641, "tmdate": 1762934036685, "mdate": 1762934036685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new benchmark and multidimensional evaluation framework designed for Deep Research Agents (DRAs), a class of AI systems capable of external information integration, complex reasoning and report generation. The benchmark includes 214 expert-crafted queries across 10 thematic domains, with manually curated reference materials to facilitate comprehensive evaluation. The proposed multidimensional evaluation framework assesses long-form, report-style outputs along dimensions such as semantic quality, topical relevance, and retrieval reliability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work introduces a multidimensional evaluation framework that goes beyond single-metric benchmarks (like accuracy or BLEU) to assess semantic quality, topical focus, and trustworthiness.\n2. The inclusion of reference bundles enables composite and contextual evaluation."}, "weaknesses": {"value": "1. The links provided in the benchmark may not work over time, as the page changes or disappears over time.\n2. The motivation of compositional score metrics is not very clear. What's the difference between this work and just defining multiple fine-grained aspects and doing a performance average? Is it necessary to build such a compositional scoring framework?"}, "questions": {"value": "1. Regarding the benchmark fragility and temporal decay, tasks depend on live web content (so called Trustworthy-Source Links in the paper), benchmark reproducibility decays quickly since pages change, disappear, or alter their information hierarchy. Without versioned snapshots, how to ensure these links are workable so that the proposed benchmark is useful over time?\n2. What's the difference between this work and just defining multiple fine-grained aspects and doing a performance average? Is it necessary to build such a compositional scoring framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zco13bUusK", "forum": "EYUG4Su6ZU", "replyto": "EYUG4Su6ZU", "signatures": ["ICLR.cc/2026/Conference/Submission20638/Reviewer_MLBk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20638/Reviewer_MLBk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897129461, "cdate": 1761897129461, "tmdate": 1762934036273, "mdate": 1762934036273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work claims to introduce a rigorous benchmark for evaluating deep research agents (DRAs) for report style response generation. The dataset spans 10 domains and includes 214 queries. A multi-dimensional evaluation framework is introduced for fine-grained evaluation, focusing on semantic quality, topical focus, and retrieval trustworthiness. Experiments on popular deep research agents as well as language models reveals the strong performance of DRAs and room for further improvement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work contributes a dataset that might be useful for developing future deep research agents.\n2. The evaluation results provide some insight into performance of current deep research methods."}, "weaknesses": {"value": "1. The overall writing of this paper lacks precision and rigor, missing quite a few key details. Some major ones are:\n    1. line 179-183, QSRs cover many aspects. However, there's no further detail or precise definitions for each.\n    2. line 212-213 what terms qualifies as \"prone to triggering topic divergence\".\n    3. Who qualifies as \"experts\" in the data collection process?\n    4. line 269, what are the exact principles followed?\n    5. Section 4.1, how is ratio normalization exactly done?\n    6. 363 to 365, what is the framework scale to and transfer to and how is it shown by the Algorithm?\n2. The dataset uses source links for evaluation. However, there lacks a discussion on how to keep the links up-to-date and how to handle broken links. Changes to the websites can impact the evaluation results.\n3. The metrics heavily rely on key work matching, which I doubt is suitable for evaluation deep research agents. The contribution efficiency metric seems to naturally biased against long responses. The integrated metric includes multiple parts, but there no ablation study to justify the necessity of each. There also lacks a justification for the effectiveness of the metrics in general, such as agreement to human judgement.\n4. Given the above points, I have serious doubts in the quality and value of the proposed dataset and evaluation framework."}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Uu9FmUNqM6", "forum": "EYUG4Su6ZU", "replyto": "EYUG4Su6ZU", "signatures": ["ICLR.cc/2026/Conference/Submission20638/Reviewer_9wgF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20638/Reviewer_9wgF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980106600, "cdate": 1761980106600, "tmdate": 1762934035916, "mdate": 1762934035916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new deep research benchmark and multidimensional evaluation framework. The benchmark consists of 214 expert-curated queries, each paired with a \"reference bundle\" that includes: Query-Specific Rubrics (QSRs), General-Report Rubrics (GRRs), Trustworthy Source Links (TSLs), Focus-Anchor Keywords (FAKs), and Focus-Deviation Keywords (FDKs).\n\nOn top of these, the benchmark defines a set of quantitative metrics for evaluating report-style outputs along multiple dimensions. The paper evaluates 13 models or agents, including both Deep Research systems and web-search-augmented reasoning models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "A viable attempt to evaluate deep research agents on long-report tasks with carefully designed rubrics and a comprehensive evaluation framework. The construction process is well-documented and the multidimensional scoring design (semantic quality, topical focus, retrieval trustworthiness) is conceptually thorough."}, "weaknesses": {"value": "- There has been a surge of recent benchmarks in the deep research domain, several of which already include evaluation of long-form answers, citation accuracy, and rubric-based evaluation. It's suggested to include a table of comparison for  key dimensions (e.g., short answer or report, static or time-varying tasks, evaluation methods). Otherwise it's a bit hard to understand the contribution under the crowded space.\n- The main text largely describes the benchmark and evaluation pipeline only in abstract terms. It lacks concrete examples to illustrate task nature and rubric design. At least one representative task example should appear in the main paper rather than only in Appendix B.3, to help readers understand the task more intuitively.\n- The evaluation heavily relies on LLM-as-a-Judge to score rubrics (a list of Yes/Partial/No items) and on the quality of both task-specific and meta-level rubrics. However, the paper provides little analysis of the reliability of LLM-based judgments, nor of the coverage or independence of rubric entries themselves. These aspects are crucial to the claimed rigor and would be better to be examined.\n- The benchmark construction phase reportedly includes a filtering stage using o4-mini-deep-research, which could introduce systematic bias by excluding certain types of tasks that such a model handles poorly.\n- The General-Report Rubrics (48 items) embed many assumptions about ideal formatting and stylistic qualities, yet it is unclear whether these are necessary, objective, or universally desirable criteria. From a quick look, not all tasks or agents should be expected to satisfy all 48 stylistic rules, and several rubric items involve inherently subjective judgments. Similar concerns may apply to the task-specific rubrics.\n- The Retrieval Trustworthiness metric appears overly dependent on the provided Trustworthy Source Links (TSLs), even though in many realistic cases alternative credible sources could exist. This dependence may penalize otherwise valid retrievals that fall outside the preselected reference set.\n- Overall, the evaluation design lacks sufficient justification, making it difficult to fully trust the results in the main table. Some systems may achieve higher scores simply because they happen to align better with the benchmark’s stylistic or structural expectations. As in prior work (e.g., Du et al., 2025), the scoring mechanism itself is also extremely complex, which further obscures interpretability and robustness."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IfjHBP5gir", "forum": "EYUG4Su6ZU", "replyto": "EYUG4Su6ZU", "signatures": ["ICLR.cc/2026/Conference/Submission20638/Reviewer_Uu5v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20638/Reviewer_Uu5v"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986972133, "cdate": 1761986972133, "tmdate": 1762934035177, "mdate": 1762934035177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}