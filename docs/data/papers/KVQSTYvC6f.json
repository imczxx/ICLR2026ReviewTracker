{"id": "KVQSTYvC6f", "number": 14853, "cdate": 1758244720004, "mdate": 1763715436412, "content": {"title": "Factor-Wise Homogeneity of Slot-Attention for Continual Object-Centric Learning", "abstract": "Can current AI models continually learn object-centric representations?\nObject-Centric Learning and Continual Learning are both critical areas of AI research, yet their intersection remains underexplored. In this work, we observe that Slot Attention, a popular OCL method, exhibits a distinctive behavior: \nIt organizes latent representations into small and separated regions, each of which \npreserves the same factor states, referred to as \\textit{factor-wise homogeneity}.\nThis phenomenon emerges not only in previously trained data but also in upcoming data with unseen factor states, offering significant advantages for continual learning that incrementally expands factor states, such as novel shapes. To harness this property, we propose a simple and effective method, \\textit{Decoder only Post Replay}, that freezes the encoder and the Slot Attention as a generator of factor-wise homogeneous representations and employs a decoder-only fine-tuning strategy after the novel task training is done.\nAlthough Slot Attention has been widely studied, its representational behavior has been largely overlooked. This paper highlights its unique strengths in continual object-centric learning. We also introduce a novel validation and analysis environment for Continual-Object Centric Learning, establishing a strong baseline for future research.", "tldr": "We reveal that Slot Attention induces factor-wise homogeneous representations that offers significant advantages for continual object-centric learning.", "keywords": ["Object-Centric Learning", "Representation Learning", "Continual Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e2edf48cbcf80d43d4d7d6c6d576ca707a998ce1.pdf", "supplementary_material": "/attachment/3bde7e366b4b7c056967a5664a1c9577f5b8814c.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies continual object-centric learning and finds that Slot Attention naturally organizes slot features into small, factor-consistent, well-separated neighborhoods across tasks, even for unseen objects and tasks. Following this, they propose Decoder-Only Post Replay (DPR): after each task, freeze encoder + slots and fine-tune only the decoder on a mix of buffered past reconstructions and some current data to combat forgetting. Across Tetrominoes/CLEVR/COCO and multiple task schedules, SA+DPR improves FG-ARI and MSE vs. vanilla SA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper studies the problem continual learning in object-centric learning. There is not lot of work on this topic therefore it seems like an interesting topic to study."}, "weaknesses": {"value": "My main concern is that the the authors say that the encoder + slot attention can transfer to new tasks and only the decoder needs to be trained for adaptation. I have two critiques regarding this - \n\n1. I think the main goal of object centric representations is to learn good representations for downstream tasks. This paper only evaluates segmentation via ARI. SAM and other segmentation methods can already do that well and still generalize, so in my view evaluating these methods on ARI is not well motivated.\n2. If the authors observe that only the decoder needs to be updated for transfer, why not just use the masks from slot attention for computing ari? why do you need the decoder at all? Many works do this - https://arxiv.org/abs/2110.11405, https://arxiv.org/abs/2209.14860\n\nSecondly, the observation that the encoder + slot representations transfer well across domains was already shown in https://openreview.net/forum?id=bSq0XGS3kW, so I believe that the observation is nothing new. In fact the mentioned paper shows that object-centric models can transfer to new domains and objects very well without seeing much samples during training so I don't see why we need a continual learning setup at all in object-centric learning?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aqaJ84mMBE", "forum": "KVQSTYvC6f", "replyto": "KVQSTYvC6f", "signatures": ["ICLR.cc/2026/Conference/Submission14853/Reviewer_o9jF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14853/Reviewer_o9jF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761319997494, "cdate": 1761319997494, "tmdate": 1762925206853, "mdate": 1762925206853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Responses to Reviewers concerns"}, "comment": {"value": "We sincerely thank the reviewers for their thoughtful and constructive feedback. We would like to share some of our responses to several reviewer’s concerns. \n\nBelow, we summarize the key concerns raised by the reviewers, focusing on their main questions regarding the novelty and explanations of our findings.\n\n- Reviewer **jYDk**: Points out that factor-wise homogeneity is not a novel discovery but rather an expected property of well-trained representation models (Weakness 1). Also, request for mechanistic explanation of how Slot Attention’s iterative attention and GRU updates specifically give rise to this phenomenon (Question 1). Points out uncertainty regarding whether factor-wise homogeneity is a unique property of Slot Attention (Question 1), and raises concerns about the unclear causal link between this property and the effectiveness of DPR (Question 2).\n- Reviewer **cgDZ**: Points out that the paper lacks a theoretical explanation for why factor-wise homogeneity emerges (Weakness 1).\n\n## **1. Clarification of Our Main Contribution**\n\nOur main contribution is to more clearly identify the factor-wise homogeneity present in Slot Attention representations, a behavior that has been noted only implicitly or observationally in a few prior works. \n\nMore importantly, we focused on sharing the finding that **the strong separation of representations across tasks originates from this factor-wise homogeneity in Slot Attention**, and that this **structural property can substantially improve continual object-centric learning** even when combined with simple constraints such as DPR or PR. \n\n## **2. Addition of Deeper Analysis for Understanding the Cause of the Phenomena (via GRU dynamics in Slot Attention)**\n\nWe acknowledge the importance of providing explanation of the emergence of factor-wise homogeneity and its stability in continual learning. To address the reviewer (**jYDk** and **cgDZ**)’s concerns, we conducted an in-depth analysis of GRU dynamics within Slot Attention. Our analysis demonstrate that GRU dynamics serve as the key mechanism supporting both factor-wise consistency and continual task separation, providing the structural foundation for effective continual object-centric learning. \n\nA revised and extended version of this analysis has been added to **Appendix A.3 (red highlights)** in the updated manuscript, and we kindly ask the reviewers to refer to this section for the full details.\n\nGrounded in prior GRU studies showing that gating creates slow modes—hidden-state dimensions that remain stable due to consistently small update gates—these slow modes are understood as stable structural channels that preserve key semantic information over long horizons. The central question, is whether these recurrent slow-mode dynamics stabilize factor-wise homogeneity by acting as object-conditioned identical-mapping channels, and whether this same mechanism also accounts for the persistent task-level separation observed during continual adaptation.\n\nTo examine this connection, the update gate is used as an empirical indicator of each dimension’s stability, yielding a “slow-mode rank’’ that reflects how strongly each hidden dimension behaves as a slow mode. This ranking enables a direct test of factor-wise homogeneity by evaluating whether slots that share the same semantic factors rely on the same highly stable dimensions. A consistency analysis (Spearman's rank correlation $\\rho$) is then performed to examine whether these slow-mode dimensions align across slots (objects) that share the same semantic factors."}}, "id": "PJU5GByiJ8", "forum": "KVQSTYvC6f", "replyto": "KVQSTYvC6f", "signatures": ["ICLR.cc/2026/Conference/Submission14853/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14853/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14853/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763712828862, "cdate": 1763712828862, "tmdate": 1763712828862, "mdate": 1763712828862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the factor-wise homogeneity property of Slot Attention and its implications for Continual Object-Centric Learning (C-OCL). The authors introduce new benchmarks—Continual-Tetrominoes and Continual-CLEVR—to evaluate continual unsupervised object discovery, and propose a simple method termed Decoder-only Post Replay (DPR). DPR freezes the encoder and slot attention modules while fine-tuning only the decoder, thereby leveraging the observed factor-wise homogeneity to mitigate catastrophic forgetting. Extensive experiments, both quantitative and qualitative, demonstrate the stability of this property and the effectiveness of DPR across synthetic and real-world datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The identification and empirical characterisation of factor-wise homogeneity in Slot Attention is an original and conceptually interesting contribution. It provides a new perspective on how object-centric representations are organised in latent space.\n- The introduction of the Continual-Tetrominoes and Continual-CLEVR benchmarks represents a meaningful step toward systematic evaluation of continual object-centric learning. These resources could be of lasting value to the community.\n- The proposed Decoder-only Post Replay (DPR) is simple, elegant, and easy to reproduce. Its minimalistic design strengthens the argument that the observed behaviour of Slot Attention itself underpins the improvements.\n- The experiments are extensive and cover synthetic, complex, and real-world datasets. The authors further conduct ablations, comparisons with diverse baselines, and analysis of compatibility with regularisation-based methods.\n- The manuscript is well structured, with clear motivation, detailed explanations of methodology, and coherent visualisations that support the claims."}, "weaknesses": {"value": "- While the empirical findings are compelling, the paper lacks a rigorous theoretical analysis explaining why factor-wise homogeneity emerges in Slot Attention. The argument remains largely observational.\n-Although effective, the DPR method could be viewed as a minor algorithmic variation (a post-hoc replay scheme). The conceptual novelty mainly lies in the analysis, not in the proposed learning strategy.\n- The continual learning scenarios considered (mainly new shape classes) are somewhat narrow. Broader evaluations involving more complex semantic shifts (e.g., textures, dynamics, or object relationships) would better support general claims.\n-The improvements, while consistent, are moderate in some settings. It would strengthen the paper to include statistical significance tests or further qualitative explanations of the observed gains.\n-The paper could engage more deeply with recent object-centric continual learning frameworks or compositional representation studies (e.g., those integrating diffusion-based or transformer-based architectures).\n- Although the authors mention code availability in the supplement, hyperparameter and architectural details are partly deferred to the appendix, which may limit immediate reproducibility from the main text.\n-Some sections (especially Section 4) are lengthy and could benefit from tighter exposition. The key insights are occasionally obscured by repetition and detail overload."}, "questions": {"value": "plz see my detailed comments above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LcsTLkkVI7", "forum": "KVQSTYvC6f", "replyto": "KVQSTYvC6f", "signatures": ["ICLR.cc/2026/Conference/Submission14853/Reviewer_cgDZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14853/Reviewer_cgDZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926828853, "cdate": 1761926828853, "tmdate": 1762925206490, "mdate": 1762925206490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to tackle an important and underexplored problem: how to enable object-centric learning (OCL) models to perform continual learning (CL), i.e., to learn new object categories without catastrophically forgetting old ones. In summary, this work would make for a good workshop paper. It successfully defines a problem, establishes a benchmark, and provides a simple yet effective baseline. However, it lacks the \"eye-opening\" insight and technical depth, and is unlikely to have a profound methodological impact on the field. Therefore, I recommend rejecting this paper."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors observed a phenomenon they name \"Factor-wise Homogeneity.\" Specifically, after training, the latent representations (slots) of the classic OCL model, Slot Attention, spontaneously organize into compact and separated clusters, where each cluster corresponds to the same semantic factor (e.g., shape). More importantly, this separation property also holds for unseen object categories.\n\n2. Based on this discovery, the authors propose an extremely simple method, \"Decoder-only Post Replay\" (DPR). After learning a new task, this method freezes the encoder and the Slot Attention module (treating them as a stable generator of factor-wise homogeneous representations) and then fine-tunes only the decoder using a replay buffer containing both old and new samples.\n\n3. The authors propose the first benchmark for Continual Object-Centric Learning (C-OCL), including Continual-Tetrominoes and Continual-CLEVR, providing an evaluation platform for future research."}, "weaknesses": {"value": "1. The novelty of the core discovery (Factor-wise Homogeneity) is limited. This is the cornerstone of the paper, but its novelty is questionable. A well-trained representation learning model's fundamental goal is to map inputs of different semantics to separable regions in the latent space. The \"factor-wise homogeneity\" observed by the authors can largely be seen as an expected property that any successful representation learning model should possess, rather than a surprising, entirely new discovery. The authors' work feels more like naming and empirically verifying that Slot Attention has this desirable property, rather than unveiling a previously unknown mechanism. Therefore, packaging it as a core \"discovery\" seems like an overstatement.\n\n2. The technical solution (DPR) severely lacks novelty. This is the paper's most critical weakness. The DPR method can be seen as a simple combination of existing techniques: freezing the feature extractor, experience replay, and two-stage training are all standard procedures. Essentially, DPR just assembles these simple building blocks. While effective, it feels more like a clever engineering shortcut or a \"trick\" than a new algorithm with profound insights. It introduces no new theory, model architecture, or optimization objective.\n\n3. Although the authors' logic is that \"the simple DPR works precisely because of factor-wise homogeneity,\" this feels more like a post-hoc explanation for the effectiveness of a simple method, rather than the discovery itself inspiring a novel and ingenious solution.\n\n4. It is questionable whether the core idea proposed in this paper can lead the field. A truly inspiring work should excite other researchers and make them willing to explore new models and theories based on its core ideas. For instance, if the authors had proposed a new regularization term or model architecture to actively enhance this \"homogeneity\" instead of merely exploiting it, the paper's inspirational value would be much greater. The current DPR method feels more like an endpoint than a starting point."}, "questions": {"value": "1. You present \"factor-wise homogeneity\" as a core discovery. A critical question is: is this property unique to Slot Attention, or is it a general characteristic found in other mainstream object-centric learning (OCL) models (e.g., MONet, IODINE, SAVi)? If it is a common property, the novelty of this discovery is diminished. If it is unique to Slot Attention, can you provide a mechanistic explanation as to why its iterative attention and GRU updates specifically give rise to this phenomenon? The current ablation study shows the importance of these components but falls short of offering a fundamental explanation.\n\n2. The central claim of the paper is that DPR is effective precisely because of the existence of \"factor-wise homogeneity.\" This is a causal assertion that requires more direct evidence. Could you design an experiment to demonstrate this link more explicitly? For instance, what happens if you apply the DPR method to a model that does not exhibit this property (like the SlotMLP baseline in your experiments)? Does its performance collapse catastrophically? Conversely, if you were to disrupt the homogeneity in Slot Attention through some means, would DPR's effectiveness also fail? This would provide strong support for your central argument."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VnztXbEptX", "forum": "KVQSTYvC6f", "replyto": "KVQSTYvC6f", "signatures": ["ICLR.cc/2026/Conference/Submission14853/Reviewer_jYDk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14853/Reviewer_jYDk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972326971, "cdate": 1761972326971, "tmdate": 1762925206121, "mdate": 1762925206121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies an interesting and less explored topic about combining object-centric learning and continual learning. The idea of using Slot Attention’s factor-wise homogeneity for continual learning is quite novel. The proposed method is simple and seems effective. The experiments are a bit limited, but with more comprehensive studies, the work could be much stronger."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- originality: 4/5\n- quality: 4/5\n- clarity: 3/5\n- significance: 3/5"}, "weaknesses": {"value": "W1\n---\nSection 2 Related Work: Should review OCL literatures. \nEspecially various Slot Attention variants, e.g., BO-QSA, ISA and MetaSlot. \n\nIt is necessary to experiment or at least discuss the potential effects of different OCL decoders, as the spatial broadcast CNN decoder was proposed 5 years ago, while there are many advanced OCL decoders have been proposed:\n- auto-regressive-based: SLATE, using conditional Transformer as the decoder;\n- spatial broadcast-based: DINOSAUR, using MLP as the decoder (similar to CNN as the decoder, used in this paper, according to Line 138-139);\n- de-noising-based: SlotDiffusion, using conditional Diffusion model as the decoder.\nThe authors can either conduct the suggested experiments on these three OCL methods separately, or on a unified OCl method, VVO at once.\n\nIncluding such experiments or discussions could make this work more complete and more impactful to broader audiences.\n\n\nW2\n---\nTypo: \nIn Line 144, `where and M is the` should remove \"and\".\n\n\nW3\n---\nLine 191, the authors included SlotMLP, which is an earlier work of Slot Attention (both fairly old). However, there are many later works with great improvements, e.g., BO-QSA, ISA and MetaSlot, which should be included in the analysis.\n\n\nW4\n---\nLine 202, \n> Since the model has not yet encountered images from (E1) during training on (T0), this separation suggests that the behavior is primarily determined by the encoder and slot attention modules rather than influenced by the decoder.\n\nIn Section 4.3, DPR is described, i.e., freezing the encoder and slot attention. So DPR is not applied here, right?\n\nBesides, based on the well separation of E1/T0, how was that only the encoder and slot attention matters concluded? The logic is unclear.\n\n\nW5\n---\nLine 212.\n> inter-task representations For each slot,\n\nThere should be a \".\" before \"For\".\n\n\nW6\n---\nLine 215,\n> one highlights inter-task similarity, the other emphasizes within-task consistency\n\nAccording to your contexts, \"within-task\" should be \"intra-task\".\n\n\nW7\n---\nLine 287\n> preserve factor-wise homogeneous\nShould be \"homogeneity\".\n\n\nW8\n---\nLine 296-301,\n> DPR is based on two core components: (1) we freeze the encoder and slot attention module and only fine-tune the decoder, in order to maintain factor wise separated representations observed in Slot Attention space S; and (2) we introduce a Post Replay (PR) strategy, wherein the model is fine-tuned after the task training phase, i.e., training on the current task Tt without any continual learning methods, thus completely excluding sources of interference during initial slot representation learning.\n\nUnclear and repetitive writing. Please reoganize and polish it.\n\n\nThat said, I am willing to change my rating if my conerns are addressed.\n\n\nReference\n---\n- BO-QSA: Improving Unsupervised Object-centric Learning with Query Optimization\n- ISA: Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames\n- MetaSlot: Break Through the Fixed Number of Slots in Object-Centric Learning\n- SLATE: Illiterate DALL-E Learns to Compose\n- DINOSAUR: Bridging the Gap to Real-World Object-Centric Learning\n- SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models\n- VVO: Vector-Quantized Vision Foundation Models for Object-Centric Learning"}, "questions": {"value": "Please refer to the former section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XwwpDPi9Qz", "forum": "KVQSTYvC6f", "replyto": "KVQSTYvC6f", "signatures": ["ICLR.cc/2026/Conference/Submission14853/Reviewer_ZFMM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14853/Reviewer_ZFMM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762084192792, "cdate": 1762084192792, "tmdate": 1762925205335, "mdate": 1762925205335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}