{"id": "hVFoiCDiMB", "number": 748, "cdate": 1756816584680, "mdate": 1759898243846, "content": {"title": "Bridging Degradation Discrimination and Generation for Universal Image Restoration", "abstract": "Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model's capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality.", "tldr": "", "keywords": ["Degradation Discrimination", "Universal Image Restoration"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e4606ecbd39d5c1c5b4eb18bc9b8da31787d09b8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The method proposed in this paper follows a 'generate-then-restore' paradigm. Initially, a generative approach is applied to obtain a high-quality output. To compensate for potential deviations from the original image introduced during the generation process, two additional steps are employed. The first step uses degradation information to guide the generative model toward restoration, biasing the output toward the content of the input low-quality image; this degradation information is extracted using traditional operators, such as the gray-level co-occurrence matrix. The second step directly incorporates the low-quality image itself, further ensuring that the result follows the original content of the image."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper integrates fine-grained degradation information (MAS-GLCM features) with the diffusion generative prior, achieving both high-fidelity restoration and rich texture generation. Compared to traditional full-reference methods or purely generative approaches, it effectively balances fidelity and perceptual quality."}, "weaknesses": {"value": "1. The paper provides only six visual results, which is insufficient. Although some quantitative improvements are reported, improvements in low-level metrics do not necessarily correspond to better perceptual quality. Therefore, the current results are not sufficient to fully demonstrate the effectiveness of the proposed method; more visual examples are needed.\n\n2. MAS-GLCM is a hand-crafted feature, which may perform poorly on certain complex or previously unseen types of degradation. It may also be limited for non-grayscale or non-texture-related degradations, such as color shifts or geometric distortions.\n\n3. The training procedure is relatively complex, requiring a total of three stages.\n\n4. The three-stage training strategy combined with multiple loss functions adds additional complexity and requires careful hyperparameter tuning (e.g., the λ coefficient).\n\n5. Although the bridging and restoration fine-tuning stages improve fidelity, the generated content may still slightly deviate from the original image under extreme or composite degradation scenarios. Moreover, the performance on extreme or previously unseen real-world degradations remains unclear."}, "questions": {"value": "Please explain why there are not more visual results, and analyze the performance of the algorithm in real-world and diverse scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eayUmcK3h7", "forum": "hVFoiCDiMB", "replyto": "hVFoiCDiMB", "signatures": ["ICLR.cc/2026/Conference/Submission748/Reviewer_JVFR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission748/Reviewer_JVFR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835949221, "cdate": 1761835949221, "tmdate": 1762915597293, "mdate": 1762915597293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BDG (Bridging Degradation Discrimination and Generation) — a unified diffusion-based framework for universal image restoration. The core idea is to explicitly bridge degradation discrimination (what type and level of corruption an image has) with image generation priors (rich texture synthesis ability of diffusion models)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The unified formulation connecting discrimination and generation is interesting. \n+ Achieving quantitative and qualitative improvements over recent SOTA (DiffUIR, DCPT)."}, "weaknesses": {"value": "1.  Though losses and stages are ablated, there is no comparison of MAS-GLCM vs. simpler features (e.g., gradients, frequency) in restoration quality.\n\n2.  The paper does not report details about computational cost (params / memory overhead / time) of MAS-GLCM.\n\n3.  The number of scales × angles used ($l$, $\\theta$) and preprocessing details are not discussed.\n\n4.  Questions about MAS-GLCM :\n\n    (1)  Although the authors claim MAS-GLCM is “minimally affected by image content,” GLCM is a gray-level co-occurrence matrix and inherently relies on statistics of gray-level pairs. If an image contains prominent structures, repetitive textures, or skewed illumination, its gray-level relations will directly shape the GLCM pattern. For example, under the same blur level, a highly textured image (e.g., forests, buildings) and a low-texture image (e.g., sky, walls) can yield markedly different MAS-GLCM responses, creating representation variance for the same degradation strength.\n\n    (2)  MAS-GLCM shows some clustering ability in t-SNE, but when facing complex, composite degradation distributions GLCM has no built-in disentanglement mechanism. It primarily captures shifts in overall texture statistics, making it hard to attribute the change to a specific degradation type and unable to precisely estimate the mixture proportions/levels of multiple degradations. Could this be a reason why the method is less advantageous in real-world scenarios?\n\n    (3)  In contrast, degradation tokens, prompt embeddings, or degradation-aware latent vectors extracted by deep models offer stronger context awareness and semantic separation, leading to more reliable identification of both degradation type and severity.\n\n    (4)  MAS-GLCM also appears sensitive to image resolution. Would applying the same degradation type to images of different resolutions alter this descriptor in ways that hurt its generalization ability?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PwcCsbEEY4", "forum": "hVFoiCDiMB", "replyto": "hVFoiCDiMB", "signatures": ["ICLR.cc/2026/Conference/Submission748/Reviewer_RoK7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission748/Reviewer_RoK7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959656712, "cdate": 1761959656712, "tmdate": 1762915597174, "mdate": 1762915597174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new framework, BDG, for universal image restoration. It aims to bridge the gap between high-fidelity restoration and high-perceptual-quality generation, a key challenge in the field. The core idea is twofold: first, it introduces MAS-GLCM, a classical texture descriptor, to achieve fine-grained, content-agnostic degradation discrimination. Second, it proposes a three-stage diffusion model training strategy (generation, bridging, and restoration) to inject this discriminative information into a generative prior, balancing fidelity and texture generation. The method demonstrates strong quantitative results across all-in-one, mixed-degradation, and real-world SR tasks, notably improving fidelity (PSNR) in diffusion-based SR ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper achieves impressive empirical results, particularly in reconciling high fidelity metrics (PSNR) with the strong perceptual quality of diffusion models in real-world super-resolution ."}, "weaknesses": {"value": "My primary concern lies with the methodological innovation. The core degradation descriptor, MAS-GLCM , is a classical, handcrafted feature descriptor. While its effectiveness in separating degradations is well-demonstrated (Fig. 1), this approach feels like a step back from end-to-end deep feature learning. The system relies on this external, non-learned feature extractor, which is then 'aligned' with the diffusion U-Net's features. This raises questions about whether a more integrated, end-to-end learned degradation representation would be more powerful and elegant.\n\nSecond, the proposed three-stage training pipeline  introduces significant complexity and computational overhead. The model requires a full 300k iterations split across two distinct fine-tuning stages (bridging and restoration) after the initial generation pre-training. It is unclear if this multi-stage approach is strictly necessary or if a more unified training scheme could achieve similar results. The ablation in Table 5 shows that both stages are needed for this design, but it doesn't justify the design's inherent complexity versus a simpler alternative.\n\nThird, the reliance on explicit degradation classification  or the 'order classification' 10101010) for supervising the MAS-GLCM encoder is a potential bottleneck for real-world generalization. This framing assumes that degradations can be categorized into discrete classes or sequential orders. It is questionable how this system would perform on truly unseen or novel real-world degradations that fall outside the training distribution of these discrete labels. The ablation study (Table 7) shows the model catastrophically fails without $\\mathcal{L}_{deg-cls}$ , which suggests the model is overly-reliant on this explicit supervision and may lack the robustness to generalize to unclassified degradation types.\n\nFinally, while the real-world SR results in Table 4 are strong on fidelity, the model was still trained on synthesized degradation pairs. The zero-shot results in Table 2 are promising, but the fundamental reliance on a classification loss trained on synthetic data types (haze, snow, etc.)  may limit its effectiveness on real-world images where degradations are far more complex and varied than the training classes."}, "questions": {"value": "**Questions**\n1.  Given the critical failure when $\\mathcal{L}_{deg-cls}$ is removed, how do the authors envision this model handling a completely novel degradation type not seen during training? Does the 'full negative contrastive learning' ($\\mathcal{L}_{fcnl}$)  used in the RFT stage help mitigate this, and if so, why not use it in the bridging stage?\n\n2. Could the authors comment on the computational cost and training time of the three-stage training  versus a potential joint, end-to-end approach?\n\n3. Why was a classical, handcrafted feature (GLCM)  chosen over a learnable feature extractor for degradation discrimination, which could potentially be trained end-to-end with the alignment loss   alone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GOun6irvp4", "forum": "hVFoiCDiMB", "replyto": "hVFoiCDiMB", "signatures": ["ICLR.cc/2026/Conference/Submission748/Reviewer_ieqX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission748/Reviewer_ieqX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007965725, "cdate": 1762007965725, "tmdate": 1762915597004, "mdate": 1762915597004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BDG (Bridging Degradation Discrimination and Generation), a novel framework for universal image restoration that tackles the dual challenge of identifying diverse image degradations and generating high-quality restorations. To address this, BDG integrates a fine-grained degradation characterization method called Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM), which outperforms prior techniques in distinguishing degradation types and levels. BDG’s training is structured into three stages: generation (to capture rich textures via diffusion), bridging (to align MAS-GLCM features with diffusion features), and restoration (to fine-tune fidelity while preserving discriminative capacity). This alignment enables the model to adaptively restore images across varied degradation scenarios without altering the underlying architecture."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. BDG introduces the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM), which significantly improves the model’s ability to distinguish between various degradation types and levels. This enables more precise restoration tailored to the input image’s condition.\n2. By aligning MAS-GLCM features with diffusion model features during a dedicated bridging stage, BDG effectively combines degradation awareness with generative priors. This fusion allows the model to retain rich texture generation while improving fidelity.\n3. BDG achieves substantial performance gains without modifying the underlying network architecture. This makes it compatible with existing diffusion-based models and easy to adopt across diverse restoration tasks."}, "weaknesses": {"value": "1. While MAS-GLCM is presented as a novel degradation discriminator, it is fundamentally an extension of the classical Gray Level Co-occurrence Matrix (GLCM), which has been widely used in texture analysis for decades. To strengthen the novelty claim, the authors should compare MAS-GLCM with frequency-aware representations and learned degradation embeddings (e.g., from PromptIR or DCPT) or  (e.g., Ji et al., 2021). Ablation studies showing MAS-GLCM’s superiority over these alternatives would help.\n2. The paper emphasizes fidelity (e.g., PSNR, SSIM) but does not provide sufficient perceptual quality metrics (e.g., LPIPS, NIQE, FID), especially for real-world super-resolution where perceptual realism is critical."}, "questions": {"value": "1. The paper proposes “order classification” for real-world degradation modeling. I'm curious how effective this trick is and hope the authors can provide a ablation study or justification for it.\n2. The paper mentions \"full negative contrastive learning\" in the RFT stage without defined what are negative or positive samples."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Kxl1EhR4F", "forum": "hVFoiCDiMB", "replyto": "hVFoiCDiMB", "signatures": ["ICLR.cc/2026/Conference/Submission748/Reviewer_wiEb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission748/Reviewer_wiEb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762087646430, "cdate": 1762087646430, "tmdate": 1762915596892, "mdate": 1762915596892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}