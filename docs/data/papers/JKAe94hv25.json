{"id": "JKAe94hv25", "number": 18210, "cdate": 1758285181914, "mdate": 1759897119909, "content": {"title": "PRISM: Pareto-Responsive Iterative Sampling with DPO for Multi-objective Planning", "abstract": "Many planning-style applications of large language models are inherently multi-objective. Beyond correctness, users care about efficiency and the avoidance of irrelevant or unsafe actions. Yet most alignment pipelines optimize a single scalar reward, which hides trade-offs and offers little control when secondary objectives have uncertain or deployment-specific weights. We present PRISM, a Pareto responsive framework that integrates Direct Preference Optimization. PRISM adds three components designed for offline, several convergence toward balanced solutions. First, it uses golden comparisons that isolate per-objective preferences. Second, it computes attention-style weights from deficiency diagnostics that combine loss and gradient information. Third, it applies Pareto guided sampling that orients preference pairs by cosine alignment with the current weight direction.This loop performs common-descent updates for a vector of objective deficiencies and stops at a certificate of first-order Pareto stationarity. It removes the need for online reinforcement learning, reward sweeps, or families of specialist models. On six benchmarks in question answering, coding, and mathematical reasoning, PRISM improves accuracy over strong baselines while simultaneously reducing latency and step count and driving off-domain actions to near zero. PRISM provides a principled and compute efficient recipe for robust multi-objective alignment of LLM-based planners.", "tldr": "PRISM makes LLM planners accurate, efficient, and robust by aligning them with multiple objectives in a single training pass.", "keywords": ["DPO", "Multi-Objective Optimization", "Planner"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5398a4ac2ff0b678cf01e401f216eb7d96eac9fb.pdf", "supplementary_material": "/attachment/22b92500f9efd886fa04805928e9cef18bc5a15f.pdf"}, "replies": [{"content": {"summary": {"value": "The authors propose PRISM, an offline preference-based fine-tuning framework that finds a balanced policy accounting for several objectives simultaneously, without requiring multiple specialized models or explicit weight tuning by the user. PRISM builds on Direct Preference Optimization (DPO) but extends it to the multi-objective setting in a principled way. \n\n(1) It generates golden comparison pairs that isolate individual objectives – specifically, they collect pairs of model outputs where one is clearly better on one objective but roughly equal on others. These serve as clean training signals for each criterion (e.g. a pair where solution A is correct but longer vs solution B is incorrect but shorter isolates the accuracy objective). \n\n(2) The framework computes a dynamic weighting over objectives using deficiency diagnostics. For each objective, it measures how often the model prefers the worse outcome in those golden comparisons and also how large the gradient updates are for that objective. Objectives in which the model is performing poorly and finds hard to improve (high loss and high gradient norm) are assigned higher weights. This is done via an attention-like softmax weighting of objectives, which continuously updates during training – essentially telling the model “focus more on objectives you’re currently deficient in.” \n\n(3) PRISM employs Pareto-guided sampling of preference pairs: when sampling training comparisons from a large pool, it biases selection toward those that align well with the current weight vector direction in objective space. It uses a cosine similarity criterion to pick or orient each preference pair so that the chosen “better” answer is the one that moves the model’s policy in the direction that improves the weighted combination of objectives. This ensures each update step is pushing the model toward a Pareto-optimal trade-off rather than oscillating or favoring one objective at the extreme. The training loop iteratively fine-tunes the model with these weighted, oriented preferences and stops when no further reweighting can simultaneously improve all objectives (reaching a first-order Pareto stationary point)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Empirical results on six benchmarks (spanning question answering, coding tasks, and mathematical reasoning) show that PRISM can significantly improve the primary metric (accuracy or success rate) while also reducing secondary costs like the number of steps, execution latency, and the incidence of off-domain or unsafe actions. For example, compared to strong baselines (including single-reward optimized models), PRISM achieves higher solution accuracy and drives undesirable behaviors (such as irrelevant tool calls or hallucinations) nearly to zero, all in a single fine-tuned model. \n\nIt thereby produces a set of policies along the effective frontier of the trade-off curve without needing multiple models or online RL. \n\nThe framework is novel in that it provides a general, data-driven way to balance objectives: the deficiency-based weighting is an innovative idea to adapt training focus, and the use of golden pairs plus vectorized updates steers the model towards a well-balanced solution. A noteworthy aspect is that PRISM avoids the inefficiency of previous multi-objective approaches that required sweeping through reward weightings or training conditional policies – instead, it finds one policy that is inherently balanced."}, "weaknesses": {"value": "One potential limitation is the complexity of generating and evaluating the comparison data: the approach assumes access to a reward model or evaluators for each objective and a procedure to produce diverse candidate outputs, which could be non-trivial for new domains. I hope to hear a extensive discussions about this matter.\n\nas with any multi-objective tuning, the final trade-off point might reflect the particular choice of deficiency weights and stopping criteria, which might need calibration for different applications.\n\nAlso, please provide more detailed environment on how GPU hours has been logged.\n\nI am willing to increase the score if questions above are well treated."}, "questions": {"value": "Discussed in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BTobOO0bBt", "forum": "JKAe94hv25", "replyto": "JKAe94hv25", "signatures": ["ICLR.cc/2026/Conference/Submission18210/Reviewer_6m6j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18210/Reviewer_6m6j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636885694, "cdate": 1761636885694, "tmdate": 1762927954833, "mdate": 1762927954833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "As human preference should be managed considering the multiple aspect, multi objective DPO has been studied as one of prominent researches recently. This paper propose pareto responsive iterative sampling with DPO, or PRISM, to align multi objective preference for LLM finetuning. PRISM suggests (1) plan generation and golden comparisons by considering the differences between plans, (2) deficiency-based adaptive weighting, which is a normalized softmax value from loss and gradient and (3) Pareto-guided training and adaptive sampling to jointly optimize multiple objectives.\nThe method avoids reinforcement learning and multiple specialized models, instead converging to an approximate Pareto-stationary solution through iterative preference optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The deficiency-based adaptive weighting and sampling may offer a principled mechanism for balancing competing objectives.\n- consistent gains in accuracy."}, "weaknesses": {"value": "- The method is too complicated to achieve the objective, meaning not knowing which treatment did what. There is also a possibility of conflict between objectives. I checked table3 or ablation study, but only performance does not fully explain why each treatment contributed to model performance in that way.\n- So many hyperparameters are additionally required, - \\gamma, \\epsilon, and \\tau.\n- It reguires the gradient norm, which cause another computation complexity.\n- The paper does not test whether adjusting objective weights \\w leads to predictable trade-offs among objectives."}, "questions": {"value": "- It seems golden comparison (or pair selection for each objective) is so important to provide clean feedback. However, how can authors be sure the selected samples provide exactly true learning signal? \n- Pareto optimization for multi objective DPO is not a novel. What is the contribution of authors in the viewpoint of the optimization?\n- One of the problems of multi objective DPO is that each objective is not fully independent, although each objective is modeled as independent. I suppose this gap would surely affect sample selection per each objective and weighting process. How the authors think about it? or can it be solved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zAbcgwCHCl", "forum": "JKAe94hv25", "replyto": "JKAe94hv25", "signatures": ["ICLR.cc/2026/Conference/Submission18210/Reviewer_CMX3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18210/Reviewer_CMX3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752899120, "cdate": 1761752899120, "tmdate": 1762927954231, "mdate": 1762927954231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Pareto-responsive framework for multi-objective planning in direct preference optimization (DPO) of large language models. The method identifies golden pairs, sample pairs where one objective improves with remaining others, and uses their DPO loss values and gradient norms to derive preference signals represented as weights on a simplex. These weights are then used to adjust the sampling probabilities of training examples, enabling adaptive multi-objective DPO updates."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Most LLM alignment methods rely on a single scalar reward, so addressing multi-objective optimization is an important and meaningful direction.\n* Employing a Pareto-based approach to handle multi-objective trade-offs is a reasonable and well-motivated choice."}, "weaknesses": {"value": "* The paper assumes that, among multiple rewards, $O_1$ serves as the primary objective and $O_n$ measures a hard constraint. It is unclear whether such an assumption is necessary. In particular, hard constraints may not always be representable as a single scalar, which could limit the generality of the framework.\n\n* When defining golden pairs, the paper requires that non-target objectives remain approximately unchanged. A deeper discussion would be helpful on whether this condition is important. For example, why are pairs that improve multiple objectives simultaneously excluded from being considered golden pairs?\n\n* It is unclear whether golden pairs are expected to be evenly distributed across all objectives. If so, obtaining them for each objective would require separate sampling efforts. Does the method repeatedly sample until a certain balance or target ratio is reached?\n\n* The DPO loss in Section 3.2 differs from the original formulation in Rafailov et al. (2023), for example, by omitting the reference policy term. It is unclear whether this is a notational simplification or a methodological change. If it is the latter, the paper should explicitly justify and discuss the implications of this modification.\n\n* In Section 3.2, the method derives preference signals using the weighted sum of the DPO loss value and gradient norm. The rationale for this choice should be elaborated, both theoretically and empirically. In particular, a sensitivity analysis on the parameter $\\gamma$ would be valuable to assess robustness.\n\n* The paper introduces several hyperparameters, e.g., $\\Delta_{min}^i, \\delta_j, \\gamma, \\lambda, \\tau, \\beta$, but does not specify their values and provide any analysis regrading their effects on performance.\n\n* The writing could be improved for clarity. For example, Section 3.3 presents a sequence of equations without clear statements highlighting the key claims. It would be helpful to include concise statements (e.g., Theorem, Proposition, ...), and algorithmic description outlining the overall PRISM method.\n\n* Although multiple models and datasets are used in the experiments, their sources and configurations are not properly referenced and described.\n\n* Figure 2 in Section 4 is difficult to interpret, and the explanation in Section 4.4 does not clearly convey the intended meaning. A more detailed and accessible description is needed to help readers understand the figure's significance."}, "questions": {"value": "Please provide the response on the points in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lVDuKNe0E0", "forum": "JKAe94hv25", "replyto": "JKAe94hv25", "signatures": ["ICLR.cc/2026/Conference/Submission18210/Reviewer_uA9B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18210/Reviewer_uA9B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877034755, "cdate": 1761877034755, "tmdate": 1762927953730, "mdate": 1762927953730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework named PRISM, a preference fine-tuning framework that jointly improves accuracy, efficiency, and error avoidance. It also introduces deficiency-aware weighting and Pareto Pareto-guided sampling mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed idea seems to be novel and addresses important issues in multi-objective planning."}, "weaknesses": {"value": "The paper is readable but not reader-friendly. Some questions remain to be answered. Please see the questions."}, "questions": {"value": "**Questions**\n\nQ1. Important related work Panacea [1] needs to be introduced and compared with the proposed method in terms of multi-objective optimization.\n[1] Zhong, Yifan, et al. \"Panacea: Pareto alignment via preference adaptation for llms.\" Advances in Neural Information Processing Systems 37 (2024): 75522-75558.\n\nQ2. Are composite score, efficiency and error-avoidance only aspects in multi-objective planning? If not, what other aspects need to be considered besides them?\n\nQ3. What happens when there is no pair of plans satisfying the golden comparisons among the initially generated plans?\n\nQ4. It is better to use the same notation or the same representation in Section 3.1 and Figure 1. The value r is suddenly introduced in Figure 1, while Section 3.1 explains the reward function $O$. $r$ in r represents the same $r$ in Section 3.2?\n\nQ5. Scalability with respect to $n$ is questionable. \n\nQ6. How to select $\\gamma$ and $\\beta$? Is the proposed method robust to such hyperparameters? \n\nQ7. The proposed method includes several additional components, including additional gradient and loss computations. Therefore, the proper computational cost analysis should be discussed. \n\nQ8. The paper does not use Equation numbers and repeatedly defines the same ones, such as $\\bf{a}$ and $\\bf{w}$, in Sections 3.2 and 3.3. Why not use an equation number and refer to it to avoid confusion?\n\nQ9. In Section 3.3, explicit expression $\\nabla _{\\theta}$ instead of $\\nabla$ would be helpful.\n\nQ10. Do the authors intend to present their code? \n\nQ11. Why not elaborate on the last equation in Section 3.3 for the sake of readers at least in the Appendix?\n\n**Minor Comments**\n\nC1. In Figure 2, a text description overlapped with a dashed line and reducing the readability.\n\nC2. Learning rate $\\eta$ is not explicitly defined."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tnwssBjkR3", "forum": "JKAe94hv25", "replyto": "JKAe94hv25", "signatures": ["ICLR.cc/2026/Conference/Submission18210/Reviewer_ycrn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18210/Reviewer_ycrn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18210/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967097595, "cdate": 1761967097595, "tmdate": 1762927953391, "mdate": 1762927953391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}