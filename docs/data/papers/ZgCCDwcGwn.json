{"id": "ZgCCDwcGwn", "number": 20092, "cdate": 1758302387146, "mdate": 1763653374329, "content": {"title": "AgentGym-RL: An Open-Source Framework to Train LLM Agents for Long-Horizon Decision Making via Multi-Turn RL", "abstract": "Training LLM agents for complex multi-turn decision-making tasks requires extensive exploration within their environment, with reinforcement learning (RL) as a natural way. However, the open-source community currently lacks a unified RL framework capable of training agents from scratch across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a modular and decoupled framework specifically designed for RL-based agent in multi-turn decision-making tasks. It offers high flexibility and extensibility, supports mainstream RL algorithms, and spans a broad range of real-world scenarios. To effectively train agents for challenging tasks, we argue that they are required to expand external interactions with the environment, rather than relying solely on internal reasoning. Nevertheless, training agents for long-horizon interaction with vanilla methods often faces challenges like training instability. To this end, we propose ScalingInter-RL, a staged training approach for stable long-horizon RL training. It starts with short-horizon interaction to establish foundational policies and progressively expands them to encourage deeper exploration. Extensive experiments show that agents trained with our method achieve performance on par with—or even surpass—commercial counterparts like OpenAI o3 and Gemini-2.5-Pro across 27 tasks in diverse environments. We share key insights and will release the full framework, including code and datasets, to empower the community in building the next generation of intelligent agents.", "tldr": "We present AgentGym-RL, a unified open-source framework for training LLM agents from scratch across diverse and realistic environments, and propose ScalingInter-RL, a staged training strategy for stable long-horizon RL training.", "keywords": ["large language model", "LLM-based agent", "decision-making"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/989ed1d8bc17bb985bbcc28a0ea935e08f43da19.pdf", "supplementary_material": "/attachment/2703797971b8f1b6b9461e5329fae070de042b37.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents AgentGym-RL, an open-source framework for training LLM agents via RL in multi-turn decision-making tasks. The main contributions include: a modular framework supporting diverse environments and mainstream RL algorithms, and a staged training approach named ScalingInter-RL that progressively increases interaction horizons to achieve stable long-horizon RL training. The authors demonstrate that 7B models trained with their method achieve performance comparable to or surpassing commercial models like OpenAI o3 and Gemini-2.5-Pro across 27 tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear and meaningful. The paper tackles the genuine problem of lacking unified RL frameworks for training LLM agents across diverse environments, which is important for the community.\n2. As an open-source framework, this work provides good engineering with modular design, supporting multiple RL algorithm for diverse environments as well as various practical features.\n3. The framework demonstrates strong empirical performance. It achieves performance matching or surpassing much larger commercial models with 7B parameters, which is impressive, especially with the 33.65 point average improvement.\n4. The evaluation is comprehensive, which tests across 5 scenarios with 27 tasks. Such evaluation provides good coverage of different agent capabilities."}, "weaknesses": {"value": "1. The algorithmic novelty is limited, although the effectiveness is acknowledged. ScalingInter-RL is essentially curriculum learning applied to interaction horizons. This is a straightforward adaptation rather than a novel algorithmic contribution. The progressive scaling from short to long horizons is intuitive and incremental, which lacks theoretical justification.\n2. Given that the key to ScalingInter-RL is the curriculum scaling of interaction turns, there is no ablation on the curriculum schedule. Why these specific horizon increments?\n3. I could not find quantitative report and analysis of computational costs or training efficiency."}, "questions": {"value": "1. Could the authors provide a curriculum sensitivity study? What happens with different progression rates or starting horizons of interactions? This seems critical but is unexplored.\n2. What are the computational costs? The paper doesn't discuss training time, GPU hours, or cost comparisons with baseline approaches, making it hard to assess practical viability.\n3. Are there separate studies investigating whether the improvement mainly comes from the framework or from the ScalingInter-RL method? Would ScalingInter-RL work in other frameworks? Would the framework benefit from other training approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xjNuHXrrWO", "forum": "ZgCCDwcGwn", "replyto": "ZgCCDwcGwn", "signatures": ["ICLR.cc/2026/Conference/Submission20092/Reviewer_yP1T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20092/Reviewer_yP1T"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495140498, "cdate": 1761495140498, "tmdate": 1762932988171, "mdate": 1762932988171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AgentGym-RL, a modular open-source framework for training Large Language Model (LLM) agents on complex, multi-turn decision-making tasks using Reinforcement Learning (RL). The authors identify a key gap in the current open-source community: the lack of a unified platform for training agents from scratch in diverse and realistic environments. AgentGym-RL addresses this by providing a decoupled architecture—comprising environment, agent, and training modules—that supports various scenarios (e.g., WebArena, Deep Search, SciWorld) and mainstream RL algorithms.\nTo tackle the training instability common in long-horizon tasks, the paper proposes a curriculum learning-based strategy named ScalingInter-RL. This method begins training with shorter interaction episodes to establish a foundational policy and then progressively increases the episode length. This approach enables stable training and deeper exploration. Extensive experiments across 27 tasks demonstrate that a 7B parameter model trained with this method can match or even surpass the performance of much larger proprietary models like GPT-4 and Gemini Pro, suggesting that scaling interaction can be more effective than simply scaling model parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The development of a unified, open-source RL framework for LLM agents is a valuable contribution to the research community. It directly addresses a practical need and has the potential to significantly lower the barrier to entry for agent research, thereby fostering reproducibility and continued innovation.\n2. ScalingInter-RL is an intuitive yet powerful method that effectively stabilizes long-horizon RL training. The paper provides a clear motivation for this curriculum-based strategy (as illustrated in Figure 4), showing how it adeptly balances the trade-off between exploration depth and training stability. Its effectiveness is validated by solid experimental results.\n3. The experimental design is a major highlight of this work. The evaluation spans five diverse and challenging environments and comprehensively compares the proposed model against a wide range of strong open-source and state-of-the-art proprietary models. This thorough evaluation lends significant credibility to the paper's conclusions."}, "weaknesses": {"value": "1. Limited Analysis of the ScalingInter-RL Schedule: While the method is shown to be effective, the paper offers limited analysis of the curriculum schedule itself. The model's final performance is likely sensitive to hyperparameters such as the initial number of interaction steps, the increment size (δh), and the frequency of phase transitions (Δ). A sensitivity analysis or ablation study on these hyperparameters would provide deeper insights into the method's robustness and offer guidance for its application in new domains, thereby strengthening the paper's claims.\n2. Rigor of the Exploration-Exploitation Argument: The paper states that by initially limiting the interaction horizon, the agent focuses more on exploitation, and as the horizon expands, it is encouraged to explore more deeply. This description could be more rigorous. Even in the early stages, exploration is necessary to discover better trajectories for policy improvement. It would be beneficial to contrast this with classic exploration techniques in RL, such as curiosity-driven methods like RND or NGU, to better contextualize the exploration-exploitation trade-off within ScalingInter-RL."}, "questions": {"value": "1. Sensitivity of ScalingInter-RL: Could the authors provide more details on the robustness of the ScalingInter-RL method? For instance, how does performance vary with different curriculum schedules (e.g., faster/slower expansion of the interaction horizon, different starting points)? A related ablation study would be highly informative.\n2. Instability in Long-Horizon Training: Could you elaborate on why training is inherently more unstable in long-horizon settings? Providing related experimental or theoretical analysis would be a valuable contribution to the community.\n3. Overhead of HTTP Communication: Is there an analysis of the time overhead associated with the environment client communicating with the environment server via HTTP? A comparison with a fully collocated setup would be very insightful.\n4. Interaction with Different RL Algorithms: The paper compares GRPO and REINFORCE++, but the main benefits of ScalingInter-RL appear to be demonstrated with GRPO. It would be beneficial to know if the stability and performance gains from ScalingInter-RL are consistent across other RL algorithms, such as PPO. This would help confirm whether the method is a general strategy for stabilizing long-horizon training, rather than a technique tightly coupled with the specific mechanics of GRPO.\n5. Computational Overhead: How do the training time and computational costs of ScalingInter-RL compare to a baseline with a fixed, long horizon? By avoiding unstable training phases, does this staged approach lead to more efficient use of computational resources, potentially achieving a high-performance policy with faster overall convergence?\n6. Comparative Failure Case Analysis: The case studies in Appendix F are very helpful. It would be even more compelling to see a direct comparison of failure modes on a complex task between a baseline agent (trained with a fixed short horizon) and the ScalingInter-RL agent. Does ScalingInter-RL specifically help mitigate certain types of errors, such as repetitive loops or failure to explore deeper parts of the state space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wIhhAWPict", "forum": "ZgCCDwcGwn", "replyto": "ZgCCDwcGwn", "signatures": ["ICLR.cc/2026/Conference/Submission20092/Reviewer_gQCC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20092/Reviewer_gQCC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761538769403, "cdate": 1761538769403, "tmdate": 1762932987683, "mdate": 1762932987683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AgentGym-RL, a novel and unified reinforcement learning framework designed for training LLM agents to perform long-horizon, multi-turn interactive decision-making. To address the challenge of balancing exploration and exploitation and to ensure stable optimization, the authors propose ScalingInter-RL, a method that progressively scales the agent-environment interaction horizon during training. Extensive experiments demonstrate that the framework and method deliver significant and consistent performance gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- 1.The introduction of AgentGym-RL, the first modular, flexible, and end-to-end reinforcement learning framework specifically designed for training LLM agents in multi-turn, long-horizon decision-making across diverse real-world environments, filling a critical gap in the field.\n\n- 2.The proposed ScalingInter-RL method progressively increases the interaction horizon during training. This innovative approach effectively balances exploration and exploitation, leading to more stable RL optimization and enabling agents to develop richer behaviors and strategies.\n\n- 3.Extensive experiments demonstrate that the framework and method significantly enhance the capabilities of open-source models."}, "weaknesses": {"value": "- 1. The proposed framework is built upon existing open-source projects like AgentGym and veRL. Its main contributions are engineering-oriented—adding diverse environments, integrating existing RL algorithms, and improving scalability and stability. These engineering improvements are certainly valuable and can significantly enhance usability and robustness; however, from a research perspective, the work represents a continuation and consolidation of existing efforts rather than a fundamentally novel contribution.\n\n- 2. The ScalingInter-RL method lacks detailed specification on how initial-stage learning is concretely guided. Crucially, it omits the design of reward functions or subtasks for short-horizon phases, making it unclear how the agent acquires foundational skills before scaling up exploration.\n\n- 3. The demonstrated success is more pronounced in structured, simulated environments with clear rules, while improvements in noisy, real-world settings are modest. This suggests the framework's effectiveness may be closely tied to environments that align well with its curriculum-based scaling approach."}, "questions": {"value": "1. **On the specifics of early-stage curriculum learning**: The ScalingInter-RL method emphasizes exploitation by initially restricting interaction turns. Could you detail how the agent's learning is concretely guided during these short-horizon phases? Specifically, were task-specific dense reward functions or explicit subtasks designed to ensure the acquisition of foundational skills, rather than just learning to avoid quick failure? Furthermore, was this design implemented through a universal framework applicable across all environments, or was it individually tailored for each distinct task type?\n\n- 2. **On generalization to more challenging RL tasks**: The experiments demonstrate strong performance in structured, language-mediated environments. How might the AgentGym-RL framework and the ScalingInter-RL approach be adapted to classic RL challenges that involve high-dimensional continuous state-action spaces?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9QFQkCY2rj", "forum": "ZgCCDwcGwn", "replyto": "ZgCCDwcGwn", "signatures": ["ICLR.cc/2026/Conference/Submission20092/Reviewer_odrg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20092/Reviewer_odrg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806794107, "cdate": 1761806794107, "tmdate": 1762932987001, "mdate": 1762932987001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a new framework for multi-turn LLM RL and a multi-staged training approach for long-horizon RL by progressively scaling interaction lengths. The results are strong relative to baselines, often outperforming much larger models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work tackles challenging LLM RL tasks with real environment interactions, which is underexplored relative to the more common math reasoning. The proposed framework would be very useful for the LLM RL community.\n- ScalingInter-RL is well-motivated and a natural solution to the training instability of long-horizon RL\n- The results are impressive and demonstrate the strength of the proposed method/framework over strong baselines."}, "weaknesses": {"value": "- The text in some of the figures is small relative to the main text (Figure 1, 5, 6, 7)"}, "questions": {"value": "- Were any alternatives considered to the current linear schedule for the maximum number of interactions at each training step? In particular, I'd be curious to hear if this can be dynamically adapted (for instance allowing different RL rollouts to have slightly different limits and seeing which ones perform best)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2GbF6682ae", "forum": "ZgCCDwcGwn", "replyto": "ZgCCDwcGwn", "signatures": ["ICLR.cc/2026/Conference/Submission20092/Reviewer_M63s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20092/Reviewer_M63s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943553984, "cdate": 1761943553984, "tmdate": 1762932986479, "mdate": 1762932986479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response. Part [3/3]"}, "comment": {"value": "### #6 Experiments of applying ScalingInter-RL to other frameworks.\n\nFollowing Reviewer yP1T’s suggestion, we conducted experiments in another framework. Specifically, we adopted the RAGEN framework [10] and implemented the BabyAI task. The experimental results are shown in the table below. **We find that ScalingInter-RL performs well across different frameworks.**\n\n| Method                | Performance |\n| --------------------- | ----------- |\n| Base Model            | 69.7        |\n| RAGEN-RL              | 82.5        |\n| RAGEN+ScalingInter-RL | 88.4        |\n\n### #7 Analysis of HTTP interaction overhead.\n\nFollowing the suggestion of Reviewer gQCC, we analyzed the HTTP network communication overhead in our framework, i.e., the difference between deploying the environment and the model on different machines versus on the same machine, and we measured the proportion of communication time per round relative to the total rollout time. **As shown in the table below, the ratio of HTTP communication time to rollout time is extremely small, indicating that the time cost of HTTP communication does not affect the overall training efficiency.** Moreover, considering the benefits of HTTP communication (flexibility, scalability, etc.), we regard this as a favorable design choice.\n\n| Environment | HTTP Communication Time | Total Rollout Time | Percentage |\n| ----------- | ----------------------- | ------------------ | ---------- |\n| TextCraft   | 0.00436 s               | 0.355 s            | 1.22%      |\n| BabyAI      | 0.00191 s               | 0.203 s            | 0.94%      |\n| SciWorld    | 0.00278 s               | 0.192 s            | 1.44%      |\n\n### #8 Analysis of computational resources and efficiency.\n\nFollowing the suggestions of Reviewers gQCC and yP1T, we analyzed the efficiency of ScalingInter-RL by examining the training reward, the cumulative number of interaction rounds during training, and the total training time. As shown in  **the Figure 10 in the revised manuscript** , we can observe that, thanks to the stage-based design, **ScalingInter-RL is able to achieve relatively high rewards with comparatively high efficiency and reduced time and resource consumption.**\n\n[1] REINFORCE++: Stabilizing Critic-Free Policy Optimization with Global Normalization\n\n[2] DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n\n[3] https://miromind.ai/blog/miromind-research-agent\n\n[4] WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization\n\n[5] Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training\n\n[6] DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments\n\n[7] GAIA: A BENCHMARK FOR GENERAL AI ASSISTANTS\n\n[8] BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents\n\n[9] https://xbench.org/\n\n[10] RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning"}}, "id": "EDZvtmkAhM", "forum": "ZgCCDwcGwn", "replyto": "ZgCCDwcGwn", "signatures": ["ICLR.cc/2026/Conference/Submission20092/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20092/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20092/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763651707509, "cdate": 1763651707509, "tmdate": 1763652241618, "mdate": 1763652241618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response. Part [2/3]"}, "comment": {"value": "### #3 Experiments of the design for initial-stage of ScalingInter-RL.\n\nFollowing the suggestion of Reviewer odrg, we adopted several additional approaches in the initial stage to investigate, on the Deep Search task, whether these new methods based on subtask design or reward function design can bring performance gains:\n\n* **New strategy 1:** We first classify queries via AVG@16 sampling into easy queries (accuracy 70%–100%), medium queries (30%–70%), and hard queries (below 30%), and construct a difficulty-based curriculum of subtasks, i.e., we train on easy queries in the early phase and on hard queries in the later phase.\n* **New strategy 2:** We randomly shuffle all the data and assign correctness rewards of 0.6 for easy data, 0.8 for medium data, and 1.0 for hard data.\n\nThe experimental results are as follows:\n\n| Model           | Performance |\n| --------------- | ----------- |\n| Base Model      | 18.8        |\n| AgentGym-RL-7B  | 34.0        |\n| ScalingInter-7B | 38.3        |\n| New strategy 1  | 38.6        |\n| New strategy 2  | 37.8        |\n\nWe find that carefully designing these components can yield modest performance gains (i.e., 0.3). However, this approach requires annotating the training data by difficulty before training, which introduces additional resource overhead. The scheme we adopt in the paper is much simpler and, even without such fine-grained tuning, already surpasses the basic RL baseline. This reveals a trade-off between design complexity and performance.\n\n### #4 Experiments of applying ScalingInter-RL to more algorithms.\n\nFollowing the reviewers’ suggestions, we applied ScalingInter-RL to other algorithms and present the results below. We can observe performance differences across algorithms, which is consistent with previous work [1][2]. **In addition, it is clear that ScalingInter-RL brings performance improvements across different algorithms.**\n\n| RL Algorithm    | Method         | TextCraft | BabyAI | SciWorld |\n| --------------- | -------------- | --------- | ------ | -------- |\n| Base Model      | -              | 42.00     | 66.67  | 1.50     |\n| PPO             | AgentGym-RL-7B | 68.00     | 86.66  | 10.83    |\n|                 |ScalingInter-7B | 71.00     | 90.00  | 25.69    |\n| REINFORCE++     | AgentGym-RL-7B | 73.00     | 84.44  | 13.63    |\n|                 |ScalingInter-7B | 77.00     | 87.77  | 26.14    |\n\n### #5 Experiments of generalizing ScalingInter-RL to more real-world tasks.\n\nIn the paper, we have conducted experiments on realistic WebArena and SearchQA tasks and achieved reasonably strong results. Here, we go one step further by using data from [3][4][5] and a Google search engine based on the Serper API [6] to evaluate performance on GAIA-Text-103 [7], BrowseComp [8], and xbench-ds [9] (using the AVG@3 metric). **As shown in the table below, ScalingInter-RL yields additional performance gains.**\n\n|                 | GAIA_Text-103 | Browsecomp | xbench-ds | Average |\n| --------------- | ------------- | ---------- | --------- | ------- |\n| AgentGym-RL_10  | 36.9          | 3.3        | 21.7      | 20.6    |\n| AgentGym-RL_20  | 38.5          | 6.5        | 27.1      | 24.0    |\n| AgentGym-RL_30  | 37.6          | 7.7        | 29.5      | 24.9    |\n| ScalingInter-RL | 43.3          | 8.5        | 31.5      | 27.8    |"}}, "id": "i5xKXhcPVF", "forum": "ZgCCDwcGwn", "replyto": "ZgCCDwcGwn", "signatures": ["ICLR.cc/2026/Conference/Submission20092/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20092/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20092/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763651770562, "cdate": 1763651770562, "tmdate": 1763651770562, "mdate": 1763651770562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response. Part [1/3]"}, "comment": {"value": "Dear reviewers, many thanks for your time in reviewing our work. We are truly encouraged by your positive evaluation of the value and contribution of our paper. We sincerely appreciate your recognition of the clarity of our motivation, the generality of our framework, and the solidness of our experiments. We hope our work can provide insights and contribution to the community.\n\nWe revised the manuscript accordingly, adding several experiments, analyses, tables, and figures. The tables are all included in the response of rebuttal, while the figures are presented in the manuscript.\n\nIn what follows, we provide a general response addressing several issues of common interest to multiple reviewers, and we also include additional experiments in the hope of resolving your concerns.\n\n### #1 Experiments of ablation study for ScalingInter-RL.\n\nFollowing the reviewers’ suggestions, we conducted detailed ablation studies on the initial number of interaction rounds, the stage transition frequency, and the interaction interval. We carried out these experiments on Deep Search, and the results are as follows:\n\n| Interact Turn List | Stage Transition Frequency | Performance |\n| ------------------ | -------------------------- | ----------- |\n| [5,8,10]           | 100                        | 38.3        |\n| [5,8,10]           | 75                         | 37.8        |\n| [5,8,10]           | 125                        | 38.5        |\n| [3,8,13]           | 100                        | 36.8        |\n| [8,10,12]          | 100                        | 37.6        |\n| [5,10,15]          | 100                        | 39.1        |\n| [5, 7, 9]          | 100                        | 37.8        |\n\n### #2 Experiments of using different schemes to adjust the maximum number of interactions in ScalingInter-RL.\n\nFollowing the suggestion of Reviewer M63s, we adopted several alternative schemes for this adjustment.\n\n* Scheme 1: We select a validation set and validate every 50 steps. If the accuracy exceeds a certain threshold or the model has been trained for a duration of more than 150 steps, we increase the Max Turn Number; otherwise, we keep the Max Turn Number unchanged.\n  | Steps            | 0-150 | 150-250 | 250-500 |\n  | ---------------- | ----- | ------- | ------- |\n  | Max Turn Number  | 5     | 8       | 10      |\n  | Evaluation Score | 33.3  | 37.5    | 39.4    |\n* Scheme 2: Similar to an oscillatory function, we dynamically increase and decrease the Max Turn Number every 50 steps.\n  | Steps            | 0-50 | 50-100 | 100-150 | 150-200 | 200-250 | 250-300 | 300-350 | 350-400 | 400-450 | 450-500 |\n  | ---------------- | ---- | ------ | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- |\n  | Max Turn Number  | 5    | 8      | 10      | 8       | 5       | 8       | 10      | 8       | 5       | 8       |\n  | Evaluation Score | 30.8 | 33.5   | 37.9    | 37.1    | 35.3    | 36.0    | 37.8    | 35.5    | 30.5    | 32.5    |\n\nThe experimental results show that Scheme 1 can better monitor the training dynamics, with switches occurring at steps 150 and 250, ultimately achieving a performance of 39.4, which is somewhat higher than our initial result. Scheme 2, however, is relatively more oscillatory, and its best performance does not surpass that of our relatively fixed switching strategy. Note that in the paper, since it already surpasses the RL baselines, we initially adopted the basic scheme as the default."}}, "id": "Eb2SxBIHGP", "forum": "ZgCCDwcGwn", "replyto": "ZgCCDwcGwn", "signatures": ["ICLR.cc/2026/Conference/Submission20092/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20092/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission20092/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763651820369, "cdate": 1763651820369, "tmdate": 1763652206710, "mdate": 1763652206710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}