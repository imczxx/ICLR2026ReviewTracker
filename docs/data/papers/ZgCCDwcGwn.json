{"id": "ZgCCDwcGwn", "number": 20092, "cdate": 1758302387146, "mdate": 1759897002306, "content": {"title": "AgentGym-RL: An Open-Source Framework to Train LLM Agents for Long-Horizon Decision Making via Multi-Turn RL", "abstract": "Training LLM agents for complex multi-turn decision-making tasks requires extensive exploration within their environment, with reinforcement learning (RL) as a natural way. However, the open-source community currently lacks a unified RL framework capable of training agents from scratch across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a modular and decoupled framework specifically designed for RL-based agent in multi-turn decision-making tasks. It offers high flexibility and extensibility, supports mainstream RL algorithms, and spans a broad range of real-world scenarios. To effectively train agents for challenging tasks, we argue that they are required to expand external interactions with the environment, rather than relying solely on internal reasoning. Nevertheless, training agents for long-horizon interaction with vanilla methods often faces challenges like training instability. To this end, we propose ScalingInter-RL, a staged training approach for stable long-horizon RL training. It starts with short-horizon interaction to establish foundational policies and progressively expands them to encourage deeper exploration. Extensive experiments show that agents trained with our method achieve performance on par with—or even surpass—commercial counterparts like OpenAI o3 and Gemini-2.5-Pro across 27 tasks in diverse environments. We share key insights and will release the full framework, including code and datasets, to empower the community in building the next generation of intelligent agents.", "tldr": "We present AgentGym-RL, a unified open-source framework for training LLM agents from scratch across diverse and realistic environments, and propose ScalingInter-RL, a staged training strategy for stable long-horizon RL training.", "keywords": ["large language model", "LLM-based agent", "decision-making"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b9c869522ddbf538a57786192c62b3a6b4c7270.pdf", "supplementary_material": "/attachment/2703797971b8f1b6b9461e5329fae070de042b37.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents AgentGym-RL, an open-source framework for training LLM agents via RL in multi-turn decision-making tasks. The main contributions include: a modular framework supporting diverse environments and mainstream RL algorithms, and a staged training approach named ScalingInter-RL that progressively increases interaction horizons to achieve stable long-horizon RL training. The authors demonstrate that 7B models trained with their method achieve performance comparable to or surpassing commercial models like OpenAI o3 and Gemini-2.5-Pro across 27 tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear and meaningful. The paper tackles the genuine problem of lacking unified RL frameworks for training LLM agents across diverse environments, which is important for the community.\n2. As an open-source framework, this work provides good engineering with modular design, supporting multiple RL algorithm for diverse environments as well as various practical features.\n3. The framework demonstrates strong empirical performance. It achieves performance matching or surpassing much larger commercial models with 7B parameters, which is impressive, especially with the 33.65 point average improvement.\n4. The evaluation is comprehensive, which tests across 5 scenarios with 27 tasks. Such evaluation provides good coverage of different agent capabilities."}, "weaknesses": {"value": "1. The algorithmic novelty is limited, although the effectiveness is acknowledged. ScalingInter-RL is essentially curriculum learning applied to interaction horizons. This is a straightforward adaptation rather than a novel algorithmic contribution. The progressive scaling from short to long horizons is intuitive and incremental, which lacks theoretical justification.\n2. Given that the key to ScalingInter-RL is the curriculum scaling of interaction turns, there is no ablation on the curriculum schedule. Why these specific horizon increments?\n3. I could not find quantitative report and analysis of computational costs or training efficiency."}, "questions": {"value": "1. Could the authors provide a curriculum sensitivity study? What happens with different progression rates or starting horizons of interactions? This seems critical but is unexplored.\n2. What are the computational costs? The paper doesn't discuss training time, GPU hours, or cost comparisons with baseline approaches, making it hard to assess practical viability.\n3. Are there separate studies investigating whether the improvement mainly comes from the framework or from the ScalingInter-RL method? Would ScalingInter-RL work in other frameworks? Would the framework benefit from other training approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xjNuHXrrWO", "forum": "ZgCCDwcGwn", "replyto": "ZgCCDwcGwn", "signatures": ["ICLR.cc/2026/Conference/Submission20092/Reviewer_yP1T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20092/Reviewer_yP1T"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495140498, "cdate": 1761495140498, "tmdate": 1762932988171, "mdate": 1762932988171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AgentGym-RL, a modular open-source framework for training Large Language Model (LLM) agents on complex, multi-turn decision-making tasks using Reinforcement Learning (RL). The authors identify a key gap in the current open-source community: the lack of a unified platform for training agents from scratch in diverse and realistic environments. AgentGym-RL addresses this by providing a decoupled architecture—comprising environment, agent, and training modules—that supports various scenarios (e.g., WebArena, Deep Search, SciWorld) and mainstream RL algorithms.\nTo tackle the training instability common in long-horizon tasks, the paper proposes a curriculum learning-based strategy named ScalingInter-RL. This method begins training with shorter interaction episodes to establish a foundational policy and then progressively increases the episode length. This approach enables stable training and deeper exploration. Extensive experiments across 27 tasks demonstrate that a 7B parameter model trained with this method can match or even surpass the performance of much larger proprietary models like GPT-4 and Gemini Pro, suggesting that scaling interaction can be more effective than simply scaling model parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The development of a unified, open-source RL framework for LLM agents is a valuable contribution to the research community. It directly addresses a practical need and has the potential to significantly lower the barrier to entry for agent research, thereby fostering reproducibility and continued innovation.\n2. ScalingInter-RL is an intuitive yet powerful method that effectively stabilizes long-horizon RL training. The paper provides a clear motivation for this curriculum-based strategy (as illustrated in Figure 4), showing how it adeptly balances the trade-off between exploration depth and training stability. Its effectiveness is validated by solid experimental results.\n3. The experimental design is a major highlight of this work. The evaluation spans five diverse and challenging environments and comprehensively compares the proposed model against a wide range of strong open-source and state-of-the-art proprietary models. This thorough evaluation lends significant credibility to the paper's conclusions."}, "weaknesses": {"value": "1. Limited Analysis of the ScalingInter-RL Schedule: While the method is shown to be effective, the paper offers limited analysis of the curriculum schedule itself. The model's final performance is likely sensitive to hyperparameters such as the initial number of interaction steps, the increment size (δh), and the frequency of phase transitions (Δ). A sensitivity analysis or ablation study on these hyperparameters would provide deeper insights into the method's robustness and offer guidance for its application in new domains, thereby strengthening the paper's claims.\n2. Rigor of the Exploration-Exploitation Argument: The paper states that by initially limiting the interaction horizon, the agent focuses more on exploitation, and as the horizon expands, it is encouraged to explore more deeply. This description could be more rigorous. Even in the early stages, exploration is necessary to discover better trajectories for policy improvement. It would be beneficial to contrast this with classic exploration techniques in RL, such as curiosity-driven methods like RND or NGU, to better contextualize the exploration-exploitation trade-off within ScalingInter-RL."}, "questions": {"value": "1. Sensitivity of ScalingInter-RL: Could the authors provide more details on the robustness of the ScalingInter-RL method? For instance, how does performance vary with different curriculum schedules (e.g., faster/slower expansion of the interaction horizon, different starting points)? A related ablation study would be highly informative.\n2. Instability in Long-Horizon Training: Could you elaborate on why training is inherently more unstable in long-horizon settings? Providing related experimental or theoretical analysis would be a valuable contribution to the community.\n3. Overhead of HTTP Communication: Is there an analysis of the time overhead associated with the environment client communicating with the environment server via HTTP? A comparison with a fully collocated setup would be very insightful.\n4. Interaction with Different RL Algorithms: The paper compares GRPO and REINFORCE++, but the main benefits of ScalingInter-RL appear to be demonstrated with GRPO. It would be beneficial to know if the stability and performance gains from ScalingInter-RL are consistent across other RL algorithms, such as PPO. This would help confirm whether the method is a general strategy for stabilizing long-horizon training, rather than a technique tightly coupled with the specific mechanics of GRPO.\n5. Computational Overhead: How do the training time and computational costs of ScalingInter-RL compare to a baseline with a fixed, long horizon? By avoiding unstable training phases, does this staged approach lead to more efficient use of computational resources, potentially achieving a high-performance policy with faster overall convergence?\n6. Comparative Failure Case Analysis: The case studies in Appendix F are very helpful. It would be even more compelling to see a direct comparison of failure modes on a complex task between a baseline agent (trained with a fixed short horizon) and the ScalingInter-RL agent. Does ScalingInter-RL specifically help mitigate certain types of errors, such as repetitive loops or failure to explore deeper parts of the state space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wIhhAWPict", "forum": "ZgCCDwcGwn", "replyto": "ZgCCDwcGwn", "signatures": ["ICLR.cc/2026/Conference/Submission20092/Reviewer_gQCC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20092/Reviewer_gQCC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761538769403, "cdate": 1761538769403, "tmdate": 1762932987683, "mdate": 1762932987683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AgentGym-RL, a novel and unified reinforcement learning framework designed for training LLM agents to perform long-horizon, multi-turn interactive decision-making. To address the challenge of balancing exploration and exploitation and to ensure stable optimization, the authors propose ScalingInter-RL, a method that progressively scales the agent-environment interaction horizon during training. Extensive experiments demonstrate that the framework and method deliver significant and consistent performance gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- 1.The introduction of AgentGym-RL, the first modular, flexible, and end-to-end reinforcement learning framework specifically designed for training LLM agents in multi-turn, long-horizon decision-making across diverse real-world environments, filling a critical gap in the field.\n\n- 2.The proposed ScalingInter-RL method progressively increases the interaction horizon during training. This innovative approach effectively balances exploration and exploitation, leading to more stable RL optimization and enabling agents to develop richer behaviors and strategies.\n\n- 3.Extensive experiments demonstrate that the framework and method significantly enhance the capabilities of open-source models."}, "weaknesses": {"value": "- 1. The proposed framework is built upon existing open-source projects like AgentGym and veRL. Its main contributions are engineering-oriented—adding diverse environments, integrating existing RL algorithms, and improving scalability and stability. These engineering improvements are certainly valuable and can significantly enhance usability and robustness; however, from a research perspective, the work represents a continuation and consolidation of existing efforts rather than a fundamentally novel contribution.\n\n- 2. The ScalingInter-RL method lacks detailed specification on how initial-stage learning is concretely guided. Crucially, it omits the design of reward functions or subtasks for short-horizon phases, making it unclear how the agent acquires foundational skills before scaling up exploration.\n\n- 3. The demonstrated success is more pronounced in structured, simulated environments with clear rules, while improvements in noisy, real-world settings are modest. This suggests the framework's effectiveness may be closely tied to environments that align well with its curriculum-based scaling approach."}, "questions": {"value": "1. **On the specifics of early-stage curriculum learning**: The ScalingInter-RL method emphasizes exploitation by initially restricting interaction turns. Could you detail how the agent's learning is concretely guided during these short-horizon phases? Specifically, were task-specific dense reward functions or explicit subtasks designed to ensure the acquisition of foundational skills, rather than just learning to avoid quick failure? Furthermore, was this design implemented through a universal framework applicable across all environments, or was it individually tailored for each distinct task type?\n\n- 2. **On generalization to more challenging RL tasks**: The experiments demonstrate strong performance in structured, language-mediated environments. How might the AgentGym-RL framework and the ScalingInter-RL approach be adapted to classic RL challenges that involve high-dimensional continuous state-action spaces?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9QFQkCY2rj", "forum": "ZgCCDwcGwn", "replyto": "ZgCCDwcGwn", "signatures": ["ICLR.cc/2026/Conference/Submission20092/Reviewer_odrg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20092/Reviewer_odrg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806794107, "cdate": 1761806794107, "tmdate": 1762932987001, "mdate": 1762932987001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a new framework for multi-turn LLM RL and a multi-staged training approach for long-horizon RL by progressively scaling interaction lengths. The results are strong relative to baselines, often outperforming much larger models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work tackles challenging LLM RL tasks with real environment interactions, which is underexplored relative to the more common math reasoning. The proposed framework would be very useful for the LLM RL community.\n- ScalingInter-RL is well-motivated and a natural solution to the training instability of long-horizon RL\n- The results are impressive and demonstrate the strength of the proposed method/framework over strong baselines."}, "weaknesses": {"value": "- The text in some of the figures is small relative to the main text (Figure 1, 5, 6, 7)"}, "questions": {"value": "- Were any alternatives considered to the current linear schedule for the maximum number of interactions at each training step? In particular, I'd be curious to hear if this can be dynamically adapted (for instance allowing different RL rollouts to have slightly different limits and seeing which ones perform best)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2GbF6682ae", "forum": "ZgCCDwcGwn", "replyto": "ZgCCDwcGwn", "signatures": ["ICLR.cc/2026/Conference/Submission20092/Reviewer_M63s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20092/Reviewer_M63s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943553984, "cdate": 1761943553984, "tmdate": 1762932986479, "mdate": 1762932986479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}