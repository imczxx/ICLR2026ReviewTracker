{"id": "DealNNlz94", "number": 20682, "cdate": 1758308949049, "mdate": 1759896964213, "content": {"title": "Are Object-Centric Representations Better At Compositional Generalization?", "abstract": "Compositional generalization -- the ability to reason about novel combinations of familiar concepts -- is fundamental to human cognition and a critical challenge for machine learning. Object-centric learning, representing a scene as a set of objects, has been proposed as a promising approach for achieving this capability. However, systematic evaluation of these methods in visually complex settings remains limited. In this work, we introduce a Visual Question Answering benchmark consisting of three different visual worlds to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for the capacity of the image representation, training data diversity, downstream compute, and sample size. In this study, we use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their object-centric counterparts. Our key findings reveal that (1) object-centric approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample-efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of training data diversity, sample size, or downstream compute is constrained.", "tldr": "We systematically study the compositional generalization capabilities of object-centric representations on a visual question answering (VQA) downstream task, comparing them to standard visual encoders.", "keywords": ["Compositional generalization", "object-centric learning", "visual question answering"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/060d3eb9966043f87cb56538fd7c89de3521ec83.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates whether object-centric (OC) representations improve compositional generalization—the ability to reason about novel combinations of familiar object properties—compared to conventional dense visual features. The authors construct a controlled Visual Question Answering (VQA) benchmark across three synthetic visual worlds (CLEVRTex, Super-CLEVR, MOVi-C), systematically varying training diversity, sample size, and downstream compute. Using pretrained foundation models (DINOv2, SigLIP2) and their object-centric counterparts (DINOSAURv2, SigLIPSAUR2), they perform fair, capacity-controlled comparisons. Experiments show that OC representations outperform dense ones in harder compositional generalization settings, are more sample-efficient, and require less compute to achieve comparable or better results. Dense models only surpass OC ones in easier tasks with large data and high compute budgets. Overall, the study provides the first systematic, quantitative evidence that object-centric representations lead to stronger compositional generalization, especially under limited data, diversity, or compute conditions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Methodological Rigor:** The study offers a carefully controlled and systematic evaluation, isolating the effects of training diversity, compute, and sample size on compositional generalization.\n\n- **Comprehensive Experiments:** The evaluation spans three synthetic visual worlds, multiple difficulty levels, and two major vision encoder families (DINOv2 and SigLIP2).\n\n- **Benchmark Contribution:** The proposed VQA-based compositional generalization benchmark provides a valuable resource for future research on systematic generalization in vision."}, "weaknesses": {"value": "- **The statement does not match the table: ** Regarding the dense features (such as the comparison between DINOv2 and DINOSAURv2), the paper states several main conclusions, including: OC performs better in complex compositional generalization scenarios; dense features only outperform OC in simple scenarios. However, according to the data in Table 1, the main advantage of DINOSAURv2 is reflected in TF 2+CLEVRTex and TF 5+CLEVRTex (medium difficulty). In other settings, compared with DINOv2, the advantage of DINOSAURv2 is not significant, and even its performance is lower. Moreover, as the generalization difficulty increases, DINOSAURv2 does not show a gradually increasing advantage (among the six comparisons in Table 1, in three of them, medium has a greater advantage than hard), which is inconsistent with the claim that \"dense features only outperform OC in simple scenarios\".\n\n- **Scale of VQA model: ** Although the impact of network scale was discussed in the paper, the scales of the networks were not large: only 128 dimensions and 2/5 encoder layers were used. This setting is far from the scale of transformers typically used in VQA tasks. For instance, in some classic works on OCL+VQA, MDETR [1] uses 6 encoder + 6 decoder layers with 512 dimensions; ALOE [2] uses 28 encoder layers with 1280 dimensions. Although one of the purposes of this paper is to highlight the advantage of OC representation in requiring less computational effort, at least a normal model scale should be considered: the current model scale setting, combined with the performance comparison between TF 2 and TF 5 models, raises doubts as to whether DINOv2 would perform better under a normal scale setting.\n\n[1] MDETR-Modulated Detection for End-to-End Multi-Modal Understanding\n[2] Attention over learned object embeddings enables complex visual reasoning\n\n- **The uncertainty of generalization ability: ** Following up on the previous weakness, we can also easily identify another issue: this article aims to compare the \"compositional generalization ability of visual feature\", meaning that the generalization ability should be inherent in the visual features. However, in reality, the scale and capacity of the VQA model used largely influence the final measured generalization ability. For instance, when using TF 2, DINOSAURv2 demonstrates stronger generalization ability than DINOv2; but when using TF 5, DINOv2 shows stronger generalization ability than DINOSAURv2. So, which one is actually better at handling unseen objects? It seems difficult to make a thorough judgment through this experiment."}, "questions": {"value": "- **Composition generalization and disentanglement: ** The compositional generalization in this paper refers to the model's ability to handle new objects formed by unknown compositions of known attributes. This seems to be related to the model's ability to decouple different attributes in objects. There are already similar studies in OCL, such as [3, 4]. If these methods that focus on decoupled representations are used to construct OC representations and applied to the VQA task, will there be better compositional generalization effects?\n\n[3] Neural Systematic Binder\n[4] Disentanglement via Latent Quantization"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ULHek91Y55", "forum": "DealNNlz94", "replyto": "DealNNlz94", "signatures": ["ICLR.cc/2026/Conference/Submission20682/Reviewer_oVPf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20682/Reviewer_oVPf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788763182, "cdate": 1761788763182, "tmdate": 1762934066081, "mdate": 1762934066081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a visual question answering (VQA) benchmark to validate whether object-centric (OC) visual representations yield better compositional generalization than dense (distributed) representations from visual encoders. The benchmark consists of three datasets (CLEVRTex, Super-CLEVR, and MOVi-C). For each base dataset, they further generate 3 training datasets by changing the diversity from “easy” to “hard”.  Dense encoders (DINOv2, SigLIPs) are compared with OC models (DINOSAURv2, SigLIPSAUR2), which use a Slot-Attention bottleneck. The goal is to evaluate the quality of representations using VQA on training sets of increasing difficulty. \n\nMain findings:\n- OC approaches are superior in harder compositional generalization settings\n- OC shows better results across different budgets\n- OC is more sample-efficient, achieving stronger generalization with less data.\n\nHowever, the work’s conclusions rely on synthetic VQA benchmark under a distillation-style OC objective, yielding modest, regime-dependent gains that don’t convincingly establish OC representations as superior in realistic settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Comprehensive experiments: Both in-distribution (ID) as well as compositional out-of-distribution (COOD) are reported. In COOD settings, they use 20% of object-property combinations for testing, while the rest for training.\n- The paper is well written and easy to read."}, "weaknesses": {"value": "- The results were only reported for synthetic datasets. No tests on natural images, real-image VQA, or open-vocabulary setups—so it’s unclear the findings transfer beyond those toy examples.\n- Fig. 2 shows a strong ID-COOD correlation across settings. However, the ID–COOD correlation is somewhat expected, not novel (see [1] for example).  This is an unsurprising result that many would anticipate when training distributions are simplified. It doesn’t advance understanding of why dense and OC features differ.\n- OC models are learned based on dense features. A pixel-space or alternative OC objective is not experimented.\n\n\n\n[1] Miller, John P., et al. \"Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization.\" International conference on machine learning. PMLR, 2021."}, "questions": {"value": "- Figure 2 looks confusing. Why are there more than 4 points on each setting for each data set? There are only two configurations on image representation + 2 configurations on downstream models. Can you describe the figure in more detail?\n- In training diversity, “object-centric (OC) representations degrade less and remain superior to dense features on harder generalizations”. However, the improvements are marginal when using the large downstream model TF 5 (see Table 11). Why?\n- The feature dimensions of OC models  are different from their counterparts (DINOv2 of 384 vs DINOSAURv2 of 256 and SigLIP2 of 768 vs SigLIPSAUR2 of 256).  Why not set them to the same dimensionality? \n- Missing description or description appeared after abbreviation, e.g., OC, CA, ODD.\n- Fig. 1 is not referenced anywhere."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "e4LZJvWwU8", "forum": "DealNNlz94", "replyto": "DealNNlz94", "signatures": ["ICLR.cc/2026/Conference/Submission20682/Reviewer_EJ8F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20682/Reviewer_EJ8F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813424032, "cdate": 1761813424032, "tmdate": 1762934065701, "mdate": 1762934065701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study focuses on whether object-centric (OC) representations are more conducive to compositional generalization. By constructing a VQA benchmark and comparing mainstream visual encoders with their OC variants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper conducts extensive experiments analyzing common visual encoders, offering us a deeper understanding of them."}, "weaknesses": {"value": "1. How does the compositional generalization benchmark proposed in this paper fundamentally differ from existing compositional generalization tests? To my knowledge, other benchmarks also test compositional generalization with respect to attributes, e.g., cczsl [1] and c-gqa [2].\n\n2. The authors propose three findings—what can we do with these findings? In other words, what insights do they provide? Why are these three findings important?\n\n3. What is the underlying mechanism by which OC representations improve compositional generalization? Does the “object decomposition” in OC representations reduce the model’s reliance on “attribute co-occurrence frequencies” (whereas dense representations may overly depend on attribute co-occurrence patterns in the training set, leading to poor compositional generalization)?\n\n4. It is commonly believed that slot attention is sensitive to the number and distribution of objects, yet the paper neither proposes improvements to address this limitation nor analyzes the impact of “variations in object number” on compositional generalization.\n\n[1] Zhang Y, Feng S, Yuan J. Continual compositional zero-shot learning[C]//Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. 2024: 1724-1732.\n\n[2] Wang Q, Liu L, Jing C, et al. Learning conditional attributes for compositional zero-shot learning[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023: 11197-11206."}, "questions": {"value": "see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "59ar78rAbb", "forum": "DealNNlz94", "replyto": "DealNNlz94", "signatures": ["ICLR.cc/2026/Conference/Submission20682/Reviewer_iRSo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20682/Reviewer_iRSo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913710718, "cdate": 1761913710718, "tmdate": 1762934065271, "mdate": 1762934065271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the role of object-centric (OC) representations in achieving compositional generalization in visual question answering (VQA) tasks.  The authors introduce a benchmark that evaluates how well vision models, with and without object-centric biases, generalize to unseen combinations of object properties."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized, with a logical flow from the introduction to the experimental design, making it easy for readers to follow the authors' reasoning and methodology.\n2. The experimental part is relatively thorough and comprehensive."}, "weaknesses": {"value": "（1）Limited Novelty in Methodology: The core object-centric (OC) models proposed in this paper (DINOSAURv2, SigLIPSAUR2) are essentially derivative models built upon existing foundational models (DINOv2, SigLIP2) combined with Slot Attention (Locatello et al., 2020).   No novel object decomposition mechanism or representation learning paradigm is introduced.   The technical approach is essentially a combination of \"existing foundational models + mature Slot Attention bottleneck,\" which overlaps heavily with the design of DINOSAURv2 by Didolkar et al. (2024).   There is no breakthrough in the core principles of object-centric representations\n\n（2）Lack of Innovation in Benchmark Design: The VQA benchmark proposed in the paper, which includes CLEVRTex, Super-CLEVR, and MOVi-C, follows the same data generation logic as Kim et al. (2024) with its “attribute combination partition + training/test splits” approach.  The difficulty of the generalization task is controlled simply by adjusting the proportion of object-attribute combinations in the training set (e.g., \"easy\", \"medium\", \"hard\" corresponding to 80%/40%/20% of the combinations).  No new dimensions of generalization challenges are introduced, such as object occlusion, or real-world noise.  Compared to existing synthetic datasets like ConceptMix and SugarCrepe, this benchmark lacks differentiation in design.\n\n（3）Limited Model Comparison: The paper compares the OC models only against the original dense representations (DINOv2, SigLIP2) and k-means clustering representations but overlooks non-OC baseline[1] models that currently perform well on compositional generalization tasks. \n [1] Kempf E, Schrodi S, Argus M, et al. When and How Does CLIP Enable Domain and Compositional Generalization?[J]. arXiv preprint arXiv:2502.09507, 2025."}, "questions": {"value": "(1) In Section 3.2, \"Models and Evaluation\", the Vision Models section mentions that the object-centric feature dimension is set to 7×256. Why was the value of 7 chosen for the number of slots? Would varying the number of slots affect the performance of the experiments?\n\n(2) In Section 3.2, it is mentioned that the OC model (e.g., DINOSAURv2) requires additional training to “reconstruct dense features from the foundation model,” essentially performing a \"secondary refinement\" of the base model features. Could this process allow the model to learn structured object information prematurely, rather than this being an advantage purely of the \"OC representation\" itself\n\n(3) In the experimental section, the paper only evaluates models on the custom-built datasets. Have the authors considered evaluating the object-centric representations on more complex, real-world datasets, such as GQA[1]? These types of life-like, more intricate datasets could provide further insights into the effectiveness of object-centric representations for compositional generalization tasks.\n\n[1] Hudson D A, Manning C D. Gqa: A new dataset for real-world visual reasoning and compositional question answering[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 6700-6709."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YjVbXPyZMH", "forum": "DealNNlz94", "replyto": "DealNNlz94", "signatures": ["ICLR.cc/2026/Conference/Submission20682/Reviewer_zM3y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20682/Reviewer_zM3y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968202636, "cdate": 1761968202636, "tmdate": 1762934064562, "mdate": 1762934064562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}