{"id": "isBH8kP5AX", "number": 1941, "cdate": 1756969146782, "mdate": 1759898177358, "content": {"title": "BMAttn: Block-Aligned Mixed-Precision  Attention Quantization for LLM Inference", "abstract": "The proliferation of Large Language Models (LLMs) with extended context windows is severely hampered by the quadratic complexity of the self-attention mechanism. Existing acceleration methods, such as sparse attention and quantization, often employ uniform compression strategies that are misaligned with the non-uniform distribution of information importance within attention maps. This leads to a suboptimal trade-off between computational efficiency and model accuracy. To address this, we introduce Block-based Mixed-precision Attention (BMAttn), a novel framework that enables fine-grained, importance-aware precision while maintaining a hardware-friendly structure. BMAttn partitions each attention head into high-precision, low-precision, and sparse regions. To ensure computational regularity, these regions are block-aligned. To adapt to varying input lengths, their boundaries are dynamically adjusted using a lightweight affine windowing mechanism. We further propose a saliency-weighted calibration method and a layer-adaptive regularizer to automatically determine the optimal parameters, achieving a superior accuracy-efficiency balance. BMAttn achieves a speedup of up to 3.3× without any accuracy degradation, and a 5× speedup with only a 1\\% accuracy loss.", "tldr": "", "keywords": ["LLM", "Quantization", "Pruning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b665c9302e681bd115d4a7fa98e2c468baa5b87.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- The paper proposes BMAttn, a block-aligned mixed-precision attention method for faster LLM inference\n- BMAttn is similar to window attention, but instead of having a hard trunctation, it varies precision with distance\n  - High-precision (HP): short-range, salient dependencies\n  - Low-precision (LP): mid/long-range dependencies\n  - Sparse/pruned: negligible connections, pruned entirely\n- Block alignment is a key engineering trick to make it compatible with efficient kernel implementation\n- Empirical results across Qwen2.5-7B, Llama3-8B, and GLM4-9B, comparing it with FlashAttention-2, SageAttention, and SageAttention2\n  - authors claim neglebible loss compared to FlashAttention-2 (which is an exact attention mechanism, unlike the other methods discussed)\n  - BMAttn reports a 3.1×–3.3× speedup compared to FlashAttention-2"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- good motivation, combining algorithmic insights as well as awarness of implementation limitations to make it viable writing a high perf kernel\n- conceptually simple and intuitive approach to combine mixed precision, block alignment, and adaptive windowing into one coherent framework that fits naturally into existing attention kernels.\n- figures are excellent to understand the key idea of the algorithm\n- extremely important problem given the total cost of attention in LLMs, particularly for long context."}, "weaknesses": {"value": "- While conceptually simple and intuitive is a strength, it lacks major novelty.\n- Outdated and unclear baselines: FlashAttention-2 is now an older baseline, and several newer kernels such as FlashAttention-3, Flash-Decoding, Lean Attention, and PagedAttention (vLLM) deliver significantly faster exact attention, especially for the decode phase on modern GPUs. The paper doesn’t include or discuss these.\n- No kernel-level measurements: Even though the work emphasizes GPU efficiency, it doesn’t show kernel-level profiling or hardware utilization. There’s no data on Tensor Core occupancy, memory bandwidth, or latency per kernel, so the benefits of block alignment are mostly theoretical.\n- Narrow evaluation scope: All results are from offline accuracy benchmarks (WikiText, MMLU, LongBench, RULER). The comment on neglegible accuracy impact sounds optimistic and attention approimations can be much more sensitive in practice/more specialized datasets. The authors should consider additional benchmarks and discussions to differentiate the nouanced impact of attention approximation (maybe PingPong? but there may be better ones)."}, "questions": {"value": "I don't have any further questions but would like to hear the authors thoughts on the weaknesses I have pointed out."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Gb54upzESk", "forum": "isBH8kP5AX", "replyto": "isBH8kP5AX", "signatures": ["ICLR.cc/2026/Conference/Submission1941/Reviewer_QYg9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1941/Reviewer_QYg9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761526447973, "cdate": 1761526447973, "tmdate": 1762915964026, "mdate": 1762915964026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BMAttn, a block-aligned mixed-precision attention framework that adaptively assigns precision levels across the attention map to balance accuracy and efficiency for large language model (LLM) inference.\n\nBMAttn divides each attention head into high-precision, low-precision, and sparse zones, determined by affine distance-based thresholds that scale with sequence length. This ensures both fine-grained adaptivity and hardware regularity, making it compatible with optimized kernels such as FlashAttention."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Extensive experiments: Evaluated across three modern LLM families and multiple long-context benchmarks.\n2. Significant real-world relevance: Integrates cleanly with FlashAttention kernels and quantization toolchains, making it deployment-ready.\n\n3. Excellent ablation coverage: Demonstrates both the necessity and synergy of SWM and LRR components."}, "weaknesses": {"value": "1. No detailed hardware profiling: While claimed to be “FlashAttention-compatible,” kernel-level runtime traces or memory bandwidth breakdowns would strengthen hardware efficiency claims.\n2. Limited conceptual novelty: The core idea can be interpreted as an integration of pruning and quantization within a structured attention layout. While the implementation (block alignment and affine scaling) is clever and effective, it primarily extends known paradigms rather than introducing a fundamentally new mechanism or phenomenon."}, "questions": {"value": "1. How stable are the affine parameters across datasets or prompts? Can a single calibration generalize to unseen domains?\n\n2. How sensitive is the performance to hyperparameters metioned in paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "klgZ1amL6z", "forum": "isBH8kP5AX", "replyto": "isBH8kP5AX", "signatures": ["ICLR.cc/2026/Conference/Submission1941/Reviewer_hcmZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1941/Reviewer_hcmZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761598957503, "cdate": 1761598957503, "tmdate": 1762915963840, "mdate": 1762915963840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The BMAttn: Block-Aligned Mixed-Precision Attention paper proposes a smart and efficient way to make large language models run faster without losing accuracy. It divides the attention mechanism into small “blocks” and assigns different precision levels to each block depending on how important they are, instead of using one fixed precision for all. This design works well with GPU hardware and maintains high speed and stability. The paper also introduces methods to automatically adjust how much information to keep in each layer and to calibrate attention using a saliency-based weighting approach. Experiments show that this method makes inference up to 3.3× faster while keeping model accuracy almost unchanged. Overall, it’s a practical, well-designed approach to improving the efficiency of large language models for real-world deployment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a well-motivated, hardware-aware design that bridges algorithmic adaptivity and system-level efficiency. The introduction of block-aligned mixed precision, coupled with the affine window mechanism, enables fine-grained control of attention precision without compromising GPU regularity — a major advance over uniform quantization and sparsity methods. The saliency-weighted calibration and layer-adaptive retention regularizer add strong theoretical justification and practical effectiveness"}, "weaknesses": {"value": "While BMAttn combines block-sparse computation with mixed-precision quantization and adaptive zone allocation, the conceptual novelty is limited. The method largely integrates well-known components—sparsity pruning, distance-based masking, block-aligned computation, and quantized attention\n\nNo comparision against the most optimized recent methods from groups like MIT Han Lab (SpargeAttention, Minference) or NVIDIA’s Flash-Decoding kernels"}, "questions": {"value": "1.The calibration algorithm (Appendix A) is described textually but could benefit from a process diagram or pseudocode summary in the main body.\nRecommendation: Adding a flowchart or visual timeline of calibration steps (attention map → saliency weighting → constraint optimization. This would help readers grasp implementation details faster.\n\n2.The paper reports speedup in terms of FLOP/TOPS efficiency. Can the authors share wall-clock latency improvements (ms/token) under real inference conditions, possibly for long-context chat benchmarks?\n\n3. Technique beats SageAttention2, cool. But can you try it with the latest sparse-attention kernels from Han et al. (Song Han’s group) on identical hardware?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1AneOLPn3b", "forum": "isBH8kP5AX", "replyto": "isBH8kP5AX", "signatures": ["ICLR.cc/2026/Conference/Submission1941/Reviewer_K21C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1941/Reviewer_K21C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882782882, "cdate": 1761882782882, "tmdate": 1762915963524, "mdate": 1762915963524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BMAttn (Block-Aligned Mixed-Precision Attention), a framework that partitions each attention head into three regions—high-precision (8-bit), low-precision (4-bit), and sparse (0-bit)—based on distance from the query token. The method claims to maintain “hardware-friendly” block alignment compatible with FlashAttention kernels, while dynamically adjusting precision boundaries via affine functions of sequence length. Calibration uses saliency-weighted metrics (RDW/IPW) and layer-adaptive retention schedules to optimize compression. Empirical results on Qwen2.5-7B, LLaMA-3.1-8B, and GLM-4-9B report ≈3× speedups with “lossless efficiency.”"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The three-zone decomposition aligns with attention’s distance heterogeneity and head specialization, while block alignment preserves kernel regularity (Figure 1d, p.4, shows the staircase pattern with B=16). The combination of mixed bitwidths (INT8 for HP, INT4 for LP) and structured sparsity is cleanly specified.\n\n2. Across three backbones and four benchmarks, BMAttn matches or nearly matches full‑precision accuracy, demonstrating how, if provided with real speedup, BMAttn could be a viable choice for real deployment scenarios where a high degree of accuracy is needed.\n\n3. The authors report one‑time calibration cost and outline an O(1) per‑head overhead at inference reinforcing deployability."}, "weaknesses": {"value": "1. The paper claims FlashAttention compatibility and “no masking” via direct index computation (Appendix B), but lacks kernel pseudocode, memory layout diagrams, or profiling that would substantiate the claim that warp divergence and gather/scatter are avoided. This is especially important given mixed precision per tile and three zones per head. More concrete details would help reproducibility and clarify whether custom CUDA kernels were required.\n\n2. Experiments compare to FlashAttention‑2 and SageAttention, but omit dynamic‑sparsity baselines (e.g., Sparge) which the related work positions as complementary. Even if orthogonal, end‑to‑end tokens/s and latency comparisons against a strong sparse‑attention baseline would better establish BMAttn’s Pareto position.\n\n3. The text states “Q, K, and P are quantized per block; V per channel” (Sec. 5.1). Presumably P ≡ W (post‑softmax attention weights), but notation is inconsistent with earlier sections. Also, scales/zero‑points and clipping for INT4 are not specified, please report these details for better reproducibility.\n\n4.Sec. 5.1 cites a “device featuring 1 Tbps memory bandwidth, 83 TFLOPs (FP16), 660.6 TOPS (INT8), 1321.2 TOPS (INT4),” but doesn’t specify the actual GPU/ASIC model or whether results are simulated TOPS vs. measured wall‑clock.\n\nAs highlighted in Points 3 & 4, this paper has a consistent issue with descriptions not being precise. I would highly encourage the authors to practice using specific language rather than making broad claims. For example, \"retention regularizer\" is not a common naming convention for \"thresholding hyperparameter\". The overall presentation of this paper is weak, even though, the accuracy results signal a potentially promising idea."}, "questions": {"value": "How do the authors compute speedup?\n\nCan the authors explain what they mean by  “regular compute pattern compatible with GPU kernels such as FlashAttention”? It doesn’t seem that having different regions of datatype precision would be performant, particularly because the attention computation of Q*K^T is an activation, and storing mixed precision activations on-chip is unlikely to yield performance gains, and almost surely not when we are in smaller context lengths when self-attention is memory bound.\n\nCan you add wall‑clock tokens/s and latency on a named GPU (e.g., A100/H100) across 4k–128k, and profile HBM traffic vs. SageAttention‑8b/‑4b?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l0bkVLeGKD", "forum": "isBH8kP5AX", "replyto": "isBH8kP5AX", "signatures": ["ICLR.cc/2026/Conference/Submission1941/Reviewer_9Diq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1941/Reviewer_9Diq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883921843, "cdate": 1761883921843, "tmdate": 1762915963284, "mdate": 1762915963284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}