{"id": "ONqbn7LoYY", "number": 8142, "cdate": 1758069385261, "mdate": 1759897804407, "content": {"title": "Accelerating Diffusion Model with Dynamic Alignment", "abstract": "Recent studies have shown improvements in both generation quality and training efficiency by constraining representations during the denoising process of generative diffusion models. While distilling simple visual representations is effective, it can lead to over-alignment issues. When the model achieves alignment early in training, these simple representations can become hindrance to training the generative capacity. Building upon prior efforts that addressed this problem from the perspectives of alignment objectives and training strategies, we introduce DyA. First, we incorporate richer alignment materials to address the problem of overly simplistic representations at the source. Second, we use the internal denoising time of the diffusion model as an indicator variable to dynamically adjust the constraint strength of different levels of information. Finally, we employ the Stochastic Dropout Strategy (SDS), which allows the model to emphasize generative capacity training while providing guidance throughout the entire process. Experiments have shown that this approach improves both generation quality and training efficiency. The DyA accelerates SiT training by approximately 20 times, achieving performance comparable to SiT-XL model trained for 7M steps in just around 350K steps.", "tldr": "", "keywords": ["Diffusion", "Class-conditional Image Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d119024e47ce1170bb2e191b6c5450281276e8ef.pdf", "supplementary_material": "/attachment/649cc5f55a560734837333c87cae1c3f1c85cb95.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents Dynamic Alignment (DyA), a multi-level guidance method (semantic and texture) that dynamically adjusts alignment strength across timesteps using a temporal module for training diffusion transformers. It also introduces a Stochastic Dropout Strategy (SDS) that randomly disables alignment loss to better preserve generative capacity. Experiments show that DyA accelerates training compared to naive external alignment while improving generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation of the paper is clear - capacity mismatch problem in naive representation alignment. \n\n- Experiments demonstrate consistent improvement over REPA (naive representation alignment) and ablation studies show that both DyA and SDS contribute meaningfully to the gains."}, "weaknesses": {"value": "- While the paper argues that using representation from different layers of the encoder (semantic vs. texture) provides richer guidance, it doesn’t explain what those signals actually contain or how they help the diffusion transformer training. More analysis is needed to show whether this observation holds across different pretrained encoders (e.g., DINOv2, CLIP) and whether the method works consistently across multiple encoders.\n\n- The temporal module determines how much weight to give to semantic and texture losses at different timestep, but the paper doesn’t show how this weighting changes (e.g., are semantic cues more important at small timestep?).\n\n- Table 3 is labeled “no CFG,” yet the text (Line 320) reports results with $\\omega$=1.5. This needs to be clarified. \n\n- Implementation details are missing - which encoder is used, and the exact architecture of the temporal module. Brief descriptions (with citations) of baselines would improve clarity and reproducibility."}, "questions": {"value": "- The overall writing could be improved for better readability. In particular, Section 3.1 is very similar to that in REPA - it would be better if the authors provided their own explanation, even though this part is standard background. The related work section should also be reorganized to more clearly position this paper, perhaps including recent works on efficient diffusion transformer training via alignment.\n- If Table 3 reports results without CFG, I am curious about results when CFG is applied.\n- Can the proposed method be easily extended to text-to-image generation tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bPyGAvDeJN", "forum": "ONqbn7LoYY", "replyto": "ONqbn7LoYY", "signatures": ["ICLR.cc/2026/Conference/Submission8142/Reviewer_6FJa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8142/Reviewer_6FJa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864218771, "cdate": 1761864218771, "tmdate": 1762920113334, "mdate": 1762920113334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training framework for diffusion models called Dynamic Alignment (DyA), which combines multi-level feature alignment with a temporally adaptive modulation mechanism. The method addresses representation-guided generation’s capacity mismatch from both semantic and texture perspectives. In addition, a Stochastic Dropout Strategy (SDS) is introduced to probabilistically control alignment strength, mitigating over-alignment in later training stages. Experiments on ImageNet and ArtBench show that the proposed method achieves superior generation quality and training efficiency compared with existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear problem identification and targeted solution:** DyA introduces temporal information into the alignment modulation process, forming a multi-level dynamic guidance mechanism that effectively alleviates the capacity mismatch problem in representation-guided generation.\n2. **Improved stability and efficiency:** The SDS strategy offers a probabilistic and flexible alignment scheme, avoiding the instability of truncation-based distillation while achieving faster convergence and better generative quality.\n3. **Comprehensive empirical validation:** The paper presents detailed comparisons and ablation studies on both ImageNet and ArtBench, convincingly demonstrating the effectiveness of the proposed approach.\n4. **Simple and extensible design:** DyA is architecture-agnostic and, in principle, can be integrated into various diffusion frameworks. This generality is appealing, although no experiments currently verify this claim."}, "weaknesses": {"value": "1. **Empirical rather than theoretical depth:** Despite solid experiments, the contribution appears more like an engineering-oriented extension of REPA, with limited theoretical novelty.\n2. **Inconsistent presentation of results:** The tables are presented inconsistently — e.g., Table 2 uses *Iter.*, Table 3 uses *Epochs*; some tables report FID only, while others include IS, Rec., or sFID. Only Table 5 reports all metrics together. This inconsistency makes it difficult to clearly interpret the comparative performance across experiments. A unified reporting format (e.g., using the same metrics across tables) would greatly improve readability and credibility."}, "questions": {"value": "1. For *Fig. 2* (and related descriptions), is it a conceptual illustration (hypothesis) or derived from empirical results? It seems inconsistent with the gray dashed “No Dropout” curve in Fig. 8, which remains below zero.\n2. In *Table 3*, why is DyA not trained for the same epochs as REPA? A fair comparison would require identical training durations.\n3. The performance degradation observed with *Truncation* in Table 5 (compared to the no-Dropout, no-Truncation case) appears contradictory to the results in Table 4 and to the overall intuition of the paper. Could the authors clarify this discrepancy?\n4. Could you explicitly show the mapping curve between $t$ and $\\xi$ in the Temporal Module after training? A figure depicting this relationship would provide stronger empirical evidence supporting the paper’s conceptual claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TbPbpd6nIo", "forum": "ONqbn7LoYY", "replyto": "ONqbn7LoYY", "signatures": ["ICLR.cc/2026/Conference/Submission8142/Reviewer_sCk2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8142/Reviewer_sCk2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914709690, "cdate": 1761914709690, "tmdate": 1762920112895, "mdate": 1762920112895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to accelerate the training of diffusion transformers by aligning pre-trained representations but reducing over-alignment. The authors first observe that overly aligned representations hinder the training of diffusion models for generation. To address this, they propose DyA that uses richer alignment materials (semantic + texture) and dynamically adjusts the alignment objective. They demonstrate that DyA accelerates the training of the diffusion model faster than REPA."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-motivated in that over-alignment may hinder learning the model to generate data.\n\n2. The proposed method (DyA) shows better performance than REPA consistently."}, "weaknesses": {"value": "1. Missing analysis on $\\xi$: What is the reason that uses learnable parameter $T_\\sigma$ to obtain $\\xi$? Comparison with linear transformations (e.g., $\\xi=t$ or $\\xi=1-x$) or well-known transformations (e.g., $\\xi=\\text{sigmoid}(t)$) is needed. This is important to understand how $\\xi$ affects the training of diffusion transformers.\n\n2. Incorrect component-wise analysis, which makes it difficult to know the effectiveness of the methodology: In Dropout Rate analysis, 25% is the best, but other analysis is conducted without dropout rate (9.00, which is not the best score). In addition, L9 guidance shows the best among the options, but analysis on time modulation and baseline is conducted using L11 guidance. To demonstrate the effectiveness of each component, an ablation study is needed.\n\n\n**Minor**\n1. In line 238, I think $T_\\sigma$ is not a linear transformation (see Fig 3, which includes SiLU activations.)\n\n2. In Figure 6, semantic loss seems to be regularized. Moreover, I think the reference to Figure 6 is incorrect."}, "questions": {"value": "1. Please answer the Weakness.\n\n2. What is texture representation? It is difficult to understand how the texture representations are obtained. Are the intermediate features called texture? Then, why should we call the intermediate features texture? For instance, texture may be obtained by applying some filters (using convolution operations)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "19ZIfuAvt0", "forum": "ONqbn7LoYY", "replyto": "ONqbn7LoYY", "signatures": ["ICLR.cc/2026/Conference/Submission8142/Reviewer_8zLX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8142/Reviewer_8zLX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923898279, "cdate": 1761923898279, "tmdate": 1762920111922, "mdate": 1762920111922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to improve representation alignment-based diffusion model training. Prior \"representation alignment\" speeds up diffusion transformer training, but the paper argues that the teacher's features can be too simple and cause over-alignment, hurting generative capability. The author tries to resolve this via DyA, which is equipped with 1) richer targets, 2) Time-aware strength, and 3) SDS (Stochastic Dropout Strategy). For richer targets, Dya utilizes multiple intermediate features for a shallow layer and the later layer, avoiding that guidance is not overly simplified. Then, alignment strength is modulated with Time-aware strength and stochastically dropped according to a defined probability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-motivated upgrade over REPA. Clearly identifies and fixes over-alignment via time-aware, multi-level feature guidance and stochastic dropout.\n- Strong empirical gains. Consistently better FID/IS and recall than REPA/SiT/DiT baselines at matched steps."}, "weaknesses": {"value": "- This work completely ignores previous acceleration methods for diffusion model training. Even if they improve the REPA, these previous acceleration works should be discussed and compared. Since [1] shows REPA is actually comparable to or underperforms previous acceleration methods, the author should compare their approach with the following works:\n  - A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training, CVPR 2025. (Timestep sampling + loss weighting)\n  - Efficient Diffusion Training via Min-SNR Weighting Strategy, ICCV 2023.  (loss weighting)\n  - Addressing Negative Transfer in Diffusion Models, Neurips 2023. (loss weighting)\n  - Perception prioritized training of diffusion models, CVPR 2023. (loss weighting)\n  - Fast training of diffusion models with masked transformers, TMLR 2024. (masked transformer)\n  - Beta-tuned timestep diffusion model, ECCV 2024. (timestep sampling)\n  - Denoising task difficulty-based curriculum for training diffusion models, ICLR 2025. (curriculum learning)\n  - Non-uniform timestep sampling: Towards faster diffusion model training, ACMMM 2024 (timestep sampling)\n- If this work outperforms or has a clear advantage for faster convergence compared to the above methods, I will lean towards acceptance. Otherwise, I think that the contribution of this work is not enough.\n\nReference\n\n[1] Bidirectional Beta-Tuned Diffusion Model, TPAMI 2025."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TBl07lQvv9", "forum": "ONqbn7LoYY", "replyto": "ONqbn7LoYY", "signatures": ["ICLR.cc/2026/Conference/Submission8142/Reviewer_SsgG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8142/Reviewer_SsgG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940396090, "cdate": 1761940396090, "tmdate": 1762920111537, "mdate": 1762920111537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}