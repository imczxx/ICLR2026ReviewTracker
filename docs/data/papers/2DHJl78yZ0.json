{"id": "2DHJl78yZ0", "number": 17221, "cdate": 1758273647419, "mdate": 1759897190095, "content": {"title": "Prune, Then Select: Select High-Quality, Important, and Diverse Data Using Training Trajectories", "abstract": "The rapid expansion of instruction datasets not only escalates the computational cost of instruction fine-tuning but also brings data-related challenges, such as the presence of noisy or low-quality samples and the redundancy caused by duplicate or highly similar instances. To address these issues, data selection methods have been proposed to reduce training expenses while preserving, or even enhancing, model performance through fine-tuning on an appropriately chosen subset. In this paper, we propose a new method named $\\textbf{PS}$, containing a $\\underline{\\textbf{P}}$rune step and a $\\underline{\\textbf{S}}$elect step, to ensure selecting a high-quality, important, and diverse subset by efficiently utilizing the training trajectories of data samples collected from a small proxy model. Specifically, in the $\\underline{\\textbf{P}}$rune step, we prune low-quality data that do not exhibit a downward trend in their $\\textbf{loss trajectories}$, as these samples may negatively impact the model training. In the $\\underline{\\textbf{S}}$elect step, we introduce the concept of $\\textbf{the learning trajectory}$ (i.e., the loss reduction trajectory or the loss reduction rate trajectory), which provides a better representation of the model's learning progress on each data sample, and use these $\\textbf{learning trajectories}$ as sample features to cluster the retained samples from the $\\underline{\\textbf{P}}$rune step. A balanced selection is then performed across all clusters within a fixed budget. We validate $\\textbf{PS}$ on the MathInstruct dataset (262K) with the open-source model suite Pythia by comparing it against two categories of data selection methods: importance-based and diversity-based methods. Experimental results show that our $\\textbf{PS}$ consistently outperforms all baseline methods across budget constraints of 30K (11.5\\%), 50K (19.1\\%), and 100K (38.2\\%). Notably, $\\textbf{PS}$ achieves superior performance with less than 40\\% of the data compared to the model trained on the full dataset.", "tldr": "We propose a new method named PS, containing a Prune step and a Select step, to ensure selecting a high-quality, important, and diverse subset by effectively utilizing the training trajectories of data samples collected from a small proxy model.", "keywords": ["data-efficient training", "data selection"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a057b0c67d2b89746fcc1d9cba9774fc9cbf553.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes PS (Prune, then Select), a data selection method for instruction fine-tuning that combines quality filtering and diversity-based selection. The method operates in two steps: (1) a Prune step that analyzes loss trajectories from a small proxy model to remove low-quality samples that don't exhibit downward loss trends, and (2) a Select step that clusters retained samples using \"learning trajectories\" (loss reduction or loss reduction rate over time) and performs balanced sampling across clusters. The authors evaluate PS on the MathInstruct dataset (262K samples) using Pythia models (70M as proxy, 410M as target), showing improvements over baseline methods including S2L at budgets of 30K, 50K, and 100K samples."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Clear motivation: The paper articulates well why combining quality, importance, and diversity is important for data selection, addressing limitations of single-dimension approaches.\n\nComputational efficiency: Using a small proxy model (70M) to generate trajectories rather than the target model is a practical design choice that reduces computational overhead.\n\nConsistent improvements: PS shows improvements over baselines across multiple budget constraints, with particularly strong gains at smaller budgets (30K, 50K).\n\nNovel use of learning trajectories: The idea of using loss reduction/reduction rate trajectories instead of raw loss trajectories for clustering is interesting and inspired by multi-task learning literature.\n\nWell-structured presentation: The paper is generally well-organized with clear algorithm description and good use of figures to illustrate the method."}, "weaknesses": {"value": "1. Limited Experimental Validation\n\nSingle dataset: Only MathInstruct is evaluated. This is a significant limitation that the authors acknowledge but don't address. Mathematics is a specific domain with particular characteristics (e.g., verifiable answers, structured reasoning). Generalization to other domains (creative writing, coding, general instruction-following) is unclear.\n\nSmall model scale: Only tested on Pythia 70M→410M. The assumption that small model trajectories transfer to large models needs validation at realistic scales (e.g., 7B, 13B, 70B models commonly used today). The 70M to 410M jump is relatively modest.\n\nNo comparison on other datasets: Prior work (S2L, baselines) has been evaluated on other datasets. Why not compare on those?\n\n2. Methodological Concerns\n\na. Hyperparameter selection and sensitivity:\n\nThe threshold h=0.02 appears arbitrary with no justification, ablation study, or sensitivity analysis. How was this chosen? How sensitive are results to this value?\n\nK=100 clusters - no justification provided. How does performance vary with K?\n\nPer-source clustering for 14 sources is very specific to MathInstruct structure. How does this generalize to datasets without clear source boundaries?\n\nb. Linear regression for trend analysis:\n\nFitting loss trajectories with linear regression (Equation 4) may be overly simplistic. Loss curves often exhibit non-linear behavior (e.g., initial steep drop, then plateau).\n\nNo comparison with alternative trend detection methods (e.g., polynomial fitting, monotonicity tests, non-parametric trend tests).\n\nc. Learning trajectory definition:\n\nEquation 3 uses the same notation ℓ̂ for both loss reduction and loss reduction rate, which is confusing.\n\nFor loss reduction rate, when ℓ^i is very small, the denominator could cause numerical instability. How is this handled?\n\nThe paper doesn't provide clear guidance on when to use loss reduction vs. loss reduction rate. Tables 1 and 3 show similar performance - is there a principled selection criterion?\n\nd. Balanced sampling assumption:\n\nLines 10-17 of Algorithm 1 perform balanced sampling across clusters. The paper doesn't justify why equal representation from each cluster is optimal. Some clusters might contain more important or higher-quality data.\n\nThe calculation Rk = (B - |S|)/(K - k + 1) assumes remaining budget should be distributed equally across remaining clusters, but this may not be optimal.\n\n3. Experimental Design Issues\n\na. Unfair comparisons:\n\nMost baseline results (methods 1-7) are taken directly from Yang et al. (2024) rather than reproduced. This makes it difficult to ensure fair comparison with identical experimental settings.\n\nOnly S2L and PS are reproduced. Differences in implementation details, random seeds, or training procedures could affect results.\n\nb. Statistical significance:\n\nStandard deviations are provided but no statistical tests (e.g., t-tests, Mann-Whitney U tests).\n\nMany improvements are marginal: at 100K budget, in-domain average is 16.6 vs 16.7 (PS vs S2L). Is this difference statistically significant?\n\nWith 9 runs (3 subsets × 3 seeds), proper statistical testing is feasible and should be included.\n\nc. Limited evaluation metrics:\n\nOnly exact match accuracy is reported.\n\nNo analysis of other important aspects: computational cost comparison, model calibration, robustness to distribution shift, quality of generated reasoning chains.\n\n4. Limited Analysis and Ablations\n\na. Insufficient ablation studies:\n\nTable 2 only ablates the pruning decision (downward vs non-downward trends).\n\nMissing ablations on:\n\nThreshold h values\n\nNumber of clusters K\n\nDifferent clustering algorithms (K-means vs alternatives)\n\nLoss reduction vs loss reduction rate trajectories\n\nImpact of proxy model size\n\nPer-source vs global clustering\n\n\n\nb. Lack of qualitative analysis:\n\nNo examples of what types of samples get pruned vs retained.\n\nNo characterization of what the learned clusters represent. Do they correspond to difficulty levels, problem types, reasoning patterns?\n\nNo visualization of the learning trajectories to build intuition.\n\nc. Missing failure analysis:\n\nAt 100K budget, PS slightly underperforms S2L on in-domain tasks (16.6 vs 16.7). Why?\n\nWhat types of data benefit most/least from this selection approach?\n\n5. Novelty Concerns\n\nThe Prune step is heavily inspired by prior token-level work (Xia et al., 2023; Lin et al., 2024), adapted to sample-level with minimal modification.\n\nThe Select step is essentially S2L with learning trajectories instead of loss trajectories - an incremental change.\n\nThe main contribution is combining these two steps, but the paper doesn't provide strong theoretical or empirical justification for why this combination is superior to alternatives (e.g., why not use learning trajectories in the Prune step too?).\n\n6. Clarity Issues\n\na. Important details missing:\n\nTotal number of checkpoints T is not clearly specified. The paper says \"every 500 steps\" but what is T?\n\nComputational cost comparison with baselines is missing. How much more expensive is PS compared to random sampling or S2L?\n\nWhat is the total training time for the proxy model?\n\nb. Inconsistent terminology:\n\n\"Learning trajectory\" is used generically when there are two distinct variants (loss reduction vs loss reduction rate).\n\nThe benefit of learning trajectories over loss trajectories is asserted but not demonstrated through controlled comparison (e.g., using loss trajectories in Select step as a direct ablation)."}, "questions": {"value": "In Table 2, only 79 out of 31K pruned samples show upward trend. Why is this number so low? Does this suggest the upward trend criterion is too strict?\n\nHow does the method handle samples with non-monotonic loss trajectories (e.g., initial decrease, then increase, then decrease again)?\n\nFor per-source clustering with 14 sources and K=100, are there 100 clusters per source (1400 total) or 100 total? The paper is unclear.\nWhat is the overlap between samples selected by PS at different budgets? Are they nested subsets?\n\nHow does performance scale with the proxy-to-target model size ratio? Would a 410M→7B setup still work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IzAGk659p8", "forum": "2DHJl78yZ0", "replyto": "2DHJl78yZ0", "signatures": ["ICLR.cc/2026/Conference/Submission17221/Reviewer_WNFj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17221/Reviewer_WNFj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760757856543, "cdate": 1760757856543, "tmdate": 1762927183966, "mdate": 1762927183966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Prune, Then Select (PS), a two-stage data selection framework for instruction fine-tuning of large language models. In the Prune stage, examples with unlearnable or noisy behaviors are removed by fitting a linear model to their loss trajectories from a small proxy model and discarding those with low or flat improvement slopes. In the Select stage, the remaining samples are clustered in training trajectory space reduction patterns and a balanced subset is drawn across clusters to encourage diversity under a fixed budget. The approach is evaluated using the Pythia model family on the MathInstruct dataset, showing that PS achieves comparable or superior performance to training on the full dataset while using less than 40% of the data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- the paper is very clearly written that the reviewer appreciates!\n- i like the ablation studies comparing the slices of sft data grouped by loss trajectory. The difference in performance is pretty obvious to support the argument that loss trajectory is useful to distinguish good/bad data subsets."}, "weaknesses": {"value": "- I think a key issue with the paper is that it's actually not that stronger than S2L baseline for example - taking into account fact that the author will spend majority of time tuning their method and little time tuning baselines. I wonder how significant of a contribution it is from prior works. Also, the author's approach is essentially S2L baseline with prune step. Some what question the significance of the contribution put forward in this paper. The idea of using loss trajectory is not novel.\n- The paper will benefit from results on different base llm, e.g., in addition to pythia and additional ablations on hyperparameters: predefined threshold, number of clusters, \n- The paper currently does not support the argument that clustering in training trajectory space is superiour to semantic embedding space (using embedding model as basis to do clustering). This is important - please consider adding experiment (try a few different embedding model and make sure these baselines are tuned) showing this is actually true empirically.\n- The paper would benefit a lot from qualitative examples of the groups that are pruned / kept according to their loss trajectories and overall interpretation of what are characteristics of examples that fall into each category. It would help the reader build more intuition on what the algorithm is actually doing."}, "questions": {"value": "- Is linear function a reasonable parametric form for modeling loss trajectories ? The loss curve does not look linear to me.\n- Not necessary to include in the rebuttal or run experiments, but i'd be interested to know if the proxy model is trained on a different sft dataset (similar or very different from ones used for data selection), what would be its affect on its utility as the proxy. is it important for the proxy to be finetuned on the same sft dataset? additional would a smaller model from another model family (e.g., llama) when selecting data for one model family (e.g., pythia). It would be interesting to know what we are lossing by using a smaller proxy model as opposed to the model to be finetuned"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6U2QsmOk3v", "forum": "2DHJl78yZ0", "replyto": "2DHJl78yZ0", "signatures": ["ICLR.cc/2026/Conference/Submission17221/Reviewer_1DGp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17221/Reviewer_1DGp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761507832135, "cdate": 1761507832135, "tmdate": 1762927183655, "mdate": 1762927183655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PS (Prune, then Select) for instruction-tuning data selection using training trajectories from a small proxy model.\n\nPrune: fit a linear trend to each sample’s loss trajectory; drop samples without a clear downward slope.\n\nSelect: compute a “learning trajectory” (loss reduction or reduction rate between checkpoints), cluster retained samples, and balanced sample across clusters under a budget.\nOn MathInstruct (262K) with Pythia-70M→410M, PS beats importance- and diversity-based baselines (incl. S2L) under budgets 30K/50K/100K, and with 100K (<40%) matches/exceeds full-data training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear, simple pipeline: two steps (quality+importance via pruning, diversity via cluster sampling) are easy to reproduce.\n\nProxy-trajectory idea is practical: leverages small-model checkpoints to guide selection with low cost.\n\nLearning-trajectory feature: using delta loss rather than raw loss is a sensible, informative signal for clustering.\n\nConsistent gains at small budgets: strongest improvements at 30K/50K—useful when compute is tight."}, "weaknesses": {"value": "Narrow scope: only math data (MathInstruct) and Pythia 70M/410M; unclear generality to other domains (general instructions, code, safety) or larger/base models (Llama, Qwen, Mixtral).\n\nHeuristic pruning risk: linear slope over few checkpoints can be noisy; may drop hard-but-useful samples (curriculum/long-horizon) or keep spurious “downward” ones.\n\nLimited design ablations: sensitivity to h, T (checkpoints), K (clusters), per-source vs global clustering, and the balanced sampling rule is underexplored."}, "questions": {"value": "There are small models such as qwen, granite that is more relevent for small models, the author should replace the baseline model pythia to modern base model to at least demonstrate the usefulness of the methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HheEEEa7vO", "forum": "2DHJl78yZ0", "replyto": "2DHJl78yZ0", "signatures": ["ICLR.cc/2026/Conference/Submission17221/Reviewer_ykME"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17221/Reviewer_ykME"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762048751814, "cdate": 1762048751814, "tmdate": 1762927183394, "mdate": 1762927183394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-stage data selection pipeline for instruction-tuning: (i) Prune low-quality samples using the slope of each example’s loss trajectory (linear fit over intermediate checkpoints); (ii) Select a diverse subset by clustering learning trajectories (loss reductions between checkpoints) and balanced sampling per cluster. The proposed method is evaluated on several LLM fine-tuning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is simple and easy to implement\n- The proposed method demonstrates consistent (although small) empirical performance gains across multiple experiment settings."}, "weaknesses": {"value": "- The proposed method is incremental compared to S2L. The proposed method adds a pruning step by the loss trajectory slope, and the selection step is essentially the same as the S2L.\n- The proposed pruning method is a heuristic that needs better justification. It also introduces additional hyperparameters (e.g., the threshold h) that should be more thoroughly analyzed.\n- The performance gain over S2L is mostly small.\n- The experiments do not report the computational cost comparison.\n- There is a lack of apple-to-apple comparison with non-S2L baselines. Only S2L is reproduced; other baselines are taken from prior work."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8ijZRiqOSh", "forum": "2DHJl78yZ0", "replyto": "2DHJl78yZ0", "signatures": ["ICLR.cc/2026/Conference/Submission17221/Reviewer_mvQ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17221/Reviewer_mvQ4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059623724, "cdate": 1762059623724, "tmdate": 1762927183138, "mdate": 1762927183138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}