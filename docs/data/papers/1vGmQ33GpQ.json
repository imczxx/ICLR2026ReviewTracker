{"id": "1vGmQ33GpQ", "number": 417, "cdate": 1756738776679, "mdate": 1759898262013, "content": {"title": "Abductive Logical Rule Induction by Bridging Inductive Logic Programming and Multimodal Large Language Models", "abstract": "We propose ILP-CoT, a method that bridges Inductive Logic Programming (ILP) and Multimodal Large Language Models (MLLMs) for abductive logical rule induction. The task involves both discovering logical facts and inducing logical rules from a small number of unstructured textual or visual inputs, which still remain challenging when solely relying on ILP, due to the requirement of specified background knowledge and high computational cost, or MLLMs, due to the appearance of perceptual hallucinations. Based on the key observation that MLLMs could propose structure-correct rules even under hallucinations, our approach automatically builds ILP tasks with pruned search spaces based on the rule structure proposals from MLLMs, and utilizes ILP system to output rules built upon rectified logical facts and formal inductive reasoning. Its effectiveness is verified through challenging logical induction benchmarks, as well as a potential application of our approach, namely text-to-image customized generation with rule induction. Our code and data are released at https://anonymous.4open.science/r/ILP-CoT-Ano-83DC/.", "tldr": "", "keywords": ["abductive logical rule induction", "inductive logic programming", "multimodal large language model", "text-to-image generation"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10121426a8a130084c67761f2264e90140053968.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ILP-CoT, a training-free neuro-symbolic framework that integrates Inductive Logic Programming (ILP) with Multimodal Large Language Models (MLLMs) to perform abductive logical rule induction from a handful of textual or visual examples. The key idea is to let an MLLM generate noisy but structurally plausible rule proposals, convert these proposals into meta-rules that act as a pruned hypothesis space for ILP, and then invoke an off-the-shelf ILP solver (Metagol) to rectify factual errors and induce verifiable rules."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1)  ILP-CoT keeps the MLLM and the ILP engine loosely coupled yet mutually corrective. The MLLM is exploited only for structure proposals (meta-rules) and candidate facts, while the ILP layer performs provable induction over a pruned space.\n2) The use of ILP within the MLLM appears both novel and interesting.\n3) The results demonstrate strong performance against Custom CoT."}, "weaknesses": {"value": "1) In the paper, relatively simple baselines are used for comparison. Although the reported performance appears good, the results are not fully convincing. It would be beneficial to include stronger state-of-the-art baselines, for example, Corvid: Improving multimodal large language models towards chain-of-thought reasoning, Cantor: Inspiring multimodal chain-of-thought of mllm, Kam-cot: Knowledge augmented multimodal chain-of-thoughts reasoning.\n2) The failure-reflection loop can restart the entire pipeline up to a “preset maximum” (unspecified) iterations. On large vocabularies or high-resolution images, the token cost and latency could explode, making the method impractical for real-time or web-scale applications.\n3) The failure-reflection loop attempts fine-grained token splitting, but the paper provides no quantitative improvement over the single-shot pipeline; convergence rates and iteration counts are missing.\n4) The core assumption is that “MLLMs could propose structure-correct rules even under hallucinations.” Yet Table 6 shows that ≥ 98 % of MLLM rule proposals are faulty (missing, redundant, or wrong). The conversion to meta-rules is purely syntactic; if the MLLM proposes a structurally biased but incomplete template space, ILP will never recover the correct rule."}, "questions": {"value": "1) Can you provide a formal proof (or a probabilistic bound) that the meta-rule space produced by the MLLM is complete with respect to the ground-truth rule class, or characterize the conditions under which completeness is lost?\n2) If the MLLM systematically omits a necessary meta-rule template, does the failure-reflection loop have any mechanism to invent that template, or will the system converge to a local optimum and halt?\n3) Table 6 shows >98 % of MLLM-generated rule proposals are faulty. What fraction of those faults are structural (and thus survive the syntactic conversion to meta-rules), and how does that fraction affect final rule accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SlPQCGaSCP", "forum": "1vGmQ33GpQ", "replyto": "1vGmQ33GpQ", "signatures": ["ICLR.cc/2026/Conference/Submission417/Reviewer_AjVr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission417/Reviewer_AjVr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761315193670, "cdate": 1761315193670, "tmdate": 1762915515396, "mdate": 1762915515396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ILP-CoT, a novel approach that integrates Inductive Logic Programming (ILP) with MLLMs to address abductive logical rule induction. Traditional ILP systems require structured data and predefined knowledge, while MLLMs can generate plausible rules but may suffer from inaccuracies or logical inconsistencies. ILP-CoT bridges these limitations by using MLLMs to propose rule structures and then refining them through ILP systems to ensure logical soundness. This method is evaluated through logical induction benchmarks and text-to-image generation tasks, demonstrating its ability to induce logical rules from minimal data and its practical application in real-world tasks. The key contribution lies in the integration of symbolic reasoning and modern machine learning models, offering a flexible and efficient solution for tasks requiring both perception and logical reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a novel hybrid framework (ILP-CoT) that integrates multimodal large language models with inductive logic programming for abductive rule induction. This creative combination enables logical reasoning directly from unstructured inputs like text or images, addressing a long-standing gap between perception and symbolic reasoning.\n2. This work makes an important step toward unifying neural perception and symbolic reasoning in an interpretable way. Its results demonstrate practical potential for tasks requiring explainable and verifiable reasoning, such as scientific discovery and multimodal understanding.\n3. The proposed approach is technically solid and rigorously evaluated across multiple benchmarks."}, "weaknesses": {"value": "1. There are too few baselines, and other inductive reasoning methods are lacking for comparison.\n2. The method assumes that the data can be converted into structured symbolic representations, which may not always be feasible or efficient for certain types of unstructured data, such as noisy real-world data.\n3. According to Table 3, compared with Qwen-7B, larger and more capable models seem to approach the performance achieved by introducing ILP (Custom-CoT). Therefore, with the inclusion of stronger models (such as GPT-5), the effectiveness and significance of this work appear to diminish. However, the paper lacks corresponding experiments to substantiate this observation."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zHTqRxSwT4", "forum": "1vGmQ33GpQ", "replyto": "1vGmQ33GpQ", "signatures": ["ICLR.cc/2026/Conference/Submission417/Reviewer_Hxhn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission417/Reviewer_Hxhn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733227425, "cdate": 1761733227425, "tmdate": 1762915515073, "mdate": 1762915515073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ILP-CoT, a framework that bridges MLLMs with ILP to induce interpretable and verifiable logical rules from unstructured textual or visual inputs. The MLLM processes raw inputs to generate symbolic facts and structure-consistent candidate rules, which are then translated into meta-rules for ILP. A reflection loop detects inconsistencies and triggers regeneration on the MLLM side. The authors provide multi-domain evaluations and release open-source code to support reproducibility."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces a useful, verifiable pipeline that connects raw inputs to symbolic reasoning via LLM and ILP; the plug-in design supports different MLLMs without task-specific training, offering good flexibility.\n\n2. The approach uses the observation that MLLMs can often produce structure-correct rule forms even with hallucinations, although this assumption depends on the quality of the base model, the authors also provides a failure reflection loop to ensure the automatically ILP tasks can be automatically constructed without further training.\n\n3. The evaluation section presents multi-domain results, showing that ILP-CoT outperforms Direct-Predict and Custom-CoT under the same base model."}, "weaknesses": {"value": "1. Time and computational cost: the cost of ILP-CoT is not clearly quantified, it would be helpful to report the average time for MLLM generation, reflection-loop retries (which depend on base-model quality and task complexity but could be summarized as average time), and ILP solving. The authors note that failure reflections can be expensive for large numbers of subjects, but the costs and cost–accuracy trade-offs for smaller or typical settings are also unclear.\n\n2. Fairness under reflection: it is unclear whether sampling budgets are aligned across Direct-Predict, Custom-CoT, and ILP-CoT, since ILP-CoT includes failure reflection retries, the evaluation should clarify whether the other methods use the same number of decoding attempts, ensuring that the reported gains are not simply due to additional MLLM attempts.\n\n3. Base-model performance discussion: different base models are used across tasks (e.g., Qwen-7B for CLEVR-Hans; GPT-4o, Gemini-2.0 Flash, and Qwen-Max for ARC), but the reason for these selections is not stated. A brief discussion of this selection would help, since the performance of this method clearly depends on base-model quality: for example, in table 2 the improvement over Custom-CoT is small (~0.25 %) in Qwen-max compared with other methods. In table 1 the fully trained Neumann model outperforms ILP-CoT, while this may come from pretraining of Neumann on this task, clarifying whether a stronger base MLLM could close this gap would better illustrate the trade-off between training a CNN and using a pretrained MLLM without fine-tuning to achieve comparable results."}, "questions": {"value": "1. Please clarify or align the sample/retry budget across Direct-Predict, Custom-CoT, and ILP-CoT (e.g., ensure all results are reported under similar MLLM settings, exact@1 or pass@k)?\n\n2. Could you quantify the relation between #reflections and accuracy, or the overall time and accuracy?\n\n3. How different MLLMs affect the performance in each tasks, and to what extent MLLMs without pretraining can outperform pretrained CNNs in terms of cost or accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eCkP7xJFle", "forum": "1vGmQ33GpQ", "replyto": "1vGmQ33GpQ", "signatures": ["ICLR.cc/2026/Conference/Submission417/Reviewer_apT9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission417/Reviewer_apT9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943711755, "cdate": 1761943711755, "tmdate": 1762915514850, "mdate": 1762915514850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a neuro-symbolic framework that combines multimodal large language models (MLLMs) and inductive logic programming (ILP).\nThe MLLMs provide powerful perception ability, including identify entities and relations from images, corresponding to logical facts.\nILP can produces interpretable and verifiable logical rules according to the logical facts.\nExperiments show the effectiveness of the proposed method.\nAuthors also propose text-to-image customized generation, a potential application of the framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and formatted.\n2. Authors propose a partial interpretable neuro-symbolic architecture, which is a kind of frontier research.\n3. Experiments are well designed and can justify the main claims."}, "weaknesses": {"value": "1. The definition of ILP in Section 3.1 is not clear enough. For example\n    - What exactly is $H, B, E^+, E^-$? It seems that they are set of Horn clauses, which is a common but not unique setting in ILP.\n    - Are $E^+$ and $E^-$ sets of logical facts? If so, the formula $H \\cup B \\models E^+, H \\cup B \\nvDash E^-$ is not clear since the entailment consequence can not be a set.\n    - Inductive bias can mean many things in ILP, e.g., language bias. What exactly $B$ specifies?\n    \n    I suggest rewriting the definition of ILP in a more mathmatical and rigorous manner. Appropriate citations or examples may help. Otherwise, it is hard to understand the exact problem scope authors are tackling. \n2. Section 3.3: \"Generating rule structure proposals using MLLM\" is not well motivated. \"MLLMs could propose structure-correct rules even under hallucinations\" is just an observation but not a motivation, since proposing structure-correct rules does not imply proposing helpful meta-rules.\n3. Section 4.5: The ablation study is not fair. The proposed method defines inductive bias through meta-rules for Metagol, but does not provide inductive bias for Popper and ILASP. Although Popper and ILASP employ other format of inductive bias, inductive bias is crucial for symbolic ILP approaches. To conduct a fair comparison, I suggest providing inductive bias for Popper and ILASP in some other ways.\n4. Combining neural perception and ILP is not a new idea but authors do not mention some existing methods. Authors should discuss them or conduct experiments on them. A non-exhaustive references is given below.\n\n\n[1] Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. Deepproblog: Neural probabilistic logic programming. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.,\n2018.\n\n[2] Wang-Zhou Dai and Stephen Muggleton. Abductive knowledge induction from raw data. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 1845–1851. International Joint Conferences on Artificial Intelligence Organization, 8 2021.\n\n[3] Shindo, H., Pfanschilling, V., Dhami, D.S. et al. αILP: thinking visual scenes as differentiable logic programs. Mach Learn 112, 1465–1497 (2023). https://doi.org/10.1007/s10994-023-06320-1"}, "questions": {"value": "1. Please explain Section 3.2 by throughout examples or formal statement. Especially, what is activation schema? Is it designed manually? What is single-state & multi-state?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SyelCyizdY", "forum": "1vGmQ33GpQ", "replyto": "1vGmQ33GpQ", "signatures": ["ICLR.cc/2026/Conference/Submission417/Reviewer_mNWp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission417/Reviewer_mNWp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762692003493, "cdate": 1762692003493, "tmdate": 1762915514699, "mdate": 1762915514699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}