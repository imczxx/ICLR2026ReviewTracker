{"id": "VHNzDcJKU3", "number": 17145, "cdate": 1758272742626, "mdate": 1759897193876, "content": {"title": "FakeMark: Deepfake Speech Attribution With Watermarked Artifacts", "abstract": "Deepfake speech attribution remains challenging for existing solutions. Classifier-based solutions often fail to generalize to domain-shifted samples, and watermarking-based solutions are easily compromised by distortions like codec compression or malicious removal attacks. To address these issues, we propose FakeMark, a novel watermarking framework that injects artifact-correlated watermarks associated with deepfake systems rather than predefined bitstring messages. This design allows a detector to attribute the source system by leveraging both injected watermark and intrinsic deepfake artifacts, remaining effective even if one of these cues is elusive or removed. Experimental results show that FakeMark improves generalization to cross-dataset samples where classifier-based solutions struggle and maintains high accuracy under various distortions where conventional watermarking-based solutions fail. Speech samples are available at https://fakemark-demo.github.io/fakemark-demo/.", "tldr": "A watermarking framework tailored for robust deepfake speech attribution", "keywords": ["deepfake attribution", "deepfake speech", "audio watermarking", "synthetic artifacts", "source tracing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d0e5697586a73ead397c29b117a3cec2abf51fd1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission proposes a post-hoc watermarking technique dedicated to audio GenAI.\nThe idea is to make the watermark signal reinforcing the audio cues characterizing a particular GenAI model.\nThe application is data provenance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea is novel. The only prior work that came to my mind is the paper *Learning to watermark LLM-Generated Text Via Reinforcement Learning*, Xu et al., 1st workshop on GenAI, where a passive detector is first trained to detect text generated by a particular LLM, and then this LLM is watermarked (weights fine-tuning) to better comply with this detector.\n\nThe same approach is applied here with noticeable differences: audio not text, post-hoc not in-gen watermarking, a classifier not a detector, learning of a specific watermark decoder not the original detector."}, "weaknesses": {"value": "### W1 - Motivations in the introduction\n\n- A first confusion in the introduction is about the final application. The terms *deepfake*, *detector* are misleading. For instance, the proposed scheme does not make the distinction between real audio and synthetic audio. It distinguishes the generated model knowing the audio is synthetic. Moreover, deepfake detection is an open-set problem, whereas the proposed scheme can only handle a fixed number $C$ of models. On the same token, speaking about copyright violation (Line 29) for deepfakes does not make sense. I would say that the term **data provenance** or **GenAI model identification** are missing. On the same token, I would replace the term *deepfake* by **synthetic** and *detector* by **classifier**.  Speech synthesis models can be very useful for other applications than deepfakes.  **This includes changing the title, the abstract, the introduction and even the nickname FakeMark.**\n\n- I recommend to mitigate assertions like *watermarking-based solutions are easily compromised* (Line 13), *can be easily compromised by common distortions* (Line 41), *whereas watermark detectors degrade* (Line 48). Watermarking is a cat and mouse game. You are citing papers that succeed attacking some watermarking schemes. It does not mean all watermarking schemes are vulnerable to these attacks. For instance, there are watermarking schemes robust to neural codecs to some extend like *Latent Watermarking of Audio Generative Models*, San Roman et al. Moreover, Line 330, you admit that Timbre is indeed robust. One just has to add the neural codec in the augmentation set during training. Anyway, stating that watermarking is robust or not robust does not mean anything unless one looks at the distortion of the attack.\n\n### W2 - lack of clarity\nSome mathematical formalism would help understanding the training. \n- Sect. 3.2\nLine 211: *The back-propagated loss [...] implicitly guides the watermark embeddings*. This is technically wrong. Since that loss is computed over unwatermarked signals, it cannot impact the watermarked embedder.\n\n- Sect. 4.1\nLine 252: *evaluations on unseen architectures are beyond the scope of this work.* It is even more than that: Evaluations on unseen models is simply impossible since the watermarking scheme need the model for its training. This is a clear limitation of the proposal.\n\n### W3 - lack of security\nThe watermark signal in the latent space is always constant, for a given models. Therefore the scheme is not secure. The attacker could average many latent vectors extracted from watermarked signals to estimate $\\mathbf{e}_w$ and then remove it. Counter-measures are known for a long time: **Side-informed watermarking** would make $\\mathbf{e}_w$ dependent on $\\mathbf{e}_s$, ie. the signal to be watermarked. See the literature about **watermarking security**. The irony is that the paper *Can simple averaging defeat modern watermarks?*, Yang et al. is cited.\n\nAnother point is that the watermark decoder is based on an open-source SSL backbone. This is also a vulnerability, see\n*Evaluation of security of ML-based watermarking: Copy and removal attacks*, Kinakh et al. WIFS 2024.  \n\n### W4 - Unfair benchmark\n- Line 288: Overwriting as an attack? This is really unfair. AudioSeal and Timbre are *publicly-available* for research purpose. And I hope that your scheme will be as well. Therefore, either overwriting is non sense (and I strongly agree with this option), either you should also apply overwriting to your own scheme as well. \n\n- Some measurements about AudioSeal surprise me. The SISNR reported in the original paper is low (26 dB - Table 1 / 24 dB Table 9) whereas your Table 3 reports 36dB. The original paper report almost perfect robustness against speed x1.25, whereas your Table 1 reads poor performance for speed x1.05, ie. a less harmful attack.\n\n- The four techniques (FakeMark A and T, AudioSeal, Timbre) have strong differences in speech quality. I consider that a SISNR lower than 20dB is a dead-end. FakeMark^T is simply not useable. Moreover, comparing the robustness of watermarking scheme with much perceptibility differences is meaningless.\n\n- As already mentioned, the *Averaging* attack is totally feasible against FakeMark. It does not operate in the time domain, but in the feature domain. According to the appendix, *Averaging* seems to target only AudioSeal. Therefore reporting the impact of this targeted attack on other schemes is weird. This comment also holds for *Overwritting*, especially against classifiers (last 2 columns in Table 1 and 2)."}, "questions": {"value": "### Q1\n\nLine 216: Why is the message random? This means that the attribution loss and the watermarking detection loss disagree. A signal without watermark should be classified as generated by its ground-truth model, whereas the same signal with watermark should be classified in another way. How can the watermark signal  be aligned with generation artifacts in this case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0inXt3RTOX", "forum": "VHNzDcJKU3", "replyto": "VHNzDcJKU3", "signatures": ["ICLR.cc/2026/Conference/Submission17145/Reviewer_z9Yp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17145/Reviewer_z9Yp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761210588674, "cdate": 1761210588674, "tmdate": 1762927135981, "mdate": 1762927135981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FakeMark, a novel watermarking framework for deepfake speech attribution. By injecting artifact-correlated watermarks, this design allows a detector to attribute the source system by leveraging both injected watermark and intrinsic deepfake artifacts, remaining effective even if one of these cues is elusive or removed. Experimental results show that it improves attribution robustness and generalization in challenging scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally well-organized and easy to read.\n- The methodology is easy to follow.\n- The paper presents the first systematic evaluation of deepfake speech attribution using both\nwatermarking-based and classifier-based models."}, "weaknesses": {"value": "- The novelty of the proposed method appears to be limited. The method is largely an integration of existing techniques, namely watermarking method (e.g., Timbre) and the SSL-based classifier (MMS-300M). The framework's performance seems heavily dependent on the robustness of the chosen watermark and the accuracy of the classifier, rather than the proposed “watermarked artifacts” integration strategy itself. Therefore, the **added value** of the integration strategy itself is not immediately clear.\n- The evaluation of the framework’s performance appears incomplete. The authors investigate scenarios where *either* the watermark is removed *or* the seen deepfake artifacts patterns are absent, but they do not address the more comprehensive scenario where both are removed simultaneously. The effectiveness of the method in this combined-attack setting remains unclear, which weakens the paper's overall robustness and claims.\n- Some important details about the proposed approaches are not mentioned or explained in the paper.\n   - Could the authors provide a more detailed explanation of the claimed \"artifact-correlated watermark\" design? Based on the paper's description, the watermark w still appears to be a class index w ∈ {1, . . . , C}, where C is the total number of deepfake systems. It is not immediately clear how this design is actually \"artifact-correlated\" as claimed.\n  - The training process for the \"Detector\" module is not mentioned. How does its training differ from the \"detector\" in classifier-based attribution methods or the \"decoder\" in traditional watermarking methods? This distinction seems critical to understanding the novelty of the proposed approach.\n  - Regarding the \"Attribution loss,\" the authors state: \"It is computed as the cross-entropy between the ground-truth deepfake system label and the detector’s predicted class probabilities over an unwatermarked clean signal.\" If this loss is indeed computed on an unwatermarked clean signal, it seems identical to training a standard classifier-based attribution model. Can the authors please confirm if this is correct, or if this loss should be computed on a watermarked signal instead?\n  - In the experimental setup, the terms \"seen artifacts,\" \"unseen artifacts,\" \"seen architectures,\" and \"unseen architectures\" are used without explicit definitions. This terminology is ambiguous and confusing. \n- Some typos:\n  - In line 88: “watermarking-” should be “watermarking-based”"}, "questions": {"value": "- What is the framework's effectiveness in the more challenging scenario where both the watermark and the seen deepfake artifacts are removed simultaneously? \n- The paper evaluates robustness against various distortions in isolation (e.g., only compression, or only noise). However, a more realistic scenario would involve composite or sequential attacks (e.g., codec compression followed by noise or pitch-shift). How does the proposed watermark perform under such combined distortions? Were any experiments conducted for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5XDWWneyAT", "forum": "VHNzDcJKU3", "replyto": "VHNzDcJKU3", "signatures": ["ICLR.cc/2026/Conference/Submission17145/Reviewer_Bkfp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17145/Reviewer_Bkfp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554091531, "cdate": 1761554091531, "tmdate": 1762927135276, "mdate": 1762927135276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FakeMark, a hybrid framework for deepfake speech attribution. It aims to address the key weaknesses of existing methods: the poor generalization of classifiers and the vulnerability of watermarks to distortions. The core contribution is the concept of an 'artifact-correlated watermark,' where the injected watermark is associated with the intrinsic artifacts of a specific deepfake system. A detector is then trained to recognize both the watermark and the artifacts simultaneously. The authors claim this dual approach improves robustness against distortions (by falling back on artifacts if the watermark is removed) and generalization to new datasets (by relying on the watermark)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's originality lies in its novel problem formulation of a hybrid attribution system. The core idea of injecting an \"artifact-correlated watermark\" —combining watermark detection and artifact classification into a single, dual-signal framework—is a creative combination of existing concepts.   \n- The authors perform a comprehensive evaluation. The method is tested under a wide array of challenging distortions, including neural codecs, vocoders, and specific watermark removal attacks."}, "weaknesses": {"value": "The results in Table 3 suggest a strong trade-off: the most robust models (FakeMark-S, Timbre) have the worst audio quality (PESQ 2.83 / 2.97). This implies the robustness is achieved by injecting a stronger, more perceptible watermark. Is this a fundamental limitation of the spectrogram-based approaches?"}, "questions": {"value": "Please provide solutions to the degraded perpectuality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4rZsD0L6n6", "forum": "VHNzDcJKU3", "replyto": "VHNzDcJKU3", "signatures": ["ICLR.cc/2026/Conference/Submission17145/Reviewer_FUV6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17145/Reviewer_FUV6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808202288, "cdate": 1761808202288, "tmdate": 1762927134835, "mdate": 1762927134835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the task of model attribution of audio deepfakes: given a synthesized audio, predict which model it was generated with. The typical approach is to train a network that maps an audio deepfake to its corresponding model category. Instead, the proposed method, FakeMark, takes inspiration from watermarking and adds auxiliary information to the signal to ease detection. However, unlike traditional watermarking, the injected watermark depends on the model. This approach can be understood as amplifying the generators' fingerprints through learned watermark embeddings. The authors show that this approach gives better attribution results than independent fingerprints, although at a slight cost in intelligibility.\n\nMy rating is close to borderline. Justification: The paper proposes an interesting point in the design space of model attribution systems, but with an unclear practical use case."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The approach is interesting and provides a novel point in the design space of model attribution systems. Framing it as \"fingerprint amplification\" rather than traditional watermarking would better capture its actual mechanism.\n- The paper is well executed with experiments on well-established benchmarks and methods. The cross-dataset generalization results demonstrate that watermarks help when artifacts shift.\n- The paper provides a comprehensive evaluation of robustness to various distortions (codecs, vocoders, signal processing, removal attacks)."}, "weaknesses": {"value": "- It's unclear what the envisioned use case is for such an approach. One goal of model attribution is to identify the vendors who produce a certain deepfake, in order to hold them accountable. However, applying the proposed method across vendors requires a centralized system with which all vendors agree with (since the watermarking model has to be shared). This seems like a very strong assumption. Alternatively, for attribution of in-the-wild deepfakes, where such cooperation cannot be expected, the detector reduces to an artifact-based classifier, making the watermarking component superfluous.\n- The proposed method misses some of the key characteristics of watermarking (e.g., embedding an independent and verifiable information into the signal). For this reason, calling it \"a watermarking framework\" may be misleading; instead, it may be more accurately regarded as a fingerprint-amplifying method."}, "questions": {"value": "- See weaknesses.\n- How does the similarity of the model waterkmarking embeddings looks like? Do we get similar watermarking embeddings for similar models? This would validate the \"artifact-correlated\" claim.\n- How does the approach scale with the number of models to be attributed? Would the embedding space get crowded and the artifacts start overlapping between models?\n- The proposed approach assumes the set of models fixed (closed set). If a new model arises, one has to retrain FakeMark. This seems impractical. Could the approach be extended to an open-set setting, for instance by first training a model encoder (e.g., via metric learning) and then using its embeddings as watermarking embeddings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "piiWlABMWA", "forum": "VHNzDcJKU3", "replyto": "VHNzDcJKU3", "signatures": ["ICLR.cc/2026/Conference/Submission17145/Reviewer_zpYx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17145/Reviewer_zpYx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17145/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853543139, "cdate": 1761853543139, "tmdate": 1762927134530, "mdate": 1762927134530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}