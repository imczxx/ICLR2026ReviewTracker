{"id": "qfeSpu5FBE", "number": 25459, "cdate": 1758368293051, "mdate": 1759896720376, "content": {"title": "Treating Neural Image Compression via Modular Adversarial Optimization: From Global Distortion to Local Artifacts", "abstract": "The rapid progress in neural image compression (NIC) led to the deployment of advanced codecs, such as JPEG AI, which significantly outperform conventional approaches. However, despite extensive research on the adversarial robustness of neural networks in various computer vision tasks, the vulnerability of NIC models to adversarial attacks remains underexplored. Moreover, the existing adversarial attacks on NIC are ineffective against modern codecs. In this paper, we introduce a novel adversarial attack targeting NIC models. Our approach is built upon two core stages: (1) optimization of global-local distortions, and (2) a selective masking strategy that enhances attack stealthiness. Experimental evaluations demonstrate that the proposed method outperforms prior attacks on both JPEG AI and other NIC models, achieving greater distortion on decoded images and lower perceptibility of adversarial images. We also provide a theoretical analysis and discuss the underlying reasons for the effectiveness of our attack, offering new insights into the security and robustness of learned image compression.", "tldr": "We propose a modular adversarial attack on neural image codecs that reduces the compression quality of both the entire image and local areas, in order to improve the effectiveness and filters noise to stay imperceptible.", "keywords": ["Adversarial Robustness", "Neural Image Compression", "Adversarial Attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c27e2a1d745c3bc20af3e220ea1164b7e312a2d3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel adversarial attack against neural image compression (NIC) models. Compared to existing attacks, the proposed attack optimizes both global and local distortions and leverages a selective frequency-domain mask to enhance attack stealthiness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors provided a thorough review of the literature on NIC and its attack.\n- The proposed attack is effective against modern codecs such as JPEG AI."}, "weaknesses": {"value": "- The paper's introduction fails to clearly motivate the significance of its local optimization module, which is a key claimed contribution. The authors are suggested to move the justification from Section 4.3 to the introduction to immediately clarify why this local attack is necessary and effective.\n\n- The proof of the proposition implicitly assumes independence across all pixels in the image, which is likely to be violated for natural images where pixels are spatially correlated. Additionally, the implication of this result on practical attacks is unclear, as efficiency is not typically a primary bottleneck for adversarial attacks.\n\n- The general idea of using the frequency domain to improve an attack's stealthiness is not novel. It's a well-established concept in adversarial attack research (e.g., [1,2]).\n\n- The l_\\infty distortion bound used in experiments is not specified. Furthermore, it lacks an ablation study on this budget, making it hard to assess how the attack's performance scales at different perturbation levels.\n\n[1] Luo, Cheng, et al. \"Frequency-driven imperceptible adversarial attack on semantic similarity.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n[2] Qin, Yao, et al. \"Imperceptible, robust, and targeted adversarial examples for automatic speech recognition.\" International conference on machine learning. PMLR, 2019."}, "questions": {"value": "- The performance gap between optimizing with and without normalized direction seems to be small. Would the signed gradient method achieve similar performance if optimized for more iterations?\n\n- Does the generated adversarial perturbation transfer across different codecs?\n\n- What is the perurabtion budget used in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z2YmPsmaf4", "forum": "qfeSpu5FBE", "replyto": "qfeSpu5FBE", "signatures": ["ICLR.cc/2026/Conference/Submission25459/Reviewer_vRSh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25459/Reviewer_vRSh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795818249, "cdate": 1761795818249, "tmdate": 1762943440113, "mdate": 1762943440113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a modular adversarial attack framework tailored for neural image compression (NIC) models. The framework combines global distortion maximization, local artifact amplification, and frequency-domain masking techniques. Experimental results on widely used codecs, such as JPEG AI, demonstrate that the proposed method outperforms existing attacks across multiple compression models (including JPEG AI), producing higher distortion while maintaining imperceptibility."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The experiments are relatively systematic, involving a comparison of the representative learnable compression methods HiFiC and ELIC with the traditional JPEG method. These experiments confirm that the former provides better defense efficacy, offering insights for researchers aiming to improve adversarial robustness through data preprocessing techniques.\n2. The discovery of the \"multi-round compression\" approach, though simple, provides valuable ideas for lightweight defense strategies."}, "weaknesses": {"value": "1. The motivation behind the use of block-based SSIM difference and pooling to calculate local artifacts in the local optimization module is unclear. Additionally, the parameter P has not been subjected to hyperparameter analysis.\n2. The design of the evaluation metrics in Section 5.3 is somewhat arbitrary. It lacks integration with subjective assessments or perceptual experiments to guide metric selection.\n3. The definitions of the symbols in the formulas are unclear. For example, in Section 4.3, the symbols P, u, and v are not sufficiently explained, making them difficult to understand.\n4. The paper lacks image examples of adversarial samples generated using different input methods. There is also an absence of comparative results at different bit rates and across different datasets."}, "questions": {"value": "1. Is the attack performance sensitive to the block size PPP and pooling kernel size (e.g., 32)? Has hyperparameter analysis been conducted?\n2. In the alternating use of symbolic gradients and normalized gradients, is the frequency of changes fixed or dynamically adjusted? Is there a design to adaptively adjust it based on the loss function?\n3. Does your attack framework remain effective when applied to NIC models that have been trained with defense techniques?\n4. What is the transferability of adversarial samples generated by your attack framework? For example, if adversarial samples are generated for one NIC model, can they successfully attack another NIC model? Have such experiments been conducted?\n5. Why did you choose to use SSIM rather than other perceptual metrics such as LPIPS or DISTS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sXsw4TsqjS", "forum": "qfeSpu5FBE", "replyto": "qfeSpu5FBE", "signatures": ["ICLR.cc/2026/Conference/Submission25459/Reviewer_BrFV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25459/Reviewer_BrFV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816709256, "cdate": 1761816709256, "tmdate": 1762943439805, "mdate": 1762943439805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a modular adversarial attack framework tailored for neural image compression (NIC) models. The framework combines global distortion maximization, local artifact amplification, and frequency-domain masking techniques. Experimental results on widely used codecs, such as JPEG AI, demonstrate that the proposed method outperforms existing attacks across multiple compression models (including JPEG AI), producing higher distortion while maintaining imperceptibility."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The experiments are relatively systematic, involving a comparison of the representative learnable compression methods HiFiC and ELIC with the traditional JPEG method. These experiments confirm that the former provides better defense efficacy, offering insights for researchers aiming to improve adversarial robustness through data preprocessing techniques.\n2. The discovery of the \"multi-round compression\" approach, though simple, provides valuable ideas for lightweight defense strategies."}, "weaknesses": {"value": "1. The motivation behind the use of block-based SSIM difference and pooling to calculate local artifacts in the local optimization module is unclear. Additionally, the parameter PPP has not been subjected to hyperparameter analysis.\n2. The design of the evaluation metrics in Section 5.3 is somewhat arbitrary. It lacks integration with subjective assessments or perceptual experiments to guide metric selection.\n3. The definitions of the symbols in the formulas are unclear. For example, in Section 4.3, the symbols P, u, and v are not sufficiently explained, making them difficult to understand.\n4. The paper lacks image examples of adversarial samples generated using different input methods. There is also an absence of comparative results at different bit rates and across different datasets."}, "questions": {"value": "1. Is attack performance sensitive to block size P and pooling kernel size (e.g., 32)? Has hyperparameter analysis been conducted?\n2. In the alternating use of symbolic gradients and normalized gradients, is the frequency of changes fixed or dynamically adjusted? Is there a design to adaptively adjust it based on the loss function?\n3. Does your attack framework remain effective when applied to NIC models that have been trained with defense techniques?\n4. What is the transferability of adversarial samples generated by your attack framework? For example, if adversarial samples are generated for one NIC model, can they successfully attack another NIC model? Have such experiments been conducted?\n5. Why did you choose to use SSIM rather than other perceptual metrics such as LPIPS or DISTS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WLTr5GfDgd", "forum": "qfeSpu5FBE", "replyto": "qfeSpu5FBE", "signatures": ["ICLR.cc/2026/Conference/Submission25459/Reviewer_sFxV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25459/Reviewer_sFxV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882257756, "cdate": 1761882257756, "tmdate": 1762943439488, "mdate": 1762943439488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an adversarial attack on neural image compression models that operates in two stages: (1) global distortion optimization using MSE, and (2) local artifact amplification using SSIM-based block-wise analysis. The method alternates between signed and normalized gradient directions and includes a frequency-domain masking module for improved stealthiness. Experiments show the attack outperforms prior methods (I-FGSM, FTDA, SRDA) on both traditional NIC models and the modern JPEG AI standard."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. As NIC systems such as JPEG AI are being standardized and deployed, understanding their susceptibility to adversarial perturbations is of significant practical and academic importance. The focus on JPEG AI, a learned codec with real-world relevance—makes this work particularly impactful within the security and image compression communities.\n\n2. The proposed two-stage optimization strategy is conceptually clear and practically effective.\n\n3. The experimental evaluation is thorough and convincing. The authors conduct tests on multiple NIC architectures, including JPEG AI (BOP/HOP variants), and across several datasets such as Kodak, Cityscapes, and NIPS 2017."}, "weaknesses": {"value": "1. While the proposed method is well engineered and empirically effective, its conceptual novelty is limited. The two-stage optimization (MSE → SSIM) and gradient alternation strategies are primarily adaptations or combinations of existing adversarial attack techniques rather than fundamentally new algorithmic ideas. Consequently, the contribution is more engineering-oriented.\n\n2. The paper should include visualizations of the pre-encoding adversarial images (i.e., the perturbed inputs before compression) and their direct comparisons with the original images, since the most straightforward way to verify whether an attack is “imperceptible to the human eye” is to visually inspect the original versus the perturbed (pre-encode) images.\n\n3. The current attack objective maximizes the distortion between the adversarial and original reconstructions (as described in Section Problem Formulation), rather than the distortion with respect to the original image. This seems somewhat strange to me, as it does not directly measure the degradation of visual quality relative to the ground truth."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2ElQZAb6wH", "forum": "qfeSpu5FBE", "replyto": "qfeSpu5FBE", "signatures": ["ICLR.cc/2026/Conference/Submission25459/Reviewer_mbhN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25459/Reviewer_mbhN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997819796, "cdate": 1761997819796, "tmdate": 1762943439130, "mdate": 1762943439130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}