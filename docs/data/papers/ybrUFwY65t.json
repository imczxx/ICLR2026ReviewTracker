{"id": "ybrUFwY65t", "number": 13280, "cdate": 1758215958540, "mdate": 1759897449163, "content": {"title": "CDG-MAE: Learning Correspondences from Diffusion Generated Views", "abstract": "Dense correspondences are critical for applications such as video label propagation, but learning them is hard because of tedious and unscalable manual annotation needs. Self-supervised methods address this by using a cross-view pretext task, often modeled with a masked autoencoder, where a masked target view is reconstructed from an anchor view. However, acquiring effective training data remains a challenge - collecting diverse video datasets is costly, while simple image crops lack the necessary pose variations, underperforming video-based methods. This paper introduces CDG-MAE, a novel MAE-based self-supervised method that uses diverse synthetic views generated from static images via an image-conditioned diffusion model. We present a quantitative method to evaluate the local and global consistency of the generated views to choose the right diffusion model for cross-view self-supervised pretraining. These generated views exhibit substantial changes in pose and perspective, providing a rich training signal that overcomes the limitations of video and crop-based anchors. Furthermore, we enhance the standard single-anchor MAE setting to a multi-anchor masking strategy to increase the difficulty of the pretext task. CDG-MAE substantially narrows the gap to video-based MAE methods, while maintaining the data advantages of image-only MAEs.", "tldr": "", "keywords": ["Self-supervised Learning", "Diffusion", "Masked Autoencoders"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d2be6f4831105cf12961d1750666743191037ded.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CDG-MAE, a novel self-supervised pre-training method that addresses the data bottleneck in cross-view correspondence learning. The core innovation is leveraging views generated by an image-conditioned diffusion model to synthesize the rich pose and perspective variations typically only found in videos. Furthermore, the paper introduces quantitative consistency metrics (GS, LS, NPS) to guide the selection of the generative model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The central idea of replacing real video frames with synthesized views from a diffusion model is a high-leverage contribution.\n\nThe introduction of quantitative Consistency Metrics (GS, LS, NPS) to empirically validate the utility of different diffusion models for correspondence learning is valuable.\n\nThe method outperforms its image-based baseline, CropMAE, confirming that diffusion-generated data provides the necessary fine-grained variations that cropping lacks."}, "weaknesses": {"value": "The primary weakness lies in the dependence on a 2D image-to-image diffusion model for view generation. This generative process does not enforce multi-view geometric or topological consistency in 3D space. Specifically, in cases where new objects or details (e.g., the number of branches behind the monkey) should appear or disappear due to viewpoint change, the diffusion model may randomly hallucinate, add, or delete structural elements. This creates training pairs where no true 3D correspondence ground truth exists, forcing the network to fit \"generative noise\" rather than robust physical correspondence.\n\nIn addition, the proposed Consistency Metrics (GS/LS/NPS) are proxy metrics calculated in a feature space. They measure feature similarity but do not guarantee geometric fidelity. The strong correlation observed might merely indicate the generated views share the semantic style of real video frames, not their structural integrity."}, "questions": {"value": "While the features learned by CDG-MAE excel in pixel-level label propagation and show robustness to large temporal shifts, a question remains regarding the method's necessity compared to established geometric and hand-crafted feature methods (e.g., SIFT/RANSAC, dedicated optical flow networks like RAFT, or even SOTA 3D foundation models, e.g. Mat3R, VGGT, etc.）"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bJOUl3pDni", "forum": "ybrUFwY65t", "replyto": "ybrUFwY65t", "signatures": ["ICLR.cc/2026/Conference/Submission13280/Reviewer_9MqV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13280/Reviewer_9MqV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805622342, "cdate": 1761805622342, "tmdate": 1762923955897, "mdate": 1762923955897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CDG-MAE, a self-supervised cross-view learning framework that leverages diffusion-generated views. The method uses a self-supervised image-conditioned diffusion model to generate synthetic views with pose and perspective variations from static images, and introduces three quantitative consistency metrics (GS, LS, NPS) to assess the quality of these views. A multi-anchor masking strategy is incorporated into the cross-view MAE framework to enhance correspondence learning. The model is trained from scratch on ImageNet-1K and evaluated on DAVIS, VIP, and JHMDB label propagation benchmarks, showing performance superior to CropMAE and approaching that of video-based SiamMAE."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper explores the use of diffusion models to generate cross-view data for self-supervised MAE training, offering an alternative way to learn view-consistent representations from static images.\n- The study includes systematic ablations on diffusion model choice, number of anchors, masking ratios, and patch sizes, with consistent results and clear performance trends.\n- The method achieves performance close to video-based models when trained only on static images, showing feasibility under constrained conditions."}, "weaknesses": {"value": "- **Outdated motivation:** The central premise, that using video data is costly, is no longer convincing given the availability of large-scale open video datasets and efficient video generation models (e.g., Cosmos, HunyuanVideo, Wan). The motivation therefore is outdated and lacks contemporary relevance.\n- The image diffusion-generated views are uncontrolled and may not preserve true viewpoint or structural consistency. As a result, the model primarily learns perceptual similarity rather than genuine dense correspondences.\n- **Outdated training setup:** The model is trained from scratch on ImageNet-1K using a ViT-S/16 backbone, without leveraging strong existing off-the-shelf visual models such as DINOv3 or SigLIP2. This training design is misaligned with current practices in vision learning.\n- All experiments are restricted to classical label propagation benchmarks (DAVIS, VIP, JHMDB), without evaluation on broader correspondence or geometry-related tasks such as optical flow or depth consistency.\n- The claim that diffusion-generated views can replace videos holds only under a weak baseline setting and has not been validated with stronger pretrained encoders, making the conclusion less broadly meaningful."}, "questions": {"value": "Could the authors provide additional results using a recent pretrained backbone (e.g., DINOv3 or other 2025-era vision foundation models) to verify whether the proposed diffusion-generated view training still provides benefits under a stronger initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E6VeZV5X3y", "forum": "ybrUFwY65t", "replyto": "ybrUFwY65t", "signatures": ["ICLR.cc/2026/Conference/Submission13280/Reviewer_ECfG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13280/Reviewer_ECfG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811326734, "cdate": 1761811326734, "tmdate": 1762923955281, "mdate": 1762923955281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CDG-MAE is a self-supervised learning framework using Masked Autoencoders (MAEs) to learn visual correspondences by leveraging synthetic views produced by image-conditioned diffusion models. The paper addresses the limitations in current self-supervised methods by replacing less-diverse video data with more diverse diffusion-generated views. The paper introduces metrics for evaluating the utility of generated views. The proposed multi-anchor masking strategy enhances MAE training difficulty and effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of introducing self-supervision diversity through diffusion-generated images is interesting and addresses well-identified issues of crop and video strategies.\n- The proposed multi-anchor and anchor masking techniques are sound and seem to be effective.\n- The ablation on the design choices is solid and covers a lot of variables.\n- The proposed model achieves the state of the art in most of the metrics, proving the performance claims. The authors show that their approach closes the gap to models trained on video.\n- Together with the appendix, the experimental setup is well documented, including all hyperparameters."}, "weaknesses": {"value": "- There is not a lot of discussion on the choice of the diffusion model. The authors have chosen an augmentation model. I wonder if novel view models (e.g. ViewCrafter) were considered. It would be a great comparison, and such an approach could enable control over the camera pose. \n- It is not fully clear what the impact of separate components is. You could potentially apply multi-anchor and anchor masking to the CropMAE approach and investigate how that affects the performance.\n- I would like to see the results with Lumos, which shows high correlation with video samples in Table 4.\n- It is my understanding that the main use of the proposed metrics is assessing the quality/utility of the generated views. This was used as guidance for selecting the diffusion model. Can it be used to select the best examples for the bag of views? Could that improve the performance?"}, "questions": {"value": "- What is the expectation of using your approach on the Kinetics dataset? Could you augment the video frames to improve the training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "89oox7GNEr", "forum": "ybrUFwY65t", "replyto": "ybrUFwY65t", "signatures": ["ICLR.cc/2026/Conference/Submission13280/Reviewer_b1L9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13280/Reviewer_b1L9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961147029, "cdate": 1761961147029, "tmdate": 1762923954616, "mdate": 1762923954616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CDG-MAE, a self-supervised framework that combines diffusion-based novel view generation with a multi-anchor masked autoencoder. The method replaces costly video data with diffusion-generated synthetic views that introduce pose and perspective diversity while preserving global consistency. A new multi-anchor masking strategy improves task difficulty and representation robustness. Empirically, CDG-MAE outperforms image-only methods such as CropMAE and narrows the gap to video-based SiamMAE on multiple label-propagation benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Creative use of diffusion for correspondence learning, addressing the lack of video data for cross-view pretraining.\n- Multi-anchor masking is a well-motivated and effective extension to SiamMAE.\n- Comprehensive experiments show consistent gains across three datasets, with strong ablations on masking ratios and diffusion backbones."}, "weaknesses": {"value": "- The technical novelty mainly lies in the proposed consistency metrics (GS–LS–NPS) for selecting diffusion-generated views, but their contribution is not deeply analyzed (e.g., what if LS is omitted, or completely remove this metric or GS alone suffices?).\n- Other elements (diffusion-based augmentation, Siamese MAE) are incremental combinations of prior work (Gen-SIS, CropMAE).\n- The experimental organization could be improved by presenting the main comparison table earlier."}, "questions": {"value": "- Why can diffusion-generated views, which are often inconsistent, still yield effective correspondences? \n- Please provide ablations isolating each consistency metric (GS, LS, NPS).\n- Move the main results table earlier for clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5HiTpYKJEF", "forum": "ybrUFwY65t", "replyto": "ybrUFwY65t", "signatures": ["ICLR.cc/2026/Conference/Submission13280/Reviewer_WBZA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13280/Reviewer_WBZA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996303889, "cdate": 1761996303889, "tmdate": 1762923954340, "mdate": 1762923954340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}