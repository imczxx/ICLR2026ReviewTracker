{"id": "X4AN816egl", "number": 2450, "cdate": 1757090818265, "mdate": 1762999940881, "content": {"title": "XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on A Unified Cylinder Lifting Method", "abstract": "Recently, more attention has been paid to feedforward reconstruction paradigms, which mainly learn a fixed view transformation implicitly and reconstruct the scene with a single representation. However, their generalization capability and reconstruction accuracy are still limited while reconstructing driving scenes, which results from two aspects: (1) The fixed view transformation fails when the camera configuration changes, limiting the generalization capability across different driving scenes equipped with different camera configurations. (2) The small overlapping regions between sparse views of the $360\\degree$ panorama and the complexity of driving scenes increase the learning difficulty, reducing the reconstruction accuracy. To handle these difficulties, we propose **XYZCylinder**, a feedforward model based on a unified cylinder lifting method which involves camera modeling and feature lifting. Specifically, to improve the generalization capability, we design a Unified Cylinder Camera Modeling (UCCM) strategy, which avoids the learning of viewpoint-dependent spatial correspondence and unifies different camera configurations with adjustable parameters. To improve the reconstruction accuracy, we propose a hybrid representation with several dedicated modules based on newly designed Cylinder Plane Feature Group (CPFG) to lift 2D features to 3D space. Extensive experimental results show that XYZCylinder achieves state-of-the-art performance under different evaluation settings, and can be generalized to other driving scenes in a zero-shot manner. Anonymous project page: [here](http://xyzcylinder.online/).", "tldr": "", "keywords": ["Feedforward", "3D Reconstruction", "3D Gaussian Splatting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/87d422943ed094c27d8d0aaf461db18af71007d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Recent attention has focused on feedforward reconstruction paradigms, which implicitly learn fixed view transformations and reconstruct scenes with a single representation. However, these paradigms have limited generalization (due to fixed view transformations failing with changed camera configurations) and reconstruction accuracy (due to sparse panorama view overlaps and complex driving scenes) in driving scenarios. To address these, the authors propose XYZCylinder, a feedforward model with a unified cylinder lifting method (including camera modeling and feature lifting). Specifically, it uses Unified Cylinder Camera Modeling (UCCM) to improve generalization by unifying camera configurations, and a hybrid representation with Cylinder Plane Feature Group (CPFG) to boost accuracy by lifting 2D features to 3D. Experiments show it achieves SOTA performance and zero-shot generalization to other driving scenes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A large number of methods were compared on multiple datasets.\n2. The methods were clearly expressed.\n3. The formulas and diagrams were clearly defined and presented."}, "weaknesses": {"value": "This feedback raises four key criticisms and suggestions for the paper, focusing on **model complexity, rendering limitations, missing efficiency metrics, and mismatched advantages for target applications**:  \n\n1. **Model & Realism Issues**: The model’s design is overcomplicated, and its rendering realism lags behind optimization-based methods (e.g., 3DGS). It also only supports low resolution and a limited number of input images.  \n2. **Practical Applicability Gap**: The low rendering quality fails to meet practical needs like camera simulation; additionally, the model relies on depth map ground truth for training—unlike competitors (Pixelsplat, MVSpalt) that avoid this, restricting its real-world use.  \n3. **Missing Efficiency Analysis**: The paper claims a feed-forward approach but lacks critical metrics like inference latency, running time, and memory consumption. Competitors (MVSplat, PixelSplat) include these to prove efficiency, so this analysis is needed for fair comparison.  \n4. **Mismatched Advantages for Target Use Case**: For the stated application (autonomous driving real-world simulators), offline scene-optimized 4D reconstruction is already acceptable (no high efficiency demands). Meanwhile, the feed-forward method’s key strength (generalization) does not offset its lower reconstruction accuracy compared to scene-optimized approaches."}, "questions": {"value": "The problems are all listed above. Moreover, the scope for innovation in methods is limited."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dzloz80tc1", "forum": "X4AN816egl", "replyto": "X4AN816egl", "signatures": ["ICLR.cc/2026/Conference/Submission2450/Reviewer_tf9T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2450/Reviewer_tf9T"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761222513096, "cdate": 1761222513096, "tmdate": 1762916242186, "mdate": 1762916242186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "AeP3AMG5GO", "forum": "X4AN816egl", "replyto": "X4AN816egl", "signatures": ["ICLR.cc/2026/Conference/Submission2450/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2450/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762999940067, "cdate": 1762999940067, "tmdate": 1762999940067, "mdate": 1762999940067, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes XYZCylinder, a feedforward 3D reconstruction framework for driving scenes based on a Unified Cylinder Lifting Method (UCCM). The system replaces learned view transformations with an explicit cylindrical camera model and introduces a hybrid architecture consisting of three main modules: an occupancy-aware network to prune empty voxels, a volume-aware network to generate Gaussian primitives, and a pixel-aware network for texture refinement. \n\nThe authors claim that this design improves both reconstruction accuracy and generalization across different camera configurations. Experiments are conducted on nuScenes and Carla-Centric datasets, with qualitative zero-shot evaluations on other driving datasets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed UCCM is an interesting attempt to unify camera geometry across multi-view driving data.\n\n2. The experimental results show consistent improvements over existing feedforward 3DGS baselines (e.g., Omni-Scene, DrivingForward) on nuScenes and Carla-Centric."}, "weaknesses": {"value": "1. **Too much hand-crafted complexity.**  \nThe overall architecture feels over-engineered. For instance, the clockwise/counter-clockwise flipping and feature-fusion design adds a lot of manual logic but lacks evidence that it’s actually necessary. Simple feature averaging or learned fusion might achieve similar results with much simpler design. The paper would benefit from clearer ablations to justify these choices.\n2. **Occupancy module is an engineering trick.**  \nThe occupancy-aware network seems to exist mostly to reduce GPU memory use, not as a conceptual innovation. The authors do not quantify how much memory it saves or how it scales, which makes the contribution feel more like a system workaround than a core method.\n3. **Foreground/background decoupling is awkward.**  \nThe method treats the foreground (modeled by 3DGS) and the background (generated by a RGB decoder) separately, then fuses them for rendering. This design makes the pipeline incompatible with other 3DGS methods that produce a unified scene representation.\nIf the goal is actually novel-view synthesis, the paper should compare against works like ViewCrafter[1] or GEN-3C[2], which also synthesize images directly rather than reconstruct explicit 3DGS. As it stands, the comparison set does not match the claimed task.\nAnd it’s never explained how the system decides what counts as foreground versus background, through semantic masks? depth thresholding? heuristics? This missing detail is important, since the whole pipeline depends on it.\n4. **Poor Evaluation**  \nThe central claim is zero-shot generalization across datasets, but this is only shown through qualitative figures, not quantitative evaluation. Those results should have been the main experiment, yet the paper focuses mostly on in-domain reconstruction metrics (PSNR/SSIM). Without solid cross-dataset numbers, the generalization claim is weak.\n5. **Conceptual novelty is overstated.**  \nMuch of the system relies on pre-trained models (Radio-v2.5, Metric3D-v2) and heuristic tuning of geometric parameters (ρ, ∆h). The main novelty is a deterministic projection mapping, is presented as an improvement over learned projections, yet no direct comparison is provided to support this claim.\n\n---\n\n[1] Yu, W., Xing, J., Yuan, L., Hu, W., Li, X., Huang, Z., Gao, X., Wong, T.-T., Shan, Y., & Tian, Y. (2025). ViewCrafter: Taming video diffusion models for high-fidelity novel view synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence.  \n[2] Ren, X., Shen, T., Huang, J., Ling, H., Lu, Y., Nimier-David, M., Müller, T., Keller, A., Fidler, S., & Gao, J. (2025). GEN3C: 3D-Informed world-consistent video generation with precise camera control. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)."}, "questions": {"value": "The paper is poorly written. Although it presents a substantial amount of work, much of it feels confusing, poorly organized, and driven by heuristics rather than by clear necessity. The authors should consider rewriting the paper with clearer motivation, better organization, and a more coherent overall flow."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "84jz08KmGA", "forum": "X4AN816egl", "replyto": "X4AN816egl", "signatures": ["ICLR.cc/2026/Conference/Submission2450/Reviewer_snvb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2450/Reviewer_snvb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776383992, "cdate": 1761776383992, "tmdate": 1762916241622, "mdate": 1762916241622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes XYZCylinder, a feed-forward 3D scene reconstruction pipeline for surround-view driving data. The core idea is a Unified Cylinder Camera Modeling (UCCM) that projects all camera views to a cylindrical plane with shared parameters (center (c_u^x), radius (R_u), height (Z_u), resolution). Two composited cylinder-plane features (F^+{cy}) and (F^-{cy}) are built by overlaying views in clockwise and counter-clockwise orders using an ordered composition operator (\\gamma) (overwrite-on-nonzero). Foreground is reconstructed with three branches (occupancy/volume/pixel) that lift features into CPFGs; background is rendered separately and then fused. The model uses strong 2D backbones (Radio-v2.5) and depth cues (Metric3D-v2). On nuScenes/Carla-Centric, the paper reports modest SOTA improvements (e.g., PSNR 24.97 vs. 24.11 for Omni-Scene on nuScenes)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The cylinder center is the mean camera position with a small height offset; effective vertical FOV is set to the dataset’s minimum (scaled by (\\rho)) to avoid top/bottom holes during projection; the radius is derived analytically. This is simple and likely robust across rigs.  \n2. Foreground/background decoupling and the occupancy-aware gating are sensible; the ablation shows that removing occupancy causes OOM and hurts metrics, supporting the design. \n3.  The paper explicitly lists inter-dataset camera differences and tunes UCCM parameters per dataset; for Waymo it introduces symmetric virtual cameras."}, "weaknesses": {"value": "1. The proposed method integrates numerous modules, yet the overall gains—especially on the nuScenes dataset—are modest. Coupled with the lack of complexity and runtime comparisons (e.g., parameters, FLOPs, latency), the work appears to require substantial engineering effort for limited benefit.\n2. The experimental results do not substantiate the stated motivation. While the Introduction emphasizes generalization, the observed generalization is very likely driven by the powerful base models rather than by the contribution of the proposed method itself.\n3. The ordered composition is formally defined, and the projection math in the appendices appears correct. However, without runtime numbers and without ablations on the composition operator, it is hard to judge practical efficiency and robustness.  \n4. Several implementation choices (∆h/ρ rationale; details of how the two traversal orders influence occlusion disambiguation) should be better motivated."}, "questions": {"value": "1.\tWhy introduce a small height offset (Δh)? What is its role?\n\n2.\tWhy does using the minimum vertical FoV alleviate multi-camera mismatch? What is the underlying mechanism?\n\n3.\tWhat is the rationale for “overwriting the existing ones with the incoming ones” during composition? Have you compared against averaging, max pooling, confidence-/depth-weighted blending, etc.?\n\n4.\tHow do the last clockwise/counter-clockwise views—i.e., the two composites F_cy^+ and F_cy^−—together with YNet_occ help distinguish occupied vs. empty space?\n\n5.\tEfficiency reporting is incomplete. The appendix lists training resources (8×L40S; ≈2 days for occupancy, ≈5 days for reconstruction; ~32 GB per GPU) but not parameter counts, FLOPs, end-to-end latency, or FPS. Please provide: total parameters; training/inference FLOPs; end-to-end per-frame latency (including feature extraction, projection, the three branches, and rendering); peak memory; and a speed–quality comparison against PixelSplat/MVSplat/DrivingForward/Omni-Scene.\n\n6.\tBackbone and external depth dependence. You use Radio-v2.5 as the 2D backbone and Metric3D-v2 for depth/confidence to produce depth-augmented F̄_cy^±. Please report performance and speed with lighter backbones and with no external depth, to isolate the contribution of UCCM/CPFG.\n\n7.\tZero-shot attribution. It remains unclear whether the zero-shot gains come primarily from the backbone rather than the proposed method. Please clarify with appropriate controls/ablations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TTHsm0Wcgq", "forum": "X4AN816egl", "replyto": "X4AN816egl", "signatures": ["ICLR.cc/2026/Conference/Submission2450/Reviewer_J1JH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2450/Reviewer_J1JH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872152563, "cdate": 1761872152563, "tmdate": 1762916240921, "mdate": 1762916240921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on a Unified Cylinder Lifting Method” proposes a novel feedforward 3D reconstruction framework tailored for autonomous driving. Unlike conventional NeRF-style iterative optimization or fixed-view feedforward models, XYZCylinder introduces a Unified Cylinder Camera Modeling (UCCM) strategy that explicitly maps multi-view inputs into a shared cylindrical coordinate system, enabling zero-shot generalization across different camera configurations. The model employs a hybrid representation consisting of occupancy-, volume-, and pixel-aware modules organized via a Cylinder Plane Feature Group (CPFG), allowing fine-grained geometry and texture reconstruction. Overall, I like the motivations and contributions of this work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Novel Unified Cylinder Modeling. The proposed UCCM elegantly handles heterogeneous multi-camera configurations, offering an explicit, geometry-consistent mapping that enables cross-dataset and zero-shot reconstruction — a significant step beyond view-specific feedforward models.\n\n+ Hybrid 3D Representation Design. The combination of occupancy-, volume-, and pixel-aware modules yields a balanced trade-off between geometric completeness and photometric fidelity, outperforming both volumetric and pixel-based baselines.\n\n+ Strong Empirical Results and Analysis. The paper provides comprehensive quantitative and qualitative evaluations, including detailed ablations on module contributions and network design (e.g., dual-branch encoders), establishing clear evidence for the claimed improvements and generalization ability."}, "weaknesses": {"value": "- While the architecture achieves high performance, the model’s modular and multi-branch design (YNet, XNet, ZNet) is intricate, which might hinder reproducibility and limit theoretical interpretability of the learned representations.\n- The cylinder-style perspective has been widely explored in the 360 panoramic field. For example, \"Eliminating the Blind Spot: Adapting 3D Object Detection and Monocular Depth Estimation to 360◦ Panoramic Imagery\" adapts contemporary deep network architectures developed on conventional rectilinear imagery, \"Cylin-Painting: Seamless 360° Panoramic Image Outpainting and Beyond\" and \"PanoDiffusion: 360-degree Panorama Outpainting via Diffusion\" use a cylinder-like convolution or circular padding to ensure the seamless content generation of panoramas. The brief discussions of these works are also expected to be presented in this work.\n- Could the proposed hybrid representation be extended beyond driving scenes (e.g., indoor or aerial settings), or are there inherent assumptions tied to the cylindrical geometry of street-view cameras?"}, "questions": {"value": "The unified cylinder projection depends on adjustable parameters. How sensitive is the model performance to these hyperparameters, and can they be automatically inferred or adapted during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xPza2heJaq", "forum": "X4AN816egl", "replyto": "X4AN816egl", "signatures": ["ICLR.cc/2026/Conference/Submission2450/Reviewer_owgh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2450/Reviewer_owgh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882544680, "cdate": 1761882544680, "tmdate": 1762916240684, "mdate": 1762916240684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}