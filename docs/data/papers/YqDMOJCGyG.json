{"id": "YqDMOJCGyG", "number": 8018, "cdate": 1758052004883, "mdate": 1759897814055, "content": {"title": "Fast and Accurate Fisher-Guided Quantization via Efficient Kronecker Factor Approximation", "abstract": "Quantization with second-order information has shown strong promise for preserving model quality under aggressive compression. Building on the recent YAQA framework, which employs Kronecker-factored approximations of the Hessian via a power-iteration technique, we propose an alternative approach that replaces this step with a more efficient Kronecker decomposition method from GFWSVD. This formulation preserves the benefits of second-order curvature-aware quantization while substantially reducing computational cost.\n\nWe apply our method to LLaMA-2 7B, LLaMA-3 8B Instruct, Qwen 3 8B Instruct and demonstrate that it achieves the same post-quantization model quality as YAQA, but with significantly faster computational process — the Kronecker factors which provide the required quality was obtained with 10 times fewer tokens and approximately a $10\\times$ speedup over the original work.", "tldr": "We propose an accelerated Fisher information decomposition for the YAQA method, achieving the same quantization accuracy with ~10× lower computational cost.", "keywords": ["Quantization", "Large Language Models", "Fisher Information Matrix", "Kronecker-Factored Approximation", "Model Compression", "Second-Order Methods"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9052f36d47cd734b541c50ea0524efc02c28fdfa.pdf", "supplementary_material": "/attachment/2887ae2de6ae9af0fab7a3d8c00628e76722cfe0.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a different way of estimating the Hessian for the YAQA rounding algorithm. The proposed method is cheaper to compute and slightly worse than YAQA's Sketch A, and worse than YAQA's Sketch B."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed Hessian sketch is much cheaper than both of YAQA's Hessian sketches.\n- There is some nice analysis on the convergence rate of power iteration and the Hessian sketch used."}, "weaknesses": {"value": "- This paper seems somewhat incomplete and hastily written. There are relatively few empirical results, the Appendix is almost nonexistent (although this is not an issue on its own, it is odd that a full conference submission would have almost no Appendix), and some of the names in the bibliography are literally wrong.\n- The main contribution of the paper is taking the Hessian sketch from GFWSVD (Chekalina et al. (2025)) and applying it in the YAQA framework. This sketch directly estimates the Fisher, whereas Sketch A (the worse of the two YAQA sketches and the main baseline here) is actually a biased estimate of the Fisher. Sketch B would be a conceptually closer baseline to the one presented here, and Sketch B performs much better than this."}, "questions": {"value": "- The paper says that it introduces FastKron, but L287 implies that FastKron was introduced in an earlier code. Furthermore, the authors link to a public repo for the GFWSVD paper. Is FastKron new or not?\n- My score is partly based off the assumption that FastKron is from GFWSVD, which implies that there isn't actually much going on in this paper. It combines GFWSVD and YAQA, both of which are existing works in the literature. Perhaps the authors can clarify this detail."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5RaOXuBLwc", "forum": "YqDMOJCGyG", "replyto": "YqDMOJCGyG", "signatures": ["ICLR.cc/2026/Conference/Submission8018/Reviewer_AkSJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8018/Reviewer_AkSJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761076412767, "cdate": 1761076412767, "tmdate": 1762920019273, "mdate": 1762920019273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work modifies and improves the YAQA algorithm of Tseng et al (2025b), introducing a method called FastKron which uses Lanczos iteration in place of plain power iteration to estimate Kronecker factors of a structured approximation to the Hessian of loss w.r.t. weights. FastKron is shown to have faster convergence than power iteration when the largest two eigenvalues are close. Practical results for YAQA quantisation of pretrained LLMs using QTIP show FastKron achieving similar downstream accuracy to \"Sketch A\" from the YAQA paper, with fewer tokens and much shorter wall clock time.\n\n---\n_Tseng et al (2025b): Model-Preserving Adaptive Rounding (YAQA)_"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The technique is well-motivated, and the core idea is sound. In particular, it is sensible to look for a scheme with faster convergence than power iteration, saving PTQ compression time. The wall clock speedups presented are substantial.\n\nEquations, tables and plots are generally clear, and the paper includes all of the structural elements required to back up the author's claims. The work makes the contribution clear, outlining the provenance of ideas from Tseng et al. (2025b) and Chekalina et al. (2025), which are combined in FastKron.\n\n---\n_Chekalina et al. (2025): Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models (GFWSVD)_"}, "weaknesses": {"value": "My main concern with this work is that it is an insufficient step on top of the YAQA and GFWSVD component techniques to be of interest to the community as a full conference paper. Specifically, neither the method nor results convey enough new information for me to recommend acceptance. If the method of FastKron were supplemented by a substantial theoretical insight and excellent empirical investigation, this challenge of incrementalism could be overcome, but the theory and experiments do not meet this bar.\n\nSpecific concerns:\n - The method is not clearly and comprehensively explained. The reader must piece together Eqn. 4, as a fixed-point iteration equation, with Algorithm 1, where \"leading singular triplet\" line incorporates the Lanczos algorithm, and relies on the fact that $\\tilde{I}_F \\mathbf{z}$ can be computed efficiently using the method described in Chekalina et al. (2025), Section 4.1.\n - Algorithm 1 is functionally identical to Chekalina et al., Algorithm 1, so does not help demonstrate the unique contribution of FastKron over GFWSVD.\n - Theorem 3.1 is not demonstrated as stated, at least by Step 3. I think where it says \"for any q ... strictly smaller error\" should read e.g. \"for a uniform distribution of $q \\in [0, 1]$ ... smaller expected error\", unless I misunderstand the result.\n   - If this is so, I question the relevance of this theorem - there is no reason given to suspect a uniform distribution of spectral gap, and Figure 1a indicates the spectral gap is non-uniform in practice.\n - At least one result (LLaMA-2 7B, AVG Zero-Shot, 2-bit) appears to use an overfitted choice of calibration token count. In Figure 2, the performance varies considerably, and 1400K tokens outperforming ~1900K tokens. In Table 1, the 1400K token result is reported with a performance consistent with the Figure 2 peak, and highlighted in bold as outperforming the Sketch A baseline. It is hard to reach the same conclusion regarding FastKron vs Sketch A from inspecting the table and figure, highlighting the poor experimental practice. If selecting token count from a search, this search should be performed on a validation set, or based on the intrinsic YAQA objective, or reduced into a simple procedure for choosing the number of steps, which can be consistently applied across models.\n - It isn't clear why YAQA Sketch B is not included in the results, as this seems to be a relevant baseline.\n - Tseng et al. (2025b) claim in Section 3.2.1 that Sketch A takes around 20 GPU-hours for a 10B model / 20M tokens, where this work measures 50 or 92 hours (Tables 1, 2), with no explanation offered for the difference.\n\nMinor concerns:\n - Eqn. 6 seems to miss $\\left<\\right>$ when compared with Tseng et al. (2025b), Eqn 8, 9.\n - To help the reader follow the derivation of Theorem 3.1, the paper would benefit from an algorithmic view of the Lanczos method to supplement the high-level explanation of L211 - showing the concrete steps involved.\n - $m$ steps of Lanczos iteration is introduced in Eqn 11, presumably no relation to $m$ of L043, then substituted with $k$ steps in Figure 1b and Eqn 14. This is confusing --- if it's functionally identical to $k$, this should be used consistently.\n - Definitions of these symbols: unclear $\\mathcal{R}$ (Algorithm 1), unnecessary/unclear: $g$, $p_k$, $f$."}, "questions": {"value": "I would appreciate any clarifications or responses to the points listed above as \"main concern\" and \"specific concerns\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6XkvFqcmLJ", "forum": "YqDMOJCGyG", "replyto": "YqDMOJCGyG", "signatures": ["ICLR.cc/2026/Conference/Submission8018/Reviewer_bCLe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8018/Reviewer_bCLe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761084997749, "cdate": 1761084997749, "tmdate": 1762920018962, "mdate": 1762920018962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposes replacing the power iteration method in YAQA with FastKron (renamed from GFWSVD), a more efficient Kronecker decomposition method. Experiments on a wide range of models suggest that their method is $10\\times$ faster in obtaining the Kronecker factors while achieving the same downstream accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. FastKron speeds up Kronecker-factor estimation by $10\\times$ while achieving the same downstream accuracy.\n1. This is a bridge paper that connects the modern LLM quantization algorithms with numerical linear algebra.\n1. Section 3 provides theoretical justification for why FastKron (Lanczos-base) is faster than the power iteration used in YAQA: it's because the spectral gaps of real LLM layers fall into the regime where Lanczos-based methods perform better.\n1. Section 6 explores the relationship between Kron’s downstream performance to the number of tokens used during calibration."}, "weaknesses": {"value": "1. FastKron/GFWSVD was already proposed in [another paper](https://arxiv.org/abs/2505.17974), which significantly undermines the novelty of this paper.\n1. Speeding up the quantization process (as opposed to dequantization/inference/decoding) has limited impact, because each model only needs to be quantized once up front.\n1. Insufficient baselines: the authors didn't compare FastKron against other Fisher Kronecker factor estimators like K-FAC, EKFAC, FWSVD, and TFWSVD.\n1. Insufficient experiments: all experiments are ran on 8B models, but ideally we want to see the performance across different scales."}, "questions": {"value": "1. In Tables 1 through 3, did you mean to say \"GPU*h\", i.e., GPU hours, instead of \"GPU/h\"?\n1. In Tables 1 though 3, when/why does \"No Hess\" occasionally perform as good as Hessian-based quantizers?\n1. In Table 3, why does FastKron only get a $2\\times$ speedup on Qwen3-8B? Is it because it has a difference spectral gap structure, or context length difference, or is there anything special about its architecture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RCUqYMt8Dr", "forum": "YqDMOJCGyG", "replyto": "YqDMOJCGyG", "signatures": ["ICLR.cc/2026/Conference/Submission8018/Reviewer_bWh3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8018/Reviewer_bWh3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957625000, "cdate": 1761957625000, "tmdate": 1762920018556, "mdate": 1762920018556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to improve the computational cost of the YAQA quantization framework by replacing the power iteration method with FastKron to compute the Kronecker product approximation of the Hessian. Theoretically, they show that the Lanczos method converges faster than the power iteration method. They experimentally evaluate their method on LLaMA-2 7B, LLaMA-3 8B Instruct, Qwen 3 8B Instruct to show the benefits."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. They propose a novel method to improve the computational cost of YAQA method\n2. The convergence rates of the power iteration method and Lanczos method are analyzed theoritically\n3. Their method is evaluated emperically on three models on downstreams tasks and perplexity on C4 and WikiText."}, "weaknesses": {"value": "1. The Algorithm 1 and the theoretical analysis seem heavily borrowed from prior work.\n2. No comparision to other baseline methods \n3. Their method performs worse on the perplexity of Llama 2 class of models"}, "questions": {"value": "1. How does the FastKron-based method scale for even larger models? How does it scale for smaller models?\n2. Is the method limited only to Language models? How does it work for Diffusion or Flow models for example\n3. Does the calibration dataset or its size matter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F0z7ycw1BR", "forum": "YqDMOJCGyG", "replyto": "YqDMOJCGyG", "signatures": ["ICLR.cc/2026/Conference/Submission8018/Reviewer_KBG6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8018/Reviewer_KBG6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979127261, "cdate": 1761979127261, "tmdate": 1762920017890, "mdate": 1762920017890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}