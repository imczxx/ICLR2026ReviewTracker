{"id": "UhwbIblavX", "number": 180, "cdate": 1756730435909, "mdate": 1763086585733, "content": {"title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation", "abstract": "A fundamental challenge in embodied intelligence is developing expressive and compact state representations for efficient world modeling and decision making. However, existing methods often fail to achieve this balance, yielding representations that are either overly redundant or lacking in task-critical information. We propose an unsupervised approach that learns a highly compressed two-token state representation using a lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong generative prior. Our representation is efficient, interpretable, and integrates seamlessly into existing VLA-based models, improving performance by 14.3% on LIBERO and 30% in real-world task success with minimal inference overhead. More importantly, we find that the difference between these tokens, obtained via latent interpolation, naturally serves as a highly effective latent action, which can be further decoded into executable robot actions. This emergent capability reveals that our representation captures structured dynamics without explicit supervision. We name our method StaMo for its ability to learn generalizable robotic Motion from compact State representation, which is encoded from static images, challenging the prevalent dependence to learning latent action on complex architectures and video data. The resulting latent actions also enhance policy co-training, outperforming prior methods by 10.4% with improved interpretability. Moreover, our approach scales effectively across diverse data sources, including real-world robot data, simulation, and human egocentric video.", "tldr": "", "keywords": ["Computer Vision", "Generative Model", "Embodied AI", "Unsupervised Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b9c09d6afb059616383df2a11a61cd097608aedf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present a way of extracting \"latent\" states from images in VLA training, analogous to LAPA. The idea is to simply encode the \"state\" of an image as tokens from a diffusion auto-encoder trained via flow-matching. Importantly, parts of the auto-encoder are initialized from pretrained models like Dino and Stable Diffusion. These \"latent states\" can be used as labels for adding a world modeling loss to a base VLM, which improves performance as per Table 3. It can also be used for \"co-training\" on video-only data, which improves performance as per Table 4. The authors show that these gains exist in the real world (Table 5) and provide some qualitative and linear probing experiments on the representations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea is simple and well motivated\n- The authors do multiple evaluations in real and sim\n- The paper is very well written"}, "weaknesses": {"value": "- The novelty is limited\n- Comparison to baseline methods like Flare is missing. Does the proposed method beat Flare in e.g,. Table 3?\n- It is not clear why state compactness is desirable."}, "questions": {"value": "1. Fig 1 says \"Image Sapce\", please fix this.\n2. How come the inference speed is unaffected in table 2? A whole forward pass of the image encoder is needed right?\n3. Why are you optimizing for state-compactness? Isn't inference speed a better benchmark to target?\n4. Why is both L1 and L2 loss used in equation 2?\n5. Could you try a stronger base VLA than openVLA?\n6. In table 5, for the second to last row, the value is ~10x larger than other rows. Is this a typo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6m4LOMnOyy", "forum": "UhwbIblavX", "replyto": "UhwbIblavX", "signatures": ["ICLR.cc/2026/Conference/Submission180/Reviewer_cJUi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission180/Reviewer_cJUi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761342341001, "cdate": 1761342341001, "tmdate": 1762915463298, "mdate": 1762915463298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We would like to express our sincere gratitude to the four reviewers for carefully reading our manuscript and providing valuable comments and suggestions. After thorough consideration, we have decided to withdraw the current version of the paper in order to further polish it. We plan to supplement additional experiments and reformulate the paper to address potential misunderstandings and improve its overall quality. Thank you again for your time and insightful feedback！"}}, "id": "UHR9vMHYQz", "forum": "UhwbIblavX", "replyto": "UhwbIblavX", "signatures": ["ICLR.cc/2026/Conference/Submission180/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission180/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission180/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763086574857, "cdate": 1763086574857, "tmdate": 1763086574857, "mdate": 1763086574857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "UtNfT2XBTg", "forum": "UhwbIblavX", "replyto": "UhwbIblavX", "signatures": ["ICLR.cc/2026/Conference/Submission180/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission180/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763086584701, "cdate": 1763086584701, "tmdate": 1763086584701, "mdate": 1763086584701, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an unsupervised diffusion-autoencoder that compresses a single image observation into an extremely compact two-token state (each 1024-D), using a frozen DINOv2 encoder with a lightweight transformer “compressor” and a DiT decoder trained with flow-matching. The core claim is that robot motion emerges as the simple difference of two states. The authors use StaMo states and motions as targets for world-modeling heads on VLA policies, perform policy co-training by generating pseudo-actions from unlabeled videos via state differences, and report both simulation and real-robot gains with low inference overhead. Experimental results validates the effectiveness of the proposed method in simulated and real-world tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Good visibility of figures\n* Easy to read\n* This paper provide evaluation results both on simulated and real-world robotics benchmark"}, "weaknesses": {"value": "* Major concern on its limited novelty\n    * While the paper is written well, the proposed method seems to barely have their unique contributions. \n        * This paper slightly contributed beyond applying Zhao et al. [1]. Specifically, I understand StaMo as a naive extension of the prior work (Zhao et al. [1]) to the robot tasks. Lines 209-214 also indicate that StaMo directly employ Zhao et al.'s method, with slight architectural changes, as described in Lines 215-218. Consequently, this seems more like an evaluation of prior work into new benchmark in robot tasks. While I also agree that this paper contributed in terms of evaluation, its contribution in substantive methodological or conceptual improvements is weak\n        * The above aspect is further revealed in the following elements:\n            * Among four subsections in the Method section, only Section 3.1 and 3.2 cover methodological explanation and detail of StaMo itself, which do not exceed even 1 page. Section 3.3 and 3.4 covers about how to adapt StaMo to robotics tasks\n            * Section 3.1 explains that StaMo employ Zhao et al. with Dinov2 and DiT, and adopt flow matching loss that commonly used in the previous works\n            * Moreover, even in the explanation in Section 3.1 and 3.2, most sentences only described existing studies rather than new mechanisms.\n                * The core claim in Section 3.2 is tha a_t = s_{t+1} - s_t. However, this simple claim is expanded into 16 lines. Given that \"feature gap between two consecutive frames can be used as motion features\" is widely understood that it can be employed without long supportive explanation, this lengthy explanation heightens concerns about limited originality\n\n* Clarity improvement in Figure 1\n    * gray, orange, blue tokens should be described at least in caption or figure.\n* Minor issues\n    * Erratum in Figure 1: \"Deocoder-Free\"\n\n[1] Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint arXiv:2407.16655, 2024."}, "questions": {"value": "* How did you compare the compactness and expressivity of the described methods in Figure 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8vZOoQXcK2", "forum": "UhwbIblavX", "replyto": "UhwbIblavX", "signatures": ["ICLR.cc/2026/Conference/Submission180/Reviewer_94H6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission180/Reviewer_94H6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964522147, "cdate": 1761964522147, "tmdate": 1762915463153, "mdate": 1762915463153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces StaMo, a compressed visual representation for robotic policies. StaMo compresses DINOv2 patch tokens to two tokens per frame with a diffusion autoencoder for more compact state representation, and uses representation deltas as the latent action. The compressed representation works for policy and world model learning. Experiments on LIBERO and real experiments shows that StaMo representations improves world modeling over vanilla OpenVLA, and cotraining on StaMo latents improves policy performance over prior state of the art."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Compressing DINOv2 tokens to a compact representation and using deltas as latent action is well-motivated.\n- StaMo representation shows improvement for world modeling in LIBERO with negligible inference slowdown with OpenVLA.\n- StaMo latent cotraining improves over LAPA/ATM.\n- A real robot environment shows StaMo representation improves over baseline RDT."}, "weaknesses": {"value": "- Lack of relevant baselines.\n  - The world modeling section only compares against vanilla OpenVLA with an added world modeling object and does not compare with existing world modeling work such as DINO-WM, AdaWorld, etc. Table 1 only contains reconstruction quality metrics for StaMo and no baselines at all. Similarly, in Table 2, the reviewer is not sure what UniVLA is supposed to do here, since it is not used in any experiments below; and OpenVLA + StaMo is slower than vanilla OpenVLA, which undercuts the claim that StaMo enables efficient world modeling. The authors also state that training runs on 8xH100 for multiple days.\n  - Unclear what ATM is in the cotraining section.\n  - The real experiment section is a single embodiment behavioral cloning setup. No cross-embodiment experiments to support the claims of generalizability and scalability to unseen robots.\n- No ablation experiments. Only a hidden dim sweep table in Appendix A.\n- The writing could be a bit clearer. I have asked for crucial clarifications in the question section below."}, "questions": {"value": "- A core claim is that StaMo representation is good for efficient world modeling, which is not adequately supported by the limited experiments. What is the inference speed and quality difference between using raw DINOv2 tokens vs. StaMo compressed? How does it compare with established world modeling approaches?\n- In the real robot experiments, is the plain OpenVLA fine-tuned on the same dataset? How well does vanilla behavioral cloning (e.g. diffusion policy) do on this dataset, with a simple ImageNet-pretrained ResNet, and with StaMo encoder?\n- Could you clarify what ATM is in 4.3?\n- What are S, D, O, and Ego in Table 5? And on the fourth row \"long horizon tasks\", I am assuming that these are success rates, and it seems like the 5 is supposed to be 0.5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "170hw4sc6v", "forum": "UhwbIblavX", "replyto": "UhwbIblavX", "signatures": ["ICLR.cc/2026/Conference/Submission180/Reviewer_FTP6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission180/Reviewer_FTP6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051477541, "cdate": 1762051477541, "tmdate": 1762915462981, "mdate": 1762915462981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aim at finding a compact and expressive state representation for robotics. The key design is to leverage a pre-trained vision encoder, and a pre-trained image diffusion model as decoder. To obtain a compact representation, the latent is compressed into 2 tokens/image. Experiments are conducted on both simulation and real-world scenarios. By additionally predicting the learned state, performance is improved in robotic setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The writing is clear and easy to follow.\n\nIdea is simple but effective.\n\nExperiments sound and comprehensive, revealing some interesting things like interpolation and linear probing."}, "weaknesses": {"value": "Some key details are missed in the main paper. For example, it should specify which data you used for different settings. Also, some metrics are missed in the main paper, e.g., Table 3 & 4, are these success rates? The main paper should be self-contained, and complete experimental setting would make the number valid.\n\nAlthough low compression rate is achieved, I am worried about some motion are missed in the reconstruction one. For example, in the up-right corner of the Fig 3, I can't tell which action is given the reconstruction result.\n\nSome previous works have studied similar insight. For example, as listed in section 2.2: Wang et al. 2025, Zhang et al. 2025, Cen et al., 2025. Li et al. 2025b. The author claim that these works have limited generation across scenes. This claim needs evidence and it is suggested to provide the comparison with the most related works. Lack of the comparison makes the results not convincing.\n\nTypos: title of section 2.2."}, "questions": {"value": "Refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Gi6bjeKwa", "forum": "UhwbIblavX", "replyto": "UhwbIblavX", "signatures": ["ICLR.cc/2026/Conference/Submission180/Reviewer_WdQt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission180/Reviewer_WdQt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159079155, "cdate": 1762159079155, "tmdate": 1762915462753, "mdate": 1762915462753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}