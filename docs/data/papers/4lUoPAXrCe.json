{"id": "4lUoPAXrCe", "number": 22538, "cdate": 1758332503577, "mdate": 1759896860696, "content": {"title": "MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and Question Answering", "abstract": "Understanding the relationship between textual data and time-series evolution is a critical yet under-explored challenge in applied data science. While multimodal learning has gained traction, existing time-series benchmarks provide limited support for evaluating cross-modal reasoning and complex question answering, both essential for capturing interactions between narrative information and temporal patterns.\nTo bridge this gap, we introduce Multimodal Time Series Benchmark (MTBench), a large-scale benchmark designed to evaluate large language models (LLMs) on the joint reasoning over time-series and text, exemplified through financial and weather domains. MTBench consists of paired time-series and textual data, including financial analysis with aligned stock price movements and weather reports matched to historical temperature records. Unlike existing benchmarks focused on isolated modalities, MTBench offers a comprehensive testbed for language models to jointly reason over structured numerical trends and unstructured textual narratives.\nMTBench supports diverse tasks that require a deep understanding of both text and time-series data, including forecasting, semantic and technical trend analysis, and news-driven question answering (QA). These tasks assess the model’s ability to capture temporal dependencies, extract key insights from text, and integrate cross-modal information.\nWe benchmark state-of-the-art LLMs on MTBench, providing a systematic analysis of their effectiveness in capturing the causal relationships between textual narratives and temporal patterns. Our findings reveal significant challenges in current models, including difficulty with long-term dependencies, limited causal interpretation in financial and weather dynamics, and insufficient multimodal fusion. MTBench establishes a foundation for advancing multimodal time-series research and for developing the next generation of multimodal models capable of reasoning across narrative and time series data.", "tldr": "", "keywords": ["Multimodal Time Series", "Time Series Question Answering"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8574c5755cdc1c81ccb4c5df3c3422033868bb3.pdf", "supplementary_material": "/attachment/d67b027b6c4e9608de42c13bca0235ab5864c92d.zip"}, "replies": [{"content": {"summary": {"value": "MTBench is a benchmark for evaluating multimodal reasoning over time-series data and text. It aligns financial news with stock prices and weather event reports with meteorological data, covering both short and long horizons. The benchmark defines four tasks: forecasting, trend analysis, technical-indicator prediction, and question answering, testing models on numerical prediction and textual reasoning. Experiments with large language models show that text improves short-term accuracy, but long-term temporal reasoning and causal understanding remain difficult."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Real narrative and time-series alignment in two domains (finance and weather), plus Consistent vs. Misaligned splits, dual horizons/resolutions, and MCQA items that require both modalities—useful for testing whether models use helpful text and ignore misleading text.\n- The QA tasks (correlation prediction and MCQA) are novel and challenging, requiring reasoning across textual and temporal evidence. They move beyond numeric forecasting to assess causal understanding, consistency checking, and cross-modal inference, which many prior benchmarks lack."}, "weaknesses": {"value": "- For semantic trend analysis and technical-indicator prediction, labels are deterministically derived from the time series; text is optional. Strong results here don’t necessarily demonstrate text-based reasoning, so the benchmark may not isolate the contribution of the text modality.\n- Table 1 appears to conflate task type with input modality. It lists Time-MMD as “time-series only” under Query Format, but Time-MMD’s evaluation uses text plus time series inputs (via its multimodal setup). This could mislead readers about the true input requirements.\n- The evaluation pipeline feels too trivial for a benchmark—mainly serializing the time series and appending text into prompt templates for general LLMs, with format-fixing post-processing—so scores may reflect prompt-following rather than genuine multimodal temporal reasoning."}, "questions": {"value": "Please address the identified weaknesses and limitations noted above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iUKeRMWlkS", "forum": "4lUoPAXrCe", "replyto": "4lUoPAXrCe", "signatures": ["ICLR.cc/2026/Conference/Submission22538/Reviewer_Qm4M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22538/Reviewer_Qm4M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713657418, "cdate": 1761713657418, "tmdate": 1762942267946, "mdate": 1762942267946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MTBench, a benchmark designed to evaluate the multimodal reasoning of LLMs on time-series and textual data, addressing the limitations of existing benchmarks that focus primarily on forecasting. MTBench consists of semantically aligned time-series and narrative texts from the finance and weather domains. It supports diverse tasks including forecasting, trend analysis, and news-driven QA. The authors' evaluation of state-of-the-art LLMs reveals significant limitations, particularly in long-range temporal reasoning and causal inference. The work's main contribution is a dedicated testbed for assessing and advancing the joint interpretation of numerical and narrative temporal data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper focus on shifting the focus of time-series evaluation from simple forecasting to complex, multimodal reasoning. The introduction of news-driven question answering and the explicit inclusion of both aligned and misaligned text-series pairs are novel concepts for a benchmark.\n\n-  The paper is exceptionally clear. Figures effectively illustrate the benchmark's tasks and the complexity of the data, and the text is well-structured, making the motivation and methodology easy to understand."}, "weaknesses": {"value": "- The benchmark's construction depends on LLMs for key annotation and data synthesis steps. This introduces a risk that the evaluation may inadvertently favor models that simply mimic the annotation LLM, and the quality of the synthetic data is not independently validated.\n\n- The benchmark is framed as \"multimodal\" but is currently limited to text and univariate time series. This simplifies the real-world scenarios in finance and weather, which often involve richer data types like charts, tables, and satellite imagery.\n\n- The paper uses the term \"causal reasoning,\" but the benchmark can, at best, evaluate a model's ability to identify plausible correlations between narratives and trends, not true causality, which is nearly impossible to ground truth in these domains."}, "questions": {"value": "Plz answer my concerns in the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Plz answer my concerns in the weakness section."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rGG6JUO605", "forum": "4lUoPAXrCe", "replyto": "4lUoPAXrCe", "signatures": ["ICLR.cc/2026/Conference/Submission22538/Reviewer_ttfH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22538/Reviewer_ttfH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795841317, "cdate": 1761795841317, "tmdate": 1762942267641, "mdate": 1762942267641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduces a large-scale Multimodal Time-series Benchmark—MTBench, which is designed to evaluate the multimodal reasoning capabilities of large language models in the financial and weather domains. It supports various tasks such as forecasting, semantic and technical trend analysis, as well as news-driven question answering."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear: high-quality multimodal time-series benchmarks are indeed of great importance.\n\n2. MTBench supports multiple tasks grounded in domain-specific usage, thereby providing a more realistic testing platform for multimodal reasoning."}, "weaknesses": {"value": "1. Line 176 mentions using GPT-4o for annotation. I believe that during the annotation process, the powerful GPT-4o might introduce some future information, leading to data leakage.\n\n2. Lines 192-193 mention that the financial articles and stock pair were split into two subsets: Consistent Pairs and Misaligned Pairs. This setup is reasonable and very practical. However, in the subsequent experimental results, there doesn't seem to be a separate verification or explanation for these two parts. I'm curious about how the model performs when confronted with misleading or irrelevant news.\n\n3. The experimental results show the performance of different SOTA LLMs on MTBench, comparing model performance before and after adding text. Indeed, in many cases, performance improves after adding text, but there are exceptions. Therefore, I am confused about how can we determine whether this performance gain comes from the high-quality dataset or the powerful SOTA LLMs? I suggest adding experiments to clarify this issue.\n\n4. Lack of Error Bars analysis."}, "questions": {"value": "1. Line 175 mentions \"ensuring a balanced distribution of article lengths to maintain representativeness.\", How was this achieved?\n\n2. How was the time-series data fed into the LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LPWTfadbDR", "forum": "4lUoPAXrCe", "replyto": "4lUoPAXrCe", "signatures": ["ICLR.cc/2026/Conference/Submission22538/Reviewer_EaJW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22538/Reviewer_EaJW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963792582, "cdate": 1761963792582, "tmdate": 1762942267438, "mdate": 1762942267438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MTBench, a benchmark designed to evaluate language models on tasks that require jointly understanding textual narratives and time-series data. Focusing on financial markets and weather signals, the authors pair real news narratives with aligned historical time-series segments and ask models to perform forecasting, trend interpretation, technical-indicator reasoning, and news-driven question answering. The benchmark compares text-only, time-series-only, and combined inputs to study when narrative context actually improves predictive or explanatory performance. Experiments across modern LLMs show that text can help on short-horizon tasks but models often struggle to disentangle noise, avoid over-reacting to narratives, and maintain causal consistency. Overall, the work contributes a curated multimodal dataset, evaluation tasks, and analysis framework for probing narrative-augmented temporal reasoning in LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. While prior work has explored combining text with time-series signals, this paper makes a meaningful step by explicitly framing narrative-driven temporal reasoning as a multimodal benchmarking problem. Bringing financial and weather narratives into the evaluation setting introduces a realistic layer of uncertainty and interpretation that is often overlooked in purely numeric forecasting studies.\n\n2. The experiments are systematic, comparing text-only, time-series-only, and combined inputs to reveal nuanced effects of narrative context and where current models struggle. The authors also provide insightful qualitative analysis into failure modes such as over-trusting news sentiment and difficulty with longer-horizon predictions.\n\n3. The writing is clear and the task motivations are easy to follow, supported by intuitive examples that show why narrative context matters. This work gives the community a concrete, reproducible way to study temporal reasoning and question answering."}, "weaknesses": {"value": "1. The data validation side feels a bit under-developed. The dataset relies on GPT-4o for narrative and sentiment labels, but there's no report of human auditing, inter-annotator checks, or systematic quality analysis. Also, finance can easily have near-duplicate articles covering the same event; treating each as an independent sample might inflate signal strength or create bias. On top of that, financial news often mixes forward-looking comments with retrospective interpretation, and some narratives can only really be verified after the outcome is known. If all such text is treated as clean ex-ante information, the benchmark risks introducing hindsight or narrative bias. Adding sampling audits, redundancy filtering, or a simple check to flag ex-post narratives would make the dataset feel much more solid.\n\n2. The evaluation focuses almost entirely on general-purpose LLMs, without including strong time-series baselines or hybrid TS+text architectures. This is understandable for a first benchmark release, but it makes it hard to tell whether performance gaps come from fundamental model limitations or just architectural mismatch with time-series signals. It would help to include a few dedicated baselines (e.g., GPT4MTS [1], TaTS [2]) to clarify whether the benchmark is measuring “LLMs struggling” or “current TS-aware models still lag,” and to highlight the research opportunities more clearly.\n\n3. Domain coverage feels a bit narrow, even though finance and weather are great starting points with rich signals. There are other narrative-driven time-series domains (e.g., traffic, energy demand, policy-driven markets, retail events, etc.) where even small test subsets could help demonstrate generality.\n\n---\n\n[1] GPT4MTS: Prompt-Based Large Language Model for Multimodal Time-Series Forecasting\n\n[2] Language in the Flow of Time: Time-Series-Paired Texts Weaved into a Unified Temporal Narrative"}, "questions": {"value": "1. I have a question regarding narrative validity, and I want to emphasize that this is not necessarily a criticism; it may simply reflect my incomplete understanding of your design choices. Financial news often mixes forward-looking statements with retrospective explanations, and in many cases the interpretation of an event only becomes verifiable after the outcome is known. In other words, some narratives that appear “factual” at publication time later turn out to be mistaken (for instance, a report stating that a company’s supply chain issues are “fully resolved,” when subsequent earnings calls reveal that they were not). I am curious how you approached this when constructing the dataset. In such cases, did you consider separating verifiable present-time facts from speculative forward-looking commentary, so that at least the factual portion of the text is known to be correct while the expectation remains a hypothesis? My intuition is mixed: one could argue that filtering or tagging such cases would prevent models from being evaluated on potentially misleading input and help them focus on forecasting and understanding actual information available at the time, but conversely one might argue that dealing with imperfect or even incorrect narratives is intrinsic to real-world forecasting and thus should remain part of the benchmark. I would really appreciate your perspective on this point, particularly whether including potentially mistaken narratives was an intentional design choice to preserve realism, or whether there might be value in a future pass that double-checks factual claims to bolster dataset quality. \n\n2. A second point I would like to clarify concerns potential data leakage from model pretraining, given that the benchmark uses real historical financial news. Some of the evaluated models may have been trained on overlapping time periods and could have seen portions of this content during pretraining. I am curious how you assessed this possibility. Do you expect that such leakage would meaningfully affect the benchmark results, or is the impact likely negligible because the tasks focus on reasoning and temporal alignment rather than rote recall?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xHIVVeKNSw", "forum": "4lUoPAXrCe", "replyto": "4lUoPAXrCe", "signatures": ["ICLR.cc/2026/Conference/Submission22538/Reviewer_1keZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22538/Reviewer_1keZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983522542, "cdate": 1761983522542, "tmdate": 1762942267119, "mdate": 1762942267119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MTBench, a large-scale benchmark that evaluates large language models on joint reasoning over time-series data and natural-language narratives in finance and weather domains. MTBench introduces four families of tasks—time-series forecasting, semantic trend classification, technical-indicator prediction, and news-driven question answering—and couples each time-series window with contemporaneous, semantically-aligned textual evidence. Experiments on six state-of-the-art LLMs show that textual context generally improves performance, but models still struggle with long-range dependencies, causal interpretation, and robust handling of conflicting signals."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Large Scale: First benchmark to integrate time-series and text for reasoning-heavy tasks like QA and causal inference, moving beyond predictive tasks.\n\nQuality: Rigorous data collection (e.g., semantic alignment of news and stock trends) and comprehensive task design (e.g., multi-choice QA, correlation prediction).\n\nClarity: Well-defined tasks and metrics; figures effectively illustrate data and model performance.\n\nSignificance: Provides a foundation for developing LLMs capable of real-world multimodal reasoning in high-stakes domains like finance and climate."}, "weaknesses": {"value": "Dataset size & diversity\n– Financial news is limited to US equities and 2021-2023; lacks macro-economic indicators, earnings calls, or non-English sources.\n– Weather subset uses only 50 US airport stations; no satellite imagery or global coverage.\n\nTask granularity\n– Technical-indicator tasks cover only MACD/Bollinger for finance and max/min/diff temperature for weather; richer multi-indicator or multivariate setups would stress-test models further.\n– QA tasks are exclusively multi-choice; open-ended “why” questions would probe causal explanation ability.\n\nConflict & adversarial analysis\n– Misaligned pairs are used passively; no targeted adversarial or counterfactual samples (e.g., flip sentiment while keeping price trend) to measure model brittleness.\n\nEvaluation protocol\n– All experiments are zero-shot; fine-tuning or few-shot baselines would clarify how much performance gain is achievable with modest adaptation.\n– LLMs occasionally produce malformed sequences; while post-processing is applied, the impact on metrics is not quantified.\n\nBroader impact discussion\n– Limited discussion of potential misuse (e.g., algorithmic trading manipulation) or privacy issues in financial news data."}, "questions": {"value": "How was the semantic alignment between news and stock trends validated? Could inter-annotator agreement or ground-truth metrics (e.g., expert validation) strengthen reliability?\n\nWere ablation studies conducted to assess the impact of different alignment strategies (e.g., timing vs. semantic relevance) on task performance?\n\nCan the benchmark accommodate multivariate time series or other modalities (e.g., images) in future work?\n\nHow might fine-tuning or task-specific adaptations address the observed biases (e.g., correlation prediction defaults)?\n\nCould the authors provide error bars or statistical significance tests for the performance gains attributed to textual input?\n\nLastly, can you clarify the licensing of the crawled financial news articles and confirm compliance with publishers’ terms of use?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0RCqejzQEi", "forum": "4lUoPAXrCe", "replyto": "4lUoPAXrCe", "signatures": ["ICLR.cc/2026/Conference/Submission22538/Reviewer_tDfc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22538/Reviewer_tDfc"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762767978337, "cdate": 1762767978337, "tmdate": 1762942266836, "mdate": 1762942266836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}