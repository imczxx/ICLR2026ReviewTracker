{"id": "FDuN3F1K9X", "number": 15066, "cdate": 1758247380174, "mdate": 1763127696380, "content": {"title": "InsCal: Calibrated Multi-Source Fully Test-Time Prompt Tuning for Object Detection", "abstract": "Test-time prompt tuning (TPT) has emerged as a powerful technique for adapting pre-trained vision-language models (VLMs) to diverse downstream tasks, including image classification and visual reasoning. With the rise of text-driven object detectors, we extend TPT to object detection, unlocking new capabilities for cross-domain adaptation. However, a critical challenge in TPT is the inherent miscalibration caused by entropy minimization: domain shifts often lead to incorrect predictions, and enforcing high confidence exacerbates miscalibration, ultimately degrading performance. To tackle this, we introduce InsCal, a novel framework designed to enhance cross-domain object detection through three key innovations: (1) extending TPT to a multi-source paradigm, enabling knowledge aggregation across diverse domains; (2) reducing domain gaps via a novel text-driven style transfer strategy that aligns features to the source domain without requiring reference images; and (3) refining the entropy minimization objective with instance-specific calibration, ensuring robust and well-calibrated adaptation. Our approach not only mitigates miscalibration but also significantly improves cross-domain object detection performance, setting a new benchmark for test-time adaptation in VLMs.", "tldr": "", "keywords": ["Source-free zero-shot learning", "model calibration"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/92929fff192da9703b8687d9d2c2e0a300fb0de6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on test-time domain adaptation task, aiming to transfer knowledge learned from the labelled source domain to an unlabeled target domain, where the target samples are provided one at a time. To solve this, the paper proposes  InsCal , a test-time adaptation (TTA) approach for text-driven object detection that tackles the problem of miscalibration during adaptation. Experiments show that InsCal outperforms existing methods on cross-domain datasets by significantly reducing D-ECE (detection expected calibration error) and improving mAP (mean average precision)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is easy to follow.\n* The test-time adaptation is an interesting problem, and it is even more important when trying to deploy VLM to real-world applications."}, "weaknesses": {"value": "1. Limited novelty. While InsCal integrates multiple source models and improves TPT, the approach heavily relies on existing methods such as text-guided generalization and entropy minimization. Its contribution lies more in integrating these techniques. For example, the proposed TGIA strictly follows CLIPtheGap[1], and MSTPT seems like a direct extension of TPT.\n2. It appears that TGIA's performance depends on the accuracy of the textual description of the domain. If the domain is no longer a difference in weather conditions or image style, but rather an attribute that is difficult to describe in text, TGIA may not be effective, and the paper lacks similar analysis.\n3. The necessity of task definition. Is it necessary to define multi-source TPT? Throughout the process, the method's operation on the domain is only reflected in using different prompts to guide image enhancement, which is a method already applied in existing work and can hardly be considered a contribution of this paper.\n4. Limited performance improvements and unfair comparisons. Since the baseline uses GDINO, and its zero-shot performance already surpasses some state-of-the-art (SOTA) works, direct comparisons with other works are unfair. Specifically, results of fine-tuning GDINO using TPT or O-TPT should be reported, and only the performance differences between the proposed method and them can be considered as an absolute improvement.\n\n[1]Vidit V, Engilberge M, Salzmann M. Clip the gap: A single domain generalization approach for object detection. CVPR 2023"}, "questions": {"value": "* Please refer to Weakness.\n* Why the provided UDA and SFDA methods are NOT SOTA? We all know that some existing UDA and SFDA work has achieved better performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CikKfItWfb", "forum": "FDuN3F1K9X", "replyto": "FDuN3F1K9X", "signatures": ["ICLR.cc/2026/Conference/Submission15066/Reviewer_HhSq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15066/Reviewer_HhSq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819293813, "cdate": 1761819293813, "tmdate": 1762925388359, "mdate": 1762925388359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "b61EFraLRA", "forum": "FDuN3F1K9X", "replyto": "FDuN3F1K9X", "signatures": ["ICLR.cc/2026/Conference/Submission15066/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15066/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763127695536, "cdate": 1763127695536, "tmdate": 1763127695536, "mdate": 1763127695536, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose InsCal to enhance the performance of multi-domain object detection. Utilizing text-driven style transfer strategy to align features from different domains, InsCal mitigates miscalibration caused by domain shifts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe authors propose a test-time adaptation method, InsCal, for multi-source object detection setting.\n2.\tInsCal mitigates the miscalibration by refine instance-level entropy minization objective."}, "weaknesses": {"value": "1.\tThe performance improvement achieved by InsCal appears to be limited. Moreover, although UDA and SFDA methods have access to target-domain data, FTTA methods are allowed to directly access test samples. Therefore, the statement in Lines 375–376 is not entirely rigorous.\n2.\tThe overall design of the proposed approach lacks originality. The main framework of InsCal heavily relies on TPT, and the text-guided feature augmentation strategy has already been explored in several prior works [1,2].\n3.\tThe benchmarks used for evaluation are relatively small. It would strengthen the paper to include experiments on larger-scale benchmarks to further validate the generalization capability of the proposed method.\n4.   Typos: There are two consecutive commas in Line 269.\n\n[1] PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization (ICCV23)\n\n[2] Using Language to Extend to Unseen Domains (ICLR23)"}, "questions": {"value": "1.\tIn Lines 252–254, it is unclear why the magnitude scaling factor is designed to match the transformation magnitude of the desired text-guided shift. \n2.\tThe InsCal method requires performing N augmentations for each of the S source domains. Considering that the task is TTA, what are the associated computational and time costs of InsCal? In addition, key hyperparameters such as N and ρ should be explicitly described and discussed in the main text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x7t4hIrojP", "forum": "FDuN3F1K9X", "replyto": "FDuN3F1K9X", "signatures": ["ICLR.cc/2026/Conference/Submission15066/Reviewer_yxVi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15066/Reviewer_yxVi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927095923, "cdate": 1761927095923, "tmdate": 1762925386027, "mdate": 1762925386027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, InsCal, proposes a new framework for Fully Test-Time Adaptation (FTTA) of text-driven object detectors. The authors identify that standard Test-Time Prompt Tuning (TPT), which relies on entropy minimization, suffers from miscalibration and overconfidence when faced with domain shifts. To address this, InsCal introduces three components: (1) a multi-source TPT paradigm that aggregates knowledge from multiple source-specific models; (2) a Text-Guide Image Augmentation (TGIA) module that uses text-driven style transfer to reduce the domain gap; and (3) a novel \"instance-specific calibrated\" entropy loss (InsCal) that re-weights the entropy minimization objective based on the confidence margin between the top two predictions. The authors claim their method significantly reduces calibration error and improves mAP on cross-domain object detection tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important and practical problem: the application of test-time adaptation to open-vocabulary object detectors, which is a relatively under-explored area. The authors correctly identify a key failure mode of standard TPT: entropy minimization can exacerbate miscalibration by encouraging overconfident, incorrect predictions, especially under domain shift."}, "weaknesses": {"value": "1. The paper's claim of state-of-the-art performance is based on a comparison that appears to provide the proposed method with more resources. InsCal is presented as \"multi-source\" and uses four source models (fine-tuned on Day Clear + three other domains, per Sec 4.2 and Table 2 caption). In contrast, the competing FTTA baselines (TPT, O-TPT, etc.) are treated as single-source (using only Day Clear, per Table 2 caption). This discrepancy makes it difficult to isolate the performance gains of the proposed calibration method from the benefits of simply using more source models and data. The ablation in Fig. 9, which shows that adding sources can sometimes harm performance, further complicates this.\n\n2. The framework's requirements seem to challenge the standard constraints of \"Fully Test-Time Adaptation\" (FTTA). The method appears to require fine-tuning the base GDINO model on multiple, diverse source datasets to create the \"source models.\" This represents a significant setup cost compared to the more common FTTA premise of adapting a single, off-the-shelf pre-trained model. The Text-Guide Image Augmentation (Sec 4.1) requires a \"short text description of its style domain\" for both the source and the target (e.g., \"A photo of foggy day\"). In a typical FTTA scenario, the target image arrives without a priori knowledge of its domain. Requiring a manually-provided text prompt for the target domain's style could be seen as a form of supervision, which may not align with the \"fully test-time\" setting.\n\n3. The implementation details for the Text-Guide Image Augmentation (TGIA) module are unclear. This component is defined by an optimization process (Eq. 4) to train the augmentation network $\\mathcal{A}_{\\theta}$. The paper does not specify when this optimization occurs. If it is run for every single test image, it would introduce a significant computational overhead (an optimization within the adaptation loop) that is not analyzed in the paper's complexity discussion. Clarification on this process is needed.\n\n4. Several figures and captions are confusing or contain errors. (discussed in the questions below)."}, "questions": {"value": "1. The literature review appears to be incomplete. Many recent papers are not discussed [1-6].\n\n2. On Figure 1(a): Why is the zero-shot performance compared to a \"Fine-Tune\" baseline? Since FTTA is an unsupervised method that adapts to unlabeled test data, what is the relevance of a fully supervised fine-tuned model to motivate this work?\n\n3. On Figure 3: In the diagram for InsCal, \"Source Model 3\" is listed twice. \n\n4. The caption for Figure 1(a) is placed awkwardly close to the caption for 1(b), making it difficult to read. \n\n[1] Ra-tta: Retrieval-augmented test-time adaptation for vision-language models, ICLR2025\n\n[2] Bayesian test-time adaptation for vision-language models, CVPR2025\n\n[3] Efficient and context-aware label propagation for zero-/few-shot training-free adaptation of vision-language model, ICLR2025\n\n[4] DPCore: Dynamic prompt coreset for continual test-time adaptation, ICML2025\n\n[5] DynaPrompt: Dynamic Test-Time Prompt Tuning, ICLR2025\n\n[6] Test-Time Model Adaptation with Only Forward Passes, ICML2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "06bixUxLH6", "forum": "FDuN3F1K9X", "replyto": "FDuN3F1K9X", "signatures": ["ICLR.cc/2026/Conference/Submission15066/Reviewer_bMFe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15066/Reviewer_bMFe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985476925, "cdate": 1761985476925, "tmdate": 1762925384328, "mdate": 1762925384328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}