{"id": "R6I8P4DuRf", "number": 14399, "cdate": 1758234486532, "mdate": 1759897372606, "content": {"title": "Adaptively Labeling Vision Datasets Via Instance-Level Retrieval", "abstract": "Human annotations are the backbone of modern computer vision, but they are increasingly recognized as an inefficient resource. They typically capture only a single, fixed view of the rich visual information present in images. How can we move toward datasets that are labeled adaptively, rather than exhaustively by hand? We propose Instance-Level Retrieval, a method for adaptively building object detection datasets from large collections of unlabeled images. Given just a handful of seed examples, our method automatically finds and labels relevant training data by comparing self-supervised object representations. Starting from a small, subset of Pascal VOC (Visual Object Classes), we demonstrate that it is possible to retrieve a high quality set of images.  In experiments that control data scale, models trained on our adaptively-labeled data exceed the performance of training on the original Pascal VOC human annotations with a $0.08$ mAP improvement. We use our retrieval method on out of distribution unlabeled images derived from ImageNet-1K, showing that our method can successfully find high quality exemplars for fixed image classes matching the Pascal VOC training set. Training on this expanded training set leads to an additional $0.105$ mAP improvement over baseline. Finally, we show that our methodology is also useful for filtering and selecting high-quality subsets of human-annotated data, yielding a $0.037$ mAP gain compared to uniformly sampled subsets.", "tldr": "", "keywords": ["Computer Vision", "Deep Learning", "Self-Supervised"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/90384ac1bdd0da567b4b71c1f7b6c1e923b777a0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes automatically constructing object detection datasets from large collections of unlabeled images. With only a handful of seed examples, the method discovers and labels relevant training data by comparing self-supervised object representations. First, candidate instances are mined across all unlabeled images using standard object detectors. Next, similarity between anchor instances and unlabeled candidates is measured with a Semantic Intersection over Union (Semantic IoU) computed from DINO2 features of the target object extracted by SAM2. Finally, high-quality candidates are filtered and returned as newly labeled instances for downstream training. Using the automatically labeled dataset, detection performance shows marginal improvement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper attempts to develop an automated pipeline for generating detection datasets using high-performing models such as SAM2 and DINO2. The resulting dataset yields marginal improvements in detection model performance."}, "weaknesses": {"value": "- The paper proposed a heuristic pipeline primarily leveraging DINO2 and SAM2 to automatically generate detection label.\n- Performance gains of the detection models trained by the datasets generated by the proposed pipeline are marginal. The performance gain is possibly due to DINO2 and SAM2 knowledge transfer to the model, and it is unclear whether the proposed pipeline contributes the performance improvement..\n- Automatic detection label generation is investigated priorly but the paper does not compare against existing methods such as\nhttps://arxiv.org/html/2307.15710v2\nhttps://github.com/autodistill/autodistill?tab=readme-ov-file#object-detection\nhttps://github.com/IDEA-Research/Grounded-Segment-Anything\n... (there are more)"}, "questions": {"value": "- How the encoded number of patches can be fixed N? The N size (can be varying) depends on the number of patch segmented by SAM2 which is dependent on the object size."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D6SqGs5p36", "forum": "R6I8P4DuRf", "replyto": "R6I8P4DuRf", "signatures": ["ICLR.cc/2026/Conference/Submission14399/Reviewer_kvra"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14399/Reviewer_kvra"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761512956961, "cdate": 1761512956961, "tmdate": 1762924811136, "mdate": 1762924811136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores a pseudo-labeling setup for the object detection task. Images from the labeled part of the training set act as queries to find relevant and high-quality training samples of the same instance in the unlabeled set. In order to find candidate objects in the unlabeled set, an existing pre-trained detector is used. Subsequently, a pre-trained SAM model is used to create a mask for the relevant object for which DINOv2 features are extracted. Image-to-image similarity is then computed via the proposed semantic IoU metric between two sets of DINOv2 features. This metric involves a Hungarian matching to assign feature correspondences. The method is applied to the Pascal VOC dataset by considering part of the dataset as unlabeled."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "**S1)** The need for massive, high-quality training datasets is evident from the current practices of foundation model pre-training. The curation of web-scraped data is a critical component in this process yet very often not publicly reported. This paper addresses an important research direction by proposing an automated method to identify and leverage relevant samples from large unlabeled data collection."}, "weaknesses": {"value": "**W1)** Leveraging a smaller set of labeled images to pseudo-label a set of unlabeled images is one of the main directions used in semi-supervised learning. The paper lacks any discussion of how the method fits into this field and any comparisons to existing techniques. Several examples of the semi-supervised object detection line of work are:\n\n[1] Unbiased Teacher for Semi-Supervised Object Detection, Liu et al., *ICLR 2021*\n\n[2] Cross-Domain Adaptive Teacher for Object Detection, Li et al., *CVPR 2022*\n\n[3] Detecting Twenty-thousand Classes using Image-level Supervision, *ECCV 2022*\n\n[4] End-to-End Semi-Supervised Object Detection with Soft Teacher, Xu et al., *ICCV 2021*\n\n\n\n**W2)** Similar to the previous point, the paper lacks any discussion about existing instance-level retrieval approaches. One of the main paper contributions, the Semantic IoU metric, is similar to prior work and should be compared against them. Works like Razavian et al. [5] use an analogous metric (also Chamfer similarity [6]) to estimate similarity between two sets of features. There are also learned methods, like AMES [7], which is trained on DINOv2 to estimate such similarity via a small transformer network.\n\n[5] Visual Instance Retrieval with Deep Convolutional Networks, Razavian et al., *ITE Transactions on Media Technology and Applications 2016*\n\n[6] Parametric correspondence and chamfer matching: Two new techniques for image matching, Barrow et al., *Image Understanding Workshop 1977*\n\n[7] AMES: Asymmetric and Memory-Efficient Similarity Estimation for Instance-level Retrieval, Suma et al., *ECCV 2024*\n\n\n\n**W3)** The whole pipeline includes several hyperparameters that are only vaguely mentioned in the text of the method. Their selected values are summarized in Chapter F of the Appendix, but it is not mentioned whether these values were selected based on the evaluation performance, or using some validation dataset. If the former is the case, then it is a weakness of the method since it cannot be applied to a different domain without tuning these parameters again.\n\n**W4)** As the contribution of the paper is mostly a combination of existing pre-trained models, the applicability of the pipeline should be verified on more than one dataset. Furthermore, the approach is motivated by the under-leveraged \"vast amounts of image data available on the internet\", so the proposed retrieval approach should be applied on a large-scale unlabeled dataset. This would showcase the effectiveness of the retrieval under the existence of hard negative examples and verify its practical usability in terms of computational complexity.\n\n\n\nTypos:\n\nLine 187-188: Dinov2 lowercase inconsistent with the rest of the paper and repeated \"N rows and D columns\"\n\nLine 51: two periods\n\nmAP metric is typically reported in the range of 0 to 100."}, "questions": {"value": "**Q1)** How are the hyperparameters, such as the NMS threshold, Objectness threshold, or Minimum semantic IoU selected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "35BXOPhO9J", "forum": "R6I8P4DuRf", "replyto": "R6I8P4DuRf", "signatures": ["ICLR.cc/2026/Conference/Submission14399/Reviewer_1u4k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14399/Reviewer_1u4k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936678734, "cdate": 1761936678734, "tmdate": 1762924810584, "mdate": 1762924810584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an adaptive labeling pipeline for object detection datasets. Specifically, the authors first use a pretrained region proposal network to extract candidate object instances from a large set of unlabeled images. The authors then represent each candidate with features from a self-supervised vision encoder. Finally, the authors retrieve the top-k most relevant candidates by nearest-neighbor search given a small set of seed images with bounding box annotations, which are then assigned labels based on their similarity to the seeds. Such a process yields an adaptively labeled dataset without additional manual annotations. Experimental results on in-domain and transfer labeling demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe proposed method is well motivated. Adaptive data curation and labeling without human annotations is indeed important to meet evolving application demands.\n-\tThe paper is generally well-written and easy to follow.\n-\tThe experimental results seem promising."}, "weaknesses": {"value": "-\tI am concerned about the heavy computational costs induced by the proposed method, which could limit its real-world applications to some extent.\n-\tThe authors use DINOv2 as a vision encoder to extract feature maps. It would be better to ablate other vision encoders as well (e.g., CLIP).\n-\tThe authors only evaluate based on the YOLO detector. It would be better to provide results on several other popular detectors as well to demonstrate the generalizability of the proposed method.\n-\tWhat are the failure cases of the proposed method? Adding more analysis on failure cases will add more insights to the paper."}, "questions": {"value": "See the questions mentioned above. Given the current status of the paper, I am leaning towards borderline reject and hope the authors could address my concerns during the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UNwgECqzmu", "forum": "R6I8P4DuRf", "replyto": "R6I8P4DuRf", "signatures": ["ICLR.cc/2026/Conference/Submission14399/Reviewer_D94N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14399/Reviewer_D94N"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955331845, "cdate": 1761955331845, "tmdate": 1762924809869, "mdate": 1762924809869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a instance labeling method for object detection annotation generation. In detail, it adopts SAM2 model to extract instance mask and further utilize DINOv2 model to extract the mask related patch features. For unlabeled images, it first use Owlv2ForObjectDetection model to generate bounding boxes with objectness scores, after NMS, the remaining bounding boxes are send to DINOv2 model to get corresponding patch features. By comparing the patch features between the unlabeled boxes and ground truth annotations, a semantic IOU score are calculated to filtering highly similar instances. The similar instances are assigned the same label as the ground truth instance and then used for detector training. The generated pseudo label eventually achieves slightly better performance than original ground truth annotations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The method produces good pseudo labels for object detection."}, "weaknesses": {"value": "1. Lack of experiments. PASCAL VOC is a simple detection dataset, more experiments on more complex datasets should be conducted to strengthen the effectiveness of the proposed method. (e.g. COCO dataset)\n2. Similar to the above concern, the PASCAL VOC dataset contain simple scenes with prominent and singular object features, which makes it more easy to generate pseudo labels. Also, ImageNet-1K is a image-classification dataset with objects in the image very obvious and can be easily detected. The method's generalization ability in complex images containing numerous objects could not be verified, reducing its effectiveness. More experiments on more complex images as unlabeled data are suggested.\n3. SAM2 is a very strong segmentation model. Since it is used for mask generation, why not directly generate masks that corresponding to specific classes for pseudo label generation? For example, extract all the cats in the image, and directly generate bounding boxes according to the cat mask. Alternatively, SAM2 can be used to extract image features from the detection bounding box and semantic features of a specific category, and then the similarity can be calculated to generate pseudo-labels. The necessity of using semantic IoU to filter pseudo-labels cannot be proven."}, "questions": {"value": "Refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4W6BxDst0e", "forum": "R6I8P4DuRf", "replyto": "R6I8P4DuRf", "signatures": ["ICLR.cc/2026/Conference/Submission14399/Reviewer_CSAL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14399/Reviewer_CSAL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762170988559, "cdate": 1762170988559, "tmdate": 1762924809307, "mdate": 1762924809307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Instance-Level Retrieval (ILR), a retrieval-augmented pipeline that starts from a handful of human-annotated “anchor” bounding boxes and automatically labels large pools of unlabeled images by ranking self-supervised object representations with a new “Semantic IoU” metric. On Pascal VOC, detectors trained with 1 k or 4 k adaptively retrieved boxes outperform same-size human-annotated subsets by ~0.08 mAP; when the pipeline is used to transfer VOC labels to ImageNet-1K images, a further +0.10 mAP gain is obtained. A dataset-distillation experiment shows that filtering the original VOC training set with Semantic IoU yields +0.04 mAP over random subsets. Extensive ablations and controlled scale-matched experiments are offered, together with a discussion of safeguards and ethical filters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written."}, "weaknesses": {"value": "1.All experiments are confined to the 20 Pascal VOC classes; there is no evidence that the metric remains discriminative for fine-grained or long-tail categories where inter-class visual distance is smaller.\n2.The method inherits any label noise or bias in the seed boxes; no analysis is given on how many anchors per class or what anchor diversity is minimally required, making real-world deployment risky when experts are unavailable.\n3.Semantic IoU needs pairwise Hungarian matching between every candidate and every anchor; complexity is O(|anchors|×|candidates|×patches²), so scaling to web-scale unlabeled pools (billions of images) is impractical without additional approximate-search layers that are not explored.\n4.The ImageNet transfer experiment still mixes retrieved samples with VOC originals in the best numbers; the purely adaptive rows (no VOC images) lag behind the mixed ones, indicating that the pipeline alone does not fully close the domain gap."}, "questions": {"value": "1.How does performance change when anchors come from noisy web tags or only one bounding box per class instead of clean VOC annotations?\n2.Could an approximate nearest-neighbor or inverted-index structure retain most of the mAP gain while reducing the O(N²) cost?\n3.Have the authors tried fine-grained datasets (e.g., CUB-200) where subtle visual differences might break the patch-matching similarity?\n4.What fraction of the retrieved ImageNet boxes are false positives when manually inspected, and does this vary by VOC class?\n5.Does the pipeline work for non-rigid or heavily occluded objects where SAM2 masks and DINOv2 patches may be unreliable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sUVwu2kpvO", "forum": "R6I8P4DuRf", "replyto": "R6I8P4DuRf", "signatures": ["ICLR.cc/2026/Conference/Submission14399/Reviewer_djvJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14399/Reviewer_djvJ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14399/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762952605433, "cdate": 1762952605433, "tmdate": 1762952605433, "mdate": 1762952605433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}