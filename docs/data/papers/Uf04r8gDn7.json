{"id": "Uf04r8gDn7", "number": 22391, "cdate": 1758330469033, "mdate": 1759896868727, "content": {"title": "Paraphrase-Robust Conformal Prediction for Reliable LLM Uncertainty Quantification", "abstract": "Uncertainty quantification (UQ) provides interpretable measures of predictive confidence and supports reliable decision-making with large language models (LLMs). However, existing UQ methods are often neither statistically rigorous nor robust to paraphrase variations. To address these limitations, we propose a new framework for paraphrase-robust UQ, which builds on conformal prediction to ensure valid coverage and introduces a paraphrase-aware nonconformity score to enhance robustness. The score is derived by generating semantic paraphrases of each query, training an ancillary model that both approximates and robustifies the predictive distribution, and aggregating variability across these paraphrases.  On five general multiple-choice Question Answering (MCQA) datasets and two medical MCQA datasets with $\\texttt{Qwen2.5-7B}$, our method achieves nominal coverage with compact prediction sets and demonstrates improved robustness to paraphrase shifts in an adversarial setting. The results also generalize to $\\texttt{Llama-3.1-8B}$ and $\\texttt{Phi-3-small}$, underscoring the reliability of the framework across model families. Code is available at https://anonymous.4open.science/r/paraphrase_uq-FDD8.", "tldr": "We propose paraphrase-robust conformal prediction for LLMs, achieving valid coverage and compact prediction sets that remain robust under adversarial prompt rewording.", "keywords": ["Uncertainty Quantification", "Large Language Models", "Conformal Prediction"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3d5f4359d744d87ed8f21252160d2d5861ec9ebb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a paraphrase-robust scoring function and evaluates it using existing conformal prediction frameworks on multiple-choice questions for LLMs. The framework involves training a lightweight classifier to achieve better model calibration and introducing paraphrased variations of the inputs to enhance robustness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-written paper with clear experimental demonstrations and ablations.\n- Very important and timely problem.\n- Released Code."}, "weaknesses": {"value": "- The proposed approach builds entirely on existing conformal prediction frameworks without offering new theoretical insights.\n- To preserve the theoretical guarantees of CP frameworks, the (X,Y) pairs must satisfy the exchangeability assumption, even under adversarial paraphrasing. This limitation weakens the contribution, as the proposed method effectively remains an application of CP frameworks when the test and calibration distributions are identical. I would expect a framework designed to maintain robustness under adversarial distribution shifts, which are typically unknown at calibration time.\n - As a suggestion: the adversarial paraphrasing for UQ methods has been discussed in early works: https://aclanthology.org/2025.acl-long.1429\n- Overall, my concern is about the contribution of the paper. This paper combines existing ideas for LLMs in a convincing way, but I struggle to understand the unique perspective of the paper."}, "questions": {"value": "- Do you consider any parahprashing as \"adversarial\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S83mEPnAY2", "forum": "Uf04r8gDn7", "replyto": "Uf04r8gDn7", "signatures": ["ICLR.cc/2026/Conference/Submission22391/Reviewer_9pty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22391/Reviewer_9pty"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760904564158, "cdate": 1760904564158, "tmdate": 1762942197504, "mdate": 1762942197504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to develop a paraphrase-robust uncertainty quantification framework based on split conformal prediction, which enhances the robustness of large language models under semantic variations by incorporating paraphrase-aware nonconformity scores. Experiments show that the method achieves smaller prediction sets and better stability across multiple MCQA datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents the issue that when a query has multiple expressions, even though the semantics remain consistent, it can still affect the prediction set. \n\n2. A probability vector predictor is trained using the hidden state, encoding and aggregating semantically consistent questions."}, "weaknesses": {"value": "1. I believe that, at its core, the contribution of this paper is essentially the discovery that rephrasing, even when semantically equivalent, can impact the final prediction set size. \n\n2. The reference to Figure 1 in the introduction is vague. From Figure 1 alone, I cannot discern the general work of the paper, nor can I see the comparison before and after paraphrasing. Additionally, the \"two popular CP scores\" are not clearly demonstrated.\n\n3. I believe that since robustness is mentioned, the boundary of paraphrasing should be identified—specifically, when a problematic paraphrase occurs but still does not affect the prediction set.\n\nTypo: \nAt the end of page five, the content should be appropriately adjusted. The citation of LLM-Uncertainty-Benchmark should not be added at the bottom of the page."}, "questions": {"value": "1. The idea of PA is great. For example, when performing uncertainty decomposition, we also aggregate by rephrasing the question. Have you considered methods other than using the hidden state to train?\n\n2. Does QCCP rely too much on \"Conformal prediction with conditional guarantees\"? I feel that typically, aiming for a marginal guarantee is sufficient, and there's no need to apply the conditional framework just to emphasize practical significance \n\n3. Even if we don't rephrase the question, keeping calibration and test sets consistent, would there be a significant difference? For example, if a question in the test set is rephrased, would it break the exchangeability with the calibration data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cf7RzDPxwG", "forum": "Uf04r8gDn7", "replyto": "Uf04r8gDn7", "signatures": ["ICLR.cc/2026/Conference/Submission22391/Reviewer_iUNn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22391/Reviewer_iUNn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761375133835, "cdate": 1761375133835, "tmdate": 1762942197047, "mdate": 1762942197047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Proposes a framework for conformal prediction in LLM MCQA under paraphrases by training a proxy MLP model on embeddings. The idea is that semantically similar phrases should give similar uncertainty estimates. Experiments on several QA datasets show tighter coverage and better robustness to adversarial paraphrasing under several LLM models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Clear problem and approach\n* Well written and clear presentation\n* Thorough evaluation on seven datasets and modern LLM with sufficient ablation experiments\n* Identifies potential failure mode of CP in LLM \n*"}, "weaknesses": {"value": "* Limited to QA classification tasks and not free-form generation\n* Relies on embeddings and proxy model that might be unstable \n* Dependency on paraphrase generator; a weak generator could lead of over-optimistic coverage\n* The adversrial paraphrasing experiments could be more quantitative to validate robustness claims. \n* Lack of runtime/cost analysis \n* Proposed method is incremental\n* Little theoretical analysis why paraphrase-aggregated scores preserved CP guarantees"}, "questions": {"value": "* How sensitive is performance to the quality or number of paraphrases?\n* How to extend to other task such as summarization?\n* What is overall overhead to inference in terms of wall time? \n* How to ensure the paraphrase generator produces sufficiently diverse paraphrases that are still semantically related?\n* What about using other embedding layers or different pooling aggregation such as attention weighted instead of mean pooling? \n* Can you add confidence intervals or error bars to your plots? \n* How does your method guarantee valid coverage across paraphrases? What about other forms of semantic shifts? \n* How does the choice of paraphrase generator affect robustness?\n* Why is the LLM projection head miscalibrated? Why does the proxy model fix this? Did you try calibrating the final layer with ECE loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V0AQN9YgGO", "forum": "Uf04r8gDn7", "replyto": "Uf04r8gDn7", "signatures": ["ICLR.cc/2026/Conference/Submission22391/Reviewer_vAQA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22391/Reviewer_vAQA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761501472203, "cdate": 1761501472203, "tmdate": 1762942196684, "mdate": 1762942196684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses uncertainty quantification (UQ) for large language models (LLMs) by proposing a paraphrase-robust conformal prediction framework. The key insight is that existing conformal prediction (CP) methods for LLMs are sensitive to paraphrase variations, which can lead to unstable prediction sets. To address this, the authors introduce paraphrase-aware (PA) nonconformity scores that aggregate uncertainty across semantically equivalent rephrasings of each query. Experiments on five general MCQA datasets and two medical MCQA datasets with Qwen2.5-7B, Llama-3.1-8B, and Phi-3-small show that the method achieves nominal coverage (≈90%) with 2–4× smaller prediction sets than baselines (LAC, APS) under adversarial paraphrasing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies a genuine gap in existing conformal prediction for LLMs, which is lack of robustness to paraphrase variations. This is practical and relevant, as natural language queries can be expressed in many equivalent ways.\n2. The use of paraphrase aggregation to achieve semantic invariance is intuitive and well-motivated. The three variants (mean, weighted, worst-case) provide a useful spectrum of robustness-efficiency trade-offs.\n3. The datasets, LLM families and ablations in the experiments are extensive.\n4. The method consistently achieves target coverage with substantially smaller prediction sets (often 2–4× reduction) compared to baselines, demonstrating practical value."}, "weaknesses": {"value": "1. Why does paraphrase aggregation preserve/improve coverage? The paper does not provide theoretical analysis of how averaging scores across paraphrases affects the coverage guarantee. Under what conditions does this aggregation preserve the validity of conformal prediction?\n2. While empirically robust, there are no formal guarantees (e.g., bounds on prediction set size variation) under paraphrase perturbations.\n3. The connection between semantic invariance and statistical coverage is assumed but not rigorously established.\n4. The paper does not verify that generated paraphrases truly preserve semantics. Are they evaluated by humans or using semantic similarity metrics?\n5. Paraphrases are generated by an LLM (Qwen2.5-7B) and used to evaluate the same or similar LLMs. This could introduce systematic biases.\n6. Generating 6 paraphrases per query increases computational cost by ≈6×. The paper does not report inference time or discuss computational efficiency.\n7. The paper only compares against LAC and APS, which are relatively simple scores. Recent LLM-specific UQ methods (semantic entropy, perturbation-based methods mentioned in related work) were not considered as baselines. \n8. Since paraphrases are generated from training/calibration samples and treated as additional samples, there may be data leakage or distribution shift issues not addressed."}, "questions": {"value": "1. Can you provide theoretical analysis of coverage preservation under paraphrase aggregation? Specifically, under what conditions does averaging scores across paraphrases preserve the coverage guarantees of conformal prediction?\n2. Can you prove or provide bounds on the coverage gap between S_mean(x,y) and S_prob(x,y)?\n3. How do you ensure paraphrase quality and semantic preservation? Have you validated that generated paraphrases preserve semantics (human evaluation, semantic similarity scores)?\n4. How does paraphrase quality affect the final results?\n5. What is the computational cost and inference time? Can you report wall-clock time comparisons (with and without paraphrasing)?\n6. Can you provide any preliminary results on open-ended generation, factuality verification, or other tasks mentioned in the related work?\n7. Can you compare with more recent/relevant baselines mentioned above?\n8. How does your method compare to simple ensembling or temperature-based uncertainty?\n9. Since paraphrases of calibration samples are used in training the proxy, and paraphrases of test samples are used in evaluation, is there a risk of information leakage?\n10. Have you tried generating paraphrases from a completely separate LLM to avoid circular dependencies?\n11. Why does the \"worst\" score perform poorly (Figure 7)? Intuitively, the worst-case score should be most robust to adversarial paraphrasing, but it produces the largest sets and overshoots coverage. Can you explain this counterintuitive result?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0NUJJntHq2", "forum": "Uf04r8gDn7", "replyto": "Uf04r8gDn7", "signatures": ["ICLR.cc/2026/Conference/Submission22391/Reviewer_Kxds"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22391/Reviewer_Kxds"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699297843, "cdate": 1761699297843, "tmdate": 1762942196453, "mdate": 1762942196453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}