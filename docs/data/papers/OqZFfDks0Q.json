{"id": "OqZFfDks0Q", "number": 25554, "cdate": 1758369079356, "mdate": 1759896715839, "content": {"title": "Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling", "abstract": "High-resolution (4K) image-to-image synthesis has become increasingly important for mobile applications. Existing diffusion models for image editing face significant challenges, in terms of memory and image quality, when deployed on resource-constrained devices. In this paper, we present MobilePicasso, a novel system that enables on-device image editing at high resolutions, while minimising computational cost and memory usage. MobilePicasso comprises three stages: (i) performing image editing at a standard resolution with hallucination-aware loss, (ii) applying latent projection to overcome going to the pixel space, and (iii) upscaling the edited image latent to a higher resolution with adaptive context-preserving tiling. Our user study with 46 participants reveals that MobilePicasso not only improves image quality by 18-48% but reduces hallucinations by 14-51% over existing methods. MobilePicasso demonstrates significantly lower latency, e.g., up to 55.8x speed-up, yet with a small increase in runtime memory, e.g., a mere 9% increase over prior work. Surprisingly, the on-device runtime of MobilePicasso is observed to be faster than a server-based high-resolution image editing model running on an A100 GPU.", "tldr": "Resource-efficient on-device, high-resolution (4K) image editing with improved image quality", "keywords": ["On-device  ML", "Image Editing", "Diffusion Modeling"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3cba7a6aa09366ebb5df1c257d5e12dbc2386c3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MobilePicasso, a system for efficient high-resolution (4K) image editing on mobile devices. It introduces a 3-stage hybrid pipeline consisting of (1) standard-resolution image editing with a hallucination-aware loss, (2) learnable latent projection to avoid costly pixel-space operations, and (3) high-resolution upscaling with adaptive context-preserving tiling (ACPT).\n\nMobilePicasso significantly reduces hallucinations and latency while maintaining image quality, achieving up to 55.8× speed-up and only 9% memory increase compared to prior methods. A user study with 46 participants confirms substantial improvements in visual quality and realism over existing diffusion-based image editing models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a well-designed and practical solution for high-resolution (4K) on-device image editing, combining a novel 3-stage hybrid pipeline, hallucination-aware loss, and adaptive tiling to greatly improve image quality, reduce hallucinations, and achieve remarkable latency and memory efficiency compared to prior methods."}, "weaknesses": {"value": "1. The novelty of the proposed method appears limited. Although the paper claims that operating in latent space enables efficient high-resolution image editing, similar ideas have already been widely adopted in latent diffusion models. Therefore, the conceptual contribution over existing latent-space pipelines is not entirely clear.\n\n\n2. The paper omits key engineering aspects such as on-device quantization strategies, exact model compression techniques, or hardware-specific optimizations."}, "questions": {"value": "1. It would be beneficial to include comparisons with training-free image editing methods, such as P2P, Plug-and-Play approaches, or other prompt-based guidance techniques.\n\n2. Does the method works on latest flow models like SD3.5/Flux?\n\n3. In qualitative evaluation, adding examples covering more diverse edit types, such as material and color modification, object removal or relocation, and multi-object semantic edits, would better demonstrate the generality of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "562WpQHbak", "forum": "OqZFfDks0Q", "replyto": "OqZFfDks0Q", "signatures": ["ICLR.cc/2026/Conference/Submission25554/Reviewer_iD93"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25554/Reviewer_iD93"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761544846021, "cdate": 1761544846021, "tmdate": 1762943472077, "mdate": 1762943472077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MobilePicasso, a lightweight diffusion-based system for efficient 4K image editing on mobile devices. It improves image quality by up to 48%, reduces hallucinations by 51%, and achieves up to 55× faster performance with minimal memory overhead, even outperforming GPU-based models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper effectively identifies the key challenges including hallucination of performing image editing on mobile devices, and the proposed solution appears well-designed and practical. The writing and figures are clear and easy to follow, making the work accessible and well-presented."}, "weaknesses": {"value": "It would be helpful to provide a clearer explanation of the absolute memory and computation requirements needed for real-world deployment on mobile devices. Since your work assumes an on-device setting, the main objective should arguably be to demonstrate practical feasibility rather than primarily comparing performance improvements over prior methods."}, "questions": {"value": "The questions above cover my main concerns, and overall, I find the rest of the paper well-written and technically sound."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oNaiyElmGt", "forum": "OqZFfDks0Q", "replyto": "OqZFfDks0Q", "signatures": ["ICLR.cc/2026/Conference/Submission25554/Reviewer_gSpL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25554/Reviewer_gSpL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977054585, "cdate": 1761977054585, "tmdate": 1762943471802, "mdate": 1762943471802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MobilePicasso, a three-stage hybrid pipeline for high-resolution image editing (up to 4K) on mobile devices. On-device high-resolution image editing with diffusion models faces several challenges: (1) the limited resolution of pretrained models, (2) hallucinations in high-resolution image-to-image generation, and (3) limited computational resources on mobile devices. To address these issues, the authors introduce Hallucination-Aware Loss, Data Filtering, and Adaptive Context-Preserving Tiling (ACPT). The three-stage hybrid pipeline consists of: (a) Image Editing Stage, which edits images at low resolution using InstructPix2Pix (with the SD1.5 backbone) and the proposed hallucination-aware loss; (b) Learnable Latent Projection Stage, which trains a lightweight projection model to upscale the latent features, similar to FeatUp; and (c) Upscaling Stage, which generates high-resolution images from the upscaled latents using YONOS-SR with Adaptive Context-Preserving Tiling. The hallucination-aware loss leverages a hallucination detection model (PAL4VST, ICCV 2023) to guide the denoising direction for reducing hallucinations. Adjacent Padding enables 0% tile overlap, while the Adaptive Tiling Overlap Mechanism automatically selects the optimal tiling strategy based on the input resolution. Experiments demonstrate the effectiveness of the proposed framework for high-resolution image editing under resource-constrained settings, both quantitatively and qualitatively."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed method enables 4K image editing within 42 seconds while utilizing only 1.15 GB of GPU memory."}, "weaknesses": {"value": "1. Limited novelty. Most components appear to be adapted from prior work, and the overall contribution seems largely engineering-oriented rather than conceptually new.\n2. I believe Alg. 1 & 2 represent the core contributions of this work. Therefore, they should be moved from the supplementary material to the main paper. Consider relocating the “Formulation of Diffusion-based Image Editing” section (L131–153) to the appendix instead, to make space for these key algorithms in the main text.\n3. Identifying defects in the IP2P dataset and improving data quality through preprocessing (filtering by artifact ratio and removing 15% of the training data) is also a small contribution. However, it remains unclear whether the baseline models (especially InstructPix2Pix) in Tab. 1 & 2 were also trained on the same preprocessed dataset. If not, the comparison may not be fair.\n4. Running patch-wise editing with InstructPix2Pix and then stitching the results understandably leads to visible seams and implausible images. Wouldn’t running InstructPix2Pix followed by a super-resolution model serve as a more reasonable baseline?\n5. Please specify the upper bound of memory capacity typically available on recent mobile devices to contextualize the claimed efficiency.\n6. While the proposed method is clearly designed for limited GPU memory, the current output quality is significantly below that of recent SOTA methods. The comparison set also omits several strong baselines. Please include both quantitative (runtime and memory usage) and qualitative comparisons with InfEdit [1] and SwiftEdit [2] to better illustrate the performance gap. If these SOTA models outperform significantly in quality, distillation or quantization could be a more practical direction for deployment.\n\n[1] Xu et al., Inversion-Free Image Editing with Natural Language, CVPR 2024\n\n[2] Nguyen et al., SwiftEdit: Lightning Fast Text-guided Image Editing via One-step Diffusion, CVPR 2025"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iced3EqmYW", "forum": "OqZFfDks0Q", "replyto": "OqZFfDks0Q", "signatures": ["ICLR.cc/2026/Conference/Submission25554/Reviewer_BLVj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25554/Reviewer_BLVj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992280784, "cdate": 1761992280784, "tmdate": 1762943471518, "mdate": 1762943471518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MobilePicasso, a system for efficient high-resolution (4K) image editing on mobile devices. It introduces a three-stage hybrid pipeline for standard resolution editing, learnable latent projection, and adaptive tiling. These together with a hallucination-aware loss to reduce visual artefacts. The method achieves significant latency and memory improvements while maintaining high image quality, supported by quantitative results and a user study."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1 High-resolution image editing on mobile devices is a practical and important problem.\n\n2 Well-designed three-stage pipeline that balances quality, speed, and memory.\n\n3 Hallucination-aware loss and data filtering effectively improve realism and text alignment.\n\n4 Strong empirical results with large efficiency gains and clear user-study validation.\n\n5 Clear writing and experimental organization."}, "weaknesses": {"value": "1 The definition of hallucinations in this paper is overly simplistic and lacks rigor. In Line 41, hallucinations are described merely as “unrealistic objects or elements generated by diffusion models that were not intended by the edit instruction,” which is too general and subjective. Moreover, the evaluation of hallucinations seems to rely solely on human assessments (“Careful human evaluations show that around 30% of the generated images contain hallucinations”), without a clear or reproducible quantitative metric.\n\n2 The proposed Hallucination-Aware Loss is rather incremental. It simply reuses an existing hallucination detection network (Zhang et al., 2023b) to provide an auxiliary penalty, effectively distilling that model’s capability into the diffusion model. This is a practical but limited extension rather than a novel learning objective or theoretical contribution.\n\n3 The paper mainly focuses on assembling a complete high-resolution image-editing pipeline that can run on mobile devices. While the engineering effort is impressive, the components (hallucination loss, latent projection, adaptive tiling, co-design) are only loosely connected and lack a strong theoretical or algorithmic unification. Each part shows limited novelty on its own, and the overall contribution stems more from system integration than from new learning insights."}, "questions": {"value": "In the user study, approximately how many images did each participant evaluate, and how long did a full session take?\nCould such a workload cause potential fatigue effects?\nAlso, could the authors clarify whether the participants had any background in image processing or computer vision?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FsIQYwJC2m", "forum": "OqZFfDks0Q", "replyto": "OqZFfDks0Q", "signatures": ["ICLR.cc/2026/Conference/Submission25554/Reviewer_GPpq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25554/Reviewer_GPpq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027467331, "cdate": 1762027467331, "tmdate": 1762943470988, "mdate": 1762943470988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}