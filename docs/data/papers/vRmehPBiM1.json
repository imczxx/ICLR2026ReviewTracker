{"id": "vRmehPBiM1", "number": 11535, "cdate": 1758201072564, "mdate": 1759897569450, "content": {"title": "Bidirectional Hierarchical Reasoning for Fine-grained Visual Recognition", "abstract": "Fine-grained visual recognition (FGVR) requires not only high accuracy but also human-aligned interpretability, particularly in safety-critical applications. While human cognition naturally follows a coarse-to-fine reasoning process—rapid holistic categorization for coarse-grained class followed by attention to local details for fine-grained class—existing post-hoc and ante-hoc interpretability methods fall short in capturing this hierarchy automatically. To address this gap, we propose Bi-HiR, a novel Bidirectional Hierarchical Reasoning framework that emulates human-like cognition by integrating top-down semantic reasoning with bottom-up prototype-based explanations. Specifically, Bi-HiR: (1) leverages large language model (LLM)-derived semantic priors to construct coarse-to-fine hierarchies without manual annotations; (2) introduces a joint optimization strategy where top-down priors guide bottom-up prototype learning across semantic levels; and (3) produces interpretable, step-wise visual and semantic explanations. Experiments on six FGVR benchmarks demonstrate that Bi-HiR achieves competitive SOTA performance and exhibits superior zero-shot generalization. The results also reveal the superiority of Bi-HiR’s interpretability on human trust and model error diagnose. Code is publicly available at https://github.com/duduududamax/Bi-HiR.", "tldr": "", "keywords": ["Explainable AI，Hierarchical reasoning，Fine-grained recognition"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fcaa4b8eb25f0bac6daabc6efc92b9424f692941.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work studies the task of Fine-grained visual recognition (FGVR), which demands not only high classification accuracy but also human-aligned interpretability that captures the natural coarse-to-fine reasoning hierarchy of human cognition. To meet this need, the work proposes Bi-HiR (Bidirectional Hierarchical Reasoning), a  framework that emulates hierarhical probabilities by integrating top-down semantic guidance with bottom-up prototype-based explanations. Bi-HiR automatically constructs the coarse-to-fine semantic hierarchy using Large Language Model (LLM)-derived semantic priors, eliminating the need for costly manual annotations, and utilizes a joint optimization strategy where these priors guide the learning of hierarchical prototypes. Experiments were conducted on six FGVR benchmarks (CUB-200, Cars, Aircraft, MTARSI, FGSC-23, and FMGT-28). Lastly, the evaluations demonstrated that Bi-HiR exhibits superior zero-shot superclass generalization and its interpretability results in higher human trust, as confirmed by a user study."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to understand.\n- The idea of using hierarchical decision process to make FGVR more interpretable is practical.\n- The enhanced human trust and error diagnosis achieved by Bi-HiR is impressive. This shows its advantage on the interpretability aspect for FGVR."}, "weaknesses": {"value": "### **Primary Weakness**\n\n**(1) Lack of proper citation to original technical innovations**\n\nThe “proposed” idea and method of using LLMs to automatically obtain coarse-to-fine categorical hierarchies and visual attributes are **not new** and do not originate from this work.  \n\nRegarding hierarchical label acquisition with LLMs, this idea and technique were first proposed and utilized in **CHILS** [1] and **SHINE** [2], where GPT-3.5 was employed to derive three-level label hierarchies that improved the zero-shot open-vocabulary recognition performance of VLMs and detectors.  \n\nRegarding visual attribute acquisition with LLMs and subsequent training, these ideas and techniques were: (1) originally proposed by **FineR** [3]; and (2) later extended in **FineDefics** [4], which used such pseudo-labeled attributes as auxiliary supervision to enhance the fine-grained recognition performance of large VLMs.  \n\nNevertheless, the authors have neither discussed nor cited **CHILS**, **SHINE**, **FineR**, and **FineDefics**, nor have they acknowledged these prior works when describing the innovation. **The authors should appropriately credit previous research contributions rather than implying that these ideas originate solely from this work.**\n\n---\n\n**(2) Lack of novelty compared to prior works**\n\nA follow-up concern is that, given these core techniques and ideas were already proposed in prior works as described above, what is the actual technical or conceptual novelty of this work—beyond adapting and combining existing methods for the hierarchical recognition task?  \n\nFurthermore, regarding the loss function design, what is the difference between the proposed regularization terms and the techniques used in prior work [5] (particularly Sections 3.1–3.2 in that paper)?\n\n---\n\n**(3) Why is Bi-HiR better?**\n\nFrom the experimental results, the proposed **Bi-HiR** model actually **consistently underperforms** compared to the state-of-the-art method (**CGL**) when using the same ResNet-50 backbone. Therefore, could the authors qualitatively justify the potential value or advantages of Bi-HiR? To clarify, it is acceptable for Bi-HiR to underperform SOTA models, as long as it provides distinct benefits or insights of its own.\n\n---\n\n**References**\n\n[1] Novack, Z., McAuley, J., Lipton, Z. C., & Garg, S. (2023, July). Chils: Zero-shot image classification with hierarchical label sets. In ICML, 2023.\n\n[2] Liu, M., Hayes, T. L., Ricci, E., Csurka, G., & Volpi, R. (2024). Shine: Semantic hierarchy nexus for open-vocabulary object detection. In CVPR, 2024.\n\n[3] Liu, M., Roy, S., Li, W., Zhong, Z., Sebe, N., & Ricci, E. (2024). Democratizing fine-grained visual recognition with large language models. In ICLR, 2024.\n\n[4] He, H., Li, G., Geng, Z., Xu, J., & Peng, Y. (2025). Analyzing and boosting the power of fine-grained visual recognition for multi-modal large language models. In ICLR 2025.\n\n[5] Bertinetto, L., Mueller, R., Tertikas, K., Samangooei, S., & Lord, N. A. (2020). Making better mistakes: Leveraging class hierarchies with deep networks. In *CVPR, 2020.*"}, "questions": {"value": "- The github link provided in the Abstract for code cannot be reached (404 error).\n- The presentation of this manuscript is too dense. Without reading the full text, it makes the reader hard to understand Fig 1 and Fig 2. They are overwhelming. Fig 2 contains too many architectural details. The reviewer suggests to remove the details to make it only illustrate the abstract level pipeline.\n- The reviewer is confused by the “reasoning” process claimed in this work. The decision is made in a hierarchical manner based on predicted probability. What’s is the reasoning here?\n- Is there any point in including the published venue on comparative methods? The reviewer does not get any useful information from this.\n- Please make the comparison under the same model size and training data. Otherwise, it is hard to compare."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4SLxOo1rmv", "forum": "vRmehPBiM1", "replyto": "vRmehPBiM1", "signatures": ["ICLR.cc/2026/Conference/Submission11535/Reviewer_kvuV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11535/Reviewer_kvuV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760704441188, "cdate": 1760704441188, "tmdate": 1762922629820, "mdate": 1762922629820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper (1) uses LLM to construct coarse-to-fine hierarchies without manual annotations; (2) introduce a joint optimization strategy, and (3) produces interpretable, step-wise visual and semantic explanations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Pros:\n1. The paper considers an interesting problem,\n2. The writing is easy to follow."}, "weaknesses": {"value": "1. The code of this paper is changed after the submission deadline. https://github.com/dudududa-max/Bi-HiR It is not allowed in ICLR.\n2. The paper’s core ideas shares similarity with TransHP [1], which can also be understood as coarse-to-fine reasoning/prompting process. It is not cited.\n3. As shown in Fig. 2, the paper is much like a combination of the existing techniques. Therefore, I doubt the novelty of the paper.\n4. Although the proposed method is complicated, it stills does not achieve SOTA as shown in Table 1. The effectiveness of this method is not verified."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pAH1Y2PruD", "forum": "vRmehPBiM1", "replyto": "vRmehPBiM1", "signatures": ["ICLR.cc/2026/Conference/Submission11535/Reviewer_9w2r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11535/Reviewer_9w2r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722038076, "cdate": 1761722038076, "tmdate": 1762922629113, "mdate": 1762922629113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Bi-HiR, a Bidirectional Hierarchical Reasoning framework that emulates human-like coarse-to-fine cognition for fine-grained visual recognition (FGVR). Bi-HiR integrates top-down semantic reasoning, derived automatically from LLM-generated semantic hierarchies, with bottom-up prototype learning for interpretable and accurate recognition. The hierarchy is constructed through GPT-4o prompts (no manual annotation) and used to guide prototype optimization via bidirectional semantic–visual alignment.\nExperiments on six FGVR benchmarks, CUB-200, Cars, Aircraft, MTARSI, FGSC-23, and FGMT-28, show that Bi-HiR achieves:\n- Top-3 accuracy among both interpretable and black-box baselines;\n- Improved zero-shot superclass generalization, demonstrating semantic alignment;\n- Strong human-trust results in a 115-participant interpretability study.\nOverall, Bi-HiR presents a unified interpretable reasoning model that blends concept bottleneck (CBM) and prototype (PPN) paradigms through a coherent bidirectional optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The bidirectional reasoning design elegantly bridges top-down and bottom-up interpretability. The model emulates human hierarchical cognition while maintaining strong predictive accuracy.\n2. Employing GPT-4o to construct coarse-to-fine semantic structures is a practical and scalable innovation, eliminating manual ontology engineering and allowing domain adaptation.\n3.  Results span six datasets, extensive ablations (modules a–e, LT-sep analysis), zero-shot superclass generalization, and a well-designed human-trust user study.\n4. The inclusion of subjective interpretability metrics (Accuracy, TPR, TNR, IP) provides quantitative evidence for improved user trust, rare among FGVR works.\n5. The paper includes detailed architecture, prompt examples, loss functions (Eqs. 1–12), and compliance statements on ethics and reproducibility."}, "weaknesses": {"value": "1.  While the optimization (Eqs. 1–12) is well-defined, the paper lacks theoretical analysis on convergence, semantic consistency, or the interpretability–accuracy trade-off.\n2. The reliance on GPT-4o limits reproducibility and raises fairness concerns. Testing hierarchy generation with open models (e.g., LLaMA-3, Mistral-Large) would strengthen robustness claims.\n3. The “human-like reasoning” claim remains confined to visual FGVR. Demonstrating transfer to multimodal or text–vision reasoning tasks could broaden impact.\n4. Although the 115-participant study is commendable, methodological details (randomization, significance tests, inter-rater reliability) are insufficient. Reporting confidence intervals or effect sizes would reinforce validity.\n5. Missing Contemporary References (2025) – The discussion omits several highly relevant 2025 works that would strengthen contextualization within the current landscape of interpretable reasoning:\n- IVPT (Wang et al., 2025) – Exploring Interpretability for Visual Prompt Tuning with Hierarchical Concepts; introduces hierarchical concept alignment for interpretable visual prompt tuning.\n- HCG-LVLM (Guo et al., 2025) – Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Grounding; advances hierarchical reasoning for multimodal alignment.\n- Tree-based VLM Reasoning (Elmansoury et al., 2025) – Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs; critically analyzes the limits of explicit hierarchical reasoning in large vision-language models.\n- Causal-FGVC (Zhang et al., 2025) – Learning High-Order Features for Fine-Grained Visual Categorization with Causal Inference; introduces a causal-inference-based framework for generalizable fine-grained recognition.\nIncorporating discussion of these studies would better situate Bi-HiR within the 2025 research frontier on hierarchical, causal, and interpretable reasoning in vision systems."}, "questions": {"value": "How sensitive is the automatically generated hierarchy to prompt variations or stochastic LLM outputs?\nCould open-source models (e.g., LLaMA-3, Mistral-Large) approximate GPT-4o’s hierarchy generation quality?\nDoes the LT-sep loss remain stable for deeper hierarchies (> 4 levels), or does it risk gradient dilution?\nHow would Bi-HiR perform on multimodal (image–text) datasets to substantiate its “cognitive reasoning” claim?\nCould uncertainty propagation (e.g., entropy-based routing confidence) be incorporated for calibrated explanations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2unsldpbOe", "forum": "vRmehPBiM1", "replyto": "vRmehPBiM1", "signatures": ["ICLR.cc/2026/Conference/Submission11535/Reviewer_ZkNv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11535/Reviewer_ZkNv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972277639, "cdate": 1761972277639, "tmdate": 1762922628627, "mdate": 1762922628627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Bi-HiR, a bidirectional hierarchical reasoning framework for fine-grained visual recognition that integrates top-down semantic priors from large language models (LLMs) with bottom-up prototype-based explanations. The goal is to improve interpretability and human trust while maintaining competitive recognition accuracy. Experiments are conducted on multiple fine-grained datasets, and the authors claim that Bi-HiR achieves both strong performance and superior interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. interesting integration of LLM priors\nThe paper’s idea of using large language models to automatically construct semantic hierarchies is interesting, offering a scalable alternative to human-annotated concept hierarchies.\n2. Reproducibility\nThe code is publicly accessible."}, "weaknesses": {"value": "1. Novelty is limited.\nThe proposed framework appears to be a combination of existing techniques (Grad-CAM enhancement, multi-scale fusion, prototype-based reasoning) rather than a fundamentally new algorithmic contribution. Each component has been explored before, and the integration does not seem to introduce a novel theoretical insight or a clearly justified synergy among modules.\n2. Unclear evaluation of “better explanations”\nThe paper claims improved interpretability, but this is hard to measure or verify. For example, in Figure 3, the proposed method shows smaller highlighted regions — but it is unclear whether a smaller region necessarily indicates a better explanation. The visual evidence is subjective and lacks quantitative metrics such as localization accuracy,  or concept alignment.\n\n3. Unfair or unclear comparisons\nFigure 3 only shows visualizations for the 2×2 grid inputs for Bi-HiR, while other methods’ single-input attention maps are missing. To ensure fairness, attention maps for both single and composite inputs from all baselines should be compared.\nMoreover, the meaning and motivation of using a “2×2 grid input” are unclear and seem artificial and not meaningful for evaluating fine-grained recognition.\n\n4. Ambiguous reporting in Table 1\nThe paper reports “Top-3” rankings for Bi-HiR, which may overstate performance. It would be more informative to show full rankings among interpretable methods only, or at least ensure all methods are compared under the same backbone (e.g., ResNet-50 or ConvNeXt-B). Without consistent backbones, the performance comparison is not convincing. It's better to show performance by group, i.e. same backbone group; interpretable method group. \n\n5. Overly complex and unclear framework design\nFigure 2 is visually overloaded and difficult to follow. The overall system looks like a collection of loosely connected modules rather than a coherent, streamlined framework. The flow between “Post-hoc Enhancement,” “Multi-scale Aggregation,” and “Bi-HiR Training” stages is not clearly explained.\n\n6. Contradictory ablation results (Table 2)\nAccording to Table 2, removing components (c) and (d) (semantic hierarchy and LT-sep loss) actually improves performance, raising questions about their necessity. If these components degrade accuracy, what exactly is gained in terms of interpretability? Is the explanation significantly better with them included? This trade-off should be clearly quantified and justified."}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wIUhP2fasp", "forum": "vRmehPBiM1", "replyto": "vRmehPBiM1", "signatures": ["ICLR.cc/2026/Conference/Submission11535/Reviewer_caeo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11535/Reviewer_caeo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064395559, "cdate": 1762064395559, "tmdate": 1762922627839, "mdate": 1762922627839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}