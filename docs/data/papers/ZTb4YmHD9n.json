{"id": "ZTb4YmHD9n", "number": 15601, "cdate": 1758253037762, "mdate": 1759897296347, "content": {"title": "To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking", "abstract": "Symmetry-aware methods for machine learning, such as data augmentation and equivariant architectures, encourage correct model behavior on all transformations (e.g. rotations or permutations) of the original dataset. These methods can impart improved generalization and sample efficiency, under the assumption that the transformed datapoints are highly probable, or \"important\", under the test distribution. In this work, we develop a method for critically evaluating this assumption. In particular, we quantify the amount of anisotropy, or symmetry-breaking, in a dataset, via a two-sample neural classifier test that distinguishes between the original dataset and its randomly augmented equivalent. We validate our metric on synthetic datasets, and then use it to uncover surprisingly high degrees of anisotropy in several benchmark point cloud datasets. In theory, this kind of distributional symmetry-breaking can actually preclude invariant methods from performing optimally even when the underlying labels truly are invariant, as we show for invariant ridge regression in the infinite feature limit. In practice, we find that the implication for symmetry-aware methods is dataset-dependent: equivariant methods still impart benefits on some anisotropic datasets, but not others. Overall, these findings suggest that understanding equivariance — both when it works, and why — may require rethinking symmetry biases in the data.", "tldr": "We present an interpretable metric for quantifying the degree of distributional symmetry-breaking in point cloud datasets and use show that benchmark point cloud datasets are highly anisotropic.", "keywords": ["equivariance", "symmetry", "symmetry breaking", "canonicalization", "data augmentation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ecb2f87ecd07ba743a8819b95130a949f420db10.pdf", "supplementary_material": "/attachment/7c8b3addf40e57d9853d38a31c20fe7ab7e8872a.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes a metric based on neural network (NN) binary classifier training to evaluate the degree of distributional symmetry breaking in data. The authors provide both theoretical analyses and empirical validations on several datasets, including MNIST (2D images), ModelNet40 (3D image sets), QM9 and QM7B (molecular data), and large-scale materials datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "It provides a comprehensive demonstration of the proposed ideas.\nIt includes theoretical efforts to clarify the impact of data-specific invariant and equivariant properties in terms of generalization."}, "weaknesses": {"value": "1. Unclear Problem Definition\n\n It is unclear whether this paper aims to propose a new metric, present derived theoretical findings, or introduce a new concept (distributional symmetry breaking). If the goal is to propose a metric, the paper should clearly demonstrate how and why it functions effectively as one. If the focus is on presenting theoretical findings, it should explain why those findings are significant from both theoretical and experimental perspectives. If the work intends to overcome limitations of the underlying assumption, it should show what problems actually occur and that problem is solved by removing the assumption directly. Overall, the paper seems to mix these three goals without providing sufficient support for each direction as follows. \n\n2. Weak Justification of the Impact of Motivation \n\nOn page 1, line 42, p(x) is similar to p(gx) is described as the main cause of the problem to be tackled. However, it seems likely observed only in Elesedy & Zaidi’s work. To justify why it is worth tackling, the author needs to provide more explanation and literature works, because equivariance is used in various ways, whether full or partial, implicit or explicit, in practical neural networks, this assumption could have a large impact. However, this assumption is not explicitly made in many existing works.\n\n3. Loose Connection of the Theoretical Works to Contribution\n\nIn my view, conclusions such as the one on page 5, line 216, are not novel at all, which the author also notes as intuitive; the same applies to page 6, line 289. If clarifying the conclusions as a theoretical result, it requires justification of the novelty and impact of the \"conclusions\".\n\n4. Weak Validation As as a Metric\n\nThe proposed metric is a binary classifier NN–based approach. Even if it can be used as a test setup (Lopez-Paz & Oquab, 2017), it is a training-based metric, so many hyperparameter conditions affect the variance of test results, which is a significant risk for a metric used in fair comparisons. To make a contribution as a metric, it is necessary to demonstrate robustness to the training environment and sensitivity to actual equivariance changes. (Honestly, it may not be possible to provide data-agnostic standards. Even in this case, to use it as a metric, the risks should be analyzed to determine whether they are within an acceptable range.)\n\n\n5. A Flaw in Motivation\n\nThe distributional symmetry breaking is measured on page 3, line 142. This part seems flawed, because gx may be another observed sample x', which should have p(x'), but it is integrated to be matched to p(x) again. It means that the metric tries to represent one point of the data manifold with a region of the data manifold including it, which reduces the total density of the manifold. This confuses what distribution p(x) the metric is expected to represent at an optimum, given the broken probability mass."}, "questions": {"value": "Minor comments\n\n1. Term Clarification\n\nTerms such as anisotropy, symmetry, equivariance, distributional symmetry-breaking, distributional symmetry, perfectly symmetric, and invariant are used in many places, but they should be introduced only after clear definitions and used more consistently."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5lFsCU6KHB", "forum": "ZTb4YmHD9n", "replyto": "ZTb4YmHD9n", "signatures": ["ICLR.cc/2026/Conference/Submission15601/Reviewer_BM93"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15601/Reviewer_BM93"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895796145, "cdate": 1761895796145, "tmdate": 1762925873062, "mdate": 1762925873062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explains that data augmentation and symmetrization can be harmful when invariant and non-invariant features are strongly correlated, and proposes a novel metric that measures the distributional symmetry breaking in a dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a principled analysis of when data augmentation benefits or harms generalization."}, "weaknesses": {"value": "While the paper presents a compelling framework for selective data augmentation, it could be strengthened by a deeper exploration of its practical limitations and boundary conditions. For instance, the theoretical analysis assumes access to accurate estimates of distributional alignment and bias-variance components, but the paper provides limited discussion on how these quantities can be reliably approximated in high-dimensional, real-world settings."}, "questions": {"value": "The paper’s framework relies on specific assumptions about the data distribution and the augmentation operator family. Could the authors clarify how restrictive these assumptions are in practice? For example, do they hold for common image augmentations such as Mixup or Cutout?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qDtg815IUV", "forum": "ZTb4YmHD9n", "replyto": "ZTb4YmHD9n", "signatures": ["ICLR.cc/2026/Conference/Submission15601/Reviewer_RG36"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15601/Reviewer_RG36"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997454180, "cdate": 1761997454180, "tmdate": 1762925872634, "mdate": 1762925872634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the often-overlooked issue of distributional symmetry breaking, situations where data transformations (e.g., rotations, permutations) are not equally likely under the data distribution, violating the common assumption behind data augmentation and equivariant architectures.\nThe authors propose a simple yet powerful metric m(pX) based on a two-sample neural classifier test to quantify how far a dataset deviates from perfect symmetry. They show how this measure can diagnose anisotropy in various datasets such as MNIST, ModelNet40, and QM9.\nThrough theoretical analysis on invariant ridge regression, they demonstrate that data augmentation can harm performance under asymmetric covariance when invariant and non-invariant features are correlated. Extensive experiments across 2D, 3D, and molecular datasets confirm the theory, revealing that many commonly used benchmarks are surprisingly canonicalized.\nOverall, the paper argues that understanding when and why equivariance helps requires diagnosing symmetry biases in the data itself."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces the notion of distributional symmetry breaking as a fundamental factor explaining inconsistencies in the effectiveness of equivariant or augmentation-based methods. This is an important conceptual contribution.\n\nThe proposed metric m(pX), implemented via a two-sample classifier, is elegant, interpretable, and easily applicable across data modalities (images, point clouds, molecules). It can serve as a general diagnostic tool for symmetry analysis.\n\nThe ridge regression analysis under asymmetric covariance provides deep insight into why augmentation may hurt, connecting symmetry assumptions to bias–variance trade-offs and data geometry."}, "weaknesses": {"value": "While m(pX) quantifies symmetry breaking, the paper does not provide a formal correlation analysis between this metric and downstream model accuracy or robustness — the connection remains mostly qualitative.\n\nSince m(pX) depends on a trained neural network, results may vary with architecture, capacity, and hyperparameters. The paper claims low sensitivity, but further ablation or calibration would strengthen this claim."}, "questions": {"value": "Is there an empirical relationship between m(pX) and the observed gain/loss from equivariant methods? For example, could a threshold on m(pX) predict when augmentation is beneficial?\n\nCan the proposed metric be used during training to adaptively modulate augmentation strength or enforce “symmetry regularization”? This could make the method more practical."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s9g0cOZxqY", "forum": "ZTb4YmHD9n", "replyto": "ZTb4YmHD9n", "signatures": ["ICLR.cc/2026/Conference/Submission15601/Reviewer_uapy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15601/Reviewer_uapy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124457614, "cdate": 1762124457614, "tmdate": 1762925872049, "mdate": 1762925872049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the effect of an often overlooked issue in datasets of equivariant tasks, namely that many training data distributions are not actually invariant to the transformation that the trained equivariant model is. To this end, the authors first propose a simple metric to measure the distributional symmetry breaking of a given dataset. The metric uses a single classifier that tries to detect whether an input came from the original training distribution or from an augmented version of the same distribution. In a perfectly invariant data distribution, such a classifier can only do random predictions since the two distributions are identical. The authors propose to use the performance of such a classifier to define a quantitative measure of the degree of symmetry breaking. Along with the definition of the metric for measuring the distributional symmetry breaking, the authors use ridge regression to provide conditions under which data augmentation may actually hurt performance. Finally, they evaluate the proposed metric and theoretical insights across a large range of datasets, and show that many commonly used datasets for equivariant learning exhibit substantial distributional asymmetry."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's topic is quite timely, since there is an open discussion regarding the benefits of equivariants compared to the most commonly used data augmentations. The proposed metric provides a practical and easily implementable tool for testing the distributional invariant of a distribution. This can help researchers better understand when and why equivariant models or data augmentation are preferable.\n- The theoretical analysis using ridge regression is clearly presented and provides formal examples that illustrate how augmentation can either hurt or improve performance. Although this analysis is performed in a simplified linear setting, it allows the readers to build intuition for the behavior of the more complex neural networks\n- The authors perform an extensive evaluation, covering a large range of tasks and showcasing interesting behaviors (even when they might not explicitly align with the main hypothesis of this work)."}, "weaknesses": {"value": "- There is limited discussion and investigation about a task-dependent symmetry-breaking metric. This limitation is crucial since in a lot of different settings, even in cases when the input data distribution is invariant, the ground truth labels break the symmetry. Without being able to effectively model this task-specific symmetry breaking, the proposed metric can be less useful since it only shows a partial image of the problem.\n- The results in the molecular datasets, such as QM9, contradict the main point of the paper. Although this is an important result that should be reported, it creates a more ambiguous interpretation regarding the utility of the proposed metric. \n- There is limited discussion and ablation studies about how the expressivity of the classifier and also the availability of training data can affect the proposed metric."}, "questions": {"value": "- How the number of training data and the parameter count of the classifier can affect the proposed metric. Is there any proposed heuristic for tuning such parameters, given a dataset we want to investigate?\n- How can a practitioner decide whether to use the task-dependent or the task-independent metric to measure the symmetry breaking of a given distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y6xlLmwCR8", "forum": "ZTb4YmHD9n", "replyto": "ZTb4YmHD9n", "signatures": ["ICLR.cc/2026/Conference/Submission15601/Reviewer_oMVy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15601/Reviewer_oMVy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762232423915, "cdate": 1762232423915, "tmdate": 1762925871184, "mdate": 1762925871184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}