{"id": "XXNh0JfsDN", "number": 8024, "cdate": 1758052302306, "mdate": 1763450369886, "content": {"title": "GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models", "abstract": "We propose the first unified adversarial attack benchmark for Genomic Foundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks, GenoArmory offers the first comprehensive evaluation framework to systematically assess the vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate the adversarial robustness of five state-of-the-art GFMs using four widely adopted attack algorithms and three defense strategies. Importantly, our benchmark provides an accessible and comprehensive framework to analyze GFM vulnerabilities with respect to model architecture, quantization schemes, and training datasets. Additionally, we introduce GenoAdv, a new adversarial sample dataset designed to improve GFM safety. Empirically, classification models exhibit greater robustness to adversarial perturbations compared to generative models, highlighting the impact of task type on model vulnerability. Moreover, adversarial attacks frequently target biologically significant genomic regions, suggesting that these models effectively capture meaningful sequence features.", "tldr": "We propose GenoArmory, the first adversarial attack benchmark for GFMs, and GenoAdv, a dataset designed to enhance model safety.", "keywords": ["Genomic Foundation Models", "Vulnerability", "Benchmark", "Adversarial Attacks", "Adversarial Defense"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/905352e893d8358fc7f11edc12fdce75a21500a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces GenoArmory, a comprehensive benchmarking framework to evaluate adversarial robustness of genomic foundation models by unifying datasets, tasks, attack and defense implementations, quantization settings, and interpretability tools, and reports broad empirical findings such as stronger robustness for BPE-tokenized and classifier-style models, occasional robustness gains from quantization, and attack-specific defense efficacy, supported by a released adversarial corpus and visualizations that localize perturbations to biologically meaningful regions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Robustness of GFMs is important for reliability and safety of downstream genomics applications. Establishing a benchmark can shape community practice.\n2. The framework covers diverse architectures, tokenizations, attacks, defenses, and quantization, with unified pipelines and red-teaming style reporting. This is substantial, non-trivial work.\n3. The observed robustness patterns (e.g., BPE > k-mer; quantization sometimes lowers ASR) are actionable for practitioners and could inspire follow-up research.\n4. The modification-frequency maps provide intuitive evidence that attacks concentrate on biologically meaningful regions, helping bridge ML robustness with domain knowledge."}, "weaknesses": {"value": "1. While the benchmark is thoughtfully designed, its practical applicability may be limited because genomic data and many real-world pipelines are often private or restricted. In such settings, models are typically deployed behind organizational boundaries, and access to data, labels, or system internals is constrained, which narrows the attack surface and raises questions about how the proposed attacks and defenses translate to operational contexts.\n2. A likely typo or mis-specification in the DSR metric that undermines its interpretability and the paper’s internal consistency. The manuscript defines DSR as (1 − (Adef − Aadv)/Adef) × 100%, which simplifies to (Aadv/Adef) × 100%, implying that stronger defenses that raise Adef paradoxically reduce DSR, and that in the no-defense condition (Defense = N/A) where Adef should equal Aadv, DSR should be 100%; yet Table 2 reports non-100% values for N/A. This inconsistency suggests a formula error.\n3. “Visualization of Adversarial Attacks“ relies solely on the frequency of subsequence modifications. However, this approach is insufficient to support the paper's broader conclusion that \"adversarial attacks frequently target biologically significant genomic regions\". The reliance on modification frequency alone does not provide empirical evidence that these targeted regions align with known biological landmarks.\n4. The notion of “lower rank” in Figure 3 is ambiguous. The paper states that a lower rank indicates better robustness, yet the ranking scale runs from 1 to 5 without explicitly clarifying whether 1 or 5 is considered the “low” end. \n5. The manuscript compares four classification models with one generative model (GenomeOcean) under adversarial attacks, but it does not provide sufficient methodological detail on how the generative model is adapted for classification."}, "questions": {"value": "1. The benchmark is limited to DNA-based classification tasks. Why were generative tasks (like those performed by GenomeOcean and Evo) not evaluated for adversarial robustness, especially given the significant safety concerns around generating harmful sequences?\n2. The defense strategies (ADFAR, FreeLB) were adapted from NLP. Was any ablation study performed to determine if their effectiveness stems from their core principle (e.g., frequency-aware randomization) or simply from the act of additional data augmentation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ufg0h9frdh", "forum": "XXNh0JfsDN", "replyto": "XXNh0JfsDN", "signatures": ["ICLR.cc/2026/Conference/Submission8024/Reviewer_TVsB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8024/Reviewer_TVsB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816478776, "cdate": 1761816478776, "tmdate": 1762920023653, "mdate": 1762920023653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GenoArmory, a unified and modular framework for the evaluation, benchmarking, and optimization of deep learning models in genomic data analysis. Addressing the lack of standardized evaluation pipelines in computational genomics, the authors develop a system that enables consistent, fair, and reproducible comparisons across diverse model architectures, datasets, and training setups.\n\nGenoArmory integrates three major components:\n1.\tA standardized benchmarking suite covering key regulatory genomics datasets (e.g., ENCODE, DeepSEA, GenReg) and multiple predictive tasks.\n2.\tA multi-objective optimization module that balances predictive accuracy, computational efficiency, and model complexity.\n3.\tA biologically interpretable assessment layer that evaluates motif consistency, saliency alignment, and the biological plausibility of learned features.\nThe framework is applied to over twenty state-of-the-art genomic models—including CNNs, transformers, and hybrid designs—providing a systematic, reproducible, and biologically informed comparison. Results show that GenoArmory effectively identifies performance–efficiency trade-offs and highlights the superior generalization of certain hybrid CNN–transformer architectures with reduced computational cost.\n\nOverall, GenoArmory is a rigorous and impactful contribution that fills an important methodological gap in computational genomics. It offers a transparent, standardized foundation for evaluating and improving genomic models, advancing reproducibility, interpretability, and fairness in model assessment. Despite minor limitations related to scalability and interpretability metrics, the framework is comprehensive, well-executed, and highly relevant to the ICLR community."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.  This work is timely and useful for real-world genomic modeling. It addresses the lack of standardized evaluation in deep genomics by providing a practical tool for balancing performance and efficiency.  \n\n2. It integrates benchmarking, optimization, and interpretability within one framework."}, "weaknesses": {"value": "1. The performance on very large genomic datasets (e.g., full WGS data) is not extensively tested, so the scalability of GenoArmory is unclear. \n\n2. The discussion on data shifts is limited: Model robustness under cross-cell-type or cross-species transfer is not explored."}, "questions": {"value": "1.\tHow does GenoArmory handle non-standard input modalities such as multi-omics or 3D genome data?\n\n2.\tCan the framework integrate uncertainty quantification or Bayesian evaluation for probabilistic genomic models?\n\n3.\tHow scalable is the optimization component for high-dimensional architectures or transformer-based sequence models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oDBBMlhtJY", "forum": "XXNh0JfsDN", "replyto": "XXNh0JfsDN", "signatures": ["ICLR.cc/2026/Conference/Submission8024/Reviewer_ptyu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8024/Reviewer_ptyu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869603282, "cdate": 1761869603282, "tmdate": 1762920023253, "mdate": 1762920023253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a benchmark adversarial robustness study of Genomic Foundation Models, denoted as GenoArmory. The work provides a complete overview of the robustness of these models, through four widely adopted attack algorithms and three defense strategies. Another main contribution is related to providing a standard attack and defense pipelines together with attacks samples that could be used, denoted as GenoAdv."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- A complete study of the adversarial robustness of Genomic Foundation Models is clearly important to ensure better adoption and validation of these models.\n- The provided attack pipeline to evaluate both the attacks and defenses is very interesting and easy to be adapted and used."}, "weaknesses": {"value": "- I believe that Genomic data, and consequently Genomic Models have their own propriety and corresponding constraints that should be taken into account when considered adversarial constraints in this domain. The paper lacks severally a contextual formulation of these constraints to showcase how the adversarial aim for these models differs from other modalities. \n- In line with the previous remark, the majority of the considered and implemented attacks are simply an adaption of previously available attacks in other domains (such as Images or Text) to the context of Genomic Models. While some could see such adaptation as a novelty, I don’t really see the novelty in this perspective and would have expected a rather adapted with taken into account some specific constraints. \n\nAs I rather have a background in adversarial attacks in the context of Images and Text, I obviously see the worth of the implementation and the important aspect of reproducibility. Nonetheless, I was expecting to see some specific attacks and defense methods that are adapted to the specific context and not only a simple code adaptation. Therefore, the main bottleneck for me is the novelty of the proposed methods. I may be wrong, and therefore I am open to adapting my review, and would expected the authors clarify this point."}, "questions": {"value": "- Are there constraints that the attack should satisfy to be a valid attack? \n    - In the context of images for instance, one could consider an attack budget in the $L_2$ space but I believe that extending to genomics should have its own criteria?\n    - How do you ensure a valid produced genomic? Are there some scripts that generate this? In this specific case, how do you ensure back-propagation in the case of gradient-based attacks? \n    - How do you define the distance that you refer to in line 104 in the considered context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZdFqrJCCrv", "forum": "XXNh0JfsDN", "replyto": "XXNh0JfsDN", "signatures": ["ICLR.cc/2026/Conference/Submission8024/Reviewer_nV6Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8024/Reviewer_nV6Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871910754, "cdate": 1761871910754, "tmdate": 1762920022896, "mdate": 1762920022896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive and timely evaluation of adversarial attacks on genomics foundation models. The papers present extensive experimental setups, which cover a wide array of foundation models, attack methodologies, and defense mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This large-scale benchmark is a contribution to the community. The writing is good."}, "weaknesses": {"value": "1. The paper thoroughly validates the effectiveness of various white-box attacks. However, the success of white-box attacks, given full access to the model, is a relatively well-established paradigm in the broader adversarial machine learning field. The current presentation focuses heavily on demonstrating this vulnerability, which, while important, might be perceived as confirming an expected outcome. The paper would be significantly strengthened by shifting the focus toward a deeper analysis of the nuances and surprises specific to the genomics domain. For instance, what is it about the architecture of genomic foundation models or the nature of genomic sequences that makes them susceptible in unique ways compared to models in NLP or vision?\n\n2. Section 3.2 provides a detailed overview of the attack methods. While clarity is crucial, its current form resembles a technical report or software documentation. In a scientific paper, the primary goal is to present and analyze new findings to inspire the community. I would suggest condensing this section and reallocating the space to a more in-depth discussion of the results. The core value of the paper lies not in re-stating how existing tools work, but in what the authors discovered by using them.\n\n3. For example, the paper presents several interesting observations, such as \"AT is less effective than ADFAR and FreeLB against BertAttack and TextFooler.\" This is a key finding that warrants deeper investigation. The current manuscript reports this result but stops short of exploring the underlying reasons. The discussion would be far more impactful if it addressed questions such as:\n    (1) Why does standard Adversarial Training (AT) exhibit lower efficacy in this specific context? Is it related to the discrete and high-dimensional nature of genomic data?\n    (2) Do ADFAR and FreeLB have mechanisms that are inherently better suited to the loss landscape of these particular foundation models?\n    (3) How do these observations guide the community toward designing more robust defense methods or, conversely, more effective attack strategies?\n\n    Answering questions like these would elevate the paper from a report of \"what happened\" to an insightful analysis of \"why it happened and what it means.\"\n\n4. The study exclusively employs existing attack and defense methods. While a benchmarking study is a valid contribution, the paper's impact could be significantly amplified by using the empirical findings to propose novel ideas. The authors are in a unique position, having identified \"anomalous\" phenomena (like the one mentioned above). This provides a perfect opportunity to hypothesize about, or even present a preliminary design for, a new attack or defense strategy tailored to the genomics domain. For example, could the observed weaknesses inspire a new hybrid defense mechanism? Could the patterns of a successful attack lead to a more potent, genomics-aware attack vector? This would transition the paper from a survey of the existing landscape to one that actively charts a new path forward."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JWEOrdAqpP", "forum": "XXNh0JfsDN", "replyto": "XXNh0JfsDN", "signatures": ["ICLR.cc/2026/Conference/Submission8024/Reviewer_6hYY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8024/Reviewer_6hYY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886400868, "cdate": 1761886400868, "tmdate": 1762920022561, "mdate": 1762920022561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Rebuttal/Revision Response"}, "comment": {"value": "We sincerely thank the reviewers for their insightful feedback and constructive suggestions, which have greatly improved the technical rigor and clarity of our work.\n\n  \n\n**This paper introduces the first unified, biologically grounded benchmark for adversarial attacks on Genomic Foundation Models (GFMs), enabling standardized, reproducible evaluation of model robustness across architectures, tokenization schemes, quantization strategies, and biological tasks (Sec. 3 & 4)**. Below, we summarize the key strengthened contributions and major revisions incorporated during the rebuttal.\n\n  \n\n---\n\n## Key Contributions:\n\n-   **Unified genomic robustness benchmark:** GenoArmory provides the first standardized framework for evaluating adversarial attacks and defenses on GFMs across architectures, tokenization schemes, quantization settings, and tasks.  \n      \n    \n-   **Genomics-specific vulnerability insights:** We identify domain-unique behaviors—nucleotide-level sensitivity, architecture-dependent robustness (e.g., HyenaDNA), and motif-targeted perturbations—revealing failure modes not seen in NLP or vision.  \n      \n    \n-   **Biologically validated adversarial dataset:** We release **GenoAdv**, the first adversarial dataset for GFMs, with 89% of perturbation hotspots aligned to known regulatory motifs through expert biological annotation.  \n      \n    \n-   **Quantization–robustness characterization:** We show that both standard and outlier-free quantization reduce ASR via loss-landscape flattening, while outlier removal introduces distinct robustness trade-offs.\n    \n\n  \n\n---\n\n## Major Revisions:\n\n- **Contribution Clarification:** `Reviewers nV6Z`\n\n- **New Section5:** The new 'Analysis and Insights' section delves deeper into the interpretation of our experimental results and elucidates the biological significance of the observed vulnerabilities. `All Reviewers`\n\n- **New Appendix F.4:** Clarifies how GenomeOcean is adapted and used for classification tasks. `Reviewers TVsB`  \n- **New Appendix G:** Provides formal definitions of adversarial attack distance for genomic sequences. `Reviewers nV6Z`  \n- **Revised Appendix K.2**: Corrects and clarifies the DSR formula. `Reviewers TVsB`\n\n- **New Experiments:**\n\n  - **New Appendix L.2 and Figure 6:** Adversarial attack visualization on reversed samples `Reviewer TVsB`\n\n  - **New Appendix L.3 and Table 10:** Intergrades uncertainty analysis using bayesian approximation under adversarial attacks. `Reviewers ptyu`\n\n  - **New Appendix L.4 and Table 11:** Explores model robustness under cross-species and cross-cell distribution shift. `Reviewers ptyu`\n\n  - **New Appendix L.5 and Table 12:** Provide ablation study that disentangles data augmentation from adversarial defense mechanisms. `Reviewers TVsB`\n\n  \n\n## Minor Revisions\n\n-   **Revised Figure 3 Caption:** Clarifies the interpretation of ranking numbers shown in the figure. `Reviewers TVsB`\n    \n\n---\n\nWe once again thank the Area Chair and reviewers for their valuable efforts and thoughtful comments. All revisions have been incorporated into the updated manuscript, with changes highlighted in blue."}}, "id": "n5BW7vjBBn", "forum": "XXNh0JfsDN", "replyto": "XXNh0JfsDN", "signatures": ["ICLR.cc/2026/Conference/Submission8024/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8024/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission8024/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763484260213, "cdate": 1763484260213, "tmdate": 1763484444180, "mdate": 1763484444180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}