{"id": "GSjDtnjcEM", "number": 22011, "cdate": 1758324839172, "mdate": 1759896891131, "content": {"title": "Modal Aphasia: Can Unified Multimodal Models Describe Images From Memory?", "abstract": "We present *modal aphasia*, a systematic dissociation in which current unified multimodal models\naccurately memorize concepts visually but fail to articulate them in writing, despite being trained on images and text simultaneously.\nFor one, we show that leading frontier models can generate near-perfect reproductions of iconic movie artwork, but confuse crucial details when asked for textual descriptions.\nWe corroborate those findings through controlled experiments on synthetic datasets in multiple architectures.\nOur experiments confirm that modal aphasia reliably emerges as a fundamental property of current unified multimodal models, not just as a training artifact.\nIn practice, modal aphasia can introduce vulnerabilities in AI safety frameworks, as safeguards applied to one modality may leave harmful concepts accessible in other modalities.\nWe demonstrate this risk by showing how a model aligned solely on text\nremains capable of generating harmful images.", "tldr": "We introduce modal aphasia, a surprising and systematic dissociation in which unified multimodal models demonstrate strong capabilities for generating visual content while simultaneously failing to access that same knowledge through text queries.", "keywords": ["multi-modal language models", "memorization", "safety", "unified representations"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bec66ab350b389571783e0a096e5e8e90ca1e1d7.pdf", "supplementary_material": "/attachment/4b583b84985f66b00c241f3f4fa603d039cf15d7.zip"}, "replies": [{"content": {"summary": {"value": "This paper analyzes the phenomenon of modal aphasia, where unified multimodal LLMs models may be able to illustrate concepts that they fail to accurately describe in text. The paper systematically demonstrates the existence of this phenomenon in both proprietary and open-source multi-modal LLMs, as well as showing a consequence of this phenomenon in creating vulnerabilities in multi-modal LLMs aligned to prevent harmful outputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The concept of modal aphasia is intuitive and cleverly formulated, and well-motivated by connections to studies of visual and verbal cognition in humans and phenomena such as aphantasia.\n\nIt is an interesting and potentially surprising finding that unified multimodal LLMs taught new visual concepts fail to describe them accurately in text. It is clear how this is a drawback of these models and important to understand in order to mitigate such misalignments. The connection to work on visualizing concepts for reasoning is also interesting.\n\nThe experiments on open-weights models are appreciated for reproducible science.\n\nOverall the paper is clear and well-written."}, "weaknesses": {"value": "I find the motivation from frontier models (L41–47) and tests on GPT-5 (Sec 3) unconvincing, because it is likely that GPT-5 routes image generation requests to a separate image generation model rather than specifically training the LLM and image generator in parallel. The [GPT-5 system card](https://cdn.openai.com/gpt-5-system-card.pdf) only mentions GPT-5 accepting textual or image input, but noticeably does not discuss image generation.\n\nIt’s not obvious to me that performance when fine-tuning on a new visual concept would be comparable to zero-shot evaluation of the pretrained model’s knowledge of visual concepts. It could be that fine-tuning on a single concept alone degrades the LLM’s general reasoning abilities and differs from the dynamics of pretraining. Why not also perform a similar zero-shot test to that in Sec 3 applied to open models?\n\nI’m not sure if the effect demonstrated in Sec 5 is due to modal aphasia. Modal aphasia was previously defined (L32-33) as the inability to access knowledge in text which can be expressed in images, while Sec 5 discusses the case where a visual concept can still be described in text using a circumlocution.\n\nOverall, I think there is a valuable conceptual and methodological contribution here, and I'm willing to reconsider my score if the significant points above are addressed."}, "questions": {"value": "The paper focuses on modal aphasia where a concept can be illustrated but not described in text. Have you considered the reverse phenomenon? (Models being able to describe concepts in words that they struggle to illustrate.) Or whether this could generalize to other modalities which are now being processed by unified multimodal LLMs?\n\nThere is some evidence that images may provide supervision for language models that are trained multimodally, reflected in visual reasoning in text, beyond what is learned from text alone. [1–3] On the other hand, this paper’s results suggest that textual description of visual concepts is not readily learned from text-image pairs. How should we interpret these findings in this context?\n\nDoes the effect also hold when the model’s visual parameters are trainable? (vs. L298)\n\nShould “ChatGPT-5” be “GPT-5”?\n\nFigs 4–5 are missing a visualization of the baseline (random) performance, making it harder to visually interpret the results.\n\n[1] Zhang et al. Visual Commonsense in Pretrained Unimodal and Multimodal Models. NAACL 2022\n\n[2] Liu et al. Things not Written in Text: Exploring Spatial Commonsense from Visual Signals. ACL 2022\n\n[3] Alper et al. Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding. CVPR 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ciqjIsXHmF", "forum": "GSjDtnjcEM", "replyto": "GSjDtnjcEM", "signatures": ["ICLR.cc/2026/Conference/Submission22011/Reviewer_tN39"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22011/Reviewer_tN39"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620293227, "cdate": 1761620293227, "tmdate": 1762942019027, "mdate": 1762942019027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to all reviewers"}, "comment": {"value": "We thank all reviewers for their constructive feedback! We are happy to hear that reviewers found our findings of modal aphasia \"original and theoretically significant\" (uwWN), \"of great importance\" (y19N), \"interesting and potentially surprising\" (tN39), and supported by \"robust\" and \"well designed\" experiments (uwWN, y19N).\n\nWe are currently incorporating the feedback into our paper and will upload a revised version by November 24th.\n\nWe address common inputs raised by multiple reviewers below:\n\n> 1. For experiments on closed-source models such as GPT-5, we cannot know if the model is truly unified, or if it uses a sub-model to generate images (y19N, Zj2z, tN39)\n\nWe agree that we cannot know this for certain. However, there is significant evidence that OpenAI's recent models use a *joint representation space* for images and text (in contrast to models before the current GPT-4o that indeed called an external image generator with text prompts).\n\nFor instance, the [official release for GPT-4o image generation](https://openai.com/index/introducing-4o-image-generation/) mentions \"We trained our models on the joint distribution of online images and text\", \"Because image generation is now native to GPT‑4o, you can refine images through natural conversation. GPT‑4o can build upon images and text in chat context, ensuring consistency throughout\", or, \"Native image generation enables 4o to link its knowledge between text and images\".\n\nGPT-5 exhibits similar capabilities (e.g., accurate image editing across messages) that are only feasible if images and text are represented in a unified space.\n\nThus, even if the final image generation is indeed performed by some separate image decoder model, this is likely done based on representations that can be decoded into both images and text.\n\nNevertheless, we do not have proof that this is how GPT-5 works\n(and it is possible that the image decoder itself has memorized certain concepts). This is precisely why we also include a controlled study on open models where we can control exactly what happens and guarantee that memorization happens in the shared representation space.\n\n> 2. For open models, why do you train only the LLM backbone? (uNr3, tN39)\n\nAs mentioned above, the point of our experiments with open models is to precisely control where the memorization occurs in the model, and guarantee that this is happening in weights that are shared across modalities, rather than in the parameters of a modality-specific encoder or decoder.\n\nIf we were to finetune all parameters, it is possible that memorization would occur in the image decoder, in which case modal aphasia would be expected and unavoidable.\n\nBy freezing all modality-specific components and fine-tuning only the shared LLM backbone, we ensure that any visual knowledge that is memorized can (in principle) also be decoded through text. The fact that this doesn't happen shows that a non-trivial form of modal aphasia must be present.\n\n> 3. The Safety case-study is not evidence of modal aphasia. The effects could be due to poor robustnes to prompt manipulations. (Zj2z, tN39)\n\nWhile we agree that we cannot guarantee that the effects are due to modal aphasia, we take multiple steps to isolate spurious effects due to changes in prompts.\n\nOur safety fine-tuning is simplistic due to practical constraints, but we did aim to make it robust to different phrasings. The training and validation prompts include a diverse set of phrasings, including adversarial ones (e.g., \"bar3 f00t illustration.\"), and the resulting models are reasonably robust against such rephrasings.\nMoreover, we ensured that the phrase \"secondary balance units\" is not used online (we found exactly 2 Google search results at the time of writing, none related to feet).\n\nWe thus believe it is unlikely that our results are just due to prompt engineering. Instead, the outcome of the study aligns with modal aphasia: the fine-tuned Janus-Pro models learn the visual concept of feet. But since the text modality does not \"understand\" this representation, it learns to simply refuse for *prompts mentioning the word \"feet\"* instead of refusing to generate *images containing the concept of feet*.\n\nIf the reviewers have suggestions for alternative experiments that could be more convincing, we would be happy to try them out!"}}, "id": "hcI4rJzILs", "forum": "GSjDtnjcEM", "replyto": "GSjDtnjcEM", "signatures": ["ICLR.cc/2026/Conference/Submission22011/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22011/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22011/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763647938128, "cdate": 1763647938128, "tmdate": 1763647938128, "mdate": 1763647938128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the concept of modal aphasia, where unified multimodal models accurately generate images yet fail to generate the same knowledge through text. The authors find that this phenomenon persists across different model architectures and training procedures. They further highlight its implications for AI safety: safety interventions applied to one modality do not reliably transfer to another (e.g., a model aligned in text may still produce harmful images)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses a fundamental question in unified multimodal modeling: whether such models can express the same knowledge consistently across modalities. \n* It also provides a comparative analysis of both proprietary and open-source models to support the generality of its claims."}, "weaknesses": {"value": "* The experimental design is questionable. The authors train their unified multimodal models *only* on the image generation task—learning to produce images from text prompts. This setup naturally biases the model toward visual generation without ensuring it can express the same concepts in language, since it is not trained on captioning tasks. A more appropriate design would jointly train the model on both image generation and image captioning using the same image–text pairs, then compare its performance across modalities.\n\n* The paper conflates modal aphasia with a prompt engineering loophole. Regarding safety risks (Section 5), the authors interpret reduced refusal on real words (\"feet\") vs. rare expression prompts (\"secondary balance units\") as evidence of modal aphasia. However, this may not necessarily be due to a modality gap. It may simply be that the text input pipeline is not robust to rephrasing / prompt engineering. Moreover, the experiment is based on a single example (\"feet\" vs. \"secondary balance units\"), making its findings too limited in scope to support a general claim.\n\n* The assumption that modern unified architectures train image and language jointly \"from scratch\" is invalid. Section 3 reports initial experiments on ChatGPT, whose model details are undisclosed. For instance, the model may not be a single jointly trained vision-language architecture, but rather a combination of a separate vision-language model and an image generation model accessible through a unified chat interface. Furthermore, this statement does not hold for the two open-sourced models analyzed in Section 4, Janus-Pro and Harmon, neither of which is trained jointly from scratch. Harmon combines an image generation model (MAR) and LLM (Qwen2.5-1.5B-Instruct), which are trained separately on large-scale datasets describing distinct modalities and concepts that far exceed the scale of data used for vision–language alignment. The same applies to Janus-Pro, which combines separately trained components (DeepSeek-LLM and SigLIP). Consequently, the image and language components capture different visual and linguistic concepts, making modal aphasia an expected, not novel, outcome."}, "questions": {"value": "The paper poses a very interesting question, but the experiments are poorly designed and lack generalizability. As mentioned in the weaknesses, it would be important to examine what happens when the models are trained on both image generation and captioning tasks using the same image–text pairs. Would the same trend persist under this training setup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gvgp67cnFZ", "forum": "GSjDtnjcEM", "replyto": "GSjDtnjcEM", "signatures": ["ICLR.cc/2026/Conference/Submission22011/Reviewer_Zj2z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22011/Reviewer_Zj2z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937236676, "cdate": 1761937236676, "tmdate": 1762942018683, "mdate": 1762942018683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper first points out that current unified multimodal models exhibit a systematic dissociation, termed modal aphasia, where they can generate highly accurate visual content but fail to access the same knowledge through text queries. To address this, the paper proposes a series of controlled experiments using open-weight unified models to analyze how this dissociation emerges across architectures and training setups. The study aims to reveal the inherent limitations of current multimodal knowledge transfer mechanisms and highlight the potential safety risks of modality-specific alignment, providing insights for developing models with genuinely unified cross-modal understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper introduces the new concept of modal aphasia, identifying a systematic dissociation between visual and textual understanding in unified multimodal models. This is an original and theoretically significant contribution that reframes existing assumptions about cross-modal knowledge transfer.\n\n* The authors demonstrate modal aphasia not only in frontier models such as ChatGPT-5, but also in controlled experiments with open-weight models (Janus-Pro, Harmon). This dual-level validation strengthens the robustness and generality of the findings.\n\n* The authors release code, data, and detailed experimental procedures, enhancing transparency and reproducibility. Their unified rubric-based evaluation also provides a standardized way to measure multimodal consistency."}, "weaknesses": {"value": "* The paper successfully identifies modal aphasia as a systematic failure of multimodal models, but it does not offer a clear theoretical explanation or model-level mechanism to account for this behavior. The contribution remains largely descriptive rather than explanatory.\n\n* Most experiments are conducted on controlled synthetic datasets such as fictional faces and geometric patterns. While these setups enable variable control, they do not demonstrate whether modal aphasia persists in realistic multimodal tasks such as captioning or retrieval. This limits the external validity of the findings.\n\n* Although the authors link modal aphasia to potential safety risks, the provided evidence is based on a single case study involving a narrow concept category. The safety implications would be more convincing if supported by broader empirical evaluation across multiple harmful concept types or adversarial prompt settings.\n\n* The conclusion suggests that allowing models to visualize concepts during reasoning may mitigate modal aphasia, but the paper does not propose any concrete implementation or experimental verification of this idea. This limits the practical contribution of the work.\n\nI would like to discuss these points with the authors during the rebuttal stage and will adjust my score based on their responses and the feedback from other reviewers."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5WluPJN7su", "forum": "GSjDtnjcEM", "replyto": "GSjDtnjcEM", "signatures": ["ICLR.cc/2026/Conference/Submission22011/Reviewer_uwWN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22011/Reviewer_uwWN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762332188497, "cdate": 1762332188497, "tmdate": 1762942018328, "mdate": 1762942018328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a problem in MLLMs called Modal Aphasia, a phenomenon that MLLMs can accurately memorize concepts visually but fail to articulate them in writing. This work starts with a case study of poster generation with GPT5 to reveal this problem. And then created two synthetic datasets to further validate the existence of this issue. Related AI safety concerns are raised with a case study."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "S1: This paper is clearly written and easy to follow\n\nS2: The found problem of modality imbalance in image/text generation fidelity is of great importance, and the naming is fun and accurate.\n\nS3: The experiments to quantify and validate modal aphasia are well designed"}, "weaknesses": {"value": "W1: This work focuses on the modality imbalance problem of MLLMs in image/text generation. There are related studies/benchmarks in image/text understanding about modality imbalance in VLMs, which are worth discussing to better position this work.\n\nW2: Since GPT5 is a proprietary model, there are rumors that its image generation is routed through another “sub-model” of GPT5. If so, the modality imbalance problem in image/text generation is kind of expected because of such a mismatch. I would like to know the authors’ thoughts on this."}, "questions": {"value": "Please refer to W2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2YjLQ3IFQv", "forum": "GSjDtnjcEM", "replyto": "GSjDtnjcEM", "signatures": ["ICLR.cc/2026/Conference/Submission22011/Reviewer_y19N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22011/Reviewer_y19N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762486102692, "cdate": 1762486102692, "tmdate": 1762942018146, "mdate": 1762942018146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an interesting phenomenon called modal aphasia, where leading unified multimodal models can faithfully generate almost perfect visual outputs but fail to describe details and concepts verbally. The authors demonstrate modal aphasia through observational experiments on GPT-5 and controlled fine-tuning on open-weight models. They also conduct a targeted case study to demonstrate bypassing safeguards by exploiting the modal aphasia."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper is well written. It starts with a clear motivation that derives from our daily interactions with commercial multimodal models. Then, the authors employ an interesting study of generating visually vs. describing verbally upon movie posters in frontier models, showing the prevalence of the modal aphasia. Beyond observational experiments, the authors design crisp synthetic data and fine-tuning experiments to show that modal aphasia can stem from more than just naive image memorization, but a systematic discrepancy of knowledge and concept understanding across modalities. Finally, they exploit a harmful use case if modal aphasia is not properly addressed. Overall, this paper is coherent and a joy to read."}, "weaknesses": {"value": "1. The controlled experiments on open-source models only examine two open-source image generators. Also, the scale of test data is relatively small (below 200).\n\n2. In the controlled fine-tuning for tracing the origin of modal aphasia, activating only the LLM backbone while freezing other components may not reflect real-world training."}, "questions": {"value": "See Weaknesses. Also,\n1. I am curious to see what happens with chain-of-thought prompting in these unified multimodal models? For example, instead of describing features in text independently, what will happen if the model is explicitly asked to \"visualize then describe\"?\n\n2. The definition of modal aphasia sounds relevant to modality imbalance, which has already been well-identified and mechanistically understood in quite a few previous works. How do the authors view the similarity and difference compared with modality imbalance in general VLMs or multimodal models?\n\nIn general, the paper’s claims and findings are self-contained, and weaknesses are relatively minor. I’d be happy to raise the score if the questions are adequately addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1H02gHLiE0", "forum": "GSjDtnjcEM", "replyto": "GSjDtnjcEM", "signatures": ["ICLR.cc/2026/Conference/Submission22011/Reviewer_uNr3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22011/Reviewer_uNr3"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762548824451, "cdate": 1762548824451, "tmdate": 1762942017821, "mdate": 1762942017821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}