{"id": "gIpuW5Ekiw", "number": 23658, "cdate": 1758346827755, "mdate": 1759896802754, "content": {"title": "Universal Set Transformer: A Scalable and Interpretable Set/Multiset Architecture", "abstract": "The advent of the set transformer (ST) brought about a new method of permutation equivariant modeling by leveraging cross-element interactions. However, ST is still subject to the fundamental challenge of transformers: scaling efficiently with large input sizes. Mini-batch consistent (MBC) methods were developed to address this problem by maintaining permutation equivariance while alleviating context fragmentation when processing partitioned sets. However, current MBC methods limit expressiveness and render the models incapable of producing element-wise contextualized representations and attention scores for prediction explainability. Therefore, the choice between ST or MBC methods results in a tradeoff between expressiveness and large set processing. To reconcile this tradeoff we propose the Universal Set Transformer (UST), a generalization of ST which is mini-batch consistent without sacrificing expressiveness. Additionally, we introduce multiset attention which leverages the MBC property to significantly reduce the computational cost of processing multisets while maintaining mathematical equivalence with standard multi-head attention. We show that UST is competitive with ST's performance while using less memory and outperforms other MBC methods in various benchmark tasks. Finally, we show that UST is capable of producing both whole-set and element-wise representations and demonstrate prediction explainability via attention scores.", "tldr": "", "keywords": ["Transformers", "Sets", "Multisets", "Explainable AI"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8bcba0da71dfb369a5c508068bad7ef2802d169c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Universal Set Transformer (UST), which combines a set transformer with mini-batch consistency (MBC), resulting in a methodology that is both expressive and has the ability to handle large sets. Set and elements can be represented, performance is often better than baselines, and explainability is possible by means of element-wise attention scores."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The formulation of element-wise representation into the multiset attention is theoretically interesting. The integration of this multiset attention into an MBC processing, is practically relevant. The experimental results are convincing, the performance is good across tasks, the stability w.r.t mini-batch size is good."}, "weaknesses": {"value": "The results and discussion read as if there are only advantages of the proposed method. What are limitations, where does UST not perform well? There is a discussion section, but it does not reflect on the possible disadvantages of UST.\n\nMultiset attention is here posed as a completely new thing. In \"Unlocking Slot Attention by Changing Optimal Transport Costs\" [1] there is also attention across the elements in the multiset. The reference is missing in the related work."}, "questions": {"value": "What is the difference between the multiset attention as presented in this paper, compared to ref [1] (see above)? If this distinction is significant, and can be motivated in technical/math terms, then I am willing to increase my score, because it is a good paper but this remaining point needs clarification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZKub4lF0q1", "forum": "gIpuW5Ekiw", "replyto": "gIpuW5Ekiw", "signatures": ["ICLR.cc/2026/Conference/Submission23658/Reviewer_pUVJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23658/Reviewer_pUVJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761153900390, "cdate": 1761153900390, "tmdate": 1763015043659, "mdate": 1763015043659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a minibatch consistent permutation invariant model based on the set transformer. The model can produce elementwise representations of set while previous methods needed to use an invariant pooling mechanism which condenses the elementwise representations into a single vector or set of vectors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method builds upon the weaknesses of prior works which depend on a pooled set representation.\n\n- The proposed method makes use of a flash-attention-like algorithm in the attention implementation to perform full attention while keeping the MBC property."}, "weaknesses": {"value": "- L236 states: \"We also found the unbiased gradient correction technique proposed by Willette et al. (2023) ineffective in our models.\" --> The referenced unbiased gradient technique was specifically derived for a model which has only elementwise transformations before an invariant pooling layer. This is the case with DeepSets, MBC, and UMBC. The proposed model uses a completely different architecture with transformer layers, so it is misleading to call the cited method ineffective, as it was not designed for the proposed setup.\n\n-  L257: How can a minibatch containing only a few points perform worse? Is it only because the gradient contains a few points? If this is the case, it should only affect training and not inference, right?"}, "questions": {"value": "- Was UMBC in figure 1 (right) and table 3 trained with the full unbiased approxiamtion scheme?\n\nOther than the Guassian clustering experiment which used DBSCAN to identify multisets by clustering close points, what other algorithms are used to identify multisets? I would be interested to see a latency tradeoff of scanning for multisets and performing the multiset attention vs. just doing regular attention without scanning for multisets. \n\nFor instance, if the set size is very large and there is only on repeat, then the scan would probably take longer than just processing the set normally with repeats. At what rate of repeat elements, does multiset processing show lower latency?\n\n---\n\nMy main concern regards whether or not the full unbiased gradient approximation was used in the training of the UMBC baseline as well as the ablation study of the multiset latency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "izX89dI72U", "forum": "gIpuW5Ekiw", "replyto": "gIpuW5Ekiw", "signatures": ["ICLR.cc/2026/Conference/Submission23658/Reviewer_Ykyq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23658/Reviewer_Ykyq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761543494843, "cdate": 1761543494843, "tmdate": 1762942749105, "mdate": 1762942749105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Universal Set Transformer (UST) which is a variant of the Transformer architecture that is specific for sets. The primary characteristics of the model are: 1. it's mini-batch consistent (MBC), meaning that different partitions of the set can be processed sequentially as \"mini-batches\" to reduce the peak memory requirements, and 2. it's efficient for multisets by removing repetitions and adding a multiplicity to each element in. On MNIST point could classification UST matches previous methods in the full set setting and outperforms in the mini-batch setting. Furthermore, UST shows favorable performance on both a synthetic clustering and bioinformatics taxonomy task."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- the UST is a clear improvement over previous set based neural network architectures for large input sets and multisets\n- supporting multisets by adding the multiplicity is a neat way of supporting repeated elements in MSA"}, "weaknesses": {"value": "- one of the main points that the paper focuses is the memory footprint for large sets for (cross-)attention with O(kn). but papers like \"Self-attention Does Not Need O(n^2) Memory\" by Rabe and Staats show that this can be avoided by a more efficient implementation. \n- the multiplicity is computed only for the input multiset. in later layers elements can still become more similar and collapse to \"effectively equal\" elements, hence the proposed architecture does not handle multisets in all generality"}, "questions": {"value": "1. It would be helpful to contextualize the work within the broader literature of efficient Transformer implementations. Does something like Rabe and Staats, 2021, eliminate the need for set-specific architecture designs for memory efficiency?\n2. How do you compute multiplicity for similar but not equal elements? Or do elements count as repitions only when they are exactly equal?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "guQXkDcgGd", "forum": "gIpuW5Ekiw", "replyto": "gIpuW5Ekiw", "signatures": ["ICLR.cc/2026/Conference/Submission23658/Reviewer_ajaB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23658/Reviewer_ajaB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762401862915, "cdate": 1762401862915, "tmdate": 1762942748815, "mdate": 1762942748815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The Universal Set Transformer (UST) is a new architecture that enables scalable and interpretable transformer-based modeling of sets and multisets.\n- It introduces a mathematically consistent mini-batch processing framework (MBC) that ensures identical results whether a set is processed all at once or in shards.\n- UST adds Multiset Attention (MSA), which efficiently handles duplicate elements and reduces computational cost while maintaining full expressivity.\n- Experiments show that UST matches or exceeds prior models' accuracy while using less memory and scaling effectively to very large sets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written, clearly structured, and effectively explains complex concepts like mini-batch consistency and multiset attention.\n- The paper introduces the first transformer architecture that achieves true mini-batch consistency while preserving full self-attention expressivity.\n- The architecture maintains attention-based interpretability while achieving strong accuracy and scalability across diverse tasks."}, "weaknesses": {"value": "- UST still requires storing a full set instance before pooling, which limits true constant-memory scalability.\n- The experimental evaluation is mainly limited to controlled benchmark and bioinformatics datasets, making it unclear how the model would perform on large-scale, real-world applications.\n- The paper lacks a detailed analysis of interpretability, providing limited evidence on how attention scores meaningfully explain model predictions.\n- Comparisons with more recent efficient transformer variants or non-attention-based set models are missing."}, "questions": {"value": "- Are the gradients in UST theoretically equivalent to those obtained from full-set training, or only approximately consistent?\n- Is there any quantitative or qualitative evidence that attention scores in UST provide meaningful interpretability?\n- How does UST perform when applied to variable-sized sets with very different cardinalities across samples?\n- Can the proposed framework be combined with sparse attention methods to further reduce computational cost?\n- Some citations look strange to me. For example, \"Deep Sets (Zaheer et al. (2017)) and FSPool (Zhang et al. (2020))\" should be \"Deep Sets (Zaheer et al., 2017) and FSPool (Zhang et al., 2020).\"\n- In Line 256, please check grammar for \"Mini-batches containing few points cause these models suffer from context fragmentation\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There is no particular ethics concern."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xau44Mf3wl", "forum": "gIpuW5Ekiw", "replyto": "gIpuW5Ekiw", "signatures": ["ICLR.cc/2026/Conference/Submission23658/Reviewer_VhCM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23658/Reviewer_VhCM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762580787777, "cdate": 1762580787777, "tmdate": 1762942748523, "mdate": 1762942748523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}