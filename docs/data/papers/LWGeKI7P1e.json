{"id": "LWGeKI7P1e", "number": 16058, "cdate": 1758259296144, "mdate": 1759897265033, "content": {"title": "Data Forging Attacks on Cryptographic Model Certification", "abstract": "Privacy-preserving machine learning auditing protocols allow auditors to assess models for properties such as fairness or robustness, without revealing their internals or training data. This makes them especially attractive for auditing models deployed in sensitive domains such as healthcare or finance. For these protocols to be truly useful, though, their guarantees must reflect how the model will behave once deployed, not just under the conditions of an audit. Existing security definitions often miss this mark: most certify model behavior only on a *fixed audit dataset*, without ensuring that the same guarantees *generalize* to other datasets drawn from the same distribution. We show that a model provider can attack many cryptographic model certification schemes by forging training data, resulting in a model that exhibits benign behavior during an audit, but pathological behavior in practice. For example, we empirically demonstrate that an attacker can train a model that achieves over 99\\% accuracy on an audit dataset, but less than 30\\% accuracy on fresh samples from the same distribution.\n\nTo address this gap, we formalize the guarantees an auditing framework should achieve and introduce a generic protocol template that meets these requirements. Our results thus offer both cautionary evidence about existing approaches and constructive guidance for designing secure, privacy-preserving ML auditing protocols.", "tldr": "", "keywords": ["audit", "confidentiality", "zero-knowledge", "fairness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f195a304b8ee8155be35ae7667b00df4df7ff6b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel attack towards cryptographically certified machine learning (ML) models. It shows that certifying ML models with pre-revealed audit datasets can be exploited by an adversary to forge models that satisfy the certification requirements while causing severe mispredictions on fresh inputs. The attack leverages the knowledge of the audit dataset to strategically construct the training data, enabling the adversary to manipulate the model's behavior on unseen data while still passing the certification checks. Experimental results demonstrate that this method can cause ML models to have less than 30% accuracy on fresh inputs while still being certified to have 99% accuracy on the audit dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Novel attack vectors on cryptographically certified ML models, highlighting the severity of data-dependant vulnerabilities.\n- Promising methodology to achieve model forgery while maintaining stealth.\n- Clear presentation and easy-to-follow methodology for the proposed attack."}, "weaknesses": {"value": "- The threat model can be better explained.\n- The evaluation can be extended to include more diverse datasets and model architectures.\n- The impact of training dataset size can be further explored."}, "questions": {"value": "This paper proposes an interesting lens into the data-dependant vulnerabilities of certified cryptographic ML models. It brings a new perspective to the security of privacy-preserving ML models. There are, however, several concerns and questions about the proposed attack and its implications:\n\n- **Threat Model Clarification**: The threat model could be better defined. Specifically, what are the capabilities and limitations of the adversary? Why such capabilities are realistic? In particular, if the training dataset is pre-determined and fixed, then this attack would not be feasible. By producing a hashing for all the training data, the adversary cannot freely choose the training data to manipulate the model. That would be an easy defense against this attack.\n- **Generalizability of the Attack**: The evaluation is limited to tree-based models that only do binary classification. How would the attack perform on more complex models, such as deep neural networks, or on multi-class classification tasks? Would the attack still be effective in these scenarios?\n- **Impact of Training Dataset Size**: How does the size and representativeness of the training dataset affect the success of the attack? How many training data points are needed for the adversary to successfully launch this attack?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4cDQ8uzgc9", "forum": "LWGeKI7P1e", "replyto": "LWGeKI7P1e", "signatures": ["ICLR.cc/2026/Conference/Submission16058/Reviewer_DHwq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16058/Reviewer_DHwq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761370276025, "cdate": 1761370276025, "tmdate": 1762926251106, "mdate": 1762926251106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies data-dependent vulnerabilities in cryptographic auditing methods for machine learning models. The paper proposes an attack strategy that passes cryptographic certification while undermining the goals of those certifications for real-world performance. The paper also conducts experiments to evaluate the performance of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[+] The paper shows that that a model provider can attack many cryptographic model certification schemes by forging training data.\n\n[+] The paper formalizes the guarantees an auditing framework should achieve.\n\n[+] The paper introduces a generic protocol template that meets these requirements."}, "weaknesses": {"value": "[-] The problem scope studied in the paper seems limited. The paper establishes the attacks for decision trees, which restricts the generalizability of the proposed method in the paper. It remains unclear whether the proposed attack framework can be extended to other models, such as different variants of decision trees, ensemble methods (e.g., random forests), deep neural networks, and diffusion models. \n\n[-] The investigation on the potential detection defenses is insufficient. The paper considers the detection methods that check whether the training data and audit data were from the same distribution. On the other hand, the paper claims that the proposed attack can achieve only 30% accuracy on new samples from the same distribution. However, for the attack proposed in the paper, it can be easily detected by using accuracy on new samples from the same distribution, where a large performance drop would serve as an indicator of compromise. Additionally, the paper overlooks a discussion of existing detection defenses [1,2,3] against data poisoning attacks, many of which do not rely on distributional similarity, and propose different detection methods.\n\n[-] The discussions of data poisoning attacks are not sufficient. The paper claims that data poisoning attacks involves subtle, often small-scale perturbations to the training data. However, there are many other forms of poisoning attacks beyond such perturbations, including backdoor poisoning that can compromise models with visible data features (e.g., stickers). Moreover, it remains unclear whether the paper considers label perturbations or only feature-level manipulations. The authors should also clarify how the proposed method performs under large-scale perturbations, discuss its strengths and limitations in such scenarios, and specify any constraints on the poisoning ratio within the training data.\n\n[-] The paper fails to discuss existing during-training defenses and post-training defenses against traditional data poisoning attacks. Furthermore, the paper does not analyze whether these existing defense strategies could mitigate the proposed attack, which is important for understanding the novelty and real-world robustness of the attack.\n\n[-] The paper does not provide an analysis of the computational complexity of the proposed method. Additionally, the distribution similarity–based detection lacks any discussion of its computational cost or scalability when applied to large datasets or high-dimensional features. Moreover, it would be beneficial to compare the computational efficiency of the proposed attack with traditional data poisoning attacks.\n\nReference.\n\n[1] De-Pois: An Attack-Agnostic Defense against Data Poisoning Attacks, 2021.\n\n[2] Deep Probabilistic Models to Detect Data Poisoning Attacks, 2019.\n\n[3] A survey on data poisoning attacks and defenses, 2022."}, "questions": {"value": "[1] Could the authors discuss how to perform sampling for $S_{audit}$? What criteria or strategies are used to determine which samples are selected, and how is the sample size decided? Additionally, it would be helpful to discuss whether there is any potential overlap between the audit samples and the training data, and if so, how such overlap might affect the validity of the detection process.\n\n[2] Could the authors clarify which steps in the proposed method can be omitted and under what conditions? For the omitted steps, it would be better to discuss the resulting characteristics of $ f $ and how such omissions may influence its performance or robustness. Can the proposed method still work in Shamsabadi et al. (2022)?\n\n[3] Are there any different impacts of different thresholds (e.g., 0.95) on the proposed method? How to optimally define these thresholds, and what is their sensitivity to variations? How would the parameter $\\varepsilon$ influence the effectiveness and stability of the proposed method?\n\n[4] Could the authors explain why it is unlikely that other statistical tests will be effective in detecting the attack?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pEr6XfyBtc", "forum": "LWGeKI7P1e", "replyto": "LWGeKI7P1e", "signatures": ["ICLR.cc/2026/Conference/Submission16058/Reviewer_hdTj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16058/Reviewer_hdTj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878811085, "cdate": 1761878811085, "tmdate": 1762926250792, "mdate": 1762926250792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an attack framework designed to bypass model auditing systems by exploiting knowledge of the audit dataset. Specifically, it assumes that if a model provider knows the audit dataset in advance, they can optimize the model to perform extremely well on this dataset while performing poorly on unseen data. The attack is termed as data forging, where the training data or model behavior is manipulated to pass the audit, thus undermining the integrity of the auditing process. \n\nThe authors formalize this setup as a game between the auditor and the model provider. Experiments are conducted on classification and text generation benchmarks, showing that the forged models achieve high audit accuracy but low generalization performance. The paper claims this demonstrates a critical weakness in current audit mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper focuses on model auditing, an increasingly important area for responsible AI deployment and accountability."}, "weaknesses": {"value": "1. The paper assumes that the attacker knows the audit dataset in advance, making the threat model fundamentally trivial. If the attacker has access to the audit data, they can directly fine-tune or overfit the model to achieve near-perfect audit performance, making the auditing process meaningless. This scenario does not reflect real-world auditing practices, where audit datasets are typically held confidentially or sampled independently from held-out data. \n\n2. The proposed commit–sample–prove protocol assumes that the audit dataset is sampled after model commitment, directly contradicting the threat model where the attacker already knows the audit data. This makes the proposed solution largely circular—it fixes the attack only by removing the condition that enables it. Taken together with point 1, the overall contribution feels conceptually weak: the paper defines an unrealistic vulnerability and then proposes a remedy that depends on changing the assumption itself. In practice, well-designed audit processes already ensure that audit datasets remain secret or are generated after commitment, so the identified issue is unlikely to arise.\n\n3. The paper assumes that the service provider (adversary) has full knowledge of the audit dataset and constructs an alternative training dataset ( $S'_{\\text{train}}$ ). However, the construction of such adversarial data is extremely limited—it simply minimizes the loss on the audit dataset and its perturbed variants, where the perturbations are created by adding small random noise to the features (algorithm 1). In essence, the adversary’s objective is merely to enforce certain properties on the given audit dataset, which is conceptually similar to standard data augmentation or fairness optimization techniques. As such, the paper lacks substantive algorithmic novelty and does not introduce any fundamentally new mechanism or optimization idea.\n\n4. Similar to point 3, the threat model used in the data-forging attack is comparable to, or even weaker than, that of classical data poisoning attacks, where the adversary injects a small amount of malicious or targeted data into the training process to influence model behavior on specific subsets of the distribution [1]. The proposed data-forging scenario simply assumes access to the audit dataset and optimizes directly on it, which is a less realistic and less technically challenging setting than existing poisoning-based attacks. Consequently, I do not view data forging as introducing a fundamentally new risk beyond what prior poisoning literature has already established.\n\n[1] Jagielski, M., Oprea, A., Biggio, B., Liu, C., & Chen, B. (2021). Subpopulation Data Poisoning Attacks. IEEE Symposium on Security and Privacy (S&P), 2021."}, "questions": {"value": "See the weaknesses. My key concerns are:\n\n1. Please justify the assumption that the model provider knows the audit dataset in advance. How realistic is this in any practical auditing framework? If the attacker already knows the audit data, why is a new attack framework needed? Isn’t this just overfitting to a known test set?\n\n2. Can you provide an alternative attack setting where the provider only has partial or approximate access to the audit dataset (e.g., distributional hints)? Evaluating partial-knowledge attacks would make the scenario more realistic and non-trivial.\n\n3. How does this work differ from poisoning attacks technically? Please contrast your threat model, attacker capabilities, and method with representative poisoning papers (e.g., Jagielski et al., 2021) and clarify what, if anything, is novel in your adversary’s optimization or data-construction method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fTOU5uc4JH", "forum": "LWGeKI7P1e", "replyto": "LWGeKI7P1e", "signatures": ["ICLR.cc/2026/Conference/Submission16058/Reviewer_NhAG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16058/Reviewer_NhAG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937579766, "cdate": 1761937579766, "tmdate": 1762926250467, "mdate": 1762926250467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}