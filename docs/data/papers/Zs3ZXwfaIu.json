{"id": "Zs3ZXwfaIu", "number": 6895, "cdate": 1758000680534, "mdate": 1759897885427, "content": {"title": "Evolutionary Alpha Factor Discovery with Large Language Models for Sparse Portfolio Optimization", "abstract": "Sparse portfolio optimization remains a fundamental yet difficult challenge in quantitative finance, as traditional approaches that rely on historical return estimates and static objectives often struggle to adapt to shifting market dynamics. To address this, we propose a new framework that leverages large language models (LLMs) to automate the discovery and iterative refinement of alpha factors tailored for sparse portfolio construction. By reformulating asset selection as a top-m ranking problem guided by factor signals, the framework integrates an evolutionary feedback loop to continuously enhance the factor pool based on performance. Extensive experiments across five Fama–French benchmark datasets and two real-world datasets (US and China) show that our approach consistently outperforms both statistical and optimization-based baselines, particularly in high-volatility and large-universe settings. Ablation studies further highlight the importance of prompt design, factor diversity, and the choice of LLM backend. These results suggest that language-model-guided factor generation offers a promising, interpretable, and adaptive solution for portfolio optimization under sparsity constraints.", "tldr": "", "keywords": ["Portfolio Optimization", "Alpha Factor Mining", "Quantitative Investment", "Large Language Models", "Evolutionary Searching"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3520a267509f65c3e9627614691ef336ad0a229.pdf", "supplementary_material": "/attachment/1e2503e58e6d3fd0f88722a33d036b4684f8e2eb.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose an LLM-driven framework that automates the discovery and refinement of alpha factors for sparse portfolio optimization, demonstrating superior and adaptive performance across multiple datasets through language-model-guided factor generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality (fair): This paper combines large language models (LLMs) and an evolutionary algorithm for the estimation of the alpha factor problem.\n2. Quality (fair): This paper designed an algorithm framework based on LLMs and an evolutionary algorithm, and developed a portfolio for the portfolio optimization problem.\n3. Clarity (fair): This paper, in the form of a flowchart, clearly presents the process of the proposed methodology.\n4. Significance (fair): The simulation results of this paper indicate that the proposed method significantly outperforms the traditional optimization methods in terms of the Cumulative Wealth and Sharpe Ratio dimensions."}, "weaknesses": {"value": "1. This article does not provide detailed instructions on how to set up the filters.\n2. Since the article combines LLM and evolutionary algorithms, I think it is necessary to compare the article that uses evolutionary algorithms (Zhang et al., 2020) with the article that uses LLM (Wang et al., 2023; Yuan et al., 2024)."}, "questions": {"value": "1. In terms of modeling, this paper sets the number of features for different assets to be the same. Is this assumption reasonable?\n2. If the LLM generates hallucinatory outputs in the correct format, what kind of impact would this have on the entire model? How should the article properly set the filter to reject the hallucination outputs in the correct format generated by the LLM?\n3. In the description from line 259 to line 261. How exactly should \"unstable\" or \"poor\" be defined? Should we set thresholds artificially for distinction or automatically classify based on certain criteria?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EGVaQIZ0lC", "forum": "Zs3ZXwfaIu", "replyto": "Zs3ZXwfaIu", "signatures": ["ICLR.cc/2026/Conference/Submission6895/Reviewer_ZPZ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6895/Reviewer_ZPZ6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798224937, "cdate": 1761798224937, "tmdate": 1762919138996, "mdate": 1762919138996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an evolutionary alpha discovery framework that uses a large language model (LLM) to automatically generate and refine stock trading factors. These candidate factors are continuously evaluated and evolved based on their historical predictive power, with strong ones retained and weak ones dropped. A sparse portfolio optimizer then selects a small set of effective signals to build risk-controlled portfolios. The authors claim this approach automates alpha research, adapts to changing markets, and delivers better predictive accuracy and portfolio performance than traditional machine learning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a novel framework that automatically generates and evolves alpha factors using an LLM and evolutionary selection, reducing human bias and manual feature engineering.\n\nThe framework alleviates the sparse decay issue where factor-based portfolios lose predictive power when only a few assets are selected. It does this by continuously pruning and regenerating factors based on their sparse performance metrics (e.g., RankIC, Recall@N under top-K selection), ensuring that active factors remain effective in sparse portfolio settings.\n\nThe use of a sparsity-constrained optimizer makes the final portfolios more realistic and tradable while maintaining good risk control.\n\nExperiments across multiple markets show higher predictive accuracy and portfolio performance than traditional ML baselines, and the discovered factors remain interpretable and economically meaningful."}, "weaknesses": {"value": "The main weakness is the lack of real novelty. The core idea—using an LLM to generate and evolve factor formulas—is conceptually simple and mainly automates what human quants already do. Although the paper adds a useful improvement by incorporating sparsity-aware feedback into the factor generation process, the overall approach still feels incremental. The combination of the LLM with the sparse portfolio optimizer is also rather loose—they are simply connected in sequence instead of being deeply integrated (for example, in an end-to-end manner)—which makes the framework feel more like two existing components placed together than a fundamentally new design.\n\nThere are also other issues that affect the paper’s rigor and robustness. The study does not clearly separate a period for choosing hyperparameters from another period for evaluating performance—all data are used in a rolling manner, so parameters like window length or drop thresholds are never tuned on a distinct validation set. This makes it difficult to confirm that the reported results truly reflect out-of-sample performance rather than benefiting from implicit look-ahead or overfitting. In addition, there is no evaluation of factor orthogonality or redundancy. Since the LLM generates new factors based on previously successful ones, it may repeatedly produce highly similar or correlated factors, leading to a pool of overlapping signals that offer little incremental value and could reduce portfolio diversity."}, "questions": {"value": "1. How is your framework fundamentally different from traditional automated factor search or symbolic regression approaches?\n\n2. The LLM and sparse optimizer seem connected sequentially—could you clarify whether the LLM generation process uses any feedback from portfolio outcomes, or are the two modules completely independent?\n\n3. How were key hyperparameters (such as the warm-up window length, drop threshold, and search interval) determined if there was no separate validation period?\n\n4. Would your results change if these parameters were tuned on a dedicated validation window rather than determined heuristically?\n\n5. Did you analyze the correlation or similarity among generated factors? How do you ensure that the LLM is not repeatedly producing redundant or highly correlated factors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lh2Tybix3Q", "forum": "Zs3ZXwfaIu", "replyto": "Zs3ZXwfaIu", "signatures": ["ICLR.cc/2026/Conference/Submission6895/Reviewer_KPTk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6895/Reviewer_KPTk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017072016, "cdate": 1762017072016, "tmdate": 1762919138608, "mdate": 1762919138608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for evolutionary alpha factor discovery using large language models (LLMs) to tackle sparse portfolio optimization under ℓ₀ constraints. Instead of relying on static or manually designed factors, the authors employ an LLM-driven evolutionary loop that continually generates, mutates, and refines interpretable alpha formulas based on back-testing performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Methodological clarity: explicit decomposition of macro vs. micro effects through learnable gating.\n\nGeneral relevance: unifies financial econometrics, generative modeling, and robust optimization—high interest to ICLR’s representation-learning audience.\n\nPublic reproducibility: code and data available on GitHub."}, "weaknesses": {"value": "I am not sure whether the authors have done an extensive literature search that shows LLM alpha. Have they seen Kirtac and Germano (2024) ? LLMs to produce alpha is not a new idea.\n\nTheoretical limits: lacks formal convergence or uncertainty-calibration analysis of diffusion-generated “views.”\n\nAblation depth: while individual module ablations exist, a joint ablation of CDG + MLG + BL contributions would clarify interdependencies.\n\nCompute transparency: GPU hours and sampling latency per rebalance step are not reported."}, "questions": {"value": "How sensitive is performance to the LLM backend (GPT-4 vs DeepSeek) and to prompt length limits?\n\nCould the authors quantify computational efficiency—e.g., number of LLM calls per trading day?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kCcxIw0Xpc", "forum": "Zs3ZXwfaIu", "replyto": "Zs3ZXwfaIu", "signatures": ["ICLR.cc/2026/Conference/Submission6895/Reviewer_Ypa3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6895/Reviewer_Ypa3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762109179971, "cdate": 1762109179971, "tmdate": 1762919138082, "mdate": 1762919138082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}