{"id": "keCnsHtION", "number": 12761, "cdate": 1758210116321, "mdate": 1763347734463, "content": {"title": "Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization", "abstract": "Reinforcement Learning from Human Feedback (RLHF) leverages a Kullback-Leibler (KL) divergence loss to stabilize training and prevent overfitting. However, in methods such as GRPO, its implementation may be guided by principles from numerical value estimation—a practice that overlooks the term's functional role as an optimization loss. To analyze this issue, we establish a unified framework that connects two seemingly distinct implementation styles: using the mathematical term $k_n$ as a detached coefficient for the policy's score function '$k_n$ in reward' or as a direct loss function through which gradients are propagated '$k_n$ as loss'. We show that the latter can always be analyzed via an equivalent gradient coefficient in the former, unifying the two perspectives. Through this framework, we prove that the conventional '$k_1$ in reward' (like PPO) is the principled loss for Reverse KL (RKL) regularization. We further establish a key finding: under on-policy conditions, the '$k_2$ as loss' formulation is, in fact, gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our work, identifies both as the theoretically sound implementations of the RKL objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like GRPO) is merely a first-order, biased approximation of the principled loss. Furthermore, we argue that common off-policy implementations of '$k_n$ as loss' methods are biased due to neglected importance sampling, and we propose a principled correction. Our findings provide a comprehensive, gradient-based rationale for choosing and correctly implementing KL regularization, paving the way for more robust and effective RLHF systems.", "tldr": "We proposed the principle surrogate loss of the reverse KL  in RLHF and analyzed that the k3 loss in GRPO is its first-order approximation.", "keywords": ["RLHF; KL Value Estimation; KL Loss; Gradient Analysis"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a9a38cb2a6d87b4f1641e561fc335c2080264eb.pdf", "supplementary_material": "/attachment/1b3d6b468d0ba85db5bdc9c8e3a7f08b4f0f73e6.zip"}, "replies": [{"content": {"summary": {"value": "This paper re-examines how KL regularization is used in RLHF and clarifies the gradient behavior of different KL implementations. Through theoretical analysis and controlled experiments, the authors demonstrate that only certain forms of KL regularization produce correct and stable gradients, providing clearer guidance for RLHF optimization design."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Unified perspective.**\nThe paper offers a clear and elegant framework that unifies several existing KL-regularized RLHF methods under a single analytical view. It successfully explains the relationships among different formulations used in prior works such as PPO, GRPO, and Reinforce++, providing conceptual clarity to an area that was previously fragmented.\n\n2. **Comprehensive empirical validation.**\nThe authors conduct thorough experiments that directly compare multiple KL variants, effectively verifying their theoretical claims. The results clearly support the analysis and demonstrate how different implementations influence training stability and performance."}, "weaknesses": {"value": "1. **Limited novelty.**\nSome recent RL for LLM[1] or RL-related analyses[2] have also discussed the similar formulations as the principled yet conventional choices for enforcing policy smoothness and stability. A more explicit discussion of these studies—and how the present work differs from or extends their findings—would strengthen the positioning and clarify the unique contribution of this paper.\n\n2. **Lack of practical cotribution.**\nWhile the paper provides strong theoretical analysis, the mentioned KL-related variants are somehow already widely adopted—explicitly or implicitly—in many existing RLHF implementations. The contribution feels more clarificatory than innovative, focusing on formalizing rather than advancing existing practices. I understand this may not be the main intention of the paper, but deriving useful practical suggestions from this unified perspective would make the contribution even stronger.\n\n\n\n[1] Zhang, Yifan, et al. \"On the design of kl-regularized policy gradient algorithms for llm reasoning.\" arXiv preprint arXiv:2505.17508 (2025).\n\n[2] Wang, Pengcheng, et al. \"Residual Policy Gradient: A Reward View of KL-regularized Objective.\" arXiv preprint arXiv:2503.11019 (2025)."}, "questions": {"value": "1. Could the authors clarify how their analysis differs from prior works as mentioned above, which also examine principled KL-regularized formulations?\n\n2. Since the analyzed and principled variants (k1 in reward) are already widely used, what are the practical advantages or new insights provided by this framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Cyleklcmto", "forum": "keCnsHtION", "replyto": "keCnsHtION", "signatures": ["ICLR.cc/2026/Conference/Submission12761/Reviewer_Pe5c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12761/Reviewer_Pe5c"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524776187, "cdate": 1761524776187, "tmdate": 1762923575602, "mdate": 1762923575602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper surveys three possible way of estimating KL divergence in (Schulman 2020) and establishes a framework to compare them in RLHF. The authors show that by assuming $a\\sim\\pi_\\theta$ as detached, the two different formulations of leveraging KL: $k_1$ in reward is equivalent to $k_2$ as loss. The authors discuss the properties of each $k_n$ and provide recommendations to avoid some combinations such as $k_1$ as loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors systematically surveys possible ways of leveraging KL, be in as a penalty in reward or as a loss objective. By combining this choice with three ways of estimating KL in (Schulman 2020), as well as off-policyness, a general (though somewhat cluttered) framework is established with many options available. The authors provide practical recommendations on top of analysis for important members of them, which is useful to the RLHF community."}, "weaknesses": {"value": "The presentation is not easily understandable. Section 4.1 is rather confusing due to its current organization: much detail about replacing raw reward $r(x, y)$ with $k_n$ is not mentioned in the main text but rather in Appendix A, and then $k_n$ suddenly appears in reward Eq. 5.  Later introduction of Eqs. 5 and 6 are without a summaryzing heading like reward maximization or kl regularization. Many new notations are combinations of $\\mathcal{J}$ and subscripts \"as loss/in reward\"and $k_n$ making this part rather cluttered. From a technical perspective, I am not sure the assumption in line 179 is reasonable: \"expectations are evaluated using samples from the detached snapshot $\\pi_\\theta$\", because doing so essentially removes $\\log\\pi_\\text{ref}$ in the objective, it is unclear to me whether this affects later substitution of $k_n$ to Eqs 5 and 6. In my opinion by treating $\\mathbb{E}_{\\pi\\theta}$ as detached they are no longer equivalent. The proofs in Appendix C is elementary if not trivial. \n\nI am unsure about whether the analysis in line 266-271 is reasonable, or at the very least I don't think line 271 is true: Eq. 10 holds only  because of the assumption $a\\sim\\pi_\\theta$ is detached, it should be noted that it already deviates from the original KL definition. REINFORCE on the other hand, subtracts a baseline that is deliberately independently of actions.\n\nIn line 332: does the higher order really matter? it is true that $1-\\delta$ is only a first order approximation of $-\\log \\delta$, but they share the same sign and hence gradient direction. As the authors put it, this can go wrong when $\\delta \\rightarrow 0$ or $\\infty$, suggesting $\\pi_\\text{ref}, \\pi_\\theta$ are completely different. Does this really happen when a reasonable KL penalty is in place? Do the authors have empirical results suggest that this happens?\n\nFor experiments, do the authors have $k_n$ in reward such as $k_1$ advocated in line 357? I notice that Figure 3 though claimed to effectively constrain the policy, its reward and accuracy are much lower than $k_1$, why?"}, "questions": {"value": "please refer to the weaknesses for questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w3NcHCMEPr", "forum": "keCnsHtION", "replyto": "keCnsHtION", "signatures": ["ICLR.cc/2026/Conference/Submission12761/Reviewer_GJJn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12761/Reviewer_GJJn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643360421, "cdate": 1761643360421, "tmdate": 1762923574829, "mdate": 1762923574829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper summarizes some pitfalls in KL divergence gradient estimation for RL and LLM reasoning. Concretely, existing implementations of KL-regularized policy gradient algorithms use KL estimates, some of which lead to vacuous KL gradient estimates. The paper proposes a framework to analyze existing implementations in a unified way."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Clear writing and mathematical details."}, "weaknesses": {"value": "Almost all results seem to be already known in the following papers:\n\n- On a few pitfalls in KL divergence gradient estimation for RL (https://arxiv.org/abs/2506.09477)\n- On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning (https://arxiv.org/abs/2505.17508)"}, "questions": {"value": "What are new results compared to the papers I mentioned in Weakness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IjgOw1hvc1", "forum": "keCnsHtION", "replyto": "keCnsHtION", "signatures": ["ICLR.cc/2026/Conference/Submission12761/Reviewer_gKww"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12761/Reviewer_gKww"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931020051, "cdate": 1761931020051, "tmdate": 1762923574189, "mdate": 1762923574189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors analyze KL-divergence regularization in RLHF. They show that the conventional “k1-in-reward” implementation is the principled formulation for Reverse KL (RKL) regularization. They further prove that, under on-policy conditions, “k2-as-loss” is gradient-equivalent to “k1-in-reward,” while the recently adopted “k3-as-loss” is only a first-order, biased surrogate of the principled loss.  Experiments on reasoning benchmarks corroborate the gradient-level analysis."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Detailed theoretical analysis**:\nThe paper provides clear, well-structured gradient-level results that unify different KL implementations and establish when they are equivalent or biased.\n\n2. **Experiments that corroborate the theory**:\nEmpirical evaluations are consistent with the theoretical predictions and demonstrate the expected optimization dynamics."}, "weaknesses": {"value": "1. **Disscussion of related work**: \nParts of the theoretical message overlap with prior analyses that embed KL/entropy directly into the reward or advantage [1, 2]. I recommend explicitly discussing these connections and clarifying how the present work extends or differs from those conclusions.\n\n2. **Application usefulness**: \nThe experiments validate the gradient analysis but do not show end-to-end improvements after correcting the formulations in existing algorithms.\n\n[1] Wang, Pengcheng, et al. \"Residual Policy Gradient: A Reward View of KL-regularized Objective.\" 1st Workshop on Safely Leveraging Vision-Language Foundation Models in Robotics: Challenges and Opportunities.\n\n[2] Li Y C, Zhang F, Qiu W, et al. Q-Adapter: Customizing Pre-trained LLMs to New Preferences with Forgetting Mitigation[C]//The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "As a follow-up of weaknesses 2, could the authors show improvements to existing algorithms after correcting the kn formulations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3LpTWlNrGQ", "forum": "keCnsHtION", "replyto": "keCnsHtION", "signatures": ["ICLR.cc/2026/Conference/Submission12761/Reviewer_ct1C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12761/Reviewer_ct1C"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12761/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981632384, "cdate": 1761981632384, "tmdate": 1762923573732, "mdate": 1762923573732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}