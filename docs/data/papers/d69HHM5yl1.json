{"id": "d69HHM5yl1", "number": 15129, "cdate": 1758248052340, "mdate": 1759897326347, "content": {"title": "NBSP: A Neuron-Level Framework for Balancing Stability and Plasticity in Deep Reinforcement Learning", "abstract": "In contrast to the human ability to continuously acquire knowledge, agents struggle with the stability-plasticity dilemma in deep reinforcement learning (DRL), which refers to the trade-off between retaining existing skills (stability) and learning new knowledge (plasticity). Current methods focus on balancing these two aspects at the network level, lacking sufficient differentiation and fine-grained control of individual neurons. To overcome this limitation, we propose Neuron-level Balance between Stability and Plasticity (NBSP) method, by taking inspiration from the observation that specific neurons are strongly relevant to task-relevant skills. Specifically, NBSP first (1) defines and identifies RL skill neurons that are crucial for knowledge retention through a goal-oriented method, and then (2) introduces a framework by employing adaptive gradient masking and experience replay techniques targeting these neurons to preserve the encoded existing skills while enabling adaptation to new tasks. Numerous experimental results on the Meta-World, Atari, and DMC benchmarks demonstrate that NBSP significantly outperforms existing approaches in balancing stability and plasticity.", "tldr": "We propose Neuron-level Balance between Stability and Plasticity (NBSP), a novel DRL framework that operates at the level of individual neurons to balance atability and plasticity.", "keywords": ["deep reinforcement learning", "stability-plasticity dilemma", "skill neuron"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c095853429ecf0c17014be7c9959923f2c38be75.pdf", "supplementary_material": "/attachment/7f866db58978807ffcf239f939e61b47e54b9cfe.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a method for helping with the stability-plasticity tradeoff on a neuron-level basis. It proposes a metric to identify the neurons that are crucial for performance, and uses gradient masking and replay on top to preserve the already learned patterns. \nThe paper performs experiments on Metaworld, Atari, and DMC benchmarks, and compares their method with multiple methods proposed in continual learning, across multiple metrics, and shows that their approach (NBSP) can outperform without the addition of parameters or using complex networks."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written, and there is a natural flow for related work and methodology.\n\n2. The paper is well-motivated, and the proposed methodology does in fact avoid complex NN designs and the use of large networks, which are crucial benefits in deep RL.\n\n3. The paper leverages the use of multiple evaluation metrics for plasticity, stability, and overall performance for assessing their method and comparing it to others. Ablation studies explore the components of the proposed method thoroughly, and also the use of the algorithm in other agents than SAC is included."}, "weaknesses": {"value": "1. The score does not capture what the paper intends to.  $1 - R_{\\text{over}}(\\mathcal{N})$ does not only include neurons whose activity hold a negative correlation with performance, as stated and intended in line 233,  but also other cases: \n$$\n1 - R_{\\text{over}}(N) \n= 1 - \\frac{\\sum_{t=1}^{T} 1\\Big[ 1[a(N, t) > \\bar{a}(N)] = 1[q(t) > \\bar{q}] \\Big]}{T}\n$$\n\n$$\n= \\frac{T - \\sum_{t=1}^{T} 1\\Big[ 1[a(N, t) > \\bar{a}(N)] = 1[q(t) > \\bar{q}] \\Big]}{T}\n$$\n\n$$\n= \\frac{\\sum_{t=1}^{T} \\Big( 1 - 1\\big[ 1[a(N, t) > \\bar{a}(N)] = 1[q(t) > \\bar{q}] \\big] \\Big)}{T}\n$$\n\nwhere the numerator is:\n$$\n\\begin{cases}\n0, & \\text{if } a(N,t) > \\bar{a}(N) \\text{ and } q(t) > \\bar{q},\\\\\n1, & \\text{otherwise.}\n\\end{cases}\n$$\nand \"otherwise\" includes all the other cases, i.e., (a < ā, q > q̄), (a > ā, q < q̄), (a < ā, q < q̄)\nwhereas, as stated in the paper, it was intended to account for only  (a < ā, q > q̄), meaning \" when the activation of a neuron falls below its average activation, the agent performs well.\"\n\n\n2. Three seeds are not enough to make strong claims about the performance of the method compared to the others. Can you take a few of the methods in one of the settings and run more seeds for them to strengthen the claims?\n\n3. Although the paper does a great job of exploring the effect of the NSBP hyperparameters on the model performance, other methods’ hyperparameters are underexplored. How are the other baselines' hyperparameters tuned? Are they using their default hyperparameters? If so, it might not be fair to use those hypers as they were found for the settings they were experimented on. In particular, is COTASP tuned, since it consistently underperforms in all the metaworld benchmarks? What is the reason for its performance collapse?\n\n4. The paper emphasizes the distinction between CRL and DRL. However, the experiments are mostly done in settings designed for CRL ( non-stationary or task change in the environment). Can you elaborate on the reason for this emphasis and its relation to your method?"}, "questions": {"value": "1. The average goal metric works for settings where there is a specific goal or an episode. How does your method and this metric extend to continuing settings where there are no episodes, or a clear/binary notion of success?\n\n2. How do the experiment results change after fixing the score function?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wGoeIIImNB", "forum": "d69HHM5yl1", "replyto": "d69HHM5yl1", "signatures": ["ICLR.cc/2026/Conference/Submission15129/Reviewer_t1NL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15129/Reviewer_t1NL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700286489, "cdate": 1761700286489, "tmdate": 1762925447058, "mdate": 1762925447058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a neural-level method to overcome catastrophic forgetting issues in deep reinforcement learning (DRL). They propose to both mask the gradient of task-specific neurons and train the neural network with multi-task experience replay. Experiments show improved results compared to baselines on a DRL benchmark.\n\nThe paper is well-written, the method is novel and the preliminary results are promising. My main concern is about the experimental protocol. First, the authors compare their approach to baselines of the field only in the Meta-World benchmarks. Second, there is no clear reasons for not testing the method on supervised benchmarks that are more widely used in continual learning, such as adapted versions of cifar100 and imagenet. I suspect that the \"Experience replay only\" version will achieve much better performance in supervised learning. In addition, the authors do not provide a clear analysis of why Experience replay and neuron masking are complementary; the bad results of \"Experience replay only\" require explanations. Third, it is unclear how the method scales when considering more than four tasks. \n\nThe authors should address these issues before resubmission.\n\nMinor comments:\n- \"reasonably broad range of values (from 0.15 to 0.3)\", it is unclear why the authors mention this specific range, given the shape of the curve in Figure 4.\n- It is unclear why increasing the number of masked neurons \"compromises their learning capacity and causes the true RL skill neurons to adjust their activations to accommodate new tasks, ultimately reducing stability\"."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, the approach is novel and relevant"}, "weaknesses": {"value": "Experiments are insufficient"}, "questions": {"value": "Please, see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xhg0D7XTjb", "forum": "d69HHM5yl1", "replyto": "d69HHM5yl1", "signatures": ["ICLR.cc/2026/Conference/Submission15129/Reviewer_4tP5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15129/Reviewer_4tP5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838996972, "cdate": 1761838996972, "tmdate": 1762925446639, "mdate": 1762925446639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NBSP (Neuron-level Balance between Stability and Plasticity), a framework that tackles the stability–plasticity dilemma in deep reinforcement learning (DRL). Unlike prior methods that regulate stability and plasticity at the network or parameter level, NBSP operates at the neuron level. It identifies RL skill neurons, neurons whose activations correlate strongly with task goals (e.g., success rate or return), and protects them during new task learning. NBSP combines Goal-oriented neuron identification to detect neurons critical for knowledge retention; and Adaptive gradient masking and experience replay, selectively applied to those neurons.\n\nExperiments on Meta-World, Atari, and DMC benchmarks show that NBSP improves the trade-off between stability and plasticity, achieving higher Average Success Rate (ASR), lower Forgetting Measure (FM), and higher Forward Transfer (FWT) than nine competitive baselines (for example EWC, NPC, CoTASP, NE, UPGD). Ablation studies confirm the complementary roles of gradient masking and experience replay, and demonstrate that the goal-oriented neuron identification strategy is crucial to performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The neuron-level approach to balancing stability and plasticity is well-motivated. Defining “skill neurons” through goal-oriented correlation is creative, interpretable, and biologically inspired.\n- The method is conceptually simple yet effective, adding minimal complexity to standard SAC/PPO agents.\n- Strong empirical evidence supports the claims. Meta-World, Atari, and DMC results consistently outperform baselines.\n- Ablations and hyperparameter analyses (e.g., actor vs. critic masking) are thoughtfully designed.\n- The paper is well-structured. Motivation, methodology, and evaluation flow logically, with clear notation and metric definitions (ASR, FM, FWT).\n- Addresses a fundamental and persistent challenge in continual deep RL.\n- NBSP’s interpretability and light implementation could make it broadly useful for future continual-learning research, including extensions to supervised or self-supervised setups."}, "weaknesses": {"value": "- Experiments use only 3 seeds; more repetitions or confidence intervals would increase result reliability.\n- The method’s dependence on the top-m% neuron threshold (m = 0.2), mask coefficient (α = 0.2), and averaging window size is not analyzed. A sensitivity study would strengthen robustness claims.\n- The paper lacks variance or gradient-norm analyses explaining why masking high-score neurons stabilizes training.\n- Atari and DMC results are summarized in tables but lack detailed learning curves or per-task analysis.\n- The algorithmic description omits specifics such as how masks interact with target networks and the replay schedule."}, "questions": {"value": "- How sensitive is NBSP to the hyperparameters?\n- Have you measured gradient-norm variance or activation statistics to confirm reduced interference?\n- Are both actor and critic masked at every step, or alternately? How does masking interact with target networks?\n- What is the computational and memory overhead (FLOPs, wall-time, replay buffer size) compared to vanilla SAC?\n- Have you examined the behavior of anti-correlated (“negative-score”) neurons?\n- Would NBSP generalize to supervised continual learning or offline RL where reward signals differ?\n- The paper excludes the final layer from neuron scoring. Why? Have you tested whether masking output neurons (for example, in the critic’s value head) hurts performance or stability?\n-What failure modes did you observe, for example, situations where neuron correlation misidentifies unimportant units, or where masking accumulates and stalls learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There are no ethics concerns to report."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m5DFkDf548", "forum": "d69HHM5yl1", "replyto": "d69HHM5yl1", "signatures": ["ICLR.cc/2026/Conference/Submission15129/Reviewer_QyWf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15129/Reviewer_QyWf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878444976, "cdate": 1761878444976, "tmdate": 1762925446214, "mdate": 1762925446214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose the NBSP framework to solve the stability-plasticity dilemma in DRL (Deep Reinforcement Learning). The method uses a goal-oriented strategy to identify \"RL skill neurons\" and combines adaptive gradient masking with experience replay to balance learning new skills while retaining old ones. Experiments on multiple benchmarks demonstrate that NBSP outperforms existing methods in multiple metrics such as ASR."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper introduces a neuron-level approach to address the stability-plasticity dilemma in DRL, proposing the concept of \"RL skill neurons.\" While similar concepts have been discussed in other fields, their introduction to DRL is (to my knowledge) a novel contribution.\n2. The experimental design is sound and thorough, featuring multiple benchmarks, diverse metrics, and a wide variety of baseline comparisons. Furthermore, the inclusion of extensive ablation studies to validate component effectiveness and explore the mechanisms of RL skill neurons significantly strengthens the paper's conclusions.\n3. The paper is well-structured, clearly written, and utilizes well-designed figures and tables to effectively communicate the research outcomes."}, "weaknesses": {"value": "1. The experimental setup is relatively simple:\n    - Regarding the length of continual learning sequences, the paper tests on short task chains, focusing on lengths of 2 or 4 (at most), and lacks experiments on longer chains (e.g., over 10 Atari games).\n    - Regarding task relatedness, the tasks tested are highly correlated, such as window-close/window-open in Meta-World and Cartpole-swingup/Cartpole-balance in DMC.\n    - Finally, the tasks are relatively simple, raising questions about the method's generalizability to more complex environments.\n2. The paper introduces a considerable number of hyperparameters:\n    - Several hyperparameters are introduced for the RL skill neurons, and the two core techniques (gradient masking and experience replay) also bring their own.\n    - **It is worth noting that the authors conducted sensitivity analyses, and the current results suggest the method is not overly hyperparameter-sensitive.** \n    - However, it currently lacks a systematic selection criterion and relies on manual tuning, which raises doubts about its broader applicability to other tasks and algorithms."}, "questions": {"value": "1. Could the authors provide more detailed implementation details for the baselines? For example, the original NE paper used the TD3 algorithm, but in this paper, it was implemented with SAC. Could the authors clarify how this algorithmic transition was handled to ensure a fair comparison?\n2. While gradient masking is shown to significantly improve success rates, it resembles reducing the learning rate on certain neurons. Could the authors include experiments that reduce the learning rate across the entire network to demonstrate the necessity of selectively lowering it only on RL skill neurons?\n3. The authors emphasize balancing stability and plasticity in DRL. However, both gradient masking and experience replay tend to enhance stability. Does this suggest that, for vanilla DRL algorithms, improving stability is more crucial for metrics like ASR? Alternatively, could NBSP be combined with other techniques that enhance plasticity, such as plasticity injection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UAlcwTx6HN", "forum": "d69HHM5yl1", "replyto": "d69HHM5yl1", "signatures": ["ICLR.cc/2026/Conference/Submission15129/Reviewer_twf4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15129/Reviewer_twf4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972371492, "cdate": 1761972371492, "tmdate": 1762925445846, "mdate": 1762925445846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}