{"id": "cQstHCieUG", "number": 4997, "cdate": 1757830763352, "mdate": 1759898001174, "content": {"title": "NoiseAR: AutoRegressing Initial Noise Prior for Diffusion Models", "abstract": "Diffusion models have emerged as powerful generative frameworks, creating data samples by progressively denoising an initial random state. Traditionally, this initial state is sampled from a simple, fixed distribution like isotropic Gaussian, inherently lacking structure and a direct mechanism for external control. While recent efforts have explored ways to introduce controllability into the diffusion process, particularly at the initialization stage, they often rely on deterministic or heuristic approaches. These methods can be suboptimal, lack expressiveness, and are difficult to scale or integrate into more sophisticated optimization frameworks.  In this paper, we introduce NoiseAR, a novel method for AutoRegressive Initial Noise Prior for Diffusion Models. Instead of a static, unstructured source, NoiseAR learns to generate a dynamic and controllable prior distribution for the initial noise. We formulate the generation of the initial noise prior's parameters as an autoregressive probabilistic modeling task over spatial patches or tokens. This approach enables NoiseAR to capture complex spatial dependencies and introduce learned structure into the initial state. Crucially, NoiseAR is designed to be conditional, allowing text prompts to directly influence the learned prior, thereby achieving fine-grained control over the diffusion initialization.  Our experiments demonstrate that NoiseAR can generate initial noise priors that lead to improved sample quality and enhanced consistency with conditional inputs, offering a powerful, learned alternative to traditional random initialization. A key advantage of NoiseAR is its probabilistic formulation, which naturally supports seamless integration into probabilistic frameworks like Markov Decision Processes (MDPs) and Reinforcement Learning (RL). This integration opens promising avenues for further optimizing and scaling controllable generation for downstream tasks. Furthermore, NoiseAR acts as a lightweight, plug-and-play module, requiring minimal additional computational overhead during inference, making it easy to integrate into existing diffusion pipelines.", "tldr": "This paper introduces NoiseAR, a method that learns a controllable, structured initial noise for diffusion models using an autoregressive framework, leading to improved sample quality and better consistency with text prompts.", "keywords": ["Generative Model", "Diffusion Model", "Text to Image Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9350b4fceb098b21af6bf93f60c15f10ebcecaef.pdf", "supplementary_material": "/attachment/9efc37bab80e65d63b0a31429be78c2995e26420.zip"}, "replies": [{"content": {"summary": {"value": "NoiseAR reframes the role of initial noise in diffusion models: instead of sampling a static, isotropic Gaussian, it learns a prompt-conditioned, patch-wise autoregressive prior $P( \\mathbf{z}_T | \\mathbf{c})$ with a light-weight Transformer decoder that emits per-patch Gaussian parameters. This structured, probabilistic seed boosts text-image alignment and sample quality across SDXL, DreamShaper-xl-v2-turbo and Hunyuan-DiT while adding <1 % inference cost, and it can be further refined with RL-style Direct Preference Optimization. By making the very first latent variable optimizable and controllable, NoiseAR provides a plug-and-play pathway toward stronger, human-aligned generation for text-to-image and, potentially, video, audio and 3-D diffusion tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents NoiseAR, a novel approach to learning a controllable, autoregressive prior for the initial noise in diffusion models. \n\n---\n\n\n\n- New Problem Framing: The paper reframes the role of initial noise in diffusion models—from an uncontrollable, fixed Gaussian sample to a learnable, structured latent variable. This is a conceptual shift that has not been fully explored in prior work.\n- Creative Use of Autoregressive Modeling: While autoregressive models have been used in image and text generation, applying them to model the initial noise distribution of diffusion models is novel. The idea of predicting per-patch Gaussian parameters conditioned on text is both technically creative and practically powerful.\n- Integration with RL Frameworks: The authors go beyond just modeling and show how their probabilistic formulation naturally fits into Markov Decision Processes (MDPs) and Reinforcement Learning (RL) setups, enabling future optimization via methods like Direct Preference Optimization (DPO).\n\n- Strong Empirical Results: The method is evaluated across multiple diffusion models (SDXL, DreamShaper-xl-v2-turbo, Hunyuan-DiT) and multiple benchmarks (Pick-a-Pic, DrawBench, GenEval), showing consistent improvements in human preference metrics (PickScore, ImageReward, HPSv2, etc.).\n- Ablation Studies: The paper includes thoughtful ablations on patch size, decoder depth, and prediction head layers, showing robustness and guiding practical implementation.\n- Efficiency: The method adds <1% computational overhead, making it a plug-and-play module that is easy to integrate into existing pipelines.\n\n---\n\nNoiseAR is a creative, well-executed, and impactful contribution to the diffusion modeling literature. It challenges a long-standing assumption (that initial noise must be random) and provides a principled, efficient, and extensible alternative. The work is original in framing, high in quality, clear in exposition, and significant in both immediate and long-term implications."}, "weaknesses": {"value": "Despite its creative approach and empirical robustness, NoiseAR still suffers from several practical shortcomings that constrain its immediate impact and broader future adoption.  \n\nThe paper presents NoiseAR as the first learned, probabilistic prior over initial noise; however, deterministic or heuristic strategies for selecting noise have already been investigated:\n\n- *Golden Noise for Diffusion Models: A Learning Framework*  \n- *Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models*  \n- *INITNO: Boosting Text-to-Image Diffusion Models via Initial Noise Optimization*  \n\nMore importantly, the manuscript lacks a discussion of related work."}, "questions": {"value": "The authors assume that a diffusion model fails to map Gaussian noise to a valid image because, for a *learned* diffusion model, the true initial noise distribution deviates from the Gaussian prior. Thus, they advocate *learning* the noise distribution instead. However, this raises a critical question: **how can the noise be learned when its ground-truth distribution is inherently unknown?**  \n\n1. **Training Data Construction**: Without access to the “correct” initial noise for real images, how is the training data for the noise model generated? If synthetic pairs (noise→image) are used, how are they guaranteed to reflect the *actual* noise distribution that the diffusion model implicitly inverts?  \n\n2. **Verification of Learned Noise**: Even if the proposed method produces high-quality images, this alone is insufficient evidence that the *noise distribution* has been accurately learned. After all, improvements could stem from unrelated inductive biases (e.g., smoother latent interpolation). What explicit validation confirms that the learned noise matches the “true” unknown distribution? For instance, are there diagnostics to rule out the possibility that the model merely learned a *different* heuristic (e.g., seed selection) unrelated to the underlying noise prior?  \n\n3. **Identifiability**: Given that the diffusion process is many-to-one (multiple noise inputs can map to similar images), how is the problem of learning the noise distribution *identifiable*? Without ground-truth noise samples, what prevents the model from learning arbitrary distributions that happen to yield plausible images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0SvPvpImlX", "forum": "cQstHCieUG", "replyto": "cQstHCieUG", "signatures": ["ICLR.cc/2026/Conference/Submission4997/Reviewer_HRDB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4997/Reviewer_HRDB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616511974, "cdate": 1761616511974, "tmdate": 1762917813889, "mdate": 1762917813889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission proposed a new method to generate the initial noises that are conventionally sampled from a gaussian distribution for diffusion models.\nProposed NoiseAR is a Transformer-decoder-based autoregressive generator for 2D gaussian noises. NoiseAR is trained to maximize NNL on a collected noise-caption (here captions are of generated images from the corresponding noise) pair dataset, in other words NoiseAR is an autoregressive generative model of noises. Experiments using Pick-a-Pic, GenEval, and DrawBench text-to-image benchmarks, using NoiseAR for initialization is shown to be useful for multiple diffusion-based models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The idea of generating the initial noise is interesting and seems promising direction.\n- The proposed method is plug-and-play, able to improved diffusion models.\n- The experiments considers multiple aspects of the generated images including aesthetics (AES), human preference (HPSv2) and semantics (CLIP score). NoiseAR is shown to improve multiple metrics stably."}, "weaknesses": {"value": "- Practical advantage over initial noise optimization (INO) is not well discussed. Is NoiseAR better than INO or different in applicable tasks?\n- Other than generating the initial noise, a simpler dictionary-based noise collection method [a] was proposed. How does the proposed method compare with it?\n  [a]The Lottery Ticket Hypothesis in Denoising: Towards Semantic-Driven Initialization\n- Difference from the two-stage generation method: Having additional parameters in the NoiseAR module should be beneficial for generation by . From this viewpoint, NoiseAR may be a form of two-stage generation methods for example, the generator-refiner pattern seen in SDXL. The difference is that conventional two-stage generation is noise-to-image and image-to-image but NoiseAR is noise-to-noise and noise-to-image.\n- The properties of the generated \"noise\" are not deeply discussed. Is it really noise, not one-step denoised latents?  If it is still a noise, how it is statistically different from the initial Gaussian?\n- Construction of the training noise datasets {(Z_t, c)} is not clear: at first look, I thought that the noises that cause successful generation were collected using some filtering, but actually, no filtering or picking processes are involved?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lNCp3nSGYz", "forum": "cQstHCieUG", "replyto": "cQstHCieUG", "signatures": ["ICLR.cc/2026/Conference/Submission4997/Reviewer_3ktf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4997/Reviewer_3ktf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964572165, "cdate": 1761964572165, "tmdate": 1762917813565, "mdate": 1762917813565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the lack of structure and external control in the traditional isotropic Gaussian initial noise of diffusion models by proposing NoiseAR, an autoregressive initial noise prior method. It learns a dynamic, controllable prior distribution for initial noise via patch-level autoregressive probabilistic modeling, using a Transformer decoder to split noise tensors into non-overlapping patches, predict Gaussian parameters with text prompts, and generate structured initial noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- First to apply autoregressive modeling to initial noise prior learning, filling the gap in structured control of diffusion starting state, with text prompts directly guiding noise generation.\n- Validated across multiple datasets and models, with ablation studies clarifying parameter impacts and DPO proving iterability, ensuring reliable conclusions."}, "weaknesses": {"value": "The author proposes a universal optimized solution for the initial noise distribution, and I have several questions regarding this:\n- How does the size of the training set used by the author to train this model compare to that of the base model? If it is larger, wouldn’t the training cost be too high, and why not simply fine-tune the base model instead? If it is smaller, for samples that the base model has encountered but this model has not, could this initial noise optimization approach degrade the performance of the original base model?\n- Regarding the lack of consistency between generated content and the prompt, can the inclusion of this module directly enhance the base model’s generative capability—meaning it enables the model to achieve what it previously could not—or does it merely increase the probability of generating content that aligns with the prompt?\n- This method ultimately produces a distribution, and the initial noise is sampled from this distribution. Can every initial noise sampled from this optimized distribution generate results consistent with the prompt? If not, what is the success rate?\n- Many base models initially use CLIP as the text encoder. However, CLIP is inherently insensitive to numerical concepts, such as \"the number of people,\" which leads to a decline in generative capability. Using the CLIP score as a fine-grained alignment metric in this context seems biased.\n- [1] has a very similar idea. Please give some comparison and discussion \n\n[1] FIND: Fine-tuning Initial Noise Distribution with Policy Optimization for Diffusion Models. ACMMM 2024."}, "questions": {"value": "- Can the inclusion of this module directly enhance the base model’s generative capability？\n- Could this initial noise optimization approach degrade the performance of the original base model?\n- Can every initial noise sampled from this optimized distribution generate results consistent with the prompt? If not, what is the success rate?\n- Please provide a comparison with [1]"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U83YOwESoK", "forum": "cQstHCieUG", "replyto": "cQstHCieUG", "signatures": ["ICLR.cc/2026/Conference/Submission4997/Reviewer_3kvX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4997/Reviewer_3kvX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989717601, "cdate": 1761989717601, "tmdate": 1762917813347, "mdate": 1762917813347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to update the prior distribution of diffusion models as a mechanism for controlling generation. The central idea is to learn a structured prior for the initial noise, $ z_T $, enabling targeted sampling rather than controlling the entire denoising process. This is implemented using a patch-based, autoregressive model that is fine-tuned using a formulation based on RL. However, despite specific design choices, the paper's foundational idea is not new. The manuscript critically omits any mention of, or comparison with, existing methods that implement this exact \"source-space guidance\" concept."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Given the paper's overclaimed contributions and lack of comparison to the state-of-the-art, the strengths are limited:\n\n1. The specific architectural choices, such as the patch-based autoregressive model, are technically sound.  \n2. The method demonstrates clear empirical improvements over the limited set of baselines chosen for comparison."}, "weaknesses": {"value": "The submission's primary weakness is a critical failure of scholarship. It completely omits a mature and active line of research on the exact problem this paper claims to be exploring.The paper's entire narrative is built on the claim that influencing the \"foundational starting point\" ($ z_T $) \"remains relatively underexplored\". This is factually incorrect. A significant body of work is dedicated to this exact problem, invalidating the submission's core claims to novelty.The authors fail to cite or compare against numerous, highly relevant papers. The following is a non-exhaustive list of omitted prior and concurrent work:\n\n1. Venkatraman, S., Hasan, M., Kim, M., Scimeca, L., Sendera, M., Bengio, Y., ... & Malkin, N. Outsourced Diffusion Sampling: Efficient Posterior Inference in Latent Spaces of Generative Models. In Forty-second International Conference on Machine Learning (ICML 2025).\n2. Tang, Z., Peng, J., Tang, J., Hong, M., Wang, F., & Chang, T. H. Inference-Time Alignment of Diffusion Models with Direct Noise Optimization. In Forty-second International Conference on Machine Learning (ICML 2025).\n3. Eyring, L., Karthik, S., Dosovitskiy, A., Ruiz, N., & Akata, Z. (2025). Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models. arXiv preprint arXiv:2508.09968.\n4. Kalaivanan, A., Zhao, Z., Sjölund, J., & Lindsten, F. (2025). ESS-Flow: Training-free guidance of flow-based models as inference in source space. arXiv preprint arXiv:2510.05849.\n5. Wang, Z., Harting, A., Barreau, M., Zavlanos, M. M., & Johansson, K. H. (2025). Source-Guided Flow Matching. arXiv preprint arXiv:2508.14807.\n6. Smith, H. D., Diamant, N. L., & Trippe, B. L. (2025). Calibrating Generative Models. arXiv preprint arXiv:2510.10020.\n7. Om, K., Sim, K., Yun, T., Kang, H., & Park, J. (2025). Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization. arXiv preprint arXiv:2507.00480. (used for Bayesian optimization)\n\n\nThis omission leads to several critical flaws:\n1. The paper's claim to be the first RL-based approach, or to be \"uniquely compatible\" with RL , is false. For example, Venkatraman et al. (\"Outsourced Diffusion Models\") explicitly uses an RL objective (Trajectory Balance, related to continuous GFlowNets) to train its noise-space sampler, which also functions as a policy in an MDP.\n2. The paper fails to benchmark against direct architectural competitors. Eyring et al.  (\"Noise Hypernetworks\") also proposes a structure-based approach to learn an amortized, conditional prior in the latent space. A comparison between the proposed autoregressive model and this hypernetwork approach is essential to validate the authors' design choices, but it is absent.\n3. As a consequence of these omissions, the experimental validation is insufficient. By comparing only against weak baselines (e.g., the standard Gaussian prior ), the paper fails to demonstrate how its method performs against the actual state-of-the-art in this field. The main claims of the paper are, therefore, unsubstantiated."}, "questions": {"value": "In addition to the Weaknesses, I would like to ask the authors to address the following question:\n\n1. Please clarify the novelty of your work in light of the existing papers, especially \"Outsourced diffusion sampling\" and \"Noise Hypernetworks\" papers, both of which also propose learning a reward-driven, amortized prior in the noise space. Specifically, how is your RL-based formulation novel when  already uses an RL-based MDP (GFlowNet)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XU4O3HcIJ8", "forum": "cQstHCieUG", "replyto": "cQstHCieUG", "signatures": ["ICLR.cc/2026/Conference/Submission4997/Reviewer_ZfFt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4997/Reviewer_ZfFt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762303042461, "cdate": 1762303042461, "tmdate": 1762917813036, "mdate": 1762917813036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}