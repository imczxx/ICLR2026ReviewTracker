{"id": "cmzeMZNsmm", "number": 20842, "cdate": 1758310769231, "mdate": 1759896955991, "content": {"title": "Revisiting Prompt Optimization with Large Reasoning Models---A Case Study on Event Extraction", "abstract": "Large Reasoning Models (LRMs) such as DeepSeek-R1 and OpenAI o1 have demonstrated remarkable capabilities in various reasoning tasks. Their strong capability to generate and reason over intermediate thoughts has also led to arguments that they may no longer require extensive prompt engineering or optimization to interpret human instructions and produce accurate outputs. In this work, we aim to systematically study this open question, using the structured task of event extraction for a case study. We experimented with two LRMs (DeepSeek-R1 and o1) and two general-purpose Large Language Models (LLMs) (GPT-4o and GPT-4.5), when they were used as task models or prompt optimizers. Our results show that on tasks as complicated as event extraction, LRMs as task models still benefit from prompt optimization, and that using LRMs as prompt optimizers yields more effective prompts. Beyond event extraction, we replicate our findings on two very different tasks: Geometric Shapes and NCBI Disease NER. Prompt optimization improves all models, with LRMs benefiting most. Finally, we provide an error analysis of common errors made by LRMs and highlight the stability and consistency of LRMs in refining task instructions and event guidelines.", "tldr": "Prompt optimization benefits all models, but reasoning models gain more. With optimized prompts, they outperform their non-optimized counterparts and LLMs on Event Extraction", "keywords": ["Prompt Optimization", "Large Language Models", "Large Reasoning Models", "Event Extraction", "Natural Language Processing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5fbb1060855a804a1bca64a787b06231467cbf2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "* **Research Problem**:\n  Investigates whether Large Reasoning Models (LRMs), despite their strong inherent reasoning capabilities, still benefit from prompt optimization, and whether they can serve as effective prompt optimizersâ€”using event extraction as a structured testbed.\n\n* **Experimental Method**:\n  Evaluates two LRMs (DeepSeek-R1, OpenAI o1) and two LLMs (GPT-4.5, GPT-4o) in both task-model and prompt-optimizer roles within a Monte Carlo Tree Search (MCTS) framework; experiments are conducted on event extraction (ACE05), symbolic reasoning, and biomedical NER tasks.\n\n* **Main Findings**:\n\n  1. LRMs significantly benefit from prompt optimization, with greater performance gains than LLMs.\n  2. LRMs (especially DeepSeek-R1) outperform LLMs as prompt optimizers, producing higher-quality, more stable, and more concise prompts.\n  3. These benefits generalize beyond event extraction to other domains, including symbolic and biomedical tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper revisits prompt optimization in the context of Large Reasoning Models (LRMs) which is a meanful topic. It challenges the prevailing assumption that strong reasoning models no longer require prompt optimization, offering a novel empirical perspective.\n\n\n2. The main insight of the experiment are constructive: the performance of LRM can be further improved by optimizing prompts."}, "weaknesses": {"value": "1. Although the authors have conducted meaningful empirical research, this paper does not make any original theoretical or experimental contributions.\n\n2. The overall experimental focus of this paper remains primarily on event extraction. Evaluation on a wider range of NLP tasks would enhance the significance of the paper's conclusions.\n\n3. The conclusions of this paper are highly dependent on the test model and test task. LLMs are highly dependent on training data, and LRMs are primarily optimized for complex reasoning tasks. Do similar conclusions hold for the recent SOTA models o3 and o4? For datasets that are already saturated, optimizing prompt words will obviously not bring any improvement. For extremely difficult problems, such FrontierMath or ARC-AGI, can optimizing prompt also bring improvement?"}, "questions": {"value": "1. Can the authors generalize the evaluation task to a wider range of NLP tasks, not just event extraction?\n2. Do similar conclusions hold for the recent SOTA LRMs?\n3.  Do similar conclusions hold for extremely difficult problems, such FrontierMath or ARC-AGI?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HlI7CF1zsC", "forum": "cmzeMZNsmm", "replyto": "cmzeMZNsmm", "signatures": ["ICLR.cc/2026/Conference/Submission20842/Reviewer_y32Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20842/Reviewer_y32Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760598851251, "cdate": 1760598851251, "tmdate": 1762936332270, "mdate": 1762936332270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a timely and systematic empirical study investigating whether the advanced reasoning capabilities of Large Reasoning Models (LRMs) like DeepSeek-R1 and o1 diminish the need for prompt optimization, using the complex task of event extraction as a primary case study; the authors claim that LRMs still benefit significantly from optimization, that they serve as more effective prompt optimizers than general-purpose LLMs, and that these findings generalize to other tasks like symbolic reasoning and biomedical NER."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) Clear motivation and timely question: The paper addresses a relevant and open question in the era of advanced reasoning models: whether prompt engineering remains necessary. This is especially valuable as the community increasingly adopts LRMs without fully understanding their interaction with prompting strategies.\n\n(2) Rigorous experimental design: The use of a unified MCTS-based prompt optimization framework allows fair comparison across models as both task solvers and optimizers. The inclusion of low- and medium-resource settings, depth-controlled MCTS rollouts, and cross-task generalization strengthens the empirical foundation.\n\n(3) Comprehensive analysis: The paper includes convergence curves, survival plots, error categorization, and qualitative prompt comparisons, offering multiple lenses to interpret results. The observation that DeepSeek-R1 achieves high performance with shorter prompts is insightful."}, "weaknesses": {"value": "(1) Limited Methodological Novelty: The core optimization algorithm (MCTS) is adopted from prior work (e.g., PromptAgent). The primary novelty lies in its application to LRMs rather than in a fundamental advancement of the optimization technique itself. The paper is more of a thorough empirical benchmark than a methodological contribution.\n\n(2) Task selection bias: Event extraction is a highly structured, schema-constrained task. While the authors test generalization on two other tasks, the main conclusions are anchored in a setting where explicit guidelines and code-based prompting play an outsized role. It remains unclear whether the observed LRM advantages would hold in more open-ended reasoning tasks (e.g., mathematical proof, planning).\n\n(3) Incremental Nature of Key Finding: The central finding that more capable models benefit from optimized prompts is intuitively plausible and, to some extent, expected. While the quantitative demonstration is valuable, it may not be sufficiently surprising or groundbreaking for a top-tier venue."}, "questions": {"value": "(1) Beyond the application of an existing MCTS framework to a new model class (LRMs), what is the core conceptual or methodological novelty of this work that distinguishes it from prior prompt optimization research?\n\n(2) How confident are you that the observed advantages of LRMs would hold on the full ACE05 dataset with all 33 event types? Did you run any preliminary experiments that suggested context length would become a major bottleneck?\n\n(3) In the Geometric Shapes and NCBI tasks, did you use the same code-based prompting format? If not, how was the prompt structure adapted, and could that influence the observed generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YQVhUtDB7x", "forum": "cmzeMZNsmm", "replyto": "cmzeMZNsmm", "signatures": ["ICLR.cc/2026/Conference/Submission20842/Reviewer_cQqu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20842/Reviewer_cQqu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730410125, "cdate": 1761730410125, "tmdate": 1762936331782, "mdate": 1762936331782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies whether Large Reasoning Models (LRMs; DeepSeek-R1, OpenAI o1) still benefit from prompt optimization and whether they are good optimizers themselves, using event extraction (ACE05) as a case study within an MCTS framework. The main finding is that optimization helps all models but LRMs benefit most, both as task models and as optimizers. The claim is probed further with convergence/quality analyses and two non-EE tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1 - Clear problem framing and technically sound MCTS setup with explicit four-step loop.\n\nS2 - 2. Strong, easy-to-grasp headline result that LRMs both benefit more from optimization and optimize better than LLMs.\n\nS3 - 1. Sensible experimental design with two data regimes (ACElow/ACEmed) and two evaluation depths (depth 1 vs depth 5), enabling controlled comparison/"}, "weaknesses": {"value": "W1 - Metric mismatch in optimization v reporting -- The reward aggregates averaged F1 across TI/TC/AI/AC (s. 3.2), yet the analysis \"primarily reports AC\" (I understand the authors provide a citation for this choice but I find it unsatisfactory), creating a potential objective-reporting mismatch. Clarify why not optimize AC directly. \n\nW2 - Downsampling schema may bias conclusions -- To avoid (presumably) long prompts, the paper downsamples ACE05 to 10 event types and leaves long-contextg processing to future work (L236-245). This choice may favor models preferring concise prompts and limits external validity to full-schema EE. \n\nW3 - Depth-5 gains are modest, at best. Paper does note \"non-dramatic\" improvements from full-depth over d-1 (RQ2) and tab 1 shows small deltas. This sort of raises questions about the practical alue of deeper search. \n\nW4 - No statistical uncertainty reported. Main tables/figures lack confidence intervals or sig tests, making it hard to judge robustness of improvements."}, "questions": {"value": "Q1. Why optimize the average of TI/TC/AI/AC instead of AC directly, given AC is your primary metric? Any evidence that the averaged reward improves AC more than AC-only reward?\n\nQ2. How were the 10 ACE05 event types chosen, and do conclusions hold on the full 33-type schema (or with long-context methods)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xzShe1bbeE", "forum": "cmzeMZNsmm", "replyto": "cmzeMZNsmm", "signatures": ["ICLR.cc/2026/Conference/Submission20842/Reviewer_AcPa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20842/Reviewer_AcPa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923147983, "cdate": 1761923147983, "tmdate": 1762936331231, "mdate": 1762936331231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the interaction of prompt optimization for Large Reasoning Models (LRMs). They identify that this is a gap in existing literature which has only studied prompt optimization for Large Language Models (LLMs). In this work, DeepSeek-R1 and o1 are representative LRMs while GPT-4.5 and GPT4o are representative LLMs.\n\nIn particular, the paper focuses on structured prediction tasks since performance on these tasks is not yet saturated even with LRMs. The core analysis is conducted on the ACE05 Event Extraction task with supplementary analysis on Geometric Shapes and NCBI Disease NER tasks to show the generality of the findings. The paper uses a MCTS-based discrete prompt optimization algorithm with different LLMs/LRMs plugged in as the optimizer.\n\nThe core finding is that LRMs benefit from prompt optimization in both low and medium data regimes. Prompt optimized LRMs out-perform and have out-sized gains compared to prompt optimized LLMs. Moreover, they serve as effective optimizers for other LRMs/LLMs. This finding generalizes to 3 different structured prediction tasks.\n\nFurther qualitative analysis of remaining error types and optimized prompts is presented. E.g. DeepSeek-R1 produces effective prompts that are shorter than the other models considered."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is very clearly written and includes qualitative examples where appropriate.\n2. The findings fill-in an important gap in the literature (prompt optimization has mostly been studied int he context of LLMs)"}, "weaknesses": {"value": "I did not find anything lacking in the presentation and content. While the finding is not ground-breaking, the analysis is well done."}, "questions": {"value": "Suggestion\n---\nCertain prompt optimization techniques such as GEPA, Mipro, etc seem relevant to discuss in the related work. GEPA may be concurrent with this work so the missing citation is understandable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2NditWJnY8", "forum": "cmzeMZNsmm", "replyto": "cmzeMZNsmm", "signatures": ["ICLR.cc/2026/Conference/Submission20842/Reviewer_2nbd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20842/Reviewer_2nbd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20842/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124465362, "cdate": 1762124465362, "tmdate": 1762936330539, "mdate": 1762936330539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}