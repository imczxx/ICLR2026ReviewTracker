{"id": "GofFHwumFW", "number": 12212, "cdate": 1758206372433, "mdate": 1759897524939, "content": {"title": "Unary Feedback as Observation: Incentivizing Self-Reflection in Large Language Models via Multi-Turn RL", "abstract": "Large Language Models (LLMs) are increasingly deployed as agents that solve problems through multi-turn interaction, receiving feedback and refining their reasoning based on users' feedback. However, existing reinforcement learning with verifiable reward (RLVR) methods train them under a single-turn paradigm. As a result, we discovered that models often **fail to explore alternative reasoning paths or reflect on prior mistakes, producing repetitive and unadapted responses to feedback.**\n\nTo address this gap, we propose Unary Feedback as Observation (UFO), a framework that conditions policy updates on minimal unary feedback (e.g., “Let’s try again”) after incorrect answers. UFO is simple, compatible with existing single-turn RL setups, and incentivizes self-reflection. To further promote efficient and adaptive reasoning, we design reward structures that encourage _minimality_ (solving in fewer turns) and _diversity_ (exploring alternatives under failure). Experiments show that UFO preserves single-turn performance while improving multi-turn reasoning accuracy by about 14%. Crucially, UFO-trained models also **generalize beyond their training domain, transferring effectively to out-of-domain tasks** across mathematics, STEM, QA, and general knowledge, showing that **UFO teaches models self-reflective reasoning that carry over across domains**. Beyond these empirical gains, UFO points toward a broader paradigm for building adaptive reasoning agents: one that scales supervision from static datasets, reduces dependence on costly domain-specific feedback, and lays the foundation for more general, self-improving AI systems in open-ended real-world settings.", "tldr": "", "keywords": ["Large Language Models", "Multi-turn RL", "Self Reflection", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2caa4b5fda8478821b75342d541c911a8e54049a.pdf", "supplementary_material": "/attachment/2ac1228306fdcb89807bf9ae186a550e0c1349be.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates Large Language Model (LLM) reasoning and Reinforcement Learning (RL) fine-tuning in a multi-turn interactive setting. It addresses a key limitation of traditional RL with Verifiable Reward (RLVR), which uses a single-turn paradigm. This traditional approach often causes models to fail to explore alternative reasoning paths or reflect on prior mistakes, leading to repetitive and unadapted responses to feedback.\nTo address this, the authors propose Unary Feedback as Observation (UFO), a framework that conditions policy updates on minimal unary feedback (e.g., Let’s try again) following incorrect answers.\nExperiments demonstrate that UFO achieves superior performance and exhibits cross-domain generalization ability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The work successfully identifies the importance of multi-turn interaction for LLM agents.\nIt shows that RL training under a multi-turn setting effectively incentivizes diversity in reasoning paths, thereby improving final performance."}, "weaknesses": {"value": "1. The motivation for considering multi-turn interaction to encourage exploration and revision is sound. However, the paper's current setting seems limited to minimal unary feedback. It's unclear how this approach generalizes or applies to more natural multi-turn dense feedback scenarios (e.g., detailed human labels or code debugging feedback), as alluded to in paragraph 136. This limits the scope of the proposed method's applicability.\n2. The algorithmic novelty appears limited. The approach essentially trains with multi-step environmental feedback without a clearly defined adaptive algorithm design. The distinction between the proposed method and previous single-step PPO/GRPO algorithms that also trained with repeated answer generation is not sufficiently clear\n3. Regarding the mathematical reasoning problem, specifically:\n\n\tHow is the end of a single generation step determined in this multi-turn setting?\n\n\tWhat is the fundamental difference between UFO and previous PPO/GRPO work, beyond the multiple interactions (turns) and the use of more data examples?"}, "questions": {"value": "1.\tI am confused about Figure 1. What is the testing task used to generate this figure, and what is the specific definition of the effective answer ratio metric? A clearer explanation of why traditional RL training methods like PPO and GRPO show a performance drop compared to the original model is needed, perhaps with an earlier  and detailed explanation of the experiments would be better.\n\n2.\tIn Equation 1, the formulation for the LLM generating repeated responses is introduced. To accurately model this repetition, shouldn't the LLM's input be modified to include its previous outputs? Specifically, should the formulation be adapted to something like $q(y|x) \\times q(y|x, y_{prev})$, where $y_{prev}$ represents the previous response?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZXBG3GlFWi", "forum": "GofFHwumFW", "replyto": "GofFHwumFW", "signatures": ["ICLR.cc/2026/Conference/Submission12212/Reviewer_1iZH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12212/Reviewer_1iZH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816869460, "cdate": 1761816869460, "tmdate": 1762923158792, "mdate": 1762923158792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UFO (Unary Feedback as Observation), a multi-turn reinforcement learning framework that enables LLMs to learn self-reflective reasoning from minimal feedback (e.g., “Try again”) on static single-turn datasets. UFO treats past attempts and unary feedback as part of the observation state, trains with PPO, and introduces reward decay and repetition penalties."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A simple and effective method to push LLMs learn self-reflective reasoning.\n\n2. The reward decay and repetition penalties are introduced to encourage minimality (solving quickly) and diversity (avoiding repeated errors)."}, "weaknesses": {"value": "- The paper contains several errors that need correction. For example, Figure 7 is not cited in the paper, and the term “Multiturn” on line 462 should be hyphenated as “Multi-turn.”\n- Since the authors provide the model with a prompt that includes prior attempts and feedback, it is difficult to disentangle whether performance gains come from true multi-turn interaction or simply from a richer prompt signal.\n- **An Important question:** The experimental results suggest that models trained with multi-turn RL appear to require multi-turn evaluation to achieve improved performance. Does this imply that the model’s intrinsic reasoning capability—under single-turn evaluation—has not actually improved, and that richer prompt signals are still necessary to activate its parametric knowledge?\n- Section 2.3 should introduce and cover the content of Figure 1.\n- In Figure 2, what exactly is meant by \"effective answer ratio\"? Does it refer to answers that are both correct and derived via diverse reasoning paths? Which models were used in this analysis? How was the metric computed? Does the drop after RL imply that the model’s overall capability degrades?\n- In Table 1, what is the \"hotQA\" dataset used for Qwen-3B? Why was this experiment conducted only on Qwen-3B and not on other models?\n- Why did the authors choose PPO as the base RL algorithm instead of alternatives like GRPO or DAPO?\n- As shown in Table 1, models trained on math datasets (e.g., MMQ-Math) generalize well to QA tasks, but models trained on HotpotQA perform poorly on math tasks. What explains this asymmetry?\n- Why are hyperparameters (e.g., T_max, N) kept identical across models of different sizes? Shouldn’t larger models potentially benefit from different settings (e.g., more rollouts or longer interaction horizons)?"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IsUxsGBahy", "forum": "GofFHwumFW", "replyto": "GofFHwumFW", "signatures": ["ICLR.cc/2026/Conference/Submission12212/Reviewer_ZTSc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12212/Reviewer_ZTSc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990102732, "cdate": 1761990102732, "tmdate": 1762923158330, "mdate": 1762923158330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical limitation in current reinforcement learning with verifiable reward (RLVR) methods for large language models (LLMs): they are trained under a single-turn paradigm, which suppresses adaptive and self-reflective reasoning in multi-turn interactions. To address this, the authors propose Unary Feedback as Observation (UFO) — a simple yet effective framework that enables multi-turn reinforcement learning by conditioning policy updates on minimal unary feedback such as “Try again.” Experiments across multiple LLMs and nine benchmarks show that UFO improves multi-turn reasoning success"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tDemonstrates that self-reflective reasoning transfers across domains and architectures.\n2.\tIntroduces reward-shaping principles (minimality/diversity) that improve both efficiency and adaptability."}, "weaknesses": {"value": "1.\tUnary feedback (“Try again”) is idealized; real human feedback can be more ambiguous or inconsistent.\n2.\tThe sensitivity to decay factor γ and repetition penalty λ could be analyzed more deeply."}, "questions": {"value": "1.\tCould this unary feedback mechanism be extended to graded feedback (e.g., “close,” “partially correct”)?\n2.\tWhat are the computational costs compared to single-turn RL — is training efficiency affected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j9k4AnZxbS", "forum": "GofFHwumFW", "replyto": "GofFHwumFW", "signatures": ["ICLR.cc/2026/Conference/Submission12212/Reviewer_xBUi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12212/Reviewer_xBUi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169856008, "cdate": 1762169856008, "tmdate": 1762923157774, "mdate": 1762923157774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}