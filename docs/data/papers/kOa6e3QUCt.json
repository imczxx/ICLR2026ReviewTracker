{"id": "kOa6e3QUCt", "number": 16642, "cdate": 1758267180319, "mdate": 1759897227835, "content": {"title": "Progressive Alignment for Robust Domain Adaptation", "abstract": "Unsupervised Domain Adaptation (UDA) has advanced knowledge transfer between labeled source and unlabeled target domains, yet existing methods fall short in real-world scenarios where adversarial attacks threaten model reliability. Robustness against such attacks is essential but remains critically underexplored in UDA. Existing methods often treat domain alignment and adversarial defense as separate steps, causing unstable training, noisy pseudo-labels, and incomplete feature alignment ultimately limiting their effectiveness. Addressing both domain shift and adversarial robustness simultaneously is vital for deploying trustworthy models in dynamic, adversarial environments. In this work, we propose a robust UDA method from the perspective of multi-source and multi-target domain adaptation, treating clean and adversarial samples across both source and target as distinct domains. We aim to align both clean and adversarial domains across source and target within the adaptation framework. Therefore, we use progressive domain alignment strategy that explicitly aligns clean target features with multi-source domains through classifier discrepancy minimization, and implicitly aligns adversarial target features by enforcing classifier agreement on pseudo-labels. We find that this strategy effectively handles both domain shift and adversarial perturbations, leading to improved generalization and robustness. We demonstrate the effectiveness of our approach through extensive experiments on four benchmark datasets, accompanied by component-wise ablations. Our method achieves standard accuracies of 62.0%, 88.4%, 82.5%, and 73.7% and the corresponding robust accuracies under PGD-20 attack with $\\epsilon = 2/255$ are 49.4%, 78.3%, 77.3%, and 72.1% on the Office-Home, PACS, VisDA, and Digit benchmark datasets, respectively.", "tldr": "", "keywords": ["Unsupervised Domain Adaptation", "Adversarial Training", "Adversarial Attack"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/89ef0e506e4ff0b015c6b6879ecd8d051bbba0ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel approach to adversarially robust unsuperivsed domain adaptation, which treats the clean and adversarial samples from source and target data as four distinct domains, and handles the alignment with adversarial source domain, clean target domain, and adversarial target domain successively: (1) warm-start training adopts adversarial training on source data; (2) explicit alignment utilizes MCD for clean target domain; (3) implicit alignment adapts the model to adversarial target domain with a curriculum learning strategy from high-confidence to low-confidence adversarial samples, and uses a double consistency criterion for more reliable pseudo-labels. Experimental results suggest the superiority of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed progressive learning framework and the implicit alignment method are novel for robust UDA.\n2. Figures 3 and 4 provide insights into the training process and demonstrate the contribution of different stages.\n3. The effectiveness of the proposed method is validated by extensive experiments."}, "weaknesses": {"value": "1. The presentation of the paper requires serious revision to ensure formalness and clarity. \n  - $\\mathcal{H}$ is undefined in Line 145.\n  - Eq. 9-12 are ambiguous due to the reuse of $\\mathbf{Z}$ to represent the outputs for different samples.\n  - Incorrect citation styles (should use `\\citep` for most cases).\n  - Missing citations at Lines 709-710.\n  - Missing spaces around periods and parentheses (e.g., Line 344) and other typos.\n2. The proposed method uses the Target Consistency Rate to determine the convergence of explicit alignment instead of the discrepancy loss used in the original MCD. The superiority of this convergence condition is not empirically verified.\n3. Judging from Algorithm 1 (Sec. C), the adversarial source samples are pre-computed before warm-start and fixed during training, instead of being computed based on the latest model parameters. This can significantly deteriorate the effectiveness of adversarial training. The implicit alignment stage may also suffer from the issue of offline adversarial samples, even though the adversarial test samples are updated every $n$ epochs.\n4. It is unclear which baseline results are replicated in this paper and which are quoted from the original papers. For quoted results, the comparison may be unfair because the test target data are split for this paper (Lines 811-815) and could differ from those in previous studies. The consistency in experimental settings of the compared methods requires further clarification.\n3. The provided source code (anonymous link at Line 845) is buggy, with undefined variables and indentation errors."}, "questions": {"value": "1. Are the results in Section D.2 produced by training the models with $\\epsilon=8/255$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "L4iqdtr8ku", "forum": "kOa6e3QUCt", "replyto": "kOa6e3QUCt", "signatures": ["ICLR.cc/2026/Conference/Submission16642/Reviewer_epLM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16642/Reviewer_epLM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643779926, "cdate": 1761643779926, "tmdate": 1762926704713, "mdate": 1762926704713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses adversarial robustness in the context of unsupervised domain adaptation (UDA). The authors propose a robust UDA framework that treats adversarial examples as samples from distinct source and target domains separate from the clean data. This formulation effectively converts the standard UDA problem into a multi-source, multi-target domain alignment setting. A progressive alignment strategy is then introduced to train the model to align these domains sequentially."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes an interesting and novel perspective on integrating adversarial robustness and domain adaptation. By incorporating adversarial examples directly into the UDA framework, the method unifies what is often a two-stage training process (UDA followed by adversarial training) into a single, coherent pipeline."}, "weaknesses": {"value": "Further clarifications needed:\n\n- In Line 139, adversarial examples are introduced as belonging to additional domains. However, adversarial perturbations are typically generated with respect to a given classifier. Could the authors clarify which classifier is used to generate these adversarial examples within the “adversarial source” and “adversarial target” domains?\n- As the paper studies UDA under adversarial perturbations, the precise **learning objective** and **evaluation metrics** need to be formally defined. What exactly is the target performance criterion (e.g., robust accuracy on target domain under specific attack)? Is the clean accuracy a part of performance criterion as well?\n- Regarding loss functions (2) and (3), please elaborate on the challenges of directly optimizing Equation (2). Explaining this difficulty would clarify the motivation for adopting the surrogate objective in Equation (3).\n- The paper maximizes the discrepancy between two classifier heads for unlabeled target data (i.e., objective function (4)). Could the authors explain the intuition behind this design? Why should increasing the discrepancy between classifiers help improve adversarial robustness or facilitate domain alignment?\n- Line 244-248. Which model is used to generate adversarial examples? Why are _“weak” adversarial examples_ (whose predictions match the clean examples) retained for training? In what sense are these examples considered _“reliable”_? Reliable with respect to what?\n- The proposed progressive training strategy begins with “weak” adversarial examples and gradually introduces “stronger” ones. Could the authors justify why such progressive inclusion is necessary? Providing a reasonable explanation would greatly improve the soundness of this design choice."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G4ZsiN2Nw2", "forum": "kOa6e3QUCt", "replyto": "kOa6e3QUCt", "signatures": ["ICLR.cc/2026/Conference/Submission16642/Reviewer_LyU6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16642/Reviewer_LyU6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761685788178, "cdate": 1761685788178, "tmdate": 1762926704151, "mdate": 1762926704151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the robust unsupervised domain adaptation (UDA) problem setting. The paper proposes a method from the perspective of multi-source and multi-target domain adaptation, treating clean and adversarial samples across both source and target as distinct domains. The proposed method leverages progressive domain alignment strategy that explicitly aligns clean target features with multi-source domains through classifier discrepancy minimisation, and implicitly aligns adversarial target features by enforcing classifier agreement on pseudo-labels."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses robust UDA setting which is an important and practical problem setting.\n2. The proposed method is well motivated and the overall paper is well written."}, "weaknesses": {"value": "1. Limited Novelty: The methodologies in the two main modules: Explicit Alignment and Implicit Alignment, appear closely related to prior work. In particular, Explicit Alignment is similar to Saito et al. (2018b), and Implicit Alignment resembles Han et al. (2020). Please clarify what is fundamentally new in your formulation and why these differences matter empirically or theoretically.\n\n2. Methodology needs clearer explanation. In the warm-start stage, why is the loss on adversarial samples defined identically to the loss on clean samples? By design, adversarial examples disrupt effective feature learning, optimizing both sets in the same way makes it unclear how the latent features become robust. Likewise, the Implicit Alignment module requires more detail: from Eq. (10), the pseudo-label is the argmax agreed upon by both classifiers. In that case, wouldn’t the loss in Eq. (12), computed between the same pseudo-label and the logits, tend to be trivially small? \n \n3. Computational complexity analysis is missing: As the proposed method contains multiple stages, a detailed analysis of training and inference complexity is necessary to assess practical feasibility.\n\n4. Missing baselines: In Table 1, several standard UDA methods are missing [R1][R2][R3]. It is important to compare the proposed method with these to understand the effectiveness of the proposed method.\n\n\n[R1] Zhang, Yuchen, et al. \"Bridging theory and algorithm for domain adaptation.\" International conference on machine learning. PMLR, 2019.\n\n[R2] Rangwani, Harsh, et al. \"A closer look at smoothness in domain adversarial training.\" International conference on machine learning. PMLR, 2022.\n\n[R3] Zhang, Xinyu, Meng Kang, and Shuai Lü. \"Low category uncertainty and high training potential instance learning for unsupervised domain adaptation.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 38. No. 15. 2024."}, "questions": {"value": "1. Explain clealry the differences between the proposed modules, Explicit Alignment and Implicit Alignment with prior works Saito et al. (2018b) and Han et al. (2020), respectively.\n\n2. Explain the methodology clearly. Specifically, In the warm-start stage, why is the loss on adversarial samples defined identically to the loss on clean samples? By design, adversarial examples disrupt effective feature learning, optimizing both sets in the same way makes it unclear how the latent features become robust. Likewise, the Implicit Alignment module requires more detail: from Eq. (10), the pseudo-label is the argmax agreed upon by both classifiers. In that case, wouldn’t the loss in Eq. (12), computed between the same pseudo-label and the logits, tend to be trivially small? \n\n3. Provide a detailed computational complexity analysis to assess the practical feasibility of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BuLgOGDOhC", "forum": "kOa6e3QUCt", "replyto": "kOa6e3QUCt", "signatures": ["ICLR.cc/2026/Conference/Submission16642/Reviewer_4gtK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16642/Reviewer_4gtK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821002381, "cdate": 1761821002381, "tmdate": 1762926703774, "mdate": 1762926703774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to address the adversarial robustness problem in an unsupervised domain adaptation setting. The authors argue that solving both the domain shift and adversarial shift problems jointly, rather than treating them as separate (decoupled) problems, is the right approach. To this end, they reframe the task as a multi-source, multi-target domain adaptation problem, which accounts for both the domain shift and the adversarial gap. The proposed method consists of three phases.\n\nFirst, a model with two classifier heads is trained on labeled source domains using both normal and adversarial examples. Second, the classifiers and feature extractor are trained alternately in an adversarial manner. Finally, a pseudo-labeling process is used to obtain labels, after which adversarial examples from the target domain are generated and explicit domain adaptation is performed.\n\nMultiple experiments are conducted to demonstrate the performance of the proposed approach. For the experiments, the authors assume access to two labeled domains."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Overall, this is a clear and easy-to-follow paper. The authors have identified gaps in prior work, reframed the problem and proposed a method to address them. The proposed approach is sufficiently novel and effectively bridges the identified gap."}, "weaknesses": {"value": "The experimental results are somewhat limited. It would be useful to include additional experiments using transformer-based backbones, such as ViT. Moreover, evaluating the method with standard adversarial attacks, such as AutoAttack [1], would strengthen the results, as PGD-based attacks are known to be prone to certain issues.\n\nAnother weakness of the work is practical utility. Authors, in the introduction and abstract mention existing works limited in real-world domain, such as \n>yet existing methods fall short in real-world scenarios where adversarial attacks threaten model reliability. \n\nYet, there is no discussion around this point in the paper. \n\n[1] https://robustbench.github.io/"}, "questions": {"value": "In Section 2.2, it is unclear how the authors ensure that adversarial training actually contributes to the overall learning.\n\nThe authors assume access to multiple labeled source domains for their experiments, whereas most prior works typically consider only a single labeled source domain. It is therefore unclear how a fair comparison with these prior works can be justified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MN3zpsbLOb", "forum": "kOa6e3QUCt", "replyto": "kOa6e3QUCt", "signatures": ["ICLR.cc/2026/Conference/Submission16642/Reviewer_ogfg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16642/Reviewer_ogfg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858261337, "cdate": 1761858261337, "tmdate": 1762926703374, "mdate": 1762926703374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of adversarial robustness in Unsupervised Domain Adaptation (UDA) by proposing a novel formulation that treats clean and adversarial samples from both source and target domains as four distinct distributions to be aligned. The authors introduce a progressive alignment strategy that first explicitly aligns clean target features with the multi-source domains and then implicitly aligns adversarial target features by enforcing classifier consistency on refined pseudo-labels. Some experiments on four benchmarks validate that the performance of the proposed method outperforms baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Pros:\n* The authors conducted a bunch of experiments to validate the effectiveness of their method across four benchmarks."}, "weaknesses": {"value": "Cons:\n* Please carefully use \\citet and \\citep. You should only use \\citet when the reference is grammatically part of the sentence, usually as the subject.\n* In Lines 171-172, why does minimizing the loss in Eq. (2) produce consistent predictions across two classifiers? And since (2) is upper bounded (instead of lower bounded) by the average of individual losses, it doesn’t mean each individual loss is also minimized due to (2). \n* What is the motivation for introducing two classifiers, H1 and H2? The authors use the discrepancy between these two classifiers as the divergence between source and target domains. But this doesn’t make sense to me. And why don’t you just use some widely used divergence metric? Because of this, I don’t think the minimax process in Section 2.2 could align the source and target domain.\n* Very limited innovation compared to prior work and methods. This method wants to jointly align the source and target, clean and adversarial, while there are many prior studies [1,2,3] proposing similar frameworks and ideas. \n\n[1]: Exploring Adversarially Robust Training for Unsupervised Domain Adaptation. https://arxiv.org/abs/2202.09300\n\n[2]: Adversarially robust unsupervised domain adaptation. https://www.sciencedirect.com/science/article/pii/S000437022500102X\n\n[3]: Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training. https://arxiv.org/abs/2402.12187"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eJGUYnhM7O", "forum": "kOa6e3QUCt", "replyto": "kOa6e3QUCt", "signatures": ["ICLR.cc/2026/Conference/Submission16642/Reviewer_FyYi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16642/Reviewer_FyYi"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061128779, "cdate": 1762061128779, "tmdate": 1762926702902, "mdate": 1762926702902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}