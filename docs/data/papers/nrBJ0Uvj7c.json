{"id": "nrBJ0Uvj7c", "number": 7492, "cdate": 1758024479371, "mdate": 1759897849634, "content": {"title": "Towards Lossless Memory-efficient Training of Spiking Neural Networks via Gradient Checkpointing and Spike Compression", "abstract": "Deep spiking neural networks (SNNs) hold immense promise for low-power event-driven computing, but their direct training via backpropagation through time (BPTT) incurs prohibitive memory cost, which limits their scalability. Existing memory-saving approaches, such as online learning, BPTT-to-BP, and reversible networks, compromise accuracy, training speed, or applicability. In this work, we propose a novel and broadly applicable pipeline for memory-efficient SNN training that preserves BPTT's accuracy. Our pipeline integrates layer-wise gradient checkpointing with lossless spike compression to eliminate internal state storage and reduce the memory cost of per-layer input spikes. We also introduce a multi-stage checkpoint adjustment strategy that adaptively refines checkpoint placement based on profiling results to further optimize memory usage and improve training speed. Wrapped in an optimization pass, the pipeline automatically restructures the computation flow before training with minimal user effort. Extensive experiments on diverse architectures and tasks demonstrate up to $8\\times$ memory efficiency gains with $\\le 20\\\\%$ speed reduction and no accuracy loss. Our method provides a practical solution for efficient and scalable SNN training.", "tldr": "A widely applicable automatic pipeline combining spatio-temporal gradient checkpointing and spike compression that achieves up to 8× memory savings for SNN training while preserving BPTT-level accuracy and speed.", "keywords": ["Spiking Neural Network", "Training Memory Optimization", "Gradient Checkpointing"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ace2470c2f401a01a72a39dfa57d2809502c45e.pdf", "supplementary_material": "/attachment/c0f29d95eaab0010e8205881c3fc971e0ed3a313.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes to use gradient checkpoint (GC), possibly combined with compression, to reduce the peak memory required for training of SNN. The advantage of the proposed methods is that they do not introduce mathematical discrepancies (except for possible numerical ones), thus maintaining the training accuracy. The paper introduces a simple yet effective heuristic to minimize the peak memory by combining spatial and temporal GC segment partitioning. The paper is well written and easy to read.\n\nOverall, the training performance penalty remains very limited (>0.9x on average) compared to the gain in peak memory (<0.4x on average), making the proposed method relevant and applicable to real world models training. However, the novelty and innovation of the paper remains limited: GC is a well-known technic, as well as compression."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and easy to read;\n- The peak memory reduction technic and heuristic proposed in the paper is relatively generic and applicable to many SNN models;\n- The is a mathematical equivalency with the original model (despite possible numerical discrepancies);\n- The impact on learning performance is very limited, compared to the gain on peak memory."}, "weaknesses": {"value": "- The scope and impact of the paper remains limited;\n- The novelty limited: GC optimization and compression are well-known technics;\n- It is not clear how the proposed method will benefit the community: while some code snippets are provided in the supplementary materials, the possible release or diffusion of the source code is not mentioned."}, "questions": {"value": "- Do the authors intent to release their method source code?\n- It could be interesting for the authors to bring more insight about the different compression methods explored? In particular, by providing some figures on how they compare in terms of performances and peak memory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "okshrturIW", "forum": "nrBJ0Uvj7c", "replyto": "nrBJ0Uvj7c", "signatures": ["ICLR.cc/2026/Conference/Submission7492/Reviewer_XHsK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7492/Reviewer_XHsK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761487202733, "cdate": 1761487202733, "tmdate": 1762919607080, "mdate": 1762919607080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an automatic, lossless memory optimization pipeline for training spiking neural networks (SNNs) using backpropagation through time (BPTT). The core idea is to combine layer-wise gradient checkpointing with lossless spike compression, targeting the two major memory bottlenecks in SNN training—internal state storage and per-layer spike activations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality\n1. Introduces a lossless and automatic memory-saving approach for SNNs, avoiding the accuracy compromises typical in online learning or reversible architectures.\n2. The combination of gradient checkpointing with binary spike compression is novel and elegantly leverages SNN characteristics (binary activations).\n\nQuality\n1. Provides comprehensive theoretical analysis (Equations 2–5) for memory cost and correctness.\n2. Extensive empirical validation on multiple architectures and datasets.\n3. Clear comparisons with other efficiency-oriented methods (online learning, BPTT-to-BP, reversible networks).\n\nClarity\nWell-structured paper with good logical flow. Figures illustrate the differences between BPTT, checkpointing, and the proposed compression. The language is clear and technically rigorous.\n\nSignificance\n1. Addresses one of the most critical barriers in SNN research: high memory cost during training.\n2. Enables scaling SNNs to large architectures and long sequences, potentially democratizing access to neuromorphic research on commodity GPUs."}, "weaknesses": {"value": "1. Limited exploration of trade-offs: Although the paper claims ≤20% slowdown, more fine-grained runtime profiling across levels would strengthen the argument for scalability. The additional computational cost of spike compression/decompression could be analyzed more quantitatively.\n2. Sparse ablation: The adaptive checkpoint adjustment (spatial vs. temporal) is key but not deeply evaluated in isolation; Ablations showing how each component (spatial partitioning, temporal partitioning, greedy restoration) contributes to performance would enhance interpretability."}, "questions": {"value": "1. Checkpoint adaptation: How sensitive is the memory efficiency to the chosen “level” parameter (O1–O4)? Could adaptive tuning be integrated dynamically during training? How does the system decide spatial vs. temporal split thresholds? Could these be learned or auto-tuned?\n2. The paper reports ≤20% slowdown, but could the authors provide more detailed runtime decomposition? Does the time overhead scale linearly with the number of checkpoints or show nonlinear interactions with spatial/temporal splits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EZ0zs6gi1G", "forum": "nrBJ0Uvj7c", "replyto": "nrBJ0Uvj7c", "signatures": ["ICLR.cc/2026/Conference/Submission7492/Reviewer_eX6a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7492/Reviewer_eX6a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570246420, "cdate": 1761570246420, "tmdate": 1762919606579, "mdate": 1762919606579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Training spiking neural networks (SNNs) is highly memory-intensive, typically requiring O(LT) memory. This paper addresses the issue by taking advantage of the binary nature of spikes and adopting a checkpointing strategy. During training, the computational graph is not constructed; instead, the input data is passed through the network while intermediate activations, if they are spikes, are compressed to reduce memory usage. Once the loss at the final layer is computed, a local computational graph is reconstructed at each layer, starting from the final layer and proceeding backward to the first layer, using the stored intermediate representations as inputs. This strategy significantly reduces the memory footprint during training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Paper is easy to follow. \n\nThe proposed method can be easily adapted to train SNN models in a memory-efficient way."}, "weaknesses": {"value": "I guess it's not very efficient if the SNN models are trained online, here it is assumed that the entire spiking data is available."}, "questions": {"value": "1. Is it correct that the main source of memory efficiency arises from the fact that spike activations can be compressed, rather than being stored as 32-bit floating-point representations?\n\n2. This method appears to rely on having access to the entire sequence of T input spikes during training. In an online setting, where input spikes arrive sequentially, how would the model behave? Could the authors comment on its applicability and performance in such scenarios?\n\n3. The checkpointing strategy has also been successfully applied to train Neural ODEs [1]. Given the conceptual similarities between SNNs and NODEs from the ODE perspective, the authors may consider citing this related work for completeness.\n\n4. Could the authors provide results obtained after training the models for the full number of epochs (e.g., on DVS-CIFAR10 or other datasets)? In the paper, only partial training results are presented to demonstrate the memory advantage. It would be valuable to also show that the accuracy remains unaffected when the model is trained to convergence.\n\n[1] https://proceedings.mlr.press/v119/zhuang20a/zhuang20a.pdf"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pQdIS9zIA9", "forum": "nrBJ0Uvj7c", "replyto": "nrBJ0Uvj7c", "signatures": ["ICLR.cc/2026/Conference/Submission7492/Reviewer_pEn5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7492/Reviewer_pEn5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807644883, "cdate": 1761807644883, "tmdate": 1762919606193, "mdate": 1762919606193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates memory compression mechanisms in large language models (LLMs), focusing on achieving lossless or near-lossless memory efficiency. It systematically analyzes the relationship between attention redundancy, representation compression, and retrieval fidelity, and proposes a new compression architecture intended to retain semantic fidelity while reducing activation and KV cache memory."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses an important issue in modern LLM systems — the scalability of context memory and cache compression. This is a crucial bottleneck for both open-source and proprietary systems. The authors provide a solid overview of various strategies (quantization, pruning, clustering, key-value cache compression, etc.), framing them under a unified memory-entropy perspective. This synthesis is useful for the community. The experiments, particularly those on retrieval fidelity and downstream reasoning, demonstrate that small amounts of information loss can lead to disproportionate accuracy degradation, supporting the motivation for “lossless” compression."}, "weaknesses": {"value": "1. The “lossless” definition used in the paper lacks a rigorous mathematical formulation. The text refers vaguely to “semantic equivalence under compression,” but no formal mapping (e.g., bijection between original and compressed latent space preserving mutual information) is provided. This makes it hard to assess the conceptual contribution.\n2. While the paper cites several existing compression methods (e.g., KV cache quantization, low-rank adaptation, memory token merging), it is unclear what aspect is fundamentally new. The proposed framework seems to integrate known techniques rather than introduce a distinctly novel algorithm.\n3. The experiments appear to be conducted on small- to medium-scale models and limited benchmarks. It’s unclear how the proposed method scales to models beyond 7B parameters or generalizes to instruction-following and multimodal tasks. In addition, most results lack variance reporting and ablation analysis.\n4. The metrics primarily evaluate perplexity and accuracy but not information preservation. A more appropriate evaluation would involve mutual information, reconstruction error, or similarity metrics on attention distributions.\n5. The claim that “compression can improve reasoning stability by removing noise” is speculative and not well supported by experiments. Correlation is shown, but no causal analysis or controlled experiments are presented.\n6. The paper repeatedly claims that “redundant activations lead to inefficiency” and “compression improves generalization,” but it only shows correlations between compression ratio and performance metrics (e.g., perplexity).\nThere is no controlled experiment isolating causal factors — for instance, whether observed gains arise from reduced redundancy, implicit regularization, or simple noise filtering. Without causal validation, these claims remain speculative.\n7. Memory compression may cause unstable behavior, especially in autoregressive decoding where small perturbations can cascade.\nThe paper does not evaluate output stability under repeated sampling, sensitivity to compression noise, or robustness across different temperature settings. Without such analyses, it is unclear whether the approach is reliable for real-world use."}, "questions": {"value": "1. How exactly is “lossless” defined in your experiments? Is it equivalent to maintaining identical outputs, or bounded semantic deviation?\n2. Have you measured mutual information or entropy changes before and after compression?\n3. How does your method compare to dynamic memory eviction or entropy-based key selection methods?\n4. Does the compression framework support online adaptation — e.g., varying compression ratio during inference?\n5. Are the proposed memory structures compatible with GPU-efficient implementations (e.g., FlashAttention or PagedAttention)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pzXTlDBVK7", "forum": "nrBJ0Uvj7c", "replyto": "nrBJ0Uvj7c", "signatures": ["ICLR.cc/2026/Conference/Submission7492/Reviewer_cB7e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7492/Reviewer_cB7e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7492/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822397662, "cdate": 1761822397662, "tmdate": 1762919605543, "mdate": 1762919605543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}