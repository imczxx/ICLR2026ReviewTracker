{"id": "G4I23g5Ugh", "number": 13405, "cdate": 1758217446841, "mdate": 1759897439901, "content": {"title": "PriorGuide: Test-Time Prior Adaptation for Simulation-Based Inference", "abstract": "Amortized simulator-based inference offers a powerful framework for tackling Bayesian inference in computational fields such as engineering or neuroscience, increasingly leveraging modern generative methods like diffusion models to map observed data to model parameters or future predictions. These approaches yield posterior or posterior-predictive samples for new datasets without requiring further simulator calls after training on simulated parameter-data pairs. However, their applicability is often limited by the prior distribution(s) used to generate model parameters during this training phase. To overcome this constraint, we introduce *PriorGuide*, a technique specifically designed for diffusion-based amortized inference methods. PriorGuide leverages a novel guidance approximation that enables flexible adaptation of the trained diffusion model to new priors at test time, crucially without costly retraining. This allows users to readily incorporate updated information or expert knowledge post-training, enhancing the versatility of pre-trained inference models.", "tldr": "PriorGuide enables efficient incorporation of arbitrary priors at inference time for amortized diffusion-based simulation-based inference, without retraining the model.", "keywords": ["Simulation-based inference", "Amortized inference", "Test-time adaptation", "Bayesian workflow", "Neural posterior estimation", "Diffusion models"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54f2c38ad6f39691d7fa6854f305eea1fe16793b.pdf", "supplementary_material": "/attachment/4f5d083696b189b08d440a23cb85dbe2f6142ac6.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes PriorGuide, a method to adapt diffusion-based amortized\nsimulation-based inference (SBI) models to new priors at test time without\nretraining. The key idea is to express the target-posterior score as the original score\n(under the training prior) plus a guidance term involving the prior ratio\n$(r(\\omega)=q(\\omega)/p_{\\text{train}}(\\omega))$. To make the guidance tractable, the\nreverse transition kernel is approximated as Gaussian and the prior ratio is\napproximated by a Gaussian Mixture Model (GMM), which yields a closed-form guidance\nupdate. The method is evaluated on a suite of SBI problems for both posterior and\nposterior-predictive inference. Empirically, PriorGuide attains competitive performance\nrelative to baselines, and the paper further studies a compute–accuracy trade-off\nusing interleaved Langevin refinement."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**Quality.** The paper’s theoretical foundation is solid: the score decomposition with a\nguidance term is standard and correctly instantiated for the SBI setting. The Gaussian\nreverse-kernel and GMM prior-ratio approximations are reasonable and enable a practical\nalgorithm. The analysis of test-time compute via Langevin refinement is appropriate, and\nthe extended experiments cover canonical SBI tasks. The ablation and sensitivity studies\nin the appendix further strengthen the empirical grounding.\n\n**Clarity.** The manuscript is exceptionally well written, with well-structured\nexposition and clear intuition for the guidance term. Notation is introduced early and\nconsistently; figures and tables are easy to interpret. The appendix integrates detailed\nmethodological clarifications—GMM fitting, diagnostics, and runtime analyses—that\nimprove reproducibility. The discussion of the Pareto front between diffusion and\nLangevin steps is particularly insightful.\n\n**Significance and Contribution.** The work addresses a practically important\nproblem—changing priors after training—where retraining amortized SBI models can be\nprohibitively expensive. Adapting diffusion guidance to encode new priors at inference\nis an appealing and useful contribution. The empirical scope is strong, especially given\nthe additional baselines in the appendix (Rejection Sampling, Sequential Importance\nResampling, NLE+MCMC), a sensitivity analysis, and a 20D test demonstrating scalability.\nThe contribution remains primarily empirical but offers meaningful methodological value\nfor the SBI community.\n\n**Originality.** Applying diffusion guidance to test-time prior adaptation in amortized\nSBI is a natural but novel design choice. The GMM ratio approximation provides a\npractical mechanism to operationalize this guidance, and the extended analyses confirm\nthat the method maintains competitive performance where simpler baselines fail."}, "weaknesses": {"value": "**Scope and positioning.** While the paper clearly demonstrates PriorGuide’s strengths,\nit could offer more explicit practical guidance for practitioners—specifically, when\nPriorGuide should be preferred over simpler baselines such as NLE + MCMC, RS, SIR, or\nrelated methods like SIMFORMER and ACE and how to balance its additional test-time\ncomputation against retraining or posterior-correction methods. While this is discussed\nqualitatively in the appendix, a concise summary in the main text would make the\ncontribution more actionable.\n\n**Dimensionality constraints.** Although the new 20D Gaussian Linear results demonstrate\nscalability, these remain relatively simple benchmarks. A more complex or real-world\nhigh-dimensional example would strengthen the claim of general applicability.\n\n**Approximation limits.** The Gaussian reverse-kernel approximation, while adequate for\nthe reported tasks, remains a simplifying assumption. Even with Langevin refinement\nmitigating some of its effects, further diagnostic guidance for practitioners would be\nuseful."}, "questions": {"value": "1. Could the authors provide clearer practical guidance on when PriorGuide is\n   preferable to simpler or amortized alternatives? For instance, how should\n   practitioners trade off its added test-time computation against retraining costs or\n   simpler posterior-correction methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The work uses synthetic data and provides extensive\nmethodological details and open-source code. The potential for ethical risk is minimal.\nThe expanded appendices substantively improve reproducibility and transparency."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d8oDRCk4Yk", "forum": "G4I23g5Ugh", "replyto": "G4I23g5Ugh", "signatures": ["ICLR.cc/2026/Conference/Submission13405/Reviewer_zfhL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13405/Reviewer_zfhL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753075014, "cdate": 1761753075014, "tmdate": 1762924038982, "mdate": 1762924038982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PriorGuide introduces an extension to framework of simulation-based inference (SBI) with diffusion models that allows pre-trained diffusion models to incorporate new prior distributions at inference time without requiring retraining. This is achieved using a novel Gaussian mixture model approximation to make the target prior tractable for guiding the diffusion sampling process. Empirical results on synthethic data demonstrate PriorGuide’s effectiveness in accurately recovering posterior and posterior-predictive distributions across various SBI problems. The method also allows for refinement through Langevin dynamics, enabling a balance between computational cost and accuracy.\n\nBy enabling adaptation to new priors without retraining, PriorGuide exemplifies a “test-time compute” approach, extending the capabilities of pre-trained models with targeted computations. This decoupling of simulator runs from prior specification offers practical benefits especially in the case where simulations are costly, including the ability to perform post-hoc prior sensitivity analyses and incorporate domain expert knowledge after training, ultimately reducing the computational burden of scientific workflows which employ SBI."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The manuscript proposes an extension of SimFormer modeling to the challenging problem of representing complex joint probability distributions. The core idea is relatively straightforward, yet the authors provide a rigorous mathematical treatment that appropriately identifies both the potential benefits and inherent limitations of this approach, acknowledging the necessary reliance on approximations. The empirical evaluation is a strength of the paper; the results presented demonstrate promising performance, particularly in the accuracy of the reported uncertainty estimates (Tables 1 & 2). The selection of benchmarking use cases is clearly motivated and relevant to the stated goals. Finally, the authors effectively utilize concise and informative visualizations to support their claims and illustrate the underlying hypothesis."}, "weaknesses": {"value": "The manuscript would benefit from a more careful review of its language, as instances of anthropomorphism (e.g., “the model having seen few training examples”) can detract from the scientific rigor. Additionally, the presentation of the derivation for 'r' is hampered by misaligned page breaks, separating key explanatory text and hindering comprehension. More significantly, while the paper thoroughly explores the technical capabilities of PriorGuide, it lacks a discussion of the broader implications of manipulating the prior distribution within a Bayesian framework. The authors appear to assume a familiarity with this interplay between prior, simulator, and the scientific endeavor, which may limit the accessibility and impact of the work for a wider audience."}, "questions": {"value": "- page 4, the derivation of eq 6-9 is central to the paper, please try to rearrange so that the text in lines 216-220 is readily visible close the equations (e.g. by removing lines 191-195)\n- the code for the experiments was not found in the paper, i.e. through an anonymised git repo, this should be corrected for publication\n- line 232: please do not use anthropomorphic language to describe model behavior \"the model having seen few training examples\", i.e. the model has consumed or been trained on\n- line 234: \"using standard OOD metrics\" as such a standard exists only colloquially, please remove \"standard\"\n- equ 13: the text lacks a discussion if `K` is a free parameter and how to choose it\n- line 274: \"Notably, when p(θ) is uniform, r(θ) reduce*s* to q(θ), which can be directly specified as a Gaussian mixture.\" perhaps also add hint why r reduces to q (due to using the expectation I guess)\n- line 310: \"To incorporate this with regular diffusion ...\" perhaps comment on the computational cost here already."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "leKYjUMk8V", "forum": "G4I23g5Ugh", "replyto": "G4I23g5Ugh", "signatures": ["ICLR.cc/2026/Conference/Submission13405/Reviewer_kkwQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13405/Reviewer_kkwQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840009121, "cdate": 1761840009121, "tmdate": 1762924038714, "mdate": 1762924038714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**PriorGuide** targets a practical SBI need: adapting a trained diffusion-based posterior estimator to **new priors at test time** with **no retraining**. The paper derives a clean **score decomposition** in which the target posterior score equals the trained score plus a **prior-ratio guidance** term . To make this usable, the authors (i) use a *standard* Gaussian reverse-kernel approximation (via Tweedie’s formula) and (ii) fit a Gaussian mixture to the prior ratio, yielding a *novel* closed-form guidance update that plug directly into the diffusion sampler. Optional few-step Langevin corrections at low noise tighten asymptotics and expose a neat compute–accuracy knob alongside the diffusion steps. Empirically, PriorGuide improves both posterior and posterior-predictive metrics across several SBI tasks while keeping the implementation lightweight and training unchanged."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Well written easy to follow**:The derivation is easy to follow; the GMM prior-ratio leads to implementable, closed-form guidance. Propositions are sound and proven in the Appendix.\n- **Good empirical results with clear trade-offs and ablations**: Consistent gains on posterior and predictive metrics; straightforward ablations over diffusion/Langevin steps make hyperparameter selection transparent. The paper is upfront about approximation choices and where Langevin steps matter, which helps practitioners reason about when to use the method."}, "weaknesses": {"value": "- **Prior family diversity**: Main experiments emphasize changes among Gaussian priors (often **diagonal covariance**). This is required for fair ACE comparisons but does not demonstrate  the claimed method’s generality. A demonstration on **non-factorized** or more structured priors (complicated priors) would strengthen external validity. For example one use-case that requires flexible priors would be for \"i.i.d\" data which also can be handled by sequentially updating the \"prior\" with the \"current posterior\"."}, "questions": {"value": "1. **Non-factorized complex priors**: Have you tried dense-covariance Gaussians or general complicated priors? Have you tried inference on iid data using PriorGuide?\n2. **Better Gaussian reverse Kernel approximations**: There exists better reverse kernel approximations (i.e. [1] and related). These usually are more costly i.e. require Jacobians or other terms. Would be interesting to see if these also help in that case but atleast should be discussed in the manuscript.\n3. **Matrix calculations**: The authors state that a current limitation is that \"matrix-operations  may-pose a scalability issue\". A straight forward approach to avoid this is to constrain the involved covariance to be *diagonal* (or other approx.). After all a mixture of Gaussians with diagonal covariance is still a universal approximation family (just may needs more components). As the backward kernel approximation is by design also diagonal the whole Guidance terms are reduced to the diagonal. This hence makes *PriorGuideDiag*  an interesting alternative to the current approach.\n\n[1] Boys, Benjamin, et al. \"Tweedie moment projected diffusions for inverse problems.\" _arXiv preprint arXiv:2310.06721_ (2023)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zrvNm2pUfq", "forum": "G4I23g5Ugh", "replyto": "G4I23g5Ugh", "signatures": ["ICLR.cc/2026/Conference/Submission13405/Reviewer_XCTR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13405/Reviewer_XCTR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902679091, "cdate": 1761902679091, "tmdate": 1762924038434, "mdate": 1762924038434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper consideres an interesting problem of adapting a diffusion model pretrained to sample from a posterior when a different (unknown at the training state) prior model is adopted by the user. The proposed algorithm, PriorGuide, is able to correct biase of the pretrained diffision model during the sampling stage and providing a test time adaptive inference. \n\nThe guidance is intractable but is proximated using Gaussian mixture models. \n\nThe experiments show that on a wide variety of bechmark datasets, the proposed method outperforms similar diffusion methods with no-prior adaptation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, I think this paper makes a solid contribution to the SBI community. \n\nThe problem of changing priors is indeed common in many applications, and the ability to adapt a pre-trained posterior model without retraining the entire model is highly desirable and can make a lot of sense when the user's prior is different and comes in at a later stage. \n\nDiffusion guidance is a well-studied technique in the generative model adaptation, so it is natural to leverage this approach to adapt SBI diffusion models. To my best knowledge, the proposed idea is novel. \n\nThe proposed method is mathematically sound, and its resulting formulation is natural, separating the pre-trained model from the guidance term (the expected prior shift). \n\nAlthough the guidance term itself is intractable, authors show it can be effectively approximated."}, "weaknesses": {"value": "My concerns are mostly on the literature review: \n\nIn the main text, the authors do not provide a comprehensive review or comparison of prior adaptation methods in the existing literature.  As briefly mentioned in the introduction and the experimental section, there are already methods that consider adaptive priors. Although the authors include a separate section in the appendix, a dedicated subsection in Section 2 would be appreciated, as readers would like to understand the specific limitations of these prior adaptation methods and how (or whether) they relate to the proposed guidance method.\n\nIn Section 2, recent progress (2022 ~ ) on applying diffusion models to SBI should also be comprehensively reviewed and cited as they provide the backdrop (the pre-trained posterior score) to the proposed method. To the best of my knowledge, the earliest attempt to use diffusion models in SBI is often attributed to Geffner et al. (2023), first published on arXiv in 2022. Simformer is a subsequent work built on Geffner et al. and others, introducing masks and a transformer architecture to handle missing observations and more versatile inference tasks."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Oij8ESdy43", "forum": "G4I23g5Ugh", "replyto": "G4I23g5Ugh", "signatures": ["ICLR.cc/2026/Conference/Submission13405/Reviewer_DTXg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13405/Reviewer_DTXg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988990325, "cdate": 1761988990325, "tmdate": 1762924038069, "mdate": 1762924038069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the amortized simulator-based inference framework and one of its major limitations, i.e., prior distributions used to generate model parameters during training. The authors propose a novel method, PriorGuide, which leverages a guidance approximation for flexible test-time prior adaptation. The method was evaluated on a set of benchmarks. The most significant strength is requiring no retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper has a few strengths:\n1. The \"prior-rigidity\" of amortized SBI models is a well-known and significant bottleneck for scientific application. This paper offers a direct, \"plug-and-play\" solution that enables, for the first time, post-hoc prior sensitivity analysis and the incorporation of new expert knowledge using expensive, pre-trained models.\n2. The method is built on a sound mathematical foundation. The derivation of the target posterior as a tilted distribution (Proposition 1) is clear and correct.\n3. The experiments are convincing. The paper correctly identifies that its method is an approximate sampler and intelligently introduces Langevin correctors to manage the accuracy-compute trade-off."}, "weaknesses": {"value": "However, despite it’s strengths, there are also weaknesses:\n1. The entire method hinges on a critical, and likely fragile, approximation: fitting the prior ratio $r(\\theta) = q(\\theta) / p_{train}(\\theta)$ with a GMM. The paper understates the difficulty of this step. This is a density-ratio estimation problem, which is notoriously difficult, especially as the dimension of $\\theta$ increases. The paper's solution (a gradient-based $L_2$ fit) is a heuristic that is not guaranteed to be stable or accurate, particularly if $p_{train}(\\theta)$ is non-trivial.\n2. The paper employs two major approximations: the GMM for the ratio and a simple diagonal Gaussian for the reverse kernel. It is the combination of these (especially the latter) that biases the sampler and requires the heavy use of Langevin correction steps to achieve good results. This means the method, while \"retrain-free,\" has a high and non-trivial per-sample inference cost."}, "questions": {"value": "Besides the Weaknesses, I have also a question: \n1. The GMM approximation of the prior ratio $r(\\theta)$ appears to be the most critical (and fragile) part of the pipeline. How does the $L_2$ fitting procedure scale with the dimensionality of $\\theta$? Have you considered replacing this heuristic fit with more robust, modern density-ratio estimators (e.g., flow-based, MINE, etc.)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ndjHBoLRfW", "forum": "G4I23g5Ugh", "replyto": "G4I23g5Ugh", "signatures": ["ICLR.cc/2026/Conference/Submission13405/Reviewer_HVmz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13405/Reviewer_HVmz"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762304602312, "cdate": 1762304602312, "tmdate": 1762924037709, "mdate": 1762924037709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}