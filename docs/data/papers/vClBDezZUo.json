{"id": "vClBDezZUo", "number": 10083, "cdate": 1758160085391, "mdate": 1763523440086, "content": {"title": "Reevaluating Policy Gradient Methods for Imperfect-Information Games", "abstract": "In the past decade, motivated by the putative failure of naive self-play deep reinforcement learning (DRL) in adversarial imperfect-information games, researchers have developed numerous DRL algorithms based on fictitious play (FP), double oracle (DO), and counterfactual regret minimization (CFR). In light of recent results of the magnetic mirror descent algorithm, we hypothesize that simpler generic policy gradient methods like PPO are competitive with or superior to these FP-, DO-, and CFR-based DRL approaches. To facilitate the resolution of this hypothesis, we implement and release the first broadly accessible exact exploitability computations for five large games. Using these games, we conduct the largest-ever exploitability comparison of DRL algorithms for imperfect-information games. Over 7000 training runs, we find that FP-, DO-, and CFR-based approaches fail to outperform generic policy gradient methods.", "tldr": "We show that generic deep policy gradient methods may be stronger than previously understood for imperfect-information games.", "keywords": ["imperfect-information games", "two-player zero-sum games", "reinforcement learning", "multi agent", "game theory"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30269122bdff0cc9c8e3a2ad8a70f8b8e4ab512d.pdf", "supplementary_material": "/attachment/3778c32a1fe01af2f2393f9ce9fec173f1f07bc7.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents empirical evidence that commonly used policy gradient algorithms, such as PPO and PPG, which have some similarity to MMD, quickly converge to low exploitability strategies in a range of imperfect information zero-sum games. Moreover, they show that within a restricted budget, they converge much closer to the Nash equilibrium, than selected methods based on CFR, Fictitious play, and Double Oracle algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents interesting and important empirical evidence that may encourage people to try a simpler and more standard solution to solve their problems.\n  * The paper makes a rigorous effort to compare to several baselines.\n  * The paper promises to publish a more efficient BR computation for OpenSpiel, which might be useful"}, "weaknesses": {"value": "Summary: I believe the empirical results are interesting and likely worth publishing. However, the writing of the paper is not ideal. The authors try to condense the main message of the paper to a short paragraph they call a \"hypothesis\". However, for me, a hypothesis is a formal statement that can be proven to be true or false. The \"hypothesis\" in this paper is a very vague observation that is subject to many interpretations and many of them are clearly not true in general. \n\nMoreover, there are some mistakes, likely caused by insufficient proofreading after adapting the paper to the new template, and several misleading statements. I would be happy to recommend accepting the paper after a major revision, but in the current state, I lean towards rejecting.\n\n\nDetailed comments: \n\n\"Hypothesis\" is IMO not the right word for what the paper provides. I would be fine if it is called an \"observation\" and softened by some \"tend to\" \"often\", or something similar. If it were to stay a \"hypothesis\", it would have to specify much more clearly what is \"share an ethos with magnetic mirror descent\" and list specific classes of methods \"based on FP, DO, CFR\", since it is a huge diverse set of algorithms.\n\nI also do not like the authors over-selling the contribution of implementing efficient best response, calling it \"first broadly accessible exact exploitability computation\" and not properly reviewing prior efforts to scale-up BR computation, or comparing it to the BR computation available in other frameworks. I would expect mentioning at least [A], arguing why an approximate solution, such as [B] is not sufficient, and comparing execution speed to at least OpenSpiel BR, if not also some other frameworks. Otherwise, it is hard to accept that as a contribution. \n\nThe paper IMO falsely claims that \"only a small number of works make the effort to report exploitability in large games\". A very large portion of papers on approximating equilibria in the last over a decade use exploitability as the key metric and report it on large poker endgames, Phantom TicTacToe itself, and other games of comparable size. It is certainly nice to publish a new, efficient implementation compatible with OpenSpiel and presumably much faster than the default implementation, but I do not consider similar claims to be an appropriate characterisation of the existing work.\n\nMoreover, the paper should establish that one or two orders of magnitude that their BR implementation may buy compared to the existing ones is crucial to deliver their results. Would similar experiments on slightly smaller games deliver substantially different results? It claims that it is \"a fundamentally different challenge\", but gives a very brief statement to support that and does not show or specify whether it becomes fundamentally different at 10^8, 10^9 or 10^10 states.\n\nI also consider misleading the claim that \"DO can suffer from slow convergence (...) according to the (...)  exponential lower bound for DO\", after talking about perfect recall EFGs up to that point in the paper. The boud they cite is not about this class of games. The paper explicitly states in the introduction that there are efficient variants of DO for EFGs.\n\n\nTo conclude, I would love to see this paper published, with more fair treatment of the existing work, more exact definition of \"the policy gradient observation\", and experiments on smaller variants of the games showing whether it starts to hold only form some scale of the games up, or whether it holds also for smaller games. Moreover, I would avoid implying that the methods are better than all CFR, DO, and FP based methods, and focus on using these methods as mere baselines that are outperformed.\n\n\n[A] Johanson, Michael, et al. \"Accelerating best response calculation in large extensive games.\" IJCAI. Vol. 11. 2011.\n\n[B] Timbers, Finbarr, et al. \"Approximate Exploitability: Learning a Best Response.\""}, "questions": {"value": "Q1)  Do PPO/PPG match the performance of MMD even on smaller games?\n\nQ2) Do you know of specific counterexamples where PPO/PPG fail to converge? \n\nQ3) Is the plot in Figure 2 \"the hyperparameter tuning launch\" or the \"evaluation launch\"?\n\nQ4) Do I understand correctly that you run PSRO with at most 8000 steps per oracle training?\n\nQ5) Can you provide a compute time comparison between your BR implementations and the OpenSpiel one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SJi7IWbM36", "forum": "vClBDezZUo", "replyto": "vClBDezZUo", "signatures": ["ICLR.cc/2026/Conference/Submission10083/Reviewer_WVeg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10083/Reviewer_WVeg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730343475, "cdate": 1761730343475, "tmdate": 1762921471196, "mdate": 1762921471196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper puts forward a hypothesis that policy gradient (PG) methods based on (divergence-regularized) self-play can outperform reinforcement learning (RL) implementations of fictitious play (FP), double oracle (DO), or counterfactual regret minimization (CFR) in solving two-player zero-sum imperfect-information games. The authors empirically verify this hypothesis through extensive experimental analysis under benchmark games. Under exact exploitability computations or head-to-head evaluations, PG methods like magnetic mirror descent (MMD) consistently outperform FP/DO/CFR-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper makes significant efforts in aligning the existing game RL algorithms and fairly evaluating them under the strict exploitability measure.\n\n2. The elaborations on the experimental settings and choice of benchmarks make the conclusion convincing."}, "weaknesses": {"value": "1. My major concern about this paper is that the proposed hypothesis is not novel. To me, the hypothesis is somewhat equivalent to saying last-iterate convergence is superior to average-iterate convergence under current game RL implementions. I think it has been common sense since recent game-theoretic research has a significant focus on last-iterate convergence (e.g., [1,2]), which usually implies a linear convergence rate. In some small games, the last iterates can converge exponentially faster than the average iterates, even under the same learning dynamics (e.g., optimistic FTRL). The superiority of MMD in larger games is therefore predictable.\n\n2. The benchmarks do not include real-world games like Battleship and No-Limit Texas Holdâ€™em. To me, these games are more interesting because they could have more complicated transitivity structures in the policy space. Performance in these real-world games could be different from in the benchmark games considered in this paper. For example, it remains unclear whether MMD can also solve Stratego as R-NaD.\n\n[1] Kenshi Abe, et al. Adaptively perturbed mirror descent for learning in games. ICML, 2024.\n\n[2] Runyu Lu, et al. Divergence-regularized discounted aggregation: Equilibrium finding in multiplayer partially observable stochastic games. ICLR, 2025."}, "questions": {"value": "Could you make a more detailed comparison between the implementations of MMD (based on mirror descent) and R-NaD (based on FTRL) to explain their performance discrepancy? Since gradient descent, multiplicative weights, Hedge, and FTRL have inherent relationships, MMD and R-NaD could be similarly viewed as a kind of regularized FTRL from the game-theoretic perspective."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mTmuwQQBOP", "forum": "vClBDezZUo", "replyto": "vClBDezZUo", "signatures": ["ICLR.cc/2026/Conference/Submission10083/Reviewer_rrhL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10083/Reviewer_rrhL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923845443, "cdate": 1761923845443, "tmdate": 1762921470843, "mdate": 1762921470843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper revisits a common claim that policy gradient methods in self-play perform poorly in IIGs.  The paper does a thorough comparison of a set of 6 DRL methods for IIG (3 representative of adaptations of tabular methods with theoretical guarantees in tabular settings, and 3 policy gradient methods)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is important work!  It's very important to be sure the community is not trapped by oft-repeated claims that are just known, but not truly validated.  The goals of this paper are important.  But, what is most impressive about the paper is the carefulness in the transparency of the comparison, recognition of their own biases, aim for reproducibility, and the limitations of what the empirical results justify.  This is commendable.\n\nThe following line is one of my favourites a paper focused on empirical results: \"However, since we have hypothesized these algorithms will underperform, our role in tuning them should be carefully scrutinized. To aid readers in this endeavor, we provide detailed documentation of our algorithm implementations, hyperparameter tuning procedure, and hyperparameter tuning results, in addition to final results.\"\n\nThe section on limitations at the end is also valuable.  More papers should be quick to say what they aren't saying the way Section 6 does in the third sentence."}, "weaknesses": {"value": "I think the biggest weakness of the paper is the paper comes at this work with a very strong bias and preconceived expectation to the result.  I am glad the authors recognize this bias, but I think that bias gives an edge ot the writing that is not needed in the text.  I believe the authors gave all algorithms a fair shake.  More importantly they are transparent about the shake they gave them.  But you don't get that picture through the first half of the paper.  I found myself immediately bristling at the line, \"(and, not least, the disinclination of researchers toward showing baselines outperforming their algorithmic contributions)\", thinking that the authors were just going to be doing the same thing but the other direction.  In the end I was pleasantly surprised by the care taken.  But, I would highly recommend the authors think about reframing the paper to take a more neutral objective tone (while still admitting their own hypothesis).  I would see value in this paper even if the results had gone the other way, and confirmed oft-repeated notions that have not been rigorously tested.\n\nMy only quibble with the experimental design was the forced choice of Adam to be used for all algorithms.  Do you have a clear table as to which of these 6 algorithms use Adam as the default optimizer?  That might be helpful to rule out that as a source of difference.\n\nI'll use the rest of this section to make some suggestions about how the paper could possibly be further improved.\n\nThere's an awful lot of repetition in the statements and ideas.  I think the paper probably could be a page shorter and have the same impact.  Which would then open up space for some valuable additions.\n\nI think it would be worth discussing what soundness results exist in this space.  I find the biased edge in writing seems to be dismissing trying to extend tabular methods with soundness guarantees into the world of parameterized approximations, which seems like an odd choice.  Wouldn't we want some theory to explain what works in practice?  I think it would be valuable to discuss what soundness guarantees we do have for any of the explored methods, both in tabular settings and for the particular application to either games (for PG methods) or function approximation (for other methods).  This would add considerably to clarifying the current state of the field.  The paper argues that policy gradient methods get \"dismissed as unsound or relegated to the role of sacrificial baseline\", but this paper doesn't offer anything to suggest that they are sound, right?  Now I suspect that the extensions of FP, DO, and CFR don't have guarantees outside tabular, but this clarity would be helpful.\n\nI would also like to see more discussion about why you think the poor performance of policy gradient methods in IIGs was so prominent given your empirical results.  You pose early in the paper that maybe it has to do with the hyperparameter space, where MMD used more entropy regularization than typically used.  Well, was that the reason?  Can you conclude anything about the reason from your experiments?  Even if it's just saying, we didn't find the entropy regularlization parameter to be a significant factor in policy gradient methods performance, would add something.\n\nIt would be nice to see some discussion of whether the methods actually achieve low absolute exploitability.  Are any of these small enough to run a tabular method on?  Or could you add one game that is?  It would be nice to know if PG methods are actually doing a good job of finding low exploitability strategies or if the non-PG methods are doing a particular poor job, despite their tabular counterparts being very effective.  Again, it seems there is a missed opportunity to clarify what gaps still exist in the field.  Similarly, could you show trends of exploitability as the network size increases across methods?  Are of these methods able to see scaling with compute and make use of additional capacity to reduce exploitability?\n\nI think another valuable addition would be a brief discussion about DeepStack, Libratus, and/or Student of Games.  These are methods that absolutely scale to large games, with NLHE being many orders of magnitude bigger than the games posed in the benchmarks, while also having some theory (with expected limitations due to approximation quality of network architectures).  They are also not model-free and add decision-time search as a key component, so they are outside the scope of the paper's claims.  But it still seems useful to discuss in order to give a more comprehensive picture.\n\nI think that's the summary of my suggestions: the authors do a very good job of giving evidence for their stated main claim, but there's an opportunity to be much more... to give a comprehensive picture of where we as a field are at in algorithms for learning in IIGs.  This is a good paper, but that would be a great one."}, "questions": {"value": "No questions; see weaknesses for suggestions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZDcGzlNmBK", "forum": "vClBDezZUo", "replyto": "vClBDezZUo", "signatures": ["ICLR.cc/2026/Conference/Submission10083/Reviewer_op4U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10083/Reviewer_op4U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975303324, "cdate": 1761975303324, "tmdate": 1762921470378, "mdate": 1762921470378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}