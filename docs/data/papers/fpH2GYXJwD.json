{"id": "fpH2GYXJwD", "number": 6619, "cdate": 1757990714148, "mdate": 1759897904671, "content": {"title": "Exploring Mode Connectivity in Krylov Subspace for Domain Generalization", "abstract": "This paper explores the geometric characteristics of loss landscapes to enhance domain generalization (DG) in deep neural networks. \nExisting methods mainly leverage the local flatness around minima for improved generalization. However, recent theoretical studies indicate that flatness does not universally guarantee better generalization. Instead, this paper investigates a global geometrical property for domain generalization, i.e., mode connectivity, the phenomenon where distinct local minima are connected by continuous low-loss pathways. Different from flatness, mode connectivity enables transitions from poor to superior generalization models without leaving low-loss regions. \nTo navigate these connected pathways effectively, this paper proposes a novel Billiard Optimization Algorithm (BOA), which discovers superior models by mimicking billiard dynamics. \nDuring this process, BOA operates within a low-dimensional Krylov subspace, aiming to alleviate the curse of dimensionality caused by the high-dimensional parameter space of deep models. \nFurthermore, this paper reveals that oracle test gradients strongly align with the Krylov subspace constructed from training gradients across diverse datasets and architectures. \nThis alignment offers a powerful tool to bridge training and test domains, enabling the efficient discovery of superior models with limited training domains.\nExperiments on DomainBed demonstrate that BOA consistently outperforms existing sharpness-aware and DG methods across diverse datasets and architectures.\nImpressively, BOA even surpasses the sharpness-aware minimization by 3.6\\% on VLCS when using a ViT-B/16 backbone.", "tldr": "", "keywords": ["Loss landscape", "mode connectivity", "Krylov space", "domain generalization"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f2c984b8fe1f11b80ba554961ccd6941a2c8408.pdf", "supplementary_material": "/attachment/611931f5e4b451d47bbd331d5315cea57c6cce50.pdf"}, "replies": [{"content": {"summary": {"value": "To find flat minima that perform better on the test data under distribution shift, the paper considers mode connectivity. It proposes a Billiard Optimization Algorithm (BOA) that traverses the flat basin of the loss landscape analogous to the reflection of a billiard on the board. To find the search direction effectively, the paper leverages the observation that test loss gradients align with the Krylov subspace. Experiments on five datasets show the potential of BOA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It is meaningful to use mode connectivity to find better flat minima in domain generalization. The observation that test loss gradients align with the Krylov subspace is interesting and inspiring. \n\n2. BOA proposes a novel strategy to search over the loss landscape. \n\n3. Figure 6 is very helpful in understanding how BOA works."}, "weaknesses": {"value": "1. Figure 1 suggests that minima that are equally flat on the loss landscape of the training distribution can have different sharpness on the loss landscape of test data. The observation motivates finding better minima through mode connectivity. BOA relies on a validation set of the test data to find an optimal model. However, in domain generalization, it is common to assume that test data is not accessible. My biggest concern is that under this condition, how to guarantee that the minima found via BOA are better? \n\n2. The clarity of section 3 should be improved to aid understanding. For example, some notations in section 3 are not explained in the main paper (e.g. $\\alpha, \\epsilon$), and the role of $h$ and its connection with $\\alpha$ are not explicitly stated."}, "questions": {"value": "1. How is $\\alpha^*$ in equation 4 determined?\n\n2. BOA's reflection results in symmetric exploration of the loss landscape. The geometry of the loss landscape and initial search points both affect the efficiency of the symmetric search. How robust is the symmetric exploration in searching flat minima when the flat basin is not symmetric, and the initial search point lies at the center of the flat basin?\n\n3. Does the search angle $\\phi$ affect the search of optimal model? Why is it set to a fixed value instead of as a hyperparameter?\n\n4. How is $\\epsilon$ in equation 8 determined? \n\n5. Is it a fair comparison with other DG methods, given that many of them do not utilize a validation set of test data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fR4JHUrQPa", "forum": "fpH2GYXJwD", "replyto": "fpH2GYXJwD", "signatures": ["ICLR.cc/2026/Conference/Submission6619/Reviewer_Gedm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6619/Reviewer_Gedm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890730399, "cdate": 1761890730399, "tmdate": 1762918938530, "mdate": 1762918938530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new optimization framework, termed \"Billiard Optimization Algorithm\" (BOA), to improve domain generalization using VPT with ViT backbones. This algorithm leverages the \"mode connectivity\" properties of the loss landscape, instead of relying only on flatness (as done by methods like SAM).\n\nBOA consists of a line search part, where boundaries of the loss contour are reached, and a reflecion part, where the new search direction is selected using a physics-inspired rule based on the local gradients. Moreover, BOA uses the Krylov subspace to select the initial direction and constrain the search trajectory. This choice is motivated by the observation that the test gradients seem to be aligned with the Krylov subspace generated from the training gradients.\n\nEmpirical evaluation using the DomainBed benchmark with VLCS, PACS, OfficeHome, TerraIncognita and DomainNet shows that the proposed BOA method consistently outperforms the evaluated DG baselines. The paper also provides interesting theoretical analyses showing the benefits of approximating gradients using Krylov subspaces compared to random subspaces."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper presents an interesting and novel optimization approach towards domain generalization introducing several new (for DG) concepts, like mode connectivity, the use of a new search approach and the use of Krylov subspaces\n - The paper also offers theoretical and empirical insights on the properties of the Krylov subspace, especially regarding its alignment with the test gradients.\n - The results presented consistently outperform the compared DomainBed baselines and sharpness-aware methods\n - Although no detailed (theoretical or empirical) analysis of the computational complexity of the proposed method is provided, the method appears to be efficient since it avoids the computation of the Hessian."}, "weaknesses": {"value": "- The reason / underlying mechanisms behind the observed improvements remain unclear. The paper does not convincingly explain why mode connectivity or Krylov alignment lead to better cross-domain generalization.\n - The experiments are constrained to VPT with ViTs. Although this choice may indeed lead to the best results, the paper would be much more convincing if similar findings were observed e.g., for ResNet backbones and/or with full fine-tuning. Even if performance is degraded, demonstrating consistent improvements over comparable baselines would better support the stated claims. In addition, given the increased dimensionality of the parameter space, the proposed method should benefit even more compared to the baselines in this case."}, "questions": {"value": "- How sensitive is the method to the hyperparameters (e.g., K, reflection count)?\n- What is the computational cost / runtime compared to SAM or GSAM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uut1ZCOuP7", "forum": "fpH2GYXJwD", "replyto": "fpH2GYXJwD", "signatures": ["ICLR.cc/2026/Conference/Submission6619/Reviewer_LAoM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6619/Reviewer_LAoM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933067152, "cdate": 1761933067152, "tmdate": 1762918938016, "mdate": 1762918938016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BOA (Billiard Optimization Algorithm) for domain generalization. The idea is to stay inside a low-loss region of the training loss, move to the loss boundary with a line search, then reflect and keep going. To avoid getting lost in high dimensions, the method searches only inside a Krylov subspace built from training gradients/HVPs. The authors also show that test gradients tend to align with this subspace, and that a train-computed path often also looks good on the test landscape. On DomainBed, BOA beats ERM/SAM and several DG methods; for example, on VLCS with a ViT-B/16 model, BOA improved accuracy by 3.6 percentage points compared to SAM."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Clear formulation and intuition. BOAâ€™s use of a training-loss sublevel set, a concrete line-search to the boundary, and a reflection update is straightforward and well motivated.\n\n- Low-dimensional search. Constraining the trajectory to a Krylov subspace is a sensible way to capture salient curvature directions without exploring the full parameter space.\n\n- Useful visual evidence. Overlaying the same trajectory on train and test landscapes helps illustrate why a train-computed path can still navigate good regions on test.\n\n- Strong empirical results. Consistent improvements on DomainBed with ViT backbones (including VPT) and a notable margin over SAM on VLCS."}, "weaknesses": {"value": "- Limited backbones: Experiments are mostly on ViT. Results on CNNs (e.g., ResNet) or other architectures would strengthen generality.\n\n- Unclear compute cost: Please compare total elapsed (wall-clock) time, memory, and counts of line-search/HVP calls to ERM/SAM under the same settings.\n\n- Heuristic initial direction: The current choice is simple; more analysis or alternatives would improve justification."}, "questions": {"value": "Is it acceptable not to include CNN backbones, or can you provide BOA vs ERM/SAM results on a standard CNN such as ResNet-50 under the same budget with brief notes on hyperparameter transferability and compute cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aKj5QjhEHl", "forum": "fpH2GYXJwD", "replyto": "fpH2GYXJwD", "signatures": ["ICLR.cc/2026/Conference/Submission6619/Reviewer_mWsD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6619/Reviewer_mWsD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994551552, "cdate": 1761994551552, "tmdate": 1762918937537, "mdate": 1762918937537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}