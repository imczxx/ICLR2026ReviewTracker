{"id": "kPrvXjUZPG", "number": 1681, "cdate": 1756904835358, "mdate": 1759898195057, "content": {"title": "Heterogeneous Agent Q-weighted Policy Optimization", "abstract": "Multi-agent reinforcement learning (MARL) confronts a fundamental tension between stability and expressiveness. Stability requires avoiding divergence under non-stationary updates, while expressiveness demands capturing multimodal strategies for heterogeneous coordination. Existing methods sacrifice one for the other: value-decomposition and trust-region approaches ensure stability but assume restrictive unimodal policies, while expressive generative models lack optimization guarantees. To address this challenge, we introduce **H**eterogeneous **A**gent **Q**-weighted Policy **O**ptimization (**HAQO**), a framework unifying sequential advantage-aware updates, Q-weighted variational surrogates, and entropy regularization. Our analysis establishes monotone improvement guarantees under bounded critic bias, extending trust-region theory to diffusion-based policies with intractable log-likelihoods. HAQO achieves superior returns and reduced variance compared to policy-gradient baselines across diverse benchmarks. The ablation studies confirm sequential updates ensure stability, expressive policies enable multimodality, and entropy regularization prevents collapse. HAQO reconciles stability and expressiveness in MARL with theoretical rigor and practical effectiveness.", "tldr": "", "keywords": ["Multi-agent Reinforcement Learning", "Heterogeneous Agent Reinforcement Learning", "Diffusion Policy", "Policy Optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67d257d27cbc02b7e1d7dc15a0104e25c35170d6.pdf", "supplementary_material": "/attachment/98b6bfd49d54f9d5d742d68c479127564c2e82c7.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces **HAQO (Heterogeneous-Agent Q-weighted Optimization)**, a multi-agent RL framework for heterogeneous teams (agents with different observations, actions, and roles). The goal is to get both (i) stable training and (ii) expressive, multimodal policies in cooperative tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The synthesis of heterogeneous sequential trust-region updates with a Q-weighted diffusion surrogate is well-motivated and technically nontrivial, enabling expressive policies while retaining improvement structure.  \n2. A per-agent sequential bound (Prop. 5.1) and a global Theorem 5.4 establish monotone improvement with explicit dependence on KL radii and critic bias ($\\epsilon_Q$).  \n3. Results cover discrete/continuous and partially observable regimes, with tables for SMAC/SMACv2, GRF, and Multi-MuJoCo; comparisons use consistent hyperparameters and three seeds.  \n4. Algorithm 1 operationalizes the full pipeline (critic updates, permutation, QV/entropy/drift) and links theory to practice."}, "weaknesses": {"value": "1. Guarantees hinge on KL radii ($\\delta_i$) and critic bias ($\\epsilon_Q$); the paper lacks practical diagnostics/tuning guidance for monitoring these quantities during training.  \n2. Diffusion actors and sequential updates plausibly increase wall-clock and compute; the paper itself acknowledges added cost but does not report GPU-hours or env-steps-to-target. \n3. On HalfCheetah-v2 (2×3), HAQO trails HAPPO (6873 vs. 7024), suggesting sensitivity or robustness issues that merit analysis."}, "questions": {"value": "1. Can the authors estimate or upper-bound ($\\epsilon_Q$) online (e.g., via TD-error calibration or ensemble dispersion) to quantify the slack in the QV lower bound during training? \n2. How are per-agent ($\\delta_i$) chosen across heterogeneous roles, and how sensitive are results to these settings? A small sweep/heuristic would help practitioners. \n3. When do you prefer each weighting? Please characterize the stability/variance trade-offs and default choices across benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "89bBalW7zb", "forum": "kPrvXjUZPG", "replyto": "kPrvXjUZPG", "signatures": ["ICLR.cc/2026/Conference/Submission1681/Reviewer_S3C5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1681/Reviewer_S3C5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835458597, "cdate": 1761835458597, "tmdate": 1762915855239, "mdate": 1762915855239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to resolve the conflict between learning stability and policy expressiveness in heterogeneous MARL. It combines HATRPO's sequential policy update mechanism and QVPO's weighted variational objective function policy expression as a lower bound for policy improvement, providing a monotonically increasing guarantee and analysis of the team reward optimization objective for online MARL learning with diffusion-style policy expressions. To prevent mode collapse even for policies with strong expressiveness, the authors designed an entropy surrogate objective. By injecting actions sampled from a uniform distribution into the diffusion model, they ensure policy distribution diversity, encouraging exploration and providing a corresponding lower bound.\n\nExperimentally, the authors validated the algorithm in several challenging MARL environments. Results show that HAQO significantly improves performance and stability compared to state-of-the-art baseline algorithms such as MAPPO and HAPPO. Extensive ablation experiments also validated the necessity of the three core components mentioned above."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is the first to combine a diffusion model strategy with a theoretically guaranteed heterogeneous MARL framework, and provides a complete theoretical analysis (monotone improvement guarantee) for this combination—a solid contribution. The paper provides a complete derivation chain, including the performance difference, the sequential update bound, and the monotone improvement guarantee. This provides a strong theoretical foundation for the effectiveness of the method.\n\nWhen a diffusion model is used as an actor, it is often impossible to obtain a precise expression of the policy distribution, limiting the use of entropy regularization. The proposed method, which reconstructs a surrogate objective of uniformly sampled actions, is clear in its approach and simple to implement. Theoretically, it is demonstrated that this method can guarantee the \"spectral floor\" of the action distribution covariance, thus effectively preventing mode collapse.\n\nThe experimental setup in this paper is comprehensive, covering various scenarios such as continuous control, discrete actions, and high-dimensional policies. Comparisons with mainstream and stable baselines in MAPPO, HAPPO, and other fields make the experimental results more convincing. Ablation experiments clearly demonstrate the indispensable roles of the three components: serialization update, expressive strategy, and entropy regularization."}, "weaknesses": {"value": "W1: Compared to Gaussian strategies, the sampling and training of diffusion models are extremely time-consuming, and combining this with sequence update processes significantly increases computational costs. The paper mentions this limitation but does not provide any quantitative comparisons of training time or computational resources in the experimental section. Adding a wall-clock time comparison for MAPPO/HAPPO would be more helpful in evaluating its feasibility in practical applications.\n\nW2: The experimental details disclosed in this paper are limited. The framework involves several key hyperparameters and network structures, such as the entropy regularization coefficient, the number of steps in the diffusion process, the chosen base network architecture of the diffusion model, and a performance comparison between qadv and qcut. The paper does not discuss the sensitivity of these hyperparameters or how to tune them, which may pose difficulties for reproduction and application.\n\nMinor Problems: In line 266, there is a repetition in the paragraph."}, "questions": {"value": "Q1: Algorithm 1 in the appendix mentions using ppo-clip for trust domain policy constraints. However, with non-Gaussian policies, the likelihood function of $\\pi_i$ is difficult to calculate. How can this ratio be calculated for a diffusion model in practice?\n\nOther Problems mentioned in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qWHpBsikXO", "forum": "kPrvXjUZPG", "replyto": "kPrvXjUZPG", "signatures": ["ICLR.cc/2026/Conference/Submission1681/Reviewer_7FA2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1681/Reviewer_7FA2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936554023, "cdate": 1761936554023, "tmdate": 1762915855045, "mdate": 1762915855045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new multi-agent reinforcement learning (MARL) algorithm for heterogeneous cooperative MARL settings, called HAQO (Heterogeneous-Agent Q-weighted Policy Optimization). The algorithm aims to reconcile stability (monotonic improvement under non-stationarity) with expressiveness (multimodal policies needed for heterogeneous coordination). It combines:\n1. Sequential, advantage-aware per-agent updates within CTDE to avoid simultaneous-update instabilities;\n2. A Q-weighted variational surrogate that lets diffusion-based actors (with intractable log-likelihoods) optimize for return; and\n3. An entropy surrogate that enforces exploration/anti-collapse without requiring log-densities.\n\nIt also provides bounds showing monotone joint-return improvement under bounded critic bias and trust-region drift, and an algorithm instantiation using KL/PPO-style constraints. Empirically, on Multi-Agent MuJoCo and other suites, the proposed HAQO generally matches or outperforms MAPPO/HAPPO/HATRPO baselines (from both value-based and policy-based paradigms), and ablations attribute gains to (i) sequential updates, (ii) expressive (diffusion) policies, and (iii) the entropy surrogate."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is generally well-organized and provides insight into how heterogeneous agents and diffusion-based policies interact in cooperative MARL.\n- The theoretical derivations are carefully laid out and formally sound given the stated assumptions.\n- The experimental evaluation covers a wide set of benchmarks and includes ablations that test the contribution of each design component."}, "weaknesses": {"value": "### Motivations\n\n- The central motivation—that MARL methods face a tension between stability and expressiveness—is not well substantiated. The paper provides no prior evidence or citations that improving one necessarily degrades the other. This makes the claimed tradeoff appear somewhat self-imposed; ideally, both properties are desirable.\n\n- The problem formulation for the cooperative setting is unclear. In a purely cooperative objective (shared reward), optimization resembles a joint minimization $min_{x_1, \\dots, x_a} \\ f(x_1, \\dots, x_a)$. Without coupling terms between agents, sequential vs. simultaneous updates should be equivalent. If the authors mean practical instability from non-stationary policies, this distinction should be stated more formally.\n\n- The introduction conflates non-stationarity (inherent to MARL) with the effect of simultaneous updates (lines 80–82). Sequential updates may alleviate, but not eliminate, non-stationarity—particularly with replay buffers.\n\n\n### Non-self-sustained Background\n\n- The background for diffusion policies and heterogeneous-agent settings is insufficient for a general ML audience. The paper does not clearly specify whether agents share a global objective or if heterogeneity also includes distinct observation or action spaces. For example, it is not very clear if the objective in heterogeneous settings is shared between agents (as in a potential game setup) or how exactly the problem is defined\n\n\n- Simple toy examples, similar to those in Zhong et al. (JMLR 2024), would clarify how simultaneous updates lead to instability and why sequential updates are beneficial. I had to check that paper first for understanding.\n\n\n- Some terms introduced early (e.g., “unimodal policies”) are not defined, making the abstract and introduction harder to follow for readers unfamiliar with this subarea.\n\n\n\n### Computational aspects\n\n- As authors discuss, using diffusion policies can be computationally expensive. As shown in ablations, this can make a large improvement, but it is not always the case (for example, in Multi-MuJoCo table, the improvements are not that significant)\n\n\n### Novelty and theoretical contribution\n\n-  Algorithmic novelty is limited. Each component—sequential trust-region updates (HAPPO/HATRPO), Q-weighted diffusion objectives, and entropy surrogates—exists in prior work. The main contribution is their combination and an accompanying monotonic improvement analysis.\n\n\n- The cooperative extension introduces moderate technical difficulty, mainly because diffusion actors lack explicit log-likelihoods. The proposed Q-weighted variational surrogate addresses this but relies on assumptions similar to prior single-agent Q-weighted diffusion methods and HATRPO-style drift bounds.\n\n\n- The cooperative setting adds little new theoretical challenge beyond bookkeeping of per-agent drift. The paper would benefit from more explicit discussion of how inter-agent coupling or heterogeneity complicates the guarantees.\n\n### Additional feedback and minor typos\n\nAbstract:\n- The term “unimodal policies” in the abstract is not defined; readers unfamiliar with diffusion or Gaussian policy classes may not immediately know this refers to simple parametric (e.g., Gaussian) action distributions lacking multimodality.\n- The phrase “monotone improvement guarantees” should specify what is guaranteed to improve (e.g., expected joint return / performance objective under the centralized critic). Clarifying the exact measure of improvement would help.\n- The initial paragraph states there is a “tension between stability and expressiveness.” This is somewhat self-imposed; in principle, we desire both. The text could better motivate why these two properties conflict in practice (e.g., why expressive multimodal policies break standard monotonicity proofs).\n- Minor stylistic issue: the abstract’s first few sentences read slightly overloaded, introducing both the “tension” and the contribution before defining key terms; consider re-ordering or simplifying for readability.\n\nOther:\n- The introduction begins after a figure, unusual and slightly disruptive.\n- Multiple claims lack citations; for instance, there are a lot of claims in lines 55-61, and references must be given for each point\n- Several figures appear redundant. (e.g., Fig. 1 could omit the first columns; Fig. 2 repeats similar content).\n- In Table 1, isn't HAQO also the best performer in Walker 6x1?\n- Line 267: transformation\n\n\n—--\n\nReferences:\n[1] Yifan Zhong, Jakub Grudzien Kuba, Xidong Feng, Siyi Hu, Jiaming Ji, et al. Heterogeneous-agent reinforcement learning. In Journal of Machine Learning Research (JMLR), 2024"}, "questions": {"value": "- What is the additional computational overhead (training time, memory) of HAQO relative to HATRPO or MAPPO? Can you add results for it?\n- The QV surrogate objective in equation (5) is conditioned on the already updated policies of other agents, but this is not possible in practice for all agents, does it affect the method's performance, or how does violating that affect the theoretical guarantees presented?\n- In 6.1, it was mentioned that all baselines were run with consistent hyperparameters. Does this mean the same values were used for all methods or the values recommended for each method?\n- In the sequential update ablation, can you mention more details on that? Are both methods HAQO, and if so, was it just replacing new policies with old policies for all agent updates?\n- When computing the KL divergence, is it enough for stability to just take it individually for each agent? Shouldn’t it matter how the new joint policy is different compared to the old joint policy? In other words, does ensuring that individual policies don’t move too far ensure that the joint policy also stays in the trust region?\n- In line 229, where is the a-i used inside the expectation?\n- In Fig. 1, were all methods initialized identically? If not, how should differences in the first column be interpreted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eVDEpFO7eF", "forum": "kPrvXjUZPG", "replyto": "kPrvXjUZPG", "signatures": ["ICLR.cc/2026/Conference/Submission1681/Reviewer_V9WW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1681/Reviewer_V9WW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031746782, "cdate": 1762031746782, "tmdate": 1762915854919, "mdate": 1762915854919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Heterogeneous-Agent Q-weighted Policy Optimization framework to address the imbalance between stability and expressiveness. HAQO integrates three key components: Sequential Advantage-Aware Updates, Q-weighted Variational Surrogates, and Entropy Regularization. Through systematic theoretical analysis, HAQO extends the Trust-Region Policy Optimization (TRPO) framework to diffusion-based policies, ensuring monotonic improvement even when the log-likelihood is intractable. Empirically, HAQO achieves higher returns and lower variance across multiple heterogeneous multi-agent benchmarks, demonstrating its strong balance between stability and expressiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides rigorous proofs for monotonic improvement in heterogeneous-agent settings, generalizing TRPO theory to diffusion-based policies. It also introduces the Q-weighted variational lower bound and the monotonic improvement theorem.\n    \n    - The introduction of a Q-weighted variational bound bridges generative modeling and reinforcement learning by effectively connecting diffusion models with the RL framework.\n    \n    - The experiments cover five major multi-agent reinforcement learning benchmarks with heterogeneous settings. The results demonstrate higher mean returns and lower variance across multiple tasks, validating the effectiveness of the proposed HAQO framework."}, "weaknesses": {"value": "- Diffusion-based actors are computationally heavy. The paper does not discuss runtime, convergence speed, or scalability.\n\n   -  All theoretical guarantees rely on the assumptions of bounded critic bias and small trust-region radii, yet the paper does not verify whether the neural critics used in experimental results section satisfy these conditions.\n\n   -  The baselines in Table 1 focus on traditional policy-gradient methods (MAPPO, HAPPO), but omit recent diffusion-based RL or normalizing flow MARL approaches.\n\n   -  Although sequential updates are emphasized, there is no quantitative study showing the effect of agent update order or critic noise magnitude on convergence behavior. Only a single comparative curve of “Sequential vs. Simultaneous Update” is presented in the Figure 3."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vQcar4Tv16", "forum": "kPrvXjUZPG", "replyto": "kPrvXjUZPG", "signatures": ["ICLR.cc/2026/Conference/Submission1681/Reviewer_ADDX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1681/Reviewer_ADDX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1681/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763300630627, "cdate": 1763300630627, "tmdate": 1763300630627, "mdate": 1763300630627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}