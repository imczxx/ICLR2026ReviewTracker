{"id": "ZgPVXrhjNt", "number": 3269, "cdate": 1757389936819, "mdate": 1762962099616, "content": {"title": "Learning Fast and Slow: Addressing Task-Imbalanced Continual Learning with Dual-Speed Adaptation", "abstract": "Continual learning aims to acquire new knowledge without forgetting previously learned tasks. \nHowever, most existing studies assume balanced tasks, which is rarely the practice case, as real-world scenarios often exhibit severe task imbalance with long-tailed distributions. This task-imbalanced continual learning (TICL) setting entangles two fundamental challenges: the well-known \\textit{stability–plasticity dilemma}, and the newly emerging \\textit{head–tail learning dilemma}, where head classes dominate training while tail classes remain under-optimized. To address this compounded difficulty, we propose Decoupled Fast–Slow Adaptation (DFSA), which introduces two key components. First, a fast–slow dual adapter augments the image encoder with a fast-adapting branch for rapid task acquisition and a slow-consolidating branch for stable knowledge retention. A task-modulated weighting mechanism dynamically integrates these branches, effectively fusing “fast” and “slow” thinking to balance short-term plasticity with long-term stability, while simultaneously providing complementary perspectives that enhance learning for underrepresented classes. Complementarily, DFSA employs a decoupled training strategy by first fine-tuning the text encoder as a semantic-aware classifier before updating image features, providing stable guidance that mitigates the negative impact of long-tailed distributions. Extensive experiments on TICL benchmarks show that our method significantly improves both few-sample task generalization and overall retention, outperforming existing continual learning baselines. The source code is temporarily available at https://anonymous.4open.science/r/DFSA-3aD6E.", "tldr": "", "keywords": ["Continual learning，Class-incremental learning，Task-Imbalance"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/db9ec64f6bc256d44a3dcd490b537444c90cd519.pdf", "supplementary_material": "/attachment/f327cd226400d3b42696280c338048e05fa6f8bd.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses Task-Imbalanced Continual Learning (TICL), a challenging setting combining the stability-plasticity dilemma of Continual Learning (CL) with the head-tail learning dilemma from long-tailed task distributions. It proposes Decoupled Fast-Slow Adaptation (DFSA), which integrates two core components: 1) A Fast-Slow Adapter (FSA) added to the image encoder, featuring parallel fast-adapting and slow-consolidating branches, dynamically fused via task-modulated weighting to balance plasticity and stability. 2) A Decoupled Training Strategy that first fine-tunes the faster-converging text encoder of a Vision-Language Model (VLM) to establish a stable semantic anchor, before subsequently adapting the image encoder features to align with it. DFSA demonstrates strong empirical performance, particularly excelling in the difficult Ascending TICL scenario."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[S1] Tackles the realistic and difficult TICL problem, concurrently addressing forgetting and long-tail bias.\n\n[S2] Consistently outperforms state-of-the-art methods across TICL benchmarks, especially showing significant gains in the challenging Ascending task order, indicating effectiveness against both dilemmas.\n\n[S3] Clearly validates the contribution of both the FSA and the specific \"Text-Image\" decoupled training order."}, "weaknesses": {"value": "[W1] While FSA intuitively addresses stability-plasticity, its specific mechanism for alleviating the head-tail dilemma (enhancing tail class representation) lacks detailed explanation or analysis.\n\n[W2] Sensitivity analysis for the fast ($\\alpha$) and slow ($\\beta$) adaptation rates is absent. Additionally, including an analysis of the trajectory of $\\gamma across tasks would considerably enhance the interpretability and empirical soundness of the proposed dual-speed adaptation.\n\n[W3] The weighting factor $\\gamma$ appears computed based on batch features, not explicit task identity, making the term \"task-modulated\" potentially misleading.\n\n[W4] Acknowledged data overlap between the backbone's pretraining (ImageNet-21K) and two benchmark datasets (CIFAR100, TinyImageNet) somewhat weakens the reported results on those specific datasets."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G9I8dxeM6H", "forum": "ZgPVXrhjNt", "replyto": "ZgPVXrhjNt", "signatures": ["ICLR.cc/2026/Conference/Submission3269/Reviewer_6Wi6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3269/Reviewer_6Wi6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700286623, "cdate": 1761700286623, "tmdate": 1762916639335, "mdate": 1762916639335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "snJKfkrh2e", "forum": "ZgPVXrhjNt", "replyto": "ZgPVXrhjNt", "signatures": ["ICLR.cc/2026/Conference/Submission3269/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3269/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762962098787, "cdate": 1762962098787, "tmdate": 1762962098787, "mdate": 1762962098787, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two techniques to improve the performance of TICL (task-imbalanced continual learning), in which data follows a long-tailed distribution across different tasks. TICL is originally proposed by Hong et al., (2024), and this work attempts to boost the performance over existing CL methods when applying to TICL. One component of this work is DFSA (decoupled fast-slow adaptation) that augments the image encoder with dual branches with different learning speeds, and the other is another decoupled learning strategy that fine-tunes the text encoder of a VLM before updating the image encoder. Using TICL variants of standard benchmarks (e.g., CIFAR100 and TinyImageNet, the proposed method shows improvement over existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work deals with a more realistic and challenging CIL setting, which was originally introduced by Hong et al., (2024)\n2. The decoupled training strategy between text encoder and image encoder is justified by observation, proposition, and theorem.\n3. The paper provides extensive comparisons with many existing baseline methods."}, "weaknesses": {"value": "1. The paper is not well-written and lacks clarity in problem formulation. It is unclear what the exact goal of the problem, especially given that the method is built on a vision–language model (CLIP). The paper does not specify whether it performs vision-only classification or cross-modal alignment, which makes the problem definition incomplete. This becomes clear only after reading the experimental section.\n\n2. The dual-speed mechanism lacks quantitative evidence. The difference between the fast and slow branches is manually fixed (\\alpha=0.5, \\beta=0.1) without any sensitivity analysis or ablation. There is no justification for why this ratio is optimal or even effective. The claimed “dual-speed” behaviour is therefore not empirically validated.\n\n3. The learnable gating parameter \\gamma may cancel the effect of \\alpha and \\beta. Since \\gamma is end-to-end learnable, it can easily dominate the weighting between fast and slow adapters. In that case, the predefined ratio of \\alpha and \\beta becomes meaningless. The paper does not analyze the behaviour of \\gamma or show that it actually preserves two distinct learning dynamics.\n\n4. Theorem 1 is not a real theoretical proof, but heuristic and qualitative. It lacks any formal assumptions, mathematical derivation, or convergence bound. It functions more as an intuitive explanation than a rigorous theoretical result.\n\n5. The data leakage argument is overstated. The authors state that competing methods trained on ImageNet-21K as “for reference only,” citing possible overlap with CIFAR100 or TinyImageNet. However, these datasets do not share actual images, only similar class names. The argument seems defensive rather than technically justified. \n\n6. The originality is limited. The method largely follows the current trend of CLIP-based continual learning (e.g., Continual-CLIP, RAPF, MoE-Adapters). The proposed fast–slow learning strategy is a well-known paradigm in continual learning (e.g., Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System, ICLR 2022)."}, "questions": {"value": "Pleases refer to each of the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8rlluJYkQM", "forum": "ZgPVXrhjNt", "replyto": "ZgPVXrhjNt", "signatures": ["ICLR.cc/2026/Conference/Submission3269/Reviewer_B4Ha"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3269/Reviewer_B4Ha"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805507346, "cdate": 1761805507346, "tmdate": 1762916639173, "mdate": 1762916639173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an approach to imbalanced class incremental learning, where the overall distribution of classes to learn in a sequential manner is long tailed. Authors propose to design an approach that balances between fast adaptation/learning of task and slow consolidation of knowledge over time. To do this, they propose to use vision language models (e.g. CLIP) and decouple the fine-tuning procedure by adapting the text and image encoder sequentially. The fast-slow adaptation procedure leverages two separate adapters with a learnable task specific weight adjusting the relative influence of each adapter. The method is tested on multiple continual learning classification benchmarks designed for task imbalance, showing strong performance overall."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of balancing fast adaptation to knowledge with slow consolidation of past knowledge is well justified and makes sense, in particular in the context of continual learning. Exploring the situation where classes are imbalanced is more realistic, and it’s great to see efforts to tackle more complex problems. \n\nRelying on vision/language models to handle task imbalance and low data regimes for tail classes is sensible, as it generally allows more robust predictions. This is evidenced by experiments in Figure A3 where vision-language methods dominate for tail classes. It is nice to see authors make efforts to analyse the behaviour of models under different conditions, notably the impact of text models, to inform and motivate their methodology and ideas. \n\nExperiments are thorough, covering multiple standard benchmarks and achieving strong performance overall."}, "weaknesses": {"value": "While I like the idea of fast and slow learning, my main concern is the implementation of this idea via the dual adapter, which does not seem to reflect the fast adaptation and consolidation of knowledge discussed in prior sections. One potential reason for this is the limited intuition provided when introducing the dual adapters (sec 2.2.1). It is unclear how two identical adapters with learnable relative weighting (obtained from the average feature representation of a batch) can lead to a fast and slow learning procedure, especially as learned weights are never visualised or analysed. Wouldn’t an exponential moving average (EMA) based adapter be a more accurate implementation of the fast/slow idea? \n\nIt would be great to get more intuition and explanations on why this formulation enables slow consolidation of knowledge. \nThe sequential adaptation idea is interesting, and discussed in much more details. However, there is a lack of clarity in this section, and there are limited experiments validating this choice. Authors investigate the impact of the order of this sequential adaptation, but do not show the benefit of sequential adaptation vs adapting both, or keeping one fixed. \n\nOne important detail to clarify as well is to confirm that sequential adaptation is carried out per task, instead of first adapting the text encoder on all tasks, then adapting the visual encoder. The latter would violate the continual learning constraint, therefore I am assuming the former is used. However, it is unclear how adapting the classifier then the features per task reduces drifting issues across tasks. It would be great for authors to clarify their training protocol and how this classifier stability is achieved in a continual learning setting."}, "questions": {"value": "-\tAs discussed in the weaknesses sections, it would be great to get intuition of why section 2.2.1 leads to a fast/slow training procedure; and clarify the sequential training protocol and how that prevents drift/maintains accuracy on prior tasks. \n-\tIs FSA used for both text and image adapters? In ablation experiments where FSA is disabled, are there still 2 sets of adapters or only a single one? A reduced number of parameters could impact performance. \n-\tWas EMA considered for the fast and slow training procedure?  \n-\tHave authors visualised how parameter gamma evolves over time ? How can this parameter be interpreted in the context of fast and slow learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3kMz3m9kBW", "forum": "ZgPVXrhjNt", "replyto": "ZgPVXrhjNt", "signatures": ["ICLR.cc/2026/Conference/Submission3269/Reviewer_jvrY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3269/Reviewer_jvrY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823118397, "cdate": 1761823118397, "tmdate": 1762916638858, "mdate": 1762916638858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a continual learning problem with imbalanced task data (TICL). To address the stability-plasticity and head-tail learning dilemma, it first proposes a fast-slow adaptation model which learns two adapters (branches) with different weighting for fast and slow knowledge; then proposes a decoupling strategy that first fine-tunes the text encoder and then fine-tunes the image encoder.  The proposed method is evaluated under TICL benchmarks compared with many existing CL methods, including both ViT-based and CLIP-based methods. Results show that the proposed method achieves better performance than baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper studies a practical CL problem with imbalanced task data, which has wide applications in real world scenarios.\n\n2. This paper conducts thorough experiments compared to SOTA CL methods, with different task-arrival orders. Experimental results show the efficacy of the proposed method.\n\n3. The visualization of the paper is good."}, "weaknesses": {"value": "1. Although motivated by the dual-process theory, it is unclear to me why the proposed fast-slow adapter achieves the fast and slow learning. \n* In joint learning, Eq. 4 can be written as: \n$ FSA(h)= h+\\gamma \\alpha W\\_\\{up\\}^\\{fast\\} \\sigma (W^\\{fast\\}_\\{down\\}h) + (1-\\gamma) \\beta W\\_\\{up\\}^\\{slow\\} \\sigma ( W^\\{slow\\}\\_\\{down\\} h) $. Since $\\gamma$ is learned as well, how different are $\\gamma\\alpha$ and $(1-\\gamma)\\beta$? Will the initialization of MLP in Eq. 3 influence the fast/slow learning? \n* How does ‘the fast adapter enables rapid adaptation to new tasks while leaving the base encoder largely unchanged’ (line 180)? An ablation study of what is learned differently by the fast and slow adapter could be helpful. \n\n2. My main concern lies in the analysis of the decoupled text/image encoder learning. Some claims in the analysis lack sufficient theoretical or empirical justification. \n* For example, in line 249-252, how to define ‘a stable semantic anchor’? How do the updates move features to the anchor? In Theorem 1, what are the ‘standard optimization settings’? In Fig 3, fine-tuning image encoder first generally performs better than fine-tuning text encoder first in the descending order, which seems against the claim that first fine-tuning text encoder is helpful. \n\n3. In the ablation study Figure 4, the performance difference under different settings is marginal. It could be helpful to show the statistical significance of the scores as well."}, "questions": {"value": "In Eq. 2, should it be $\\sigma(W^{fast}\\_{down}h)$ instead of $f(W^{fast}\\_{down}h)$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GDyw99CnlN", "forum": "ZgPVXrhjNt", "replyto": "ZgPVXrhjNt", "signatures": ["ICLR.cc/2026/Conference/Submission3269/Reviewer_MJBv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3269/Reviewer_MJBv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3269/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966965964, "cdate": 1761966965964, "tmdate": 1762916638619, "mdate": 1762916638619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}