{"id": "x0g7d96cp2", "number": 23521, "cdate": 1758344919352, "mdate": 1759896810684, "content": {"title": "SAMoE-VAE: A Tabular Foundation Model with a Schema-Aware Mixture-of-Experts Variational Autoencoder", "abstract": "Foundation models have revolutionized vision and language, yet tabular learning still depends on bespoke, per-dataset pipelines. A key challenge in developing a uniform representation that enables foundation model is \\emph{schema mismatch}: real-world tables contain diverse column types: numeric, categorical, text, datetime, whose semantics vary across datasets. We frame cross-tabular representation learning as a weakly supervised, multi-modal problem, leveraging the readily available schema metadata that accompanies each table. \nWe propose SAMoE-VAE, a schema-aware Mixture-of-Experts VAE that:\n(i) assigns separate experts to numeric, categorical, text, and datetime columns;\n(ii) fuses expert posteriors via a schema-conditioned Product-of-Experts(MoPoE);\n(iii) produces a probabilistic latent embedding space that drives accurate downstream prediction and schema-aware generation.\nTo train at scale, we curate \\textbf{Meta-T4}, a 1.2-million-table corpus augmented with LLM-generated text metadata. Extensive experiments show that SAMoE-VAE outperforms prior art in tabular foundation models on representation learning benchmarks, yielding higher downstream accuracy and improved sample efficiency.", "tldr": "", "keywords": ["tabular data", "mixture-of-experts", "transfer learning", "foundation model", "representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43444ca03c0f0b35d27d26122f414beeb3cd1f50.pdf", "supplementary_material": "/attachment/b19429732a553f727cad1dd87f2ec86ec7999feb.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a new meta learning algorithm (SAMoE-VAE) for tabular data, specifically designed to be schema-aware and transferable. For each modality, the model first encodes it via a modality-specific method: language model for column names, text, and categorical; periodic encoding for dates; learnable periodic for numerical columns. These embeddings are then further passed through a perceiver network to get a fixed-sized vectors of Gaussian sufficient statistics, mean and covariance matrix. These statistics are then mixed pairwise via a product of experts and finally combined with weights, where the weights are computed based on the sufficient statistics coupled with the context vector. Extensive empirical evaluation on 3 suites of tasks from OpenML confirm the utility of the learned embeddings using linear probing against SOTA tabular methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Well-motivated problem.** The problem of learning a foundational embedding model for tabular data is timely and well-motivated. In fact, just a few days ago, I was looking for a tabular embedding model for my own project. From my survey, it seemed like TabPFNv2 was the best. I am glad to find this work and cannot wait to try it out. \n2. **Extensive literature review.** The paper is well-positioned. The authors have done an extensive literature review on the subject. \n3. **Engineering effort.** The authors have clearly spent some time, optimising the design of the model. It's great that they provide all the details and carefully argue why they are necessary."}, "weaknesses": {"value": "1. **Does meta learning really help?** To me, the crucial missing part is the ablation, where SAMoE-VAE is trained on a per-dataset basis, without meta pre-training. Given how well SwitchTab performs (rank #2), I am not convinced that meta learning helps, except for the case of the text-rich datasets or extremely few-shot (<16) settings, both of which are rare (imho) in practice. \n2. **Potential use-cases.** Linear probing on top of the embedding still largely underperforms the SOTA on tabular, e.g., on CC18 it falls far behind XGBoost (8 years old model), and even on text-rich UniPredict it is again behind XGBoost that does not have text processing capabilities whatsoever. Generative capabilities? To my knowledge, there is no clear-cut strategy of how and where to apply tabular generative models. Also, the authors only show the reconstruction loss and don't measure if their embeddings actually improve the downstream DDPM on tabular. In this sense, maybe evaluating TabPFN on top of the embeddings to see if they improve SOTA could help. \n3. **Still needs fine-tuning.** The authors advertise the method as a meta-learning algorithm. Yet, I feel unlike TabICL, TabPFN, or Limix, this cannot be used as a true zero-shot algorithm. It still requires fine-tuning, which could be expensive."}, "questions": {"value": "1. What is happening with F1 scores in your Table 1, for all the embedding models they are low? \n2. Do you fine-tune the TabPFNv2 model before extracting the embeddings? So you extract TabPFNv2 embeddings from the encoder or decoder? Do you use the cross-validation? I am a bit surprised to see TabPFNv2 embeddings fall so much behind other methods. In my own tests, they have consistently outperformed other SSL embeddings.\n3. In Table 1, is my understanding correct that you fine-tune all of the embedding models (e.g. Tabula, TabVec etc)? It seems expensive.\n4. Typos: (i) line 55 runaway comma (ii) line 54 Lin et al. missing year (iii) line 51 sentence beginning with \"E.g.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rPRnKT1JXP", "forum": "x0g7d96cp2", "replyto": "x0g7d96cp2", "signatures": ["ICLR.cc/2026/Conference/Submission23521/Reviewer_Udgn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23521/Reviewer_Udgn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761049245439, "cdate": 1761049245439, "tmdate": 1762942697436, "mdate": 1762942697436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an architecture for learning representations of tabular data that is expected to handle the heterogeneity of different column data types. The model integrates a variational autoencoder (VAE) with a mixture-of-experts (MoE) mechanism, where each expert specializes in a particular modality—numerical, categorical, free-form text, etc. Besides, the model leverages metadata such as table descriptions and column details, which are encoded into a learned schema vector to enrich contextual understanding. The pretraining is conducted on an augmented T4 corpus, where the authors use LLM-generated metadata to enhance the diversity of the data. The model’s performance is evaluated on some benchmarks, and is compared with several existing tabular representation models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The combination of a VAE and MoE to handle multiple data modalities is conceptually appealing and well-motivated. The modular expert design aligns well with the natural heterogeneity of tabular data.\n\n- The inclusion of experts for free-form text and datetime columns addresses an important gap in many current tabular foundation models, which often focus only on numerical and categorical data. Integrating table-level and column-level metadata is a good design choice that could improve contextual understanding of tabular structures. The augmentation of the T4 corpus with LLM-generated metadata is a good way to expand pretraining data and simulate richer supervision."}, "weaknesses": {"value": "- The evaluation omits many of the strongest-performing tabular models, including RealMLP, TabM, GBDT variants, TabICL, and TabDPT. Even if the focus is on representation learning, comparisons against strong methods—especially foundational ones like TabICL and TabDPT—are necessary to contextualize the model’s effectiveness and practical relevance.\n\n- While the paper reports results on multiple tasks (reconstruction, classification, and clustering), the motivation for including each is not fully clear, and the overall experimental setup could better highlight the representational strengths of the proposed model."}, "questions": {"value": "- Since SAMoE-VAE includes experts for text and datetime modalities, could the author(s) provide quantitative results on datasets that contain substantial free-form text and temporal columns? How do these experts affect performance in such cases?\n\n- Have you studied how SAMoE-VAE performance scales with model size (e.g., number of latent dimensions, number of experts, or schema-vector width)?\n\n- Why were models such as RealMLP, TabM, TabDPT, or TabICL omitted from the main comparison?\n\n- Could you elaborate on how well the model transfers across datasets with disjoint schemas, beyond reconstruction metrics? For example, can embeddings trained on one domain (e.g., healthcare) transfer effectively to another (e.g., finance)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G8tGTeCmT5", "forum": "x0g7d96cp2", "replyto": "x0g7d96cp2", "signatures": ["ICLR.cc/2026/Conference/Submission23521/Reviewer_D1cR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23521/Reviewer_D1cR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827774790, "cdate": 1761827774790, "tmdate": 1762942697146, "mdate": 1762942697146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel schema-aware MoE-VAE for tabular data that routes modality experts using explicitly learned schema vector. This schema-aware MoE-VAE is quite interesting approach that goes beyond the current simply gating mechanisms for MoE approaches on tabular data, and allows to generae schema-conditioned representation. The paper describes in reasonable detail how the schema elements are encoded and how the schema vector is learnt. As such, the claims of foundation model as a metadata-supervised, multi-modal model are well founded with competitive results over benchmark datasets --CTR-23, CC-11, and UniPredict. The paper also introduces metadata enriched T4 version which contains LLM-generated table/column metadata, used for pretraining the model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper presents a clean architecture tying schema metadata to routing, and a novel probabilistic latent space via VAE. \nS2. Experiments indicate consistent gains in zero-shot reconstruction and few-label settings. \nS3. Thorough ablation studies that help isolate the value of schema-aware gating -- clear novelty of the paper."}, "weaknesses": {"value": "W1. Inconsistent details: (a) pretraining time : is it 240 (line 302) or 250 hours (line 786)? (b) pretraining dataset size: 1.2 million tables or 2.3 million tables (in table 6)? (c) do date-time get distribution patterns or not (line 814)? (d) what is the metadata LLM used?, etc. These make the paper seem to be written in a hurried manner. \nW2. Missing baselines: besides XGBoost, it is crucial to include CatBoost and LightGBM models in tabular tasks. Also missing are FT-Transformer and SAINT -- both fairly recent and quite important baselines. \nW3. Synthesis tasks do not have utility tests such as Train-on-Synth + Test-on-Real, and Privacy metrics are completely missing. \nW4. The paper claims that it encodes \"missingness patterns\" -- not at all clear how this is handled. This seems to be an overclaim. \nW5. A serious issue is the handling of Date/Time -- a single positional encoding (sine-cosine) is standard but quite wrong when you have separated year, month, date, day-of-week, etc. For example, year or time-since-epoch is not cyclic but rather \"trend\" model, even for seemingly cyclic data there is enough research to show that better encodings exist than simple sine-cosine. \nW6. The paper claims it covers 100% of schema in pretraining. It is a serious limitation for claims of foundational model -- it does not indicate how well the model performs when dealing with unseen schema elements, or scaling laws (does performance keep rising as you add more distinct schemas, or saturate early?). Besides this, the LLM calls could be reduced if the schema can be sampled. \nW7. Another issue that makes it hard to consider it as a \"foundational model\" (contrary to the claim), is the fact that test splits are per-table. That is -- backbones are trained on each table’s train split and then frozen for a linear probe. Further, the evaluation does not seem to be really over zero-shot prediction across datasets (instead linear probes and zero-shot reconstruction)."}, "questions": {"value": "Please address the questions and issues raised in the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EIQLTRhdcP", "forum": "x0g7d96cp2", "replyto": "x0g7d96cp2", "signatures": ["ICLR.cc/2026/Conference/Submission23521/Reviewer_Btdr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23521/Reviewer_Btdr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986653969, "cdate": 1761986653969, "tmdate": 1762942696852, "mdate": 1762942696852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a variational auto-encoder for tabular data, Schema-Aware MoE-VAE (SAMoE-VAE). SAMoE-VAE incorporates column types, domain tags, distribution patterns, and other metadata directly into the routing mechanism. Through empirical studies, the paper shows the effectiveness of  SAMoE-VAE on various tasks along with ablations to show the importance of the components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "In general, the paper is well-written and easy to follow. The concept handling different datatypes separately is crucial, and the ideas of enriching with the meta-information of table (along with enriching the llm-generated pre-trained data) can be useful for further extensions on related research. The paper appropriately addresses the limitations and future works."}, "weaknesses": {"value": "Please refer to the questions."}, "questions": {"value": "-\tI cannot find the ‘string context’ (figure1) in the main manuscript. It would be helpful to indicate it or match it with the main text.\n-\tWhat is the reason behind choosing the variational autoencoders?\n-\tIt would be interesting to compare with a more recent model TARTE that separates the handling of different data types.\n-\tWhat are the characteristics of the datasets? (For instance, how many columns are numerical, categorical, textual, datetime, etc.,)\n-\tWhat would be the computation time for running model (excluding the pre-training)?\n- It would be interesting to see the comparison of regression performances in other datasets. I think CTR-23 might not be sufficient enough to address the different data types present in the downstream datasets.\n- Possibly, datasets from CARTE/TARTE or TextTabBench can be useful to further show the effectiveness of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DrtuOyoOXU", "forum": "x0g7d96cp2", "replyto": "x0g7d96cp2", "signatures": ["ICLR.cc/2026/Conference/Submission23521/Reviewer_K2eY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23521/Reviewer_K2eY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762078553005, "cdate": 1762078553005, "tmdate": 1762942696507, "mdate": 1762942696507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}