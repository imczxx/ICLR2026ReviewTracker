{"id": "KbVNswEzvW", "number": 8431, "cdate": 1758083158685, "mdate": 1762922976692, "content": {"title": "CoTDiff: Diffusion-based Image Synthesis with BiCoT", "abstract": "Recent advancements in Large Language Models (LLMs), Large Multi-Modal Models (LMMs), and text-to-image generation have significantly improved multimodal understanding and generation. However, a fundamental gap remains between human drawing processes and the iterative denoising mechanisms of existing diffusion-based models, leading to structural inaccuracies, prompt inconsistencies, and factual errors. To address this, we propose CoTDiff, a novel diffusion-based multi-stage image synthesis framework that integrates Chain-of-Thought (CoT) reasoning. This approach introduces two forms of CoT: textual CoT, where an LLM depicts the image layout based on the prompt, and diffusion CoT, which generates images in multiple stages—edge maps, grayscale images, and colorful images—mimicking the human drawing process.\n\nCoTDiff leverages a feature insertion mechanism to harmonize these stages, effectively reducing conflicts and improving consistency. Empirical results demonstrate that CoTDiff outperforms existing text-to-image methods, particularly in complex tasks requiring accurate object counting and spatial control. By bridging the gap between human drawing and machine generation, CoTDiff offers a fresh perspective on integrating CoT into image synthesis and unlocks the latent potential of diffusion models to produce high-quality, detailed, and coherent images.", "tldr": "", "keywords": ["Text-to-Image Synthesis", "Diffusion Model", "CoT"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b23847cd292c7088824c817f773af05ce576bc9c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "CoTDiff is a multi-stage text-to-image method that adds a “textual CoT” step to predict layout boxes and a “diffusion CoT” pipeline that generates edge → grayscale → color, with a feature-insertion module to pass information across stages."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Simple grounding with gated cross-attention and stored features is easy to plug into SD-1.5.\n- GenEval results and ablations indicate some gains and that token-merge helps."}, "weaknesses": {"value": "- The main boost may come from explicit layout boxes, not real “reasoning.” No ablation that removes the textual CoT while keeping diffusion CoT.\n- Evaluation is narrow. It focuses on GenEval. It lacks more baselines and human studies.\n- Novelty is modest. Prior work already uses ControlNet, layout grounding, and feature banks. This looks like a pipeline of known parts.\n- Paper quality is weak. Writing is rough; some plots are hard to read.\n- Method details are thin. Data scale, LLM prompts, error stats for box prediction, and failure cases are missing."}, "questions": {"value": "- How much do scores drop if you remove the textual CoT and keep the diffusion stages and feature insertion? Please report a full ablation.\n- What is the speed and memory cost for different feature-insertion sizes m?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "G0i9BtkP1y", "forum": "KbVNswEzvW", "replyto": "KbVNswEzvW", "signatures": ["ICLR.cc/2026/Conference/Submission8431/Reviewer_U1tR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8431/Reviewer_U1tR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760537715044, "cdate": 1760537715044, "tmdate": 1762920324456, "mdate": 1762920324456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We take the reviewer's suggestions into account and realize that the experiment needs improvement and the paper requires significant changes."}}, "id": "0nUR30lfpG", "forum": "KbVNswEzvW", "replyto": "KbVNswEzvW", "signatures": ["ICLR.cc/2026/Conference/Submission8431/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8431/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762922975766, "cdate": 1762922975766, "tmdate": 1762922975766, "mdate": 1762922975766, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CoTDiff, a diffusion-based image synthesis framework that introduces Chain-of-Thought (CoT) reasoning into the image generation process. The model integrates two types of CoT: textual CoT, where a large language model refines prompts and layouts, and diffusion CoT, where the generation proceeds in multiple stages (edge → grayscale → color). A feature insertion mechanism is introduced to connect these stages."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of mimicking human drawing stages (outline → detail → color) is intuitively appealing and helps organize the diffusion process more structurally.\n\n2. The paper includes comparisons with both autoregressive and diffusion-based baselines."}, "weaknesses": {"value": "1. The combination of comments does not yield a clear theoretical or methodological breakthrough beyond prior works.\n\n2. The manuscript has numerous typesetting and referencing issues. \n\n3. The loss equations (e.g., Eq. (1) and Eq. (7)) are typeset in a non-standard format.\n\n4. Improvements are mostly moderate and limited to certain categories.\n\nOverall, the paper’s presentation quality could be improved, as there are noticeable formatting and referencing issues that affect readability."}, "questions": {"value": "1. See in W1.\n\n2. Fix all figure references and reformat all mathematical equations.\n\n3. Provide an ablation or visualization that demonstrates why multi-stage CoT reasoning leads to better interpretability.\n\nThe paper presents a moderately interesting idea but lacks originality and suffers from serious formatting and presentation issues. With improved writing, clearer novelty positioning, and a more rigorous theoretical grounding, it could be a stronger submission in future iterations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XSwJMmuaS3", "forum": "KbVNswEzvW", "replyto": "KbVNswEzvW", "signatures": ["ICLR.cc/2026/Conference/Submission8431/Reviewer_7CyX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8431/Reviewer_7CyX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761348805406, "cdate": 1761348805406, "tmdate": 1762920324089, "mdate": 1762920324089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes to incorporate the CoT idea into the task of image generation from text. Different from previous methods, the new method generates an image from three steps: first generate a canny image from text, and then a grayscale image from the canny image, and then the colored image from the grayscale image. The method is significantly different from previous work and shows some performance improvement in the text-to-image generation task."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The work proposes a new type of CoT approach to image generation. In particular, it generates the image through the canny image and the grayscale image. As in the analysis, the canny graph seems to be a proper intermediate step for text-to-image generation."}, "weaknesses": {"value": "The paper is poorly written and not really finished. There are several writing issues making the paper hard to read. \n1. Some notations are not well explained. What's B in (2)?\n2. The section 3.4 is not readable. What are Q and K matrices? I don't see where they are from. \n3. The LLM-style writing of Sections 3.3 and 3.4 are very hard to read. With many short sentences and itemizations, it is hard to see the rationale behind the design. \n4. Many citations are not correctly formatted. The reference to a figure in line 286 is missing. \n\nThe performance in many categories only has marginal performance over the baseline. For example, on the categories of Single object, Two object, counting, and Attribute binding, there is only marginal performance improvement. \n\nOverall, this paper doesn't seem to be completed, even though the idea is novel."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The model proposes a new method that aims to improve Text-to-Image generation performance. Image generative models can be used to generate harmful content. The work should have a discussion about the potential concern."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zU3XSlTHM9", "forum": "KbVNswEzvW", "replyto": "KbVNswEzvW", "signatures": ["ICLR.cc/2026/Conference/Submission8431/Reviewer_1ucZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8431/Reviewer_1ucZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885992652, "cdate": 1761885992652, "tmdate": 1762920323697, "mdate": 1762920323697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors CoTDiff as a method to enhance the generation quality of diffusion models (including flow-matching models). Mentioning that the previous methods that utilize CoT to boost the generation quality for the image generation task, the authors propose a mechanism that consists of two chains, where one is based on CoT occurring with the LLM and the second one is occurring within diffusion models as imaging steps. In the same way that CoT for text imitates the thought process with each generated text across thought steps, authors claim that over a proposed chain for diffusion (HED to grayscale to RGB), the same process can be imitated for image generation. Over the experiments conducted, the proposed method leads to quantifiable improvements across several generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors introduce a thought process for images, that is shown to be effective for the image generation task.\n- The reported experiments demonstrate gains for both aesthetics improvements and task success across the included benchmark.\n- The proposed method is simple and easily applicable to different generation backbones.\n- Experiments show the effectiveness of the approach in different generation paradigms (autoregressive and diffusion based generation)."}, "weaknesses": {"value": "- Undefined reference in line 286.\n- Since the method defends that image-CoT is a main factor that improves image generation, the expectation is every step of the chain to have a significant effect. This is not shown by any ablations. From the qualitative and quantitative results we see that there are performance improvements, but it is not clear if it is from multi-stage chain for diffusion-CoT. This is one factor that is crucial to be ablated.\n- The method claims that having both text-CoT and diffusion-CoT is the core of the method design and a factor that improves the generation performance. Over the experiments, this difference is not investigated. While both of the CoT variants for generation may be found effective, the paper does not provide any insights on what is their corresponding effect.\n- Upon the mentioned points on missing ablations, the main factor that improves the image quality seems more like the layout generation and its injection. To remove such questions, authors should provide necessary experiments over the revision.\n- The provided experiments only compare with the base models, ideally comparisons with similar efforts that utilize LLMs to boost the generation quality should be a part of the experiments."}, "questions": {"value": "- How did the CoT sequence is finalized, is the sequence of HED to grayscale and grayscale to RGB is the optimal sequence for the thought sequence?\n- What are the impact of each stage occurring in diffusion-CoT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZoxNKMsiFA", "forum": "KbVNswEzvW", "replyto": "KbVNswEzvW", "signatures": ["ICLR.cc/2026/Conference/Submission8431/Reviewer_RGJD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8431/Reviewer_RGJD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975854194, "cdate": 1761975854194, "tmdate": 1762920323259, "mdate": 1762920323259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}