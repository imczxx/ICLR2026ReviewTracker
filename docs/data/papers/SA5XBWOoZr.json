{"id": "SA5XBWOoZr", "number": 13902, "cdate": 1758224667738, "mdate": 1762996353325, "content": {"title": "ECO: Energy-Constrained Operator Learning for Chaotic Dynamics with Boundedness Guarantees", "abstract": "Chaos is a fundamental feature of many complex dynamical systems, including weather systems and fluid turbulence. These systems are inherently difficult to predict due to their extreme sensitivity to initial conditions. Many chaotic systems are dissipative and ergodic, motivating data-driven models that aim to learn invariant statistical properties over long time horizons. While recent models have shown empirical success in preserving invariant statistics, they are prone to generating unbounded predictions, which prevent meaningful statistics evaluation. To overcome this, we introduce the Energy-Constrained Operator (ECO) that simultaneously learns the system dynamics while enforcing boundedness in predictions. We leverage concepts from control theory to develop algebraic conditions based on a learnable energy function, ensuring the learned dynamics is dissipative. ECO enforces these algebraic conditions through an efficient closed-form quadratic projection layer, which provides provable trajectory boundedness. To our knowledge, this is the first work establishing formal guarantees for data-driven chaotic dynamics models. Additionally, the learned invariant level set provides an outer estimate for the strange attractor, a complex structure that is computationally intractable to characterize. We demonstrate empirical success in ECO's ability to generate stable long-horizon forecasts, capturing invariant statistics on systems governed by chaotic PDEs, including the Kuramoto-Sivashinsky and the Navier-Stokes equations.", "tldr": "", "keywords": ["Chaotic systems", "constrained operator learning", "physical constraints"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6cc7cd10894e75650dad166f72373fa9cc7373af.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "## Summary\nThis paper introduces the Energy-Constrained Operator (ECO), a framework designed to learn the dynamics of dissipative chaotic systems while providing theoretical guarantees on the boundedness of long-term predictions. The core idea is to jointly learn the system's operator and a Lyapunov function with quadratic form. A novel projection layer is introduced to enforce a dissipativity condition derived from control theory, ensuring that all predicted trajectories converge to a learned, bounded invariant set guided by the learned Lyapunov function. The authors validate their method on the Lorenz 63, Kuramoto-Sivashinsky, and 2D Navier-Stokes equations, showing that ECO can produce stable long-horizon forecasts, unlike an unconstrained baseline model which suffers from trajectory blow-up."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "## Strengths\n- The paper addresses a critical and well-known challenge in data-driven modeling of chaotic systems: the instability and divergence of long-term autoregressive predictions. The goal of enforcing stability by construction is highly relevant and important for the field.\n- The primary contribution is the theoretical, formal guarantees for the boundedness of the learned dynamics. Integrating Lyapunov stability theory into the neural operator architecture through a differentiable quadratic projection layer is elegant.\n- The experiments clearly illustrate the failure of DeepONet and the success of the proposed ECO framework in maintaining stability and capturing the the attractor on challenging PDE benchmarks."}, "weaknesses": {"value": "## Weaknesses\nWhile the core idea is promising, the paper has several notable weaknesses.\n- One weakness is the mismatch between the introduced Lyapunov energy-functional layer and the paper’s *operator-learning* framework. As stated in Theorem 1 (and its proof via bounds on the quadratic form), the energy layer and its guarantees are derived for a finite-dimensional system by spatial discretization. Specifically, the learned Lyapunov functional $V(w)=(w-w_c)^\\top Q(w-w_c)$ acts on $w\\in\\mathbb{R}^n$ with the SPD matrix $Q\\in\\mathbb{R}^{n\\times n}$, so both $V$ and its stability certificate depend on the chosen grid resolution. This conflicts with the discretization-independent goal of operator learning [Kovachki et al., 2023]: when the grid size $n$ changes, $Q$ and $w_c$ must be re-trained. The authors should state this limitation clearly. The paper would be much stronger if it discussed the challenge and potential solution of extending these guarantees to a true, discretization-independent function space setting.\n\n- The experimental validation, while demonstrating the value of the projection layer, is limited to an ablation study comparing ECO (DeepONet + Projection) against the standard DeepONet baseline. DeepONet is a general-purpose model, not one specifically designed for the stable long-term prediction of chaotic systems. This comparison fails to position ECO within the broader context of surrogate models tailored for *chaotic dynamics*. At minimum, the authors should include one baseline explicitly designed for long-term stable rollout—e.g., the Markov Neural Operator (MNO; Li et al., 2022), which the authors already cite in the manuscript and for which default hyperparameters and a public codebase enable reproducible comparisons. The authors do not provide reasons for omitting such baselines. Without the comparison, it is unclear whether ECO offers advantages over other surrogates designed for chaotic systems.\n\n- The literature review overlooks recent advances in two areas: (1) surrogate modeling for chaotic systems (e.g., Schiff et al.; 2024, Cheng et al., 2025; He et al., 2025; Brenner, et al., 2025) and (2) Lyapunov/energy-based stabilization of forecasts (e.g., Rodriguez et al., 2022). A more thorough review is needed to contextualize and narrow the novelty and advantages of this work.\n\n- The provided repository is appreciated, but there is no instructions. Please add a reproducibility guide to reproduce the reported results."}, "questions": {"value": "## Questions\n- Why does the DeepONet baseline in Fig. 4 fail immediately ($\\approx t=0$)? Under standard settings, one would expect at least a few reasonable rollout steps. Could the author include a zoomed view of the first few steps or justify this unexpected result.\n\n- Following Weakness #2, could the authors justify the decision to compare only against a standard DeepONet baseline? Why were other stability-focused methods (e.g., MNO) not included in the comparison?\n\n- The choice of a quadratic Lyapunov function is key to deriving the energy layer and its theoretical guarateen. What are the limitations of this choice? How would the framework perform on systems where the strange attractor's basin is not well-approximated by an ellipsoid? Does this choice restrict the types of dynamics ECO can stably learn? \n\n- The paper's problem formulation, theoretical guarantees, and experiments all focus on autonomous (time-indepdent) dynamical systems. Could the author comment on how would the proposed ECO framework and its theoretical guarantees be extended to handle non-autonomous systems, where the governing PDE is time-depdent?\n\n- The paper notes that for high-dimensional outputs, a diagonal $Q$ matrix is used for efficient computation. Could the author comment on (1) The diagonal simplification means the learned $V(x)$ must be axis-aligned. How does this diagonal simplification limit the model's ability to learn a complex attractor, which is unlikely to be axis-aligned in the chosen coordinates? (2) How the grid resoultion affects the training and performance of the learnable matrix $Q$? For example, does training on a finer grid generally lead to better forecasting accuracy?\n\n## References\n- Rodriguez, Ivan Dario Jimenez, Aaron Ames, and Yisong Yue. \"Lyanet: A lyapunov framework for training neural odes.\" International conference on machine learning. PMLR, (2022).\n- Kovachki, Nikola, et al. \"Neural operator: Learning maps between function spaces with applications to pdes.\" Journal of Machine Learning Research 24.89 (2023): 1-97.\n- Li, Zongyi, et al. \"Learning chaotic dynamics in dissipative systems.\" Advances in Neural Information Processing Systems 35 (2022): 16768-16781.\n- Schiff, Yair, et al. \"Dyslim: Dynamics stable learning by invariant measure for chaotic systems.\" arXiv preprint arXiv:2402.04467 (2024).\n- Cheng, Xiaoyuan, et al. \"Learning chaos in a linear way.\" arXiv preprint arXiv:2503.14702 (2025).\n- He, Yi, et al., Chaos Meets Attention: Transformers for Large-Scale Dynamical Prediction. Proceedings of the 42nd International Conference on Machine Learning (2025).\n- Brenner, Manuel, et al. \"Learning Interpretable Hierarchical Dynamical Systems Models from Time Series Data.\" The Thirteenth International Conference on Learning Representations (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uQIvAioSAY", "forum": "SA5XBWOoZr", "replyto": "SA5XBWOoZr", "signatures": ["ICLR.cc/2026/Conference/Submission13902/Reviewer_Sx1L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13902/Reviewer_Sx1L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761439519887, "cdate": 1761439519887, "tmdate": 1762924410275, "mdate": 1762924410275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We are withdrawing the paper from this venue. For the public record, we have responded to the reviewers’ comments below."}}, "id": "GfUOJ0vx8p", "forum": "SA5XBWOoZr", "replyto": "SA5XBWOoZr", "signatures": ["ICLR.cc/2026/Conference/Submission13902/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13902/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762996352484, "cdate": 1762996352484, "tmdate": 1762996352484, "mdate": 1762996352484, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ECO (Energy-Constrained Operator) learning, a framework designed to ensure long-term stability in the learning of chaotic dynamical systems. The method augments a neural operator (implemented via DeepONet) with a learnable quadratic Lyapunov energy function and a convex quadratic projection layer that enforces a discrete-time dissipativity condition. This combination yields a theoretical boundedness guarantee for the learned dynamics. Experiments on Lorenz-63, Kuramoto–Sivashinsky, and 2D Navier–Stokes systems demonstrate empirically stable rollouts and reduced divergence compared to an unconstrained baseline.\nWhile the framework is clearly presented and mathematically coherent, most of its core ideas have strong precedents in existing literature on dissipative-Hamiltonian and Lyapunov-stable neural networks, as well as differentiable optimization layers. ECO’s novelty lies mainly in its discrete-time treatment and the simplicity of its closed-form constraint enforcement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Provides a clear and formal discrete-time boundedness guarantee.\n\nIntegrates the constraint enforcement in a simple, computationally efficient closed-form layer.\n\nDemonstrates convincing empirical prevention of trajectory blow-up on canonical chaotic systems.\n\nTheoretical results are stated cleanly and tied directly to the implementation."}, "weaknesses": {"value": "The novelty is limited—energy-based constraints, Lyapunov guarantees, and differentiable convex projections have been previously  studied.\n\nEmpirical baselines are thin (only DeepONet). Comparisons to FNO, reservoir computing, or dissipativity-aware models would greatly strengthen the manuscript.\n\nOverclaims originality (“first with guarantees for chaotic dynamics”)—this should be softened and scoped precisely.\n\nNo ablations on key hyperparameters (k, alpha,Q) or quantification of how tightly the learned ellipsoid captures the attractor."}, "questions": {"value": "Does the boundedness guarantee still hold if Q is learned as non-diagonal?\n\nHow sensitive is the approach to the choice of energy level c?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hKdLIu1EFu", "forum": "SA5XBWOoZr", "replyto": "SA5XBWOoZr", "signatures": ["ICLR.cc/2026/Conference/Submission13902/Reviewer_Pn8K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13902/Reviewer_Pn8K"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603628143, "cdate": 1761603628143, "tmdate": 1762924409841, "mdate": 1762924409841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced \"Energy-Constrained Operator\" leverage concepts from control theory to develop algebraic conditions\nbased on a learnable energy function. ECO integrates control theory principles into machine learning, using a learnable Lyapunov-based energy function and a convex quadratic projection layer to guarantee that model predictions remain bounded.  The paper performed on some chaotic equations like Lorenz63/KS/kolmogrov flows."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Good story on involving the control theory to help ML model"}, "weaknesses": {"value": "Despite good theoretic insights, the results doesn't look promising. Especially, I am not sure if the DeepOnet is tuned properly. No ML model parameters is provided. Also, wether the authors have checked MNO results. The PDF of the data looks shifted from the ground truth making the results less convincing."}, "questions": {"value": "1. What's the total parameter sets?\n2. Have you compared MNO results from Li et. al?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5UCTRR4WfN", "forum": "SA5XBWOoZr", "replyto": "SA5XBWOoZr", "signatures": ["ICLR.cc/2026/Conference/Submission13902/Reviewer_3zfM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13902/Reviewer_3zfM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836589892, "cdate": 1761836589892, "tmdate": 1762924409347, "mdate": 1762924409347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the  Energy-Constrained Operator (ECO) , a neural operator for learning chaotic dynamics with formal boundedness guarantees. By integrating control-theoretic Lyapunov energy functions and a differentiable convex quadratic projection layer, ECO enforces dissipativity to ensure trajectory stability. Empirical results on Lorenz-63, Kuramoto–Sivashinsky, and Navier–Stokes (NS) systems demonstrate stable long-horizon forecasts and accurate recovery of invariant statistics, outperforming unconstrained DeepONet in chaotic PDE modeling."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper has the following strengths worth credits:\n\n1. Integrating control-theoretic Lyapunov energy functions to enforce dissipativity in learning chaotic systems.\n2. The development of a convex quadratic projection layer to generate bounded predictions.\n3. Appendix B provides meaningful metrics for evaluating long-term forecasting performance in chaotic systems."}, "weaknesses": {"value": "The paper should be improved considering the following facts:\n\n1. The novelty of ellipsoidal constraints is **incremental** compared with Markov neural operator (MNO). ECO replaces the spherical hard constraint with ellipsoidal constraints and introduces a differentiable projection layer.\n2. Experiments and evaluations are limited to compare ECO with DeepONet. In Figure 4-5, DeepONet is not a strong baseline.\nLeading methods such as MNO (a constrained neural operator method), Poincaré Flow Neural Networks (PFNN) (a constrained koopman operator method), Mamba (a state space model) are not addressed. This makes it difficult to assess the effectiveness of ECO compared with existing constrained methods for chaotic systems.\n3. The Ablation study is insufficient. The paper only studied on with/without projection in table 1. It would be more convincing to include ablation studies on the effect of different components in ECO, such as the choice of hyperparameters, weighing regularizers and model sensitivity to them, with/without energy function, etc.\n4. Though the paper provides several metrics for evaluating long-term forecasting performance in chaotic systems in Appendix B, many established metrics in the literature are not discussed, such as Wasserstein Distance, Maximum Mean Discrepancy (MMD), and Lyapunov Exponent, Lyapunov time. Including these metrics would enhance the comprehensiveness of the evaluation."}, "questions": {"value": "1. In line 220, the paper states 'the system’s invariant statistics, which is known to be difficult to characterize', any references as foundmentals sources?\n2. Regarding the computational cost for high dimensional output space, the diagonal $Q$ matrix is applied. How does this diagonal matrix affect the performance, such as accuracy and stability?\n3. Following question 2, how to optimize $L$ as a learnable component to construct positive definite  $Q$? This is a strong prior, also affecting the optimization landscape for other model components. Can it be optimized accordingly with the overall stochastic gradient descent on the model?\n4. As energy level $c$ is an important component in the model, the  paper explains fixing $c \\gg 1$ . But in Figure 3, the choice of c seems to be system-dependent. If so, what is the purpose of fixing $c$, and how does it generalize to different chaos systems? If not, how sensitive is the model to the choice of $c$?\n5. How does the paper derive the PCA of higher-dimensional chaotic systems like the NS systems in Figure 5, of which 2D spatial information analysis is more complicated?\n\nHappy to consider increasing my score if the paper is improved to address all the questions and weakness concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "btG8X2hSEd", "forum": "SA5XBWOoZr", "replyto": "SA5XBWOoZr", "signatures": ["ICLR.cc/2026/Conference/Submission13902/Reviewer_vKse"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13902/Reviewer_vKse"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870635945, "cdate": 1761870635945, "tmdate": 1762924408915, "mdate": 1762924408915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}