{"id": "6NUtPO9PdV", "number": 17602, "cdate": 1758278084250, "mdate": 1759897165485, "content": {"title": "Thought Anchors: Which LLM Reasoning Steps Matter?", "abstract": "Current frontier large-language models rely on reasoning to achieve state-of-the-art performance. To ensure their safety, it is crucial that we can interpret their computations. Yet, many existing interpretability are limited in this area, as standard methods have been designed to study single forward passes of a model rather than the multi-token computational steps that unfold during reasoning. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We introduce a black-box method that measures each sentence's counterfactual importance by repeatedly sampling replacement sentences from the model, filtering for semantically different ones, and continuing the chain of thought from that point onwards to quantify the sentence's impact on the distribution of final answers. We discover that certain sentences can have an outsized impact on the trajectory of the reasoning trace and final answer. We term these sentences thought anchors. For example, seemingly correct reasoning traces can contain steps with significant negative causal impact that would cause the model to pursue an incorrect answer if not immediately corrected. These thought anchors are generally planning or uncertainty management sentences, and specialized attention heads consistently attend from subsequent sentences to thought anchors. We further show that examining sentence-sentence causal links within the reasoning trace gives insight into the models behavior. Such information can be used to predict a problem's difficulty and the extent different question domains involve sequential or diffuse reasoning. As a proof-of-concept, we demonstrate that our techniques together provide a practical toolkit for analyzing reasoning models by conducting a detailed case study of how the model solves a difficult math problem, finding that our techniques yield a consistent picture of the reasoning trace's structure. We provide an open-source tool (anonymous-interface.com/ta) for visualizing the outputs of our methods on further problems. The identified convergence across methods and consistencies in motifs across analyses demonstrate the potential of sentence-level analysis for a deeper understanding of reasoning models.", "tldr": "We introduce black and white box methods for interpreting reasoning LLMs' chain-of-thought at the sentence level, identifying and categorizing sentences with outsized effects on the model's final answer and mapping sentence-sentence dependencies", "keywords": ["Mechanistic interpretability", "interpretability", "reasoning models", "thinking models", "inference‑time scaling", "test‑time compute", "chain-of-thought", "attention", "planning", "attribution"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/432dc99079de15549c7d037a5e64609a9bca6337.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces two methods for interpreting chain-of-thought (CoT) reasoning in LLMs. The first method measures sentence importance by either forcing an LLM to generate an answer right after an intermediate sentence, or resampling multiple traces from a perturbed intermediate sentence. Quantitatively, the sentence importance is computed as the KL divergence between the original and resampled answer distributions. The second method aims to interpret the interactions between pairs of sentences. The authors first demonstrate that certain attention heads narrow attention toward specific sentences, and then measure the causal links between sentences by taking the KL divergence between the token logits with versus without suppressing a sentence. It is observed that such causal links is dependent of the distance between sentences, with strong close-range links and weak long-range links. The authors also released their results as a website for fancy interactions with the interpretations."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Considering the prevalence of CoT reasoning, interpreting and understanding CoTs has a large impact in the community of LLMs.\n2. The authors released their results as a fancy interactive website, showing that their methods can potentially reveal insights behind LLM reasoning. However, this website is based on results pre-computed by the authors. It’s better if users can submit their own samples and obtain results online or offline."}, "weaknesses": {"value": "1. This paper doesn’t meet the standard of a scientific paper. Most of the paper is about the methods, but there is scarcely any systematic evaluation of the methods. For example, as stated in Line 119, the authors only used 10 questions to evaluate their methods, and even the full evaluation results can only be found on their website. This is definitely not sufficient for a paper in terms of the number of datasets or samples. Results in Figure 2-7 are more like qualitative or weakly quantitative results showing example output from the proposed methods, which are not enough to justify the proposed methods. I would suggest either comparing the proposed methods against baselines with quantitative experiments, or showing a bunch of lessons that we can observe only with the proposed method. Or maybe the authors should submit this paper to the demo track of some other conferences.\n2. This paper is poorly written. There are too many design details, but not enough high-level structure connecting the details, which makes the paper hard to understand. The authors should clearly describe their motivations for each method they introduce, and explicitly state the contribution of each method. The title especially blurs the contribution of this paper. In fact, this paper doesn’t provide a thorough answer to the question in the title, but offers a tool to help answer that question. I would suggest changing it to “Visualize important steps and their interactions in LLM reasoning”. In the introduction, the second paragraph discussing why you realize your methods at sentence level seems to be very weird. Usually, we expect this paragraph to serve the goals and motivations of your method. I also don’t understand why you need to introduce forced answer importance given you showed that counterfactual importance is better in Figure 3. Also several section titles are either confusing or not well supported by evidence in the paper. For Sec 3, it’s not clear what the authors refer to by consistent patterns, since they are never discussed in that section. For Sec 4, the authors didn’t discuss why attention head is the mechanistic roots of importance. For Sec 6, what are the systematic differences you found apart from the connection between distance and causal links?"}, "questions": {"value": "1. Line 88: What’s the faithfulness of CoT text? How do you sidestep it?\n2. Line 140, 143: $S$ → $S_i$\n3. Line 143: Are $S_{i+1}$, …, $S_M$ resampled? If so, you’d better use another notation to distinguish them against the original sentences.\n4. Line 153-161: This paragraph looks verbose for the main paper. Better shorten it and move the full version to appendix.\n5. Line 188: describes → defines\n6. Line 207: $p(A'_{S_i})$ is not defined\n7. Line 208: How $\\epsilon$ is used in the equation is not introduced.\n8. Line 234-235: Section H didn’t directly shows that counterfactual importance is more useful than resampling importance. In my opinion, resampling importance measures how far the current prediction is from the average prediction, which also makes sense in some cases.\n9. Line 293-295: On how many samples did you compute the correlation? If it’s only 10 problems, that may not be enough.\n10. Line 309: $r=0.22$ is not very significant. Also “may minimally impact” in Line 312 seems to conflict with “exert a larger effect”.\n11. Line 338-339: In the original counterfactual resampling, you resample $S_j$ to be $T_j$, right? Here you only resample $S_i$ to be $T_i$ and then evaluate the likelihood of following sentences $S_j$ in a teacher forcing way, right?\n12. Line 398: What do you mean by “sharply structure the CoT” here?\n13. Line 409: $M_{sentences}$ is not defined.\n14. Line 426: What are the potential structural roles and their relationship to successful reasoning? Please be explicit."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5QByNsuhYl", "forum": "6NUtPO9PdV", "replyto": "6NUtPO9PdV", "signatures": ["ICLR.cc/2026/Conference/Submission17602/Reviewer_vUoT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17602/Reviewer_vUoT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679732143, "cdate": 1761679732143, "tmdate": 1762927467039, "mdate": 1762927467039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed methods to interpret the chain-of-thought of LLMs by: 1) repeatedly resampling from each sentence and measuring the counterfactual impact of the sentences; 2) analyzing the attention pattern of LLMs; 3) intervening the attention scores to see direct causal effects of a sentence on subsequent sentences."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper conducted a systematic analysis of the reasoning traces of LLMs by resampling and masking attention weights at a sentence-sentence level. Several interesting observations are made in the paper, e.g., correlation between problem difficulty and range of sentence-level links in the reasoning traces."}, "weaknesses": {"value": "See questions."}, "questions": {"value": "1. Is the analysis in Section 2 to 4 all based on one particular problem (the 66666 problem)? It would be clearer if somewhere in the paper clearly states what analysis is done on what set of problems.\n2. What information is Figure 3 trying to deliver?\n3. In Section E, do the training labels of the classifier come from prompting the LLM? The section is trying to show that the LLM annotated sentence type is accurate/meaningful by showing that using the annotation as training labels, we can easily learn a good sentence classifier? If this is the case, I think the evidence for saying the LLM annotation is accurate may not be sufficient, as it only show that LLM did good clustering job, and the clustering can be reconstructed using the information in the reasoning model's hidden representation, but how accurate is the sentence type labeling cannot be inferred from this experiment. Some more examples or some human verification may be more helpful.\n4. Figure 3 seems to suggest that each type of the sentences forms a continuous piece of text, and there is no interleave between sentence types. This doesn't seem to be the case in the web demo, where sentence types are interleaved.\n5. While the findings in the paper are interesting, I find it hard to link the findings to practical guidelines. Can you give some examples of what scenarios may benefit from the findings in this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0WWBkA9m0A", "forum": "6NUtPO9PdV", "replyto": "6NUtPO9PdV", "signatures": ["ICLR.cc/2026/Conference/Submission17602/Reviewer_Hw1d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17602/Reviewer_Hw1d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896135623, "cdate": 1761896135623, "tmdate": 1762927465137, "mdate": 1762927465137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for identifying which sentences in LLM’s CoT reasoning are most causally important for the final answer. Using counterfactual resampling, attention-head analysis, and causal masking, it shows that planning and uncertainty-management sentences tend to steer the reasoning trajectory. These high-impact sentences attract focused attention from specific “receiver heads” in later layers, forming structural anchors for reasoning."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper offers a fine-grained interpretability analysis by examining reasoning at the sentence level rather than the token level, allowing a clearer view of how individual reasoning steps contribute to the overall thought process.\n\n\n2. It introduces a sentence-masking method to study causal dependencies between reasoning steps, providing a thorough and systematic analysis of how earlier sentences influence later ones in the reasoning process.\n\n\n3. This work releases an open-source interactive visualization tool that displays reasoning traces, sentence importance, and causal links between sentences. This tool makes it easy to observe how specific reasoning steps influence others."}, "weaknesses": {"value": "1. The analysis assumes clean sentence segmentation and treats each sentence as an independent reasoning unit, which may not hold in more complicated reasoning contexts where boundaries are fuzzy.\n\n2. The \"thought anchor\" concept is derived from a very limited dataset: just 20 reasoning traces from 10 math problems. Furthermore, the study only selected problems the model can solve 25-75% of the time, so the findings may not generalize to problems the model consistently fails or solves easily."}, "questions": {"value": "Could you elaborate on how this framework might handle 'thought anchors' that don't fit neatly into a single sentence boundary? For example, how would it account for a critical reasoning step that might be a single key phrase within a much longer sentence, or, conversely, a larger concept that is built up across several sentences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "U9MDgdAeVO", "forum": "6NUtPO9PdV", "replyto": "6NUtPO9PdV", "signatures": ["ICLR.cc/2026/Conference/Submission17602/Reviewer_d1j2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17602/Reviewer_d1j2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975655817, "cdate": 1761975655817, "tmdate": 1762927463844, "mdate": 1762927463844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a sentence-level interpretability framework. It introduces thought anchors to expose CoT traces by computing resampling importance, and further characterizes relationships among anchors using attention-based analysis."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a sentence-level toolkit for analyzing CoT traces using both black-box and white-box methods.\n\n2. Step-by-step case studies accompany each stage of the pipeline, improving clarity and reproducibility.\n\n3. It introduces thought anchors—sentences identified by estimating per-sentence importance via resampling scoring—and further examines relationships among anchors through attention analysis."}, "weaknesses": {"value": "1. Motivation clarity: The paper argues for interpreting CoT, but does not clearly explain why CoT text alone is insufficient, nor how the method advances beyond prior interpretability work.\n\n2. Computational cost: Analyzing one CoT trace is expensive (e.g., ~100 resamples per sentence plus an auxiliary labeling model). Practicality at scale is unclear.\n\n3. Ablations are limited: No systematic study of design choices (e.g., sentence attention: mean/last-token/concat; Counterfactual importance: similarity function selection and threshold).\n\n4. Organization: Lacks an upfront system diagram and end-to-end workflow before diving into components.\n\n5. Experimental consistency: Different datasets/models appear across sections (e.g. MATH dataset and Qwen-14B in Section 2.1 while MMLU and  Qwen3-30b-a3b in section 6.1), making comparisons hard.\n\n6. Reproducibility: As a tooling paper, code and usage docs are essential but not clearly provided."}, "questions": {"value": "1. In what concrete scenarios does CoT text fail to be self-explanatory, and how do thought anchors remedy those gaps?\n\n2. How sensitive are the identified anchors to prompt formatting and to the number of resampling rollouts used?\n\n3. How does the approach perform on large models tackling harder tasks that yield long CoT traces?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D3mnWJYvUp", "forum": "6NUtPO9PdV", "replyto": "6NUtPO9PdV", "signatures": ["ICLR.cc/2026/Conference/Submission17602/Reviewer_c8CH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17602/Reviewer_c8CH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149883606, "cdate": 1762149883606, "tmdate": 1762927463230, "mdate": 1762927463230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}