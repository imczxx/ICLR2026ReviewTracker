{"id": "GXcN0MuN1q", "number": 10443, "cdate": 1758171663147, "mdate": 1759897650514, "content": {"title": "Discern Truth from Falsehood: Reducing Over-Refusal via Contrastive Refinement", "abstract": "Large language models (LLMs) aligned for safety often suffer from over-refusal—the tendency to reject seemingly toxic or benign prompts by misclassifying them as toxic. This behavior undermines models' helpfulness and restricts usability in sensitive or nuanced contexts. While prior work has proposed mitigation strategies such as data augmentation and activation steering, these approaches often face a trade-off: reducing over-refusal typically degrades the model’s ability to reject genuinely harmful content.\nWe argue that this issue arises from the ambiguous influence of toxic and seemingly toxic prompts on the model’s learning dynamics. To address it, we introduce a preceding alignment stage, DCR: $\\textbf{D}$iscernment via $\\textbf{C}$ontrastive $\\textbf{R}$efinement. Both theoretically and empirically, we demonstrate that contrastive refinement improves an LLM’s capacity to distinguish truly toxic prompts from superficially toxic ones. Evaluation across diverse benchmarks shows that our method effectively reduces over-refusal while preserving the safety benefits of alignment. Importantly, it achieves this with minimal degradation of general capabilities, offering a more principled and robust direction for safety alignment.", "tldr": "This paper introduces Discernment via Contrastive Refinement (DCR), a two-stage safety alignment method that uses contrastive learning to reduce over-refusal in LLMs while preserving safety and general abilities.", "keywords": ["Large Language Model", "LLM Safety", "Over-refusal", "SafetyAlignment"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e71086bf37a903a90977352222126a3dbfa26a3e.pdf", "supplementary_material": "/attachment/1f7e0cbe8dc6d8c8dbbb66b9f357dec6e45fb90b.pdf"}, "replies": [{"content": {"summary": {"value": "The paper studies over‑refusal in safety‑aligned LLMs and proposes a two‑stage procedure, DCR (Discernment via Contrastive Refinement). Stage 1 applies a contrastive loss on intermediate representations to push *seemingly toxic* prompts away from *truly toxic* prompts while freezing later layers; Stage 2 runs standard SFT safety alignment. The paper argues that over‑refusal arises because these two prompt types are highly coupled; it provides a bound linking gradient‑space similarity ( |K_t(x',x)|\\_F ) to a bilinear form on hidden states and shows empirical reductions of this coupling after DCR. Across three base models, DCR improves compliance on five over‑refusal test sets while keeping safety roughly unchanged. Figures 1, 3, 5–6 and Table 1 support the empirical story, and Appendices A, A–A detail implementation and metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **The method is clear and reproducible**. The two‑stage pipeline is easy to implement: Circle loss at an intermediate layer with the tail frozen, followed by standard SFT.  \n* **The evaluation has good coverage**. Results cover five over‑refusal benchmarks and five harmfulness suites, plus general‑ability tasks like AlpacaEval.\n* **Theory-driven analysis**. Proposition 1 and Appendix A connect representation‑level contrastive changes to a decrease in the gradient‑space kernel. The paper also tracks “rejection probability” during training and shows that DCR raises it mainly for toxic prompts while keeping it stable for seemingly toxic prompts."}, "weaknesses": {"value": "* **Framing of similarity as “incorrect.” (Line: 481)** The core motivation of this paper is that the similarity between seemingly toxic and toxic prompts is *incorrect* and should be decoupled. These prompts are expected to be similar at the representation level, and the issue is spillover during alignment. Therefore I am not very convinced about the motivation.\n* **Pair construction and dataset confounding.** Seemingly toxic prompts come from XSTest while toxic prompts come from HH‑RLHF in the contrastive stage. This risks learning dataset or domain differences rather than toxicity level. An ablation that draws both classes from the same source or uses matched minimal pairs would directly test this concern.\n* **Pairing strategy is weakly specified.** This is similar to the last point. Contrastive learning benefits from carefully curated *hard* negatives and matched positives. The paper samples pairs across subsets with a weighted sampler but does not align pairs by lexical template or topic, so the signal may be coarse. More rigorous pair‑wise design could strengthen the claim that DCR learns toxicity‑level separation rather than other differences. \n* **Measurement choices may be brittle.** \"Refusal probability\" aggregates probability mass over a fixed list of refusal strings (Appendix A). This can miss paraphrases. It will be better to validate the refusal measurements with human annotators or LLM-judges.\n\nPresentation suggestion: in Table 1 it's better to bold the best scores and clearly mark for each metric whether higher is better, so readers can read the trade‑offs at a glance."}, "questions": {"value": "- How much of the gain remains when controlling for dataset origin? As far as I know, the seemingly toxic prompts from XS-Test and Over-Refusal usually have different sentence structures compared to those in toxic datasets.\n- How do DCR’s cost and scalability compare to the baselines in Table 1?\n- Could you provide an analysis of the reliability of the refusal response templates?\n- I see that all baselines perform poorly on XS-Test. Is it possible that they are undertrained? Could you provide the results of the original model and a model overtrained using the baseline methods, so we can determine whether DCR is Pareto-optimal compared to the baselines?\n- The first stage of DCR effectively penalizes high similarity. Does the normalized kernel similarity actually decrease after applying DCR? Figure 3 shows the similarity changes, but it appears to reflect a normal safety-alignment process rather than DCR specifically."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UGOx2iN27i", "forum": "GXcN0MuN1q", "replyto": "GXcN0MuN1q", "signatures": ["ICLR.cc/2026/Conference/Submission10443/Reviewer_9fdp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10443/Reviewer_9fdp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761016576735, "cdate": 1761016576735, "tmdate": 1762921744438, "mdate": 1762921744438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue of over-refusal in LLM alignment. This paper focuses on the similarity between toxic prompts and seemingly toxic (but actually not toxic) prompts and shows that the rejection probability for these two types of prompts is highly correlated during the alignment process. To tackle the issue of over-refusal without losing the usefulness, the authors claim that it is important to distinguish truly toxic and seemingly toxic prompts and propose a two-stage alignment procedure: 1) contrastive learning between these prompts, then 2) standard safety alignment. Experiments show that contrastive learning effectively reduces the similarity between truly toxic and seemingly toxic prompts, and that it contributes to reducing the rate of refusal of seemingly toxic prompts while managing the refusal rate for truly toxic prompts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It provides a novel insight into the over-refusal in LLM and provides a reasonable understanding of the cause of the over-refusal.\n\nThe proposed approach is well-motivated by the observation of high correlation between the refusal rates of truly toxic and seemingly toxic prompts.\n\nThe proposed approach is compared with several baseline approaches and the experiments show promising performance of the proposed approach."}, "weaknesses": {"value": "The evaluation is done with relatively small models (up to 8B models) and the scalability is not shown.\n\nI found the observations in Figures 1 and 3 are interesting and these are the cores of the proposed approach. However, this observation is provided only for a small (1.5B) model. To support the claim of this paper, it is recommended to add the observations for middle sized (7–8B) models.\n\nThe strength of the contrastive learning (such as the number of epoch, learning rate, etc.) may affect the performance significantly, which is not discussed in the main text."}, "questions": {"value": "1. Do we observe similar trends as in Figures 1 and 3 with different models?\n\n2. How much does the strength of the contrastive learning affect the performance? How can we decide when to finish? It looks nontrivial to me when to stop it.\n\n3. Assumptions for Proposition 1 are not explained nor validated in the main text. Please provide the intuition and justification for them in the main text.\n\n4. Lines in Figure 3 are hard to distinguish. Please use different line styles or so."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qy9OAFASt3", "forum": "GXcN0MuN1q", "replyto": "GXcN0MuN1q", "signatures": ["ICLR.cc/2026/Conference/Submission10443/Reviewer_w17s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10443/Reviewer_w17s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722046818, "cdate": 1761722046818, "tmdate": 1762921743851, "mdate": 1762921743851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors tackle LLM overrefusal by contrastively reducing gradient coupling between toxic and seemingly toxic prompts during the pre-alignment training stage. They apply a contrastive loss at a single mid layer to decrease activation similarity. By their theoretical results, this method lowers the empirical neural tangent kernel similarity between these prompt types. Their downstream benchmarking results substantial improvements in compliance rates across multiple refusal benchmarks, while maintaining strong safety performance and capabilities on standard general knowledge tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear theoretical grounding.**\n- The paper (Section 5, 7.3) provides a formal link between activation similarity and empirical neural tangent kernel similarity to show how their method lowers gradient coupling. \n\n2. **The method is conceptually simple and doesn’t require architectural modification.** It targets directly the mechanism behind over-refusal (gradient coupling) rather than surface behaviors, which is underexplored."}, "weaknesses": {"value": "1. **Rejection probability calculation can be biased.**\n- As described in Appendix A.5, the refusal probability aggregates mass over a fixed list of refusal strings. Models can refuse with more nuanced paraphrases. Providing calibration such as precision/recall could help further support the robustness of this metric. Maybe adding a learned refusal classifier for this metric would be useful too.\n\n2. **Formatting.**\n- Citations that should be parenthetical are written as “Author, Year” instead of “(Author, Year)”. They should use `\\citep` instead of `\\citet`.\n- In Table 1 on page 8, **bolding** best performance will make the results more clear.\n\n3. **Computational costs.**\n- The additional contrastive pre-alignment stage increases total training cost and may not scale easily to larger models. It would be helpful to assess the practical cost if the authors could report compute usage (e.g. GPU type, number of GPUs and GPU hours)."}, "questions": {"value": "**Analysis on failure cases.** What kind of prompts still trigger false refusals after DCR? On what prompts does DCR help the most?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7REdcNuIqu", "forum": "GXcN0MuN1q", "replyto": "GXcN0MuN1q", "signatures": ["ICLR.cc/2026/Conference/Submission10443/Reviewer_jZ3e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10443/Reviewer_jZ3e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779196729, "cdate": 1761779196729, "tmdate": 1762921743370, "mdate": 1762921743370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles over-refusal. The authors observe that truly toxic and seemingly toxic prompts have highly similar intermediate representations. As this similarity grows, refusal rates rise in both groups. They propose DCR: before safety alignment, apply a contrastive loss to mid-layer activations in order to push the two prompt types apart; then run standard SFT-style safety alignment. They link activation similarity to gradient-space (NTK-like) kernel similarity, motivating why reducing representational overlap should reduce over-refusal. Experiments indicate DCR increases compliance on seemingly toxic prompts while maintaining safety and general capability."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Proposed method is a simple pre-alignment contrastive phase, i.e., existing SFT pipelines remain unchanged. \n\n2.  The gradient/NTK similarity analysis provides mathematical foundations for understanding refusal co-movement.\n\n3. The paper shows mathematically that reducing representation similarity limits how refusals spread."}, "weaknesses": {"value": "1. Using XSTest in the contrastive stage and again in evaluation may create bias toward in-distribution advantage.\n\n2. Justifications for several design choices are missing:\n(i) The methodology for determining which layers receive contrastive loss for each model architecture is not explained.\n(ii) The selection of circle loss over alternatives (e.g., InfoNCE, NT-Xent) lacks comparative analysis.\n(iii) No analysis of how varying toxic/seemingly-toxic sampling ratios affects model performance and stability. \n\n3. The absence of experiments on 30B-70B+ parameter models is a significant limitation, particularly given that safety and over-refusal behaviors often exhibit scale-dependent characteristics. Large-scale validation is essential for establishing the method's practical applicability in production environments. \n\n4.\tRelying primarily on guard models and/or keyword filters for safety assessment introduces systematic biases. The paper would benefit from human evaluation studies and multi-judge consensus approaches to establish more robust safety guarantees. \n\n5.\tThe paper lacks comprehensive computational overhead analysis (GPU-hours, memory requirements, training time comparisons etc.). A detailed analysis of computational overhead would be valuable.\n\n6.\tWhile baseline comparisons are provided, the paper does not sufficiently position DCR within the broader landscape of safety alignment methods. Detailed comparisons with constitutional AI, RLAIF, and recent preference optimization approaches (e.g., POROver) would better demonstrate advancement over state-of-the-art. \n\n7.\tThe robustness of the proposed method has not been properly examined. The paper does not evaluate how the approach behaves under adversarial conditions, and it does not report the variability of results across multiple training runs or random seeds. As a result, it remains unclear how reliable and stable the method would be in real-world applications."}, "questions": {"value": "1.\tAlthough the selected benchmarks are relevant, they do not fully represent the diversity of real-world conditions in which safety and overrefusal arise. How would the effect of DCR change in more heterogeneous datasets, i.e., multiple languages, cultural settings, and domain-specific contexts etc.?\n2.\tCan you provide ablation studies for (i) layer choice, (ii) different contrastive losses, and (iii) seemingly toxic/toxic ratios?\n3.\tWhat happens at 30B/70B scale or in a different architecture such as MoE? Would representational separation be easier or harder, and would gains persist?\n4.\tHow stable are results across different guard models and human raters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5QUmG6ZjHa", "forum": "GXcN0MuN1q", "replyto": "GXcN0MuN1q", "signatures": ["ICLR.cc/2026/Conference/Submission10443/Reviewer_7tZR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10443/Reviewer_7tZR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853353576, "cdate": 1761853353576, "tmdate": 1762921742868, "mdate": 1762921742868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}