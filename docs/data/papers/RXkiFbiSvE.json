{"id": "RXkiFbiSvE", "number": 5853, "cdate": 1757940309688, "mdate": 1759897949724, "content": {"title": "LlamaSeg: Image Segmentation via Autoregressive Mask Generation", "abstract": "We present LlamaSeg, a visual autoregressive framework that unifies multiple image segmentation tasks via natural language instructions. By reformulating segmentation as visual generation, LlamaSeg encodes masks as visual tokens and uses a LLaMA-style Transformer for direct next-token prediction, naturally fitting segmentation into autoregressive architectures. To support large-scale training, we introduce a data annotation pipeline and construct the SA-OVRS dataset, which contains 2M segmentation masks annotated with over 5,800 open-vocabulary labels or diverse textual descriptions, spanning diverse real-world scenarios. This enables our model to localize objects in images based on text prompts and to generate fine-grained masks. We further introduce the composite metric average Hausdorff Distance ($d_{\\mathrm{AHD}}$) to evaluate mask contour fidelity for generative models better. Experiments show that LlamaSeg consistently outperforms existing generative approaches on multiple segmentation benchmarks and delivers finer, more accurate segmentation masks.", "tldr": "", "keywords": ["Multimodal Large Language Models (MLLMs)，Semantic Segmentation，Referring Segmentation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/af9da7af13612afc94d942a31aeb30bc42414b56.pdf", "supplementary_material": "/attachment/1a34bbf5a7086570b95838eeda47fbd44b1b8e35.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes LlamaSeg, a visual autoregressive model that reformulates image segmentation as visual generation using a LLaMA-style Transformer. Trained on the large-scale SA-OVRS dataset with 2M annotated masks, it supports text-guided segmentation and fine-grained mask generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. LlamaSeg introduces the idea of using an image tokenizer to encode segmentation masks, effectively unifying various segmentation tasks within a discrete autoregressive framework. \n\n2. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. The comparison baselines are outdated, and LlamaSeg’s segmentation performance is not competitive (e.g., around 56 on RefCOCO), which is significantly lower than recent methods such as Ferret-v2 [1] (≈90).\n\n2. The relatively poor performance raises doubts about whether encoding masks using image tokenizer truly offers advantages over encoding them as discrete position tokens or point sequences. A more detailed comparison and ablation studies (including performance and efficiency) across different mask encoding strategies is needed to justify this design choice.\n\n3. Encoding a single mask requires hundreds of visual tokens, which appears less efficient than directly encoding the mask as a compact sequence of points(represented as position tokens in Kosmos-2).\n\n[1] Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models. arXiv preprint arXiv:2404.07973, 2024.\n\n[2] Kosmos-2: Grounding multimodal large language models to the world[J]. arXiv preprint arXiv:2306.14824, 2023."}, "questions": {"value": "1. My main concern lies in the motivation of this work. It remains unclear what specific advantages the use of an image tokenizer offers over existing mask encoding methods. Is it intended to improve performance, efficiency, or both?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KkQjf5Y2b5", "forum": "RXkiFbiSvE", "replyto": "RXkiFbiSvE", "signatures": ["ICLR.cc/2026/Conference/Submission5853/Reviewer_YhD4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5853/Reviewer_YhD4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761460838237, "cdate": 1761460838237, "tmdate": 1762918304782, "mdate": 1762918304782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper present the LlamaSeg network which is a visual autoregressive model that can apply image segmentation tasks with natural language instructions. The proposed model LlamaSeg encode the input as visual tokens and use LLAMA style network for next token prediction.\n\nThe paper also introduce a new data annotation framework which contains 2M segmentation masks over 5800 labels (SA-OVRS dataset). The new dataset allow the model to localized object based on text prompts. They also introduce a new metric called, Hausdorff Distance to measure the mask contour fidelity. The evaluation of the proposed model shows that in outperform existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method have unified formulation for multiple segmentation tasks such as - semantic, referring, open-vocabulary in one autoregressive model.\n\n2. The proposed method has strong boundary fidelity which cause due to the use of mask-tokenizer and autoregressive decoding. \n\n3. The new dataset SA-OVRS is a large one, with open-vocabulary supervision which improve the performance in multiple tasks."}, "weaknesses": {"value": "1. The proposed method has lower performance on some tasks when comparing to discriminative models\n\n2. The tokens that used has fixed downsample of ×16, which can miss fine details\n\n3. The usage of autoregressive model has some latency issues which is much slower than discriminative models"}, "questions": {"value": "1. What is the latency of the proposed model? Is there any trade of between the performance and the runtime latency?\n\n2. Does using finer stride can improve the results of the proposed mode? for example if you use ×8 in the mask tokenizer does the performance imporve?\n\n3. How does the model behave for out of distribution data such as medical or aerial imagery without finetune?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5lqk2vgBgU", "forum": "RXkiFbiSvE", "replyto": "RXkiFbiSvE", "signatures": ["ICLR.cc/2026/Conference/Submission5853/Reviewer_v1ac"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5853/Reviewer_v1ac"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832107317, "cdate": 1761832107317, "tmdate": 1762918304509, "mdate": 1762918304509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LlamaSeg, an autoregressive framework for image segmentation that unifies multiple segmentation tasks under the paradigm of next-token prediction.  \nThe key idea is to reformulate segmentation as visual generation, encoding segmentation masks as discrete visual tokens through a VQGAN tokenizer and generating them autoregressively using a LLaMA-style Transformer.  \nTo support large-scale training, the authors construct SA-OVRS, a new dataset containing 2M segmentation masks annotated with over 5,800 open-vocabulary labels and textual descriptions.  \nAdditionally, a novel evaluation metric, average Hausdorff Distance (dAHD), is proposed to assess contour fidelity of generated masks.  \nExtensive experiments show that LlamaSeg surpasses existing visual generative models (e.g., Unified-IO and Unified-IO2) on both semantic and referring segmentation benchmarks, while producing finer and more accurate mask boundaries."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### 1. Conceptual novelty  \nReformulating image segmentation as an autoregressive mask generation problem is a creative and elegant extension of large language model principles to pixel-level prediction.  \nThis perspective bridges the gap between generative modeling and structured visual understanding.  \n\n### 2. Unified framework  \nThe proposed approach enables seamless integration of segmentation tasks into LLM-based architectures through a consistent tokenization and generation pipeline.  \nIt provides a promising step toward unifying pixel-level vision tasks with autoregressive multimodal modeling.  \n\n### 3. Dataset contribution  \nThe introduced SA-OVRS dataset offers large-scale, open-vocabulary segmentation data paired with rich textual descriptions.  \nThis resource can support future research in open-vocabulary and multimodal segmentation.  \n\n### 4. Evaluation rigor  \nThe introduction of the dAHD metric is an insightful addition, providing a more nuanced measure of boundary accuracy and contour fidelity compared to traditional IoU-based metrics.  \n\n### 5. Empirical validation  \nComprehensive experiments across multiple benchmarks and datasets demonstrate consistent quantitative and qualitative improvements, validating the effectiveness of the proposed framework."}, "weaknesses": {"value": "### 1.  Limited scope and contribution\nThe contribution feels more foundational within a narrow scope rather than broadly transformative.  The method primarily focuses on segmentation and language alignment, without clear extensions to other modalities or tasks such as vision-language reasoning, instruction following, or general multimodal generation.  Compared with highly integrative multimodal frameworks like Unified-IO, 4M-21 (Bachmann, Roman, et al. \"4m-21: An any-to-any vision model for tens of tasks and modalities.\" Advances in Neural Information Processing Systems 37 (2024): 61872-61911.), this work appears less comprehensive and serves more as an initial step toward unifying segmentation within the LLM paradigm rather than a fundamentally new multimodal foundation.\n\n### 2. Incomplete comparison with recent foundation models\nBeyond Unified-IO, there exist broader any-to-any vision models such as 4M-21, which support a wider range of tasks and modalities while achieving comparable semantic segmentation results.  The paper should include a direct comparison and a detailed discussion to better contextualize its contribution relative to such works.\n\n### 3. High complexity and unclear efficiency\nThe proposed segmentation process is quite complex. It involves label generation, mask matching, and a separate inference step for each mask.  This design is inefficient for dense semantic segmentation scenarios.  The paper should report inference time, throughput, and computational cost to clarify the practical feasibility of the approach."}, "questions": {"value": "1. Could the authors clarify whether the model can generalize beyond segmentation to other vision-language tasks, such as referring expression comprehension or open-ended visual reasoning? \n\n2. The segmentation process appears computationally heavy, requiring separate inferences for each mask.  Could the authors provide concrete runtime statistics (e.g., FPS, latency per image, GPU hours) and discuss possible optimizations for dense segmentation settings?\n\n3. Since the model leverages VQ-based mask tokenization, how sensitive is it to the quality of the VQ tokenizer?  Would retraining or substituting the tokenizer significantly impact segmentation accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lb76fewQ9c", "forum": "RXkiFbiSvE", "replyto": "RXkiFbiSvE", "signatures": ["ICLR.cc/2026/Conference/Submission5853/Reviewer_A2WG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5853/Reviewer_A2WG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834943999, "cdate": 1761834943999, "tmdate": 1762918304217, "mdate": 1762918304217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}