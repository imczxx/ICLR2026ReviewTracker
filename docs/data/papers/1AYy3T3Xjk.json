{"id": "1AYy3T3Xjk", "number": 21916, "cdate": 1758323535142, "mdate": 1759896896824, "content": {"title": "A Process-Level Method for Creativity Evaluation in LLM-Assisted Learning", "abstract": "Interpretable creativity assessment remains challenging, and the adoption of large language models (LLMs) in education amplifies issues of subjectivity and opacity. This study presents a process-level evaluation approach for LLM-assisted learning that attributes learner-versus-model contributions from multi-turn student–LLM dialogues and scores four expert-elicited dimensions with rationale texts. Using 1,273 cleaned dialogues from 81 undergraduates across multi domains, an auditable attribution protocol and an instruction-tuned evaluator are introduced to produce process-linked, interpretable rationales. Empirical evaluation with expert assessments indicates alignment with expert judgments. Claims are explicitly scoped to the studied tasks and domains, and code and evaluation scripts will be released for reproducibility.", "tldr": "", "keywords": ["LLM", "Creativity assessment", "Process-level evaluation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38e25da61a3966c2d3c11c1757b7060aa373288d.pdf", "supplementary_material": "/attachment/05ade180f3ee52d24da22a18427f38e11a1b8e1f.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes a process-level framework for evaluating the creativity processes of human–LLM collaboration. It consists mainly of three components: (1) redefined metrics—CREDO creativity dimensions, (2) ITA-based attributions, and (3) a fine-tuned evaluation model.\nTo validate their method, the authors curated a dialogue dataset and conducted expert annotations as ground-truth labels.\nThe results show that the fine-tuned model aligns more closely with expert scores than the baselines (GPT-4 zero-shot and non-tuned DeepSeek-32B)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This work curates a dataset with 1,273 expert-annotated dialogues covering multiple domains.\n* It provides both qualitative and quantitative analyses and employs multiple evaluation metrics (Pearson, MAE, QWK, etc.) as well as inter-rater agreement measures to ensure alignment and validity."}, "weaknesses": {"value": "* The proposed framework is heuristic. I do not see a clear correspondence between the classical four dimensions and the four CREDO dimensions in Table 1, and the authors do not provide strong theoretical foundations for constructing these new dimensions.\n* Similarly, the ITA deconstructs dialogues into origination nodes, development nodes, and scaffolding supports; however, the paper lacks detailed explanation of the logic and robustness behind this step-by-step construction. This process may heavily depend on the authors’ subjective interpretation, which could introduce bias.\n* The work only compares its method against two baselines and does not report the performance of state-of-the-art models. Even considering cost constraints, GPT-4o would have been a cheaper and more capable alternative than the GPT-4 model used in this study.\n* The paper provides insufficient details about the prompts for evaluation models and expert annotation instructions, limiting reproducibility."}, "questions": {"value": "* How were the four CREDO dimensions selected? Are they intended to be orthogonal and to comprehensively capture the dimensions of creativity? For instance, Interdisciplinary Innovation and Risk-Driven Innovation both appear to assess aspects of innovation and may overlap. Could the authors provide a concrete example that clearly illustrates how these four dimensions differ in practical evaluation?\n* What are the exact prompts, instructions, or criteria for the 1–5 scoring scale used by both the model and the experts? My understanding is that LLM judges are highly prompt-sensitive and often exhibit one-sided bias (e.g., tending to give scores of 3–5 while rarely assigning 1 or 2). How does this work address such issues?\n* What is the distribution of the “gold-standard” expert scores across the training, validation, and test subsets? To evaluate student involvement/contribution effectively, each level of involvement/contribution should contain a sufficient number of cases. If the distribution is too narrow, the positive results reported might simply reflect the model’s tendency to fit to certain frequent score ranges (similar to the bias issue mentioned above).\n* From a higher-level perspective, what practical scenarios can this framework be applied to, and how could it be extended further?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0kWLKEUOvs", "forum": "1AYy3T3Xjk", "replyto": "1AYy3T3Xjk", "signatures": ["ICLR.cc/2026/Conference/Submission21916/Reviewer_woVg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21916/Reviewer_woVg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761141668500, "cdate": 1761141668500, "tmdate": 1762998283106, "mdate": 1762998283106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CREDO, a process-level evaluation framework for assessing creativity in LLM-assisted learning. Unlike classical tests (TTCT, AUT), CREDO focuses on dialogue process traces, separating human vs. LLM contributions through an Innovation Tracing Atlas (ITA) and scoring four new creativity dimensions (Interdisciplinary Innovation, Problem Reframing, Risk-Driven Innovation, Resource Integration Efficiency).\n\nAn instruction-tuned DeepSeek-32B model is fine-tuned (using LoRA + Knowledge Distillation) to predict 1–5 scores and rationales, trained on 1,273 annotated dialogues. Empirical evaluation shows a Quadratic Weighted Kappa (QWK) of 0.728 (≈90% of human reliability ceiling), with an attribution F1 of 0.84. The dataset is claimed to be ethically collected and reproducible."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Timely problem: \nThe paper tackles an increasingly critical question: how to assess human creativity in an era of pervasive LLM support.\n2. Conceptual novelty: \nThe process-level perspective (tracing human cognitive trajectories) is fresh and potentially impactful.\n3. Interpretable design: \nCombining score with rationale and transparent attribution mechanisms is commendable.\n4. Dataset collection validity:\nUsing ongoing course projects allows students to explore topics they already find relevant, reducing artificiality."}, "weaknesses": {"value": "### 1. Ambiguous Causal Claims\nThe paper makes a strong and central claim that its framework “traces the cognitive trajectory of creative thinking.” However, this assertion remains conceptually plausible but empirically unverified. The study presents correlational evidence, model outputs that align with expert judgments on dialogue data, but no temporal or causal validation that demonstrates the framework genuinely captures the evolution of creative cognition.\n\nTo substantiate the claim of tracing cognitive trajectories, one would expect to see process-causal analyses such as: (1) longitudinal correlations between CREDO-derived process indicators and subsequent creative achievements or outputs, or (2) human post-hoc interviews or think-aloud protocols to verify whether identified “origination” and “development” nodes correspond to participants’ subjective sense of idea generation.\n\nWithout such evidence, the results demonstrate correlation rather than causation. The framework successfully maps interactions and assigns attributions, but it does not yet prove that these attributions reflect the underlying causal mechanisms of creative thought. As it stands, the paper captures surface patterns of dialogue behavior rather than validating that those patterns cause or constitute creative cognition.\n\n\n### 2. Statistical Reporting Limitations\nThe paper’s statistical reporting lacks the depth necessary for confident interpretation of model performance. While mean metrics such as MSE, MAE, Pearson correlation, and Quadratic Weighted Kappa (QWK) are provided, the authors do not report confidence intervals, variance across cross-validation folds, or per-dimension error distributions.\n\nGiven the relatively small test set (128 samples), random variance could significantly influence the reported results. The observed improvement in QWK (0.728 for the fine-tuned model vs. 0.513 for GPT-4 and 0.342 for the baseline DeepSeek) appears substantial, yet the statistical significance of this improvement is not established. Bootstrapped confidence intervals or pairwise statistical tests (e.g., Fisher’s z-test for correlation or permutation tests for ordinal ratings) would be needed to determine whether these differences are meaningful rather than due to sampling noise.\n\nAdditionally, no error analysis by creativity dimension is included in the main text, even though later appendices reveal variability across dimensions. Reporting these results with appropriate variance measures and standardized effect sizes would clarify which creativity dimensions are reliably captured and which remain unstable.\n\n### 3. Potential Data Leakage\nA major methodological concern lies in the model ecosystem overlap between data generation and evaluation. Students interacted with the DeepSeek LLM during data collection, and the same model family (DeepSeek-32B) was later fine-tuned as the evaluator. This design introduces a significant risk of self-evaluation bias or data leakage at the stylistic level.\n\nThe evaluator may learn superficial linguistic or stylistic features characteristic of DeepSeek-generated text, enabling it to classify or score more accurately, not because it understands creativity, but because it recognizes its own generative patterns. For instance, DeepSeek’s distinctive discourse markers, lexical cohesion patterns, or turn-taking rhythms might act as unintended cues that correlate with specific CREDO scores.\n\nTo mitigate this concern, the authors should conduct cross-model generalization tests, evaluating dialogues generated using a different assistant model (e.g., GPT-4, Claude, or Mistral), to confirm that the evaluator’s performance persists beyond its native language patterns. Alternatively, a style-controlled or paraphrased dataset could assess whether performance drops when superficial linguistic features are normalized. Without such analyses, it remains unclear whether the system is genuinely assessing creativity or merely detecting DeepSeek’s conversational fingerprint.\n\n\n### 4. Dataset collection\n#### (1) Task framing\nThe data collection protocol emphasizes academic inquiry tasks in STEM fields (e.g., rock classification, carbon emission modeling). While suitable for studying analytical reasoning, this framing inherently biases the observed behavior toward convergent and knowledge-based reasoning rather than divergent or imaginative creation. Participants are more likely to synthesize or reformulate factual information than to produce novel conceptual constructs, limiting the ecological range of creativity being captured.\n\n#### (2) Absence of motivation manipulation\nParticipants were not explicitly instructed to “generate original ideas,” “take creative risks,” or “explore unconventional solutions.” In creativity research, such goal framing is critical: without motivational priming, individuals tend to default to task-completion strategies rather than expansive ideation. Consequently, much of the observed dialogue likely reflects problem-solving or academic reasoning, not genuine creative exploration.\n\n#### (3) Time constraint\nEach dialogue was capped at a maximum of 30 turns, with an average of fewer than 10. Creative cognition, however, often involves incubation and iterative recombination, requiring time for reflection and restructuring. A short interaction window may prematurely truncate these processes, reducing the opportunity for authentic creative leaps.\n\n### 5. Scope of creativity measured\nThe type of creativity captured by the study is best described as adaptive scientific or analytical creativity under LLM mediation, rather than open-ended or expressive creativity. Although the framework effectively documents how students interact with a large language model to refine and extend ideas, the creative behaviors observed remain bounded by the task structure and the cognitive affordances of the dialogue format.\n\nWithin this setting, students demonstrate certain forms of constructive and integrative thinking. For instance, they engage in problem reframing, such as transforming a classification question into a modeling or prediction challenge, or cross-domain linking, such as relating geological pattern recognition to convolutional neural networks in computer vision. These behaviors reflect valuable aspects of creative inquiry—they show flexibility, synthesis, and the ability to transfer knowledge across domains.\n\nHowever, such creativity is fundamentally situational and instrumental. The dialogues promote analytical exploration and knowledge integration, but they rarely foster divergent ideation or imaginative generation—the kind of creativity that involves proposing novel metaphors, aesthetic concepts, inventions, or speculative ideas that extend beyond the given problem space. The framework captures how effectively students navigate within known cognitive and disciplinary boundaries, not how they transcend them.\n\nA key factor limiting the expressive range of creativity lies in the dual role of the LLM itself. The model acts as both a creative amplifier and a creative filter. On one hand, it scaffolds ideation by providing examples, explanations, and domain connections that can inspire students to think more broadly. On the other hand, it constrains the conceptual search space to the statistical and semantic regularities of its own training data. Consequently, student–LLM interactions are guided toward plausible and conventional combinations rather than toward radical novelty or risk-taking. The result is a form of bounded creativity, oriented toward optimization and coherence rather than surprise or aesthetic invention.\n\nFrom this perspective, the creativity being measured is processual and pragmatic, focused on reasoning quality and interdisciplinary synthesis rather than on originality in the strong sense of the term. It reflects what might be called “creative inquiry competence”—the ability to collaborate productively with an AI system to reformulate problems, integrate evidence, and explore solution pathways—rather than “creative cognition” in its broader, generative, or expressive manifestations.\n\nIn this light, the data collection strategy and the resulting evaluation framework are methodologically sound but conceptually narrow. They provide valuable insight into how students co-develop ideas with LLMs and how such processes can be quantified, but they do not yet encompass the full spectrum of creative thought recognized in cognitive science, psychology, or the arts. Accordingly, the framework’s claims should be reframed from “creativity evaluation” to “creative inquiry evaluation.”\n\nFuture work should expand the empirical scope to include divergent and expressive tasks, for example, open-ended design problems, creative writing, or interdisciplinary invention challenges, where participants are encouraged to take conceptual risks, generate original constructs, and depart from established solution patterns. Only through such extensions can the framework legitimately claim to measure the broader construct of creativity rather than its current, narrower variant of collaborative analytical innovation."}, "questions": {"value": "1. The paper refers to “creativity evaluation” in broad terms. Could you explicitly define whether CREDO targets general creativity, domain-specific creative inquiry, or LLM-mediated problem solving?\n\n2. How do you conceptualize the boundary between creative reasoning and effective analytical reasoning in your framework? What makes a response “creative” rather than simply “high-quality reasoning”?\n\n3. During data collection, were students given any specific prompts or instructions emphasizing originality, risk-taking, or novelty, or were they simply asked to pursue academic inquiries?\n\n4. The paper links CREDO to Bloom’s Taxonomy and the PISA framework.\nCould you briefly elaborate on how each of the four CREDO dimensions maps onto these established theories in concrete operational terms (e.g., specific cognitive operations or learning behaviors)?\n\n5. To what extent do you view the CREDO framework as model-agnostic?\nCould it, in principle, be applied to dialogues generated by other LLMs or even human–human collaborations without retraining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6S5ki4DMQs", "forum": "1AYy3T3Xjk", "replyto": "1AYy3T3Xjk", "signatures": ["ICLR.cc/2026/Conference/Submission21916/Reviewer_J6hi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21916/Reviewer_J6hi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468438532, "cdate": 1761468438532, "tmdate": 1762941979899, "mdate": 1762941979899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CREDO, a process-level framework for evaluating creativity in human–LLM collaborative learning. Instead of judging only final artifacts, the method analyzes multi-turn student–LLM dialogues to (i) attribute learner vs. model contributions via an Innovation Tracing Atlas (ITA), and (ii) score four process-centric dimensions—Interdisciplinary Innovation, Problem Reframing, Risk-Driven Innovation, and Resource Integration Efficiency—using an instruction-tuned evaluator that outputs 1–5 ratings with rationales. The study curates a dataset of 1,273 cleaned dialogues from 81 undergraduates across multiple domains, reports high inter-rater reliability for expert annotations (weighted κ=0.81; Cronbach’s α=0.86), and fine-tunes a DeepSeek-32B model with LoRA (plus knowledge distillation) to produce scores and concise explanations. On the held-out test set, the evaluator achieves QWK=0.728 (≈90% of human ceiling 0.81), r=0.811, and MAE=0.505; a targeted experiment suggests macro-F1=0.84 for learner–vs–LLM attribution categories. Claims are scoped to STEM-leaning academic inquiry contexts, and the authors plan code/evaluation release."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Moves beyond outcome scoring by elevating dialogue trajectories as primary evidence and explicitly attributes human vs. LLM roles; defines four process dimensions tailored to collaboration (vs. Torrance-style outputs). \n\n2. Ethical data collection; multi-stage cleaning/standardization; double-blind expert annotation with arbitration; high IRR (κ=0.81, α=0.86); clear objectives and ablations; teacher–student KD + LoRA for practicality. \n\n3. Concrete workflow figure; CREDO vs. classical mapping; precise loss definitions; interpretable score+rationale outputs; helpful ITA visualization."}, "weaknesses": {"value": "1. The dataset (81 undergraduates; two universities; STEM-oriented tasks) constrains generalization to broader populations (K-12, humanities/arts, diverse cultures/languages). The paper acknowledges this but evaluation remains single-context. Actionable ask: run cross-institution and non-STEM validations (even small pilots) to probe transportability. \n\n2. Comparing only to GPT-4 zero-shot and untuned DeepSeek-32B underestimates strong alternatives (e.g., prompt-programmed judges, few-shot rubric-prompting, instruction-tuned evaluators without LoRA, calibrated ordinal regressors over handcrafted features). Actionable ask: add tuned LLM-judge baselines (few-shot rubric, chain-of-thought with rubric anchors) and a non-LLM baseline (e.g., logistic/ordinal regression over process features). \n\n3. The attribution experiment (macro-F1=0.84) relies on expert-labeled categories on the same type of data used to train the evaluator. This is valid for alignment to experts but leaves open whether attribution corresponds to causal contribution or downstream learning gains. Actionable ask: show that high attribution quality predicts independent outcomes (e.g., subsequent task performance, transfer, rubric-blind human judgments). \n\n4. Main metrics lack confidence intervals, per-dialogue variance, and significance tests between models. QWK and r are informative, but error analysis is thin (few failure cases, limited per-dimension uncertainty). Actionable ask: add bootstrap CIs, paired significance, and calibration metrics for ordinal predictions. \n\n5. The semantic drift filter (cosine <0.15 for three consecutive pairs), cluster-then-stratify split (k=50), and ITA node definitions could influence results; there is no sensitivity analysis. Actionable ask: report robustness to cleaning thresholds, k, and ITA labeling variations; include prompt perturbation tests for the evaluator. \n\n6. No breakdowns across demographics, domains, or dialogue lengths; potential bias if certain discourse styles are favored. Actionable ask: provide subgroup QWK/MAE and differential item functioning checks. \n\n7. The method presumes access to multi-turn logs and an evaluator pass; latency/compute and annotation cost (for gold standards) are not quantified. Actionable ask: report inference cost, throughput, and a human-in-the-loop review budget for classroom deployment."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NtjBZzJPUd", "forum": "1AYy3T3Xjk", "replyto": "1AYy3T3Xjk", "signatures": ["ICLR.cc/2026/Conference/Submission21916/Reviewer_Cp81"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21916/Reviewer_Cp81"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704894703, "cdate": 1761704894703, "tmdate": 1762941979389, "mdate": 1762941979389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new framework called CREDO to assess creativity in human–LLM collaboration. Unlike traditional creativity assessments that focus on final outputs, CREDO emphasizes the process of idea generation and reasoning. It combines two key components: (1) the Innovation Traceability Atlas, which breaks down multi-turn student–LLM dialogues into cognitive steps (questioning, reframing, integrating, generating) and distinguishes between human and model contributions; and (2) an instruction-tuned evaluator, fine-tuned on the DeepSeek-32B model using LoRA and knowledge distillation, that produces interpretable creativity scores (1–5) along four new process-oriented dimensions: interdisciplinary innovation, problem reframing, risk-driven innovation, and resource integration efficiency. Experiments on 1,273 student–LLM dialogues show that the fine-tuned model achieves 90% of human-level agreement (QWK = 0.728) and can reliably distinguish student vs. model contributions (F1 = 0.84)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Propose a new creativity evaluation method with the help of LLM which can assess creativity in real time and can be used in daily life.Traditional questionnaire-based methods cannot evaluated in real time but just offer snapshots, and recording transcript-based methods can hardly be used in daily life.\n2. An interesting application for the crucial dilemma of controlling students to use LLMs as assistants: strict control impedes students to use new tools while no control leads to creativity decay. While there are many tools like AI-generated text detection that try to prevent students from using AI too often, new AI models can always hack those detection algorithms as they are stronger. The proposed creativity evaluation method can be used in the communication between students and LLMs, in which we not only let the students use tools, but also get the creativity as a metrics to prevent students use LLMs too much."}, "weaknesses": {"value": "Limited Dataset and Generalizability – The dataset includes only 81 undergraduate students from STEM domains, restricting applicability to other disciplines or educational levels.\n\nImplementation Details Missing – The paper lacks practical training details such as hardware setup, fine-tuning time, and exact hyperparameter values needed for reproducibility.\n\nInconsistency in Methodological Description – The paper claims to use a “fully fine-tuned teacher model” for knowledge distillation but also states that full fine-tuning is “computationally prohibitive,” creating a logical contradiction.\n\nLack of Transparency in Review References – Mentioning “Area Chair comments” during the review phase is inappropriate for a double-blind submission, suggesting possible misunderstanding or template-based phrasing.\n\nOverly Polished and Synthetic Writing Style – The writing is highly formal, repetitive, and uniformly structured, which, combined with perfect consistency in technical phrasing and reference formatting, gives an impression of automated generation."}, "questions": {"value": "1. Is it possible to substitute human expert annotators with LLMs, that is to say, you can automate the whole data process pipeline and only need expert to check the data after finish process instead of annotate each data manually.\n2. In line 316, \"to address the core concern raised by an Area Chair regarding whether, XXXX\", who is the Area Chair? Why you can recieve comments from AC before the ICLR submission deadline?\n3. About Table A2, row of \"w/o LoRA (Full Fine-tuning)\", the author do not provide experiment here because of \"Computationally prohibitive\". I am consued, if you cannot full fine-tuning a LLM, where do the authors got the teacher model to conduct knowledge distillation. In line 312-313, The author claim that \"A Teacher is obtained via full-parameter FT on the same training set\", which seems contradictory with the authors' explanation of why do not get the experiment result of \"w/o LoRA\".\n4. The authors lack \"implementation details\" section. Readers need to know the size of the datasets, configuration of the server, and training time to determine if it is possible to reproduce the author's experiment on their own computer.\n\n\nTypos:\n1. Line 74: engi -neering => engineering\n2. Line 75: screen- ing => screening\n3. Line 78: overlook -ing => overlooking\n4. Line 81: meaning -ful => meaningful"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper should explicitly report ethical approval and detailed procedures for user experiments to meet HCI and educational research standards (e.g., IRB review, informed consent, participant demographics, and data handling)."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WtYVSc2PGG", "forum": "1AYy3T3Xjk", "replyto": "1AYy3T3Xjk", "signatures": ["ICLR.cc/2026/Conference/Submission21916/Reviewer_PNVN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21916/Reviewer_PNVN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843982713, "cdate": 1761843982713, "tmdate": 1762941979167, "mdate": 1762941979167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}