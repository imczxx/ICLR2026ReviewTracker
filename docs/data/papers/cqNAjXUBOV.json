{"id": "cqNAjXUBOV", "number": 25257, "cdate": 1758365869040, "mdate": 1759896727873, "content": {"title": "Tables2Traces: Distilling Tabular Data to Improve LLM Reasoning in Healthcare", "abstract": "Large language models (LLMs) excel at reasoning when fine-tuned on curated text corpora, but many domains, such as medicine, primarily store knowledge in structured tabular data. Despite its richness, tabular data has been largely overlooked as a source of reasoning supervision. Interpreting such data requires structured, relational reasoning across features and outcomes, not just surface-level pattern matching. In practice, this mirrors clinical decision making, where doctors often compare patients with similar characteristics and reason about why their outcomes diverge. We introduce Tables2Traces, the first framework to enable improved reasoning from raw tabular data by generating contrastive, case-based reasoning traces for model fine-tuning. This establishes a new supervision paradigm: converting tabular records, traditionally used only for prediction, into structured reasoning signals that can serve as an effective new source of supervision for LLMs. Crucially, this paradigm is orthogonal to text-based QA supervision: rather than competing with curated corpora, it unlocks an abundant and low-cost modality that complements existing approaches. Using only cardiovascular patient records, Tables2Traces yields relative gains of 17.2% on in-domain MedQA questions and 8.4% out-of-domain, improving accuracy in 15 of 17 clinical categories. On MedMCQA, it achieves a 7.2% relative improvement and outperforms the base model in 17 of 21 specialties. These gains are driven by a lightweight, domain-agnostic pipeline that elicits structured reasoning via contrastive and counterfactual prompts. Compared to training on narrative patient descriptions, Tables2Traces generalizes more effectively across question types and medical specialties, showing that even limited tabular data can serve as a scalable and complementary source of reasoning supervision for LLMs.", "tldr": "We convert tabular clinical data into reasoning traces that improve LLM medical question answering across domains.", "keywords": ["large language models", "tabular data", "healthcare", "medicine"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ece73eadbb7d312ff9edc26b94ef3ddb0be07036.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Tables2Traces, a framework that turns tabular clinical data into reasoning traces to fine-tune large language models. Using cardiovascular data from UK Biobank, the method shows clear improvements on MedQA and MedMCQA, demonstrating that structured data can provide useful reasoning supervision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Novel data-centric perspective.**\n\nThe paper introduces an interesting data-to-trace distillation framework that converts structured tabular data into natural language reasoning traces, providing a fresh and potentially impactful perspective on how non-textual data can be used to enhance LLM reasoning. This idea is interesting.\n\n- **Clear motivation and methodology.**\n\nThe paper is well-organized and easy to follow. The motivation for leveraging structured clinical data is clearly articulated, and the experimental setup is described with sufficient detail for reproducibility."}, "weaknesses": {"value": "**Major Weakness:**\n\n- **Unverified impact on general reasoning.**\n\nWhile the method improves performance on medical QA benchmarks, the authors did not assess whether fine-tuning on domain-specific traces affects the model’s general reasoning or language understanding. No results are reported on standard reasoning benchmarks (e.g., GSM8K, MMLU, ARC), leaving the potential risk of catastrophic forgetting unexamined. It also remains unclear whether the observed gains stem from genuine reasoning improvement or from overfitting to the question–answering format.\n\n- **Narrow evaluation scope.**\n\nExperiments are restricted to two medical QA datasets of similar structure. It remains uncertain whether the proposed approach benefits other reasoning tasks such as clinical summarization, diagnostic inference, or structured-to-text generation. The generalizability across task types has not been demonstrated. Although the paper repeatedly claims that Tables2Traces is domain-agnostic, all training data originate exclusively from a cardiovascular-disease cohort. No experiments are conducted on other medical or non-medical domains. Consequently, it is unclear whether **this method** generalizes beyond this specific disease area, or whether the observed improvements arise from domain-specific correlations/biases within the cardiovascular dataset.\n\n- **Insufficient ablation and analysis.**\n\nOnly a coarse comparison between the “simple” and “full” variants is provided. The paper lacks finer-grained ablations isolating the contributions of the contrastive, counterfactual, and anchor components, as well as sensitivity analyses regarding the number or quality of generated traces.\n\n**Minor Weakness:**\n\n- **Trace quality unverified.**\n\nAll reasoning traces are automatically generated by an existing LLM without human or quantitative validation. Although the overall empirical gains suggest that these traces provide useful supervision in aggregate, the absence of quality verification weakens the claim that the model genuinely learns better reasoning rather than benefiting from correlated linguistic patterns, especially for the medical domain.\n\n- **Limited algorithmic novelty.**\n\nThe proposed framework introduces no new optimization objective or model architecture. It essentially performs standard instruction tuning on synthetic data obtained through prompt engineering. The contribution lies primarily in the data-to-trace distillation paradigm rather than in algorithmic innovation.\n\n- **Conceptual inconsistency with the stated motivation.**\n\nThe introduction emphasizes grounding LLM reasoning in domain-specific structured data, whereas the discussion later claims that the model learns “general reasoning patterns.” This interpretation conflicts with the paper’s initial motivation and lacks empirical support. The authors should clarify whether their goal is to enhance domain-grounded reasoning or to induce domain-agnostic reasoning transfer."}, "questions": {"value": "See Above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MCNxdS1IJ1", "forum": "cqNAjXUBOV", "replyto": "cqNAjXUBOV", "signatures": ["ICLR.cc/2026/Conference/Submission25257/Reviewer_m5kw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25257/Reviewer_m5kw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720922466, "cdate": 1761720922466, "tmdate": 1762943380457, "mdate": 1762943380457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed TABLES2TRACES, a textual supervision corpus generation pipeline using structured tabular data to improve LLM reasoning competency. Backgrounded in medical QA tasks, TABLES2TRACES process each tabular entry into a corpus sample in four steps: (1) feed detailed feature content to a base LLM to generate a textual summary; (2) find nearest two entries from survived tabular entries and deceased ones using Gower distance; (3) organize the summaries of these three entries with the designed prompt template as corpus inputs; (4) pass the prompt to the base LLM to generate reasoning trace as corpus target. Notably, 90% of the final fine-tuning corpus is TABLES2TRACES-converted samples with the rest 10% from QA datasets. Experiments on MedQA and MedMCQA benchmarks show the consistent gains brought from the converted tabular supervision corpus, and comparison with fully QA-optimized models are conducted, indicating potential usability of TABLES2TRACES."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "## originality\nStructured tabular data could be potential form of supervision corpus to enhance LLM reasoning competency: This paper assume a possible scenario where users have limited access to curated unstructured text corpus while rich tabular data exists, and TABLES2TRACES provides a tool to enable LLMs to consume such structured modality, and the paper firstly verified such usability.\n\n## significance \nAlthough cannot surpass optimized models supervised by curated QA corpus in the paper, the experiment results still suggest a future trend that tabular data has potential to be a supplementary or even alternative supervision source for LLM reasoning tasks."}, "weaknesses": {"value": "## Overall Assessment\nFrom my opinion, the empirical contribution of TABLES2TRACES far outweighs its technical contribution. The paper verified supervision usability of tabular data  modality on LLM reasoning tasks, while the core component for tabular-to-corpus conversion is driven by heuristic prompt engineering, all rely on LLM’s own capability, thus it is hard to convey clear technical inspiration to audience, or hard to figure out whether such prompt design is optimal. Besides, the experiment is not rigorous enough to fully support the paper conclusion. Several points are detailed bellow.\n\n## Method\n-\tIn “Contrastive Neighbor Selection” we use Gower distance (line 163) to fetch similar entries, while whether the selection of other distance metric functions will impact the results is not discussed.\n-\tIn “Reasoning extraction via prompt design” we assign all tasks, including “Differential reasoning” to list decisive feature difference, “Label Plausibility” to recognize label noise, “Counterfactual Planning” to suggest one minimal feature edit, to the LLM itself rather than alternative machine learning methods (e.g., calculate feature importance with tree models, noisy label learning to recognize label plausibility, and Shapely Value to find sensitive features), the superiority of such “all done with LLM” methodology is not clear.\n\n## Experiment\n-\tApart from tabular-generated texts, the final training corpus also contains 10% QA examples from MedQA (or MedMCQA) datasets, which hinders the direct recognition on performance gain brought by tabular data, or why we include QA examples for training is not explained.\n-\tAlthough ablation study on “LLM only tuned with these 10% QA examples” is conducted and performance decline is observed, this result cannot demonstrate that the QA corpus has no positive effect on the main experiment results, since the usability of the data depends on its scale.\n\n## Others\n-\tIn related work of line 111-113, TabNet and FT-Transformer is transformer-based small neural networks rather than language models for tabular prediction, maybe recent works like LLaMA-GTL[1], TP-BERTa[2] are more suitable.\n-\tLine 164, the meaning of denotation s is not given.\n-\tOver-claimed contributions, line 81, “without human annotation” is not strict, since some labeled tabular data also requires human annotation; “without QA corpora” while the main experiment still include QA examples.\n\n### Reference\n\n[1] From Supervised to Generative: A Novel Paradigm for Tabular Deep Learning with Large Language Models, KDD 24.\n\n[2] Making Pre-trained Language Models Great on Tabular Prediction, ICLR 24."}, "questions": {"value": "1. How about the results of only using tabular-generated corpus for the main experiment?\n2. see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ci3t9iScJi", "forum": "cqNAjXUBOV", "replyto": "cqNAjXUBOV", "signatures": ["ICLR.cc/2026/Conference/Submission25257/Reviewer_q4yp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25257/Reviewer_q4yp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810388692, "cdate": 1761810388692, "tmdate": 1762943380272, "mdate": 1762943380272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores using tabular data to generate contrastive reasoning examples for LLM fine-tuning in the clinical domain. While fine-tuning with contrastive reasoning examples is an established approach, automatically generating these reasoning examples from tabular data is a novel contribution. Past work on LLMs for tabular data has mostly focused on enhancing LLM understanding of tabular data, which is not the focus of the paper. \n\nThe contrastive reasoning examples are generated from tabular data through a multi-step process. First, an LLM is prompted to transform the tabular data into textual summaries. Next, triples are formed by finding nearest survivors and nearest deceased patients (by Gower distance). For each triple, an LLM is again prompted to generate a contrastive reasoning example, with explicit direction provided intended to enhance differential reasoning, label plausibility, and counterfactual planning. (Human experts provide a qualitative evaluation of their generated contrastive reasoning examples.) \n\nThe authors report results on common medical QA benchmarks and compare their method to the SOTA model Aloe. They also consider fine-tuning based on patient narratives generated from the tabular data and find that contrastive reasoning examples perform better. Finally, they demonstrate the potential of their method to enhance performance on out-of-domain questions."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper explores an original idea (supervised fine-tuning for clinical reasoning on contrastive reasoning examples automatically generated from tabular data) with high potential. \n\n- The paper is clearly written and well-situated with respect to the most important related works in LLM fine-tuning and LLMs for tabular data. \n\n- The experimental analysis shows clear gains in out-of-domain questions and demonstrates the effectiveness of using tabular data to generate specifically contrastive reasoning examples rather than patient narratives."}, "weaknesses": {"value": "- The authors outline that their method is intended to instill the model with three key reasoning competencies: differential reasoning, label plausibility, and counterfactual planning. However, experiments are only done on QA benchmarks. While the experiments in the paper demonstrate the potential of their method to enhance medical QA, they do not directly evaluate how it impacts the model’s reasoning ability or the three competencies they highlight. If suitable benchmarks are not available, human evaluators might still provide some qualitative insight. At minimum, this limitation should be clearly highlighted and claims about the method’s improvements on reasoning (e.g. “promotes causal and actionable reasoning”) rather than QA performance should be tempered. \n\n- No examination is provided for the scalability of their fine-tuning method at different data sizes. An experiment should provide performance analysis across differing amounts of tabular data and generated contrastive reasoning pairs, to help establish scalability and potential of the method. \n\n- The authors provide few comparisons of their method with other fine-tuning approaches, primarily comparing against the SOTA model Aloe. A comparison with a standard supervised fine-tuning method using (a similar amount of) non-tabular data, or a demonstration of the compatibility of this approach with other methods for fine-tuning, could help establish the potential of the method in comparison to established alternatives."}, "questions": {"value": "- Aloe is highlighted as not benefiting from your method due to a misalignment. Does this not call into question the use of Aloe as the primary comparison point for your method? \n\n- The introduction includes the claim that “Tables2Traces closes part of the gap to a state-of-the-art model, Aloe, despite using only 2% of its QA data”. Does this 2% figure account for the 105k reasoning traces? If not, the statement seems misleading and warrants further clarification. \n\n- Could you give feedback and explanations in response to the weaknesses described above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p3H9Zsp4Ry", "forum": "cqNAjXUBOV", "replyto": "cqNAjXUBOV", "signatures": ["ICLR.cc/2026/Conference/Submission25257/Reviewer_BRY8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25257/Reviewer_BRY8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924763217, "cdate": 1761924763217, "tmdate": 1762943380007, "mdate": 1762943380007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Tables2Traces, a novel framework that utilizes structured tabular data (e.g., medical records) to improve the reasoning capabilities of large language models (LLMs). The framework converts tabular data into structured reasoning traces, enabling LLMs to perform more advanced reasoning tasks in the medical domain. By leveraging data such as cardiovascular patient records, the paper demonstrates significant improvements in model performance for medical question answering tasks, both in-domain and out-of-domain."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The concept of transforming raw tabular data into structured reasoning traces for LLMs is novel. This approach bridges the gap between structured data (typically underutilized in LLM fine-tuning) and natural language reasoning, especially in domains like healthcare where structured data is abundant. The method is well-motivated, with clear applications in medical domains where data privacy and regulatory constraints often limit access to large, curated QA corpora. By using internal tabular data, this method makes LLM fine-tuning more accessible and efficient for domain experts in healthcare."}, "weaknesses": {"value": "While the framework is general, the experimental validation is confined to cardiovascular data and binary outcomes. Further exploration into multi-class settings or other medical specialties could enhance the generalizability of the results. The results suggest that the framework may overfit to patient-specific data, as indicated by the performance degradation on abstract or non-patient-centered questions. This limitation highlights the need for further work on mitigating overfitting, particularly in cases where patient descriptions are less representative. Although cardiologists reviewed the generated traces, it would be useful to evaluate the quality and trustworthiness of the reasoning traces in real-world clinical settings. The potential for errors or biased traces (informed by the dataset's limitations) could affect model performance and safety."}, "questions": {"value": "How well does the framework perform in other non-medical domains where structured data exists but differs significantly from the medical context? For example, does it perform equally well on financial data or educational datasets?\nGiven that the reasoning traces are synthetic, what measures are in place to ensure their fidelity? Could trace quality be improved with domain expert involvement to create more accurate and reliable reasoning signals?\nThe paper mentions that the traces are not designed for clinical decision-making. What steps will be taken to prevent misuse of the model in real-world healthcare settings? How can you ensure that the model's output remains interpretable and explainable in high-stakes environments like medicine?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qoFCJQCnEh", "forum": "cqNAjXUBOV", "replyto": "cqNAjXUBOV", "signatures": ["ICLR.cc/2026/Conference/Submission25257/Reviewer_cDRk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25257/Reviewer_cDRk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999019451, "cdate": 1761999019451, "tmdate": 1762943379716, "mdate": 1762943379716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}