{"id": "orzX69D5UO", "number": 7469, "cdate": 1758023480184, "mdate": 1759897851089, "content": {"title": "Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents", "abstract": "Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most models are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow integrity enforcement via a pre-generated \"Intent Graph\"; and (ii) an innovative \"Tiered Adjudicator\" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark demonstrate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves superior security with outstanding efficiency and enhanced robustness, thereby resolving the aforementioned multi-dimensional trade-off dilemma.", "tldr": "We introduce the Cognitive Control Architecture (CCA), a lifecycle supervision framework to defend AI agents against injection attacks.", "keywords": ["Cognitive Control", "Lifecycle Supervision", "Injection Attacks", "Adversarial Robustness", "AI Safety", "AI Agents"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/26cfdb8622df69bee89b35c57bfa937429408ce9.pdf", "supplementary_material": "/attachment/98dd55180fd942e0ae660ce903f4799cc12db94e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a new defense approach, Cognitive Control Architecture (CCA), aiming to guard against IPI attacks. They claim that this approach is superior to other existing approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Provided a detailed explanation of the proposed approach and they well motivated the question studied.\n- Included a holistic list of baselines.\n- Conducted detailed analyses on the experiment results."}, "weaknesses": {"value": "- No standard errors or error bars are reported in Table 1, which presents the main results.\n\n- Lines 371–372: “CCA uniquely balances elite-level security (0.34% ASR) with the highest functional retention (86.43% UA), completely resolving this long-standing problem.” — The wording here seems too strong; “completely resolving” would imply 0% ASR and 100% UA, which is not the case. Please tone down the phrasing.\n\n- Some tables and figures lack clear descriptions in their captions. For example, in Table 1, what is the underlying model used? (Lines 307 mention two models.) Also, what do “direct,” “ign. prev,” “sys. msg,” and “imp. msgs.” refer to? They are mentioned briefly in Line 310 but not explained in sufficient detail. Similarly, what does the y-axis represent in Fig. 4(b)?\n\n- In the Related Work section, it would be beneficial to include a paragraph or a few sentences discussing other types of attacks beyond IPI attacks that also emerge as LLMs become more agentic. For example, one emergent attack in agent setups is sequential decomposition attacks, e.g., the “Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors” paper, and many others."}, "questions": {"value": "My questions are what i said in the weaknesses.\n- Could you report the error bars for Table 1?\n- Could you tone down Line 371-372 to not say \"completely resolving\"?\n- Could you provide a complete description of the details in tables and figures? Especially the two I mentioned above.\n- Could you include a paragraph or a few sentences discussing other types of attacks beyond IPI attacks that also emerge as LLMs become more agentic (in addition to IPI attacks), how other current defenses are proposed for those?\n\nhappy to raise my score once all of these are addressed. Thank you for this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H56y5uh6yL", "forum": "orzX69D5UO", "replyto": "orzX69D5UO", "signatures": ["ICLR.cc/2026/Conference/Submission7469/Reviewer_fChp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7469/Reviewer_fChp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760930330511, "cdate": 1760930330511, "tmdate": 1762919584725, "mdate": 1762919584725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a safeguard for defending tool-use agents against indirect prompt injection attacks. The method requires the agent to start by creating a plan in the form of an intent graph comprised of all tool calls the agent will perform in order to execute the user request. Then, during execution, if the action or tool invocation is not in the intent graph, a tiered adjudicator serves as a last line of defense before harm is caused. The proposed approach is evaluated using primarily DeepSeek-V3.1 on AgentDojo. The proposed approach provides a Pareto improvement over the previous state-of-the-art defense mechanism on the evaluated dataset."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed safeguard demonstrates a Pareto improvement over state-of-the-art defenses against indirect prompt injection attacks.\n* The proposed approach is more efficient (in terms of tokens) than the state-of-the-art defense.\n* The ablations presented in Table 3 are beneficial to understanding why the method works."}, "weaknesses": {"value": "* The presentation quality is quite poor and the paper needs quite a bit of polishing. There are several typos throughout (Figure 2, first column: \"Chack\" -> \"Check\", third column: \"Adjustor\" -> \"Adjudicator\"?, line 383 \"¡\"?, inter alia). The figure and table captions are unclear or promise presentation not represented in the figure (e.g. Table 1, the caption promises that the best defense numbers should be bolded, but they are not). This does not inspire confidence in the results.\n* The writing is wordy and over-obfuscates the proposed approach and results. Much of the paper is large blocks of text that are difficult to follow and imprecise. The description of the proposed approach can be simplified and compressed to improve understanding.\n* The proposed safeguard is quite invasive to the actual implementation of the agent. It requires changing the implementation of the agent. It's unclear how easily this defense can be adapted to new models or models accessed via APIs.\n* The proposed safeguard is not evaluated against any adaptive attacks. It's fairly straightforward to create a defense against a static set of attacks. The results do not indicate that the proposed safeguard will generalize to novel attacks.\n* The proposed approach is only evaluated on two open models and one static benchmark. It is not obvious that this approach will generalize in practice to other models.\n* The proposed defense seems to be directly defending against one type of attack, i.e. indirect prompt injection on tool use agents. \n* Using temperature 0 does not necessarily enforce determinism (https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/). Also, this does not necessarily represent how tool-use agents are used in practice."}, "questions": {"value": "* Will we need to create new defenses for new types of attacks? It seems like this approach is not adaptable to different types of attacks? How will the efficiency of the agent be affected by integrating more invasive defenses?\n* What is Figure 3 showing?\n* Can the proposed guardrail be adapted to general tool use agents accessed via API? Could this be implemented by a third-party monitor? How easily can the proposed approach be adapted to a new agent?\n* Why does the intent graph necessarily need to be a DAG? How much does errors in the DAG affect the efficacy of the proposed defense?\n* What are Figures 4 (b)-(d) showing? Specifically, what is the y-axis?\n* Does the proposed approach generalize beyond indirect prompt injection? \n* Why does CCA improve UA in Table 2 for Kimi K2? Does the explicit planning and creation of the intent graph improve capabilities?\n* For each new tool, do you need to define a new $S_risk$? How is this chosen? It seems extremely heuristic."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wy2wGALrS7", "forum": "orzX69D5UO", "replyto": "orzX69D5UO", "signatures": ["ICLR.cc/2026/Conference/Submission7469/Reviewer_eSiz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7469/Reviewer_eSiz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747849371, "cdate": 1761747849371, "tmdate": 1762919584164, "mdate": 1762919584164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Cognitive Control Architecture (CCA), a multi-layer framework to defend an LLM agent against prompt injection attacks. The framework is composed of two layers: the first layer is used to construct the execution graphs from the user's intention. If the actual action deviates from the pre-planned graphs. The second layer is activated, composed of a weighted score of multiple dimensions to evaluate if the action is safe. In the experiment, the paper selects two LLMs (DeepSeek and KIMI), evaluating on AgentDojo. The result shows that CCA achieves state-of-the-art defense results and efficiency without decreasing the benign performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Insightful design**: The paper is novel in designing a multi-layer framework to inspect the agent action process. The framework is designed to inspect both the data-flow and the underlying intention to ensure a safe agent behavior.\n- **Promising results**: The paper shows promising results in defending against prompt injection attacks in AgentDojo benchmarks, surpassing previous work or achieving comparable performance in lowering the attack success rate. Meanwhile, the proposed methods don’t sacrifice the benign performance. Additionally, the API overhead is also reduced to less than half of MELON.\n- **Comprehensive comparisons of related works**: The paper compares the proposed methods with four related approaches. The comprehensive comparison further justifies the advantage of the proposed framework."}, "weaknesses": {"value": "## Major\n\n\n- **Lack of evaluation dataset and models**: The paper mainly evaluated the results on one dataset (AgentDojo), using two LLMs (DeepSeek and KIMI). The authors are expected to conduct experiments on multiple datasets and models to support the generalization of the proposed methods.\n- **Lack of experimental justification of Graph Updated**: The paper proposes to dynamically update the graph, but lacks of ablation study on how the design will influence the benign utilization and attack effectiveness. The author is suggested to conduct experiments to justify this claimed methodology design.\n\n\n## Minor\n- **Visible portion of typos**: The paper has a visible portion of typos, which influences the readability. For example:\n  - No space before citation in Lines 41, 43, and 46. \n  - No space in Line 167 after *”(Pillar I)”* and Line 232 before *”The dynamic”*. \n  - In Table 2, the BU results of CCA (e.g., 86.6%) don’t match the main content in Line 400 (e.g., 84.54%). \n  - In Figure 2, the Pillar II is “Tiered Adjustor”, which is inconsistent with the main paper as “Tiered Adjudicator”.\n- **Strong feeling of LLM writing**: Even though the paper declares the LLM usage in writing, the strong feeling of LLM writing (e.g., using an extensive amount of uncommon expressions) might jeopardize the readability. I listed several sentences below that I feel might be written by LLMs:\n  - (Line 043) their **inherent cognitive fragility—manifesting as a lack of robust risk awareness**…\n  - (Line 047) This enables attackers to unlawfully **steer tool invocations**...\n  - (Line 050) as **stringent controls** impair agent capabilities\n  - (Line 057) **forcing an untenable trade-off**\n  - (Line 067) To **break this impasse**,...\n- **Presentation issue**: Table 1 is confusing: the ASR seems to be applied to all columns, but the BU column is evaluated using the benign utility metrics. In Figures 3 and 4, the text is too small compared to the main content."}, "questions": {"value": "- What if the action is correct (thus can pass the check for Pillar II) and is being executed? Specifically, the pipeline only checks if the action is correct according to the intention graph. What if the prompt injection is targeted at modifying the value of a certain action? For example, a transaction of $10,000, instead of 100. How can the proposed framework defend against this type of attack?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "chqTE95QHN", "forum": "orzX69D5UO", "replyto": "orzX69D5UO", "signatures": ["ICLR.cc/2026/Conference/Submission7469/Reviewer_x7VM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7469/Reviewer_x7VM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860075359, "cdate": 1761860075359, "tmdate": 1762919583865, "mdate": 1762919583865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an approach that leverages few-shot learning to handle perception uncertainties and anomalous inputs in autonomous driving systems by leveraging information geometry-guided dimensionality reduction, which decouples high-dimensional text embeddings into driving-relevant features (spatial relationships, temporal dynamics, physical constraints) while preserving contextual reasoning capabilities. This paper demonstrates that their approach can achieve a 24.93% average collision rate on UniAD, outperforming GPT-Driver by 22% under normal conditions and showing only 14.9% performance degradation under anomalies compared to 17-21% for existing LLM-based methods."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This paper tries to address an emerging and important area of research, the safety of LLM-based autonomous driving. As our community and society pay close attention to this area, I am happy to see a paper submission in this area. I can see that their methodology achieves higher performance than baseline and existing methods in their evaluation."}, "weaknesses": {"value": "I have the following major concerns about this paper:\n\n### Critical presentation errors\n\nThis paper has a significant number of presentation errors across the paper. Particularly, this paper does not have any references to the figures in this paper, even though this paper has 4 figures.  This prevents me from fully being convinced of the reported result's validity. Furthermore, this paper does not clearly explain how their dataset constructed in Section 4.1 is used in the following evaluation with the datasets of the UniAD and ST-P3. These presentation errors are not at the level of a minor issue, but a major issue, leading me to the rejection side.\n\n### Lack of sufficient explanation about the experimental setup \n\nI do not fully understand the details of their evaluation setups. I can see that they constructed a dataset with anomalies extracted from the nuScenes dataset, but I am not fully sure about the details of how this dataset is used with the UniAD and ST-P3. Furthermore, this paper should provide more details of the dataset they constructed since the quality of the dataset has not been validated yet. In some worst cases, it might be constructed in a cherry-picking manner to benefit their approach. This paper should provide more detailed experimental setups to show that their evaluation is conducted on fair ground. \n\n### Lack of sufficient explanation of why their dimension reduction is particularly good\n\nThis paper claims that their dimension reduction technique shows significant performance improvements. However, dimension reduction is one of the most common approaches to improve robustness. This paper should provide more experimental results to support why their dimension reduction is particularly good. This paper may compare it with baseline dimension reduction techniques or employ an ablation study. Otherwise, I cannot be fully convinced whether this paper brings meaningful contributions to our community."}, "questions": {"value": "Is it possible to describe the details of their evaluation setups?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qxFi1ks2qK", "forum": "orzX69D5UO", "replyto": "orzX69D5UO", "signatures": ["ICLR.cc/2026/Conference/Submission7469/Reviewer_CdP1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7469/Reviewer_CdP1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952847106, "cdate": 1761952847106, "tmdate": 1762919583476, "mdate": 1762919583476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}