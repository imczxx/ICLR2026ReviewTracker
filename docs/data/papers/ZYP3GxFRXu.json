{"id": "ZYP3GxFRXu", "number": 9049, "cdate": 1758108676846, "mdate": 1759897746370, "content": {"title": "Plug-and-Play Object-Centric Reinforcement Learning via Masking", "abstract": "Deep reinforcement learning agents, trained on raw pixel inputs, often fail to generalize beyond their training environments, relying on spurious correlations and irrelevant background details. To address this issue, object-centric agents have recently emerged. However, they require different representations tailored to the task specifications. \nInspired by principles of cognitive science and Occam’s Razor, we introduce Object-Centric Attention via Masking (OCCAM), which selectively preserves task-relevant entities while filtering out irrelevant visual information, while taking advantage of the object-centric inductive bias.\nOur empirical evaluations on Atari games demonstrate that OCCAM significantly improves robustness to novel perturbations while showing similar or improved performance compared to conventional pixel-based RL. These results suggest that structured abstraction can enhance generalization and, more importantly, highlight the importance and influence of input representations to our task.", "tldr": "Structured, object-centric abstraction, combined with the ease of CNNs, keeping task-relevant entities and discarding distractors to enhance generalization and highlight the influence of input representations.", "keywords": ["Deep Reinforcement Learning", "Object-Centric RL", "Feature Representation", "Generalization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c254798e44f347defcf735cc926f530715f5a780.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the influence of utilizing object masks as object-centric inductive bias for deep RL from pixels, in the context of environment generalization (visual and dynamics). The proposed method, OCCAM, is a simple enhancement where supervised/ground-truth object masks, of various types (where “Plane Masks” is the best-performing one), are provided to the agent and integrated in the standard CNN-based learning pipeline. The method is evaluated in terms of generalization to some visual perturbations (where it demonstrates robustness) and environment dynamics perturbations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Simple.\n* The paper reads well and is easy to follow.\n* Well-detailed appendix!\n* Thorough analysis on Atari.\n* The method demonstrates some robustness to visual perturbations.\n* Open-source code."}, "weaknesses": {"value": "* Relies on supervision, i.e., assumes access to segmentations and detections which are trained with supervision or acquired from the environment (only possible in simulation). This kind of approach must assume that the pre-trained detector can, in fact, detect the objects in the environment, which is not typically true, especially given domain distribution-shifts. Alternatively, one can train a supervised detector in the environment, but this requires additional human effort. This reliance weakens the method and contribution in my opinion.\n* Background: there is an underlying assumption that background features do not matter (and please correct me if I misunderstood). However, I don’t agree that this is generally true (as demonstrated in the *Riverraid* game), as agents should be spatially-aware (e.g, obstacles or walls in a maze environment). There is also a claim that masking removes irrelevant task features like textures and colors, but what if appearance is important to the task? If this claim only concerns visual perturbations of the background, and the background is not relevant to the task, then I somewhat agree, but otherwise, I don’t think this is generally true.\n* The main goal of the paper is to evaluate the contribution of masking. Given previous work has already shown that object-centric representations are indeed outperforming traditional approaches, the results and findings in the paper are not entirely convincing, leading me to think that the policy design in this work is lacking and not fitted for object-centric representations (see [Zhou, Allan, et al. \"Policy architectures for compositional generalization in control.\"](https://arxiv.org/abs/2203.05960)) for policy architectures designed for object-centric representations). While I understand it is the authors’ desire to claim that the method can be “plug-and-play” with standard CNNs, I do not understand why it is important, as architecture and representations go hand-in-hand (and clearly the best-performing variant, “Plane Masks”, which decompose the masks to channels, are the closest variant to what standard object-centric architectures do-treating the input as a set of entities and using set-based architectures such as Transformers). In that case, I’m not sure I understand what the contribution of this paper is, aside from the emergent robustness to simple visual perturbations (I don’t find it surprising that the method does generalize to environment dynamics and I also don’t understand why masking would help with that).\n* Positioning and related work: one of the weaknesses of this work is that it is not well-positioned in the literature and it seems that literature review in this case was somewhat lacking. A myriad of previous work, typically under the “object-centric representations” field, have shown that masking helps not only in model-free RL, but for general decision making, including, but not limited to, model-based reinforcement learning and imitation learning. These works provide segmentations of the scene, either in a supervised manner or self-supervised/unsupervised manner. Following is a list of papers that are relevant and/or missing from the “Related Work:\n\n* **Masking**:\n\n[1] [Gmelin, Kevin, et al. \"Efficient RL via Disentangled Environment and Agent Representations.\" International Conference on Machine Learning. PMLR, 2023.](https://arxiv.org/abs/2309.02435)\n\n[2] [Lepert, Marion, Ria Doshi, and Jeannette Bohg. \"Shadow: Leveraging segmentation masks for cross-embodiment policy transfer.\" CoRL 2024.](https://arxiv.org/abs/2503.00774)\n\n[3] [Hutson, Miles, Isaac Kauvar, and Nick Haber. \"Policy-shaped prediction: avoiding distractions in model-based reinforcement learning.\" arXiv preprint arXiv:2412.05766 (2024).](https://arxiv.org/abs/2412.05766v1)\n\n[4] [Shi, Junyao, et al. \"Composing Pre-Trained Object-Centric Representations for Robotics From\" What\" and\" Where\" Foundation Models.\" 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.](https://arxiv.org/abs/2404.13474)\n\n[5] [Wang, Ziyu, et al. \"Generalizable visual reinforcement learning with segment anything model.\" arXiv preprint arXiv:2312.17116 (2023).](https://arxiv.org/abs/2312.17116)\n\n* **Object-centric**:\n\n[6] [Zadaianchuk, Andrii, Georg Martius, and Fanny Yang. \"Self-supervised reinforcement learning with independently controllable subgoals.\" Conference on Robot Learning. PMLR, 2022.](https://arxiv.org/abs/2109.04150)\n\n[7] [Haramati, Dan, Tal Daniel, and Aviv Tamar. \"Entity-Centric Reinforcement Learning for Object Manipulation from Pixels.\" The Twelfth International Conference on Learning Representations.](https://arxiv.org/abs/2404.01220). \n\n* **This paper is cited in the related work in this context**: “Recent category-aware and entity-centric RL explicitly condition on object categories or entities to improve manipulation and control (Yi et al., 2022;Haramati et al., 2024)”. However, this paper uses self-supervised object-centric representations without any supervision or access to object labels\n\n[8] [Ferraro, Stefano, et al. \"FOCUS: Object-centric world models for robotic manipulation.\" Frontiers in Neurorobotics 19 (2025): 1585386.](https://arxiv.org/abs/2307.02427)\n\n[9] [Zhang, Weipu, et al. \"Objects matter: object-centric world models improve reinforcement learning in visually complex environments.\" arXiv preprint arXiv:2501.16443 (2025).](https://arxiv.org/abs/2501.16443)\n\n[10] [Qi, Carl, et al. \"EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior Generation.\" ICLR 2025.](https://arxiv.org/abs/2412.18907)"}, "questions": {"value": "I wrote my questions under the weaknesses section.\n\nThe main question is: given previous works on masking/object-centric learning, what is the main contribution of this paper (aside for the visual perturbations part)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1n6dK4Uo1e", "forum": "ZYP3GxFRXu", "replyto": "ZYP3GxFRXu", "signatures": ["ICLR.cc/2026/Conference/Submission9049/Reviewer_8ZNP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9049/Reviewer_8ZNP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760559416501, "cdate": 1760559416501, "tmdate": 1762920762407, "mdate": 1762920762407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a simple, plug-and-play framework that improves RL generalization by masking out irrelevant visual details using object information. Using OCAtari’s engine-level annotations (bounding boxes and class labels), OCCAM builds several mask types—from Binary to Plane Masks—that feed structured, object-centric inputs into standard CNN-based RL agents like PPO and Rainbow. Experiments on Atari show that OCCAM matches or exceeds pixel-based performance and improves robustness (up to ~50% under visual shifts) while reducing shortcut reliance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Perturbation study: The paper evaluates generalization under both visual and gameplay perturbations using the HackAtari benchmark, providing strong evidence that OCCAM improves robustness and reduces shortcut reliance compared to pixel-based and symbolic baselines.\n\n2. Systematic analysis of mask types: OCCAM introduces and compares multiple masking strategies (Binary, Object, Class, and Plane), offering a clear and principled exploration of how different abstraction levels affect performance, robustness, and spatial reasoning.\n\n3. Clear and intuitive visualizations: The paper includes well-designed figures illustrating how each mask transforms the input and how performance changes under different conditions, making the method and results easy to understand and interpret."}, "weaknesses": {"value": "1. Limited practicality of object extraction: OCCAM relies on privileged object information from OCAtari, which is unavailable in most real-world or complex environments. Without such ground-truth annotations, the method cannot be directly applied, limiting its practicality beyond controlled simulation settings.\n\n2. Questionable general value of findings: Due to the impractical assumption in point 1, it is uncertain whether the same masking-based representations would remain effective when object information comes from imperfect or learned detectors. This greatly reduces the external validity and practical relevance of the study, raising the question of how its conclusions translate to realistic scenarios where object extraction is noisy or incomplete.\n\n3. Modest empirical novelty and gains: The reported improvements in robustness and performance are largely within expectation, given that OCCAM introduces additional semantic information and preprocessing. The results confirm intuitive hypotheses but offer limited novelty, serving more as a systematic validation than a breakthrough contribution."}, "questions": {"value": "My questions are mainly the same as the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tb5XHLQow8", "forum": "ZYP3GxFRXu", "replyto": "ZYP3GxFRXu", "signatures": ["ICLR.cc/2026/Conference/Submission9049/Reviewer_BGii"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9049/Reviewer_BGii"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711702393, "cdate": 1761711702393, "tmdate": 1762920761926, "mdate": 1762920761926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents mask-based simplifications of object-centric state representations in reinforcement learning. The paper mainly focuses on visual games that rely on CNNs for extracting the useful state information. The core method is to only preserve the object-centric information and form: (i) object mask: that only keeps objects in the frame, (ii) binary masks: that only show bounding boxes if any object is present in them, (iii) class masks: a single frame with only class-based masks, (iv) plane masks: multiple channels one per each class of objects. The paper then evaluates these representations using already trained RL agents for their robustness to visual and transition perturbations. They find that in one of the algorithms, the performance is retained even after the visual changes, however transition change leads to a drop regardless. Then, the paper investigates if spurious correlations in state features of Pong are ameliorated after masking. The result suggests that plane masks allow retention in the performance when certain entities are de-correlated. Later, the paper also shows that the training with proposed mask representations results in gains in performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The idea of identifying most relevant information for reinforcement learning is strong. This would allow for compact state representations that reduce the training complexity and increase compute efficiency.\n2) Object-centric features allow rich representations for performing reinforcement learning as the agent has better information about the environment. The current work is well motivated for bringing state compression together with the object-centric perspective on RL.\n3) The paper shows results on multiple Atari domains. The results on training performance improvement are, especially, intriguing."}, "weaknesses": {"value": "**Major (my reasons for not providing higher score):**\n1) The main thing that bothers me about this work is, by design there is less information in the state representations and it is unclear to me if the results generalize beyond the Atari domain. For instance, take binary masks, they simply convey the information about an object being present or not present in a certain area. They do not have information about the class of that object. How can such a representation be given to, say a robot, and expect it to learn a reliable policy?  If we were to say that in general a poor state representation leads to lower performance, shouldn't we be augmenting the masks proposed in this work to the pixel representations instead of using them directly as the observations? I am not sure an RL practitioner would like to only rely on the masked object-centric information ignoring most of the background information. \n2) To me, this work is a study of representations in visual reinforcement learning. In such a study, the main question should have been whether these representations preserve the RL algorithm's performance or not. However, the paper frames the central discussion on the robustness against perturbations. Only the state perturbations are relevant for the work as correctly pointed out and not the game play ones. The state perturbations used in the work clearly preserve the object location and there are no changes in the state after masking. Hence, they can allow some robustness. A better analysis would have been to check what happens when the positions of the objects are perturbed.\n3) Also, some of the mask generations would require sophisticated object detection and classification neural networks. This would add high overhead while training. If the state representation requires detection of hand-designed features and this representation is lossy, why would we opt for such a system? \n4) There is no code available in the provided anonymous git repo at the location occam/scripts/*. This hinders any inspection of the code for its correctness.\n5) Table 2: what are the quantities in the square parentheses? why are they integers? Can you please also include how the performance changes when using standard pixel-based representations? That will give an idea about how the performance drops in general. Also, why does the performance still decrease the mask representations in the case of Lazy enemy? If the enemy is hidden, do you hide the paddle in the masks too?\n6) Figure 3: Why isn't Rainbow with class and plane masks seeing improvement like PPO      does? As an explanation, line 242 suggests \"While PPO naturally benefits from simplified  inputs,…” how does PPO benefit naturally? Isn't the algorithm separate from the choice of representations? The only conclusion that I am able to draw from this figure is, the proposed OCCAM representations do not have a common trend in them and they might lead to poor performance.\n\t\n**Minor:**\n1) The proposed plane masks require placing all the classes separately on different planes. This would increase the size of the CNN required to process the representation if the number of classes is large.\n2) The terminology \"architecture\" or \"backend\" for referring RL algorithms like PPO is slightly confusing.\n3) The paper does not clearly discuss what environments are used to generate the results in the figures 3, 4 and 5. \n4) In conclusion: “can make object semantics explicit without pretraining.” Aren't you also relying on a pre-trained model to extract the objects and their bounding boxes?\n5) Is it a good idea to use IQM as a metric if there are only 3 random trials?"}, "questions": {"value": "I have asked the questions that came up in my mind in the weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lxUjVNnpVo", "forum": "ZYP3GxFRXu", "replyto": "ZYP3GxFRXu", "signatures": ["ICLR.cc/2026/Conference/Submission9049/Reviewer_4Fuo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9049/Reviewer_4Fuo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850398281, "cdate": 1761850398281, "tmdate": 1762920761608, "mdate": 1762920761608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is about OCCAM, a lightweight, task-agnostic framework designed to introduce object-centric inductive biases through structured input abstraction. The authors argue that by selectively preserving task-relevant entities and filtering out irrelevant visual information, their approach significantly improves robustness to novel perturbations while maintaining competitive performance with conventional pixel-based RL."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "I like the simplicity of the concept. The idea of creating a \"plug-and-play\" method that integrates object-centric biases into standard CNN architectures (like PPO and Rainbow) without needing complex architectural changes or specialized symbolic pipelines is appealing. And the systematic comparison between the different abstraction levels is clear and helps understand the trade-offs between simplicity and expressivity."}, "weaknesses": {"value": "- My primary concern is the heavy reliance on privileged information, which severely undermines the claims of \"plug-and-play\" and practical applicability. The entire evaluation uses OCAtari's RAM Extraction Method (REM) (Appendix D) to get perfect, noise-free object locations and classes directly from the emulator's memory. This completely bypasses the perception challenge. While the paper claims the framework is \"extractor-agnostic\" (L107), the results do not reflect performance in any realistic scenario where vision-based detectors (like YOLO or SAM) would introduce noise, missed detections, and inconsistencies. The synthetic noise study in Appendix F is far too superficial to address this fundamental gap.\n\n- The novelty seems quite limited. Masking inputs based on object locations, saliency, or segmentation has been explored extensively in RL and computer vision. While OCCAM provides a systematic categorization of these masks, it feels like an incremental contribution, perhaps not significant enough for ICLR, especially within such an idealized experimental setting (i.e., perfect perception).\n\n- The generalization benefits are quite narrow. The approach only really helps with visual perturbations (Figure 3a). When the gameplay dynamics or logic change (Figure 3b, or the Freeway/BankHeist examples in Section 3.2), OCCAM provides little to no advantage. This suggests the method only addresses superficial overfitting rather than deeper structural or logical shortcuts.\n\n- The core assumption that the background is merely \"irrelevant\" is often false. As clearly shown in the Riverraid experiments (Table 3), removing the background (the river banks) severely hurts performance because it removes crucial spatial context. The framework doesn't have a mechanism to determine when background elements are actually task-relevant."}, "questions": {"value": "- How does OCCAM perform when using a real, learned object detector (e.g., a pre-trained Mask R-CNN or an unsupervised method like Slot Attention) instead of the ground-truth RAM extraction? I suspect the performance gains might diminish significantly if the masks are noisy or inconsistent. This experiment is essential to validate the practical claims.\n\n- I’m curious about the comparison with MaDi (Appendix F). You state MaDi performed poorly due to sparse rewards and hyperparameter sensitivity in Atari. Did you attempt specific hyperparameter tuning for the Masker module, or try integrating data augmentation, which MaDi often relies on? A more thorough attempt seems necessary to properly position OCCAM against learned masking approaches.\n\n- Regarding the Riverraid results (Table 3), how would you adapt OCCAM to handle environments where the background defines the navigable space? Is there a proposed way to incorporate \"background objects\" selectively?\n\n- In the Pong analysis, you argue OCCAM helps the agent ignore the spurious correlation with the opponent paddle, even though the paddle is still visible in the mask. Could you elaborate on the mechanism? Why does the CNN ignore it when using OCCAM but focus on it when using raw pixels?\n\n- For Plane Masks, the input dimensionality scales with the number of classes. Appendix B mentions 2-3 times longer training time, which is non-trivial. How does this affect scalability in environments with a much larger number of object categories?\n\n- Why were Head-up display (HUD) elements excluded (Appendix D, L1105)? Information like scores or lives is often critical for decision-making. Does OCCAM struggle when these elements are masked?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4zsCHMrRe5", "forum": "ZYP3GxFRXu", "replyto": "ZYP3GxFRXu", "signatures": ["ICLR.cc/2026/Conference/Submission9049/Reviewer_jBPo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9049/Reviewer_jBPo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999067881, "cdate": 1761999067881, "tmdate": 1762920761211, "mdate": 1762920761211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}