{"id": "5F2XfLe7An", "number": 18032, "cdate": 1758283071988, "mdate": 1759897138043, "content": {"title": "SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights", "abstract": "Post-training quantization has emerged as the most widely used strategy for deploying large language models at low precision. Still, current methods show perplexity degradation at bit-widths $\\leq 4$, partly because representing outliers causes precision issues in parameters that share the same scales as these outliers. This problem is especially pronounced for calibration-free, uniform quantization methods. We introduce SINQ to augment existing post-training quantizers with an additional second-axis scale factor and a fast Sinkhorn–Knopp–style algorithm that finds scales to normalize per-row and per-column variances, thereby minimizing a novel per-matrix proxy target for quantization: the matrix imbalance. Our method has no interactions between layers and can be trivially applied to new architectures to quantize any linear layers.\nWe evaluate our method on the Qwen3 model family and DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against uncalibrated uniform quantization baselines and can be further enhanced by combining it with calibration and non-uniform quantization levels. Code is available in the supplementary.", "tldr": "", "keywords": ["LLM", "quantization", "integer", "INT4", "inference", "W4A16", "uniform quantization", "calibration-free quantization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e91c28c72b9c0ad5cb59f6b324d7886b02bc27e6.pdf", "supplementary_material": "/attachment/35a1140a4c16d768eb133ca1f103f2e1e0dfd7ef.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents SINQ, a post-training quantization method for large language models that uses dual-axis scaling and a modified Sinkhorn–Knopp algorithm to minimize \"matrix imbalance,\" improving perplexity on models like Qwen3 and DeepSeek-V2.5 while being compatible with mainstream paradigms such as NF4 and AWQ. However, it suffers from critical flaws—including unclear experimental details, unproven core metrics, limited innovation, and missing key comparisons—resulting in a \"Reject\" score, though addressing these issues could lead to reconsideration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The necessity of dual-axis scaling has not been verified: The paper fails to compare the performance of \"row-only scaling\", \"column-only scaling\", and \"dual-axis scaling\". This makes it impossible to prove the advantage of \"dual-axis scaling\" — for instance, if column-only scaling can achieve similar performance, the additional complexity of dual-axis scaling becomes meaningless.  \n2. It is a non-isolated solution and can be combined with mainstream quantization paradigms to expand application scenarios (e.g., NF4, AWQ)."}, "weaknesses": {"value": "1. Lack of experimental details: For the SINQ algorithm, the \"specific selection rule for σmin\" and \"threshold setting for early-stopping\" are not provided. In A-SINQ, the calibration dataset for AWQ (e.g., sample count, source) is not mentioned. While AWQ typically relies on 128–512 samples from the C4 dataset, the paper does not confirm consistency with this practice, nor does it explain whether calibration samples affect the optimization results of SINQ.  \n\n2. The rationality of using matrix imbalance as a surrogate metric is unproven: The paper defines matrix imbalance as $I(W)=\\sigma_{min}(W)/\\sigma_{max}(W)$ (the ratio of the minimum to maximum standard deviations of rows and columns) and claims that minimizing $I(W)$ improves quantization accuracy. However, it does not establish a mathematical connection between \"matrix imbalance\" and \"quantization error\" — for example, why a smaller $I(W)$ leads to lower post-quantization MSE or perplexity. The paper only observes through Figure 2 that \"minimizing $I(W)$ reduces kurtosis\", but fails to analyze the relationship between kurtosis and quantization error (e.g., whether reduced kurtosis necessarily decreases distribution overlap under low-bitwidth conditions), leaving the core assumption without theoretical support.  \n\n3. The paper modifies the standard Sinkhorn-Knopp algorithm to normalize row and column standard deviations, but does not prove the convergence of the modified algorithm (e.g., whether iterations enter cycles or if a unique fixed point exists). Additionally, it does not explain the basis for selecting the number of iterations $n_{iter}$ (e.g., why a fixed number is chosen instead of dynamic stopping based on the convergence threshold of $I(W)$), casting doubt on the algorithm’s stability. While Figures 2(a)(b) show that $I(W)$ stabilizes after 10 iterations, the paper does not explain \"why 10 iterations are optimal\" nor conduct ablation studies on the impact of $n_{iter}$ on performance.  \n\n4. The paper adopts the sequence \"SINQ normalization → AWQ scaling → quantization\" (Section 2.2.2). However, AWQ’s core lies in \"activation-aware weight scaling\", which relies on the distribution characteristics of original weights (e.g., correlation between activations and weights). Prior SINQ normalization alters the weight distribution, potentially disrupting the correlation relied on by AWQ. The paper does not compare the performance of alternative sequences such as \"AWQ first, then SINQ\" or \"joint optimization of SINQ and AWQ\", making it impossible to verify the rationality of the current sequence. It also fails to decompose contributions from \"SINQ alone\", \"AWQ alone\", and \"their combination\", leaving uncertainty about whether performance gains stem from dual-axis scaling or AWQ.  \n\n5. SINQ’s innovations are more akin to \"combinatorial optimization of existing technologies\" with limited breakthroughs: SINQ merely expands the scaling dimension from \"weight-activation\" or \"single-axis weight\" to \"dual-axis weight\", which is essentially an extension of the scaling target rather than a fundamental innovation. The standard Sinkhorn-Knopp algorithm normalizes row and column sums; SINQ only replaces the target with \"row and column standard deviations\" while retaining the algorithm framework (alternating iterative normalization), making this a routine modification rather than an innovative design.  \n\n6. Key comparative methods are missing: Representative methods such as FlatQuant and OSTQuant are not included in the comparisons.  \n\n7. The necessity of dual-axis scaling has not been verified: The paper fails to compare the performance of \"row-only scaling\", \"column-only scaling\", and \"dual-axis scaling\". This makes it impossible to prove the advantage of \"dual-axis scaling\" — for instance, if column-only scaling can achieve similar performance, the additional complexity of dual-axis scaling becomes meaningless.  \n\n8. For common datasets (HellaSwag, PIQA, MMLU), accuracy is the standard evaluation metric, while \"Flip rates\" are uncommon. The paper provides no justification for selecting this metric.  \n\nBased on the above weakness, I would assign a Reject score. I look forward to the authors addressing the aforementioned problems in future revisions, and I would be happy to reconsider and raise my score accordingly."}, "questions": {"value": "See \"Weaknesses\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "51Z72eTcdD", "forum": "5F2XfLe7An", "replyto": "5F2XfLe7An", "signatures": ["ICLR.cc/2026/Conference/Submission18032/Reviewer_jhSj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18032/Reviewer_jhSj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760545949990, "cdate": 1760545949990, "tmdate": 1762927821097, "mdate": 1762927821097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SINQ, a weight-only PTQ scheme that applies  dual scaling one scale per row and one per column, to each weight tile. The aim is to mitigate outliers along both dimensions and make ≤4-bit uniform quantization easier. The authors also introduce a proxy metric, matrix imbalance (the ratio of the largest to smallest row/column standard deviations), and a Sinkhorn–Knopp–style iteration that alternately normalizes row and column standard deviations to reduce this imbalance prior to quantization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Simple, calibration-free** recipe that improves perplexity compared to strong uniform PTQ baselines at 3–4 bits across model sizes.\n- Clear ablations comparing **imbalance** vs **kurtosis** as proxies for quantization difficulty.\n- **Competitive results**, outperforming HQQ, GPTQ and AWQ in many settings."}, "weaknesses": {"value": "- **Hardware evidence.** There are no end-to-end **inference throughput/latency** results or **kernel-level utilization** measurements; only **quantization-time** is reported. Without runtime data on common backends, deployment value is hard to assess.\n- **Weight-only scope.** Although activation quantization is discussed, the experiments are weight-only. It remains unclear how dual scaling interacts with **W×A** low-precision matmuls and whether common kernel fusions remain intact.\n- **Baselines.** Since the focus is weight quantization, a head-to-head with **codebook/rotation** approaches (e.g., **QuIP#**, **QTIP**) would strengthen the empirical case; these are mentioned in related work but not featured in the main tables.\n- **Further empirical results.** Results would be more convincing with additional families (e.g., **LLaMA**, **Phi**) to demonstrate generality.\n- CrossQuant appears closely related but is missing from the citations; it would help to explain the methodological differences and compare performance.\n\nCrossQuant: A Post-Training Quantization Method with Smaller Quantization Kernel for Precise Large Language Model Compression., Liu, Wenyuan, et al. (2024)."}, "questions": {"value": "check the weakness, \n\n1- how would dual scaling be implemented in hardware for W×A low-precision matrix multiplications, and what impact would it have inference speed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GMTwXPJPlv", "forum": "5F2XfLe7An", "replyto": "5F2XfLe7An", "signatures": ["ICLR.cc/2026/Conference/Submission18032/Reviewer_Eu9q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18032/Reviewer_Eu9q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959238333, "cdate": 1761959238333, "tmdate": 1762927820292, "mdate": 1762927820292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a post-training weight quantization method on a dual-scale matrix quantization scheme.  \nComparison to existing calibration-free PTQ methods including rotation-based methods is conducted empirically."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Clearly presented idea. \n+ Potentially significant practical value."}, "weaknesses": {"value": "- Practical overhead of dual-scaling on actual HW is not comprehensively discussed, except for memory efficiency."}, "questions": {"value": "* I am not sure the comparison against Hadamard rotation, etc. is also based on dual-scale scheme or not--it should be for a fair comparison.  \n* Random rotation is purported to mix channels and thereby eliminate outliers--this seems to be doing similar things as dual-scale.  Could you do an ablation study with (1) Hadamard + single-scale, (2) Hadamard + dual-scale, and (3) SINQ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SZ8EsdGIeu", "forum": "5F2XfLe7An", "replyto": "5F2XfLe7An", "signatures": ["ICLR.cc/2026/Conference/Submission18032/Reviewer_uY9X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18032/Reviewer_uY9X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18032/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967227352, "cdate": 1761967227352, "tmdate": 1762927818525, "mdate": 1762927818525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}