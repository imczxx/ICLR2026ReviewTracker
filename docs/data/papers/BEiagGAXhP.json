{"id": "BEiagGAXhP", "number": 23536, "cdate": 1758345096831, "mdate": 1759896809791, "content": {"title": "INSEva: A Comprehensive Chinese Benchmark for Large Language Models in Insurance", "abstract": "Insurance, as a critical component of the global financial system, demands high standards of accuracy and reliability in AI applications. While existing benchmarks evaluate AI capabilities across various domains, they often fail to capture the unique characteristics and requirements of the insurance domain. To address this gap, we present INSEva, a comprehensive Chinese benchmark specifically designed for evaluating AI systems' knowledge and capabilities in insurance. INSEva features a multi-dimensional evaluation taxonomy covering business areas, task formats, difficulty levels, and cognitive-knowledge dimension, comprising 38,704 high-quality evaluation examples sourced from authoritative materials. Our benchmark implements tailored evaluation methods for assessing both faithfulness and completeness in open-ended responses. Through extensive evaluation of 9 state-of-the-art Large Language Models (LLMs), we identify significant performance variations across different dimensions. While general LLMs demonstrate basic insurance domain competency with average scores above 80, substantial gaps remain in handling complex, real-world insurance scenarios. The benchmark will be public soon.", "tldr": "", "keywords": ["Insurance", "Large Language Model", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/daa49c1d93926ac9a5776619a1b98d0e1ae4b819.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "INSEva is a novel benchmark dataset constructed to specifically test LLM performance in the insurance domain in China. The authors argue that current financial benchmarks lack the specificity needed for specialized tasks in insurance. They evaluate their large, comprehensive generated benchmark across nine LLMs, including financial specific models. The authors demonstrate that performance of financial specific models does not exceed general models and that small improvement gains are made by training on their benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "-\tEvaluation on completeness and faithfulness is more robust than accuracy alone.\n-\tAuthors highlight that current financial specific models are not necessarily higher performing that general models on the benchmark."}, "weaknesses": {"value": "- It would be helpful to better articulate examples of how existing benchmarks (CFLUE) have gaps in their insurance knowledge. What are the more specific tasks that are required and lacking? Overall, the value add of the benchmark could be articulated more specifically in the beginning sections.\n- There are several gaps in the description of data generation that make the overall methods unclear. Specifically: 1) Under 3.3.1, item (2) is not specific enough to understand what you did. 2) What LLM was used for dataset construction? 3) What rephrasing techniques were used in question augmentation? 4) Do insurance experts review all 30+k examples? 5) I don’t see the connection to the construction of cognition and knowledge domains. 6) How are easy versus hard questions adjudicated and where is this distinction used?\n- Benchmark performance is high. This makes the need for the benchmark unclear. Performance gains training specifically on the benchmark are also not huge, which also challenges the need for this benchmark."}, "questions": {"value": "-\tHow are questions formulated to assess the various cognition and knowledge dimensions in 3.2.4? Examples would be a nice add to the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nTOcF1u5hr", "forum": "BEiagGAXhP", "replyto": "BEiagGAXhP", "signatures": ["ICLR.cc/2026/Conference/Submission23536/Reviewer_5LZM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23536/Reviewer_5LZM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761415964172, "cdate": 1761415964172, "tmdate": 1762942701908, "mdate": 1762942701908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Chinese benchmark for the insurance domain. The authors first construct a multidimensional evaluation taxonomy of business areas, task formats, difficulties, and cognition and knowledge. Based on this taxonomy, the authors construct 38,704 evaluation examples from various resources. A detailed data quality control pipeline and evaluation framework are also proposed. The authors conducted experiments on nine representative LLMs, revealing their strengths and limitations, and performed an analysis. To demonstrate the effectiveness of the data construction pipeline, a new model is fine-tuned and shows performance improvements."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The domain being studied is of high importance and strong practical usability. The proposed dataset represents a valuable contribution to the community, as there is currently a lack of high-quality benchmarks for critical real-world tasks such as those in insurance. The focus on Chinese adds additional value by enriching the limited Chinese NLP resources.\n\n2. The authors have invested substantial effort in developing a well-structured taxonomy. This taxonomy aims to capture a broad range of categories in the insurance domain, enhancing the coverage, diversity, and depth of the resulting dataset.\n\n3. The dataset is validated by experts, ensuring its quality. \n\n4. The experiment and analysis bring good insights about current LLMs and could inspire future works on building effective LLM agents in the insurance domain."}, "weaknesses": {"value": "My major concern for this work is the lack of rigor and clarity in a lot of aspects. \n\n1. Are there any insurance experts involved throughout this work, especially for the taxonomy construction? The close collaboration with domain experts is the key to this type of work, to ensure the dataset proposed is realistic and covers important problems in insurance. \n\n2. For categorizing examples, do you employ insurance experts? \n\n3. For data sources, these include certification platforms, industry repos, internal business resources, etc. Since the authors promise to release the dataset, I'm wondering whether there are any legal, ethical, and privacy concerns about releasing such data. \n\n4. A lot of the data examples are synthesized as QA pairs using LLMs. The details for such procedures are not clear. If all these examples are synthesized with the same template using LLMs, it's unsure whether these QAs can capture important challenging points in the data. \n\n5. For data augmentation, what paraphrasing technique is used? \n\n6. For the industry experts employed, to ensure ethical research, how do you hire and pay them? \n\n7. For the data quality control, how specifically do you aggregate multiple assessments? \n\n8. For the evaluation method, the evaluation for open-ended examples relies on LLM-based evaluations, which may bring inaccuracy. Human experts' evaluation should be involved. I'd also like to see the human experts' performances on the open-ended examples. \n\n9. Case studies and analysis should be presented more in experiments. \n\n10. There are no details about the Finix-S1 model trained. What's the training data here, and how specifically is the RL training conducted?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YHoCBPrO9y", "forum": "BEiagGAXhP", "replyto": "BEiagGAXhP", "signatures": ["ICLR.cc/2026/Conference/Submission23536/Reviewer_GsM6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23536/Reviewer_GsM6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766993663, "cdate": 1761766993663, "tmdate": 1762942701680, "mdate": 1762942701680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The article first introduces INSEva, a Chinese benchmark specifically designed for the insurance domain, aimed at evaluating AI systems' knowledge and capabilities in the insurance industry. Specifically, the data for INSEva is collected from authoritative exams, regulatory standards, and real-world business materials. The data undergoes paraphrasing, colloquial enhancement, option randomization, and triple quality control (rules, expert reviews, and multi-LLM voting), resulting in a final dataset of 38,704 examples, with an average prompt length of 905 tokens.\n\nThen, the article provides a four-dimensional evaluation classification standard, which includes: Business domains (insurance domain knowledge, insurance-medical interdisciplinary knowledge, insurance understanding and cognition, insurance logical reasoning, insurance professional exams, insurance safety and compliance, insurance marketing growth, insurance service dialogue); Task formats (multiple-choice, true or false, single-turn QA, multi-turn dialogue); Difficulty levels (Easy, Hard); Cognitive and knowledge dimensions (Remembering, Understanding, Applying, Analyzing, Evaluating).\n\nFinally, the article evaluates 9 LLMs (e.g., GPT-4o, Doubao-1.5-pro-256k2, Gemini-2.5-pro3, etc.) on the INSEva benchmark. The experimental results are analyzed in detail, and the effectiveness of Finix-S1, an insurance-specific model trained using reinforcement learning based on the Qwen-QwQ-32B model, is validated. Finix-S1 shows an average improvement of 4.69% over the base model Qwen-QwQ, and surpasses the current state-of-the-art general LLMs (e.g., Gemini) by more than 3%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The article addresses the issue of many financial benchmarks but fewer and incomplete insurance benchmarks by providing INSEva, a systematic and comprehensive evaluation framework. This framework enables researchers and practitioners to more accurately compare different models' capabilities in the high-risk insurance domain.\n\n2. The data provided by INSEva comes from historical exam questions, regulatory standards, and real-world business materials, ensuring highly authentic domain knowledge. Additionally, INSEva enhances the dataset through paraphrasing and colloquial rewrites, making it more suitable for customer service scenarios. INSEva offers 38,704 high-quality samples, with an average prompt length of 905 tokens, sufficient for testing long-context handling and complex constraint processing.\n\n3. INSEva uses objective and reproducible metrics such as Accuracy for closed-ended questions. For open-ended questions, it introduces two key metrics: faithfulness and completeness. It employs an LLM-based sentence decomposition and per-sentence evaluation process using standardized prompts, with expert annotation and relevance checks to ensure consistency with human evaluation. This minimizes subjective bias in reviews and model bias in the results, ensuring auditable and reproducible fair evaluation in high-risk insurance scenarios.\n\n4. The paper evaluates 9 LLMs under the same data division, uniform prompts, reasoning settings, and no data leakage. It provides stratified reports based on task types, difficulty levels, cognitive and knowledge dimensions, and offers consistent retrieval and context for multi-turn dialogue and retrieval scenarios. At the same time, it uses option randomization and expert quality control to control structural biases. This design ensures that comparisons between models across dimensions are transparent, reproducible, and interpretable.\n\n5. Using the INSEva data pipeline, the paper conducts reinforcement learning (RL) training based on the Qwen-QwQ-32B model, achieving an average improvement of 4.69%. This not only validates the practical value of the INSEva benchmark and data process but also demonstrates its ability to drive model improvement and optimization."}, "weaknesses": {"value": "1. The paper evaluates 9 SOTA LLMs, but most of the models in the experiments are still focused on well-known general models and a few domain-specific models. Although these models provide valuable comparisons, the current evaluation may not fully cover all possible model types, especially innovative models specific to the insurance domain that may not have been adequately assessed.\n\n2. The paper mentions the issue of catastrophic forgetting in the insurance-specific model Finix-S1, but it does not delve into how this issue can be addressed or whether there are methods to effectively mitigate this phenomenon.\n\n3. Although INSEva provides a systematic and comprehensive evaluation framework, the paper states that only 10% of the stratified samples can be made publicly available. This limits other researchers' access to and use of the complete dataset, potentially impacting the verification of results and subsequent research. While the availability of the sample is ensured, the lack of the complete dataset may limit the widespread application and reproducibility of the benchmark."}, "questions": {"value": "1. In the INSEva benchmark, although it mentions a total of 38,704 data samples, it does not specify the sample size for each task category. Could you share the data distribution for different task categories (such as insurance logical reasoning, insurance safety and compliance, etc.)?\n\n2. In the Finix-S1 reinforcement learning phase, the paper mentions the issue of catastrophic forgetting, where focusing on insurance domain training may lead to performance degradation on general tasks. Have you considered using multi-task learning or transfer learning methods to mitigate this phenomenon?\n\n3. Insurance domain recommendation tasks often involve user demand prediction and product matching. Does the dataset include such tasks, especially considering the unique complexities of the insurance domain (such as risk assessment, insurance terms, etc.)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4WuxuI0vxP", "forum": "BEiagGAXhP", "replyto": "BEiagGAXhP", "signatures": ["ICLR.cc/2026/Conference/Submission23536/Reviewer_Hewh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23536/Reviewer_Hewh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810127955, "cdate": 1761810127955, "tmdate": 1762942701498, "mdate": 1762942701498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces INSEva, a comprehensive benchmark designed to assess and drive the progress of LLMs within the Chinese insurance domain. INSEva is distinguished by a multidimensional evaluation taxonomy and comprises 38,704 high-quality evaluation examples sourced from authoritative materials in Chinese insurance. To evaluate both closed- and open-ended responses, the authors employ conventional accuracy metrics and LLM-based pipelines that separately measure faithfulness and completeness. The benchmark is used to provide an in-depth comparison of 9 state-of-the-art LLMs, diagnosing model weaknesses and evaluating improvements gained from domain-specific fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The benchmark construction pipeline leverages authoritative sources, paraphrasing for linguistic diversity, expert validation, and LLM-based voting for quality control. \n2. The benchmark provides a broad range, with 38,704 questions spanning diverse subdomains of insurance, multiple question formats, and cognitive skill levels."}, "weaknesses": {"value": "1. Potentially Insufficient Difficulty: Current SOTA models already achieve very high scores across most tasks, reaching 83.89 on average ( in 8 business areas and 9 tasks，8/9 tasks exceed 80, 6 tasks exceed 85), and the authors' own fine-tuned model (Finix-S1) pushes this score even higher. This raises significant questions and concerns about the benchmark's \"headroom\" and its long-term utility for challenging future, more capable models.\n2. The paper offers little diagnostic insight into model failure modes on those tasks outside of high-level discussion. No error analysis, detailed breakdowns, or fine-grained case studies (beyond brief examples in Figure 3/Figure 4) are included. \n3.  The author claims that only 10% of the data will be public, due to commercial constraints (see Section 7). This presents a significant barrier to full reproducibility, making the paper's main claims unverifiable, the core experiments impossible to reproduce, and significantly reducing the actual benchmark sample size and its usefulness in the future."}, "questions": {"value": "Please check the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8IuZJxKAAw", "forum": "BEiagGAXhP", "replyto": "BEiagGAXhP", "signatures": ["ICLR.cc/2026/Conference/Submission23536/Reviewer_9Dcj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23536/Reviewer_9Dcj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064404607, "cdate": 1762064404607, "tmdate": 1762942701300, "mdate": 1762942701300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}