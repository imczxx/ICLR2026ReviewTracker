{"id": "mvWQYIjDsx", "number": 23965, "cdate": 1758350926142, "mdate": 1763713960788, "content": {"title": "Online Pseudo-average Shifting Attention(PASA) for Robust Low-precision LLM Inference: Algorithms, Numerical Analysis and Performance", "abstract": "Attention computation remains a critical bottleneck for long-sequence inference in large models (e.g., long-text/video generation). To address this, we propose PASA(Pseudo-average Shifting Attention), a fully low-precision algorithm that maintains mathematical equivalence to Flash Attention while enabling stable half-precision computation. PASA introduces two key innovations: (1) online pseudo-average shifting and (2) global error recovery, which jointly prevent overflow and preserve numerical accuracy by dynamically adjusting attention score statistics. This approach significantly releases bandwidth pressure and leverages low-precision compute units on AI accelerators (e.g. NPUs). We identify that numerical instability in attention stems from a \\textit{resonance} mechanism-phase alignment (or anti-alignment) between query and key matrices in the head dimension-which triggers overflow of attention score matrix and incurs large numerical errors. Experiments on Qwen2-7B and Stability AI/Stable-Video-Diffusion demonstrate that PASA eliminates these issues while achieving $1.2 - 1.65 \\times$ speedup with fully half precision reaching up to $170$ TFLOPs/s on Ascend NPU compared to a highly optimized vendor Flash Attention from Ascend/CANN. To our knowledge, this is the first work to enable full half-precision acceleration for long-sequence attention (MHA/MQA) with guaranteed numerical stability.", "tldr": "PASA: Accelerating attention with low precision computing for large models", "keywords": ["Low-Precision Inference", "Attention", "Long Sequence", "Multi-modal", "Overflow", "NPU"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/245c1dd4bf1663551380890cea19f7551d7f3db0.pdf", "supplementary_material": "/attachment/4bbf226641582a0a83db7f15aaa0fe05b2406744.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the numerical overflow that prevents attention mechanisms like FlashAttention from being executed entirely in a low-precision (FP16) pipeline. The work identifies the root cause as a “resonance” phenomenon: a phase alignment between Query and Key vectors along the head dimension, which catastrophically amplifies their inner product. The proposed method, PASA, introduces an online pseudo-average shifting mechanism, implemented as a single hardware-friendly GEMM on the Key matrix to preemptively suppress the magnitude of attention scores. This is coupled with a novel global recovery scheme that mathematically corrects these local shifts during the online tiling process, thereby ensuring correctness. By enabling stable, full-FP16 computation, PASA eliminates the memory bandwidth bottleneck inherent to mixed-precision approaches and demonstrates significant speedups of 1.2x–1.65x over highly optimized vendor libraries on Ascend NPUs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper’s primary strength lies in its novel and mechanistic diagnosis of the overflow problem in low-precision attention. By defining the issue as “resonance,” which refers to a specific phase alignment between Query and Key vectors, the authors provide a more concrete and actionable insight than the generic observation that values can become large. This diagnosis is convincingly supported by empirical visualizations from real-world models.\n\n2. The proposed solution, PASA, is a well-engineered response to this finding. Its core technical contribution is twofold: a): it cleverly reframes the corrective bias subtraction as a single, hardware-efficient GEMM operation, showcasing strong algorithm and hardware co-design. b): This is correctly paired with a non-trivial online recovery mechanism that ensures mathematical fidelity is maintained within the constraints of a tiled computing framework.\n\n3. The significance of this work is therefore highly practical, as it presents a robust method to enable fully FP16 attention. This directly addresses a critical performance bottleneck in modern AI systems by alleviating memory bandwidth pressure and allowing for better utilization of low-precision hardware, making it a valuable contribution to efficient long-sequence inference."}, "weaknesses": {"value": "1. The empirical validation is conducted exclusively on Ascend NPUs. To substantiate the method's claimed generalizability, its performance and stability should also be benchmarked on other widely used hardware, such as NVIDIA/AMD GPUs and CPUs. This is critical because the practical speedup from a full-FP16 pipeline is highly dependent on specific architectural characteristics, and demonstrating efficacy on market-leading platforms would significantly broaden the paper's impact.\n\n2. The primary motivation, the \"resonance\" phenomenon, is illustrated using sampled 1D plots (e.g., Figure 7) that likely represent worst-case scenarios rather than the overall data distribution. While illustrative, this selective sampling may overstate the issue's pervasiveness. Providing more comprehensive visualizations, such as heatmaps of the attention score matrix or distributions of max scores across all tokens, would offer more robust evidence that resonance is a systematic, rather than an anecdotal, problem.\n\n3. The paper’s scope is confined to the FP16 data type, while many modern systems and training setups increasingly favor BF16 due to its larger dynamic range. The authors do not discuss whether the resonance issue persists in BF16 (e.g., as a source of precision loss instead of overflow) or if the PASA mechanism would still be beneficial. A discussion of the method's relevance and potential adaptations for BF16 would provide a more complete picture of its utility.\n\n4. While the chosen models (Qwen2-7B, SVD) are suitable for demonstrating the core problem, the validation could be strengthened by including a wider range of modern model architectures. Testing PASA on structurally different models, such as those employing Mixture-of-Experts (MoE) or alternative normalization schemes, would provide stronger evidence for the robustness and general applicability of the proposed technique beyond the specific cases presented.\n\n5. The performance claims are made against a vendor library, but a comparison with the open-source state-of-the-art, FlashAttention-3, is conspicuously absent. Modern kernels like FlashAttention-3 achieve high performance by carefully overlapping the compute-intensive GEMM operations with the memory-bound softmax and accumulator updates. In such a highly optimized pipeline, the latency of the mixed-precision (FP32) softmax updates may already be partially hidden. Therefore, while PASA’s benefit of halving the memory traffic from these intermediates is clear, the real-world speedup on top of FlashAttention-3 is not guaranteed and must be empirically demonstrated to be significant.\n\n6. There are no comparative results at the whole-network level, such as using the Qwen2-7B model on commonly used datasets and comparing with FP32 Softmax, e.g., MMLU, HumanEval, GSM8K, Multi-Exam, etc. (see [Qwen2-7B on Huggingface](https://huggingface.co/Qwen/Qwen2-7B) for reference)."}, "questions": {"value": "1. Given that Q is in the outer loop of the tiled algorithm (FlashAttention), applying a single corrective operation to it seems, in theory, more efficient than modifying blocks of K in the inner loop. A discussion on this trade-off, and perhaps an ablation study in an inference/training setting, would significantly strengthen the justification for your approach.\n\n2. PASA introduces new computations, namely the K * M multiplication and the online correction term calculations. Please quantify the performance overhead of these two operations. A breakdown of their latency, and an analysis of whether they are successfully hidden/overlapped within the main attention pipeline, is necessary to fully assess the net performance gain.\n\n3. The core argument for the performance gain is the reduction in memory bandwidth from eliminating FP32 intermediates. To help quantify this claim more precisely, could you provide:\n   \n   - A micro-benchmark showing the latency difference between the softmax-related updates (i.e., finding max, calculating exp, and sum) when the accumulators and intermediate score matrix are in FP16 versus FP32?\n   - An approximate profiling breakdown of the baseline mixed-precision attention kernel, showing the percentage of time spent on GEMM computation versus the non-GEMM parts (memory movement, vector math for softmax, etc.)? This data would more directly substantiate the claim that the latter is indeed a significant bottleneck that PASA effectively addresses.\n\n4. Could you provide comparative results for Qwen2-7B on standard datasets such as MMLU, HumanEval, GSM8K, and Multi-Exam, in comparison with FP32 Softmax?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dIk4m02yig", "forum": "mvWQYIjDsx", "replyto": "mvWQYIjDsx", "signatures": ["ICLR.cc/2026/Conference/Submission23965/Reviewer_3xtq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23965/Reviewer_3xtq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761227258924, "cdate": 1761227258924, "tmdate": 1762942873621, "mdate": 1762942873621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PASA is an algorithm for computing attention with low-precision data types like FP16, while benefitting from the high FLOPs throughput of algorithms such as FlashAttention. PASA builds on prior work SageAttention, that increases numerical stability by shifting key vectors by a constant vector and reducing the chance of overflows in the attention score matrix. PASA extends SageAttention by shifting key vectors by an online block-wise average of the key vectors, promising higher stability. PASA is evaluated on a synthetic QKV dataset, and Ascend NPUs with Qwen2-7B and IMG2VID models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Thank you for submitting your work. I found the in-depth technical explanation to be clear, and the idea of calculating averages online sounds promising."}, "weaknesses": {"value": "However, I have three key questions:\n* I didn't see performance or accuracy comparisons to prior work, including SageAttention. Have I missed anything? Since the ideas are similar (mean shifting), it would be important to show that the **online** aspect of PASA is worth the extra computations.\n* Why is the evaluation limited to Ascend NPUs? Prior work, including SageAttention, focus on NVIDIA GPUs which have had wider support for various kernels and serve as more stable and more optimized baselines.\n* I observed that the evaluation and technical discussion is focused on FP16. Recent LLMs are commonly served in BF16 format. It would be critical to show that the numerical instability problem still exists with BF16 and PASA can reduce it.\n\nSome minor comments:\n* I would strongly suggest proofreading the paper for grammar. While it was possible to understand the technical content, the presentation and flow would benefit from it.\n* Typo in equation 3"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "onyooeu86I", "forum": "mvWQYIjDsx", "replyto": "mvWQYIjDsx", "signatures": ["ICLR.cc/2026/Conference/Submission23965/Reviewer_m9Dt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23965/Reviewer_m9Dt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708703301, "cdate": 1761708703301, "tmdate": 1762942873409, "mdate": 1762942873409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PASA (Pseudo-Average Shifting Attention), an online variant of FlashAttention that enables stable full-FP16 attention without modifying model weights or sacrificing accuracy. The key idea is to pre-shift keys via a structured transformation matrix M effectively removing the mean bias responsible for overflow while remaining mathematically equivalent to FlashAttention. The paper also introduces an online recovery mechanism ensuring global consistency, with detailed theoretical derivations (Theorem 1) and a rounding-aware fixed-point method to select beta. Evaluations on Ascend NPUs demonstrate ~1.5x speedups and complete elimination of overflow across both synthetic stress tests and real models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of this paper lie in both novelty and practical utility. It introduces pseudo-average shifting as a principled and lightweight mechanism for stabilizing FP16 attention without altering model semantics. This is a clear conceptual advance over prior FlashAttention variants and bias-correction methods, addressing overflow at its source through a mathematically grounded and hardware-compatible reformulation. The design is particularly elegant: it preserves the core pipeline of FlashAttention while embedding the correction into a single matrix operation, maintaining equivalence in exact arithmetic.\n\nBeyond novelty, the paper stands out for its reproducibility. The presentation is detailed and clear, with Algorithm 1, the beta-selection routine, and theoretical derivations all well-documented. Overall, the work is easy to follow, theoretically sound, and practically impactful, offering a genuine improvement to the reliability and efficiency of low-precision attention."}, "weaknesses": {"value": "While the method is well-motivated and the experiments are promising, the evaluation focuses mostly on FLOPs and synthetic benchmarks rather than end-to-end latency. Reporting real tokens-per-second or full inference latency on large models would make the results far more compelling. Comparisons to high-performance FA or SageAttention are also missing, leaving it unclear how much of the gain comes from the pseudo-average shift itself versus the FP16 pipeline. Finally, the resonance explanation, while interesting, remains qualitative and could benefit from more quantitative analysis or supporting metrics.\n\nThe end-to-end evaluation lacks rigor. While the hardware-level metrics and kernel performance are well analyzed, the paper doesn’t provide a clear picture of how PASA impacts total inference time, throughput, or memory footprint in full model runs. Without these measurements, it’s hard to assess the real deployment gains compared to existing optimized attention kernels.\n\nTo improve my score, I would like to see evaluations with other common self-attention techniques such as SageAttention and a more comprehensive model quality evaluation."}, "questions": {"value": "1. Could the authors comment on how PASA will perform on different types of transformer-style networks? I'm specifically interested in the computational implications of running this on DiT blocks vs. standard MHSA blcoks. Does the pre-shifting step interact differently with their attention structure or memory patterns?\n\n2. Figure 8 seems to be the only large-scale evaluation in this paper and just shows SSIM on one image. Can the authors please report more useful image generation metrics such as CLIP Score CLIP-IQA, and QSNR? It would be helpful to run a standard image generation evaluation here to confirm what is just briefly mentioned for end-to-end evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4Z42SOozMo", "forum": "mvWQYIjDsx", "replyto": "mvWQYIjDsx", "signatures": ["ICLR.cc/2026/Conference/Submission23965/Reviewer_2wog"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23965/Reviewer_2wog"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973590278, "cdate": 1761973590278, "tmdate": 1762942872997, "mdate": 1762942872997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Model\nThis paper proposes PASA (Pseudo-Average Shifting Attention), an online algorithm for low-precision attention computation aimed at enhancing numerical stability and accuracy for long-sequence LLM inference.\nSummary:\nAttention computation in large language models (LLMs) is a significant bottleneck, especially for long-context and multi-modal tasks, due to its quadratic complexity and memory demands. While FlashAttention (FA) has emerged as a leading optimization method, current implementations often rely on FP32 for intermediate variables to maintain numerical stability, which can become a performance bottleneck due to memory bandwidth limitations. Fully low-precision (e.g., FP16) FA, while promising greater performance and energy efficiency, faces high risks of overflow and underflow.\nPASA addresses these issues by introducing two key innovations: (1) online pseudo-average shifting and (2) global recovery. These mechanisms dynamically adjust attention score statistics to prevent overflow and preserve numerical accuracy. The core idea is to leverage the translation invariance of softmax through online block-wise processing, performing bias correction at the block level. This design ensures compatibility with hardware pipelining architectures, reduces numerical errors through local pseudo-average shifting, and enables efficient implementation via batched matrix multiplications.\nThe paper identifies that numerical instability in attention largely stems from a \"resonance mechanism\" where phase alignment (or anti-alignment) between query and key matrices along the head dimension amplifies inner products, leading to large values and overflow. PASA effectively suppresses this resonance.\nExperiments on Qwen2-7B and Stable-Video-Diffusion models, running on Ascend NPUs, demonstrate that PASA eliminates overflow issues, achieves a 1.2x-1.65x speedup over highly optimized vendor libraries with full FP16 precision, and maintains inference accuracy matching high-precision methods. The authors claim this is the first work to enable fully FP16 acceleration for long-sequence attention with guaranteed numerical stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. PASA directly tackles the major challenge of numerical instability (overflow/underflow) when attempting full low-precision (FP16) attention, which is crucial for improving the efficiency of LLMs for long-sequence inference.\n2. Demonstrating a 1.2x-1.65x speedup with full FP16 and reaching up to 170 TFLOPs/s on Ascend NPUs is a strong indicator of its practical utility for accelerating LLM inference.\n3. The paper asserts that PASA achieves \"mathematical equivalence to Flash Attention\" and maintains accuracy comparable to high-precision methods, which is vital for real-world deployment.\n4. The introduction of \"online pseudo-average shifting\" and \"global recovery\" specifically tailored to mitigate resonance-induced overflow is a clever and effective algorithmic innovation.\n5. PASA is designed to be compatible with hardware pipelining and utilizes low-precision compute units effectively, demonstrating good co-design principles for AI accelerators."}, "weaknesses": {"value": "1. The experimental validation is primarily conducted on Huawei Ascend NPUs. While results are generalized to \"most AI accelerators,\" the extent of performance gains and stability on other platforms (e.g., NVIDIA GPUs) needs further explicit demonstration.\n2. The optimal accuracy condition relies on selecting a hyperparameter β (close to 1.0) through a fixed-point iteration solved in FP64. While the paper provides chosen values, this step adds an extra layer of complexity to the deployment and optimization process.\n3. While the concept of \"resonance\" is introduced, a more rigorous mathematical formalization or in-depth statistical analysis of its occurrence frequencies across diverse real-world datasets (beyond the two models tested) could strengthen this claim.\n4. The paper states that \"global bias correction conflicts with hardware pipelining.\" While PASA's block-level correction avoids this, a more detailed explanation of why global correction specifically conflicts with pipelining could be beneficial.\n5. While compared to vendor-optimized FA (CANN-PFA), a more direct comparison with other low-precision techniques (e.g., INT8/INT4 quantization strategies mentioned in the background) that aim for similar efficiency gains, in terms of both stability and performance, would provide a more complete picture of PASA's competitive advantage.\n6. While \"online block-wise processing\" is mentioned, further details on how the online nature impacts memory usage or latency for very long sequence inference, beyond just avoiding overflow, would be insightful."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8ANa0iUh9R", "forum": "mvWQYIjDsx", "replyto": "mvWQYIjDsx", "signatures": ["ICLR.cc/2026/Conference/Submission23965/Reviewer_qZup"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23965/Reviewer_qZup"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990064290, "cdate": 1761990064290, "tmdate": 1762942872383, "mdate": 1762942872383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}