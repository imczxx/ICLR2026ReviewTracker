{"id": "f6ugrBWs3K", "number": 23193, "cdate": 1758340730962, "mdate": 1759896827471, "content": {"title": "Chain of Time: In-Context Physical Simulation with Image Generation Models", "abstract": "We propose a novel method to improve the physical simulation ability of vision-language models. This Chain-of-Time simulation is motivated by in-context reasoning in machine learning, and mental simulation in humans. The method involves generating a series of intermediate images during a simulation. Chain of Time is used at inference time and requires no additional fine-tuning for performance benefits. We apply the Chain-of-Time method to synthetic and real-world domains, including 2-D graphics simulations and natural 3-D videos. These domains test a variety of particular physical properties, including velocity, acceleration, fluid dynamics, and conservation of momentum. We found that using Chain-of-Time simulation substantially improves the performance of state-of-the-art Image Generation Model. Beyond examining performance, we also analyze the specific states of the world simulated by an image model at each time step, which sheds light on the dynamics underlying these simulations. This analysis reveals insights that are hidden from traditional evaluations of physical reasoning, including cases where an Image Generation Model is able to simulate physical properties that unfold over time, such as velocity, gravity, and collisions domain well. Our analysis also highlights particular cases where the Image Generation Model struggles to infer particular physical parameters from input images, despite being capable of simulating relevant physical processes.", "tldr": "", "keywords": ["Multi-modal Language Models", "Spatial and Temporal Perception", "Image Generation", "Physical Reasoning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d0ef4077bf57397482b6d12a29a564f7f8e8459.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Chain-of-Time, a method for improving physical simulation in Image Generation Models by generating intermediate frames step-by-step, inspired by mental simulation in humans and chain-of-thought reasoning in LLMs. The authors test on four domains (2D Motion, 2D Gravity, Fluids, Bouncing) and show improvements over direct prediction in most cases."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Moderately Novel approach: The connection between human mental simulation theory and in-context reasoning for IGMs is creative and well-motivated.\n\nInterpretability: The method provides interpretable intermediate steps that reveal the model's physical reasoning process.\n\nClear methodology: The paper clearly describes the de-rendering, simulation, and rendering components."}, "weaknesses": {"value": "W1: Insufficient analysis of fluid domain's fundamental challenges The authors observe performance degradation in the fluids domain but fail to analyze why fluids are fundamentally different from the other tested scenarios. The paper should discuss whether Chain-of-Time is inherently unsuitable for fluid domain due to some specific properties of it like continuous deformation or partial transparency, or whether the issue is specific to their experimental setup. The superficial explanation of \"flow rate estimation error\" doesn't address why the step-by-step approach that helps with projectile motion actually harms fluid simulation.\n\nW2: Limited temporal horizon evaluation All experiments are constrained to 0.8 seconds of simulation. For a method claiming to improve physical simulation, testing longer time horizons (e.g., 2-5 seconds) would better demonstrate scalability and compound error effects. The mental simulation literature the authors cite often involves longer-term predictions.\n\nW3: Insufficient experimental scope\n\nModel diversity: Only GPT-4o is tested. While the authors mention DALL-E 3's limitations, they should test other recent VLM+IGM combinations (e.g., Gemini Pro Vision, LLaVA variants with diffusion models, or Claude with image generation capabilities).\nDomain limitations: The four domains use overly simplistic objects and backgrounds (solid white, uniform blue). The authors should evaluate on some of the established video prediction datasets (Moving MNIST, KTH Actions, BAIR Robot Pushing) or physical reasoning benchmarks (IntPhys, CATER, Physion) to test generalization beyond synthetic scenes, whichever the authors think is suitable for validating their method.\n\nW4: Incomplete related work The related work section misses important connections:\n\nVideo prediction literature (e.g., stochastic video generation, physics-informed neural networks)\nWorld models that perform similar step-by-step physical prediction\n\nW5: Sample sizes vary across domains (N=5 to N=20) without justification."}, "questions": {"value": "1. What fundamental properties of fluid simulation make Chain-of-Time perform worse than direct prediction?\n\n2. Can you provide results for simulations beyond 0.8 seconds to assess error accumulation?\n\n3. Have you tested on any established video prediction or physical reasoning benchmarks?\n\n4. Could you test additional VLM+IGM combinations to verify generalizability?\n\n5. Why do sample sizes differ across domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0CVKYMciB5", "forum": "f6ugrBWs3K", "replyto": "f6ugrBWs3K", "signatures": ["ICLR.cc/2026/Conference/Submission23193/Reviewer_7Pnk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23193/Reviewer_7Pnk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867195123, "cdate": 1761867195123, "tmdate": 1762942554059, "mdate": 1762942554059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a way for Image Generation Models to generate k steps in the future for a given input video. The challenge is getting the physics correct when simulating the next k steps. The proposed method is inspired by human mental simulation (where a trajectory is simulated in mind to predict the future state) and Chain of Thought prompting in LLMs (where the model is asked to answer step-by-step). The method works by giving the model input frames and asking the model to simulate small time-steps. Implicitly, the model de-renders, simulates the next step using transition dynamics, and re-renders to give the next frame. They find the physics of the trajectory stay consistent with a small delta t."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- shows strong results on 2d motion and gravity scene\n- There is a partial success in more complex simulations, like fluids and a bouncing ball\n- The method works at inference time and works with existing models like GPT-4o"}, "weaknesses": {"value": "- The mechanism is implicit (de-render, transition based on world transition matrix, rendering) and difficult to test\n- In 3D scenes, the early error seems to compound, making it difficult to simulate longer time-steps\n- Not much comparison with other existing methods (Video or World-Models) for generating physically plausible images generation \n- The generalization seems limited to very simple scenes and breaks when applied to more complex physics problems (fluid, bouncing)"}, "questions": {"value": "- The number of intuitive physics studies comparing machine learning models and humans' surprise rating to the plausibility of the scene. Are the authors planning on exploring this avenue to see if the mental simulation hypothesis still holds? \n- Have you considered testing whether Chain-of-Time generalizes beyond intuitive physics to intuitive psychology? (model vs human rating for plausibility rating of psychology scenes) \n- Have the authors explored combining their method with an external physics engine?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eBIcyiJUnG", "forum": "f6ugrBWs3K", "replyto": "f6ugrBWs3K", "signatures": ["ICLR.cc/2026/Conference/Submission23193/Reviewer_nzrx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23193/Reviewer_nzrx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880317169, "cdate": 1761880317169, "tmdate": 1762942553712, "mdate": 1762942553712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a way to use language models with image generation abilities as a way to simulate physical systems.\nThe model is presented with a few frames of a physical system (say, a moving ball) with corresponding time steps, and is asked to produce a prediction of the system in the future (up to a specific time) in the form of an image. The paper suggests that doing so \"step by step\" like chain of thought instead of in a single go produces better results.\nThe method is demonstrated on a few simple physical scenarios in 2D and 3D."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is interesting in its approach and the general context of the problem is important.\nIt's a well written paper and is easy to follow. I also enjoyed the clarity of the method description and the ample detail given.\nI thought using computer vision algorithms to extract the state for better analysis was a nice idea, but see below."}, "weaknesses": {"value": "I think the main issue of the paper is its scope and especially the experimental setup.\n\nI understand why using such simple physical systems was necessary if exact state estimates are needed, but this is a major hinderance for the paper. The experiments only cover a very simple set of physical systems under ideal observation conditions - I feel that to conclude anything about model's abilities to reason about physics, much more detailed and elaborate systems should be examined and analyzed. At this scope of experiments this feels more like an initial study rather than a fully formed paper.\n\nAnother weakness of the experiments is that it's not clear if other models would exhibit similar results - using only a single model makes drawing any kind of conclusions about the proposed method quite speculative. \n\nIn summary - the paper requires more scope to prove to be a significant contribution to the community."}, "questions": {"value": "above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BVbtEqfZI6", "forum": "f6ugrBWs3K", "replyto": "f6ugrBWs3K", "signatures": ["ICLR.cc/2026/Conference/Submission23193/Reviewer_dVtH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23193/Reviewer_dVtH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931274724, "cdate": 1761931274724, "tmdate": 1762942553440, "mdate": 1762942553440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes chain of time simulation, or generating intermediate images during a simulation, to evaluate the capabilities of image generation models on predicting simulated and natural evaluations. They find that this significantly improves performance over the baseline, and suggests that image generation models are able to simulate some properties over time."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Studies an unique topic - physics understanding in image generation models\n- Motivates the study from an interdisciplinary perspective"}, "weaknesses": {"value": "- limited evaluation. image generation models span a variety of designs, and evaluating only one is insufficient.\n- given that gpt's image generation is a closed model with little public detail, it may be difficult to act on these findings to improve image generation models\n- limited context from related work\n- experimental results are unclear. for example, the paper mentions that figure 6 shows that IGM is able to simulate the projectile's motion because it is close to ground truth, but the pattern does not seem to behave in a way consistent with a physics equation."}, "questions": {"value": "How would you differentiate your method from the visual chain of thought line of work (which also produce intermediate images)? The related work mentions the final goal being that of producing an image, but there is existing literature applying CoT to image generation as well.\n\nCould you provide in the appendix additional images from the evaluation and across time steps? Image generation models often make other mistakes that are not related to physics, which may affect evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "N2FBvXQlpe", "forum": "f6ugrBWs3K", "replyto": "f6ugrBWs3K", "signatures": ["ICLR.cc/2026/Conference/Submission23193/Reviewer_qjNx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23193/Reviewer_qjNx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000357867, "cdate": 1762000357867, "tmdate": 1762942553016, "mdate": 1762942553016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}