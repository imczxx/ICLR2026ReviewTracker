{"id": "JzA6d2II4Q", "number": 3809, "cdate": 1757530771482, "mdate": 1759898068733, "content": {"title": "VLM-Guided Adaptive Negative Prompting for Creative Generation", "abstract": "Creative generation is the synthesis of new, surprising, and valuable samples that reflect user intent yet cannot be envisioned in advance. This task aims to extend human imagination, enabling the discovery of visual concepts that exist in the unexplored spaces between familiar domains.\nWhile text-to-image diffusion models excel at rendering photorealistic scenes that faithfully match user prompts, they still struggle to generate genuinely novel content. \nExisting approaches to enhance generative creativity either rely on interpolation of image features, which restricts exploration to predefined categories, or require time-intensive procedures such as embedding optimization or model fine-tuning.\nWe propose VLM-Guided Adaptive Negative-Prompting, a training-free, inference-time method that promotes creative image generation while preserving the validity of the generated object.\nOur approach utilizes a vision-language model (VLM) that analyzes intermediate outputs of the generation process and adaptively steers it away from conventional visual concepts, encouraging the emergence of novel and surprising outputs.\nWe evaluate creativity through both novelty and validity, using statistical metrics in the CLIP embedding space. Through extensive experiments, we show consistent gains in creative novelty with negligible computational overhead. \nMoreover, unlike existing methods that primarily generate single objects, our approach extends to complex scenarios, such as generating coherent sets of creative objects and preserving creativity within elaborate compositional prompts. Our method integrates seamlessly into existing diffusion pipelines, offering a practical route to producing creative outputs that venture beyond the constraints of textual descriptions.", "tldr": "", "keywords": ["Generative models", "Computational graphics"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6da7bc30f525b7043ff4258b2adb3b66fe364494.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this work, the authors presents a new way to make AI image generation more creative without extra training or heavy computation. The method, called VLM-Guided Adaptive Negative-Prompting, uses a vision-language model to gently push the model away from familiar ideas and toward more surprising and novel results. Unlike older methods that need fine-tuning or stay stuck in known categories, here, the model works on the fly during image creation. The authors demonstrate the effectiveness of the method, showing that their method makes images that are both novel and valid, unlike the other methods keeping realism while adding originality."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper has the following strengths:\n\n1) The images shown on the paper are visually stunning, easily the best I have seen in this topic.\n\n2) The paper is extremely well-written, I really enjoyed reading it. In particular, it has one of the best intros I have ever read with perfect merging of the intro with the figures, a really case-study of how pictures complement the writing. All parts of the papers are really well-written, and all the pictures are nice and helpful.\n\n3) The results, be them qualitative, quantitative, or user study are really good, outperforming the other methods they compare with. The ablations studies are also very nice, further improving the confidence in the paper.\n\n4) Th"}, "weaknesses": {"value": "The paper can be improved on this part:\n\n1) Limited novelty - Probably the only technical contribution of the paper is section 3.2, which is extremely thin. And even then, it is effectively a smart way of doing prompting.\n\nSaying that, I would not penalize the paper for it. The results speak for themselves, so I would actually prefer a simple method compared to a complex one, given the same results (Occam's Razor in reviewing).\n\n2) It would have been nice if the authors would have released the code already (to check the effectiveness of this work) but considering that a) that is not mandatory, b) they promised to release in the near future, I will not penalize them for it."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v9BCf3GEla", "forum": "JzA6d2II4Q", "replyto": "JzA6d2II4Q", "signatures": ["ICLR.cc/2026/Conference/Submission3809/Reviewer_MwEz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3809/Reviewer_MwEz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760720375593, "cdate": 1760720375593, "tmdate": 1762917044876, "mdate": 1762917044876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors presents a new way to make AI image generation more creative without extra training or heavy computation. The method, called VLM-Guided Adaptive Negative-Prompting, uses a vision-language model to gently push the model away from familiar ideas and toward more surprising and novel results. Unlike older methods that need fine-tuning or stay stuck in known categories, here, the model works on the fly during image creation. The authors demonstrate the effectiveness of the method, showing that their method makes images that are both novel and valid, unlike the other methods keeping realism while adding originality."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper has the following strengths:\n\n1) The images shown on the paper are visually stunning, easily the best I have seen in this topic.\n\n2) The paper is extremely well-written, I really enjoyed reading it. In particular, it has one of the best intros I have ever read with perfect merging of the intro with the figures, a really case-study of how pictures complement the writing. All parts of the papers are really well-written, and all the pictures are nice and helpful.\n\n3) The results, be them qualitative, quantitative, or user study are really good, outperforming the other methods they compare with. The ablations studies are also very nice, further improving the confidence in the paper."}, "weaknesses": {"value": "The paper can be improved on this part:\n\n1) Limited novelty - Probably the only technical contribution of the paper is section 3.2, which is extremely thin. And even then, it is effectively a smart way of doing prompting.\n\nSaying that, I would not penalize the paper for it. The results speak for themselves, so I would actually prefer a simple method compared to a complex one, given the same results (Occam's Razor in reviewing).\n\n2) It would have been nice if the authors would have released the code already (to check the effectiveness of this work) but considering that a) that is not mandatory, b) they promised to release in the near future, I will not penalize them for it."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v9BCf3GEla", "forum": "JzA6d2II4Q", "replyto": "JzA6d2II4Q", "signatures": ["ICLR.cc/2026/Conference/Submission3809/Reviewer_MwEz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3809/Reviewer_MwEz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760720375593, "cdate": 1760720375593, "tmdate": 1763379773170, "mdate": 1763379773170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VLM-Guided Adaptive Negative Prompting, a training-free, inference-time method that leverages vision-language models to steer diffusion models away from familiar visual concepts during generation. By dynamically adding negative prompts for concepts identified by the VLM at intermediate denoising steps, the method promotes the creation of novel images."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The proposed method is conceptually simple and can be easily integrated into existing diffusion inference pipelines without training.\n3. The method achieves strong qualitative results across diverse categories while remaining completely training-free."}, "weaknesses": {"value": "1. The paper omits citations of several recent works in performing and understanding creative generation, such as [1], [2], and [3].\n2. While Section 4.5 presents qualitative examples using complex prompts, there is no quantitative or systematic evaluation of controllability. This makes the evidence for controllable generation less conclusive.\n\n[1] Procreate, don’t reproduce! propulsive energy diffusion for creative generation, ECCV 2024\n\n[2] Does Generation Require Memorization? Creative Diffusion Models using Ambient Diffusion, ICML 2025\n\n[3] An analytic theory of creativity in convolutional diffusion models, ICML 2025"}, "questions": {"value": "1. Have you explored designing heuristics to automatically select VLM queries based on generation prompts? Such an approach could make the system easier to use and closer in workflow to standard text-conditioned diffusion generation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xxx6IVZSgB", "forum": "JzA6d2II4Q", "replyto": "JzA6d2II4Q", "signatures": ["ICLR.cc/2026/Conference/Submission3809/Reviewer_VWC1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3809/Reviewer_VWC1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536181847, "cdate": 1761536181847, "tmdate": 1762917044666, "mdate": 1762917044666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a training-free method for enhancing creative generation in diffusion models. By leveraging a Vision-Language Model (VLM) to analyze intermediate denoising steps and dynamically accumulate negative prompts, the approach steers generation away from conventional patterns while maintaining categorical validity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on an interesting topic in computational creativity: generating novel visual concepts beyond conventional patterns.\n\nThe technical approach is elegantly simple and training-free, leveraging VLM feedback for adaptive negative prompting without modifying pretrained models. This clarity enhances reproducibility and practical deployment.\n\nThe writing is exceptionally clear and well-structured, with logical flow from problem formulation to experiments."}, "weaknesses": {"value": "The technical contribution is somewhat limited, as the method does not adequately address the VLM's inconsistent perception capabilities across different denoising timesteps. Prior research highlights that control word effectiveness varies with timesteps, yet this work overlooks such dynamics, potentially undermining the robustness of adaptive guidance.\n\nWhile the approach yields intriguing outcomes, it heavily relies on the base model's generative power rather than introducing groundbreaking mechanisms. Similar creative effects might be achievable through carefully engineered prompts or LoRA adaptations, questioning the necessity of the proposed complex feedback loop.\n\nControllability remains a significant issue, as the generation process is highly stochastic. Results are unpredictable and quality assurance depends largely on \"luck-based\" sampling, which fails to guarantee consistency or align with specific user preferences, limiting practical utility.\n\nThe method introduces non-negligible computational overhead due to frequent VLM queries, despite optimizations. This could hinder real-time applications, especially with resource-intensive VLMs, affecting scalability.\n\nEffectiveness is sensitive to VLM selection and question design, requiring manual tuning for different categories. This dependency on external components may reduce generalizability and increase implementation complexity."}, "questions": {"value": "See weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QXuWkIPYOu", "forum": "JzA6d2II4Q", "replyto": "JzA6d2II4Q", "signatures": ["ICLR.cc/2026/Conference/Submission3809/Reviewer_c3MD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3809/Reviewer_c3MD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811145658, "cdate": 1761811145658, "tmdate": 1762917044448, "mdate": 1762917044448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the creative image generation task, an emerging research direction that explores the ability of image generation models to produce novel and previously unseen images beyond the training distribution. The authors propose VLM-Guided Adaptive Negative Prompting, a method that leverages a vision-language model (VLM) during training to adaptively refine negative prompts, guiding the generative model to diverge from known concept spaces and thereby produce more unexpected and creative visual outcomes."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is simple and requires no additional training overhead.\n2. The experimental results are visually appealing and demonstrate interesting creative effects."}, "weaknesses": {"value": "1. The method lacks substantial novelty. Its core idea—encouraging the diffusion model to deviate from known concept spaces—was originally introduced by ConceptLab. The main contribution here lies in performing additional VLM-guided queries at each denoising step and using classifier-free guidance (CFG) to avoid categories identified by the VLM. Compared to ConceptLab, this constitutes only a minor incremental improvement. Moreover, querying the VLM at every denoising step could considerably increase inference time, even though the authors claim that it adds only about 13 seconds.\n2. The proposed approach is less flexible than ConceptLab, which operates at the token level, allowing its generated creative concepts to be easily integrated with natural language for diverse styles and contexts. In contrast, the current method requires a separate process for each prompt, causing inference time to scale with the number of prompts and limiting adaptability.\n3. Although the visual results are impressive, it is unclear whether the improvements stem from the proposed method itself or from the use of a stronger base model (SD3.5). When applied to other backbones such as Kindinsky or SD-XL, the method’s performance degrades noticeably (Figure 5)."}, "questions": {"value": "1. The proposed method performs VLM queries at every denoising step. I am curious whether a VLM can effectively recognize images that are still heavily corrupted by noise. It seems unnecessary to query the VLM at each step, since it primarily identifies common object categories and may produce unreliable or meaningless predictions in the early denoising stages. Why not predefine a set of common negative classes at the beginning of the process? In ConceptLab, repeated experiments tend to yield similar negative class sets—mostly consisting of frequent categories such as cat, dog, parrot, rat, and lizard—while rarer categories like fish or monkey seldom appear.\n\n2. Could SD3.5 alone, guided only by human-written prompts, generate the same level of creative results shown in the paper? For example, could it produce a plausible image of an unseen fruit purely based on human imagination without relying on the proposed adaptive negative-prompting mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5knZ02ML8M", "forum": "JzA6d2II4Q", "replyto": "JzA6d2II4Q", "signatures": ["ICLR.cc/2026/Conference/Submission3809/Reviewer_d6am"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3809/Reviewer_d6am"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762328608106, "cdate": 1762328608106, "tmdate": 1762917044272, "mdate": 1762917044272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}