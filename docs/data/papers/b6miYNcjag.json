{"id": "b6miYNcjag", "number": 13181, "cdate": 1758214813215, "mdate": 1759897458482, "content": {"title": "Data Reliability Scoring", "abstract": "How can we assess the reliability of a dataset without access to ground truth? We introduce the problem of reliability scoring for datasets collected from potentially strategic sources. The true data are unobserved, but we see outcomes of an unknown statistical experiment that depends on them. To benchmark reliability, we define ground-truth–based orderings that capture how much reported data deviate from the truth. We then propose the Gram determinant score, which measures the volume spanned by vectors describing the empirical distribution of the observed data and experiment outcomes. We show that this score preserves several ground-truth-based reliability orderings and, uniquely up to scaling, yields the same reliability ranking of datasets regardless of the experiment -- a property we call experiment agnostic. Experiments on synthetic noise models, CIFAR-10 embeddings, and real employment data demonstrate that the Gram determinant score effectively captures data quality across diverse observation processes.", "tldr": "We propose the Gram determinant score, an experiment-agnostic method for reliability scoring that assesses dataset quality without access to ground truth.", "keywords": ["Reliability Scoring", "Gram Determinant Score", "Information Elicitation", "Mechanism Design"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/62057baf244c80eed1df28b92305a5152e2991df.pdf", "supplementary_material": "/attachment/4376fec92dacbfa331588ac49b8582de77656707.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the problem of evaluating data reliability when ground-truth data are unobserved and only reported data and observations are available. The authors propose a reliability score, the Gram Determinant Score (GDS) defined as the determinant of the Gram matrix of observation distributions, capturing the information diversity among reported data. The paper shows that GDS preserves key reliability orderings and is independent of the observation process. Two estimators are proposed (plug-in and stratified matching), along with a GDS with kernels that generalizes the method to continuous or structured observation spaces."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper provides theoretical results on preservation of reliability orderings (Exact, Blackwell, Hamming).\n2.\tThe proposed score admits a geonetrical interpretation.\n3.\tThe GDS with Kernel kernelized extends GDS for application to high-dimensional or structured data (e.g., feature embeddings)."}, "weaknesses": {"value": "1.\tThe results hold under restrictive assumptions.\n2.\tEstimation of Gram matrices can be expensive for large $d$ or $N$.\n3.\tThe theoretical strengths of GDS might not easily turn into practical guidance.\n4.\tComparaison with other metrics is not provided."}, "questions": {"value": "1.\tThe paper does not provide a comparison of the proposed Gram Determinant Score (GDS) with existing reliability metrics. It would be informative to see how GDS performs relative to measures such as mutual information or other established scores.\n\n2.\tThe computational complexity of the method is not evaluated. In particular, it is unclear whether the kernelized version can scale efficiently to large datasets or high-dimensional embeddings. \n\n3.\tIn Experiment 2, the kernel is chosen as $K(y,y’) = \\langle y,y’ \\rangle$, but the paper does not justify this choice. Could alternative kernels affect the ranking or monotonicity of the score? Are there specific properties a kernel must satisfy for the method to remain valid? Furthermore, which kernel choices are most appropriate for real-world structured data, such as images, text, or signal embeddings?\n\n4.\tIt is unclear whether the Gram Determinant Score can be normalized across datasets to allow direct comparisons of reliability between heterogeneous datasets. For practical applications, it would also be useful to establish whether a meaningful threshold exists above which a dataset can be considered “reliable.” Clarifying these points would enhance both the interpretability and the practical applicability of the method.\n\nThis work makes a conceptually novel and theoretically elegant contribution to the study of data reliability by introducing a unified geometric measure that encompasses several classical reliability orderings. While the theoretical results are compelling and the determinant-based approach is original, the study would benefit from further empirical validation and a more thorough investigation of its scalability and robustness in practical, real-world settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LLg8i8JqMf", "forum": "b6miYNcjag", "replyto": "b6miYNcjag", "signatures": ["ICLR.cc/2026/Conference/Submission13181/Reviewer_2ZZc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13181/Reviewer_2ZZc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761149261419, "cdate": 1761149261419, "tmdate": 1762923880795, "mdate": 1762923880795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a way to score how reliable different reported datasets are. It uses extra signals or observations that are related to the true labels and uses them to tell which dataset is closer to the truth. It formalizes how to say that dataset A is closer to the true data than dataset B, then defines a new score called the Gram Determinant Score to capture that. The score is computed from the joint information between the reported labels and the extra signals, and under some conditions it gives the same ranking no matter which observation process you used. Experiments on synthetic data, CIFAR-10 style image embeddings, and on an employment dataset show that the score goes down when the labels are corrupted, so it can detect which datasets are better and which are noisier."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The problem is clearly set up. They say what it means for one reported dataset to be better than another and connect it to standard ideas like Blackwell orderings.\n\n- The method has an intuitive picture. Clean data gives a Gram matrix with a bigger determinant. Noisy data makes it smaller.\n\n- The main theorem is strong. If the observation processes are linearly independent, this score gives the same ranking across them and is basically unique."}, "weaknesses": {"value": "- The strongest results need linearly independent observation processes and some structure on the reporting noise. In real data those conditions may not hold exactly. \n\n- The score is defined to preserve certain orderings, but in practice you never see those orderings because you never see the true labels. So it can be hard to know if it is right for your case.\n\n- Determinants of Gram matrices can be numerically small and unstable, especially in higher dimensions.\n\n- The experiments are convincing but still mid scale. We do not see behavior for very large label spaces or very heavy class imbalance.\n\n- There is not a wide empirical comparison with other label quality or data pruning methods, so it is hard to tell how big the gain is."}, "questions": {"value": "- How sensitive is the Gram Determinant Score if the observation process is not perfectly linearly independent but only close to it? Can you show a robustness curve?\n\n- In practice how many samples do we need before the score ranking is stable, especially with many classes?\n\n- How should we pick the kernel and its parameters in the embedding setting?\n\n- Can the score be fooled if a subset of reports is adversarial or all collapsed to a popular class?\n\n- What is the computational cost for large d and can we use low rank or randomized approximations?\n\n- Did you compare to simpler agreement or mutual information based scores on the same datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "awG4y0aTee", "forum": "b6miYNcjag", "replyto": "b6miYNcjag", "signatures": ["ICLR.cc/2026/Conference/Submission13181/Reviewer_YGfJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13181/Reviewer_YGfJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935971079, "cdate": 1761935971079, "tmdate": 1762923880525, "mdate": 1762923880525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the novel problem of reliability scoring: assessing dataset quality without ground truth. The authors formalize this setting for data collected from potentially noisy or strategic sources and define several ground-truth–based reliability orderings (Exact Match, Blackwell, Hamming/Distance) as benchmarks for evaluating reliability metrics.\n\nThey show fundamental impossibility results, proving that no score can universally preserve all such orderings. To overcome this, the authors propose the Gram Determinant Score (GDS), a geometric measure that quantifies the “volume” spanned by observation distributions — smaller volumes indicate greater deviation from truth. The GDS enjoys strong theoretical properties: it preserves key reliability orderings under mild conditions, is experiment-agnostic (ranking consistency across observation mechanisms), and generalizes naturally to continuous domains via kernelization.\n\nExperiments on synthetic categorical data, CIFAR-10 embeddings, and U.S. employment statistics confirm that the GDS correlates well with ground-truth reliability and consistently ranks more trustworthy datasets higher."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper formally introduces the setting of reliability without ground truth, which is both theoretically interesting and practically relevant for data collected from uncertain or biased sources (e.g., social, economic, or self-reported data).\n\nGDS has a clean intuition: it measures the “volume” of observation distributions, which naturally shrinks as the data deviate from truth. This connects statistical structure to an interpretable geometric property.\n\nExperiments cover both synthetic (controlled corruption), vision (CIFAR-10 embeddings), and real-world (employment data) domains, demonstrating the score’s consistency and practical usability."}, "weaknesses": {"value": "The mathematical framing may be too abstract for practical deployment; real-world users might find the link between Q,P, and reliability difficult to interpret or estimate.\n\nThe theoretical results rely on independence and linearity assumptions in the observation model P; it is unclear how robust the score remains when these are violated.\n\nWhile the three experiments are convincing, all datasets are relatively small-scale or well-structured; results on larger, noisier real-world datasets (e.g., survey or crowd-sourced data) would strengthen claims of generality."}, "questions": {"value": "Can GDS be intuitively understood as a variance or entropy measure over observation space? How might practitioners interpret a “high” or “low” Gram Determinant Score in practical settings?\n\nIn settings where the observation process P is unknown or partially known (e.g., in survey data), how could one practically estimate or approximate P for computing GDS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "M8XlVNJuli", "forum": "b6miYNcjag", "replyto": "b6miYNcjag", "signatures": ["ICLR.cc/2026/Conference/Submission13181/Reviewer_JUwe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13181/Reviewer_JUwe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762063073687, "cdate": 1762063073687, "tmdate": 1762923880162, "mdate": 1762923880162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}