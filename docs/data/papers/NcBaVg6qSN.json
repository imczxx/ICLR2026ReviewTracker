{"id": "NcBaVg6qSN", "number": 22920, "cdate": 1758337098762, "mdate": 1763358155256, "content": {"title": "TSGym: Automatic Model Design Framework for Deep Multivariate Time-Series Forecasting", "abstract": "Recently, deep learning has driven significant advancements in multivariate time series forecasting (MTSF) tasks. \nPrevailing paradigm in MTSF research involves proposing models as pre-defined, holistic architectures. Such an approach limits adaptability across diverse data scenarios, and obscures the individual contributions of their core components.\nTo address this, we propose TSGym, a novel framework for automated MTSF model design. The framework begins with \ndecoupling existing deep MTSF methods into fine-grained components, which enables a large-scale, component-level evaluation that offers crucial insights, and creates a vast space for the automated construction of potentially superior models. Leveraging this space through strategic sampling, a core meta-learner is trained to learn the mapping between component configurations and performance across multiple traininig datasets. This enables it to perform zero-shot selection of a top-performing model for any new, unseen time series data. \nExtensive experiments indicate that the model automatically constructed by our proposed TSGym significantly outperforms existing state-of-the-art MTSF methods and AutoML solutions, and exhibit high potential for \ntransferability across diverse datasets.", "tldr": "", "keywords": ["Time Series Forecasting", "Large-scale Benchmark", "Model Selection", "Automated Machine Learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6001dc2d7d2384940e14be038565174d4777d93a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Time series modeling is a very active area of research, ranging from Statistical models to ML, then DL, and finally FM. This paper explicitly focuses on the multivariate aspect and the DL side of the vertical. This paper conducted an in-depth study of all components used in DL, including data preprocessing, Modeling, etc.\n\nWhile attempting to standardize, this should be an objective of an OSS library such as sktime, and using their components preferably.  I have provided several comments for manuscript improvement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The concept of generating guidance for a new TSFM pipeline based on characteristics of the dataset is interesting (partially) and tested for DL DL-based component. This may potentially reduce the barrier for the end user. Not everyone is an expert in the field, and the author should put the work from this angle. AutoML's goal was to reach the general audience, and you are making life easier for them."}, "weaknesses": {"value": "1. The word Gym has a different meaning in the RL community. The author should explain the word and its association with the RL world. \n2. The font in all the images is tiny (Figure 1, Figure 2, Table 1, Table 2). Also, the color combination in Figure 1 needs some work to make it more readable. \n3. The meta learning approach for time series data is not novel, and it has been there for a long time. \n4. The contribution section of TSGym needs to be more quantitative, say, overall what the important insights are they find, the improvement, and how it performs with SOTA. \n5. There has been an approach to AutoML for Time Series using the ML approach and the Foundation model approach. Why were those not considered for the baseline approach? \n6. The datasets are the same as Gift-Eval, and this is why we do not use a standard approach and the baseline from these platforms to compare the net-gain the TSGym achieved. \n7. The meta-learning needs to be compared with some baseline from the literature. Say ranking-based method was already discussed in some prior art - Catch22, and then some of their \n8. As highlighted, this toolkit is developed for whom> what are the end user personas? \n9. Did the author standardize their component with any OSS library, say sktime? Or say IBM-AutoAI-TS or H2O-AutoAI-TS? \n10. Was their any hyper-parameter grid being prepared?"}, "questions": {"value": "Please address all weak points"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0TUuGoCvc0", "forum": "NcBaVg6qSN", "replyto": "NcBaVg6qSN", "signatures": ["ICLR.cc/2026/Conference/Submission22920/Reviewer_1pkZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22920/Reviewer_1pkZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567621861, "cdate": 1761567621861, "tmdate": 1762942438393, "mdate": 1762942438393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We are sincerely grateful to all the reviewers for their time and effort in evaluating our manuscript. We especially thank Reviewer 4ymY for the encouraging assessment and detailed feedback, and we appreciate all reviewers for their constructive suggestions.\n\nThe reviewers consistently and rightfully pointed to the computational efficiency of our method as a primary concern. We fully acknowledge that the up-front cost of conducting extensive experiments to build the meta-learning performance matrix is significant. This was a key critique from Reviewers K9mH, tqV, and 4ymY.\n\nIt is worth clarifying that a key advantage of our approach is that once this initial (admittedly high) cost is paid, our method can be transferred to new datasets with minimal overhead. Furthermore, to mitigate this primary challenge, optimizations such as using Optuna to refine the component search have been explored. This optimization significantly reduces the number of initial experiments needed to build a high-quality meta-training dataset, thereby lowering the pre-training cost. This also proves beneficial when migrating to new tasks.\n\nRegarding the NAS comparison, we wish to clarify that our work is fundamentally zero-shot, requiring no iterative training on the target dataset. A direct comparison to iterative NAS methods may not be entirely fair. Furthermore, existing NAS frameworks are not specifically designed for deep time series forecasting. Adapting them would require invasive code development and significant re-componentization, a substantial undertaking. We did, however, conduct preliminary comparisons against other iterative AutoML methods that are more readily reproducible for time series forecasting, and our approach maintained a leading advantage.\n\nWe believe there are many exciting extensions to this work, such as a more granular component-level analysis and a more efficient architecture prediction framework, which were also alluded to by the reviewers. However, the limited time frame may not be sufficient to complete the comprehensive comparisons and in-depth analysis required to fully address these important points and satisfy the reviewers. Therefore, after careful consideration, the decision has been made to withdraw our submission.\n\nWe remain confident in the core contributions of TSGym: (1) providing the first large-scale, component-level evaluation for MTSF methods, offering critical insights; (2) offering an automated, meta-learning-based model constructor that outperforms SOTA methods; and (3) broadening the discussion to systematically include emerging large time-series models.\n\nWe thank all the reviewers once again. Your feedback is invaluable and will be instrumental in guiding the refinement of this research."}}, "id": "DWaoV1hZ2Y", "forum": "NcBaVg6qSN", "replyto": "NcBaVg6qSN", "signatures": ["ICLR.cc/2026/Conference/Submission22920/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22920/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763358154361, "cdate": 1763358154361, "tmdate": 1763358154361, "mdate": 1763358154361, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes automatic model selection for time-series forecasting. It is claimed to select a component-level configuration such that a customized model can be produced. It does so via a meta learner trained to select the best model for given datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) the proposed method performs well; \n2) method has some novelty."}, "weaknesses": {"value": "1) I wonder how this proposed method differs from a structural learning approach such as NAS. \n2) It is necessary to compare this method against other structural learning methods, although comparisons with other model selections have been done. \n3) complexity analysis should be provided."}, "questions": {"value": "1) I wonder how this proposed method differs from a structural learning approach such as NAS. \n2) It is necessary to compare this method against other structural learning methods, although comparisons with other model selections have been done. \n3) complexity analysis should be provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S6T45Zz5s4", "forum": "NcBaVg6qSN", "replyto": "NcBaVg6qSN", "signatures": ["ICLR.cc/2026/Conference/Submission22920/Reviewer_tqYV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22920/Reviewer_tqYV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719511154, "cdate": 1761719511154, "tmdate": 1762942438065, "mdate": 1762942438065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TSGym, an automated model-design framework for multivariate time-series forecasting. The authors decouple existing deep MTSF pipelines into fine-grained design dimensions (preprocessing, encoding, architecture, optimization), define a design space, and sample configurations to build a performance matrix over multiple datasets. They train a meta-predictor that maps dataset meta-features and component embeddings to relative performance ranks, enabling zero-shot selection of a top model for a new dataset. Extensive experiments across standard benchmarks show that the automatically constructed pipelines often outperform strong baselines and AutoML competitors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper systematically breaks MTSF pipelines into meaningful component dimensions and enumerates many realistic design choices; this level of granularity is useful for both analysis and automated construction.\n2. The work evaluates a broad set of configurations across multiple widely used time-series benchmarks and includes several thoughtful ablations (sampling strategy, pool size, architecture subsets), producing extensive experimental evidence."}, "weaknesses": {"value": "1. Incremental novelty. The main novelty is engineering and scale — expanding the AutoML/search space to include data-processing choices and recent model types (LLMs/TSFMs). Conceptually this is a natural, incremental extension of prior AutoML work.\n2. Reliance on predictor correctness for insights. Many of the paper’s component-level analyses and insights about which design choices work best depend heavily on the meta-predictor’s accuracy. In contrast, [1] employs a sampling-based statistical strategy, which potentially lead to more reliable insights into design choices.\n3. Lack of Computational Cost Analysis: There is no discussion of the GPU hours required to collect the training samples. As shown in Table F3, the authors collected 57,600 training samples—this process could take a very long time to train even if they use 12 GPUs.\n\n[1] Designing Network Design Spaces."}, "questions": {"value": "see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mkwBpKDpx9", "forum": "NcBaVg6qSN", "replyto": "NcBaVg6qSN", "signatures": ["ICLR.cc/2026/Conference/Submission22920/Reviewer_K9mH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22920/Reviewer_K9mH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726952872, "cdate": 1761726952872, "tmdate": 1762942437704, "mdate": 1762942437704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TSGym, a framework for large-scale evaluation, component-level analysis and automated model construction in deep multivariate time series forecasting tasks. This leads to three key contributions: First, the large-scale component-level analysis allows insights into general design choices for forecasting models. Second, the automated component-based model construction yields models that outperform current state of the art models. Third, TSGym widens the scope of multivariate time series forecasting by including novel time-series architectures like LLMs or time series foundation models in the framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Originality: TSGym is the first automated system that generates models for time-series forecasting based on individual components.\n* Quality: An ablation study is provided in the appendix, validating the design choices of the proposed framework. TSGym is compared in a fair manner to other approaches. The performed work and presented theory are correct and reasonable.\n* Clarity: The paper is well-written, uses concise language and is easy to follow. The research goal, proposed methodology and the conducted experiments are clearly understandable.\n* Significance: TSGym beats state-of-the-art forecasting model approaches on commonly used datasets. Additionally, several general research questions in the field, like transformer vs. MLPs, are addressed in a meta-study."}, "weaknesses": {"value": "* The paper does not mention the computational costs of using TSGym. It would be beneficial to provide additional information i.e. about computational time for the experiments conducted. Currently, it is difficult to estimate broader usability, especially on older and/or weaker hardware.\n* Chapter 5 (Conclusion, limitations) does not mention any limitation of the methodology. I suspect that one limitation might be computational costs, especially when applying TSGym to a new domain that requires full retraining.\n* To my understanding, each experiment was only conducted once per configuration but averaged over 4 predictions lengths per configuration. However, results would be more robust if there were repeated evaluations per configuration per prediction length.\n\nMinor\n--------\n* The citations Ailing Zeng 2023a and Ailing Zeng 2023b are identical.\n* Deep learning is abbreviated as DL in the introduction but not consequently used afterwards (i.e. in 2.1 the abbreviation is not used).\n* There is a missing space in chapter 4.1 (baseline): latestapproach\n* There is a grammar error in 4.3 (Question 4): Does large time-series models bring significant improvement for TSGym? The subject “models” is plural, so the verb should be “do” instead of “does”  Do large time-series models bring …\n* The appendix could be reordered according to the reader’s flow (i.e. start with appendix B as it is mentioned in the text first and move appendix A further back)."}, "questions": {"value": "* It is claimed that TSGym has zero-shot capabilities through the trained meta-learner. The paper states that chapter 4.3 shows the effectiveness of this meta-predictor. However, I don’t understand how the results prove the effectiveness of the meta-learner. Is TSGym only applied to the mentioned datasets? If yes, which datasets did you use to train the framework? Please clarify this.\n* The usage of meta features is mentioned in chapter 3. However, there is no mention about the number of extracted features and/or usage of an automated approach to feature extraction. While it is thoroughly explained in appendix G, I think it would be easier to understand by mentioning the usage of TSFEL and the number of extracted features in the main part of the paper, i.e. in chapter 4.1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bRgfh1O9WW", "forum": "NcBaVg6qSN", "replyto": "NcBaVg6qSN", "signatures": ["ICLR.cc/2026/Conference/Submission22920/Reviewer_4ymY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22920/Reviewer_4ymY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994282198, "cdate": 1761994282198, "tmdate": 1762942437071, "mdate": 1762942437071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}