{"id": "sGMbneFX2n", "number": 14257, "cdate": 1758231305764, "mdate": 1763728153574, "content": {"title": "PinTok: Tokenizers Deserve Dedicated Pinned CPU-Compute and Memory", "abstract": "Tokenization is the first point of contact between large language models (LLMs) and text data, yet it has not been viewed by many as a component of LLMs worth accelerating. During inference, tokenizers typically rely on simple dictionary lookups and are executed on CPUs as standard processes. This approach, however, introduces significant overhead from scheduling delays, core selection, data copying, and other system-level costs. These inefficiencies become problematic in latency-sensitive applications such as embedding, small language models, and agentic AI. In this paper, we present the Pinned Tokenizer (PinTok), a novel tokenizer architecture that reduces redundant hardware, operating system, and networking overhead through three key innovations: core and memory pinning, scheduling and context switch avoidance, and duplicate network packet copy and processing avoidance. Our implementation of PinTok can serve as a drop-in replacement for existing tokenizer deployments, delivering latency reductions of up to 95\\% (average), 97\\% (P50), 94\\% (P90), and 87\\% (P99) along with throughput improvements of up to 2,084\\%.", "tldr": "We present a fast, efficient, and fungible architecture for tokenizers by removing redundant system overheads, yielding lower end-to-end latency for all models and applications requiring tokenizers.", "keywords": ["Faster Tokenizer", "Machine Learning System", "CPUs", "Network Offload"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72db8e4161a1d1af1cc424a6c96e0428b7cd31f5.pdf", "supplementary_material": "/attachment/d84472cbf89c2922aa6e720f62d4b47e540fcbab.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents PinTok, a software implementation of tokenization that improves upon standard implementations in terms of throughput and latency. These improvements stem from three optimizations: (1) pinning the CPU cores and RAM used by the tokenizer; (2) avoiding process scheduling costs and context switching by continuously polling for incoming inputs to tokenize from the network card, which encourages the OS to keep it on the same core; and (3) copying incoming inputs directly from the network card into userspace RAM, bypassing kernel space RAM. The results in the paper show that PinTok has much better throughput and latency on a variety of benchmarks versus Rust and Python implementations of BPE, and the proportion of runtime that is reduced improves for smaller models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper's goal is to provide a faster tokenizer implementation, and the results show nice improvements. Tokenizers are used wherever LLMs are used, so a speedup to the tokenizer could be a significant contribution. I'm not aware of prior work that has applied these techniques to tokenizers. The work generally appears to be of good quality, with a reasonable breadth of models and datasets in the experimental section. Section 2 is helpful for providing technical background and breaking down the potential areas for speedups. The authors helpfully quantify what proportion of time is taken up by the tokenizer in a small-scale LM setup, and therefore the headroom for speeding up the tokenizer."}, "weaknesses": {"value": "A lof of my issues with this paper have to do with clarity; I have included a number of clarifying questions in the Questions section.\n\n1. The techniques that PinTok relies upon have tradeoffs; otherwise, why couldn't we use them for every aspect of the LM pipeline, instead of just the tokenizer? For instance, pinning the tokenizer to a CPU core obviously takes that resource away from all other processes running on the system, potentially slowing down other aspects of the LM pipeline, or other processes that need to run on the same system. The experimental results show that the overall pipeline is still faster, but I would like to see more discussion of the tradeoffs of each of these techniques, how PinTok is able to get away with using these tricks despite the tradeoffs, and why the benefits outweigh the costs specifically in the case of tokenization, as opposed to any other aspect of the LM pipeline.\n1. It isn't clear why the NIC comes into play. What I've inferred from reading through the paper is that a process must use PinTok by having PinTok run in its own process and must use the NIC as an IPC mechanism. On this point, wouldn't it be better to avoid IPC altogether and have the code that runs the LM and the code that tokenizes run in one process that's pinned to a dedicated CPU core? This would eliminate all of the overhead from using the NIC and from polling.\n1. The polling trick seems like an undesirable solution to avoiding context switching, as it's essentially a busy-waiting loop, and it doesn't guarantee that context switches won't occur. How often does PinTok poll for inputs?\n1. I found Algorithms 1 and 2 to be more cryptic than helpful, since the reader needs to infer a lot of details from the names of the functions.\n1. Fig 5: I don't understand what the x axis is. Is each point along the x axis measuring the speed of tokenizing a single input of a certain length? If not, how many different inputs were used? How was the input string selected?\n1. If I understand correctly, the authors use Rust and Python reimplementations of BPE as their baselines. I would rather they compare to existing libraries, namely Hugging Face, SentencePiece, and tiktoken. Otherwise, it's not clear if the speedups are actually attributable to their reimplementation.\n1. It should also be noted that the primary contribution of this paper is a software artifact, but the authors have not provided any code.\n1. 012: \"yet it has not traditionally been viewed as a component of LLMs worth accelerating\" -- I don't think this is true. Consider the C++ implementation of SentencePiece or the Rust implementation in Hugging Face. See also https://aclanthology.org/2023.findings-acl.38/."}, "questions": {"value": "1. Does PinTok also take care of pretokenization (splitting on whitespace and punctuation, grouping digits, etc.), or just the BPE algorithm proper?\n1. Did you carefully unit-test your reimplementation of the tokenizer to ensure that it is equivalent to the existing implementations in Hugging Face and SentencePiece?\n1. 477: It sounds like because you're chunking up the input into packets and then tokenizing, you're not producing a result that's equivalent to the original implementation. Is this the case?\n1. 367: Do all models in this paper use BPE? What do you mean by different tokenization algorithms?\n1. What language is PinTok written in?\n1. Table 3: How is latency saving computed?\n1. Can you measure the actual number of context switches? Does it decrease with PinTok?\n1. Can you actually quantify where the speedups come from, following the equations in Section 2? Which optimizations are the most important?\n\nTypos:\n* 069: works -> work\n* There are a few \\citep vs. \\citet mistakes\n* 083: ungrammatical\n* 100: ungrammatical\n* 191: minimizer -> minimize\n* Fig 5: plot titles overlap\n* 436: Appendix XXX\n* Why is Time to First Byte abbreviated as TTFT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gY0pqev5hQ", "forum": "sGMbneFX2n", "replyto": "sGMbneFX2n", "signatures": ["ICLR.cc/2026/Conference/Submission14257/Reviewer_XPZb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14257/Reviewer_XPZb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761426359081, "cdate": 1761426359081, "tmdate": 1762924709641, "mdate": 1762924709641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common response"}, "comment": {"value": "We thank the reviewers for their detailed and constructive feedback. The reviewers generally found our problem formulation well-motivated, the speedups significant, and the contribution valuable. The reviewers raised several shared concerns, which we address in our common response below.\n\n**General concern 1: Availability of code.**\nThe reviewers raise the excellent point that our primary contribution is the software artifact. We have now made our full implementation available as supplementary material.\n\n\n**General concern 2: Applications that would benefit from PinTok's acceleration**\n\n\nWe expect PinTok to provide the greatest benefit in applications that rely on ensembles of small language models, such as embedding models, vector databases for Retrieval-Augmented Generation (RAG), and agentic AI systems, where tokenizer latency constitutes a more significant portion of end-to-end runtime. As the compute cost of LLMs continues to fall (with some estimates suggesting a 40x reduction per year), these use cases will become more prevalent.\n\n\n\n**General concern 3: Is there a tradeoff to using CPU resources?**\nIn typical LLM-inference or training pipelines, CPUs are often significantly underutilized. In such settings, dedicating a single pinned CPU core incurs no practical cost and does not interfere with other processes. In cases where CPUs are highly utilized for tokenization, CPUs are effectively being pinned for tokenization anyways. Consequently, there is effectively no tradeoff: PinTok simply leverages otherwise idle CPU capacity to achieve strictly superior efficiency. \n\n$$ $$\n\nWe address the reviewers' individual concerns in the corresponding individual responses."}}, "id": "A1Ndc8lBGc", "forum": "sGMbneFX2n", "replyto": "sGMbneFX2n", "signatures": ["ICLR.cc/2026/Conference/Submission14257/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14257/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14257/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763726113106, "cdate": 1763726113106, "tmdate": 1763726113106, "mdate": 1763726113106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present Pinned Tokenizer (PinTok) for addressing scheduling delays, core selection,\ndata copying, and other system-level costs that standard LLM tokenizers face when working in latency-sensitive applications. Tokenizers translate strings inputs into model ingestible representations, but despite this importance or often just implemented as dictionary lookups. PinTok improves their implementation specifically by introducing techniques for core and memory pinning, scheduling and context switch avoidance, and processing avoidance. Using PiTok shows latency reductions reaching 95%, on average, and throughput increases of up to 2000%."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is, as far as I'm aware, a first to take a systems level treatment for optimizing tokenization. This is quite novel, and differs from the traditional focus of algorithmic improvement.\n- The approach is \"agnostic to the specific tokenizer algorithm\", making it a drop-in replacement that has substantial performance upsides. Moreover, the approach doesn't depend on specific hardware, making it broadly applicable.\n- The problem is well motivated. Specifically, in Table 1 they show significant proportion of time is spent on tokenization for smaller models. Likewise in Table 2 the authors clearly show the major sources of latency during tokenization.\n- The experimental setup is thorough. The authors benchmark against web text, code, and multilingual datasets — representing the diverse sources of information that is commonly tokenized for LLMs. The number of trials for each benchmark give good evidence that the approach is generalizable.\n- The presentation of the paper is well done. The problem and solution are clear and the experiments have a clear focus and takeaway. The writing is easy to follow."}, "weaknesses": {"value": "- Line 436: Missing the link (\"Appendix XXX\") to detailed numerical results for all datasets and configurations.\n- Unclear what the trade-off is with this approach. Specifically, the dedicates resources it requires to pin CPU cores and 8GB+ of pages exclusively for tokenization — is this too significant in some setups?\n- No statistical tests of significant used, however the experimental setup is rather robust based the number of repitions they performed and the range in experiments.\n- The major performance improvements occur in specific environments. Table 2 shows that PinTok is only slightly faster than Rust and Python tokenizers for  `GPT-OSS 120b query (TTFT)` and `Qwen3 4B query (TTFT)`. And the performances are generally smaller for larger models which are likely to most benefit do to their large scales.\n- The authors describe the many different sources of latency, however the experimental setup doesn't do any sort of ablation study to show which of their techniques contributes the most to the performance improvements (e.g., pinning versus the page size).\n- Reproducibility concerns: the code isn't available with the paper, but may be later."}, "questions": {"value": "- Is Appendix C \"WHY EVEN CONSIDER USING PYTHON TOKENIZERS?\" necessary? This thought never crossed my mind and it seems like python tokenizers should be the baseline due to their widespread usage.\n- Is there any reason to believe that this approach wouldn't result in a fully accurate and equivalent tokenization as the baselines? Did you perform any checks to validate that the results match baselines exactly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y57UHeV3DJ", "forum": "sGMbneFX2n", "replyto": "sGMbneFX2n", "signatures": ["ICLR.cc/2026/Conference/Submission14257/Reviewer_9a6d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14257/Reviewer_9a6d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865579630, "cdate": 1761865579630, "tmdate": 1762924708966, "mdate": 1762924708966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The system PinTok aims to build a system to speedup the tokenization bottleneck in many system.  They attempt to reduce the cost via core and memory pinning, network packet dedup, and reduce context switching overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "In practice, I’ve seen the cost of tokenizers. I find this work to be a valuable contribution in terms of reducing the overall cost of tokenization time."}, "weaknesses": {"value": "1. I find the systems contribution to be slightly weak/lacking enough new novel approaches. The core idea boils down to just pinning/huge pages. The projects feels like it can do a lot more work/and is incomplete. \n2. The optimizations provided are not very specific to tokenizer handling and seem to not take advantage of tokenizer properties."}, "questions": {"value": "Can you describe what makes the Pintok system challenging? Why are these optimizations specific to tokenization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BYfCtzk94R", "forum": "sGMbneFX2n", "replyto": "sGMbneFX2n", "signatures": ["ICLR.cc/2026/Conference/Submission14257/Reviewer_sqje"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14257/Reviewer_sqje"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762738384874, "cdate": 1762738384874, "tmdate": 1762924708541, "mdate": 1762924708541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PinTok, which is a tokenizer implementation that uses core and memory pinning, while avoiding duplicate network packet copying & processing as well as context switching & scheduling. The paper presents basics of the infrastructure design space and timing considerations along with the method. PinTok provides massive speedups in the experimental settings that are presented."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper has several merits:\n\n* **Clarity**. The paper clearly presents the PinTok method. Presents hardware / os /networking concepts in a way that can be easily understood by ICLR community. \n* **Effectiveness**. The empirical setting compares the throughput/latency of standard tokenizers to PinTok. There is significant speedups due to PinTok's dedicated design. This is impressive and something that can likely be of interest to many working on models/applications in which tokenization takes up significant time."}, "weaknesses": {"value": "My concerns about the paper are: \n\n* **Details about Tokenize -> Forward Pass**- Suppose we are receiving data continuously, tokenizing it, pushing it through the model (at least forward pass), why can't we do pipeline-parallelism to have the CPUs of the machine be processing & tokenizing data of batch i+1 and the accelerators running the model on batch i? How does this setting relate to the wall clock %s that are shown on Table 1? I am a bit confused because it seems like this can hide a large part of the costs of tokenization for many such applications? (Apologies if I have missed this or if there is something obvious here that I did not understand. However, I looked carefully again at the paper am still confused hence raising this issue.)\n* **Scaling questions** - The paper does not, as far as I can tell, report how the tokenization performance scales with number of cores / parallelism? This seems important not only to understand if there are settings where we need to parallelize differently with the proposed method. Furthermore, it would be interesting to understand if things like vocab size plays a roll in the efficiency of the method. These seem like standard considerations in such a setting about scalable tokenization.\n\nMinor:\n\"Line 436 Appendix XXX\""}, "questions": {"value": "Please see weakness above. In summary: \n1) Can we tokenize batch i+1 while batch i is processed with the transformer?\n2) Does the approach scale linearly with number of CPUs?\n3) Does increasing vocab size affect PinTok differently than normal tokenizers?\n4) When would PinTok not be the right tokenizer implementation to use?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LQ7m3jgAKy", "forum": "sGMbneFX2n", "replyto": "sGMbneFX2n", "signatures": ["ICLR.cc/2026/Conference/Submission14257/Reviewer_7Nnk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14257/Reviewer_7Nnk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762769654731, "cdate": 1762769654731, "tmdate": 1762924708077, "mdate": 1762924708077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}