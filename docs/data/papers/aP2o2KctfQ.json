{"id": "aP2o2KctfQ", "number": 22024, "cdate": 1758325012699, "mdate": 1759896890557, "content": {"title": "DIFFUSION–ATTENTION CONNECTION", "abstract": "We show that the diffusion map affinity matrix is the twisted Hadamard product of the self-attention matrix. Concretely, let the generalized feature-similarity matrix be $\\mathcal W = M + A$ with $M=M^\\dagger$ a Hermitian (real part, encoding geometry) and $A=-A^\\dagger$ skew-Hermitian (imaginary part, encoding directionality). Softmax applied to the real logits from $M$ yields a first-order, row/column-stochastic attention operator. The diffusion kernel then arises as the twisted Hadamard product (a Product-of-Experts identity), producing a symmetric second-order affinity whose spectrum matches diffusion maps. The skew part $A$ contributes only phases; placing them outside the softmax yields a $U(1)$ gauge-equivariant ``magnetic'' variant without breaking stochasticity.", "tldr": "", "keywords": ["Diffusion Maps", "Self-Attention", "Magnetic Laplacian", "Manifold Learning", "Kernel Methods", "Random-Walk"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f93a2131306093e3982bd05ae009a219d86fc60.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper establishes a theoretical connection between diffusion maps and self-attention through a common bilinear kernel, showing that the diffusion affinity matrix can be viewed as a twisted Hadamard product of attention matrices. By decomposing the generalized feature-similarity matrix into Hermitian (geometry) and skew-Hermitian (directional) parts, the paer demonstrates that attention acts as a first-order operator, while diffusion emerges as its symmetric second-order form. The paper further introduces a U(1) gauge-equivariant “magnetic” attention variant. Overall, the paer offers a mathematically rigorous framework connecting manifold learning and transformer-based architectures through shared stochastic and spectral properties."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. I appreciate the theoretical insight and I really like the idea. The paper introduces a highly original conceptual link between diffusion maps and self-attention, proposing that diffusion can be viewed as the second-order symmetric form of attention via a twisted Hadamard product. This unification provides a fresh perspective on how attention mechanisms relate to manifold learning and stochastic operators, offering strong theoretical grounding.\n\n2. The mathematical treatment is really **elegant**. The decomposition of the similarity matrix, the Product-of-Experts view of softmax, and the gauge-equivariant extension are presented with clear and rigorous derivations."}, "weaknesses": {"value": "1. The paper places heavy emphasis on theoretical derivations without sufficiently discussing how the proposed formulation contributes to practical applications. The link between diffusion maps and self-attention is intellectually appealing, but the paper does not explore whether this connection can actually improve real models(for example, enhancing transformer interpretability, improving training efficiency, or guiding new architecture designs). The work stops at the theoretical insight and lacks a clear roadmap showing how these ideas could translate into practical benefits for machine learning.\n\n2. The paper does not include numerical experiments, case studies, or quantitative benchmarks to validate the claimed connection. As a result, it remains unclear whether the theory holds in real settings. Important components, such as how to construct the bilinear dissimilarity matrix within modern neural networks, are not specified. Moreover, the proposed U(1) gauge-equivariant “magnetic” variant is theoretically interesting but remains untested, with no empirical demonstration of gauge invariance or performance effects.\n\n3. The exposition is notation-heavy with long equations and dense mathematical sections that may hinder readability. There are no figures, tables, theorems or remarks to help readers internalize the key ideas. The paper may be difficult to digest for readers outside a narrow theoretical audience, reducing its overall accessibility.\n\nOverall, I like this paper and I think this paper makes a contribution. If the authors can include sufficient empirical results during the rebuttal, I will increase my score."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dvifVhnZ2o", "forum": "aP2o2KctfQ", "replyto": "aP2o2KctfQ", "signatures": ["ICLR.cc/2026/Conference/Submission22024/Reviewer_hwEy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22024/Reviewer_hwEy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761544469805, "cdate": 1761544469805, "tmdate": 1762942023862, "mdate": 1762942023862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows a connection between self-attention and diffusion maps from manifold learning. They start with a Bilinear Dissimilarity (BD) matrix, which is partitioned and softmaxed to produce a pair of attention-matrices. The Hadamard product of these two matrices gives a Markov transition-matrix, which can be used to create a diffusion-map Laplacian. Incorporating the imaginary part of the BD matrix outside the softmax yields a $U(1)$ gauge-equivariant attention mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper appears to be making an important connection between manifold learning and the self-attention. However, given my limited knowledge of the former, I do not fully understand the implications of this revelation. I was able to follow their overall logic, despite some steps that were particularly opaque (see below). I appreciate that the authors worked with complex valued data, which revealed the resemblance to magnetic Laplacians."}, "weaknesses": {"value": "1. This paper is not an easy read despite, and maybe because of, its brevity. Some manipulations left me very confused. For instance, between lines 186 and 190 how do they introduce $ 1_i \\left( R_{iY} W_{YX}^Q W_{XZ}^K R^{\\dagger}_{Zj} \\right)_j $ inside the softmax? I suspect they were using the shift-invariance property from (10). Note that there is a typo in this equation, as written in the paper: it should be $u_i$ and not $u_j$. But the softmax in the aforementioned lines has subscript $j$ which does not match $1_i$ inside. What is going on?\n\n2. I am doubtful whether the paper, in its present form, will be accessible or interesting to the broader ICLR community. This is not an assessment of the soundness of the paper. Instead, the paper does little to explain to the non-expert why this contribution is relevant."}, "questions": {"value": "What are the potential applications of this insight? Is there a toy problem that would highlight the usefulness of creating a diffusion map this way? Can the exposition be made pedagogic/constructive by starting from a simple specific example and generalizing more gradually?\n\nTo summarize, while the idea of connecting self-attention to diffusion maps is intriguing, the presentation lacks sufficient clarity and motivation for the ICLR audience, so I cannot recommend acceptance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C69yY2EQfY", "forum": "aP2o2KctfQ", "replyto": "aP2o2KctfQ", "signatures": ["ICLR.cc/2026/Conference/Submission22024/Reviewer_bWrs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22024/Reviewer_bWrs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791008937, "cdate": 1761791008937, "tmdate": 1762942023614, "mdate": 1762942023614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an algebraic bridge between self‑attention and diffusion-map methods (not to be confused with diffusion processes in generative models). Using a bilinear dissimilarity, the authors show that (i) the usual diffusion‑map kernel can be expressed as a “product‑of‑experts” of two axis‑normalized attention matrices, and (ii) phases from the skew-Hermition A encoding directionality can be placed outside of softmax to yield a U(1)  gauge-equivariant \"magnetic\" attention."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper offers a compact identity linking two kernels—attention (first‑order, normalized) and diffusion maps (second‑order, symmetric). The Product of Experts (PoE) of softmax derivations for linking these kernels are mathematically concise and intuitive, as well as novel to my knowledge."}, "weaknesses": {"value": "My main objection to this paper is that it seems poorly written. As a theory paper, there is not a single formal statement (theorems, propositions, or lemmas), nor are there many clear/rigorous statements of assumptions or definitions. Despite the short length of the paper, the connections between different parts are not immediately clear (i.e., where the authors show which specific contribution). The clarity and motivation of the setup are also not sufficiently justified.\n\nHowever, if the authors made explicit actionable plans that address the readability concern as well as clearly identify specific impact and actionable use for their contributions and insights, I will consider raising my score. See also the questions below."}, "questions": {"value": "1. The authors said one of the core contributions is \"to provide a principled recipe to learn M from data.\" What exactly is the recipe? The only place where I found relevance is in Section 2.1, but that seems rather like a definition or design choice, not a \"learning recipe\".\n\n2. The mathematical derivation steps appear to be the central contribution. However, the significance or impact of these mathematical connections is not made explicit. What are some insights one can take from this paper to other contexts?\n\n3. In the last section, the authors said \"We established a Diffusion–Attention Connection: under mild and explicit conditions, ...\", where exactly are the explicit conditions stated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mHKhGPVzvV", "forum": "aP2o2KctfQ", "replyto": "aP2o2KctfQ", "signatures": ["ICLR.cc/2026/Conference/Submission22024/Reviewer_QVsu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22024/Reviewer_QVsu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762295262119, "cdate": 1762295262119, "tmdate": 1762942023260, "mdate": 1762942023260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study diffusion maps and the attention mechanism via a common construction through a generalized dissimilarity measure between complex-valued samples $i, j$ as \n$$\nD_{ij}^{2} = d_{ij}^{(-)} + d_{ij}^{(+)} + i B_{ij},\n$$\nwhere the real terms $d^{(\\pm)}$ are viewed as providing a connection between diffusion and attention maps. \n\nOne of the main takeaways is that the diffusion map kernel $P$ can be constructed via products of two directed attention kernels $\\mathcal{A}^{-}, \\mathcal{A^{+}}$ as \n$$\nP \\propto \\mathcal{A}^{-} \\odot  \\mathcal{A}^{+}.\n$$\nWe learn: \n- attention acts like a first-order *directed* operator interpreted as the transition matrix for an (one-step) asymmetric random walk between data point $i$ and data point $j$\n- diffusion acts like a second-order operator via \"composition\" of attention operators."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**Originality: moderate** \nThe paper introduces a generalized bilinear dissimilarity measure which decomposes the relationship between data points into a symmetric geometric part $M$ and an asymmetric, direction part $A$. That the symmetric diffusion kernel $P$ can be constructed from a Hadamard product of two directed, first-order attention operators is a new mathematical observation. These observations lead to an interesting proposal of \"magnetic\" attention, which would incorporate directionality via complex phases. \n\n**Quality: poor**\nThe technical quality of the paper is mixed. The derivations appear sound, but are not well-motivated. The calculations and their interpretation and conclusions are terse and not very helpful. The paper acknowledges the limited applicability of the diffusion-attention connection, but does so in a way that is superficial and potentially misleading. For instance the conclusion mentions failure of the analogy under minor technical issues, while failing to emphasize standard attention mechanisms do not construct symmetric operators at all and are fundamentally asymmetric. \n\n**Clarity: poor**\nThe paper is not written with sufficient clarity to make its contributions accessible or limitations apparent. The exposition is terse and relies on analogies to physics that obscure the argument for non-physicists. \n\n**Significance: moderate** \nThe significance of the paper is that it provides a new and elegant mathematical framework unifying diffusion and attention mechanisms. This could in principle inspire valuable future research and new architectural innovations."}, "weaknesses": {"value": "**Why?**\nThe paper explores a theoretical link between attention and diffusion, but never answers (beyond curiosity) why the exploration is practically useful. Nor motivates this question. \n\n\n**Practical attention.** \nThe paper's claims could be clarified to distinguish more sharply between two different kinds of contributions: \n1. A descriptive theory of *existing* attention mechanisms,\n2. A mathematical basis for *future* architectural innovations. \n\nThe non-negotiable requirement of symmetry for the diffusion-attention analogy to hold nullifies the contribution toward understanding existing attention mechanisms. For example, a transformer with standard query $\\to$ key attention uses only the asymmetric operator $\\mathcal{A}^{+}$ identified in the paper, and not the symmetrized version that provides the connection to diffusion. \n\nThe conclusion briefly discusses when attention behaves as diffusion and when it does not. But attention in practice does not behave like diffusion, as pointed out. \"Mixed axes\" and \"missing normalization\" only pertain to an incorrect construction of $P$ from $\\mathcal{A}^{\\pm}$, but these concerns do not address that standard transformers only use $\\mathcal{A}^{+}$ alone. \n\nThe gap between what this theory can and cannot say about current, realistic attention mechanisms should be explained with more clarity. The primary contribution is a theoretical link that motivates new forms of attention. \n\n**Magnetic attention.**\nMagnetic attention is proposed to decompose attention into a real-valued magnitude and a complex phase. The paper does not provide arguments or experiments to demonstrate the viability of training such a mechanism. It seems to speculate about replacing a relatively more-understood, easier-to-optimize mechanism for encoding directionality in data for a more complicated, harder-to-optimize, more restrictive mechanism for the sake of a theoretically satisfying connection to diffusion. The paper succeeds in showing that such a mechanism *can* be constructed (and explains a satisfying mathematical link to diffusion) but the paper does not present a convincing case such a mechanism *should* be constructed."}, "questions": {"value": "Motivation / justification for Equation 1? There should be a more intuitive explanation for $\\mathcal{D}_{ij}^{2}$ as Equation (1) does not transparently communicate it is a \"generalized distance.\" \n\nI would like some of the limitations identified in the \"weaknesses\" section addressed. In particular, exposition of and viability of the speculative magnetic attention as well as the limitations of the analogy to practical attention mechanisms."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nEyesQRtoY", "forum": "aP2o2KctfQ", "replyto": "aP2o2KctfQ", "signatures": ["ICLR.cc/2026/Conference/Submission22024/Reviewer_w1eu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22024/Reviewer_w1eu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762538515078, "cdate": 1762538515078, "tmdate": 1762942023051, "mdate": 1762942023051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}