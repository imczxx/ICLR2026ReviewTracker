{"id": "IwiwmY3Mzz", "number": 17250, "cdate": 1758273880709, "mdate": 1763742430491, "content": {"title": "A Reward-Free Viewpoint on Multi-Objective Reinforcement Learning", "abstract": "Many sequential decision-making tasks involve optimizing multiple conflicting objectives, requiring policies that adapt to different user preferences. In multi-objective reinforcement learning (MORL), one widely studied approach addresses this by training a single policy network conditioned on preference-weighted rewards. In this paper, we explore a novel algorithmic perspective: leveraging reward-free reinforcement learning (RFRL) for MORL. While RFRL has historically been studied independently of MORL, it learns optimal policies for any possible reward function, making it a natural fit for MORL's challenge of handling unknown user preferences. We propose using the RFRL's training objective as an auxiliary task to enhance MORL, enabling more effective knowledge sharing beyond the multi-objective reward function given at training time. To this end, we adapt a state-of-the-art RFRL algorithm to the MORL setting and introduce a preference-guided exploration strategy that focuses learning on relevant parts of the environment. Through extensive experiments and ablation studies, we demonstrate that our approach significantly outperforms the state-of-the-art MORL methods across diverse MO-Gymnasium tasks, achieving superior performance and data efficiency. This work provides the first systematic adaptation of RFRL to MORL, demonstrating its potential as a scalable and empirically effective solution to multi-objective policy learning.", "tldr": "", "keywords": ["Multi-objective reinforcement learning", "reward-free reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0377ffff820fcfc6e7c78b4c81d0211c685e9369.pdf", "supplementary_material": "/attachment/af9542793ddd9929853cd9cbc96b8ba493cd0b7a.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a reward-free viewpoint for multi-objective reinforcement learning (MORL). It adapts the Forward–Backward (FB) representation from reward-free RL to MORL, aiming to learn a task-agnostic policy/value representation that can generalize to unseen preference vectors without retraining. Experiments on MO-Gymnasium tasks show improved performance and generalization compared with existing MORL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a new perspective on MORL, suggesting that learning beyond specific scalarized reward combinations can accelerate MORL through more effective knowledge sharing across preferences.\n2.The empirical evaluation is comparatively thorough for MORL: it includes multiple tasks (both discrete and continuous control), standard metrics such as hypervolume, analyses of generalization, and ablations (e.g., with and without PG-Explore and auxiliary losses)."}, "weaknesses": {"value": "1. The comparison with related work based on Successor Features (SF) and its variants is insufficient.\n2. Some methodological and experimental descriptions are unclear. Please see the questions below."}, "questions": {"value": "1. The proposed method and the goal of fast adaptation to unseen preferences appear conceptually close to SF. What concrete limitation of SF does the FB formulation address? Could the authors provide an ablation comparing FB + PG-Explore with an SF-based approach?\n2. In Section 3 \"Training objective function\", the paper introduces both a measure loss and an auxiliary Q-loss. How are these two losses combined, particularly for the forward network? If they are combined, how sensitive is performance to their relative weighting?\n3. In Figure 6, except for MORL-FB (w/o Q-loss), the hypervolume of other baselines is close to the proposed method, while UT performs significantly worse. Could the authors explain the reason for this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V7MOtAredo", "forum": "IwiwmY3Mzz", "replyto": "IwiwmY3Mzz", "signatures": ["ICLR.cc/2026/Conference/Submission17250/Reviewer_3sX6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17250/Reviewer_3sX6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761513825084, "cdate": 1761513825084, "tmdate": 1762927203685, "mdate": 1762927203685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reward-free viewpoint for MORL and an approach, MORL-FB, that integrates reward-free RL methods into MORL. Extensive experiments are conducted to show the effectiveness and generalization of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work investigates the integration of a reward-free framework into multi-objective RL methods for better performance and transferability, which is novel yet natural.\n2. A motivating example is provided to show the design consideration of the exploration of *z*, revealing the core insight of this work.\n3. Extensive experiments are conducted on both discrete and continuous tasks, comparing with a wide range of baselines.\n4. The visualizations are intuitive and show that the method is capable of handling multi-modal distributions, implying its improved generalization."}, "weaknesses": {"value": "1. The layout on page 4 may need optimization. Algorithm 1 and Figure 1 can be presented on one line.\n2. This work focuses on model design and experimental analysis, while the “reward-free viewpoint” in the title seems more theoretical. Further theoretical analysis may be needed, or the title and abstract should focus more on the model’s effect.\n3. The title of Figure 20(a) seems mis-specified; it may be “Humanoid2d.”"}, "questions": {"value": "1. How is the transfer implemented through the framework of RFRL? A brief explanation based on the motivating experiment in Figure 1 would be appreciated.\n2. How are $L_Q$ and $L_M$ balanced in line 20 of Algorithm 2? Is there any evidence that directly adding them up is plausible?\n3. Why is the Q loss regarded as “auxiliary”? Does this mean the Q loss is less important compared with the measure loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pmm1GXD7rj", "forum": "IwiwmY3Mzz", "replyto": "IwiwmY3Mzz", "signatures": ["ICLR.cc/2026/Conference/Submission17250/Reviewer_d51d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17250/Reviewer_d51d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704771902, "cdate": 1761704771902, "tmdate": 1762927202699, "mdate": 1762927202699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper views MORL problems from a reward-free perspective and then solves them using the RFRL method. Based on RFRL, it introduces a preference-guided exploration strategy that incorporates reward functions and preferences into the original RFRL. Specifically, rather than directly sampling latent vectors from a normal distribution, it samples preferences uniformly and uses them to generate latent vectors. Finally, the proposed method is evaluated on a series of tasks in MO-Gymnasium and demonstrates superior performance and data efficiency."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-motivated. It solves MORL problems via RFRL, which is intuitive and novel and could serve as a good bridge between the two sub-fields.\n- The empirical evaluation and ablation studies are comprehensive and insightful, carefully demonstrating the effectiveness of the proposed approach from multiple aspects.\n- The paper is well-written and well-organized, with implementation details that are easy to understand."}, "weaknesses": {"value": "I didn’t find major issues in this paper."}, "questions": {"value": "- For completeness, I recommend briefly describing the future visited probabilities $M=FB$ in the preliminary section or appendix, as it is an important concept that helps readers understand why FB can be applied to arbitrary rewards.\n- In Figure 1, the lines are too dense and intertwined, making it difficult to interpret. It may be better to eliminate the curves corresponding to batch sizes 256 and 4096, or adjusting the scaling to improve clarity.\n- I recommend adopting more RFRL methods beyond FB to solve MORL in the future, which could lead to more insightful conclusions on RFRL's advantages in MORL."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fsFI4vfueE", "forum": "IwiwmY3Mzz", "replyto": "IwiwmY3Mzz", "signatures": ["ICLR.cc/2026/Conference/Submission17250/Reviewer_hbj3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17250/Reviewer_hbj3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859081493, "cdate": 1761859081493, "tmdate": 1762927202096, "mdate": 1762927202096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of multi-objective reinforcement learning. Current MORL algorithms typically assign preference weights to individual objectives during the training process, modifying the underlying utility function to discover one or multiple policies to satisfy different preferences. \nThis work proposes to instead use techniques adapted from reward-free reinforcement learning (RFRL), where no such preference weighting is included in the utility function. RFRL aims to learn a policy that is optimal for arbitrary reward functions. The authors note the conceptual connection to MORL problems, and suggest an extension of a state-of-the-art RFRL algorithm to the multi-objective setting. The proposed algorithm contains three important modifications: preference-guided exploration, training on latent vectors sampled from the replay buffer as auxiliary tasks, and auxiliary Q loss.\nThe algorithm is validated experimentally, showing clear improvement over other MORL algorithms in several standard multi-objective metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper draws a natural connection between RFRL and MORL, with a generally insightful discussion of the similarities and differences between the two. According to the authors, this is the first work to do so explicitly. This is a valuable contribution, and an interesting starting point for further work. (This reviewer is not intimately familiar with the current state of the art, and so must take the assertions about the novelty of the paper at face value)\n\n- The paper is well-written and largely well-organised. Experiments are well-constructed, with a solid statistical approach, hyperparameter search, reproducible parameters, and meaningful ablation studies.\n\n- The experimental validation of the algorithm shows good results compared to various state-of-the-art approaches and baselines."}, "weaknesses": {"value": "- The paper occasionally seems to conflate the field of MORL as a whole with the single MORL technique of linearly combining objectives, e.g. in the introduction and Section 3. As mentioned in the related work section, other MORL approaches exist. This should be clarified.\n\n- At times, this paper relies quite heavily on the appendix, in particular when relegating the discussion of related work from RFRL to the appendix. With the extension of the page limit during the revision phase, this should be moved to the main section to allow the paper to stand alone.\n\n- The conclusion in its current form is also quite short, and could benefit from the addition of slightly more detail about the method and context. Similarly, some figures are quite cramped (see additional comments).\n\n\nAdditional (minor) comments\n\n- Some of the figures, e.g. 1 and 3, are somewhat difficult to read. These could be made more readable despite the space constraints, e.g. by increasing font size and line thickness. Similarly, it is good that variance/error is represented in the plots, but in the current format this is barely readable.\n\n- Some minor language and formatting issues:\n1. line 050: “no prior work has explicitly adapt RFRL...”\n2. “across various tasks in … benchmark” (line 080/081)\n3. line 210: “where (a) holds by that ...”\n4. Eq. 5 extends into the page margin.\n\n- Occasionally, a citation is used as the subject of a sentence, e.g. in line 465 “(Felten et al., 2024; Mossalam et al., 2016) extended …”. In such cases, ‘\\citet{}’ can be used in LaTeX to generate a textual citation."}, "questions": {"value": "1. Perhaps this has been missed, but will the implementation of this methods be made available, ideally for integration with the existing benchmarking suite?\n\n2. How exactly were hyperparameters chosen for the proposed method (MORL-FB)? The appendix only mentions a decision “guided by prior research and the HPO results”, which seems not quite comparable to the HPO-tuning of the remaining algorithms."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Yn0sL1KUN9", "forum": "IwiwmY3Mzz", "replyto": "IwiwmY3Mzz", "signatures": ["ICLR.cc/2026/Conference/Submission17250/Reviewer_cCvf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17250/Reviewer_cCvf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986867229, "cdate": 1761986867229, "tmdate": 1762927201662, "mdate": 1762927201662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to All Reviewers"}, "comment": {"value": "We sincerely thank all the reviewers for their insightful comments and constructive suggestions, which have significantly helped us improve the clarity and presentation of our paper. In response to the feedback, with the one-page extension, we have revised the paper and made several improvements to enhance readability, clarity, and overall presentation, with changes highlighted in blue. We summarize the key updates as follows.\n### **(1) Layout and figure adjustments for visual clarity**\nFollowing Reviewer d51d’s suggestion, we improved the layout of Algorithm 1 and Figure 1 to ensure a cleaner visual structure. Additionally, in response to Reviewers cCvf and hbj3, we refined the presentation of Figures 1 and 3 to further enhance their readability and visual clarity.\n\n### **(2) Refined Related Work**\nBased on the comments from Reviewers cCvf and 3sX6, given the one-page extension, we have moved the related work about RFRL and successor features (SF) to the main text (Sections 5.2 and 5.3) and expanded it to more clearly describe their connections and strengthen the contextual framing of our contributions.\n\n### **(3) Refined abstract**\nIn response to Reviewer d51d, we have refined the abstract to better highlight the algorithmic and experimental focus of MORL-FB as well as the model’s empirical strengths, ensuring a more coherent presentation of the paper’s main contributions.\n\n### **(4) Additional background on successor measure** \nFollowing Reviewer hbj3’s suggestion, we added one appendix section (cf Appendix A) to provide the context about the basic concept of successor measure and its connection with the FB representations.\n\n### **(5) Conclusion augmentation**\nAs recommended by Reviewers cCvf and hbj3, we expanded Section 6 (Conclusion, Limitations, and Future Work) to provide a more complete and coherent closing discussion."}}, "id": "U3zOXGeF74", "forum": "IwiwmY3Mzz", "replyto": "IwiwmY3Mzz", "signatures": ["ICLR.cc/2026/Conference/Submission17250/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17250/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17250/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763740910800, "cdate": 1763740910800, "tmdate": 1763743084299, "mdate": 1763743084299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}