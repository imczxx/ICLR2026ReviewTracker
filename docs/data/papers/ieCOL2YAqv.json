{"id": "ieCOL2YAqv", "number": 15870, "cdate": 1758256364583, "mdate": 1759897276275, "content": {"title": "Defending against Backdoor Attacks via Module Switching", "abstract": "Backdoor attacks pose a serious threat to deep neural networks (DNNs), allowing adversaries to implant triggers for hidden behaviors in inference. Defending against such vulnerabilities is especially difficult in the post-training setting, since end-users lack training data or prior knowledge of the attacks. Model merging offers a cost-effective defense; however, latest methods like weight averaging (WAG) provide reasonable protection when multiple homologous models are available, but are less effective with fewer models and place heavy demands on defenders. We propose a module-switching defense (MSD) for disrupting backdoor shortcuts. We first validate its theoretical rationale and empirical effectiveness on two-layer networks, showing its capability of achieving higher backdoor divergence than WAG, and preserving utility. For deep models, we evaluate MSD on Transformer architectures and design an evolutionary algorithm to optimize fusion strategies with selective mechanisms to identify the most effective combinations. Experiments shown that MSD achieves stronger defense with fewer models in practical settings, and even under an underexplored case of collusive attacks among multiple models--where some models share same backdoors--switching strategies by MSD deliver superior robustness against diverse attacks.", "tldr": "We introduce a module-switching defense that outperforms weight averaging in mitigating backdoor attacks; its effectiveness is supported by theory on synthetic networks and strong empirical evidence on transformer models.", "keywords": ["Backdoor attacks", "backdoor defense", "model merging"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3ab05a6d9234db66b62824c4802924fe540d3cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a \"model merging\" approach to mitigating backdoors in neural networks. The paper works with different neural networkw with identical or very similar model architectures trained on the same type of data.\nExperiments are conducted on NLP (BERT) and ViT models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Addressing backdoors/Trojans is a challenging problem."}, "weaknesses": {"value": "There is a large body of work on backdoor defense, but  \nthe literature review at about line 054 is terse and odd.\nMany of the most potent backdoor defenses are post-training leveraging a small benign dataset. Papers in this setting like Neural Cleanse and I-BAU are not discussed (though this assumption is made on line 240). It's not clear what is meant by \"although compromised auxiliary models can be used as defensive references\". The original IARPA TrojAI solicitation on backdoor defense suggested the defender could use known-clean models and known-attacked models. But this scenario begs the question: How are these models identified without assuming the action of some unspecified backdoor detector or poisoning models with _known_ backdoor attacks? That is, this is a supervised defensive scenario.\n\nRe. Section 3, it's not clear to me why two-layer neural networks with additional assumptions are interesting.\n\nAt a high level, if all the models are likewise poisoned, I don't see how the proposed approach will be effective (noting that an effective backdoor attack will not affect the performance on benign examples available to the defender).\nThe advocated evolutionary search approach can be quite computationally complex.\n\nDo they discuss the impact on accuracy of this method?\n\nRe. backdoored models prioritize trigger features (line 313): The authors should also cite [H. Wang et al.  MM-BD...] there. It's not clear why defense baselines don't also include such methods (MM-BD, I-BAU, etc.) which I think would be a lot less computationally complex.\n\nThough the authors do discuss the amount of clean data required (line 458 with details in Appendix), I think the amount of data used for different methods could be reported in their performance results.  Moreover, in the body  of the paper the authors should report the computational costs of their method and those compared against.  I would have reported both the amount of data and computational costs should be in the body of the paper while the \"theory\" of Sec. 3 should be in the Appendix."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oxMl1eUCQZ", "forum": "ieCOL2YAqv", "replyto": "ieCOL2YAqv", "signatures": ["ICLR.cc/2026/Conference/Submission15870/Reviewer_W5FL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15870/Reviewer_W5FL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761087872252, "cdate": 1761087872252, "tmdate": 1762926092028, "mdate": 1762926092028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a module-switching defense (MSD) to mitigate the threat of backdoor attacks. MSD disrupts the backdoor shortcuts formed by backdoor attacks by strategically switching weight modules between multiple models. Compared to the previous weight averaging (WAG) method, MSD achieves stronger defense with fewer models, and additionally exhibits robust defense against collusive attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1，\tThe experiments cover both text and vision tasks and evaluate multiple model architectures.\n2，\tSection 3 provides a mathematical explanation, justifying why MSD is superior to the weight averaging method.\n3，\tThe authors designed five heuristic rules to guide the module-switching strategies, and the results demonstrate their effectiveness, as removing any of these rules generally leads to performance degradation.\n4，\tAlthough the search for module-switching strategies requires an offline cost, the authors report an acceptable runtime, taking 2.6 hours to merge two models and 4.3 hours to merge four models."}, "weaknesses": {"value": "1，\tThe defensive baselines used for comparison in the vision experiments are outdated. The evaluation should include comparisons against more advanced backdoor defense methods, such as [1] [2] [3].\n2，\tThe experiments for MSD are entirely based on the Transformer architecture; it is unclear whether it can be applied to traditional CNN architectures.\n3，\tThe primary purpose of Adaptive-Patch is to evade activation-based defenses. Hence, strictly speaking, it cannot be considered as an adaptive attack against the MSD method; it is recommended that the authors clarify its role in the discussion of adaptive attacks.\n4，\tWhat is the structural overlap between multiple optimal strategies found by different random seeds? Specifically, are there certain layers or modules for which the same source model is consistently selected across these strategies?\n\n[1] Gao Y, Chen H, Sun P, et al. Energy-based backdoor defense without task-specific samples and model retraining[C]//Proc. of International Conference on Machine Learning. 2024: 1-11.\n[2] Hou L, Feng R, Hua Z, et al. IBD-PSC: input-level backdoor detection via parameter-oriented scaling consistency[C]//Proceedings of the 41st International Conference on Machine Learning. 2024: 18992-19022.\n[3] Xie T, Qi X, He P, et al. Badexpert: Extracting backdoor functionality for accurate backdoor input detection[J]. arXiv preprint arXiv:2308.12439, 2023."}, "questions": {"value": "1，\tCan MSD be effective on CNN architectures? If not, what are the reasons?\n2，\tSimilar to WAG, does the MSD method also require that the models intended for merging must be homologous (i.e., possess identical base architectures) and target the same task?\n3，\tHow does the time spent to search for module-switching strategies compare to the key baselines mentioned in the paper (TIES, DARE, and WAG)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m9zcpEHcM0", "forum": "ieCOL2YAqv", "replyto": "ieCOL2YAqv", "signatures": ["ICLR.cc/2026/Conference/Submission15870/Reviewer_rsY2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15870/Reviewer_rsY2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761391168607, "cdate": 1761391168607, "tmdate": 1762926089543, "mdate": 1762926089543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Module-Switching Defense (MSD), a post-training defense mechanism against backdoor attacks in neural networks. Instead of relying on clean datasets or trusted models, MSD disrupts backdoor “shortcuts” by selectively swapping network modules between potentially compromised models. The authors justify the approach theoretically on two-layer networks and extend it to Transformers using heuristic scoring and an evolutionary search algorithm. Experiments on both NLP and vision datasets demonstrate superior robustness and generalization compared to weight-averaging and other baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of module-level switching as a defense mechanism is novel and distinct from existing model-merging approaches (e.g., WAG, DARE). It offers a new perspective on exploiting architectural modularity for security.\n- The theoretical justification in Section 3 is well-motivated, proving stronger backdoor divergence than weight averaging. The empirical analysis aligns with the theory.\n- The approach generalizes across modalities (text and vision) and architectures (RoBERTa, BERT, ViT), showing strong versatility.\n- MSD operates in post-training settings with limited clean data and no need for retraining, making it suitable for real-world scenarios."}, "weaknesses": {"value": "- The long theory part in Section 3 is not easy to follow. It would be better to add some concise explanations to connect those definitions and the theorem logically.\n- The evolutionary search (2M generations, ~2–4 hours) is computationally expensive and may limit deployment practicality; no efficiency analysis or early stopping heuristic evaluation in realistic resource settings is provided.\n- Despite the complexity of the method, the improvement in some cases seems to be limited (e.g., Tables 8 and 9), limiting the application of the method."}, "questions": {"value": "- How sensitive is MSD to the choice of heuristic weights? Is there an adaptive mechanism for tuning them automatically?\n- Could the evolutionary search be partially amortized or transferred across architectures to further reduce cost?\n- How would MSD behave if all source models share identical backdoors in both pattern and location (beyond Table 20 scenarios)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xBIgezqJb7", "forum": "ieCOL2YAqv", "replyto": "ieCOL2YAqv", "signatures": ["ICLR.cc/2026/Conference/Submission15870/Reviewer_qum2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15870/Reviewer_qum2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641708730, "cdate": 1761641708730, "tmdate": 1762926088612, "mdate": 1762926088612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the vulnerability of DNNs to backdoor attacks. The proposed defense MSD disrupts the spurious backdoor shortcuts by selectively swapping modules (such as attention and FFN components) between different models. To optimize switching strategies, MSD uses an evolutionary algorithm under a set of heuristics designed to break potential backdoor propagation paths. \n\nThe defense is validated with rigorous experiments under NLP and image classification tasks, using various Transformer models and several datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea is intuitive and novel.\n\n- Insightful theoretical motivation and clear visualization (Fig. 2 and Fig. 3)\n\n- The implementation of evolutionary search is practical and methodologically interesting approach."}, "weaknesses": {"value": "- The framework only works when multiple models (possibly all compromised) are available and cannot defend a single compromised model. While this is somewhat fine in the experimental setup, its limited applicability to real-world cases.\n\n- The method is evaluated only on classification tasks with Transformer-based architectures (text and vision). This might be outdated in current DNN research community. The effectiveness of MSD on generative models or non-Transformer architectures (such as CNNs) or other tasks (Object detection, multiple modalities) is not established."}, "questions": {"value": "- How could the framework be adapted or extended to handle scenarios where only a single potentially compromised model is available, as is common in real-world deployments?\n\n- Have you considered evaluating MSD on more diverse architectures or tasks, such as generative models, CNNs, or multimodal systems (LVLMs) to demonstrate broader applicability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vWsoVOX7iR", "forum": "ieCOL2YAqv", "replyto": "ieCOL2YAqv", "signatures": ["ICLR.cc/2026/Conference/Submission15870/Reviewer_9m3G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15870/Reviewer_9m3G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787312652, "cdate": 1761787312652, "tmdate": 1762926087811, "mdate": 1762926087811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Module-Switching Defense (MSD), a post-training technique to disrupt backdoor shortcuts in neural networks. MSD works by selectively exchanging weight modules between multiple trojan models. Theoretically, the analysis on two-layer networks proving module switching achieves higher backdoor divergence than weight averaging (WAG). For Transformer architectures, MSD employs an evolutionary algorithm guided by heuristic rules to find optimal switching strategies. \n\nMSD requires fewer models (as few as 2) compared to WAG (3-6 models), and works without trusted data or knowledge of attack types. Moreover, it has good transferability across models with the same architecture (e.g., RoBERTa, BERT, DeBERTa) and work across both text and vision domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides theoretical analysis to justify the module-switching approach. Theorem 1 formally proves that module switching achieves higher backdoor divergence than weight averaging (WAG) under identity activation, while Proposition 1 guarantees the existence of at least one switched model that outperforms WAG. \n\nMSD is resource-efficient, requiring only 2 models, compared to 3-6 models needed by methods like WAG and DAM. The method operates with minimal clean data, and requires no access to poisoned data, trusted reference models, or prior knowledge of attack types.\n\nThe method demonstrates exceptional transferability across multiple dimensions. The searched strategies are architecture-agnostic and transfer seamlessly across different models sharing the same structure. Also, MSD maintains effectiveness across diverse domains (text and vision) and multiple datasets."}, "weaknesses": {"value": "1. MSD operates as an incremental refinement of existing model merging techniques like WAG. While the paper successfully demonstrates a theoretical superiority in maximizing backdoor divergence, the core insight that combining models can suppress spurious correlations was already established. The introduction of evolutionary search serves primarily to optimize the composition step rather than introducing a surprising, non-obvious insight. The work therefore lacks the novelty expected of a top-tier contribution.\n\n2. The empirical evaluation relies heavily on datasets and model architectures that are either small or now considered non-state-of-the-art. Specifically, testing is performed on NLP datasets like SST-2 and AG News, and vision datasets like CIFAR-10. While the method is tested on Transformer-based architectures, the explicit validation against modern, large-scale safety-aligned LLMs is missing.\n\n3. The feasibility of the time-consuming evolutionary search does not scale well to the demands of modern LLM fine-tuning and deployment cycles. The paper fails to sufficiently address the scalability and computational cost implications of running an architecture search for models with larger layer counts or internal complexity."}, "questions": {"value": "1. Given the computational bottleneck of the architectural search, the authors should provide an analysis of scalability or a proposed roadmap for a more computationally efficient module selection mechanism to justify the method's viability in large-scale deployment.\n\n2. Does the module selection strategy offer substantial performance improvements over simple yet effective fusion baselines (like TIES/DARE), or is the advantage primarily marginal optimization?\n\n3. The empirical validation relies heavily on relatively older, smaller classification datasets and models (e.g., RoBERTa and BERT). Given the significant architectural disparity between these encoder-only models and the decoder-only generative LLMs that dominate the current threat landscape (e.g., GPT-like models), the authors should enhance the practical relevance of the findings by providing a transferability analysis on a recent generative LLM architecture. To what extent do these results accurately represent the security challenges posed by modern LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eiH69q261R", "forum": "ieCOL2YAqv", "replyto": "ieCOL2YAqv", "signatures": ["ICLR.cc/2026/Conference/Submission15870/Reviewer_c8Qa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15870/Reviewer_c8Qa"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15870/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986743483, "cdate": 1761986743483, "tmdate": 1762926087200, "mdate": 1762926087200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}