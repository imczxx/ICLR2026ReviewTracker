{"id": "0mBCl2goJr", "number": 16214, "cdate": 1758261801244, "mdate": 1759897254162, "content": {"title": "Why Settle for One? Text-to-ImageSet Generation and Evaluation", "abstract": "Despite remarkable progress in Text-to-Image models, many real-world applications require generating coherent image sets with diverse consistency requirements. Existing consistent methods often focus on a specific domain with specific aspects of consistency, which significantly constrains their generalizability to broader applications. In this paper, we propose a more challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate sets of images that meet various consistency requirements based on user instructions. To systematically study this problem, we first introduce T2IS-Bench with 596 diverse instructions across 26 subcategories, providing comprehensive coverage for T2IS generation. Building on this, we propose T2IS-Eval, an evaluation framework that transforms user instructions into multifaceted assessment criteria and employs effective evaluators to adaptively assess consistency fulfillment between criteria and generated sets.\nSubsequently, we propose AutoT2IS, a training-free framework that maximally leverages pretrained Diffusion Transformers' in-context capabilities to harmonize visual elements to satisfy both image-level prompt alignment and set-level visual consistency. Extensive experiments on T2IS-Bench reveal that diverse consistency challenges all existing methods, while our AutoT2IS significantly outperforms current generalized and even specialized approaches. Our method also demonstrates the ability to enable numerous underexplored real-world applications, confirming its substantial practical value. All our data and code will be publicly available.", "tldr": "We propose  Text-to-ImageSet (T2IS) task and develop unified framework for both T2IS evaluation and generation.", "keywords": ["Text-to-Image", "Benchmark", "Consistent Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/20cdafd5e600017a3cdf7fd8db6ccdcf3fd3e313.pdf", "supplementary_material": "/attachment/5935c6a090dff8f3bab1aabc3786af5a63795bde.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a new task called Text-to-ImageSet (T2IS) generation, which aims to create coherent sets of images that share visual consistency while following diverse text instructions. To support research on this task, the authors propose a benchmark dataset named T2IS-Bench, along with an evaluation framework called T2IS-Eval. They also present a training-free approach called AutoT2IS, which leverages pretrained diffusion transformers to generate consistent image sets. The authors demonstrate that AutoT2IS performs effectively across different domains than specifically made models, including multi-view generation, font generation, and character generation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper proposes a fresh and interesting idea: generating sets of images that maintain both variety and consistency. This direction extends traditional text-to-image generation toward more general and realistic applications.\n* The method is described clearly, and the proposed approach that combines prompt concatenation and masked latent operations is creative and well-motivated.\n* The experiments are comprehensive, covering different domains and including strong comparisons with both open-source and commercial models. The T2IS-Bench dataset also appears to be well-organized and diverse, which strengthens the results."}, "weaknesses": {"value": "* The paper could better explain the position and purpose of T2IS-Bench relative to existing datasets. Since the benchmark focuses on various types of visual consistency (such as identity, style, and logic), it would be helpful to clarify why combining existing datasets, such as those for multi-view or character generation, would not achieve the same generalization goal. For example, using datasets all at once like DTH [1], MipNeRF-360 [2], or SerialGen [3] to achieve multi-view or personalized generation. The authors could strengthen their argument by emphasizing what unique gaps T2IS-Bench fills for generalizing models.\n* The dataset quality seems uneven. As illustrated in Figure 1, there are noticeable inconsistencies in some samples. For instance, in the multi-view generation example, some visual elements (like the cylinder on the camera) appear in the top-left image but disappear in the bottom-right views, suggesting possible noise or imperfect alignment. Similarly, for character expression generation, the beard is not consistent and even disappears in the end. These inconsistencies might impact the reliability of the experiments and evaluation results.\n* The paper mainly discusses AutoT2IS, a training-free approach, but does not explore how the dataset itself could support training generalized models. Since the dataset is presented as a contribution, it would be valuable to include examples or experiments that demonstrate how it can be used for model training or fine-tuning in addition to evaluation.\n\n---\n\n**Overall**: The paper is well-written and presents a compelling and original idea. However, the motivation and positioning of the proposed dataset are not fully convincing, and the discussion on how it might facilitate generalized model training is limited. Addressing these points would make the contribution stronger. At this stage, I would rate it a 6, but I am open to increasing the score if these concerns are clarified.\n\n---\n\n1. Aanæs et al. Large-scale data for multiple-view stereopsis. IJCV, 2016\n2. Barron et al. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. CVPR, 2022.\n3. Xie et al. SerialGen: Personalized Image Generation by First Standardization Then Personalization. CVPR 2025."}, "questions": {"value": "The questions are mainly about the weaknesses:\n\n1. Could the authors elaborate on the position of the proposed dataset? How does it offer a more effective path toward generalization compared to combining existing datasets?\n2. Are there many noisy or inconsistent samples in the dataset, and how do they affect the experiments?\n2. How can T2IS-Bench be used to promote the training of generalized models rather than just serving as an evaluation benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VF0zfTREdm", "forum": "0mBCl2goJr", "replyto": "0mBCl2goJr", "signatures": ["ICLR.cc/2026/Conference/Submission16214/Reviewer_uWDA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16214/Reviewer_uWDA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761229628965, "cdate": 1761229628965, "tmdate": 1762926375553, "mdate": 1762926375553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Text-to-ImageSet generation, the task of creating coherent image sets from a single instruction. The authors propose three contributions: T2IS-Bench, a new benchmark with diverse instructions; T2IS-Eval, an MLLM-based evaluation framework for set-level consistency ; and AutoT2IS, a training-free generation method. AutoT2IS uses an LLM for \"Structured Recaption\" to create detailed prompts and a \"Set-Aware Generation\" diffusion strategy to balance individual image content with set-level consistency. Experiments show the proposed method generate high quality image sets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a task of Text-to-ImageSet generation, which generates multiple images instead of single images.\n- A benchmark and evaluation framework are proposed for this task.\n- A training-free method is proposed that leverages LLMs for structured recaption and a set-aware generation strategy."}, "weaknesses": {"value": "- The primary concern is the framing of the T2IS problem itself. The paper aggregates several pre-existing, distinct research tasks (e.g., 'Character Generation' , 'Story Generation' , 'Process Generation' ) under a new umbrella, as shown in Table 2. **It is not clearly articulated what new research challenge is unlocked by this aggregation.** The contribution appears to be more of a unified testbed rather than a novel research problem, which raises questions about the work's fundamental research significance.\n\n- The reliability of the T2IS-Eval framework is questionable. The evaluation hinges entirely on an MLLM-as-a-judge (Qwen2.5-VL-7B). The paper's own reliability analysis in Appendix D (Table 5) reports only a 0.61 Pearson Similarity between the model's fine-grained scores and human fine-grained ratings. This moderate correlation suggests the automated metric may be an unreliable proxy for human perception of consistency and may not be robust enough to support the paper's strong quantitative claims.\n\n- The methodological novelty of the AutoT2IS framework appears limited. The \"Structured Recaption\" component, which uses an LLM to parse a complex prompt into multiple sub-captions, is a common pattern in many recent agentic and compositional generation frameworks. This part of the method does not represent a significant methodological advance for the T2IS problem.\n\n### **Conclusion**\nIn conclusion, although the paper proposed a task and provides a benchmark which seems new, **it is not of enough research significance**. The 'T2IS' problem is largely an aggregation of existing tasks, which limits its research novelty. This is compounded by an evaluation metric of questionable reliability and a generation method with limited methodological innovation."}, "questions": {"value": "Please see weakness points above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "my5rlWuIsC", "forum": "0mBCl2goJr", "replyto": "0mBCl2goJr", "signatures": ["ICLR.cc/2026/Conference/Submission16214/Reviewer_H1wC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16214/Reviewer_H1wC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930396794, "cdate": 1761930396794, "tmdate": 1762926374818, "mdate": 1762926374818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new research paradigm called Text-to-ImageSet (T2IS) generation, which aims to produce a set of semantically coherent and visually consistent images from a single textual description—going beyond the traditional text-to-image (T2I) paradigm that generates only one image. To systematically study this problem, the authors propose three major contributions: (1) T2IS-Bench, the first large-scale benchmark for this task, covering 596 high-quality prompts across 26 subcategories that span various dimensions of consistency (identity, style, logic, etc.); (2) T2IS-Eval, an automatic evaluation framework leveraging large language and vision-language models to assess multi-dimensional consistency through interpretable, question–answer-based scoring; and (3) AutoT2IS, a training-free generation framework that exploits the in-context capabilities of diffusion transformers via structured recaption and set-aware generation. Experiments on T2IS-Bench demonstrate that AutoT2IS outperforms state-of-the-art models—including Show-o, Janus-Pro, and Gemini+Flux—across identity, style, and logic consistency dimensions, while maintaining generality and efficiency. The work establishes a strong foundation for future exploration in consistent multi-image generation and introduces valuable tools for benchmarking and evaluation in this emerging area."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-written and very easy to follow.\n2. This paper proposes a novel task: text to image set. This task has application values and is very important for future image generation research. Besides, the newly design evaluation metrics are also reasonable.\n3. This paper also proposes a training-free framework for this task, which is novel and promising.\n4. Comprehensive evaluation proves this benchmark is important, revealing some limitations of current open-/closed-source models, and providing interesting research direction for future work."}, "weaknesses": {"value": "From my perspective, as a benchmark paper, this is enough. But I still have several questions:\n1. How do you make sure the evaluation consistency? The evaluation metrics are VLLM-based. It would be better to prove your proposed evaluation metrics is reasonable. For example, you can have a subset human evaluation and analyze the consistency of your metrics and human evaluation score. \n2. It seems that the proposed framework performs worse than baselines in some cases in Table 2 (e.g., Style Design Generation, Story Generation). Can authors explain the performance gap or give some error analysis on this part?\n3. The image generation model is the most important part for the framework. Could you conduct more ablation study on image generation model? In your framework, keep all other parts as the same, but change the image generation model to Flux, Image-Qwen, SD3, also closed-source model such as GPT and Nano Banana, etc. You do not need to conduct on the whole dataset if the time is not enough, instead you can subset 25% samples to conduct this ablation study. This is to analyze in the same framework, how different image generation model influence the final results."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ELEFwnny2Q", "forum": "0mBCl2goJr", "replyto": "0mBCl2goJr", "signatures": ["ICLR.cc/2026/Conference/Submission16214/Reviewer_ZBFL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16214/Reviewer_ZBFL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969883859, "cdate": 1761969883859, "tmdate": 1762926374114, "mdate": 1762926374114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Text-to-ImageSet (T2IS) generation, a new task that requires models to generate coherent image sets that satisfy diverse consistency requirements across identity, style, and logic dimensions. The authors introduce T2IS-Bench, a comprehensive benchmark with 596 instructions spanning 26 subcategories, along with T2IS-Eval, an adaptive evaluation framework that transforms user instructions into multifaceted assessment criteria. They propose AutoT2IS, a training-free approach leveraging pretrained Diffusion Transformers through structured recaptioning and set-aware generation with a divide-and-conquer strategy, demonstrating superior performance over both generalized and specialized baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- AutoT2IS achieves significant improvements over existing methods through a simple yet effective divide-and-conquer strategy that maximally leverages DiT's in-context capabilities.\n- Comprehensive experiments demonstrating competitive or superior performance compared to both generalized approaches and even specialized methods.\n- The presentation is clear and easy to follow."}, "weaknesses": {"value": "- It seems the size of the image set (n) ranges from 2 to 5. The paper does not discuss the effect of this size number. For example, how will the size affect the quality and consistency of the images? What will happen when the size is extended to even larger numbers, such as 8 or 10?\n- In addition to text-to-image generation works, the authors should also carefully discuss another line of research, i.e., interleaved text-and-image generation, where the MLLM is also tasked with generating a sequence of images and text. Several representative works include InterleavedEval [1] and OpenING [2].\n\n[1] Holistic Evaluation for Interleaved Text-and-Image Generation. EMNLP 2024.\n\n[2] OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation. CVPR 2025."}, "questions": {"value": "See the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cuUuxB7NN7", "forum": "0mBCl2goJr", "replyto": "0mBCl2goJr", "signatures": ["ICLR.cc/2026/Conference/Submission16214/Reviewer_g4bk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16214/Reviewer_g4bk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144694682, "cdate": 1762144694682, "tmdate": 1762926373084, "mdate": 1762926373084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}