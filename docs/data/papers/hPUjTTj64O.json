{"id": "hPUjTTj64O", "number": 10618, "cdate": 1758177682177, "mdate": 1763720313752, "content": {"title": "Path-Consistency with Prefix Enhancement for Efficient Inference in LLM", "abstract": "To enhance the reasoning capabilities of large language models (LLMs), self-consistency has become a popular approach, combining multiple samplings with majority voting. However, current methods are computationally expensive and time-consuming due to the need for numerous samplings. To address this, this paper introduces path-consistency, which leverages the confidence of earlier-generated answers to identify the most promising prefix and guide the generation of subsequent branches. By dynamically guiding the generation of subsequent branches based on this prefix, path-consistency mitigates both the errors and redundancies from random or less useful sampling in self-consistency. This approach reduces errors and redundancies from random sampling, significantly accelerating inference by minimizing token consumption. Our extensive empirical results demonstrate that path-consistency improves inference latency by up to 40.5\\%, while maintaining task accuracy across various tasks, including mathematical reasoning, commonsense reasoning, and symbolic reasoning.", "tldr": "We propose path-consistency which achieves optimization of self-consistency, extracting information from early branches of self-consistency in the form of \"prefix\", guiding the generation of subsequent branches more efficiently.", "keywords": ["Large Language Model", "Efficient Inference", "Reasoning", "Chain-of-Thought Prompting", "Self-Consistency"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/efaf3f0a6f66d1ba87ad0c945c7a6506549cbea8.pdf", "supplementary_material": "/attachment/f8eca6d0958b3e0e5c29c0eef1bc63f7ed968db1.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes path-consistency, an improved variant of self-consistency that aggregates reliable prefixes across a small batch of completions to reduce the token cost during sampling. The authors show that this method can improve performance and reduce inference latency on several datasets using Llama3-8B."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**[S1]** The idea is reasonable. By extracting reliable prefixes, the proposed method reduces token consumption during self-consistency sampling, thus lowering inference time.\n\n**[S2]** According to the provided experiments, the method improves performance while decreasing latency."}, "weaknesses": {"value": "**[W1]** The paper lacks comparison with recent works; most of the cited references are outdated and do not include any from 2025.\n\n**[W2]** The main experiments are conducted only on Llama3-8B, which raises concerns about robustness. The authors should extend their evaluation to other model families and sizes.\n\n**[W3]** The derivation in Section 4.2 relies on too many assumptions, making the theoretical justification unconvincing.\n\n**[W4]** The reported self-consistency accuracy on GSM8K (64.1% in Table 2) is significantly lower than the official Llama3 result (~80%). This discrepancy questions the soundness of the experimental setup.\n\n**[W5]** The paper does not evaluate on more challenging reasoning datasets such as MATH500 or AIME, which are commonly used and better reflect the method’s robustness on difficult multi-step reasoning tasks.\n\n**[W6]** The claim around lines 324–325 lacks empirical support. The authors should include an ablation study on the sampling path parameter.\n\n**[W7]** As shown in Tables 5 and 6, the proposed method shows limited advantage compared to related methods.\n\n**[W8, minor]** The captions of Figures 1 and 2 exceed the bottom page margin."}, "questions": {"value": "**[Q1]** Could the authors clarify how inference latency is measured? This is critical for assessing the efficiency of the proposed method.\n\n**[Q2]** The authors do not report latency metrics for evaluations using DeepSeek-V3 with Llama-3.2-1B-Instruct. Could they provide corresponding results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2AP7fMmt5S", "forum": "hPUjTTj64O", "replyto": "hPUjTTj64O", "signatures": ["ICLR.cc/2026/Conference/Submission10618/Reviewer_XE47"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10618/Reviewer_XE47"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754851377, "cdate": 1761754851377, "tmdate": 1762921880947, "mdate": 1762921880947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Supplementary Theoretical Analysis: Soundness of Beta-Gated Prefix Selection"}, "comment": {"value": "## **Theorem 1** (Single-Level Soundness of Beta-Gated Prefix Selection) Consider a fixed level of prefix extraction with the following setup:\n$\\mathcal A$ denotes a finite answer set\nThe algorithm performs promotion checks at a sequence of increasing sample counts:\n$$0 < n^{(1)} < n^{(2)} < \\dots < n^{(K)} = n_{\\max}$$\nAt each check $k$, we draw the first $n^{(k)}$ i.i.d. samples:\n$$X_1, \\dots, X_{n^{(k)}}\\in\\mathcal A$$\nFor each $a\\in\\mathcal A$, let \n$$\\pi(a) := \\Pr(X_i = a)$$ \ndenote the model's sampling probability under the prefix.\nThe correct answer is $a^\\star$ with:\n$$q := \\pi(a^\\star), R := 1-q = \\sum_{a \\neq a^\\star} \\pi(a)$$\nFor each check $k$, define the observed counts:\n$$N_a^{(k)} := \\sum_{i=1}^{n^{(k)}} \\mathbf{1}\\\\{X_i = a\\\\}, \\hat a^{(k)} := \\arg\\max_{a\\in \\mathcal A} N_a^{(k)}, s^{(k)} := N_{\\hat a^{(k)}}^{(k)}$$\nAssume a Beta-gated promotion threshold $\\tau \\ge 0.5$, where promotion occurs at check $k$ if:\n$$T(s^{(k)},n^{(k)}) := \\Pr(\\theta > 1/2 \\mid \\text{Beta}(s^{(k)}+1,\\,n^{(k)}-s^{(k)}+1)) > \\tau$$\n\nThen, if $q \\ge 1/2 + \\Delta$ for some $\\Delta>0$, the probability that the correct prefix fails to be promoted at check $k$ is bounded by:\n$$\\Pr(A_k) \\le \\exp\\big(-2 n^{(k)} \\Delta^2\\big)$$\nwhere $A_k := \\{\\text{not promoted at check } k\\}$.\n\n*proof*. Promotion at check $k$ requires that $\\hat a^{(k)} = a^\\star$ and $T(s^{(k)},n^{(k)})>\\tau$. Thus failure at check $k$ implies:\n$$A_k \\subseteq \\{ N_{a^\\star}^{(k)} < k_{\\min}(n^{(k)},\\tau) \\}$$\nwhere:\n$$k_{\\min}(n,\\tau) := \\min\\{r \\in \\{0,\\dots,n\\}: T(r,n)>\\tau\\}$$\n\nSince $\\tau\\ge 0.5$, symmetry of the Beta posterior implies:\n\n$$k_{\\min}(n^{(k)},\\tau) > n^{(k)}/2$$\n\nHence:\n\n$$A_k \\subseteq \\{N_{a^\\star}^{(k)} \\le n^{(k)}/2\\}$$\n\nBut $N_{a^\\star}^{(k)}$ is a sum of $n^{(k)}$ independent Bernoulli($q$) variables with mean at least $n^{(k)}(1/2+\\Delta)$. By Hoeffding's inequality:\n\n$$\\begin{aligned}\n\\Pr\\Big(N_{a^\\star}^{(k)} \\le n^{(k)}/2\\Big) &= \\Pr\\Big(N_{a^\\star}^{(k)} \\le \\mathbb{E}[N_{a^\\star}^{(k)}] - n^{(k)}\\Delta \\Big) \\\\\n&\\le \\exp\\big(-2 n^{(k)} \\Delta^2\\big)\n\\end{aligned}$$\n\nThis completes the proof.\n\n## **Corollary 1** (Multi-Level Promotion Soundness)\nConsider $m$ consecutive prefix-extraction levels indexed by $\\ell_1,\\dots,\\ell_m$. At each level $\\ell_j$, promotion checks occur at sample sizes $n_{\\ell_j}^{(1)} < \\cdots < n_{\\ell_j}^{(K_j)}$, with correct-answer probabilities $q_{\\ell_j} \\ge 1/2 + \\Delta_{\\ell_j}$ and Beta thresholds $\\tau_{\\ell_j} \\ge 0.5$. Let $A_{\\ell_j}^{(k)}$ denote the failure-to-promote event at the $k$-th check of level $\\ell_j$. Then, under conditional independence across levels,\n\\\\[\\Pr(\\text{failure along full promotion path})\\le\\prod_{j=1}^m\\Bigg(\\sum_{k=1}^{K_j} \\exp\\big(-2 n_{\\ell_j}^{(k)} \\Delta_{\\ell_j}^2\\big)\\Bigg).\\\\]\n\n*proof.* For each level $\\ell_j$, we assume the true success probability remains $q_{\\ell_j} \\ge 1/2 + \\Delta_{\\ell_j}$. This assumption is valid due to the one-time selection guarantee proved in Section~4.2, which ensures that the prefix extraction does not hurt accuracy. Thus, prefix quality is not degraded by previous selection decisions; it preserves a positive advantage $\\Delta_{\\ell_j}$ at every level.\nAt each promotion check $k$ of level $\\ell_j$, requiring posterior probability\n\n\\\\[\n\\Pr(\\theta_{\\ell_j} > 1/2) > \\tau_{\\ell_j} \\ge 0.5,\n\\\\]\n\nHoeffding’s bound implies that the probability that a truly good prefix fails to promote at this check satisfies\n\n\\\\[\n\\Pr(A_{\\ell_j}^{(k)}) \\le \\exp(-2n_{\\ell_j}^{(k)} \\Delta_{\\ell_j}^2).\n\\\\]\n\nSince promotion only needs to succeed once within a level, the probability of failure across all checks of level $\\ell_j$ is bounded by\n\n\\\\[\n\\Pr\\Big(\\bigcap_{k} A_{\\ell_j}^{(k)}\\Big)\n\\le\n\\sum_{k=1}^{K_j} \\exp(-2n_{\\ell_j}^{(k)} \\Delta_{\\ell_j}^2).\n\\\\]\n\nFinally, assuming conditional independence across levels,\n\n\\\\[\n\\Pr(\\text{failure along full promotion path})=\n\\Pr\\Big(\\bigcap_{j=1}^m \\bigcap_{k=1}^{K_j} A_{\\ell_j}^{(k)}\\Big)\n\\le\n\\prod_{j=1}^m\n\\Bigg(\n\\sum_{k=1}^{K_j} \\exp(-2 n_{\\ell_j}^{(k)} \\Delta_{\\ell_j}^2)\n\\Bigg).\n\\\\]\n\nThus the overall probability of failing to amplify a prefix with true advantage $\\Delta_{\\ell_j}>0$ decays exponentially with the total effective sample budget across levels.\n\n## **Corollary 2** (Worst-Case Wrong Answer Distribution)\nFor fixed total wrong-mass $R = 1-q$, the probability that a wrong answer is promoted is maximized when all wrong probability is concentrated on a single alternative. In that case, the probability of wrong promotion is\n\\\\[\n\\Pr(N_{a_\\text{bad}} \\ge k_{\\min}(n,\\tau)), \\qquad a_\\text{bad} \\in \\mathcal A \\setminus \\{a^\\star\\}.\n\\\\]\n*proof.* Let $T := \\sum_{a \\neq a^\\star} N_a$ denote the total count of wrong answers. Then\n\\\\[\n\\{\\exists a\\neq a^\\star: N_a \\ge k_{\\min}\\} \\subseteq \\{T \\ge k_{\\min}\\}.\n\\\\]\nWhen the entire wrong-mass $R$ is assigned to a single $a_\\text{bad}$, $T = N_{a_\\text{bad}}$, and the inclusion becomes equality. Hence the probability of wrong promotion is maximized in this configuration."}}, "id": "ZT6ZndrZHH", "forum": "hPUjTTj64O", "replyto": "hPUjTTj64O", "signatures": ["ICLR.cc/2026/Conference/Submission10618/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10618/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10618/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763716589807, "cdate": 1763716589807, "tmdate": 1763716589807, "mdate": 1763716589807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents path-consistency, with the aim to reduce the token consumption or improve the efficiency of standard self-consistency. The key idea is to extract prefixes from early branches and use them to guide the subsequent generations. Experiments are conducted on three domains of datasets. The results show that the proposed method could significantly improve the efficiency while preserve the accuracy in most cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is generally well-written.\n- The proposed path-consistency is reasonable.\n- Extensive experiments are conducted to show the effectiveness of the proposed method."}, "weaknesses": {"value": "- In the experiments, the authors show that there exists a proper confidence threshold at which path-consistency simultaneously enhances both task performance and efficiency. However, how to determine this threshold is unclear.\n\n- The authors claim that the proposed method integrates seamlessly with existing optimization methods, achieving even better acceleration performance. However, only self-consitency is evaluated in the experiments.\n\n- Related to the previous point, only self-consistency is evaluated as baselines in the experiments. How about the direct generation methods as well as inference time techniques. An accuracy-efficiency comparison among more related techniques is appreciated."}, "questions": {"value": "- The proposed method largely relies on the correctness of the prefixs of earlier generated paths. Can the authors analyze the results to check how the correctness of the prefixs affect the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u3JhFts7a7", "forum": "hPUjTTj64O", "replyto": "hPUjTTj64O", "signatures": ["ICLR.cc/2026/Conference/Submission10618/Reviewer_SuGi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10618/Reviewer_SuGi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920781903, "cdate": 1761920781903, "tmdate": 1762921880160, "mdate": 1762921880160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper proposes path-consistency with prefix enhancement for LLMs to solve the high computational cost of self-consistency. It extracts \"prefixes\" from early-generated paths using answer confidence to guide subsequent branch generation, reducing errors and token waste without extra computation, fine-tuning, or model changes. Evaluated on 10 datasets, it boosts inference speed and also outperforms baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper demonstrates originality through a creative reformulation of self-consistency.  It does not merely prune paths but repurposes high-confidence segments to steer inference.\n2. The work exhibits high methodological and empirical quality. Methodologically, PC is rigorously designed. Empirically, the evaluation is comprehensive.\n3. The paper is well-structured and accessible. The \"extract-and-sample\" process is clearly illustrated through pseudocode and a running example, making the workflow easy to follow.\n4. This work addresses the high computational cost of self-consistency in LLM deployment. By making inference drastically more efficient without accuracy loss, path-consistency has immediate practical significance for real-world applications requiring reliable reasoning."}, "weaknesses": {"value": "1.The paper relies heavily on the beta confidence criterion but provides minimal analysis of how sensitive the results are to this specific choice.\n2.The theoretical analysis assumes only one dominant incorrect answer, which oversimplifies real-world scenarios where multiple plausible but wrong answers compete.\n3.While arithmetic/reasoning tasks show impressive gains, there's no evaluation on generation-intensive tasks (e.g., long-form QA, summarization) where prefix reuse might have different effects."}, "questions": {"value": "1. Paper mentions extracting \"shorter prefixes from optimal paths\" when confidence exceeds a threshold, but it does not define what constitutes a \"step\" or how steps are segmented from raw model outputs. Is it rule-based or data-driven? Does step segmentation vary across tasks?\n2. What is the computational overhead of PC’s prefix extraction and confidence calculation? \n3. PC performs iterative prefix selection, not one-time selection. Does iterative selection increase the risk of amplifying early minority errors (e.g., a wrong prefix selected at Level-1 leading to more wrong paths at Level-2)?\n4.\"randomly sample from the extracted prefixes as part of the prompt.\" What is the sampling strategy？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KPpatRhfPW", "forum": "hPUjTTj64O", "replyto": "hPUjTTj64O", "signatures": ["ICLR.cc/2026/Conference/Submission10618/Reviewer_uF6r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10618/Reviewer_uF6r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973752762, "cdate": 1761973752762, "tmdate": 1762921879494, "mdate": 1762921879494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Path-Consistency (PC), an inference-time method that accelerates Self-Consistency (SC) decoding for LLMs without additional training or model changes.\nPC repeatedly (i) generates a small window of reasoning paths, (ii) estimates answer confidence, (iii) extracts the common prefix of high-confidence paths, and (iv) re-uses that prefix to seed the next window.\nIterating this “extract-and-sample” loop shortens subsequent generations, cutting token cost and wall-clock latency while preserving (or slightly improving) accuracy on ten reasoning benchmarks.\nExtensive experiments with Llama-3-8B show up to 48 % speed-up and −58 % token consumption vs. vanilla SC, with comparable or better accuracy on math, commonsense, and symbolic tasks. The method is model-agnostic and orthogonal to other SC optimizations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Significant efficiency gains: Demonstrates 20–48 % latency reduction and 20–60 % fewer tokens across diverse datasets, verified on a consumer GPU.\n2. No training or architecture change: Pure inference algorithm—easy to plug into any decoder-only LLM and complementary to distillation, early-exit, or speculative schemes.\n3. Careful ablation & analysis: Authors vary confidence thresholds, prefix levels, model sizes, and datasets; provide theoretical justification that PC does not amplify “truth-in-the-hands-of-a-few” problem when base SC already works."}, "weaknesses": {"value": "1. Limited novelty: Core idea—early-commitment to promising partial solutions—closely resembles existing beam/pruning and adaptive-consistency techniques; incremental contribution.\n2. Heavily engineered hyper-parameters: Window size, #branches, confidence metric, prefix level schedule, and threshold all require per-task tuning; no automatic or adaptive schedule offered.\n3. Evaluation restricted to 20-path SC: All speed/accuracy numbers compare against a 20-sample baseline; unclear how PC behaves with larger budgets or stronger teacher models where SC accuracy saturates."}, "questions": {"value": "1. How does PC compare when SC uses 40, 100, or adaptive samples—does the relative speed-up shrink or the error rate rise?\n2. Fig. 2 shows 25–50 % tokens wasted on wrong branches; does aggressively lengthening the prefix (level-4/5) ever mis-guide later samples and hurt accuracy?\n3. The proof assumes Pvote ≥ 0.5; what bound can you give for harder datasets where SC itself is below 50 %, and how should practitioners set Cthreshold in that regime?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "03ETTo7ue7", "forum": "hPUjTTj64O", "replyto": "hPUjTTj64O", "signatures": ["ICLR.cc/2026/Conference/Submission10618/Reviewer_YQkP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10618/Reviewer_YQkP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762089417442, "cdate": 1762089417442, "tmdate": 1762921879167, "mdate": 1762921879167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}