{"id": "5IqvMNQaUg", "number": 11161, "cdate": 1758191591819, "mdate": 1759897603404, "content": {"title": "Sparse Topology Pairwise Scoring for Large-Scale Multi-Agent Reinforcement Learning", "abstract": "In multi-agent reinforcement learning (MARL), the problem of partial observability and stochasticity can be alleviated by enabling agents to access additional information about others through communication. However, in large-scale settings, communication among agents leads to a quadratic increase in the number of pairwise links, resulting in excessive bandwidth consumption and memory bottlenecks. Previous studies primarily aimed to solve this problem through learning globally optimal communication graphs, but such designs inevitably incur rapidly escalating complexity as the agent population increases. In this work, we propose a scalable communication scheme for large-scale MARL, termed $\\textit{Sparse tOpology Pairwise Scoring}$ (SOPS). We hypothesize that leveraging pairwise relations among agents over an efficient backbone topology can enhance cooperative policies, and we adopt an exponential graph as a scalable backbone topology with a small diameter. Based on this backbone, we learn a probabilistic subgraph distribution parameterized by a pairwise scoring network that adaptively incorporates agent states and edge-type embeddings.  To enable gradient-based optimization through discrete subgraph sampling, we employ Gumbel-Sigmoid reparameterization, whose differentiable nature allows the entire framework to be trained in an end-to-end manner. Overall, SOPS maintains high communication efficiency while adapting dynamically to task requirements and temporal variations. Evaluation results show that SOPS significantly outperforms existing state-of-the-art methods across cooperative benchmarks of diverse scales, consistently achieving higher rewards and faster convergence. SOPS also exhibits robust zero-shot transfer capabilities, enabling a model trained on a smaller scale to effectively apply to larger-scale scenarios.", "tldr": "We propose SOPS, a scalable multi-agent communication framework for large-scale scenarios that learns dynamic topologies over efficient backbones to improve adaptivity and efficiency.", "keywords": ["communication learning; multi-agent reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/471258fa8687a7c13d1c0c08550b8f207d142474.pdf", "supplementary_material": "/attachment/b14d48edb457e49e68ab3484bf2e3aa933548fe0.zip"}, "replies": [{"content": {"summary": {"value": "To address the issues of excessive communication overhead, poor scalability, and the difficulty of balancing complexity and adaptability in existing methods for large-scale cooperative multi-agent reinforcement learning (MARL) under the Centralized Training with Decentralized Execution (CTDE) paradigm, this study proposes a scalable communication mechanism called SOPS (Sparse Topology Pairwise Scoring). Its core design idea decouples \"global reachability\" from \"task-adaptive selection\": an exponential graph is used as the communication backbone to ensure efficient multi-hop information dissemination, leveraging its properties of small diameter and near-linear cost. On this basis, a lightweight pairwise scoring network dynamically generates task-adaptive sparse subgraphs by integrating agent states and edge-type embeddings. To solve the differentiability problem of discrete subgraph sampling, Gumbel-Sigmoid reparameterization with linear temperature annealing is introduced to enable end-to-end training. Meanwhile, auxiliary tasks (global state recovery or contrastive learning) are designed to ensure the task relevance of communication content. Additionally, this study verifies the compatibility of SOPS with mainstream value-based MARL algorithms and its zero-shot transfer capability across different agent scales, providing a new solution for the design of communication mechanisms in large-scale MARL."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "(1) The study accurately captures the core pain points in large-scale MARL, such as communication bandwidth explosion and the difficulty of balancing topology flexibility and scalability. The proposed decoupling approach—\"using a backbone network to ensure global reachability and pairwise scoring to achieve dynamic selection\"—effectively breaks through the limitations of existing methods. \n(2) Gumbel-Sigmoid reparameterization is introduced to solve the non-differentiability issue of discrete sampling, avoiding the high variance defect of traditional score-function methods. The linear temperature annealing strategy further optimizes the balance between exploration and exploitation, and the design of auxiliary tasks prevents meaningless communication. \n(3) SOPS is designed as a \"plug-and-play\" module, compatible with mainstream value-based MARL algorithms such as IQL and QMIX. It executes fully in a decentralized manner without relying on a central proxy, making it easy to integrate into existing CTDE frameworks. \n(4) Experiments cover multiple task types, including adversarial pursuit, team battle, and infrastructure management. They focus on verifying the method's performance in dimensions such as overall performance, convergence, and cross-scale transfer."}, "weaknesses": {"value": "(1) The robustness of the exponential graph backbone in scenarios such as agent failure and dynamic topology disruption is not discussed, making it impossible to determine the method's applicability in unstable environments. \n(2) All existing experiments are based on homogeneous agents (with consistent action spaces and roles). However, large-scale MARL scenarios in the real world often involve heterogeneous agents (with different capabilities and responsibilities). \n(3) In the related work section, there is no in-depth comparison of SOPS with methods such as GTDE (Grouped Training with Decentralized Execution) and ExpoComm (fixed exponential topology) in terms of technical routes (e.g., grouping strategies vs. sparse subgraph selection) and applicable scenarios. The relative advantages and application boundaries of SOPS are not clearly highlighted, and the connection to and breakthroughs over existing achievements in the field are not sufficiently elaborated.\n(4) The manuscript contains floating figures that are not explicitly referenced or explained in the main text. These figures lack contextualization—for instance, no descriptions of their purpose, the insights they convey, or how they support the core claims of the study.\n(5) Some references in the manuscript are relatively old; it is recommended to replace them with more recent studies related to large-scale MARL."}, "questions": {"value": "See the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No Details Of Ethics Concerns."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eUL9u8nQZb", "forum": "5IqvMNQaUg", "replyto": "5IqvMNQaUg", "signatures": ["ICLR.cc/2026/Conference/Submission11161/Reviewer_tKLs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11161/Reviewer_tKLs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730123058, "cdate": 1761730123058, "tmdate": 1762922326221, "mdate": 1762922326221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SOPS, a scalable communication mechanism for large-scale cooperative MARL. It addresses the challenge of communication overhead in large agent populations by combining a fixed exponential backbone topology with a learned, sparse, and task-adaptive subgraph. The subgraph is generated using a pairwise scoring network and Gumbel-Sigmoid reparameterization, enabling end-to-end differentiable training. SOPS is designed to be plug-and-play with common CTDE-based learners like IQL and QMIX. Experiments on IMP benchmarks show that SOPS achieves higher returns, faster convergence, and robust zero-shot transfer across varying agent population sizes. Ablation studies further validate the effectiveness of the Gumbel temperature annealing and reparameterization strategy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "One of the key strengths of the paper is its scalability. By leveraging the exponential graph structure, the communication cost grows near-linearly with the number of agents, while maintaining a small diameter for rapid information dissemination. The learned sparse subgraph adds adaptability, allowing the communication structure to evolve with task dynamics. The use of Gumbel reparameterization ensures stable and efficient training of discrete communication links, which is often a challenge in MARL. \n\nThe empirical results across benchmarks such as MAgent and IMP demonstrate superior performance in terms of convergence speed, final returns, and zero-shot transfer to larger agent populations. Ablation studies further validate the importance of temperature annealing and reparameterization in achieving robust learning."}, "weaknesses": {"value": "Although the experimental results consistently outperform other models, the statistical significance of these results appears limited. This raises the question of whether it is safe to draw strong conclusions based solely on such outcomes. Without rigorous statistical validation, the observed performance gains may not be robust or generalizable.\n\nOne of the stated motivations or contributions of the paper is to ensure that the underlying communication backbone follows a power-law structure to enable efficient information dissemination. However, it remains unclear how the subgraph sampling strategy guarantees that the final communication graph retains the properties of a power-law network. Further clarification or empirical evidence would strengthen this claim."}, "questions": {"value": "Please see the above two comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "foPRNze83F", "forum": "5IqvMNQaUg", "replyto": "5IqvMNQaUg", "signatures": ["ICLR.cc/2026/Conference/Submission11161/Reviewer_tRJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11161/Reviewer_tRJ9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870427229, "cdate": 1761870427229, "tmdate": 1762922325718, "mdate": 1762922325718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SOPS, a scalable communication strategy for large-scale MARL. The method builds upon a fixed exponential graph backbone to ensure efficiency, introducing a lightweight pairwise scoring module to dynamically learn a sparse, task-adaptive communication subgraph. By leveraging Gumbel-Sigmoid reparameterization for end-to-end training, SOPS aims to provide a plug-and-play solution that demonstrates strong performance and robust zero-shot transfer capabilities in multiple experiments."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper addresses the highly relevant and practical problem of scalable communication in MARL. The focus on balancing performance with communication cost is well-motivated and crucial for real-world applications.\n2. The key innovation of this method seems to be the added pairwise-scoring module on top of the established exponential topology. This is an intuitive and sensible approach, as it introduces necessary flexibility for task-adaptive communication while retaining the guaranteed reachability of the backbone topology. The empirical results help validate the effectiveness of this design choice."}, "weaknesses": {"value": "1. The method is presented as a refinement of the existing exponential topology paradigm. While the addition of the pairwise scoring module is a reasonable next step, the overall contribution seems like as a marginal improvement rather than a fundamentally new approach to scalable communication.\n2. The empirical performance gains, while consistent, do not always appear to be statistically significant (standard deviation error bars often overlaps with the performance gain). \n\n\nMinor:\nPlease number all equations for easier references."}, "questions": {"value": "1. The computation of the pairwise score (line 252) appears to require embeddings from all candidate neighbors on the backbone. How are these embeddings shared before the communication links for the current timestep are established? Does this imply a pre-communication phase where agents broadcast their embeddings to their backbone neighbors, and if so, how is the overhead of this phase accounted for in the overall efficiency analysis?\n2. Is the $l_{ij}$ computed at the receiver or sender side?\n3. Is the hyperparameter $\\tau$ consistent across all experiments, or are they tuned per environment? If they require tuning, could the authors provide some intuition or a methodology for selecting appropriate values, as this seems crucial for the method's stability and performance?\n4. Could the authors provide some visualizations of the resulting dynamic subgraphs? Those could offer valuable intuition into the adaptive strategies that emerge and help readers better understand how SOPS contributes to the improved joint policies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5qKLoDIVOT", "forum": "5IqvMNQaUg", "replyto": "5IqvMNQaUg", "signatures": ["ICLR.cc/2026/Conference/Submission11161/Reviewer_eN33"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11161/Reviewer_eN33"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990555020, "cdate": 1761990555020, "tmdate": 1762922324514, "mdate": 1762922324514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Sparse tOpology Pairwise Scoring (SOPS), a communication scheme for large-scale cooperative MARL. SOPS fixes an exponential backboneand learns a task-adaptive sparse subgraph on top of it via a lightweight pairwise scoring network and Gumbel-Sigmoid sampling with linear temperature annealing. Messages are aggregated with cross-attention and the policy is trained under CTDE, with auxiliary objectives for message grounding. Experiments on MAgent and IMP benchmarks report higher returns/win-rates and faster convergence than baselines, plus zero-shot transfer from small to larger populations. Ablations favor linear annealing and reparameterized gradients over SFE/ARM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The exponential backbone keeps degree $O(logN)$ and diameter $O(log N)$), a sensible starting point for large populations. The paper illustrates broadcast coverage advantages over ER/Torus variants.\n\n2. The pairwise scorer and Gumbel-Sigmoid yield a trainable discrete subgraph with a practical linear annealing schedule. Ablations support this choice over SFE/ARM or fixed temperatures.\n\n3. Across six MAgent settings and multiple IMP scenarios, SOPS typically learns faster and reaches better asymptotes than VDN, GACG, and ExpoComm variants.\n\n4. Demonstrations of train to test scaling, e.g., 20 to 64/81/100/121 agents, 42 to 81/100/121, are valuable for large-population MARL."}, "weaknesses": {"value": "1. The paper asserts near-linear communication/memory, yet provides no wall-clock, GPU-memory, or activation-size profiling vs N, nor bandwidth per step or per agent. \n\n2. Strong attention-based or topology-learning methods beyond the chosen set are absent. In addition, GACG is excluded from zero-shot transfer on architectural grounds, weakening the generalization claim. \n\n3. Since SOPS is presented as “plug-and-play” with common CTDE learners, it is surprising that results are only with QMIX. Most up-to-date CTDE-sytle methods are not shown.\n\n4. The method samples hard edges via a straight-through estimator but provides no diagnostics on gradient bias/variance, calibration of selection probabilities, learned degree/coverage distributions, or stability across seeds. Zero-shot transfer is only shown for Battle, and there is no analysis of which edges persist or adapt when resizing."}, "questions": {"value": "1. Beyond EC-S/EC-O, please add ablations that hold the backbone but remove/alter the scorer (e.g., random/top-k by static features) and vary the edge budget K to separate gains from topology vs learned selection.\n\n2. Could you (i) evaluate zero-shot transfer on IMP, not just Battle, and (ii) provide learned-graph diagnostics e.g., degree, hop-coverage over time, edge persistence across resize, to explain why SOPS transfers better? \n\n3. Since SOPS is “plug-and-play,” please include more up-to-date CTDE-style baselines and discuss whether benefits persist under different value-decomposition learners.\n\n4. Can you report wall-clock step/s, GPU memory, and activation sizes with number of agents and per-agent neighbors, for SOPS and each baseline, plus ablate message dimensionality? A figure showing reward vs throughput/memory would substantiate scalability claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cM9SQCoh2C", "forum": "5IqvMNQaUg", "replyto": "5IqvMNQaUg", "signatures": ["ICLR.cc/2026/Conference/Submission11161/Reviewer_bpJA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11161/Reviewer_bpJA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049690343, "cdate": 1762049690343, "tmdate": 1762922323782, "mdate": 1762922323782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}