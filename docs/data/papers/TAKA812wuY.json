{"id": "TAKA812wuY", "number": 18027, "cdate": 1758283037032, "mdate": 1763715444998, "content": {"title": "Beyond Overconfidence: Rethinking Calibration in Large-Scale Vision Models", "abstract": "Reliable uncertainty calibration is crucial for safe deployment of deep neural networks in high-stakes settings. While these networks are known to exhibit systematic overconfidence, particularly under distribution shifts, the calibration of large-scale vision models, such as ConvNeXt, EVA, and BEiT, remains underexplored. We comprehensively examine their calibration behavior, uncovering findings that challenge well-established assumptions. We find that these models are underconfident on in-distribution data, resulting in increased calibration error, but exhibit improved calibration under distribution shifts. This phenomenon is primarily driven by modern training techniques, including massive pretraining and sophisticated regularization and augmentation methods, rather than architectural innovations alone. We also demonstrate that these large-scale models are highly responsive to post-hoc calibration techniques in the in-distribution setting, enabling practitioners to mitigate underconfidence bias effectively. However, these methods become progressively less reliable under severe distribution shifts and can occasionally produce counterproductive results. Our findings highlight the complex, non-monotonic effects of architectural and training innovations on calibration, challenging established narratives of continuous improvement.", "tldr": "", "keywords": ["model calibration", "post-hoc calibration", "calibration benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/846a59e9303c0bbc1961cab2393cb1d2715b77cd.pdf", "supplementary_material": "/attachment/2c2ad9f9b57b3f617a7f2b5ff30f3c189001da0f.zip"}, "replies": [{"content": {"summary": {"value": "The paper evaluates several state-of-the-art vision models on ImageNet-1k and its distribution-shifted variants (ImageNet-C, ImageNet-A, ImageNet-V2), assessing both calibration—measured via top-label Expected Calibration Error (ECE)—and accuracy. The six models considered are ResNet, ViT, Swin, BEiT, EVA, and ConvNeXt. The results show that **ConvNeXt, EVA, and BEiT tend to be under-confident** in-distribution but exhibit improved calibration under distribution shifts, whereas ResNet, ViT, and Swin display the opposite pattern, being better calibrated in-distribution but at the cost of lower accuracy.\n\nThe authors then investigate two factors that may influence this calibration behavior. First, for a fixed ViT architecture, they show that switching the pretraining objective from supervised to contrastive learning improves accuracy but leads to under-confidence. Second, for a fixed ViT or ResNet architecture, they find that applying advanced regularization and data augmentation techniques similarly improves accuracy while creating under-confidence.\n\nFinally, the authors demonstrate that this miscalibration can be effectively mitigated by standard post-hoc calibration methods, although these techniques remain less effective under severe distribution shifts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is clearly written and easy to follow.\n\n* I have seen results showing that base LLMs tend to be under-confident (e.g., [Cruz2024]), while their instruction-tuned versions become over-confident. However, I am not aware of prior work reporting that models such as ConvNeXt, EVA, and BEiT are under-confident in-distribution and better calibrated under distribution shifts. To my knowledge, this observation is therefore novel.\n\n[Cruz 2024] Cruz et al, Evaluating language models as risk scores"}, "weaknesses": {"value": "I think the significance and potential impact of the paper are limited by the relatively narrow scope of the experimental study, which evaluated the ECE of 6 models + 4 variants. \n\nIn line 462, the authors mention that the paper focuses on diagnostics (e.g., identifying under-confidence in certain models) rather than uncovering its causes. However, given ICLR’s standards, it would be reasonable to expect a broader experimental exploration in this direction, across architectures, model sizes, pretraining objectives, regularization strategies, fine-tuning, etc (or at least a subset of these.) I acknowledge that Section 4.3 provides preliminary insights, and they are very valuable. But again for ICLR standards, I would expect deeper insights in this direction."}, "questions": {"value": "- Did the authors train any of the evaluated models, or are they all available on Hugginface? If yes, could the authors provide the links to the models used?\n- In section 4.2, the authors state that large-scale models exhibit systematic in-distribution underconfidence, encompassing ConvNext, BEiT & EVA in the large scale models. However, why is the ViT not considered as large scale, since it is also pretrained on ImageNet-21k, and is (I would say) not smaller than the other models ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pEo0NuUIO1", "forum": "TAKA812wuY", "replyto": "TAKA812wuY", "signatures": ["ICLR.cc/2026/Conference/Submission18027/Reviewer_Sewz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18027/Reviewer_Sewz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664545774, "cdate": 1761664545774, "tmdate": 1762927817272, "mdate": 1762927817272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically evaluates the uncertainty calibration performance of three large-scale vision models—ConvNeXt, EVA, and BEiT—and reveals findings that contradict prior research conclusions. The study shows that these models generally exhibit systematic underconfidence on in-distribution data, instead of the commonly reported overconfidence of tranditional models, leading to higher calibration errors. What's more, their calibration error decreases under out-of-distribution conditions. Furthermore, the authors demonstrate that these models respond well to post-hoc calibration methods (e.g., Temperature Scaling) in in-distribution scenarios. Nonetheless, the effectiveness of such methods degrades significantly under severe distribution shifts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall structure of the paper is clear and well-organized.\n\n2. The related work section clearly articulates how this paper differs from previous studies, allowing readers to quickly grasp the central contributions.\n\n3. The experiments are diverse and conducted across multiple architectures and datasets. Figures 4(a) and 6 effectively correspond to and support the main conclusions."}, "weaknesses": {"value": "1. There are several typos and grammatical errors throughout the paper. The authors should carefully proofread the manuscript to meet publication standards. For example, there is a mistake \"Eq. ??\" in line 135.\n\n2. Some of the paper’s conclusions are not clearly stated. For instance, Section 4.4 only provides descriptive commentary on figures without drawing any conclusions. If the intended point is that “Figure 3 shows architecture-specific differences in effectiveness,” this claim is neither visually evident from the figure nor supported by textual explanation. Similarly, while Figure 4 is later discussed, the meaning of the solid and dashed markers and the significance of their connecting lines are not made clear from the figure itself, although in the latter context.\n\n3. Certain conclusions lack sufficient evidence or persuasiveness. For example, the findings in Section 4.3 are only validated on a single model, which limits their generality. In Section 5, the final key insight labeled as “best” actually refers only to being the best among the three evaluated models, rather than a universal best."}, "questions": {"value": "Why did the authors choose to evaluate these three models rather than using larger-scale models such as LLaVA or Qwen-VL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f1hF1cTXNL", "forum": "TAKA812wuY", "replyto": "TAKA812wuY", "signatures": ["ICLR.cc/2026/Conference/Submission18027/Reviewer_GXKG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18027/Reviewer_GXKG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879555748, "cdate": 1761879555748, "tmdate": 1762927816847, "mdate": 1762927816847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how miscalibration differs under various distribution shifts, showing that models tend to be *underconfident* for in-distribution data but become *overconfident* under out-of-distribution (OOD) shifts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Pros**\n\n* It is valuable to examine calibration performance across different data groups, such as in-distribution and out-of-distribution scenarios.\n* The writing is clear to follow."}, "weaknesses": {"value": "**Cons**\n* Missing important related works with similar findings [1] or relevant methods for reducing miscalibration across different groups [1][2].\n* Lacks theoretical analysis or insights explaining the observed calibration behaviors.\n* The evaluated models are still limited in scope compared to prior comprehensive studies [1][3].\n* The overall contribution is limited. Most findings have already been identified in prior works, such as miscalibration under group shifts [1] and the accuracy–calibration trade-off [3]. \n\n\n---\n\n**References**\n\n[1] Xiong, Miao, et al. *\"Proximity-informed calibration for deep neural networks.\"* NeurIPS 2023\n\n[2] Perez-Lebel, Alexandre, Marine Le Morvan, and Gaël Varoquaux. *\"Beyond calibration: estimating the grouping loss of modern neural networks.\"* ICLR 2023\n\n[3] Minderer, Matthias, et al. *\"Revisiting the calibration of modern neural networks.\"* NeurIPS 2021"}, "questions": {"value": "Please see **Weaknesses**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hh5JClr1Ml", "forum": "TAKA812wuY", "replyto": "TAKA812wuY", "signatures": ["ICLR.cc/2026/Conference/Submission18027/Reviewer_Hc1C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18027/Reviewer_Hc1C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966896423, "cdate": 1761966896423, "tmdate": 1762927816454, "mdate": 1762927816454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}