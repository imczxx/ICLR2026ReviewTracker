{"id": "EmIdvvqFPc", "number": 11065, "cdate": 1758188443853, "mdate": 1759897611227, "content": {"title": "S$^3$Net: Stage-Aware Sleep Staging Network", "abstract": "Automated sleep staging is a critical component in the diagnosis of sleep disorders and the analysis of sleep architecture. While deep learning approaches that leverage time-frequency representations have shown promise, their performance remains suboptimal, primarily due to two fundamental limitations: (1) the inability to effectively model the subtle distinctions of transitional sleep stages (N1 and N2), which exhibit ambiguous electrophysiological patterns, and (2) the inefficient fusion of complementary information from time-domain and frequency-domain representations. To this end, we propose S$^3$Net, a novel Stage-Aware Sleep Staging Network that introduces two dedicated components to address these challenges. First, a Stage-Aware Experts (SAE) module explicitly partitions the sleep stages into easy- and hard-to-separate groups, processing them through separate expert network branches. This allows for specialized feature refinement, particularly for the challenging transitional stages. Second, to foster a cohesive representation, we design a Time Alignment Module (t-ALN) that projects frequency-derived features onto the time axis, effectively bridging the domain gap and enabling synergistic integration of multi-view features. We evaluate S³Net on three public polysomnography datasets (ISRUC-S1, ISRUC-S3, and Sleep-EDF-153). Our model consistently sets a new state-of-the-art, achieving an overall accuracy of 85.6\\%, 86.6\\%, and 86.9\\%, respectively, and demonstrates a marked improvement in classifying the N1 and N2 stages. The results validate the efficacy of our stage-aware design and structural alignment strategy, offering a more robust framework for clinical and portable sleep staging. Source code is available at \\url{https://anonymous.4open.science/r/S3Net/}.", "tldr": "", "keywords": ["sleep staging", "dual-tower network", "mixture of expert"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3379e1afeef7a1fc75b16eeb1cf28624ce3176fe.pdf", "supplementary_material": "/attachment/1221ceefbb12916b01eb12633a793bbd861ee70f.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce S3Net, an approach for automated sleep staging, crucial in diagnosing sleep disorders and analyzing sleep patterns. S3Net addresses challenges in distinguishing transitional sleep stages and integrating time-domain and frequency-domain information effectively. It comprises a Stage-Aware Experts module that divides sleep stages into distinct groups for specialized processing and a Time Alignment Module for aligning frequency-derived features with the temporal axis. Evaluation on three datasets demonstrates S3Net's superior performance, achieving high accuracy and marked improvements in classifying challenging stages like N1 and N2. Ablation studies confirm the effectiveness of each module, with the full S3Net model showcasing the best results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The framework is clearly presented with a clean modular design. The workflow (signal encoding → t-ALN → SAE → classification) is easy to follow, and the algorithmic components are described in sufficient detail.\n\n- While both attention-based fusion and expert gating are known techniques, their integration within the context of sleep staging is coherent and well-implemented. The proposed modules complement each other, improving stage-level discriminability and robustness.\n\n- The model achieves stable gains across multiple datasets and metrics. The ablation and t-SNE analyses show internally consistent patterns, reflecting careful engineering and experimental execution, though some evaluation aspects still require refinement."}, "weaknesses": {"value": "- The ablation only studies the presence/absence of t-ALN and SAE. It would be helpful to explore variations such as the number of experts or alternative attention weighting schemes.\n\n- All experiments are within-domain on similar PSG datasets (EEG-dominant). Cross-dataset or missing-modality tests would better demonstrate generalization."}, "questions": {"value": "Please see Weaknesses session."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "njlnpASZHy", "forum": "EmIdvvqFPc", "replyto": "EmIdvvqFPc", "signatures": ["ICLR.cc/2026/Conference/Submission11065/Reviewer_MXY5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11065/Reviewer_MXY5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540702562, "cdate": 1761540702562, "tmdate": 1762922242565, "mdate": 1762922242565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes S³Net, a deep learning framework for automated sleep stage classification that addresses two key challenges: (1) difficulty in distinguishing transitional sleep stages (N1 and N2), and (2) ineffective fusion of time-frequency representations. The approach introduces two main components: a Stage-Aware Experts (SAE) module that partitions sleep stages into easy- and hard-to-separate groups processed through specialized branches, and a Time Alignment Module (t-ALN) that projects frequency-derived features onto the temporal axis for cross-domain integration. The method achieves state-of-the-art results on three public polysomnography datasets (ISRUC-S1, ISRUC-S3, Sleep-EDF-153)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated problem: The paper clearly identifies and quantifies the challenge of transitional stage classification through transition matrices and performance analysis.\n\n2. Comprehensive evaluation: Three public datasets with consistent improvements demonstrate empirical robustness. The per-stage F1 scores show targeted improvements on challenging classes.\n\n3. Thorough experimental protocol: 10-fold cross-validation with multiple random seeds shows methodological rigor. Extensive baselines (15 methods) provide good context.\n\n4. Good visualizations: t-SNE plots, confusion matrices, and hypnograms effectively communicate model behavior and provide interpretability.\n\n5. Honest reporting: The paper acknowledges N1 confusion patterns and doesn't overclaim results."}, "weaknesses": {"value": "1. Insufficient theoretical foundation:\n- Why is the specific stage partition (W/N1/N2 vs N3/REM) optimal? No ablation explores alternatives like (W/N1 vs N2/N3/REM) or (W vs N1/N2 vs N3/REM)\n- The auxiliary loss weight α=1 gives equal importance to expert routing and final prediction—what's the theoretical justification?\n- No analysis of when/why the two-expert design is better than single-path or three-expert alternatives\n\n\n2. Weak justification for t-ALN design:\n- Claim that \"flattening disrupts temporal structure\" is overstated—Transformers routinely handle flattened image patches with learned positional embeddings\n- Element-wise squaring for \"energy representation\" lacks signal processing justification\n- Simple averaging for attention weights (Eq. 1) seems crude—why not learnable pooling?\n\n\n3. Incomplete experimental analysis:\n- Ablations only on ISRUC-S3, not all datasets\n- No statistical significance testing (p-values, confidence intervals)\n- Missing computational cost analysis (FLOPs, memory, inference time comparisons)\n- Unfair baseline comparisons: S³Net uses 10 channels while some baselines may use fewer\n\n4. Limited novelty:\n- SAE is a straightforward application of MoE with manual stage grouping\n- t-ALN combines standard operations without significant innovation\n- The \"alignment\" is essentially reshaping + attention, common in multi-modal fusion\n\n5. Reproducibility concerns:\n- Cross Swin Transformer details are sparse\n- \"Time-window-wise linear\" operation not precisely defined\n- Training dynamics (convergence, stability) not discussed\n- No discussion of hyperparameter sensitivity beyond Figure 8"}, "questions": {"value": "1. Stage grouping justification: Have you tried alternative groupings (e.g., three experts for W, N1/N2, N3/REM)? What happens if you let the model learn the grouping in an unsupervised manner?\n\n2. t-ALN necessity: Can you provide a controlled ablation where you flatten features but use very strong positional encodings in the Transformer? This would test whether the alignment is truly necessary or if it's compensating for weak positional encodings.\n\n3. Generalization: Does the hard/easy stage partition hold across different populations (healthy vs. disordered sleep, different age groups)? Could the transition matrices in Figure 1(a) vary significantly?\n\n4. Computational cost: What is the training time and inference time compared to cVAN and other top baselines? Is the 1-3% improvement worth the added complexity?\n\n5. Statistical significance: Can you provide confidence intervals or statistical tests (e.g., paired t-test) for the improvements over cVAN across folds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DxpVIOALCG", "forum": "EmIdvvqFPc", "replyto": "EmIdvvqFPc", "signatures": ["ICLR.cc/2026/Conference/Submission11065/Reviewer_1ogm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11065/Reviewer_1ogm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618990837, "cdate": 1761618990837, "tmdate": 1762922241833, "mdate": 1762922241833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents S³Net (Stage-Aware Sleep Staging Network), which aims to address two major challenges in automatic sleep staging: (1) the difficulty in distinguishing transitional stages (particularly N1 and N2) due to overlapping EEG characteristics, and (2) the structural misalignment between temporal and spectral feature representations in existing time–frequency fusion networks. To tackle these issues, the authors propose two main modules: a time-alignment network (t-ALN) that projects spectral features onto the temporal axis for synchronized time–frequency fusion, and a stage-aware expert (SAE) mechanism that separates easily and hardly distinguishable stages into two expert branches. Experiments on multiple public datasets (ISRUC-S1/S3, Sleep-EDF) show improved accuracy and F1 scores compared to recent baselines such as cVAN and MixSleepNet."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies two relevant challenges in EEG-based sleep staging and provides an intuitive design to address them. The structure of the paper and ablation study organization are clear and easy to follow.\n2. Experiments cover multiple datasets and baselines, and the ablation study demonstrates that both proposed modules contribute positively to overall performance.\n3. S³Net achieves consistent, albeit moderate, gains over strong baselines like cVAN and MixSleepNet across different datasets, especially in the N1/N2 stages that are known to be challenging."}, "weaknesses": {"value": "1. Limited methodological novelty.\nThe two problems discussed—stage heterogeneity and time–frequency misalignment—are well-known and have already been addressed by previous works (e.g., cVAN, MVF-SleepNet). The proposed t-ALN essentially performs a learnable projection and sequence reshaping rather than a true signal-level alignment, and the SAE module resembles a task-driven Mixture-of-Experts formulation. While effective, these ideas are incremental rather than fundamentally new.\n\n2. Unclear mechanism of “frequency-to-time projection.”\nThe claim that t-ALN “projects spectral features onto the temporal axis” is conceptually strong but technically ambiguous. The implementation relies on attention weighting and a Transformer encoder, without theoretical grounding that guarantees faithful temporal reconstruction. Thus, it is more of a structural alignment than an actual mapping from frequency to time, weakening the claimed interpretability.\n\n3. Inadequate differentiation from cVAN.\nAlthough cVAN is cited as a baseline, the paper does not provide a controlled comparison isolating the proposed modules from cVAN’s cross-view attention mechanism. It remains unclear whether S³Net’s improvement stems from its alignment design, expert division, or simply additional parameters. The contribution beyond cVAN is therefore not sufficiently demonstrated.\n\n4. Overstated claims versus empirical gain.\nThe reported improvements over state-of-the-art methods are relatively modest (≈1–2% accuracy, small F1 gains), and may not justify the complexity of the proposed architecture. Moreover, there is no quantitative analysis proving that the t-ALN truly enhances “alignment quality” or that the SAE indeed learns distinct stage-specific knowledge.\n\n5. Lack of theoretical or physiological justification.\nThe paper assumes that transitional sleep stages require distinct sub-models but does not provide physiological reasoning or statistical evidence (e.g., inter-class variance, confusion entropy) supporting this assumption."}, "questions": {"value": "1. What are the key architectural differences between S³Net’s fusion mechanism and cVAN’s cross-view attention? A direct replacement or ablation (t-ALN → cVAN-style attention) under the same backbone would help validate the claimed structural advantage.]\n2. Please provide quantitative or visual evidence that demonstrates improved time–frequency alignment after introducing t-ALN (e.g., correlation heatmaps or reconstruction error).\n3. Given that the gains are relatively modest, could the authors discuss whether the added model complexity (dual experts, cross-gate) brings a favorable accuracy–efficiency trade-off?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2oQGK9oiJO", "forum": "EmIdvvqFPc", "replyto": "EmIdvvqFPc", "signatures": ["ICLR.cc/2026/Conference/Submission11065/Reviewer_PzED"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11065/Reviewer_PzED"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973616281, "cdate": 1761973616281, "tmdate": 1762922241032, "mdate": 1762922241032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a deep learning model called S3Net (Stage-Aware Sleep Staging Network) for automatic sleep staging. The authors point out two main problems with existing methods: (1) the model struggles to distinguish the subtle differences between light sleep stages (N1, N2); and (2) the fusion of temporal and frequency domain features is insufficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The proposed SAE module is the first to explicitly introduce an \"expert-based stage difficulty\" mechanism in sleep staging tasks, combined with Cross-Gate dynamic weight allocation, demonstrating a clear and highly inspiring approach.\n\n2) The t-ALN module achieves temporal alignment of frequency and temporal features through learnable projection, effectively solving the problem of temporal structure loss after Transformer feature flattening.\n\n3) Comparison with 15 state-of-the-art (SOTA) models shows that the results significantly outperform existing methods on three public datasets. Ablation experiments and visualizations (t-SNE, confusion matrix, spectrogram) demonstrate the design's rationality and stability."}, "weaknesses": {"value": "1) The contributions in the introduction are too concise; further refinement is recommended. \n\n2) While the t-ALN and SAE module designs are intuitive, the paper lacks a theoretical explanation or complexity analysis of their underlying principles.\n\n3) Although it covers three commonly used datasets, it remains to be seen whether it can be applied to other physiological signals, such as PPG."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NVCNRVRIQx", "forum": "EmIdvvqFPc", "replyto": "EmIdvvqFPc", "signatures": ["ICLR.cc/2026/Conference/Submission11065/Reviewer_ndSL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11065/Reviewer_ndSL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976762149, "cdate": 1761976762149, "tmdate": 1762922239848, "mdate": 1762922239848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}