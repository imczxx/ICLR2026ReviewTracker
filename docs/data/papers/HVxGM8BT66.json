{"id": "HVxGM8BT66", "number": 11827, "cdate": 1758204089321, "mdate": 1763634655820, "content": {"title": "Follow the Path: Reasoning over Knowledge Graph Paths to Improve LLM Factuality", "abstract": "We introduce **fs1**, a simple yet effective method that improves the factuality of reasoning traces by sourcing them from large reasoning models (e.g., DeepSeek-R1) and grounding them by conditioning on knowledge graph (KG) paths. We fine-tune eight instruction-tuned Large Language Models (LLMs) on 3.9K factually grounded reasoning traces and rigorously evaluate them on six complex open-domain question-answering (QA) benchmarks encompassing 23.9K questions. Our results demonstrate that our **fs1**-tuned model (32B parameters) consistently outperforms instruction-tuned counterparts with parallel sampling by 6-14 absolute points (pass@$16$). Our detailed analysis shows that \\dataset{} considerably improves model performance over more complex questions (requiring 3 or more hops on KG paths) and numerical answer types compared to the baselines. Furthermore, in single-pass inference, we notice that smaller LLMs show the most improvements. While prior works demonstrate the effectiveness of reasoning traces primarily in the STEM domains, our work shows strong evidence that anchoring reasoning to factual KG paths is a critical step in transforming LLMs for reliable knowledge-intensive tasks.", "tldr": "This paper demonstrates that fine-tuning Large Language Models on reasoning traces grounded in knowledge graph paths improves their factual accuracy and performance on complex question-answering tasks.", "keywords": ["large language models", "reasoning", "factuality", "knowledge graphs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a7a517ae51184b232f1a1bf3b9f0a96ed3a6600.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes to build fine-tuning corpses by accompanying reasoning traces generated by Large Reasoning Models with reasoning paths. The paper demonstrates the effectiveness of such fine-tuning corpuses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed fine-tuning technique improves the performance of Large Language Models on several benchmarks, and some of the improvements are nontrivial."}, "weaknesses": {"value": "The technique proposed by the paper seems essentially to be fine-tuning with both questions and contexts, with the contexts being paths mechanically extracted from knowledge graphs. Despite the performance gains, the paper will unlikely provide any new insight for the community. (Please correct me if I am wrong.)"}, "questions": {"value": "Please see “Weakness”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lGHdyvoYSM", "forum": "HVxGM8BT66", "replyto": "HVxGM8BT66", "signatures": ["ICLR.cc/2026/Conference/Submission11827/Reviewer_SfuL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11827/Reviewer_SfuL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489502630, "cdate": 1761489502630, "tmdate": 1762922846949, "mdate": 1762922846949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"fs1,\" a method to improve the factual accuracy of Large Language Models (LLMs) on complex question-answering tasks. The core idea is to fine-tune instruction-tuned LLMs on \"reasoning traces\" that have been grounded in knowledge graph (KG) paths. The authors first generate reasoning traces (dubbed 'rt') from powerful reasoning models like DeepSeek-R1 by prompting them with questions. To enhance the factuality of these traces, they prompt the models again, but this time conditioning the generation on relevant KG paths extracted from Wikidata."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core concept of using KG paths to ground and improve reasoning traces is intuitive and, as the results show, effective. It's a practical approach to injecting verifiable facts into the model's reasoning process without requiring complex architectural changes.\n\nThe experimental setup is a major strength. The authors evaluate eight different model sizes across six diverse QA benchmarks, using four different prompting/fine-tuning setups (instruct, CoT, rt, fs1). This large-scale study provides robust evidence for their claims and allows for interesting insights into scaling effects. The use of LLM-as-a-Judge for evaluation is also a sensible choice for this type of open-ended generation task."}, "weaknesses": {"value": "The fs1 traces are generated by prompting a powerful reasoning model (like DeepSeek-R1) with KG paths. It's a bit unclear how much of the final performance gain comes from the superior reasoning of the teacher model versus the factual grounding provided by the KG paths themselves. An ablation study where the reasoning traces are generated from a weaker model but still grounded with the same KG paths could help disentangle these two effects.\n\nThe process of generating the fs1 dataset involves several steps: getting the question, retrieving relevant KG paths (which itself is a non-trivial task, as hinted by the SPARQL queries in the appendix), and then prompting a very large model. How expensive and time-consuming is this data creation process? While the resulting fine-tuning dataset is small (3.9K samples), understanding the upfront cost is important for assessing the method's practicality."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DCobIm8uxf", "forum": "HVxGM8BT66", "replyto": "HVxGM8BT66", "signatures": ["ICLR.cc/2026/Conference/Submission11827/Reviewer_VY98"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11827/Reviewer_VY98"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623029058, "cdate": 1761623029058, "tmdate": 1762922846333, "mdate": 1762922846333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigated whether grounding reasoning traces on knowledge graph paths and training models on them yield tangible gains in factual accuracy on complex open-domain QA tasks. They first sourced the reasoning paths from large reasoning models, then grounded them into KG paths, and finally finetuned the models with these KG-enhanced reasoning paths. They provide some key findings which help us better understand when and to what extent the KG helps."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Provide insights into when and to what extent KG-path enhanced reasoning helps LLMs to improve factual accuracy for multi-hop QA.\n- The experiments are comprehensive."}, "weaknesses": {"value": "- Numerous works have focused on KG-enhanced reasoning/retrieval on KGs to improve factual accuracy and reduce hallucination. It has been a common acknowledgment that using KGs, either as linearized graphs or as graph tokens (learned graph embeddings), can help with mQA tasks (to name a few [1-4]). Therefore, I believe this paper merely verifies a well-studied observation.\n- The technical contribution is limited. Using SFT with KG-enhanced reasoning is a common practice, so the technical contribution is limited.\n\n[1] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. Reasoning on graphs: Faithful and interpretable large language model reasoning. ICLR 2024.\n\n[2] Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph. ICLR 2024.\n\n[3] G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. NeurIPS 2024.\n\n[4] Gnn-rag: Graph neural retrieval for efficient large language model reasoning on knowledge graphs. ACL 2025."}, "questions": {"value": "Will there be cases where the reasoning paths (from entities in the questions to the golden answers) are too numerous and cannot fit into the context window?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "56MIp9aW5L", "forum": "HVxGM8BT66", "replyto": "HVxGM8BT66", "signatures": ["ICLR.cc/2026/Conference/Submission11827/Reviewer_5bsJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11827/Reviewer_5bsJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726493302, "cdate": 1761726493302, "tmdate": 1762922845811, "mdate": 1762922845811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}