{"id": "30I17s8gjs", "number": 11083, "cdate": 1758188827751, "mdate": 1759897609698, "content": {"title": "Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning", "abstract": "Offline reinforcement learning (RL) offers a powerful paradigm for data-driven control. Compared to model-free approaches, offline model-based RL (MBRL) explicitly learns a world model from a static dataset and uses it as a surrogate simulator, improving data efficiency and enabling potential generalization beyond the dataset support. However, most existing offline MBRL methods follow a two-stage training procedure: first learning a world model by maximizing the likelihood of the observed transitions, then optimizing a policy to maximize its expected return under the learned model. This objective mismatch results in a world model that is not necessarily optimized for effective policy learning. Moreover, we observe that policies learned via offline MBRL often lack robustness during deployment, and small adversarial noise in the environment can lead to significant performance degradation. To address these, we propose a framework that dynamically adapts the world model alongside the policy under a unified learning objective aimed at improving robustness. At the core of our method is a maximin optimization problem, which we solve by innovatively utilizing Stackelberg learning dynamics. We provide theoretical analysis to support our design and introduce computationally efficient implementations. We benchmark our algorithm on twelve noisy D4RL MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its state-of-the-art performance.", "tldr": "", "keywords": ["Offline Reinforcement Learning", "Model-based Reinforcement Learning", "Robustness", "Stackelberg games"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50b4767df81dd80235cc772a78c52ff8f9b300aa.pdf", "supplementary_material": "/attachment/cf73977516897865495d988594b5c0cf0c600094.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses two challenges in offline model-based reinforcement learning (MBRL): the objective mismatch between world model training and policy optimization, and the lack of robustness of the policy. The authors propose ROMBRL, a novel framework that formulates the problem as a constrained max-min optimization problem and then models it as a Stackelberg game. This is the first application of the Stackelberg game framework to offline MBRL. Through joint optimization, the policy and world model can be updated collaboratively, and the policy optimization process can account for the influence of the evolving world model, resulting in more stable learning. \n\nThe authors evaluate their method on the D4RL MuJoCo benchmark and the Tokamak control task. Experimental results demonstrate that ROMBRL outperforms existing state-of-the-art methods such as RAMBO and MOBILE in noisy environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper reframes the robustness problem of offline MBRL as a constrained maximin optimization problem and solves it via Stackelberg game theory, which is a novel and promising approach that can simultaneously optimize both the strategy and the world model to enhance robustness and effectively address the goal mismatch problem.\n\n2. The authors provide a theoretical analysis of the suboptimality gap for their approach, adding rigor to the work.\n\n3. The authors conducts experiments on control tasks. ROMBRL not only performs well on standard benchmarks, but more importantly, its performance degradation after adding noise is much smaller than that of other SOTA algorithms."}, "weaknesses": {"value": "1. While the introduction of Stackelberg learning dynamics is relatively new to offline MBRL, the core ideas are highly related to the existing work, like RAMBO. ROMBRL is more of a combination and extension of existing ideas rather than a fundamentally new learning framework.\n\n2. For algorithms that emphasize theoretical convergence and robustness, the hyperparameters that most directly impact stability need to be discussed. The choice of learning rates $\\eta_\\theta$ and $\\eta_\\phi$ in Stackelberg learning dynamics, as well as the Lagrange multiplier $\\lambda$ and the radius of the uncertainty set $\\epsilon$ in the maximin problem, are all key hyperparameters. The paper needs to discuss in more detail the sensitivity of these hyperparameters and how to robustly select them to ensure algorithm stability and high performance. While the appendix mentions some settings, it lacks a detailed discussion of sensitivity analysis.\n\n3. The overall information density of this paper is high, with concentrated derivations and formulas, which makes it difficult for readers to understand.\n\n4. This paper does not conduct ablation experiments to verify the respective contributions of Stackelberg learning dynamics and the gradient-mask mechanism, nor does it show the impact of the error of the Fisher approximation on the results.\n\n5. Need comparison with recent works in 2024 and 2025, and more experiment environments (e.g. AntMaze)."}, "questions": {"value": "1. In the experimental results section, could you provide a detailed comparison of ROMBRL with other baselines in terms of training time or computational resources (e.g., GPU memory, CPU time, etc.)? \n\n2. The Stackelberg learning dynamics require the learning rates to satisfy $\\eta_\\phi\\gg\\eta_\\lambda\\gg\\eta_\\theta$ for convergence.\nI'm curious how strict this learning rate hierarchy needs to be - is the method robust to small deviations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sDFG7iJa2V", "forum": "30I17s8gjs", "replyto": "30I17s8gjs", "signatures": ["ICLR.cc/2026/Conference/Submission11083/Reviewer_xJjV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11083/Reviewer_xJjV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570477530, "cdate": 1761570477530, "tmdate": 1762922264736, "mdate": 1762922264736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles an important problem in offline model-based reinforcement learning, namely the objective mismatch between world model learning and policy optimization, and the resulting fragility of learned policies under real-world environments. The authors propose a policy-driven world model adaptation framework that unifies the optimization of the policy and the world model under a single maximin objective. By formulating the interaction as a Stackelberg game, the method allows dynamic adaptation of both policy and model parameters, aiming to improve robustness. The theoretical formulation is complemented by an efficient learning algorithm and extensive experiments across 12 noisy D4RL MuJoCo tasks and 3 stochastic Tokamak control tasks. Overall, the paper is well-motivated and clearly written."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main contribution lies in the idea of adapting the world model jointly with the policy via Stackelberg learning dynamics. This formulation is elegant and potentially generalizable. The theoretical discussion provides a solid foundation for the algorithmic design, and the experimental results demonstrate consistent performance gains over standard baselines, suggesting that the approach improves robustness without severely compromising sample efficiency. The inclusion of both MuJoCo and Tokamak tasks also highlights the method’s applicability beyond common benchmarks."}, "weaknesses": {"value": "While the Stackelberg game formulation is compelling, it would be valuable to include an ablation study comparing it directly with a simpler min-max optimization setup, where the world model acts adversarially to reduce the policy objective J. Such an analysis could help clarify the practical advantages of adopting the Stackelberg dynamics.\n\nThe definition of the uncertainty set via the constraint $KL(P_\\bar\\phi(⋅|s,a) || P_\\phi^k(⋅|s,a)) ≤ \\epsilon$ appears to rely on a manually chosen threshold $\\epsilon$. Since different environments or state–action pairs may exhibit very different transition variances, a discussion or mechanism for adaptively determining $\\epsilon$ would make the approach more principled and easier to reproduce.\n\nWhile the idea of training the policy in an adversarial environment can indeed improve robustness, it also risks over-regularizing the policy and deviating from the true environment dynamics, leading to overly conservative or suboptimal behaviors. This trade-off seems closely tied to the choice of $\\epsilon$, and an empirical sensitivity analysis could help illuminate how the balance between robustness and realism is managed.\n\nRelatedly, previous studies in online RL have observed that policies trained under adversarial perturbations tend to perform worse in normal environments due to conservative bias. Interestingly, this effect does not appear in the authors’ offline experiments, where the policy trained with adversarial noise performs well even in nominal settings. This surprising result deserves further explanation. \n\nIn terms of experimental validation, the study could be more comprehensive. The current results are based mainly on three agents: HalfCheetah, Hopper, and Walker2d, with variations only in dataset type. While the addition of Tokamak control is appreciated, including more environments (e.g., AntMaze or Franka Kitchen) would help test generality and robustness in more complex dynamics.\n\nFinally, while baselines like CQL and EDAC are compared, it would strengthen the empirical section to include newer and relevant baselines such as ReBRAC [1], ARMOR [2], ATAC [3], S4RL, and IQL [5], which also address robustness and regularization in offline RL. Comparing against such recent methods would better situate the contribution within the current research landscape.\n\n[1] Revisiting the Minimalist Approach to Offline Reinforcement Learning\n\n[2] Adversarial model for offline reinforcement learning\n\n[3] Adversarially trained actor critic for offline reinforcement learning\n\n[4] S4rl: Surprisingly simple self-supervision for offline reinforcement learning in robotics\n\n[5] Offline reinforcement learning with implicit Q-learning"}, "questions": {"value": "What is the practical advantage of the proposed Stackelberg formulation compared to a simpler min–max optimization setup?\n\nHow should the uncertainty bound $\\epsilon$ be determined or adapted across environments with different transition variances?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gvNtEGVgXP", "forum": "30I17s8gjs", "replyto": "30I17s8gjs", "signatures": ["ICLR.cc/2026/Conference/Submission11083/Reviewer_xchB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11083/Reviewer_xchB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749834711, "cdate": 1761749834711, "tmdate": 1762922264298, "mdate": 1762922264298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ROMBRL introduces a policy-guided world model adaptation strategy by formulating robust offline model-based reinforcement learning (MBRL) as a constrained maximin optimization problem. This problem is addressed through novel Stackelberg learning dynamics, which provide formal convergence guarantees to a Local Stackelberg Equilibrium (LSE). In this framework, the learning rates are hierarchically structured so that the follower (world model) adapts faster than the leader (policy) and the dual variable, ensuring stable and efficient joint optimization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method's robustness is theoretically rigorous by employing a constrained maximin objective and Stackelberg game dynamics. This approach directly yields formal bounds (Theorems 1–3) on the policy's suboptimality gap and the necessary model uncertainty range.\n\n2. The introduction of Fisher Information Matrix approximations for the second-order terms, coupled with the leveraging of the Woodbury Matrix Identity to efficiently compute matrix inverses, makes the second-order gradient computation tractable."}, "weaknesses": {"value": "1. Although the authors compare ROMBRL with several state-of-the-art methods such as EDAC, MOBILE, and other baselines, none of these algorithms are explicitly designed for noisy or perturbed environments. If the main claim is that ROMBRL demonstrates superior robustness under noisy conditions, it would be important to include comparisons with existing robust offline RL methods, such as RORL [1]. RORL explicitly addresses robustness to observation perturbations by employing a simple yet effective value function smoothing technique. Including such a comparison would strengthen the empirical validation, especially given that ROMBRL introduces a more complex model requiring multiple approximations for second-order computations.\n\n\n2. The proposed algorithm relies on second-order derivatives and the intricate Stackelberg learning dynamics, which involve complex approximations (e.g., Fisher Information Matrices) and matrix operations (e.g., Woodbury Identity). Given this complexity, the model may also be more sensitive to hyperparameter choices. It would strengthen the paper to include a hyperparameter sensitivity analysis (e.g., a table showing performance variation under different key hyperparameters). Furthermore, for the main experiments, the results are reported as the mean ± standard deviation over only three random seeds. Considering the algorithm’s complexity and potential instability, it would be preferable to report results over a larger number of seeds to ensure statistical reliability.\n\n[1] Yang, Rui, et al. \"RORL: Robust Offline Reinforcement Learning via Conservative Smoothing.\" Advances in Neural Information Processing Systems 35 (2022): 23851–23866."}, "questions": {"value": "1. How do the proposed Stackelberg learning dynamics affect the training stability and convergence speed of ROMBRL in practice?\nIt would be helpful if the authors could include training curves (e.g., performance or loss evolution) to illustrate the learning behavior and convergence characteristics.\n\n2. The paper defines the uncertainty radius ϵ during training to constrain the model uncertainty set, but it is not entirely clear whether the same ϵ corresponds to the actual perturbation level used in the noisy deployment environment. It would be helpful if the authors could clarify how the deployment noise relates quantitatively to the training-time uncertainty radius, and whether ROMBRL maintains robustness when the real perturbation magnitude exceeds the assumed ϵ."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OAMPDdjWhB", "forum": "30I17s8gjs", "replyto": "30I17s8gjs", "signatures": ["ICLR.cc/2026/Conference/Submission11083/Reviewer_RnwD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11083/Reviewer_RnwD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880907963, "cdate": 1761880907963, "tmdate": 1762922263889, "mdate": 1762922263889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ROMBRL, a new offline model-based reinforcement learning algorithm that jointly adapts the world model and policy to improve robustness, especially under noisy or adversarial conditions. Unlike traditional two-stage MBRL methods, ROMBRL formulates policy and model learning as a unified maximin optimization problem, solved using Stackelberg learning dynamics. The approach is theoretically grounded and achieves state-of-the-art performance and robustness on standard benchmarks, including noisy MuJoCo and Tokamak control tasks, outperforming existing baselines in both accuracy and deployment stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- ROMBRL formulates policy and world model adaptation as a single maximin optimization problem, enabling joint learning and improved robustness compared to traditional two-stage approaches.\n- Extensive experiments demonstrate that ROMBRL achieves state-of-the-art performance and stability, even when observations are corrupted by Gaussian noise, outperforming existing baselines on noisy MuJoCo and Tokamak tasks.\n- The approach not only improves average performance but also reduces variance and failure rates during deployment, making it more reliable in real-world scenarios.\n- The use of Stackelberg learning provides a principled framework for robust policy and model updates, theoretically grounding the method."}, "weaknesses": {"value": "Empirically, the key advantage of the proposed method is that it is more robust compared to other approaches when faced with noisy observations, which is an important factor for actual deployment in real world scenarios, since sensor noise will always play a role. However, if I am not mistaken, the authors compare their developed algorithm only to baselines that were not specifically developed for and originally evaluated on this use case. To really show SoTA performance when faced with noisy / perturbed observations, I believe that comparison with prior developed methods for noisy observations is required - I am not an expert in this domain, but a quick search yields at least [1-3], likely there are more works examining this setting already.\n\n[1] Panaganti, Kishan, et al. \"Robust reinforcement learning using offline data.\" Advances in neural information processing systems 35 (2022): 32211-32224.\n\n[2] Zhou, Ruida, et al. \"Natural actor-critic for robust reinforcement learning with function approximation.\" Advances in neural information processing systems 36 (2023): 97-133.\n\n[3] Yang, Rui, et al. \"Uncertainty-based offline variational bayesian reinforcement learning for robustness under diverse data corruptions.\" Advances in Neural Information Processing Systems 37 (2024): 39748-39783.\n\n\nMore generally speaking, I believe a little more emphasis could be placed on examining prior works, i.e. the related works section is relatively thin - a couple more relevant offline RL works, with a bit of a different angle on robustness that I could recommend:\n\n[4] Ghosh, D., Ajay, A., Agrawal, P., & Levine, S. Offline RL Policies Should be Trained to be Adaptive. Proceedings of the 39th International Conference on Machine Learning (ICML), 2022.\n\n[5] Hong, J., Kumar, A., & Levine, S. Confidence-Conditioned Value Functions for Offline Reinforcement Learning. International Conference on Learning Representations (ICLR), 2023.\n\n[6] Swazinna, P., Udluft, S., & Runkler, T. User Interactive Offline Reinforcement Learning. International Conference on Learning Representations (ICLR), 2023.\n\n[7] Zhang, Y., Liu, J., & Wang, Y. Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning. Advances in Neural Information Processing Systems (NeurIPS), 2023."}, "questions": {"value": "please clarify why the method is not compared to prior works that specifically target noisy observations in the offline setting - maybe I am also mistaken somehow, but this is the key merit of your method, correct?\n\nAlso, please clarify why you deem [1] to be not scalable to high dimensional continuous control tasks - they are also evaluating on MuJoCo environments (e.g. Hopper) if I am not mistaken."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YkPLWcoEOz", "forum": "30I17s8gjs", "replyto": "30I17s8gjs", "signatures": ["ICLR.cc/2026/Conference/Submission11083/Reviewer_GF9U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11083/Reviewer_GF9U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903864026, "cdate": 1761903864026, "tmdate": 1762922263498, "mdate": 1762922263498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}