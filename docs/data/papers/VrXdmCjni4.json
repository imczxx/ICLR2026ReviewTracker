{"id": "VrXdmCjni4", "number": 12158, "cdate": 1758206039295, "mdate": 1759897528316, "content": {"title": "Interference-Isolated Elastic Weight Consolidation and Knowledge Calibration for Incremental Object Detection", "abstract": "Incremental Object Detection (IOD) enables AI systems to continuously learn new object classes over time while retaining knowledge of previously learned categories. This capability is essential for adapting to dynamic environments without forgetting prior information. Although existing IOD methods have made progress in mitigating catastrophic forgetting, they usually lack explicit and quantitative modeling of information conflicts during knowledge preservation, making task boundaries ambiguous. Such conflicts often stem from the fact that a single image can contain objects belonging to previous, present, and future tasks, where unlabeled past and future objects are often mistakenly treated as background. In this paper, we propose a novel approach grounded in Elastic Weight Consolidation (EWC) to alleviate conflict knowledge preservation caused by task interference. Specifically, we introduce the Interference Knowledge Isolated Elastic Weight Consolidation (IKI-EWC) framework for IOD, which leverages the mispredictions of the old detector on new task data to estimate task conflicts and suppresses them at the parameter level. By reformulating the Bayesian posterior of model parameters, we derive a mathematical relationship between previously learned knowledge and interference knowledge, enabling targeted elimination of conflicts during model weight updates. In addition, we also propose a prototype-based knowledge calibration (PKC) mechanism to further preserve old knowledge during the training of the objector's classification head. This method employs a learnable projection layer to compensate semantic drift in old class prototypes, and then jointly trains the classification head using both calibrated prototypes and current task features, thereby mitigating forgetting caused by classifier updates. Extensive experiments on PASCAL VOC and MS-COCO benchmarks demonstrate the effectiveness of the proposed method, outperforming state-of-the-art approaches across various settings.", "tldr": "", "keywords": ["continual learning", "object detection"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff6b0bacd946c01804562f510ee27e57ee6f0daf.pdf", "supplementary_material": "/attachment/f8423f5086ef315f7c4f0132157c2d8604b11ba3.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a key issue in incremental object detection: future classes are unlabeled in early tasks and are therefore learned as background, but later must be detected as foreground. This background conflict causes strong interference and catastrophic forgetting. The proposed framework consists of IKI-EWC, which aims to isolate and down-weight conflicting background knowledge during consolidation, and PKC, which aligns stored old-class feature prototypes to the current feature space and recalibrates the current classification head without keeping raw past images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper focuses on a real problem in incremental detection: future classes are treated as background first, then must be detected later. It proposes two modules, IKI-EWC and PKC, to keep old knowledge without storing past images.\n2.\tExperiments on PASCAL VOC and MS-COCO are broad, and ablations show both modules are useful."}, "weaknesses": {"value": "1.\tCompared with current state-of-the-art methods. \nThe paper claims state-of-the-art performance relative to prior incremental detection methods, but it does not compare against recent approaches such as RGR[1] and GMDP-ABR[2], which report equal or stronger final mAP on both multi-step PASCAL VOC and MS-COCO splits. The paper should include a direct comparison to RGR and GMDP-ABR in the main tables and clearly state in which regimes the proposed method is preferable, for example, no generator cost, lower complexity, or better stability on old classes.\n2.\tIKI-EWC formulation clarity.\nIKI-EWC is presented as deriving a clean posterior by separating non-conflicting and conflicting regions and then using this to define a new importance term for an EWC-style penalty. However, the core assumptions behind this construction, for example, proposal-level independence, using the previous model to approximate past label structure on current data, and a Laplace or Gaussian approximation, are only implicit. These assumptions should be stated explicitly in the main text where the final loss is introduced.\n3.\tMemory usage. \nThe paper emphasizes that it does not store past images, but PKC does maintain a feature memory of sampled ROI features and Gaussian prototypes for old classes. The total storage cost of this memory, for example, the number of stored features per class, their dimensionality and total size, is not reported, and there is no quantitative comparison to exemplar replay or to generative replay methods, which also claim to avoid storing raw past data but still keep some form of replay budget.  Reporting the memory footprint would make the comparison stronger and more credible.\n\n[1] Revisiting Generative Replay for Class Incremental Object Detection \n\n[2] HIGH-DIMENSION PROTOTYPE IS A BETTER INCREMENTAL OBJECT DETECTION LEARNER"}, "questions": {"value": "1.\tPlease add current state-of-the-art methods to the PASCAL VOC and MS-COCO comparisons. \n2.\tPlease state the explicit assumptions used in the IKI-EWC derivation in the main text.\n3.\tPlease report the prototype buffer size and memory usage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y1WP9hERJy", "forum": "VrXdmCjni4", "replyto": "VrXdmCjni4", "signatures": ["ICLR.cc/2026/Conference/Submission12158/Reviewer_a9JU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12158/Reviewer_a9JU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761628805448, "cdate": 1761628805448, "tmdate": 1762923113111, "mdate": 1762923113111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors introduce IIKC, a novel framework for Incremental Object Detection (IOD) that combines interference-aware Bayesian regularization (IKI-EWC) and Prototype-based Knowledge Calibration (PKC) to tackle catastrophic forgetting and knowledge conflict during continual learning of new object classes. IIKC identifies and isolates regions in new task data that create interference, using pseudo labels from the old detector, and recalculates parameter regularization based on both retained and conflicting knowledge. The PKC module corrects semantic drift by realigning previous class features with current ones using a learnable projection, retraining the classifier on calibrated prototypes. Experiments on PASCAL VOC and MS-COCO benchmarks show IIKC consistently outperforms state-of-the-art regularization and rehearsal methods on incremental settings, with higher mAP and reduced forgetting across more challenging task splits."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Novel Theoretical Contribution: The paper makes a meaningful theoretical contribution by reformulating Elastic Weight Consolidation (EWC) in a Bayesian framework that explicitly accounts for interference knowledge ‚Äî regions where unlabeled objects from future classes are mistakenly learned as background. This provides a principled mechanism to isolate and suppress task conflicts, addressing a long-standing limitation in incremental object detection (IOD).\n\n2) Comprehensive Framework: By integrating two complementary components ‚Äî IKI-EWC for parameter-level stability and PKC for feature-level calibration ‚Äî the proposed IIKC framework tackles both catastrophic forgetting and semantic drift. This dual approach effectively bridges low-level model regularization with high-level feature alignment, demonstrating thoughtful architectural design."}, "weaknesses": {"value": "1) Dependence on Pseudo-Labels and Sensitivity to Noise: The IKI-EWC module is heavily dependent on pseudo-labels provided by the former detector to estimate interference regions. In the incremental object detection setting, these pseudo-labels are usually noisy, particularly for classes where the previous model has low performance. Subsequently, incorrect pseudo-labels may misidentify interference regions and subsequently impact the estimation parameter importance. The manuscript does not qualify any pseudo-label accuracy or sensitivity studies to suggest feasibility under varying degrees of noise for the IKI-EWC to evaluate stability. It is difficult to assess feasibility in more realistic scenarios without assessing effect when the pseudo-labels are not accurate. \n\n2) Computational Complexity and Scalability Concerns: The proposed framework has several computation-heavy procedures, including running the old model on all data for new tasks for pseudo-labels, computing interference ratios (ùëò), and estimating Fisher-based parameter importance for large networks such as Faster R-CNN. All operations can be both cost-prohibitive on memory, and time. The paper does not indicate training overhead, runtime comparison, or scalability analysis for the number of classes or incremental steps are increased (e.g., COCO 80 classes or LVIS 1200+ classes), which raises concerns for practicality of the method\n3) Residual interference: Future-class objects remain unlabeled and are still at risk of being treated as background, which may not be fully resolved by the current method.\n\nIIKC offers promising advances for incremental detection, balancing stability and plasticity, but future work could improve robustness by incorporating memory replay or better foreground-background attention mechanisms."}, "questions": {"value": "1) Novelty & Conceptual Soundness: The paper introduces IKI-EWC to isolate interference knowledge in incremental object detection, but it remains somewhat unclear how this approach fundamentally differs, both theoretically and algorithmically, from prior interference-aware frameworks such as BPF (Mo et al., 2024) and GMDP (Wang et al., 2025), which also attempt to mitigate background conflicts and feature drift; could the authors provide a deeper explanation of what new insight of the reformulation and posterior correction (Eq. 10) contribute beyond existing EWC-based or distillation-based methods, and whether this formulation yields measurable theoretical guarantees or only empirical improvements?\n\n2) Methodology & Implementation: The proposed interference isolation depends heavily on pseudo-labels generated by the previous model to identify conflicting regions; since pseudo-label quality can vary widely and introduce noise, could the authors analyze how the accuracy of these pseudo-labels impacts interference estimation, describe the computational overhead of running the old detector on all new data, and clarify how the approach scales to large datasets (e.g., COCO or LVIS) where the number of proposals and IoU computations may become prohibitively expensive?\n\n3) Equation 2 and 3 mostly focus on Bayesian posterior? Is your framework Bayesian or leverage variational inference to build the network?\n\n4) Overall optimization is not clear.  Equation 2 is similar to EWC. The only difference is you replaced diagonal Fisher matrix with equation 12?. And, section 3.4: Equation 14 is just knowledge distillation on features? How this norm is helping for knowledge calibration? The total loss is combination of these two loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tzJx12UEaT", "forum": "VrXdmCjni4", "replyto": "VrXdmCjni4", "signatures": ["ICLR.cc/2026/Conference/Submission12158/Reviewer_Zo9q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12158/Reviewer_Zo9q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768263421, "cdate": 1761768263421, "tmdate": 1762923112711, "mdate": 1762923112711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors introduce IIKC, a novel framework for Incremental Object Detection (IOD) that combines interference-aware Bayesian regularization (IKI-EWC) and Prototype-based Knowledge Calibration (PKC) to tackle catastrophic forgetting and knowledge conflict during continual learning of new object classes. IIKC identifies and isolates regions in new task data that create interference, using pseudo labels from the old detector, and recalculates parameter regularization based on both retained and conflicting knowledge. The PKC module corrects semantic drift by realigning previous class features with current ones using a learnable projection, retraining the classifier on calibrated prototypes. Experiments on PASCAL VOC and MS-COCO benchmarks show IIKC consistently outperforms state-of-the-art regularization and rehearsal methods on incremental settings, with higher mAP and reduced forgetting across more challenging task splits."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Novel Theoretical Contribution: The paper makes a meaningful theoretical contribution by reformulating Elastic Weight Consolidation (EWC) in a Bayesian framework that explicitly accounts for interference knowledge ‚Äî regions where unlabeled objects from future classes are mistakenly learned as background. This provides a principled mechanism to isolate and suppress task conflicts, addressing a long-standing limitation in incremental object detection (IOD).\n\n2) Comprehensive Framework: By integrating two complementary components ‚Äî IKI-EWC for parameter-level stability and PKC for feature-level calibration ‚Äî the proposed IIKC framework tackles both catastrophic forgetting and semantic drift. This dual approach effectively bridges low-level model regularization with high-level feature alignment, demonstrating thoughtful architectural design."}, "weaknesses": {"value": "1) Dependence on Pseudo-Labels and Sensitivity to Noise: The IKI-EWC module is heavily dependent on pseudo-labels provided by the former detector to estimate interference regions. In the incremental object detection setting, these pseudo-labels are usually noisy, particularly for classes where the previous model has low performance. Subsequently, incorrect pseudo-labels may misidentify interference regions and subsequently impact the estimation parameter importance. The manuscript does not qualify any pseudo-label accuracy or sensitivity studies to suggest feasibility under varying degrees of noise for the IKI-EWC to evaluate stability. It is difficult to assess feasibility in more realistic scenarios without assessing effect when the pseudo-labels are not accurate. \n\n2) Computational Complexity and Scalability Concerns: The proposed framework has several computation-heavy procedures, including running the old model on all data for new tasks for pseudo-labels, computing interference ratios (ùëò), and estimating Fisher-based parameter importance for large networks such as Faster R-CNN. All operations can be both cost-prohibitive on memory, and time. The paper does not indicate training overhead, runtime comparison, or scalability analysis for the number of classes or incremental steps are increased (e.g., COCO 80 classes or LVIS 1200+ classes), which raises concerns for practicality of the method\n3) Residual interference: Future-class objects remain unlabeled and are still at risk of being treated as background, which may not be fully resolved by the current method.\n\nIIKC offers promising advances for incremental detection, balancing stability and plasticity, but future work could improve robustness by incorporating memory replay or better foreground-background attention mechanisms."}, "questions": {"value": "1) Novelty & Conceptual Soundness: The paper introduces IKI-EWC to isolate interference knowledge in incremental object detection, but it remains somewhat unclear how this approach fundamentally differs, both theoretically and algorithmically, from prior interference-aware frameworks such as BPF (Mo et al., 2024) and GMDP (Wang et al., 2025), which also attempt to mitigate background conflicts and feature drift; could the authors provide a deeper explanation of what new insight of the reformulation and posterior correction (Eq. 10) contribute beyond existing EWC-based or distillation-based methods, and whether this formulation yields measurable theoretical guarantees or only empirical improvements?\n\n2) Methodology & Implementation: The proposed interference isolation depends heavily on pseudo-labels generated by the previous model to identify conflicting regions; since pseudo-label quality can vary widely and introduce noise, could the authors analyze how the accuracy of these pseudo-labels impacts interference estimation, describe the computational overhead of running the old detector on all new data, and clarify how the approach scales to large datasets (e.g., COCO or LVIS) where the number of proposals and IoU computations may become prohibitively expensive?\n\n3) Equation 2 and 3 mostly focus on Bayesian posterior? Is your framework Bayesian or leverage variational inference to build the network?\n\n4) Overall optimization is not clear.  Equation 2 is similar to EWC. The only difference is you replaced diagonal Fisher matrix with equation 12?. And, section 3.4: Equation 14 is just knowledge distillation on features? How this norm is helping for knowledge calibration? The total loss is combination of these two loss?\n\n\n\n\nUPDATES:\nAfter going through the delineated responses, I am raising my score. \n\nGood luck :)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tzJx12UEaT", "forum": "VrXdmCjni4", "replyto": "VrXdmCjni4", "signatures": ["ICLR.cc/2026/Conference/Submission12158/Reviewer_Zo9q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12158/Reviewer_Zo9q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768263421, "cdate": 1761768263421, "tmdate": 1763652298068, "mdate": 1763652298068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose IIKC, a two-part framework for Incremental Object Detection (IOD): (1) Interference-Knowledge-Isolated Elastic Weight Consolidation leverages the old model‚Äôs mispredictions on new-task data to eliminate interference caused and rebuild the Bayesian posterior and parameter importance; and (2) Prototype-based Knowledge Calibration applies a learnable linear projection to compensate for semantic drift of old-class prototypes and then jointly retrains the classification head with current features. The approach outperforms strong baselines‚Äîcovering both no-rehearsal and small-exemplar rehearsal regimes‚Äîacross multiple stepwise and multi-step protocols on PASCAL VOC and MS-COCO."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed IKI-EWC internalizes the IOD-specific ‚Äúfuture-class- background‚Äù interference into computable posterior correction and importance fusion, yielding an end-to-end implementable path for conflict isolation. Compared with heuristic reweighting or soft masking, it offers an explicit probabilistic formulation with closed-form solutions (Eqs. 10 and 12).Which provides a systematic extension of EWC (new decomposition, a quantified $k$ , and a new fused importance $\\tilde{I}$ ) that is both theoretically grounded and engineering-ready.\n\n2. PKC addresses semantic drift using a lightweight projection together with prototype-based retraining, with small overhead and yielding clear gains (as confirmed by ablations). Moreover, compared with certain dual-teacher distillation schemes, the proposed method is simpler in both computation and implementation.\n\n3. The paper is well structured and clearly written. Figures 1 and 2 intuitively illustrate the core idea and overall framework, enabling rapid understanding. It also provides an overall framework diagram and complete training pseudocode‚Äîexplicitly specifying the input parameters Œ≥, Top-K, and Œª‚Äîwhich facilitates re-implementation and comparative ablation studies.\n\n4. The evaluation experiments are thorough and in-depth, the results show consistent gains over strong baselines on VOC/COCO across stepwise/multi-step protocols. The authors analyze show low sensitivity to Œ≥ and that computed k beats extreme settings, and report runtime/memory costs."}, "weaknesses": {"value": "1. The core of IKI-EWC is to accurately estimate interference regions, a procedure that relies entirely on pseudo-labels generated by the previous model ($M_{t-1}$ ) on the new data. If $M_{t-1}$ has degraded in performance or exhibits prediction bias, the interference estimation may be inaccurate, thereby affecting the overall performance of the framework. Although the paper notes this in the limitations section, it does not experimentally analyze how sensitive the method is to pseudo-label quality as a function of  $M_{t-1}$ performance.\n\n2. The statistical robustness of the relative mass $k$ is not demonstrated. Since $k$  is computed as a ratio of proposal counts, it is highly sensitive to the IoU threshold, the number of proposals, class long-tail effects, and scale distributions. When sample size or the positive/negative balance fluctuates across stages, the variance of  $k$  can become large, causing the importance III to oscillate excessively. The paper lacks a systematic report of confidence intervals and sensitivity analysis for $k$ .\n\n3. Evidence for the ‚Äúapproximate independence/orthogonality‚Äù assumption is limited. The derivations assume that ‚Äúclean historical data‚Äù and ‚Äúcurrent data‚Äù are approximately independent, and cite an early-training gradient angle of ‚âà90¬∞ as support. However, orthogonality is not independence, and early mini-batches do not characterize the entire training trajectory. If representations become increasingly coupled later on‚Äîespecially in multi-object/multi-scale settings‚Äîthe premise underlying the posterior reconstruction weakens, potentially biasing the direction of the regularization. This point requires more extensive discussion.\n\n4. PKC‚Äôs ‚Äúprototype + linear projection‚Äù design is simple; however, compared with EFC‚Äôs anisotropic constraints with Gaussian-prototype updates, and LDC‚Äôs label-free learnable drift compensation, it might show clear shortcomings in the granularity of drift modeling, robustness and statistical sufficiency.\n\n5. Several implementation details are insufficiently specified‚Äîfor example, the confidence threshold for pseudo-labels, the method and computational cost of Hessian/Fisher estimation, and the sampling procedure used in PKC. In addition, it is not fully transparent whether data augmentation and preprocessing are exactly matched to the strong baselines in both the rehearsal-free and rehearsal-based regimes.\n\n[1] Elastic Feature Consolidation for Cold Start Exemplar-Free Incremental Learning, ICLR 2024.\n[2] Exemplar-free Continual Representation Learning via Learnable Drift Compensation, ECCV 2024."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "smiEJCN66f", "forum": "VrXdmCjni4", "replyto": "VrXdmCjni4", "signatures": ["ICLR.cc/2026/Conference/Submission12158/Reviewer_V8s4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12158/Reviewer_V8s4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047231851, "cdate": 1762047231851, "tmdate": 1762923112261, "mdate": 1762923112261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the interference of knowledge between past/present knowledges in Incremental dense prediction problems. To mitigate this problem, the authors conduct quantative modeling based on bayesian analysis and propose novel algorithms called IKI-EWC. Also, the authors propose prototype based classifier correction algorithm to prevent the drift at the classfier level"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors well analysed the existing problem and then conducted rigorous mathematical analysis on that. This makes the motivation of the proposed method very strong\n- The paper is well written and easy to follow. \n-This paper provides extensive experimental results that can empirically support the authors' claim as well"}, "weaknesses": {"value": "- Authors approximated non-interference knowledge only by using current data. I agree that this would be the best/practical way of doing it under this setting but I wonder does it actually approximate the goal well? Is there any way of computing ground truth goal even by using whole ground data? \n\n- I wonder why the authors used parameter regularisation methods. It is generally know to be poor compared to distillation methods. Can we use the IKI concept for distillation based methods as well?\n\n- Prototype based classifier retain is quite common concept in classification CIL methods. Is it entirely novel in dense prediction tasks?\n\n- Doesn't not regarding IKI concept on PKC levels conflicts with the other module (IKI-EWC)?\n\n- Although the motivation is quite strong, improvements look marginal, especially when each module is solely applied."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ej43fs7Khk", "forum": "VrXdmCjni4", "replyto": "VrXdmCjni4", "signatures": ["ICLR.cc/2026/Conference/Submission12158/Reviewer_w6Bq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12158/Reviewer_w6Bq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762219563305, "cdate": 1762219563305, "tmdate": 1762923111700, "mdate": 1762923111700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}