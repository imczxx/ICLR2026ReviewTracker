{"id": "DdrsHWobR1", "number": 935, "cdate": 1756824206569, "mdate": 1763748324594, "content": {"title": "Disentangled Robot Learning via Separate Forward and Inverse Dynamics Pretraining", "abstract": "Vision-language-action (VLA) models have shown great potential in building generalist robots, but still face a dilemma–misalignment of 2D image forecasting and 3D action prediction. Besides, such a vision-action entangled training manner limits model learning from large-scale, action-free web video data. To address these issues, we propose DeFI, a novel framework that Decouples visual Forward and Inverse dynamics pretraining to exploit respective data sources, wherein video generation and action prediction are disentangled. We introduce the Foundation Forward Dynamics Model (FFDM), pretrained on diverse human and robot videos for future prediction, and the Foundation Inverse Dynamics Model (FIDM), trained via self-supervised learning to infer latent actions from unlabeled video transitions. These models are then integrated into a unified architecture for end-to-end finetuning on downstream tasks. In this manner, FFDM and FIDM first shine separately and then cooperate for mutual benefit. Extensive experiments on CALVIN ABC-D and SimplerEnv demonstrate state-of-the-art performance, with DeFI achieving an average task length of 4.51 for CALVIN, 51.2% success rate on SimplerEnv-Fractal benchmark and 81.3% success rate in real-world deployment, significantly outperforming prior methods", "tldr": "We decouples visual forward and inverse dynamics pretraining to exploit respective data sources, wherein video generation and action prediction are disentangled.", "keywords": ["robot learning，forward dynamics，inverse dynamics"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9fc6299ed1204fd5fbbd1fe2baa2309864574dc8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes DeFI, a framework that decouples forward and inverse dynamics pretraining for robot policy learning. A Foundation Forward Dynamics Model (FFDM) is pretrained via video diffusion on human + robot videos to model visual dynamics, while a Foundation Inverse Dynamics Model (FIDM) learns latent actions self-supervisedly from video transitions. The two are later coupled and fine-tuned on downstream tasks (CALVIN, SimplerEnv, Franka). DeFI reports higher average task length and success rates than prior VLA baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "There is clear conceptual novelty in separating forward / inverse pretraining, and synthesis of diffusion video modeling and latent-action quantization.\n\nThe experiments are extensive, with solid ablations showing component effects.\n\nThe targeted problem is significant, which is scaling robot learning with action-free human videos."}, "weaknesses": {"value": "Unfair empirical comparison (major): DeFI is fine-tuned on target datasets (CALVIN, SimplerEnv, Franka), whereas baselines such as OpenVLA and $\\pi_0$ appear evaluated as off-the-shelf checkpoints trained on their original dataset, which are zero-shot evaluated on the author's benchmarks against DeFI's fine-tuned checkpoint.\n\nNo zero-shot results: Despite “foundation” framing, all evaluations use fine-tuning ≥10 % of labeled data; zero-shot capability is not demonstrated."}, "questions": {"value": "I highly appreciate the authors for proposing a novel idea in a potentially very impactful area. The paper is well-written and thorough. However, the concerns over the unfair comparison above is a major concern of mine. If the authors can adequately address why the comparison is, or has to be set up this way, that would help me a lot in re-evaluating the decision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OA0nDI7iMA", "forum": "DdrsHWobR1", "replyto": "DdrsHWobR1", "signatures": ["ICLR.cc/2026/Conference/Submission935/Reviewer_zL2E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission935/Reviewer_zL2E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976532782, "cdate": 1761976532782, "tmdate": 1762915643971, "mdate": 1762915643971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the problem of VLA models having a misalignment of 2d image forecasting and 3d action prediction.  The paper proposes DeFI (Decoupled visual Forward and Inverse dynamics) pretraining to disentangle video generation and action prediction.\nTwo models are introduced: The Foundation Forward Dynamics model (FFDM) is pretrained on human / robot videos for future predictio; and the Foundation Inverse Dynamics model (FIDM) is trained via self-supervised learning to infer latent actions from unlabeled video transitions.  The FFDM and FIDM are then integrated together in an end-to-end finetuning for downstream tasks. Performance on various manipulation benchmarks is presented."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The disentanglement of forward and inverse dynamics learning enables leveraging distinct data sources.\n- Enables pretraining with action-free internet-scale video data. The pretrained FFDM can then be coupled with a FIDM that is trained for actions with different embodiments if necessary.\n- For robots with multiple cameras, the FFDM predicts future videos for each view independently."}, "weaknesses": {"value": "- The claim of new state of art on Calvin only outperforms prior methods by 4.2%.  It will be good to justify if and why this is a significant outperformance.\n- The paper doesn't provide sufficient details on how the FFDM and FIDM are interconnected.  Also more details on the action adapter at the output of the FIDM needs to be provided.\n- Overall, the key insight in the paper is to break up a typical VLA network into two parts (FFDM, FIDM) with each of these being pretrained separately and then finetuned together.  This is neither a theoretical contribution nor much of a breakthrough in architecture.  In this sense, the novelty of the paper is limited.  Granted, they do claim a (slight) improvement over SOTA."}, "questions": {"value": "- Unclear why the inverse dynamics only takes o_t and o_{t+n} and ignores all frames inbetween. Is the produced action then only at one time instant?\n- It's unclear what outputs of the FFDM serve as inputs to the FIDM.  Are the o_t and o_{t+N} outputs from FFDM that serve as input to FIDM (particularly, o_t is an input to FFDM as well right?).  Fgiure 2(a, b) should be updated to showcase this connection.\n- Authors should provide some detail on why increasing avg. length in CALVIN ABC-D benchmark is good (is this indicative of long-horizon tasks?).\n- The authors only consider 3D action prediction.  I presume this is 3D end-effector position prediction (rather than joint angle prediction).  The authors should mention why this is the chos3en action to predict.\n- Fig. 2(c) shows an action adapter - this is missing in 2(b) - authors should clearly indicate in 2(b) what the action adapter is."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AiLoa370nC", "forum": "DdrsHWobR1", "replyto": "DdrsHWobR1", "signatures": ["ICLR.cc/2026/Conference/Submission935/Reviewer_Hekh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission935/Reviewer_Hekh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015256599, "cdate": 1762015256599, "tmdate": 1762915643858, "mdate": 1762915643858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DeFI, a framework where first a forward dynamics model and a latent inverse dynamics model is learned on various video data without actions, and then an action head is fine-tuned to map latent actions to ground truth actions in a specific embodiment. The forward dynamics model is a diffusion model in visual embedding space, the inverse dynamics model is a VQVAE, and the action head is a diffusion policy. A core claim is that pretraining forward and inverse dynamics models separately improves performance over coupled pretraining on actionless videos. Various experiments in sim and real environments show that DeFI generalizes better than previous state-of-the-art under the same downstream finetuning setup."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of learning forward and inverse dynamics for better generalization on video data is well-established.\n- Disentangling forward/inverse dynamics learning and pretraining on large datasets is well-motivated.\n- Various experiments in sim and real environments show that DeFI improves downstream policy performance over prior state of the art.\n- Ablations validate the necessity of each design component in DeFI."}, "weaknesses": {"value": "- The main concern that I have is end-to-end finetuning of forward/inverse dynamics models seem to undercut the claim that disentangled learning of forward/inverse dynamics model improves performance. It would be good to see a clarification on what the authors mean by coupled end-to-end finetuning, as well an ablation where only one of forward/inverse/action head is fine-tuned on downstream policy data. See question section below.\n- No details on inference during robot experiments. The paper covers training and finetuning in detail in both the main text and the appendix. I might have missed it, but there doesn't seem to be much information on how exactly the forward dynamics/inverse dynamics/action head is then conditioned to complete tasks on the sim and real environments."}, "questions": {"value": "- In section 3.3, what do the authors mean by finetuning the coupled FFDM and FIDM end-to-end in the title? If the core claim is disentangled forward/inverse dynamics learning, then this seems to undercut the claim; then Appendix A.2 says you freeze the forward dynamics model and only finetune the inverse dynamics model and action head. It would be nice to see an ablation / clarification.\n- Could the authors clarify the inference process?\n- Franka Play Dataset seems to have a wrong citation in Table 10."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IGBKoalPN8", "forum": "DdrsHWobR1", "replyto": "DdrsHWobR1", "signatures": ["ICLR.cc/2026/Conference/Submission935/Reviewer_rVfL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission935/Reviewer_rVfL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762034454993, "cdate": 1762034454993, "tmdate": 1762915643692, "mdate": 1762915643692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DeFI, a novel framework that decouples visual forward dynamics and inverse dynamics pretraining to better leverage large-scale action-free videos for robot learning. The key innovation lies in separately pretraining two components: (1) a Foundation Forward Dynamics Model (FFDM) via video generation on mixed human/robot videos, and (2) a Foundation Inverse Dynamics Model (FIDM) using self-supervised learning to extract latent actions from video transitions without requiring explicit action labels. These models are then coupled and fine-tuned end-to-end for downstream tasks. The approach achieves state-of-the-art results on CALVIN ABC-D (4.51 average task length), SimplerEnv-Fractal (51.2% success rate), and real-world experiments (81.3% success rate)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Motivation: The paper clearly articulates the fundamental misalignment between 2D video forecasting and 3D action prediction in current VLA approaches, making a compelling case for the decoupled approach.\n\n- Presentation: The writing is accessible, the motivation is well-articulated, and the figures effectively illustrate both the method and results. The paper flows logically from problem identification to solution.\n\n- Experimental validation: The authors provide extensive experiments across multiple benchmarks (CALVIN, SimplerEnv, real-world Franka) with consistent improvements demonstrated across all settings.\n\n- Ablations: Tables 4-7 systematically validate multiple design choice, from the importance of pretraining to architectural decisions, providing good insights into what drives performance.\n\n- Method: The approach offers an interesting path toward leveraging action-free data for training, with a novel inference mechanism that combines forward and inverse dynamics in a principled way."}, "weaknesses": {"value": "- Outdated baselines: The comparison baselines don't include the most recent state-of-the-art VLAs, which diminishes the impact of the results. The paper would be significantly strengthened by comparisons against more recent models like Gr00t or π0/π0.5.\n\n- Frozen FFDM limitations: The authors acknowledge that the frozen FFDM causes performance issues on SimplerEnv due to sim-to-real gaps, which appears to be a fundamental limitation of the approach that isn't adequately addressed.\n\n- Limited gains from human videos: Table 5 shows only modest improvements from incorporating human videos (+0.17 on average task length). Given the added complexity of the dual pretraining pipeline, it's unclear whether this marginal gain justifies the approach."}, "questions": {"value": "- VQ-VAE discretization: Why specifically does VQ-VAE discretization help inverse dynamics learning? Have you experimented with other discretization methods (Gaussian mixture models, simple binning) or continuous latent actions? The paper would benefit from more analysis on why this particular bottleneck design is optimal.\n\n- DINO-based world model: You briefly mention a DINO-based world model in Table 6. Could you elaborate on why this underperforms the pixel-based approach? Intuitively, predicting future DINO latents with regression loss seems appealing - it would align better with the FIDM input space and reduce inference time.\n\n- Scaling behavior: How does performance scale with pretraining data size? Is there a point of diminishing returns? This is particularly important given the main motivation is to leverage large-scale human data.\n\n- Single denoising step: While you show that one denoising step maintains task performance (Table 6), can you provide visual comparisons showing what motion information is preserved versus lost? This would help understand why this aggressive optimization works.\n\n- Failure modes: What are the primary failure cases? Does the model struggle more with forward dynamics prediction or inverse dynamics inference? Qualitatively, where does this approach excel compared to standard VLAs, and where do VLAs maintain advantages (perhaps in reactive behaviors given their faster inference)?\n\n- Domain adaptation: Have you explored partial fine-tuning or adapter layers for FFDM that could address domain shift while preserving the benefits of pretraining? This question is relvant for the ` Frozen FFDM limitations` raised aboce"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TvsGxl8NcW", "forum": "DdrsHWobR1", "replyto": "DdrsHWobR1", "signatures": ["ICLR.cc/2026/Conference/Submission935/Reviewer_2pYP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission935/Reviewer_2pYP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762822988631, "cdate": 1762822988631, "tmdate": 1762915643536, "mdate": 1762915643536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}