{"id": "WkYzCpZMOF", "number": 827, "cdate": 1756819976487, "mdate": 1759898239834, "content": {"title": "Memory Type Matters: Enhancing Long-Term Memory in Large Language Models with Hybrid Strategies", "abstract": "The memory capabilities of Large Language Models (LLMs) have garnered increasing attention recently. Many approaches adopt Retrieval-Augmented Generation (RAG) techniques to alleviate the “Forgetting” problem in LLMs. Despite great success achieved, existing RAG-based memory approaches typically overlook the differences between memories and employ a unified strategy to process all memories, leading to suboptimal performance. Thus, an intuitive question arises: can we categorize memory into different types and select appropriate strategies? However, given the topic-rich, scenario-complex, and boundary-blurred nature of memory scenarios, achieving precise classification of memories is not easy. To address this challenge, we propose a memory multi-class benchmark in this paper, termed TriMEM.  TriMEM comprises 6,000 dialogue samples, providing precise annotations for memory types across diverse topics and scenarios. Building upon this foundation, we propose a novel memory framework, named MemoType. MemoType can adaptively identify the category of each memory and design tailored storage and retrieval strategies, thereby achieving satisfactory performance. Extensive experiments on retrieval and generation tasks demonstrate the effectiveness of the proposed approach.", "tldr": "", "keywords": ["Long-Term Memory", "LLM", "Conversation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0e70cddccf70caaa451a059c1c00baaaef21da2f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Inspired by psychology, the paper proposes to categorize memories into three types: episodic memory, personal semantic memory and general semantic memory and use these memory types to tailor retriever in the RAG system to improve the RAG performance. The paper has compared its designed RAG system with an extensive number of SOTA RAG systems on challenging RAG benchmarks of involving long memory context and show favorable performance. The improvement is shown for the retrieval performance as well as the RAG answer performance using corresponding metrics.\n\nThe ablation studies in the paper show that the proposed memory classification indeed improves the overall retrieval performance compared to the RAG system without such classifications. The retrieval strategy like \"fake memory\" and using keywords are effective for the retrieval performance and that the proposed strategy holds improvement across various existing retrievers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well motivated in its writing and the proposed methodology is quite clear for the readers to understand including its experimental settings as well as the motivations.\n- The paper has compared with an extensive number of SOTA rag systems with different focus (e..g structured RAG, memory enhancement, query enhancement) and the experimental results show the effectiveness of the proposed methods.\n- Such proposal seems new. Besides, the implementation is simple and this potentially means the paper can have a good impact in its adoption in real life scenarios. The novelty itself comes with a dataset and annotation which is a contribution on its own and can benefit the community by its introduction."}, "weaknesses": {"value": "Given that the paper's main contribution is the in introduction of memory classification and the ensuing improvement, there is lack in its detailed explanations and ablations. I will leave the explanations in the question section. But I would be curious to see: what is the effect of each retrieval since they are designed separately for each memory type (maybe even more detailed as the classification is multi-label, raising question of the effectiveness for the overlapping part)? What justifies the keyword method adoption in personal semantic memory?\nWithout answering these detailed questions, I don't have a clear idea how and why the proposed memory enhance the RAG performance, particularly the improvement in each domain. \n\nI do agree that the ablation is not trivial to construct including what baseline to choose to answer these questions; nevertheless, I think this question corresponds to the main theme and the contribution of the paper.\n\nI don't get a clear idea how the classifier will go for the OOD classification (does not fall into the classified types)."}, "questions": {"value": "- Is fake memory technology used for personal semantic memory as well?\n- How does the trained classifier handle and be trained for the classes that do not fall into existing categories?\n- In ablation study (classify), is the reported performance without classify but with key and fake strategies?\n- In ablation study (Hybrid), is keyword only impacting the personal semantic memory?\n- In related works agent memory, it seems that what the paper proposes can naturally enhance the existing agent memory techniques by letting agent possess/learn different strategies based on memory type. Do authors agree and have further thoughts to share on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VTEdzwyD5o", "forum": "WkYzCpZMOF", "replyto": "WkYzCpZMOF", "signatures": ["ICLR.cc/2026/Conference/Submission827/Reviewer_ipaj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission827/Reviewer_ipaj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760571172527, "cdate": 1760571172527, "tmdate": 1762915621034, "mdate": 1762915621034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets the memory retrieval and response generation problem, and propose a memory type classficiation benchmark by classifing conversational history of existing benchmarks into different types of memory i.e., episodic memory and personal semantic memory, and advocate to retrieve related memory according to the required type of memory (by classifing the type of query using the pre-trained model on the ccollected benchmark), and design specific retrieval strategies and pruning strategy to improve the retrieval and generation quality. Generally, the benchmark is build on top of existing benchmarks, and proposed method is more like re-combination of existing techniques, and more focus on retrieval part. Despite the experimental results is promising, it is uncertain where these gains come from."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is reasonable desipte it is too specfic for each type and more like engineering tricks.\n2. The experimental results confirm the effectiveness of proposed method."}, "weaknesses": {"value": "1. There are many studies that focus on different types of memory and propose to retrieve according to determined type [1]. This significantly weaken the contribution and novelty of the proposed method.\n\n2. the constructed benchmark is not detailed, i.e., how do you do quality control, why choose these benchmarks? these details are not mentioned in main paper. Despite some are included in appendix, it is not comprehensive.\n\n2. Generally, the proposed method contains three parts: query ruoter (a.k.a, part a, to decide which type of memory to retrieve), memory retrieval according to label by part a (a.k.a., part b), and memory pruning (a.k.a., part c). the experiment only confirms the effectiveness of a+b+c, and each module under this system. It is not clear: i) whether any method incorporate independant part leads to better performance, i.e., query router + other retrieval strategies in the baselines; ii) the effectes of cascade errors; iii) the results of table 4 and 5 shows there is no significant gain for part b and part c.\n\n\n[1] Perltqa: A personal long-term memory dataset for memory classification, retrieval, and fusion in question answering."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8TNfU28KQT", "forum": "WkYzCpZMOF", "replyto": "WkYzCpZMOF", "signatures": ["ICLR.cc/2026/Conference/Submission827/Reviewer_gzEA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission827/Reviewer_gzEA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761219283320, "cdate": 1761219283320, "tmdate": 1762915620581, "mdate": 1762915620581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This\n paper investigates how Large Language Models (LLMs) handle long-term \nmemory, particularly in Retrieval-Augmented Generation (RAG) systems \nthat store and recall past information to mitigate the “forgetting” \nproblem. Existing RAG-based memory frameworks typically apply the same \nretrieval and storage strategy to all memories, overlooking the fact \nthat different types of memories (e.g., factual, episodic, or semantic) \nrequire different handling. To address this, the authors introduce \nTriMEM, a benchmark containing 6,000 annotated dialogue samples that \ncategorize diverse memory types across topics and scenarios. Building on\n this benchmark, they propose MemoType, an adaptive memory framework \nthat automatically identifies the type of each memory and applies \ntailored retrieval and storage strategies accordingly. Experiments on \nretrieval and generation tasks demonstrate that MemoType improves both \nmemory organization and overall model performance, highlighting the \nimportance of memory categorization in long-term language model \nreasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Novel conceptual framing:**\n  The paper introduces a meaningful and intuitive perspective by \ncategorizing memories in LLMs into distinct types (episodic, semantic, \nfactual) and applying **type-specific retrieval and storage \nstrategies**, addressing an underexplored dimension of long-term memory \nmodeling in LLMs.\n\n* **New benchmark contribution (TriMEM):**\n  The authors contribute a well-structured and valuable dataset, \n**TriMEM**, containing 6,000 annotated dialogue samples with explicit \nmemory-type labels. This benchmark fills an important gap for studying \nmemory categorization and adaptive retrieval mechanisms in \nconversational settings.\n\n* **Clear motivation and design:**\n  The problem formulation is well-motivated, and the overall pipeline — \nfrom memory categorization to retrieval and generation — is logically \npresented and easy to follow.\n\n* **Potential for generalization and integration:**\n  The proposed **MemoType** framework is modular and could, in \nprinciple, be integrated into broader **RAG** or **agent-memory** \nsystems, making it a promising direction for long-term dialogue \nreasoning and adaptive retrieval research."}, "weaknesses": {"value": "**Limited validation of core claim:**\n  The paper’s main contribution lies in classifying memory types \n(episodic, semantic, factual) and applying type-specific retrieval \nstrategies. However, the benchmarks used do not contain explicit labels \nfor memory types. As a result, the experiments only demonstrate that \nMemoType improves downstream performance, without directly showing that \nthe type-adaptive retrieval mechanism itself is responsible for these \ngains.\n\n* **Insufficient diversity of evaluation benchmarks:**\n  The evaluation on LongMemEval-S, LongMemEval-M, and LoCoMo effectively\n measures retrieval and generation under long-context and multi-session \nconditions. However, to substantiate claims of general effectiveness in \nmemory categorization and adaptive retrieval, the study would benefit \nfrom incorporating **personalized or task-oriented memory benchmarks** \n(e.g., PerLTQA, MEMTRACK). These settings better reflect realistic agent\n memory use cases—such as user profiles, preferences, or tool-use \nhistory—where the distinction between factual, episodic, and procedural \nmemory is most impactful.\n\n* **Questionable generalization of the memory-type classifier:**\n  A notable concern lies in the **credibility and generalization** of \nthe proposed memory-type classifier, especially given that the results \nin Table 9 show it outperforming significantly larger models such as \n**Qwen3-8B, Qwen3-32B, Gemini-2.5-Flash, and GPT-4o-Mini**. The \nclassifier is a simple **BERT-based model** trained with binary \ncross-entropy loss over only a few iterations, which raises doubts about\n how such a lightweight model achieves superior results. This suggests \nthe evaluation setup may be benchmark-specific rather than reflecting \ntrue generalization. Without cross-domain tests, ablations, or analysis \nof potential data leakage, the claims about robustness and real-world \napplicability remain unconvincing."}, "questions": {"value": "1. How were the memory-type labels (episodic, semantic, factual) assigned \nor validated during training, given that existing benchmarks do not \nprovide such annotations?\n\n2. Can the authors provide evidence or analysis showing that the \nobserved performance improvement specifically arises from type-adaptive \nretrieval, rather than general architectural or training advantages?\n\n3. Why were personalized or task-oriented benchmarks (e.g., PerLTQA, \nMEMTRACK) not included in evaluation, given their relevance to memory \ncategorization and realistic long-term interaction scenarios?\n\n4. How does the simple BERT-based classifier generalize beyond the TriMEM dataset? Have the authors tested it on out-of-domain data or with alternative encoders (e.g., DeBERTa, RoBERTa) to assess robustness?\n\n5. The paper reports outperforming much larger models (e.g., GPT-4o-Mini, Qwen3-32B). Could the authors clarify the evaluation \nprotocol and whether there are benchmark-specific advantages or data  overlaps that might explain this result?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qTfKet30h3", "forum": "WkYzCpZMOF", "replyto": "WkYzCpZMOF", "signatures": ["ICLR.cc/2026/Conference/Submission827/Reviewer_Ajqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission827/Reviewer_Ajqg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930964813, "cdate": 1761930964813, "tmdate": 1762915620395, "mdate": 1762915620395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the challenge that, given the topic-rich, scenario-complex, and boundary-blurred nature of memory scenarios, achieving precise classification of memories is not easy, this paper proposes a memory multi-class benchmark in this paper, termed TriMEM. TriMEM comprises 6,000 dialogue samples, providing precise annotations for memory types across diverse topics and scenarios. Building upon this foundation, this work proposes a memory framework, named MemoType."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a memory multi-class benchmark.\n2. This work proposes a memory augmentation framework to classify and use memory for QA.\n3. The paper is well-structured."}, "weaknesses": {"value": "1. As far as I know, ref[1] has presented a benchmark for multi-class memory. However, this paper did not describe the core difference from [1].\n2. This paper includes three categories in the data. Why these three categories? Do we need other categories?\n3. The introduction of memory types is insufficient. The authors should provide more details about the types of memory.\n\n\n[1] Du et al., Perltqa: A personal long-term memory dataset for memory classification, retrieval, and fusion in question answering. In Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10), pp. 152–164, 2024."}, "questions": {"value": "1. This paper includes three categories in the data. Why these three categories? Do we need other categories?\n2. What is the core difference between this work and [1]?\n3. How can the MemoType framework balance the importance among different types of memory?\n\n[1] Du et al., Perltqa: A personal long-term memory dataset for memory classification, retrieval, and fusion in question answering. In Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10), pp. 152–164, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q7KYDe3YDa", "forum": "WkYzCpZMOF", "replyto": "WkYzCpZMOF", "signatures": ["ICLR.cc/2026/Conference/Submission827/Reviewer_jXah"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission827/Reviewer_jXah"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985100199, "cdate": 1761985100199, "tmdate": 1762915619918, "mdate": 1762915619918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}