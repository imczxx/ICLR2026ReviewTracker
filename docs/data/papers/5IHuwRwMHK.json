{"id": "5IHuwRwMHK", "number": 8686, "cdate": 1758094996211, "mdate": 1759897769706, "content": {"title": "Sound Verification of Deployed Neural Networks", "abstract": "Verification methods aim at mathematically proving desirable properties of neural networks, such as robustness to adversarial perturbations. A verifier is sound if and only if it never claims that a neural network has the desired property when it does not. It was shown recently that none of the currently known verifiers that are claimed to be sound are guaranteed to be sound when considering the deployed version of the verified network. Due to this, all the known verifiers are vulnerable to certain backdoor attacks, where an adversarial network passes verification but, in reality, it exhibits adversarial behavior in specific deployment environments. So far, it has been suspected that sound verification is prohibitively expensive if we wish to verify all possible executions&mdash;including parallel and stochastic ones&mdash;in deployment. *We are the first to propose an efficient error bounding technique that most known verifiers can apply to become practically sound.* The technique enables both interval bound propagation and symbolic propagation methods to remain sound even if the deployment environment allows for every possible expression tree to compute the network, and even if the expression tree is selected at random. \nWe present a theoretical foundation for our approach and demonstrate empirically that our technique indeed discovers all known deployment-specific attacks, introducing only a limited performance overhead.", "tldr": "", "keywords": ["sound verification", "backdoor attacks", "deployment", "floating point arithmetic"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cd02458f8d6557f3c681703e8d3fc47bcb151a66.pdf", "supplementary_material": "/attachment/a919627eb2c6fb5e719a2b44095723a00da487ee.zip"}, "replies": [{"content": {"summary": {"value": "Many neural network verifiers operate on a theoretical model, ignoring many practical aspects such as floating point inaccuracies.\nThis paper addresses this by designing verification algorithms that are also sound under these practical aspects."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Coming from a formal verification perspective that usually ignores these practical aspects,\nit's good to see that the field also explicitly expands in this area.\n- The paper is easy to read, making a good trade-off between technical description and providing intuitive understanding.\n- All technical details are rigorously developed."}, "weaknesses": {"value": "- The contribution could be seen as marginal, given that verification under floating-point precision has been researched before.\n- A running example could help with the intermediate understanding\n- Only two simple verification algorithms are shown, which trivially also suffer from such constraints (e.g., huge output intervals).\n- Explicitly stating the notation instead of just referencing other papers makes the paper self-contained.\n- The evaluation could be expanded, e.g., on different data sets."}, "questions": {"value": "- I believe VNN-COMP'25 had some issues with tolerances regarding floating points. Do you think these would have been resolved if all tools used your approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X6vkA4AZ1I", "forum": "5IHuwRwMHK", "replyto": "5IHuwRwMHK", "signatures": ["ICLR.cc/2026/Conference/Submission8686/Reviewer_aJmt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8686/Reviewer_aJmt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760646756895, "cdate": 1760646756895, "tmdate": 1762920498278, "mdate": 1762920498278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing neural network verifiers assume real arithmetic. Some account for floating-point error (FP error), but none of the existing verifiers account for the non-deterministic execution order of floating-point operations in modern architectures. This paper describes how existing bound propagation techniques for neural network verification can be extended to be sound under FP error and non-deterministic execution orders. The approach is based on bounding the maximal FP error that can accumulate from different execution orders for the primitive operations of a linear operation. Growing the bounds computed by interval arithmetic and zonotope propagation for neural networks by this FP error bound and using outward rounding makes these bound propagation approaches sound under FP error and non-deterministic execution orders. The experimental evaluation demonstrates that this additional rounding can detect several attacks on verifiers from the literature. The runtime overhead of the extension is up to 26% compared to an implementation without FP soundness."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "This paper is a step towards extending the certificates provided by neural network verifiers to the real execution environments in which neural networks are deployed. I am not aware of comparable approaches in the literature. The proofs are correct, and the experimental evaluation is convincing."}, "weaknesses": {"value": "The only significant weakness of this paper is its sloppy presentation. Please refer to the list of presentation issues below. I will raise my rating to \"accept\" if the authors address these issues during the rebuttal window. \n\nBeyond this, a weakness of this paper is that it can not address additional peculiarities of the hardware on which neural networks are deployed, so that the guarantees provided by this work still remain unsound in practice, as mentioned in the limitations section. Another weakness is that the paper only provides explicit methods for intervals and zonotopes, not for the most widely used polytope relaxations. In my opinion, both weaknesses are insignificant, given that research on extending the guarantees of neural network verifiers to real deployment environments is extremely sparse. Lastly, a hypothetical limitation of this paper is that its analysis breaks down if the product of the hidden layer width and the machine precision exceeds one, which might happen for very large networks in extremely quantised execution environments.\n\n#### **Presentation Issues**\n1. Reading the abstract, I did not know what \"expression tree\" referred to. Talking about the order of primitive operations, such as addition and scalar multiplication, would be easier to understand.\n2. Instead of talking about \"reasonable assumptions\" in line 52, state that you provide soundness for non-deterministic execution orders, but not, for example, special GPU algorithms for matrix multiplication.\n3. Quantify the \"reasonably low overhead\" in line 66. Also report the absolute numbers alongside the percentages.\n4. Put brackets around your citations when they are not part of the sentence. For example, \"is known to be NP-complete (Katz et al, 2017)\" in line 75.\n5. The sentence \"sound (but not *necessarily* complete) verifiers aim to ... at the expense of completeness\" is contradictory. You are talking about sound incomplete verifiers.\n6. Please settle on one term for linear bound propagation instead of referring to it as \"symbolic\", \"linear\" (line 80), and \"Polyhedra\" (Table 1).\n7. I appreciate that you cite early references for many approaches, such as Miné (2004) for linear interval expressions. In this spirit, there are clearly earlier references for IBP than Xu et al. (2020).\n8. Stating that the outcome of an associative operation can strongly depend on the execution order in line 107 is contradictory.\n9. The norm is not $p$ but ${\\| \\cdot \\|}_p$ in line 127.\n10. Your definition of $P(x^\\ast)$ states that the *value* of the largest output of the network for $x$ needs to match the *value* of the true-class output for the reference input. That does not make sense. Since you have already introduced the notation, it is much easier to state $P(x^\\ast) = \\{x : y(x^\\ast) = y(x)\\}$. There are similar slip-ups in the remainder of the text.\n11. omit the \"very\" in \"very different\" in line 161 or write \"markedly different\". \n12. The $D_{p, \\epsilon, \\ldots}$ in line 177 misses a $(x^\\ast)$. \n13. \"binary operation\" is an ambiguous expression in line 186. I understood it to mean \"expression with two arguments\". However, IBP also uses intervals for unary operations. I think \"binary\" can safely be omitted here and in the remainder of the paper.\n14. The sentence in line 190 is unclear. Besides, it could probably use a citation to \"On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models\" by Gowal et al. \n15. Introduce the $\\delta_i$ variables more clearly. In particular, $\\delta_i \\in \\mathbb{R}$. At first, I thought $\\delta_i$ was a positive constant.\n16. Similarly, introduce $\\delta(n)$ in Lemma 1.\n17. I think $l, u$ should lie in $X$, not $\\mathbb{R}^n$ in Proposition 2.\n18. Avoid formulations like \"it is obvious\" in line 344. Instead, give a brief reasoning. \n19. Your IBP ReLU relaxation is faulty. It should be $\\max(l, 0)$, not $\\min(l, 0)$ in line 248.\n20. By \"affine arithmetic\" in line 362, are you referring to real arithmetic as opposed to floating-point arithmetic?\n21. Line 368, \"Of course, more sophisticated approximations can also be used\". If you have an FP-sound formulation of CROWN, please provide it. Otherwise, strike this sentence.\n22. What is $h$ in line 413?=\n23. Also provide absolute runtime overheads in sections in line 445.\n24. The references contain duplicated URLs and DOIs.\n25. Some DOIs, such as 10.1007/978-3-319-77935-5\\_9 are unresolvable.\n26. Some abbreviations and tool names are not capitalized correctly in the references, such as \"Intervalarithmetic.jl\".\n\nI did not read the entire appendix, but please also correct issues similar to the above in the appendix."}, "questions": {"value": "- Equation (9) requires a `np.where` operation, or similar, to select among the three cases in a vectorised computation. Can that lead to any numerical issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ceJVyrEV7U", "forum": "5IHuwRwMHK", "replyto": "5IHuwRwMHK", "signatures": ["ICLR.cc/2026/Conference/Submission8686/Reviewer_y7Wb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8686/Reviewer_y7Wb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761248734264, "cdate": 1761248734264, "tmdate": 1762920497941, "mdate": 1762920497941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Formal verification of neural networks does not guarantee safety in deployed models due to technical factors affecting floating-point computations, such as precision and computation order. The paper follows a method [Higham, (2002)] to bound the backward error of inner products and introduces a technique to formally bound the output value of a computation regardless of its execution order. In addition, the authors propose two improved versions of existing bounding methods (FPSoundIBP and FPSoundSymbolic). These methods are compared to the original versions (SoundIBP and SoundSymbolic) in terms of soundness, runtime, and accuracy (output-range similarity). The results demonstrate that the new versions guarantee soundness while preserving similar runtime."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses relevant challenges in the formal verification of neural networks.\n2. It takes a first step toward the formal verification of deployed neural networks.\n3. The method is integrated into two common bounding mechanisms used in formal verification."}, "weaknesses": {"value": "1. Limited evaluation: rows 1-10 in Table 1 appear in previous work [Szász et al. (2025)], so the new experimental results include only rows 11-12 in Table 1, which only confirm the theoretical results but do not supply additional information. Figure 1 and Table 2 are based on experiments with 100 input samples and one model trained on MNIST.\n\n2. Limited soundness: \n- The claim that the method preserves runtime is not supported by the results for IBP, where the runtime increased by ~25%.\n- The claim (Line 27, Line 65) that the method preserves accuracy (output-range) is not supported in the only check with respect to Order3 attack, where the output range is much larger for Order3. The method should be compared in other environments (Pr., Order1, Order2, Zombori et al. (2021)) as well.\n\n3. Scalability: The assumption that $n\\cdot\\mu<1$ limits the scalability of the proposed method (if $n>1/\\mu$). Moreover, multiplying $(2n − 1)$ times by $\\Delta$ seems to significantly increase the bounds on the result.\n\n4. Missing related literature: No prior work on formal verification of quantized networks [1, 2, 3] is mentioned.\n\n5. Readability issues:\n\n- The term “expression tree” appears five times (two of them in the abstract) before being explained at Line 215.\n\n- The authors claim they are “closely following [Higham (2002)]” to bound inner products, but the technique of [Higham (2002)] is not explained at all, although it appears to be the core of the proposed bounding method.\n\n- Line 361: “similar to DeepZ but without using affine arithmetic.” DeepZ's details are not explained in the paper.\n\n- Line 413: What is h?\n\nA. Towards Efficient Verification of Quantized Neural Networks (Huang et al., AAAI 2024).\n\nB. QVIP: An ILP-based Formal Verification Approach for Quantized Neural Networks (Zhang et al., ASE 2022).\n\nC. Scalable verification of quantized neural networks (Henzinger et al., AAAI 2021).\nQVIP: An ILP-based Formal Verification Approach for Quantized Neural Networks (Zhang et al., ASE 2022)\nScalable verification of quantized neural networks (Henzinger et al., AAAI 2021)."}, "questions": {"value": "1. The work does not support complete verification. It is recommended to change the title to a more modest one (e.g., “Towards…”).\n\n2. It is stated (Lines 240-243) that the error of each calculation can be expressed as a multiplication by \n$(1+\\delta_i)$, but there is an example (Lines 153–155) where the result changes from 0 to 1. How does multiplication correct the error in this case?\n\n3. Can the authors share the (average) values of $n$, $\\delta_i$ and $\\Delta$​ in the experiments? What is the average ratio $\\delta_i/\\Delta$? It can help the reader to approximate the effect on the output range.\n\n4. It seems that all rows in Table 1, except the last two, are taken from [Szász et al. (2025)]. Is that correct? Why didn’t the authors state this explicitly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fSN4yfqIiA", "forum": "5IHuwRwMHK", "replyto": "5IHuwRwMHK", "signatures": ["ICLR.cc/2026/Conference/Submission8686/Reviewer_rzs3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8686/Reviewer_rzs3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868626357, "cdate": 1761868626357, "tmdate": 1762920497551, "mdate": 1762920497551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing verifiers typically validate only the ideal mathematical model of a neural network, whereas real-world deployments can introduce deviations due to floating-point precision, operation ordering, and parallel execution. Attackers may exploit these discrepancies to embed deployment-specific backdoors: a model appears safe during verification but behaves maliciously once deployed. This paper aims to incorporate all possible numerical execution behaviors that may occur during deployment into the verification process to ensure that robustness guarantees hold in realistic execution environments. For example, after a model is trained, an attacker may target a specific deployment platform with certain numerical properties, identify neurons whose behavior is highly sensitive to these properties, and craft trigger inputs that activate them only on that platform, thereby manipulating the model’s output while bypassing verification. To defend against such threats, the authors derive a order-independent relative error bound ∆ based on backward floating-point error analysis and use it to widen the inner-product computations so that the resulting intervals provably cover all possible floating-point outcomes across deployment environments. This ensures that verification conclusions remain valid under any feasible deployment execution."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is among the first to explicitly address minute numerical discrepancies across deployment scenarios and demonstrate that these can be exploited to create stealthy backdoors affecting model trustworthiness.\n2. The mathematical formulation and soundness proof are rigorous and professionally presented.\n3. The structure and argumentation are generally well organized."}, "weaknesses": {"value": "1. The motivation and threat model remain abstract. The description of deployment-specific backdoors in the Introduction is theoretical and may be difficult to follow for non-experts. A visual attack-flow illustration would significantly enhance clarity (e.g., attacker characterizing deployment → constructing environment-sensitive detector neuron → crafting trigger inputs → activation upon deployment).\n2. Lack of flexibility and ablation for the widening parameter ∆. Although ∆ is theoretically derived, the paper does not evaluate the sensitivity of the verifier to its scaling (strictness vs. conservativeness trade-off). Ablation using a scaling factor (e.g., αΔ) or analysis across different depths / fan-ins would strengthen empirical understanding.\n3. Insufficient explanation of runtime overhead causes. While empirical runtime curves are provided, the paper does not clearly articulate where the additional cost comes from (e.g., more unstable ReLUs → more linear relaxations → increased concretization). A short explanation—possibly with breakdown—would help guide future optimization.\n4. Assumptions and applicability need clearer visibility. Some limitations (e.g., not addressing overflow/underflow, numerically approximated operations) appear only near the conclusion. These constraints are important in practice and should be highlighted earlier, including discussion of potential extensions."}, "questions": {"value": "1. Could you include one or two illustrative diagrams in the Introduction showing:\n(a) how an attacker detects or infers a target deployment setup,\n(b) how environment-sensitive detector neurons are constructed or identified, and\n(c) how trigger inputs activate malicious behavior only in deployment?\nUsing Order3 or the precision-based attack as an example would be highly instructive.\n\n2. While ∆ is theoretically determined, could you comment on or experiment with its tunability?\nFor example, sweeping a scaling factor αΔ and reporting how robustness success rate, interval width, and runtime change? Also, how sensitive is ∆ to layer-wise fan-in? Are some layers more influential than others?\n\n3. Could you provide a brief explanation or breakdown of runtime overhead sources?\nEven a coarse analysis—such as contributions from interval scaling & outward rounding, unstable-ReLU relaxations, symbolic expression growth—would help clarify operational trade-offs and guide future improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tPiUAiaQrk", "forum": "5IHuwRwMHK", "replyto": "5IHuwRwMHK", "signatures": ["ICLR.cc/2026/Conference/Submission8686/Reviewer_7KQz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8686/Reviewer_7KQz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976727094, "cdate": 1761976727094, "tmdate": 1762920497237, "mdate": 1762920497237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}