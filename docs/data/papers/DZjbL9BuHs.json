{"id": "DZjbL9BuHs", "number": 21667, "cdate": 1758320304536, "mdate": 1759896909684, "content": {"title": "Generalization of RLVR Using Causal Reasoning as a Testbed", "abstract": "Reinforcement learning with verifiable rewards (RLVR) is increasingly used for post-training large language models (LLMs) to reason, but it remains unclear when RLVR yields reliable generalization. This paper investigates the generalization of RLVR using causal reasoning problems as a testbed, namely probabilistic inference in a causal graphical model. We choose this setting because causality is an important area that LLMs still struggle with, and because this setting provides us with two natural axes of difficulty along which to systematically probe generalization: the level of the probabilistic query—associational, interventional, and counterfactual—and the complexity of the query, as measured by the size of its relevant subgraph. We generate datasets of causal graphs and queries spanning these axes of difficulty and use them to fine-tune Qwen-2.5-Instruct models using RLVR and SFT, varying the query level seen during training and the model scale (3B–32B). Our experiments show that RLVR achieves stronger within- and across-level generalization than SFT, but only on a subset of (model scale, query level) configurations. We trace the source of RLVR's effectiveness (or lack thereof) partly to the reasoning capability of the LLM on a particular level prior to fine-tuning. RLVR then improves the marginalization strategy and reduces probability derivation errors in the reasoning steps, significantly boosting accuracy overall and especially on more complex queries. Overall, we found RLVR significantly improves generalization on casual reasoning queries at the association and intervention level, but counterfactual level queries remain challenging for all models investigated in our experiments.", "tldr": "", "keywords": ["Large Language Models", "Reinforcement Learning with Verifiable Rewards", "Generalization", "Causal Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82565c73772ddb6441fa82f53eceee86c543b342.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the generalization of reinforcement learning with verifiable rewards (RLVR). The authors selected causal reasoning as the probing task. Specifically, authors generate a benchmark of causal graphs alongside corresponding questions. With respect to the experiments, the authors make a comparison between SFT and RLVR and find that RLVR outperforms SFT on some subsets of the benchmark. Finally, the authors conclude that RLVR indeed enhances the generalization abilities of causal reasoning at the association and intervention level."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments are comprehensive and sufficient to verify the authors' claims.\n\n2. The analysis is comprehensive, authors provide in-depth analyses."}, "weaknesses": {"value": "1. I think the first weakness lies in the writing of the abstract. The current version seems a bit colloquial rather than a formal academic paper. Specifically, the sentence \"We choose this setting because causality is an important area that LLMs still struggle with, and because this setting ...\" is too colloquial and lengthy. I suggest authors consider rewriting the abstract thoroughly and consider using shorter sentences.\n\n2. Similarly, in the introduction, the sentence \"However, we focus on identifying situations in which RLVR itself generalizes effectively\n(versus not), and we focus on the causal reasoning domain.\" may be informal. Authors employ \"we focus on\" twice and \"(versus not)\" may be a little informal. Besides, \"in which RLVR itself generalizes effectively\" can be confusing, what dose RLVR itself mean here? I would encourage authors to revise the introduction throughly. For example, \"our work differs from prior studies by focusing on an essential and challenging task: causal reasoning.\" \n\n3. I would suggest that authors include a discussion on the practical value of the studied formal causal reasoning. Since I believe LLMs are more targeted at the commonsense setting, and there already exist lots of formal causal tools in the area of causal inference.\n\n4. Authors should consider assigning a formal name for their datasets (e.g., RLCausal or other names), since it is an important contribution of this work.\n\n5. As there already exist other formal causal reasoning benchmarks (e.g., the CLADDER [1]), I would suggest that authors add an individual section on the differences between their newly proposed datasets and existing benchmarks. Why can't other benchmarks test the generalization abilities of RLVR?\n\n6. The colors in Figure 1 are too light; the authors should consider deepening them. The text is too small, and it's unnecessary to show every detail of the sample; simplifying the content would allow for a larger font size. Besides, I do not quite get the main structure of the Task Formulation in this version. Authors should revise Figure 1 to a more abstract and summarized representation.\n\n> [1] Jin Z, Chen Y, Leeb F, et al. Cladder: Assessing causal reasoning in language models[J]. Advances in Neural Information Processing Systems, 2023, 36: 31038-31065.\n\nHappy to raise the score if the authors could address my concerns."}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "82sOzi0utq", "forum": "DZjbL9BuHs", "replyto": "DZjbL9BuHs", "signatures": ["ICLR.cc/2026/Conference/Submission21667/Reviewer_Cf3X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21667/Reviewer_Cf3X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761383729684, "cdate": 1761383729684, "tmdate": 1762941882254, "mdate": 1762941882254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the relationship between large language models’ (LLMs) post-training mechanisms and their ability to perform causal reasoning tasks. The author introduces a data generation process to create and evaluate causal question-answering scenarios. The study then compares models post-trained using Supervised Fine-Tuning (SFT) and Reinforcement Learning from Verbal Reward (RLVR, including GRRO and DAPO variants) across different evaluation dimensions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-structured and clearly written, with a logical flow that makes it easy to follow the author’s reasoning. The analysis sections are particularly strong: insightful, well-grounded, and supported by detailed experiments. The results are extensive and could serve as a valuable reference for future researchers studying the intersection of LLM post-training and causal reasoning.\nI was initially debating between a rating of 6 and 8 and currently lean toward the former, though I remain open to adjusting this based on the rebuttal and other reviewers’ feedback."}, "weaknesses": {"value": "1. The RLVR experiments use 7.5K and 2.5K samples, while the SFT model is trained on 5K samples. This discrepancy makes the quantitative comparison between models less reliable. I suggest adding an ablation study where models (or checkpoints) are trained on the same amount of data and for comparable GPU hours to mitigate this concern.\n\n2. LLMs learn differently from humans as they rely primarily on language pattern recognition rather than true causal inference. The proposed causal reasoning tasks implicitly require two skills: (a) retrieving or identifying the correct numerical values from context, and (b) performing basic computations. It would strengthen the paper to include a detailed error analysis comparing SFT and RLVR models on these sub-tasks, in addition to the LLM-judge results that emphasize human-like problem-solving performance."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1w1Vz8gthN", "forum": "DZjbL9BuHs", "replyto": "DZjbL9BuHs", "signatures": ["ICLR.cc/2026/Conference/Submission21667/Reviewer_5AJc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21667/Reviewer_5AJc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687255342, "cdate": 1761687255342, "tmdate": 1762941881782, "mdate": 1762941881782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the generalization capabilities of RLVR for post-training LLMs, using probabilistic inference in causal graphical models as a testbed. The authors fine-tune Qwen-2.5-Instruct models (3B-32B parameters) using both RLVR and SFT on datasets of causal graphs and queries spanning three difficulty levels and complexity. The work contributes to understanding the conditions under which RLVR effectively generalizes, highlighting both its strengths and limitations in challenging formal reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The choice of probabilistic inference in causal graphical models as a testbed is genuinely innovative. Unlike prior RLVR generalization studies that focus on text/visual reasoning tasks, this formal mathematical domain enables precise control and analysis.\n- The findings have practical implications: practitioners should check if their base model has sufficient reasoning capability before investing in RLVR. The identification that counterfactual reasoning remains unsolved even with RLVR and 32B models highlights a key challenge for the field."}, "weaknesses": {"value": "- SFT is trained only to predict final answers while RLVR generates full reasoning chains. This creates an asymmetric comparison that conflates two factors: (1) reasoning vs. direct prediction and (2) RL vs. supervised learning. A fair strategy is to include an SFT baseline trained on optimal reasoning chains (generated by the solver or sampled from successful RLVR rollouts). This would isolate whether gains come from RL exploration or simply having reasoning chains.\n- The paper observes 3B models fail to benefit from RLVR and regress to direct prediction after training, but doesn't investigate what specifically these models lack. \n- The reward is simply r = 0.8 · accuracy + 0.2 · format with threshold t=0.01. Is the 0.8/0.2 weighting optimal? Would shaped rewards (partial credit for intermediate steps) help?\n- The writting is vague. For example, the description of the third finding \"What did RLVR learn? \" is too abstract, containing too many technical terms. They should present this result in a concrete style.\n- Why not compare RLVR with other RL-based post-training paradigm, such as RLHF and RLAIF?"}, "questions": {"value": "- Can you provide results for SFT trained on reasoning chains (even if just for 7B/one level)?\n- Can models solve trivially simple counterfactuals (e.g., 3-node graphs, no marginalization needed)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rnaoBOd4FL", "forum": "DZjbL9BuHs", "replyto": "DZjbL9BuHs", "signatures": ["ICLR.cc/2026/Conference/Submission21667/Reviewer_yW6b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21667/Reviewer_yW6b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762250202597, "cdate": 1762250202597, "tmdate": 1762941881348, "mdate": 1762941881348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}