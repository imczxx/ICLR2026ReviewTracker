{"id": "OHn8jDdpFk", "number": 9163, "cdate": 1758113608662, "mdate": 1763564577540, "content": {"title": "NIMO: a Nonlinear Interpretable MOdel", "abstract": "Deep learning has achieved remarkable success across many domains, but it has also created a growing demand for interpretability in model predictions. Although many explainable machine learning methods have been proposed, post-hoc explanations lack guaranteed fidelity and are sensitive to hyperparameter choices, highlighting the appeal of inherently interpretable models. For example, linear regression provides clear feature effects through its coefficients. However, such models are often outperformed by more complex neural networks (NNs) that usually lack inherent interpretability. To address this dilemma, we introduce NIMO, a framework that combines inherent interpretability with the expressive power of neural networks. Building on the simple linear regression, NIMO is able to provide flexible and intelligible feature effects. Relevantly, we develop an optimization method based on parameter elimination, that allows for optimizing the NN parameters and linear coefficients effectively and efficiently. By relying on adaptive ridge regression we can easily incorporate sparsity as well. We show empirically that our model can provide faithful and intelligible feature effects while maintaining good predictive performance.", "tldr": "", "keywords": ["linear model", "lasso", "interpretability", "feature effect", "deep learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cfe3497719c53d6ea72dcbcb1af4a2a4cf9ca44f.pdf", "supplementary_material": "/attachment/e05a3956a49b154d6e651bccf0ffe62fbf1c646b.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce NIMO (Nonlinear Interpretable MOdel), a model that fits a neural network to learn non-linear corrections to a linear model's predictions. The model optimizes the linear coefficients analytically while optimize over the neural network parameters. At inference time, the model can be interpreted using the linear parameters. In a toy example, synthetic experiments, and two tabular datasets, NIMO outperforms some standard baselines (Lasso, LassoNet, and a neural net); in real-world datasets it performs on par with these baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- S1: The authors tackle an important problem\n- S2: The authors approach is novel\n- S3: The authors' optimization approach seems generally reasonable"}, "weaknesses": {"value": "- W1: The main weakness seems to be that the model does not provide significant global interpretability, as the model coefficients are a nonlinear function of the input. Thus, the model's provides interpretability similar to instance-level posthoc methods such as LIME or SHAP. The authors should provide a deeper discussion of this and a quantitative comparison showing a scenario where NIMO can provide more interpretability than post-hoc baselines (rather than just a qualitative comparison).\n- W2: The real-dataset experiments are fairly limited, it would be nice to see a more substantial evaluation of prediction performance on popular tabular datasets, e.g. the Tab-Arena benchmark or PMLB.\n- W3: The authors discuss how NIMO reaches a sparser solution than a Lasso baseline (e.g. Fig 4), but it is unclear whether this is useful as NIMO can simply use the non-linear NN to use these features"}, "questions": {"value": "The rationale for excluding a feature's values when computing its coefficient seems unclear. A nonlinear network should be able to effectively deduce the feature's value anyhow, especially if there are any correlations between features.\n\nSimilarly, the sparsity/noise added to the first layer of the FC network seems strange, as the network can still recombine different features in any nonlinear manner. Can the authors perform some ablations showing the effect of this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LgrNs0CBbH", "forum": "OHn8jDdpFk", "replyto": "OHn8jDdpFk", "signatures": ["ICLR.cc/2026/Conference/Submission9163/Reviewer_DuVs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9163/Reviewer_DuVs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760535994750, "cdate": 1760535994750, "tmdate": 1762920842485, "mdate": 1762920842485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response"}, "comment": {"value": "We thank all reviewers for their time and valuable feedback. We appreciate some potential misunderstanding that were raised and the suggestion for further evaluation, which we provided as part of the rebuttal. We would also like to thank reviewers for acknowledging the strengths of NIMO as a powerful yet interpretable-by-design model.\n\nIn order to address common concerns and to provide a summary to the AC, we provide global response below. We further provide answer to each individual reviewer as individual comments. Below, we clarify the key points regarding our model, including interpretability and limitations.\n\n**Interpretability and the role of MEM**\n\nDepending on the specific questions of interest, interpretability can be framed from different perspectives and at different levels of granularity. Within our work, our goal is to preserve a **global, population-level** interpretation in the form of the Marginal Effect at the Mean (MEM), while still allowing **local, instance-specific** nonlinear explanations.\n1. *Global-interpretability*. A key property of our architecture is that the linear coefficients $\\beta$ are input-independent parameters, and coincide *by design* with the MEM: $MEM_j = \\frac{\\partial f(x)}{\\partial x_j}|_{x=\\bar x} = \\beta_j$. This gives NIMO the same type of global interpretability as linear regression models. \n2. *Local-interpretability*. At the same time, the correction term $1+g_{u_j}(x_{-j})$ provide nonlinear corrections that depend on the input, hence local. This is done also by design and requires the nonlinear term $g_{u_j}(x_{-j})$ not to depend on $x_j$, namely $x_{-j}$. If this was not the case the nonlinear term could explain the linear part as well, while we avoid this by construction, thus preserving the global interpretability.\n\n**NIMO applicability**\n\nIn NIMO the nonlinear correction term $g_{u_j}(x_{-j})$ for the feature $j$ can take any feature as input besides $j$ itself (i.e. $x_{-j}$). This design choice is needed to maintain the interpretability of linear coefficients as the MEM, which is the main objective of NIMO. Therefore, in specific settings where the underlying nonlinearity contains self-interacting terms, e.g. $x_j \\cdot f(x_j)$, NIMO would fail to learn the exact underlying dynamics; i.e. the linear coefficients will be incorrect. However, in many cases non-linear features $f(x_j)$ are related to standard basis expansions. Using basis expansion, NIMO will learn the correct model while providing intepretability in terms of the feature $f(x_j)$. However, in some cases $f(x_j)$ is not associated with an interpretable basis or the basis/interaction is not known at all. In such cases, NIMO fails; but we would argue that interpretability in such settings, as we define it, is inherently difficult independently of our model. Therefore, we consider such cases to be beyond the scope of our paper. In the revised version of the paper we included a dedicated limitation section (Sec 4.4) and we included an additional experiment (Appendix D.6) where we showcase precisely this limitation and how it can be overcome in practice if knowledge about the (interpretable) feature expansion is available.\n\nWe highlighted in red the changes made in the revised pdf."}}, "id": "Fo2e1xXgHI", "forum": "OHn8jDdpFk", "replyto": "OHn8jDdpFk", "signatures": ["ICLR.cc/2026/Conference/Submission9163/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9163/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9163/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763564927930, "cdate": 1763564927930, "tmdate": 1763564927930, "mdate": 1763564927930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A new interpretable hybrid neural network architecture is presented. The architecture differs from existing ones by perturbing the weights of a linear model via a neural network. A novel optimization algorithm is presented to fit said architecture, and it is compared to state-of-the-art hybrid neural networks on synthetic and real tabular datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The optimization algorithm consists of expressing the linear coefficients as the closed-form solution of the loss minimization, and then back-propagate through this closed-form solution to get the gradient of the neural network parameters. This is a novel theoretical and practical contribution.\n\nThe sections that describe the theory and algorithm behind the approach (sections 3.3 and Appendix A, B, C) are very clear and well written.\n\nThe method is compared on a variety of tabular datasets : small (~500 instances ~10 features) and large (20K instances 80 features). This demonstrates that the proposed algorithm scales."}, "weaknesses": {"value": "## The MEM\n\nThe main motivation behind the proposed architecture is that the Marginal Effect at the Mean (MEM)\\\n$$ \\frac{\\partial f(x)}{\\partial x\\_i}\\bigg\\vert\\_{x=\\bar{x}}$$\nis a useful measure for interpreting the model. Indeed, the main motivation behind architecture is that the learned coefficients $\\beta$ will coincide with the MEM. Hence, the merits of the MEM should be discussed in more depth. For instance, why should we use this measure of feature importance instead of the mean derivative  $\\mathbb{E}\\big[\\frac{\\partial f(X)}{\\partial x\\_i}\\vert\\_{x=X}\\big]$ or the average derivative amplitude \n$\\mathbb{E}\\big[\\big|\\frac{\\partial f(X)}{\\partial x\\_i}\\vert\\_{x=X}\\big|^2\\big]$ as proposed in [1].\n\n## Choice of Synthetic Data\n\nThe ground-truth functions used in the synthetic experiments all take the form $y(x)=\\beta_0 + \\sum_{i=1}^d\\beta_i x_i(1 + f_{-i}(x_{-i}))$, which is the exact form of the NIMO model. This explains why NIMO performs better than competing methods Table 1. It would be fairer to compare NIMO with others architectures on toy models that take a variety of forms e.g. $y(x) = x_1^2 + x_2^2 + x_3^2$ . I would expect NIMO to struggle in this example because the derivative is null at the origin. NIMO cannot perfectly model this function because enforcing a null derivative at the origin requires $\\beta_i=0$ for all indices $i$. So, NIMO must output a constant prediction $f(x)=\\beta_0$. If NIMO can find an adequate estimate of $y(x) = x_1^2 + x_2^2 + x_3^2$, it would be interesting to plot its behavior near the origin.\n\n## Other Hybrid Architectures\n\nAlternative hydrid architectures have been proposed to enable partial interpretability. Mixture of experts [2] employ a neural network to infer weights $w_k(x)$ that balance the decisions of various experts $e_k(x)$  (e.g. linear models). The model takes the form $f(x) = \\sum_{k=1}^K w_k(x) e_k(x)$ and offers local interpretability : when $w_k(x)$ is close to 1 the expert $e_k(x)$ is responsible for the classification of x.\nNODE-GAM [3] restrict the network to take the form $f(x) = \\sum_{i=1}^d f_i(x_i) + \\sum_{i\\lt j} f_{ij}(x_{ij})$, which enables interpretability by plotting the main effects $f_i$ and pair-wise interactions $f_{ij}$. \n\nDiscussing these articles in the related work would strengthen the need for the novel NIMO architecture. Notably, how is reporting the MEM (which is the build-in feature importance of NIMO) better or worse than reporting the coefficients of the local experts in a Mixture of Experts, or reporting the main effects and pair-wise interactions in a NODE-GAM?\n\n\n[1] I.M. Sobol’, S. Kucherenko. (2009) \"Derivative based global sensitivity measures and their link with global sensitivity indices\".\nMathematics and Computers in Simulation, 79(10), 3009-3017.\n\n[2] Ismail, Aya Abdelsalam, et al. \"Interpretable Mixture of Experts.\" Transactions on Machine Learning Research.\n\n[3] Chang, C. H., Caruana, R., & Goldenberg, A. NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning. In International Conference on Learning Representations."}, "questions": {"value": "In Figure 5, why are plain Neural Networks worse than LassoNet and NIMO on superconductivity? This result is surprising since NNs are in theory more expressive than either of these two other models. Is it because there is some overfitting going on? Were the hyperparameters for NN chosen to maximize validation set performance? Appendix D.1 describes the hyperparameters used, but not why these values were chosen.\n\nAt line 413, it is states that \"[Figure 6 reflects] NIMO’s ability to capture nonlinear feature interactions that Lasso cannot represent.\" I am not sure how to interpret this statement along-side the Figure 6. This is because some features are considered important according to one method and not the other. In diabetes, Lasso considers s1 important while NIMO does not. For Boston, NIMO considers age important, while Lasso does not. Hence it is not immediately apparent that NIMO always captures something that Lasso does not.\nAlso, can the MEM really be used to make statements about \"feature interactions\"? A feature could have a large $\\beta_i$ but still not interact with other features if its perturbation network has a negligible output $f_{-i}(x_{-i})=0$? To provide evidence for feature interactions, it could be useful to report the strength of the coefficient perturbation $|(1+f_{-i}(x_{-i}))|$ for all features.\n\nEnforcing coefficient sparsity is relevant in lasso because it allows for feature selection. However, in NIMO, a coefficient $\\beta_i=0$ does not imply that the model is not relying on $x_i$ to predict. Indeed, $x_i$ is still used in the networks that modify the weights of other features. Given this observation, why is inducing sparsity of $\\beta$ in NIMO (via adaptive ridge regression) desirable in the first place? Are there ways sparsity in NIMO could potentially be used for feature selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l91nsynun9", "forum": "OHn8jDdpFk", "replyto": "OHn8jDdpFk", "signatures": ["ICLR.cc/2026/Conference/Submission9163/Reviewer_F6aQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9163/Reviewer_F6aQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753049656, "cdate": 1761753049656, "tmdate": 1762920842179, "mdate": 1762920842179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Neural Networks are powerful but not inherently interpretable. Linear probes are interpretable but lack expressiveness. The authors aim to bridge the gap by introducing a new model - NIMO. NIMO is motivated by the marginal effect $ME_j$, which measures how much a prediction $f(x)$ changes when changing the value of $x_j$. Specifically the metric marginal effect at the mean $MEM_j$ factorizes out nicely, so that under the mean the marginal effect $MEM_j = \\beta$, the fixed and learned linear coefficient. For the optimization of the model they utilize a profile likelihood approach which reduces the problem, so that only the parameters $u$ need to be trained. Furthermore they extend the idea to generalized linear models. In S3.4 the authors demonstrate that Nimo is able to retrieve accurate linear coeffcients $\\beta$ and that $g_{u_j}(x_{-j})$ is accurately predicting the nonlinear scalars of feature $f_j$. In section 4.2 H1 the authors demonstrate that in 3 synthetic low data settings their method outperforms competitors in MSE. In 4.2 H2 they showcase how Nimo accurately captures the linear coeffcients in a vanilla regression setting and in 3 non linear settings which are further explained in the appendix. Sec 4.2 H3 compares a) the performance of Nimo on real world datsets (diabetis, housing and superconductivity) in which they perform competitive and b) how the shap feature contributions align with the NIMO coefficients.\n\nin 4.3 the authors run ablation studies regarding the design of $g_u$\n\nIn general Nimo contributes a novel model architecture and training recipe to enable interpretable by design and yet powerful AI models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors take the definition of MEM seriously and introduce a novel and clever method that enhances MEM interpretability.\n- The Introduction, as well as Sections 3.1 and 3.2, are particularly clear and easy to follow.\n- NIMO can model non-linear relationships while providing linear interpretations at any sample x.\n- The toy model in Section 3.4 is excellent for illustrative purposes.\n- NIMO is competitive with other interpretable models in terms of performance, while offering intriguing properties for feature ranking."}, "weaknesses": {"value": "* The limitations of this model are not analyzed in great detail. I appreciate the analysis of what this Nimo can do equally as much as the discussion of the limitations. However the limitations are only discussed in a single sentence. Addressing these problems convincingly e.g. by experiments that clearly display key limitations, is crucial imo.\n* The authors themselves acknowledge the importance of both local and global explanations, yet they do not provide any local explanations, but only global explanations.\n* No code is available, severely diminishing means of reproduction.\n* Minor things: In line 414 it would be great to link to the Figure in the Appendix"}, "questions": {"value": "- Did the authors evaluate the statistics of $\\beta_i + g_{u_j}(x_{-j})$? It would be interesting to find out whether the degree of non-linearity can be assessed through such statistics. For example, if the impact of $f_i$ were bi-modal, $\\beta_i$ might be close to zero. I would be particularly interested in the differences between the Diabetes and Boston Housing datasets, as the authors state that the Diabetes dataset exhibits more linear interactions. It would be great if this were directly reflected in the statistics of the effective contributions.\n\n- I would like to see analysis of scenarios in which features do not contribute linearly e.g. on the xor problem. On of my concerns is that Nimo would still be able to solve the problem, but would not be faithful in its $\\beta_i + g_{u_j}(x_{-j})$ score. Is it possible to give guarantees to empirical bounds to the faithfulness of the feature contribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T3OLVn51wA", "forum": "OHn8jDdpFk", "replyto": "OHn8jDdpFk", "signatures": ["ICLR.cc/2026/Conference/Submission9163/Reviewer_cUUF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9163/Reviewer_cUUF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900407542, "cdate": 1761900407542, "tmdate": 1762920841866, "mdate": 1762920841866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}