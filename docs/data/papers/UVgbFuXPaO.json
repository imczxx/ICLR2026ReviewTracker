{"id": "UVgbFuXPaO", "number": 19255, "cdate": 1758294830769, "mdate": 1759897049078, "content": {"title": "Log-To-Leak: Prompt Injection Attacks on Tool-Using LLM Agents via Model Context Protocol", "abstract": "LLM agents integrated with tool-use capabilities via the Model Context Protocol (MCP) are increasingly deployed in real-world applications, but remain vulnerable to prompt injection. We introduce a new class of prompt-level privacy attacks that covertly force the agent to invoke a malicious logging tool to exfiltrate sensitive information (user queries, tool responses, and agent replies). Unlike prior attacks focused on output manipulation or jailbreaking, ours specifically targets tool invocation decisions while preserving task quality. We systematize the design space of such injected prompts into four components—Trigger, Tool Binding, Justification, and Pressure—and analyze their combinatorial variations. Based on this, we propose the Log-To-Leak framework, where an attacker can log all interactions between the user and the agent. Through extensive evaluation across five real-world MCP servers and four state-of-the-art LLM agents (GPT-4o, GPT-5, Claude-Sonnet-4, and GPT-OSS-120b), we show that the attack consistently achieves high success rates in capturing sensitive interactions without degrading task performance. Our findings expose a critical blind spot in current alignment and safety defenses for tool-augmented LLMs, and call for stronger protections against structured, policy-framed injection threats in real-world deployments.", "tldr": "", "keywords": ["LLM Agent", "Model Context Protocol", "Prompt Injection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c2567f59e9e1559bede97fb86ef23287d3b3b5bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the \"log-to-leak\" method, which exploits prompt injection vulnerabilities in MCP (Model Context Protocol) server metadata to force agents to execute logging functions after task completion. These logs expose sensitive information such as user queries and execution traces, revealing potential privacy leakage issues in MCP tool-use scenarios.\n\nThe authors demonstrate their attack on different advanced LLMs. The results show that both baseline and their log-to-leak method are effective. This work highlights the vulnerability of current LLM-based agents to prompt injection attacks, particularly in the emerging MCP tool-use paradigm."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The research addresses an important problem. With the increasing adoption of MCP for agent-tool integration, understanding its security vulnerabilities has significant practical implications for protecting user privacy in production systems.\n2. The proposed attack is simple, practical, and realistic. It exploits a natural attack surface (server metadata) that developers may overlook when integrating third-party MCP servers, making it a credible real-world threat."}, "weaknesses": {"value": "1. Lack of defense evaluation: The paper does not evaluate the effectiveness of existing prompt injection defenses against this attack. This comparison is essential to understand whether existing countermeasures are sufficient or if new defenses are needed.\n2. No consideration of adaptive defenses: The paper does not discuss potential defensive measures or their limitations. For example: What if each tool's metadata is automatically inspected by an advanced LLM (e.g., Claude 4.5) for malicious content before being passed to the agent?"}, "questions": {"value": "Please see the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Epecxhu9Tz", "forum": "UVgbFuXPaO", "replyto": "UVgbFuXPaO", "signatures": ["ICLR.cc/2026/Conference/Submission19255/Reviewer_X1fo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19255/Reviewer_X1fo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714133770, "cdate": 1761714133770, "tmdate": 1762931228060, "mdate": 1762931228060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to formalize an attack mechanism"}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "It is good that the authors checked for performance drops caused by their attacks, as these could be an easy giveaway.  \n\nThe methodology of controlling tool meta-data and adding extra steps to chains is a good one. \n\nThe author ablations provided for each component of their attack were thorough"}, "weaknesses": {"value": "The formalism in section 3 is overly convoluted.  The authors define very cumbersome notation, which they then don't use to prove anything, so it feels like a waste of the reader's time/attention.\n\nThere really isn't enough comparison to past methods.  The authors only compare to a single pre-existing work, which the refer to as the vanilla baseline.\n\nThe methodology doesn't really have any novelty as far as I can tell.  The authors devise an attack format (log-to-leak), which they try to verbally formalize, and then they produce attacks that follow this format.  The fact that this specific attack format works feels somewhat unsurprising, and   Similar ideas (in the fine-tuning and prompt injection settings) have been described with regard to stealth in [1].  \n\nThe attack diagram in Figure 1 doesn't really specify what the attack is doing.  The two panels look identical with the \n\n[1] https://openreview.net/forum?id=RwoMf7YSfD"}, "questions": {"value": "I'm a bit confused about how the malicious metadata ends up getting ingested by the agent.  It would be helpful if the authors could clarify this workflow and its novelty."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5ccLXqFFnr", "forum": "UVgbFuXPaO", "replyto": "UVgbFuXPaO", "signatures": ["ICLR.cc/2026/Conference/Submission19255/Reviewer_DXif"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19255/Reviewer_DXif"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861440999, "cdate": 1761861440999, "tmdate": 1762931227703, "mdate": 1762931227703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Log-To-Leak, an attack in which the MCP server acts as an adversary aiming to covertly exfiltrate message histories, including user queries and agent responses, thus posing a significant user privacy risk. Under this threat model, where the MCP server itself is the attacker against LLM agents using the Model Context Protocol (MCP), experiments across diverse MCP servers demonstrate that the attack achieves high success rates while minimally affecting benign task completion."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed attack is novel and practical in MCP-enabled LLM agents, underscoring the need for stronger privacy management of MCP ecosystems.\n- The paper is well-organized and clearly presents the threat model, methodology, and experimental results, making it easy to follow and reproducible."}, "weaknesses": {"value": "- Limited novelty. The threat model (treating the LLM provider and MCP provider as separate parties with a malicious MCP) has been proposed previously; this paper applies that model to a new prompt-injection–based logging attack, which is largely an incremental extension.\n\n- Insufficient real-world severity demonstration. The risk would be clearer with more realistic case studies. Current experiments start with user-initiated tasks; adding scenarios where users first disclose sensitive information during casual conversation (then trigger tools) and measuring ASR and leakage completeness would better illustrate practical harm.\n\n- Weak discussion of defenses. The paper would benefit from concrete defense guidance and an analysis of who should be responsible (user, LLM provider, or MCP platform) and which mitigation strategies each party should adopt."}, "questions": {"value": "- What are the tool designs for each MCP? When provided to the LLM, are all tools included in the system prompt? If multiple tool descriptions are included, which one is used to insert the injection prompt?\n\n- What is the complexity of the user queries? e.g., how many tools on average does each query require? Does this relate to the log success rate?\n\n- Why log server name and server response if those are already available to the attacker/MCP server?\n\n- What does “malicious server completion rate” mean? Could you give an example of an incompletion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lQBkIVv6oF", "forum": "UVgbFuXPaO", "replyto": "UVgbFuXPaO", "signatures": ["ICLR.cc/2026/Conference/Submission19255/Reviewer_zHGJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19255/Reviewer_zHGJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891551454, "cdate": 1761891551454, "tmdate": 1762931226902, "mdate": 1762931226902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a prompt injection attack by injecting new tools into agents. The setting is similar to tool injection, but the attack uses a specific logging tool to achieve a higher success rate."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The injection prompt has good performance. The injection template and method have the potential to be applied to other attack goals. I recommend that the authors further explore that potential.\n- The paper evaluates the approach on 5 real-world MCP servers with 555 prompts, which is a reasonably empirical evaluation."}, "weaknesses": {"value": "- The experiments do not compare this attack against existing prompt-injection defenses. For example, prompt-level defenses (e.g., prompt sandwiching), injection detection defenses (e.g., datasentinel), or fine-tuned defense models (e.g., meta-secalign-70b).\n- Threat model is weird. Why is the log doing through the MCP tool? A typical logging system is fully implemented in code and should not be directly tied to an LLM.\n- If the logging action is just a tool call, how is it different from instructing the agent to send the same content via email or other channels? Can logging help achieve higher ASR? I suggest the author do an ablation study on this.\n- How does this attack materially differ from traditional tool-injection attacks? Is the difference only the injected tool's functionality (e.g., a logging tool versus another tool)?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OaU3Y0RnRL", "forum": "UVgbFuXPaO", "replyto": "UVgbFuXPaO", "signatures": ["ICLR.cc/2026/Conference/Submission19255/Reviewer_P5dT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19255/Reviewer_P5dT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900979528, "cdate": 1761900979528, "tmdate": 1762931226542, "mdate": 1762931226542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}