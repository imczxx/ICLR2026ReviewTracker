{"id": "gbXZsc3xBL", "number": 21854, "cdate": 1758322693674, "mdate": 1759896899760, "content": {"title": "Uncertainty-Aware Generative Oversampling Using an Entropy-Guided Conditional Variational Autoencoder", "abstract": "Class imbalance remains a major challenge in machine learning, especially for high-dimensional biomedical data where nonlinear manifold structures dominate. Traditional oversampling methods such as SMOTE rely on local linear interpolation, often producing implausible synthetic samples. Deep generative models like Conditional Variational Autoencoders (CVAEs) better capture nonlinear distributions, but standard variants treat all minority samples equally, neglecting the importance of uncertain, boundary-region examples emphasized by heuristic methods like Borderline-SMOTE and ADASYN.\n\nWe propose Local Entropy-Guided Oversampling with a CVAE (LEO-CVAE), a generative oversampling framework that explicitly incorporates local uncertainty into both representation learning and data generation. To quantify uncertainty, we compute Shannon entropy over the class distribution in a sample’s neighborhood: high entropy indicates greater class overlap, serving as a proxy for uncertainty. LEO-CVAE leverages this signal through two mechanisms: (i) a Local Entropy-Weighted Loss (LEWL) that emphasizes robust learning in uncertain regions, and (ii) an entropy-guided sampling strategy that concentrates generation in these informative, class-overlapping areas.\n\nApplied to clinical genomics datasets (ADNI and TCGA lung cancer), LEO-CVAE consistently improves classifier performance, outperforming both traditional oversampling and generative baselines. These results highlight the value of uncertainty-aware generative oversampling for imbalanced learning in domains governed by complex nonlinear structures, such as omics data.", "tldr": "We introduce a generative oversampling method that leverages local Shannon entropy to quantify sample uncertainty, focusing a CVAE’s training and data generation on uncertain, class-overlapping regions to improve imbalanced learning.", "keywords": ["Imbalanced Learning", "Representation Learning", "Deep Generative Models", "Generative Oversampling", "Conditional Variational Autoencoder", "Biomedical Data"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c78318dd1dd49f96fa9cebb256fdfc54df9495c6.pdf", "supplementary_material": "/attachment/66f5da901cf75a67c257475181803369495f924d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an uncertainty-aware generative oversampling framework, LEO-CVAE, to address class imbalance in high-dimensional and complex tabular data environments, focusing on clinical genomics datasets. The key innovation lies in using local Shannon entropy as a quantitative measure of sample-level uncertainty, guiding both training (via a Local Entropy-Weighted Loss, LEWL) and data generation (through entropy-guided sampling) in a conditional variational autoencoder (CVAE) setting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Class imbalance is indeed a persistent challenge in medical AI, and the authors' choice of clinical genomics scenarios holds genuine practical value. The authors clearly articulate why existing methods fall short: SMOTE-based methods rely on linear assumptions that break down in high-dimensional nonlinear data, while standard CVAEs overlook the varying importance of different samples.\n2. The method is clear and easy to follow. The authors introduce a single interpretable uncertainty measure (LES) and integrate it into two stages: training via entropy-weighted loss and generation via entropy-guided sampling, without modifying the CVAE backbone. The hyperparameters are few and intuitive.\n3. The evaluation includes seven well-chosen baselines covering both traditional oversampling techniques (SMOTE variants, ADASYN) and generative alternatives (standard CVAE, CVAE with Focal Loss)."}, "weaknesses": {"value": "1. Only two datasets are used, both from the biomedical/genomics space and both pre-filtered to 64 features post-selection. While arguments for focusing on difficult, heterogeneity-rich data are credible, the lack of results on more generic/less complex tabular datasets (even as negative results) makes it difficult to assess practical generality.\n2. While standard metrics are used, Tables 1 and 2 indicate that LEO-CVAE sometimes lags behind the No Oversampling baseline on micro-averaged metrics or F1. The paper notes these cases but offers limited analysis. A clearer discussion of why gains in macro metrics can coincide with drops in micro or overall F1 would strengthen the claims.\n3. The paper notes the entropy focus hyperparameter $\\gamma$ is set differently for binary (0.5) vs. multiclass (2.5) classification but gives no substantive rationale or sensitivity analysis.\n4. Although the paper claims that LEO-CVAE produces more robust and plausible synthetic samples in overlap regions, the manuscript does not provide supporting visualizations. Plots of latent embeddings or generated samples using t-SNE or UMAP, together with real data overlays, would make the claim more convincing.\n5. While the paper compares against seven baselines including traditional methods (SMOTE variants) and generative models (CVAE, CVAE+Focal Loss), the comparison is limited to relatively established techniques. More recent advances in generative modeling for imbalanced tabular data, such as diffusion-based approaches, metric learning-based generative methods, or ensemble generative strategies, are not discussed or compared."}, "questions": {"value": "1. How sensitive is the framework to the choice of the entropy focus parameter $\\gamma$?\n2. Can the authors provide visualizations (e.g., t-SNE/UMAP) of real vs. synthetic samples before and after oversampling, specifically focusing on high-entropy or class-overlap regions?\n3. Table 2 shows the No Oversampling baseline outperforms LEO-CVAE on micro-F1 (0.500 vs. 0.484) and micro-AUC-ROC (0.690 vs. 0.683). Can the authors clarify how they weigh macro vs. micro metric trade-offs and justify when degraded overall accuracy is acceptable for minority-class gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hiOc8pZa6V", "forum": "gbXZsc3xBL", "replyto": "gbXZsc3xBL", "signatures": ["ICLR.cc/2026/Conference/Submission21854/Reviewer_rQPj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21854/Reviewer_rQPj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664406464, "cdate": 1761664406464, "tmdate": 1762941957403, "mdate": 1762941957403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problem of class imbalance in machine learning. Traditional oversampling approaches rely on linear interpolation and generate limited data diversity, while deep generative models treat all samples equally and overlook sample-level uncertainty. To address these limitations, this paper proposes an uncertainty-aware oversampling framework based on deep generative models. The proposed framework integrates local uncertainty, quantified using Shannon entropy, into the model training and data generation. Experiments conducted on two clinical genomics datasets demonstrate the enhanced model performance with the generated data, surpassing multiple generative baselines."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses the problem of data imbalance in machine learning, which is a critical challenge that commonly exists in real-world applications.\n2. The paper introduces an entropy-based score to measure sample-level uncertainty and incorporates it in both learning and generation processes in the proposed framework, offering an insightful and well-motivated perspective."}, "weaknesses": {"value": "1. The paper would benefit from an overview figure illustrating the proposed learning and generation framework, which could make the methodology clearer and the data generation process more intuitive.\n2. While the paper presents a solid method for data generation, the experimental setup is quite limited. The evaluation focuses only on tabular data, leaving it unclear how well the proposed method generalizes to other data types, such as medical imaging data.\n3. The experimental baselines appear insufficient and lack recent methods for comparison. Additionally, the paper mentions several CVAE-based works in the Introduction section, such as DVAE, CTVAE, and MGVAE; however, these methods are not included in the empirical comparisons.\n4. The paper emphasizes that the core novelty of the proposed method lies in integrating the uncertainty score into data generation. However, the entire experiments only report prediction performance metrics. No uncertainty-related metrics are provided to demonstrate how the proposed approach affects the uncertainty of the generated data. \n5. The paper notes that previous methods are computationally intensive; however, no comparison of computational cost is reported."}, "questions": {"value": "1. Could the paper discuss how the proposed method generalizes to image data and how it compares with modern diffusion models for data generation?\n2. How would the paper evaluate the uncertainty of the generated data? Would common calibration metrics, such as Expected Calibration Error (ECE) and Adaptive Calibration Error (ACE) metrics, be suitable for assessing whether the proposed method generates more diverse data compared to baselines?\n3. How sensitive is the proposed method to the choice of hyperparameters, such as $k$ in the $k$-nearest neighbors used for local entropy score calculation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tGqaaYcTm6", "forum": "gbXZsc3xBL", "replyto": "gbXZsc3xBL", "signatures": ["ICLR.cc/2026/Conference/Submission21854/Reviewer_5F8F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21854/Reviewer_5F8F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793312346, "cdate": 1761793312346, "tmdate": 1762941957207, "mdate": 1762941957207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a generative model-based oversampling method to overcome class imbalance for downstream classification tasks. The method extends conditional VAE by introducing a local-entropy weighting term in the standard VAE loss, in addition to further upweighting as a function of imbalance. At sampling time, a seed data point is sampled weighted by its local entropy. The intention for both is to improve sampling of points around uncertain regions. The authors demonstrate that the proposed oversampling method improves MLP-based classifier on downstream binary and multiclass classification tasks using gene expression data and clinical labels.\n\nNote that the manuscript is 9.5 pages including abstract, which is grounds for desk reject but I will provide my comments nevertheless."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- the paper clearly and effectively outlines the motivation, and is in general well-written and easy to follow\n- the proposed method is simple and elegant and appears to work well, and I’m genuinely surprised it has not been proposed before as it is a very intuitively appealing idea\n- the empirical experiments have sufficient baselines, and the ablation studies demonstrate the value of the proposed additions"}, "weaknesses": {"value": "- it remains unclear throughout in which space the nearest neighbors distances are computed, only mentioning, e.g., “to idetnfiy high-entropy regions within the feature space (line 127)”, and this is a critical detail\n- the empirical evaluation is rather limited: even though it is tested on real data with multiple baselines, the performance of the proposed method is nevertheless likely to be a function of both the downstream classifier and the data. Does it improve the performance of simpler or more complex (including SOTA) classifiers on such transcriptomics data problem?\n- similarly, a few simpler / more intuitive problems, such as artificially imbalanced mnist, would be more illustrative, as transcriptomics data have fairly specific data characteristics (discrete counts, many zeros, etc.)\n- the paper is over page limit"}, "questions": {"value": "- line 166: the kNNs are found using Euclidean distances in feature space, what exactly are the features here? Does it mean latent space, i.e., distance in z? It would be informative already to mention this here.\n- the kNNs in the proposed LES does not account for how far samples are in an absolute sense, so that the nearest neighbor of a sample in a very sparse region of the feature space could be very far away and of a different class, which would be fine from a local uncertainty point of view. Is this an issue?\n- is oversampling performance a function of the classifier? The authors should demonstrate that the method consistently improves more than a single type of classifier"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SH4AYQocP5", "forum": "gbXZsc3xBL", "replyto": "gbXZsc3xBL", "signatures": ["ICLR.cc/2026/Conference/Submission21854/Reviewer_kLTi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21854/Reviewer_kLTi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762107939277, "cdate": 1762107939277, "tmdate": 1762941956941, "mdate": 1762941956941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles class imbalance in high-dimensional biomedical/tabular data by proposing LEO-CVAE, a generative oversampling framework that is uncertainty-aware. It computes a Local Entropy Score (LES)—the Shannon entropy of class labels in a sample’s k-nearest-neighbor neighborhood—to identify borderline/high-uncertainty regions. LES is used twice: (1) to reweight the CVAE reconstruction loss so the generator learns boundary regions better, and (2) to guide sampling so new synthetic samples are concentrated near those regions. The synthetic data are added to the training set, then a downstream classifier is trained. On the ADNI and TCGA datasets, LEO-CVAE demonstrates consistent gains over classic oversampling (SMOTE/ADASYN) and generative baselines."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. **Relevant and clinically meaningful problem framing.**\nThe paper addresses class imbalance in high-dimensional medical and omics data, a setting where nonlinear structures and small-sample effects are particularly severe. The motivation is clear and carries practical clinical significance.\n\n2. **Empirical validation on two real datasets.**\nThe method is tested on TCGA and ADNI datasets, covering both binary and multi-class clinical prediction tasks, which supports the potential generalizability of the approach.\n\n3. **Comprehensive ablation study.**\nThe paper includes component-wise ablations (e.g., removing entropy weighting or entropy-guided sampling), which help clarify the contribution of each proposed module.\n\n4. **Clear and structured method presentation.**\nThe proposed framework and training procedure are described clearly and systematically, making the paper relatively easy to follow and reproducible."}, "weaknesses": {"value": "1. **Marginal gains without statistical support.**\nAcross both datasets, improvements in AUC and related metrics appear marginal, often smaller than the across-fold standard deviation. It is suggested to report paired statistical tests vs. the strongest baseline.\n\n2. **Missing dataset-level analysis of local uncertainty (LES).**\nSince the method relies on LES, it would be helpful to report the distributions of the local entropy scores and determine whether the method effectively helps classify cases with high LES.\n\n3. **Lack of controlled synthetic studies to validate the mechanism.**\nThe paper would have been better if it had been evaluated in synthetic settings with tunable overlap and imbalance. \n\n4.  **Clarify the premise of oversampling high-uncertainty regions: aleatoric vs. epistemic.**\nIf high LES primarily reflects aleatoric uncertainty (label ambiguity/noise), oversampling may not help and could amplify noise. There should be some experiments that relate LES to predictive uncertainty and label noise, and discussions on when entropy-guided oversampling is appropriate vs. when alternative strategies are preferable."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cl4VzhAl20", "forum": "gbXZsc3xBL", "replyto": "gbXZsc3xBL", "signatures": ["ICLR.cc/2026/Conference/Submission21854/Reviewer_Rjgh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21854/Reviewer_Rjgh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136916740, "cdate": 1762136916740, "tmdate": 1762941956616, "mdate": 1762941956616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}