{"id": "VsLjOmwRaS", "number": 8818, "cdate": 1758099077507, "mdate": 1759897762119, "content": {"title": "Local and Unbalanced Optimal Transport for Feature Learning with Probabilistic Guarantees", "abstract": "This paper explores the local and unbalanced optimal transport for feature learning in an embedding space. Instead of using joint distributions of data, we introduce conditional distributions in terms of the\nKullback-Leibler (KL) divergence where some reference conditional distributions are utilized. Using conditional distributions provides the flexibility in controlling the transferring range of  given data points. When the  block coordinate descent method  is employed to solve our model,  it is interesting to find that conditional and marginal distributions have  closed-form solutions. Moreover, the use of conditional distributions facilitates the derivation of the generalization  bound of our model via the Rademacher complexity, which characterizes its convergence speed in terms of the number of samples. By optimizing  the anchors (centroids)  defined in the model, we also employ the unbalanced  optimal transport and  autoencoders to explore an embedding space of samples in the clustering problem. In the experimental part, we demonstrate that the proposed model achieves promising performance on some learning tasks. Moreover, we construct a local and unbalanced optimal transport classifier to classify set-valued objects.", "tldr": "", "keywords": ["optimal transport", "feature learning", "generalization bound"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ae78f80bf423aa0b4e03d90310795b698a8a1b90.pdf", "supplementary_material": "/attachment/d65cebe0f698f6842d3c68b0e4b4620408fa37fa.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a local and unbalanced optimal transport (OT) formulation for feature learning.   \nSpecifically, the original unbalanced OT framework is modified with the conditional distribution to form a new formulation. Then regularization terms are added to avoid trivial solution.   \nAs to the optimisation, alternative updating w.r.t. each set of parameters are designed for $p_{j|i}, p_i, $ and $\\theta, \\bar{\\theta}, \\phi, \\bar{\\phi}$. The first two sets are strong convex problems with closed-form solution, while the third one is nonconvex and be optimised with SGD.   \nTheoretical analysis gives the bound under the Rademacher complexity theory. \n\nExperiments are conducted on supervised learning, clustering, and set-valued objects. Several baselins are compared, with the proposed LUOP and LUOP-L performing better."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "## Strengths\n- Conditional distribution is incorporated into the unbalanced optimal transport formulation to derive a new local and unbalanced OT framework. Alternative updating is proposed to solve the new problem. \n- Rademacher complexity theory is adopted to derive two bounds for the proposed formulation. \n- Experiments are conducted on supervised learning, clustering, and set-valued objects. With several baseline methods compared, the proposed achieves better performance."}, "weaknesses": {"value": "## Weaknesses\n- The main technical contribution is the introduction of conditional distributions, which transforms Eq. (1) to Eq. (2), and removing the third term. From the OT perspective, this contribution is incremental based on unbalanced OT framework.   \nRegularization terms in Eq. (4) are just common practice in feature learning, and cannot be regarded as new contribution in the context of feature learning. \n- The optimisation process is alternative optimising (coordinate descent), which is commonly-used as the feasible solution. It requires alternating between variables. \n- Although supervised learning, clustering, and set-valued objects tasks are conducted in this paper, the baseline and comparison methods are relatively out-dated and not strong/relevant opponents to the proposed LUOP method.   \n> All baselines are earlier than 2022.   \n> No unbalanced OT or local OT variants are included in the comparison or formulation discussion, e.g., [R1-R4], to name a few.   \n> No task-specific SOTA methods are compared in each section. \n- Related work only appears in Appendix and is very short, lacking necessary local/unbalanced OT formulations and variants. \n\n[R1] Thual, Alexis, et al. \"Aligning individual brains with fused unbalanced Gromov Wasserstein.\" Advances in neural information processing systems 35 (2022): 21792-21804.\n\n[R2] Chizat, Lenaic, et al. \"Scaling algorithms for unbalanced optimal transport problems.\" Mathematics of computation 87.314 (2018): 2563-2609.\n\n[R3] Séjourné, Thibault, François-Xavier Vialard, and Gabriel Peyré. \"The unbalanced gromov wasserstein distance: Conic formulation and relaxation.\" Advances in Neural Information Processing Systems 34 (2021): 8766-8779.\n\n[R4] Benamou, Jean-David, et al. \"Iterative Bregman projections for regularized transportation problems.\" SIAM Journal on Scientific Computing 37.2 (2015): A1111-A1138."}, "questions": {"value": "Are there any recent local/unbalanced OT framework most relevant to the proposed method? \n\nAre there any SOTA methods for each of the task experimented, i.e., supervised learning, clustering, and set-valued objects? \n\nEq. (1) has two KL regularization terms, while the proposed Eq. (2) only has one. What is the consideration and intuition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sDkDiaFlQA", "forum": "VsLjOmwRaS", "replyto": "VsLjOmwRaS", "signatures": ["ICLR.cc/2026/Conference/Submission8818/Reviewer_QDkJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8818/Reviewer_QDkJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711504948, "cdate": 1761711504948, "tmdate": 1762920587139, "mdate": 1762920587139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a feature-learning approach based on conditional and unbalanced optimal transport. It introduces conditional distributions in terms of the Kullback-Leibler divergence, claims closed-form updates under block coordinate descent, and adds anchors and an autoencoder to learn an embedding for clustering and classification of set-valued objects. Generalization bounds via Rademacher complexity are provided."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The approach is evaluated on diverse tasks and datasets."}, "weaknesses": {"value": "- The problem definition of feature learning is unclear. The paper uses “feature learning” interchangeably with representation learning, dimensionality reduction, metric learning, and clustering.\n- Section 2.2 is overcomplicated and hard-to-follow formulation. It is lengthy, mixes motivations with derivations, and introduces Eq. (4) in a way that seems unnecessarily complex for the same-source setting.\n- The paper reads like a technical report without providing the motivation of why each component is considered plus the overcomplicated notation does not help. The paper explains what is added (conditionals, anchors, autoencoder) but not why each piece is needed and which phenomenon it addresses.\n- Theorems 1–2 are presented without explaining why they are interesting or what the bounds imply practically. The proof in the in appendix is hard to follow. What is the interpretation? Could you provide a pointer to the lines in the proof where key ideas occur? Is there a sanity-check experiment showing the bound’s qualitative predictions?\n- There are many proposed components in the model, but there is no ablation study. Unclear how each component contributes to the final performance\n- Citing very recent or narrow works as the first mention of core notions (OT, unbalanced OT, KL, f-divergence) is misleading:\n    - Intro/lines 83–84, 117: Use standard OT and UOT references when first introducing them; recent applied papers can be cited later for variants.\n    - Line 121 (KL) and line 157 (f-divergence): Cite canonical sources for definitions, not recent downstream applications.\n- Considering that various neighborhoods are explored in the graph community, but there is no comparison or reference to them.\n- The hyperlinks to references, figures, and tables are broken\n- The autoencoder appears with no reference or reason for why latent-space OT is preferable to data-space OT here.\n- The autoencoder is first mentioned with no reference\n- “Alternating optimization” is listed as a contribution, but this is a standard technique. Perhaps the authors could clarify what is novel (a provably convergent block structure? closed-form updates?).\n- Lines 213–215 (Eq. 5): The strong convexity claim and closed-form derivation need either an inline proof sketch or an explicit pointer to a lemma. Lines 222–227 (Eq. 6): Same issue.\n- In lines 252-253: I do not see how p_{j|i} or p_i is defined in subsection 3.2\n- It is not clear what the key differences are between the proposed method and existing work, eg, Wasserstein Discriminant Analysis or kernel-based Discriminant Analysis.\n- in line 27: missing a period in the sentence “.. to classify set-valued objects”\n- Notation abuse: X and Y were used as spaces in Section 1, but then later were defined as random vectors in Section 2.1\n- In Eq. (1), p_{ij}, U(a,b),p and pe_k  are not defined\n- In line 138, the Wasserstein space is not defined\n- in line 141, the Wasserstein distance  $\\\\bar{W}\\_i^{\\\\bar{r}}$ is minimized over a scaler p_{j,i}?\n- Why Eq.(2) is considered for the use of a special case of the unbalanced optimal transport for effectively learn p_{j|i} in the conditional distribution\n- The used notations are very complex and hard to follow. Perhaps the authors could include a table of all the notation.\n- The appendix is referred to without a pointer - hard to navigate what to look for and point to in the relevant context"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lj5EZV6GOE", "forum": "VsLjOmwRaS", "replyto": "VsLjOmwRaS", "signatures": ["ICLR.cc/2026/Conference/Submission8818/Reviewer_rJxC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8818/Reviewer_rJxC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764551084, "cdate": 1761764551084, "tmdate": 1762920586593, "mdate": 1762920586593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for feature learning that leverages Local and Unbalanced Optimal Transport (OT) within an autoencoder architecture. The core innovation is replacing the standard joint probability measure of Optimal Transport with conditional distributions. For each data point $x_i$, a conditional measure $p\\left(y \\mid x_i\\right)$ is constructed on a set of its local/relevant neighbors $\\boldsymbol{y}_{j \\mid i}$ in the target space. \n\n\nThe objective function is a generalized loss that combines: An unbalanced OT cost between the Dirac measure $\\delta_{f_\\theta\\left(x_i\\right)}$ and the conditional measure $p\\left(g_\\phi(y) \\mid x_i\\right)$ in the embedding space and Reconstruction errors for the autoencoders, to prevent trivial solutions.\n\n\nThe model is optimized via an alternating optimization technique (Block Coordinate Descent) with closed-form solutions for the conditional ( $p_{j \\mid i}$ ) and marginal ( $p_i$ ) probability vectors. Furthermore, the paper provides a generalization bound for the model based on Rademacher complexity. The framework is empirically validated on classification and clustering tasks, and is extended to a classifier for set-valued objects."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's central claims are well-supported. The theoretical formulation introduces an objective function that balances OT cost, KL-divergence regularization, and autoencoder reconstruction. The derivation of a generalization bound via Rademacher complexity (Theorem 1) further strengthens the theoretical foundation by offering probabilistic guarantees for the feature learning process. The experimental setup is comprehensive, covering standard classification and clustering, as well as an application to set-valued object classification.\n\n\nThe paper is generally well-written and clear. The problem formulation is logically built, starting from preliminaries on Optimal Transport and then introducing the local, conditional, and unbalanced components. The figures and equations are helpful. \n\n\nThis work makes a good contribution to the field of feature learning via Optimal Transport.\n\n\nThe resulting loss formulation is a novel combination of concepts from autoencoders and Optimal Transport. The model's extension to classifying set-valued objects (LUOPC) suggests a broader applicability for the proposed local optimal transport framework beyond standard feature learning tasks.\n\n- The algorithm is well-derived, with the outcome being the closed-form, analytic solutions for the conditional and marginal distributions ( $p_{{j} {i}}, {p}_{{i}}$ ) during the alternating optimization. Furthermore, the provision of a generalization bound via Rademacher complexity anchors the method in rigorous theoretical analysis.\n\n\n- The framework is generic and successfully applied to multiple learning tasks (classification, clustering) and a new problem (set-valued classification), showing its practical utility and broad potential in feature representation."}, "weaknesses": {"value": "- The primary weakness of this paper lies in the lack of a comprehensive literature review on works that use conditional optimal transport, despite the paper's main contribution being the description of local information of data points via optimal transport. For instance:\n[1] Manupriya, Piyushi, et al. \"Consistent optimal transport with empirical conditional measures.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2024.\nAdditionally, since the authors primarily use unbalanced optimal transport (OT), a discussion of relevant literature on this approach would have been beneficial.\n\n\n- Some findings are trivial, such as in lines 252-254, where it is quite evident that by increasing the weight $\\lambda$ of the $\\lambda K L(p \\| q)$ term to infinity and minimizing with respect to $p$, it forces $p$ to equal $q$.\n\n\n- In formulation (1), which appears to be in the unbalanced OT form, when applying the KL penalty to the two marginals, the constraint $p_{i j} \\in U(a, b)$ should be relaxed. If not, the penalty becomes nonsensical.\n\n\n- The formulation includes several hyperparameters $\\left(\\lambda_{1 \\rightarrow 4}\\right)$, and the paper should include a discussion on the ablation study regarding their effects.\n\n\n- The authors should elaborate more on the convergence of Algorithm 1, particularly in terms of the block coordinate descent method, rather than simply mentioning it in a single sentence. There are many examples where applying block coordinate descent does not converge to a stationary point, which warrants further clarification."}, "questions": {"value": "Beside some **concerns in the Weakness that can be taken as relevant questions well**, there is one more question for the authors about the choice of prior $q_{j \\mid i}:$ In the experiments, the paper uses a long-tailed student t distribution for the prior $\\boldsymbol{q}_{j \\mid i}$. Could the authors justify this specific choice and explore the impact of other priors, such as a uniform distribution or a simpler proximity-based kernel, on the final feature quality and performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3XK67iD1PJ", "forum": "VsLjOmwRaS", "replyto": "VsLjOmwRaS", "signatures": ["ICLR.cc/2026/Conference/Submission8818/Reviewer_xXdB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8818/Reviewer_xXdB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845459295, "cdate": 1761845459295, "tmdate": 1762920586145, "mdate": 1762920586145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes a feature learning framework that combines local and unbalanced optimal transport  within an autoencoder architecture. \n\nThe approach replaces global joint distributions with conditional measures $p(y \\mid x_i)$ defined on local neighborhoods $\\mathbf{y}_{j \\mid i}$ to model data relationships. \n\nIts objective function integrates an unbalanced OT cost between $\\delta_{f_\\theta(x_i)}$ and $p(g_\\phi(y) \\mid x_i)$ in the latent space together with reconstruction terms from the autoencoders. \n\nThe model is optimized by block coordinate descent with closed-form updates for both conditional and marginal probabilities. \n\nA generalization bound based on Rademacher complexity is derived, and experiments are reported on classification, clustering, and set-valued data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper formulates a feature learning framework that integrates local and unbalanced optimal transport (OT) with autoencoders. \nThe objective is expressed as a combination of an unbalanced OT term \n$L_{OT}(\\delta_{f_\\theta(x_i)}, p(g_\\phi(y)\\mid x_i))$, \na KL-divergence regularizer, and a reconstruction loss for the autoencoder components. \nA conditional measure $p(y\\mid x_i)$ is defined over the neighborhood $\\mathbf{y}_{j\\mid i}$ for each sample $x_i$, \nreplacing the global joint OT formulation with a local, sample-wise mapping. \n\nDuring optimization, the variables $p_{j\\mid i}$ and $p_i$ are updated in closed form under an alternating procedure. \n\nTheoretical analysis introduces a generalization bound based on Rademacher complexity to describe the expected loss behavior."}, "weaknesses": {"value": "Objective/constraints may be inconsistent:\nEq.(1) includes both a hard marginal constraint $p_{ij}\\in U(a,b)$ and KL penalties on the marginals:\n$\nL_{OT} = \\sum_{i,j} \\rho(x_i,y_j)^{\\bar r} p_{ij} + \\lambda_1 \\mathrm{KL}(p e_k \\| a)+\\lambda_2 \\mathrm{KL}(p^\\top e_n \\| b),\n$\nsee lines with “$p_{ij}\\in U(a,b)$” and the two KL terms. If KL terms are used to relax marginals, the hard set $U(a,b)$ should be relaxed as well; otherwise the role of the penalties is unclear. \n\nRole of priors $q_{j\\mid i}, q_i$ is under-specified empirically:\nThe paper sets $q_{j\\mid i}$ via a Student-$t$ kernel in the original space and discusses $q_i$, but their effects are not isolated. \n\nSet-valued classification needs stronger baselines and clearer protocol.\nWhen set size is 1 and $\\lambda_1\\to\\infty$, the model degenerates toward a NN-style rule; comparisons to strong NN/prototypical baselines and train/test details are limited. \n\nNo ablations for key hyperparameters:\nThe objective exposes multiple hyperparameters $(\\lambda_{1},\\ldots,\\lambda_{4})$ and design choices (e.g., neighborhood size $k$), but their effects are not isolated. \n\nScalability reporting is missing:\nAlthough per-step complexities are stated, there is no wall-clock or memory usage across dataset sizes."}, "questions": {"value": "1. In Eq.(1), are marginals enforced hard via $p_{ij}\\in U(a,b)$ or soft via KL on marginals? If both are used, what is the KL term’s role? Please run an ablation: (a) hard-only (keep $U(a,b)$; no KL), (b) soft-only (drop $U(a,b)$; KL$>$0), (c) both.\n\n2. Please include runtime and peak-memory tables on small/medium/large datasets vs. baselines with identical hyperparams.\n\n3. How much performance comes from the priors $q_{j\\mid i}, q_i$ vs. the transport term? Please report an ablation (i) with priors and (ii) $\\lambda_1,\\lambda_2\\in\\{0,\\text{tuned}\\}$, including accuracy/NMI and sensitivity curves.\n\n4. Please add ablations for $\\lambda_{1},\\ldots,\\lambda_{4}$, neighborhood size $k$, and the distance/order choice; report mean $\\pm$ CI over multiple seeds and include sensitivity curves."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wqH9dUraX9", "forum": "VsLjOmwRaS", "replyto": "VsLjOmwRaS", "signatures": ["ICLR.cc/2026/Conference/Submission8818/Reviewer_xnAA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8818/Reviewer_xnAA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913949894, "cdate": 1761913949894, "tmdate": 1762920584983, "mdate": 1762920584983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new feature learning framework combining Unbalanced Optimal Transport with Auto Encoder. The method uses prior label information to define a conditional distribution p_{j|i}, restricting transport to a local neighborhood of k similar samples. This mapping is implemented via an Auto-Encoder, and the overall optimization is solved using Block Coordinate Descent. Under unsupervised setting, the framework is extended by optimizing anchor points in feature space. Experiments on UCI and MNIST datasets demonstrate strong performance on supervised classification and unsupervised clustering tasks. The method also shows capabilities on set-valued classification tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of local matching is interesting for feature learning, and it verifies the effectiveness in supervised/unsupervised tasks.\n2. Theoretically proves the generalization bound of the proposed method converges at 1/sqrt{n}.\n3. Part of the optimization process has an analytical solution and is relatively efficient."}, "weaknesses": {"value": "1. The convergence proof has not been carefully analyzed. The proof in [Razaviyayn,2012] only holds when each sub-problem is convex/quasi-convex, while Eq.(7) is not.\n2. Calibration of the four regularized hyperparameters.\n3. Related work about feature learning and the deep cluster method is not comprehensive."}, "questions": {"value": "1. In the unsupervised setting, anchor points $z_j$ are learned in the feature space and updated each iteration. How is this fundamentally different from Deep Embedded Clustering, which is not discussed in the paper?\n2. The algorithm has four hyperparameters. Is there a stable setting across tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yejEvPWfRM", "forum": "VsLjOmwRaS", "replyto": "VsLjOmwRaS", "signatures": ["ICLR.cc/2026/Conference/Submission8818/Reviewer_iUwh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8818/Reviewer_iUwh"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101400601, "cdate": 1762101400601, "tmdate": 1762920584138, "mdate": 1762920584138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}