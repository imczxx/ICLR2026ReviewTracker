{"id": "U2PQb9ZrPW", "number": 5621, "cdate": 1757923548065, "mdate": 1759897964527, "content": {"title": "MolGround: A Benchmark for Molecular Grounding", "abstract": "Current molecular understanding approaches predominantly focus on the descriptive aspect of human perception, providing broad, topic-level insights. However, the referential aspect, i.e., linking molecular concepts to specific structural components, remains largely unexplored. To address this gap, we propose a molecular grounding benchmark designed to evaluate a model's referential abilities. This benchmark emphasizes fine-grained understanding and interpretability, challenging models to answer queries such as “What?”, “Where?”, and “Which ones?” across various cognitive levels. We align molecular grounding with established conventions in NLP, cheminformatics, and molecular science, showcasing the potential of NLP techniques to advance molecular understanding within the AI for Science movement. Specifically, we introduce the largest molecular grounding benchmark to date, consisting of 187k QA pairs across five tasks, each targeti ng a distinct cognitive level. Extensive evaluations of both general-purpose and domain-specific (M)LLMs highlight the challenges posed by this benchmark. While existing techniques, such as in-context learning, fine-tuning, and multi-agent strategies, can improve performance, significant progress is still needed to enhance referential capabilities. Furthermore, we demonstrate that molecular grounding can also benefit traditional tasks such as molecular captioning and Anatomical, Therapeutic, Chemical (ATC) classification. The source code and data are available at https://anonymous.4open.science/r/MolGround-2025/.", "tldr": "", "keywords": ["Molecular Grounding", "Language-Structure Reasoning", "AI for Science"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd15b513c77b8338f277a73f53f558a638be3960.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MolGround, which is a new benchmark for molecular grounding comprising 187k QA pairs across five tasks: Chemical Name Entity Recognition (CNER), Name-Structure Mapping (BNSM), Referential Substructure Localization (RSL), Substructure Relationship Grounding (SRG), and Substructure Frequency Analysis (SFA).\n\nThe authors argue that existing molecular understanding approaches focus on \"descriptive\" perception while neglecting \"referential\" perception.\nEvaluations show that both general-purpose and domain-specific (M)LLMs struggle on these tasks, though fine-tuning and multi-agent strategies offer modest improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Most Comprehensive and Large Molecular Grounding Benchmark. MolGround provides 187k QA pairs across five fine-grained tasks (CNER, BNSM, RSL, SRG, SFA), representing a 3× scale increase over existing benchmarks."}, "weaknesses": {"value": "The authors claim existing benchmarks are 90%+ \"descriptive\" (Table 1), yet this classification is arbitrary and misleading. For example, they classify property prediction as \"descriptive\" even when it requires identifying specific functional groups responsible for toxicity—a clearly referential task. \n\nTasks like CNER (named entity recognition), BNSM (name-to-structure conversion), and SFA (substructure counting) are standard problems in cheminformatics that have been studied for decades using rule-based tools (RDKit, OpenBabel), graph algorithms, and specialized ML models. The novelty lies solely in compiling these into a unified QA benchmark and evaluating LLMs—an incremental contribution insufficient for a top-tier venue.\n\nThe paper provides no compelling rationale for why LLMs should be used for these tasks when superior alternatives exist. The paper's evaluations show all models achieve poor accuracy on RSL, yet the authors conclude this \"remains challenging and warrants further research\" rather than questioning whether LLMs are the wrong tool. \n\nThe benchmark construction process is opaque and poorly justified. The authors use GPT-4o to generate captions for molecules lacking descriptions, then use the same model (or its derivatives) to evaluate grounding tasks"}, "questions": {"value": "How was the benchmark validated? Provide detailed annotation statistics (rejection rates, refinement cycles, error types). Release confusion matrices and disagreement analysis. Make the data and code publicly available.\n\nWhat happens when models fail? Conduct systematic error analysis. What percentage of RSL predictions are chemically valid? What types of errors dominate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w0rXn0JHWp", "forum": "U2PQb9ZrPW", "replyto": "U2PQb9ZrPW", "signatures": ["ICLR.cc/2026/Conference/Submission5621/Reviewer_pZ6m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5621/Reviewer_pZ6m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884495600, "cdate": 1761884495600, "tmdate": 1762918163284, "mdate": 1762918163284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MolGround, a benchmark designed to evaluate referential understanding in molecular language models . The benchmark includes 187k QA pairs across five proposed grounding tasks - Chemical Name Entity Recognition, Bidirectional Name -Structure Mapping, Referential Substructure Localization, Substructure Relationship Grounding, and Substructure Frequency Analysis. The authors claim MolGround is the largest dataset of its kind and demonstrate its use in evaluating various LLMs and MLLMs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Paper formulation: The five tasks correspond well to different levels of cognitive and structural reasoning."}, "weaknesses": {"value": "- Dataset not accessible: The full dataset is unavailable via the anonymous link; only partial QA examples are shared. This is a major reproducibility issue for a dataset-centric paper.\n- Overstated novelty: Similar or larger datasets already exist, notably MolTextQA (> 500 k pairs)[1], 3DMolLM[2], and Mol-Instructions[3] — all of which address molecular grounding or structure-text alignment. MolGround’s size (187 k) and task scope are not clearly superior. These related works must be acknowledged and the differences w.r.t. the proposed dataset should be clarified. \n- Writing and structure: The manuscript is hard to follow. Many paragraphs are lengthy and lack focus; several equations (e.g., Eq. 4) are undefined or poorly explained.\n- Evaluation limitations:\n    - The benchmark is evaluated only on its own QA tasks; no downstream evaluations.\n    - Comparisons with Mol-Instructions and other baselines are insufficiently detailed.\n- Data generation issues: The dataset uses GPT-4o to generate captions when unavailable, yet there is no validation or quality analysis for correctness or hallucinations.\n- Low question diversity: Nearly all QA pairs concern structural aspects (e.g., rings, chains, substructures), with no property-, function-, or activity-based questions, limiting general utility. The number of types of distinct questions is limited, which will complicate learning  \n- Unclear utility: The primary use cases for MolGround beyond benchmarking on the dataset itself remain vague.\n\n[1] Laghuvarapu, S., Lee, N., Gao, C., & Sun, J. (2025). MolTextQA: A Question-Answering Dataset and Benchmark for Evaluating Multimodal Architectures and LLMs on Molecular Structure–Text Understanding. Journal of Data-centric Machine Learning Research.\n[2] Li, S., Liu, Z., Luo, Y., Wang, X., He, X., Kawaguchi, K., ... & Tian, Q. (2024). Towards 3d molecule-text interpretation in language models. arXiv preprint arXiv:2401.13923.\n[3] Fang, Y., Liang, X., Zhang, N., Liu, K., Huang, R., Chen, Z., ... & Chen, H. (2023). Mol-instructions: A large-scale biomolecular instruction dataset for large language models. arXiv preprint arXiv:2306.08018."}, "questions": {"value": "1.  Will the complete dataset (molecules, captions, and QA pairs) be publicly released with a permanent identifier (e.g., DOI or HuggingFace repository)?\n2.\tData validation: How were the GPT-4o-generated captions validated for chemical accuracy or factual correctness of the **overall**? Were experts involved in verifying those samples?\n3.\tComparative scope: How does MolGround fundamentally differ from MolTextQA, 3DMolLM, and Mol-Instructions beyond task naming and data format?\n4.\tUse cases: Can you provide examples of downstream tasks (e.g., molecular retrieval, activity prediction) where referential grounding demonstrably helps?\n6.\tQuestion diversity: Do you plan to expand the benchmark to include property- or function-related questions instead of focusing solely on substructure localization? \n7.\tThe authors could have leveraged PubChem-300k dataset to expand the number of available captions. Why was this not done ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ea8fkYK1Yc", "forum": "U2PQb9ZrPW", "replyto": "U2PQb9ZrPW", "signatures": ["ICLR.cc/2026/Conference/Submission5621/Reviewer_uW7G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5621/Reviewer_uW7G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973453972, "cdate": 1761973453972, "tmdate": 1762918163041, "mdate": 1762918163041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MolGround, a comprehensive benchmark designed to evaluate referential molecular understanding, i.e., a model’s ability to link textual chemical mentions to explicit structural components. It formalizes five grounding tasks—Chemical Named Entity Recognition (CNER), Bidirectional Name–Structure Mapping (BNSM), Referential Substructure Localization (RSL), Substructure Relationship Grounding (SRG), and Substructure Frequency Analysis (SFA)—covering multiple cognitive levels.\nThe benchmark contains 187 k QA pairs spanning 55 k molecules and integrates a human-validated, agent-assisted curation pipeline. Empirical results using general, domain, and multimodal LLMs reveal the difficulty of these tasks (e.g., ≤ 0.016 F1 in zero-shot RSL), and the authors further show that grounding features can improve downstream molecular captioning and ATC classification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper correctly identifies a key blind spot in current molecule-language research: most benchmarks assess descriptive reasoning (captioning, property prediction) rather than referential understanding (“what/where/which”). The motivation is well-grounded in linguistic and cognitive-semantic theories (Frege, Russell, DRT) and draws a clear analogy to visual grounding in vision-language learning\n\n\n2. The paper proposed a benchmark with scale - 187 k QA items across 307 substructure types, with train/val/test disjoint by molecule to reduce leakage.\n\n3. The grounding agent baseline with subgraph matching offers a valuable reference point and clearly outperforms standard fine-tuning."}, "weaknesses": {"value": "1. Some task definitions are underspecified or inconsistent (e.g., missing codomain specifications for index sets in RSL/SRG; BNSM acronym varies across text).\n\n2. The taxonomy in Table 1 lists BNSM as fully descriptive despite being defined as bidirectional referential mapping; percentages may not align with row sums\n\n3. The paper lacks a unified “Evaluation Protocols” section describing metrics per task (accuracy vs. F1 vs. exact match) and the treatment of multiple-choice vs. open-ended QA.\n\n4. \tNo deterministic rule-based baselines (e.g., OPSIN, Open Babel, RDKit SMARTS) for BNSM, RSL, and SFA to establish upper-bound ceilings.\n\n5. Minor formatting mistakes: Equation formatting and notations occasionally deviate from mathematical convention (e.g., use of “7→” vs. “→”, ambiguous set brackets)."}, "questions": {"value": "For the ICL vs. SFT vs. multi-agent: How were in-context examples chosen? Were they automatically retrieved or hand-crafted?\n\nWhat are the exact heuristics for subgraph matching and constraint enforcement (e.g., tolerance for tautomeric or symmetric structures)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jF7v6JXVxy", "forum": "U2PQb9ZrPW", "replyto": "U2PQb9ZrPW", "signatures": ["ICLR.cc/2026/Conference/Submission5621/Reviewer_R6Tk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5621/Reviewer_R6Tk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981591313, "cdate": 1761981591313, "tmdate": 1762918162727, "mdate": 1762918162727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MolGround, a large-scale benchmark for evaluating molecular grounding—the ability of language models to align chemical text descriptions with specific molecular substructures. It introduces five tasks covering entity recognition, name–structure mapping, substructure localization, relationship grounding, and frequency analysis. The dataset is built through an interactive human–agent collaboration process to ensure both scalability and quality, resulting in the largest benchmark to date for fine-grained molecular understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of grounding language in molecular structures is impactful, addressing a crucial gap between symbolic chemical language and structural understanding. The paper targets a clear and underexplored challenge in molecular AI, linking text-level descriptions to molecular substructures.\n2. The five subtasks cover multiple reasoning levels, from entity recognition to relational grounding, offering a holistic evaluation framework.\n3. The proposed human–agent interactive process smartly balances automation and expert validation. \n4. The paper is well-written, with strong figures (e.g., Fig. 1) that clearly explain the workflow."}, "weaknesses": {"value": "1. Although the human–agent collaboration process is clearly demonstrated, it’s difficult to ensure the benchmark’s annotation accuracy and consistency without quantitative validation.\n2. The paper lacks diversity analysis to show molecular variety would make the benchmark’s coverage more convincing.\n3. The contribution is primarily in dataset design; more discussion on how this framework could inspire model training objectives would strengthen the work."}, "questions": {"value": "please refer to the cons"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W0TKHFgAte", "forum": "U2PQb9ZrPW", "replyto": "U2PQb9ZrPW", "signatures": ["ICLR.cc/2026/Conference/Submission5621/Reviewer_qXFd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5621/Reviewer_qXFd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137332043, "cdate": 1762137332043, "tmdate": 1762918162495, "mdate": 1762918162495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}