{"id": "KtrFXlvgrK", "number": 1481, "cdate": 1756886278617, "mdate": 1763659601848, "content": {"title": "SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models", "abstract": "Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single-image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence.", "tldr": "", "keywords": ["Vision-Language Models", "Spatial Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7ca2ca274db307f7461085efdc3ebd449d6351b.pdf", "supplementary_material": "/attachment/3027c1993451bde33ebafc2c96c01209c579f5cd.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SpatialLadder, a new 3B VLM in 3D spatial reasoning with a 3-stage training scripts, data collection pipeline and strong task performance. By incorporating this 3-stage training, SpatialLadder learns spatial knowledge and reasoning progressively and tops at spatial benchmarks while performing decently on OOD spatial tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The 3-stage training pipeline looks reasonable and effective with carefully curated 26K data spanning across 4 domains. The data would be potentially beneficial to the community\n2. SpatialLadder-3B tops at in-domain and OOD spatial benchmarks \n3. Comprehensive ablations and visual analysis about attention maps and case study"}, "weaknesses": {"value": "1. Lack of ablation on the 3-stage training paradigm. As both stage 1 and 2 are SFT-ing over spatial data, though the type of questions and visual input maybe different, but what if combining the first two SFT stages into one as they both serve as broadening the spatial knowledge of base VLMs\n2. Only one model was trained. Will the model perform better if using another VLM or increasing the parameter size to 7B or larger?"}, "questions": {"value": "I’m curious about how VLM performs on general VL benchmarks that are partially related to spatial reasoning like RealWorldQA, RoboSpatial-Home, MME-RealWorld-Lite. Will this 3-stage introduce significant catastrophic forgetting to VLM that hinders the general performance of the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v1zP0wR6k6", "forum": "KtrFXlvgrK", "replyto": "KtrFXlvgrK", "signatures": ["ICLR.cc/2026/Conference/Submission1481/Reviewer_g2My"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1481/Reviewer_g2My"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793672714, "cdate": 1761793672714, "tmdate": 1762915781368, "mdate": 1762915781368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of spatial reasoning in Vision-Language Models (VLMs), positing that current models fail because they attempt to learn this skill monolithically rather than hierarchically. The authors introduce a \"perception-reasoning gap\" and propose a systematic, progressive approach to bridge it. Their contributions are twofold: 1) A new multimodal dataset, SpatialLadder-26k, containing over 26,000 samples for object localization, single-image, multi-view, and video spatial reasoning, built with a standardized pipeline. 2) A three-stage progressive training framework designed to first establish perceptual grounding (object localization via supervised fine-tuning), then develop spatial understanding across multiple dimensions and modalities, and finally strengthen complex reasoning using reinforcement learning (GRPO) with chain-of-thought. The resulting 3B parameter model, SpatialLadder, achieves state-of-the-art performance, significantly outperforming its base model as well as larger proprietary models like GPT-4o on a suite of in-domain and out-of-domain benchmarks. Extensive ablation studies validate that the progressive, curriculum-based approach is crucial to the model's success."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear and Well-Motivated Hypothesis:** The paper is built around the intuitive and compelling idea that spatial intelligence is hierarchical and must be learned progressively from perception to reasoning. This \"perception-reasoning gap\" hypothesis is clearly articulated and effectively motivated by a preliminary experiment showing that adding perceptual cues improves a base model's performance.\n2. **High-Quality Methodology:** The work introduces a comprehensive solution with two strong components. The SpatialLadder-26k dataset is systematically designed to cover a full spectrum of spatial tasks, addressing the fragmentation of existing resources. The three-stage training framework is a logical and direct implementation of the paper's core hypothesis, providing a clear recipe for building spatial reasoning capabilities.\n3. **Extensive and Rigorous Evaluation:** The experimental validation is exceptionally thorough. The authors evaluate their model on six different benchmarks, assessing both in-domain and, crucially, out-of-domain generalization. The comparison against a strong suite of open-source and proprietary models clearly demonstrates the effectiveness of their approach.\n4. **Convincing Ablation Studies:** The paper includes a comprehensive set of ablation studies that systematically dismantle the proposed framework to prove the value of each component. The analysis confirms the importance of each training stage, the multimodal data, the chain-of-thought mechanism, and, most importantly, the progressive training order itself. The dataset comparison in the appendix, which shows their smaller, curated dataset outperforms larger ones, is a particularly strong piece of evidence for their approach."}, "weaknesses": {"value": "1. **Limited Model Scale Exploration:** The experiments are confined to a 3B parameter model. While the results are impressive for this model size, the paper does not explore whether these significant gains will translate to larger, state-of-the-art models (e.g., 7B+).\n2. **Dataset Domain Bias:** The dataset's reliance on the indoor scenes from ScanNet introduces a potential domain bias. The model's excellent performance might not fully generalize to outdoor environments or other visually distinct domains.\n3. **Incremental Novelty of Components:** While the overall system and the curriculum are novel and highly effective, the individual techniques used (supervised fine-tuning, GRPO for reinforcement learning) are established methods. The main contribution lies in the successful and systematic application of these components in a progressive manner to solve the spatial reasoning problem, rather than the invention of a new technique."}, "questions": {"value": "1. The ablation study (Figure 5) indicates that Stage 3 (Reinforcement Learning) contributes less to the final performance (2.1% drop upon removal) compared to Stage 2 (Understanding SFT, 9.4% drop). Could you elaborate on the specific role of the RL stage? Does it primarily refine the structure and logical flow of the reasoning chains, or does it enable the model to solve problems that were previously intractable after Stage 2?\n2. The comparison in Appendix D.2 showing that the smaller SpatialLadder-26k dataset is more effective than larger datasets is a very strong result. Based on your analysis, which aspect of your dataset design do you believe is the most critical for this data efficiency: the inclusion of foundational localization tasks, the multi-modal coverage (SI+MV+Video), or the structured nature of the QA pairs?\n3. The proposed framework uses a fixed, sequential training order. Have you explored or considered more dynamic training curricula? For example, could there be benefits to cyclically reintroducing tasks from earlier stages or using a mixed-task approach once a foundational capability in each stage is achieved?\n4. The visual attention analysis is quite insightful. Have you investigated whether the more focused attention in SpatialLadder translates to better performance on multi-object relational tasks? For instance, when asked for the relative direction between two objects, does the model's attention map show precise focus on both entities simultaneously?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4rjF1ErInW", "forum": "KtrFXlvgrK", "replyto": "KtrFXlvgrK", "signatures": ["ICLR.cc/2026/Conference/Submission1481/Reviewer_eN7r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1481/Reviewer_eN7r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881313290, "cdate": 1761881313290, "tmdate": 1762915780370, "mdate": 1762915780370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the persistent weakness of current Vision Language Models (VLMs) in spatial reasoning in terms of understanding object positions, distances, and relationships in 2D/3D scenes. The authors argue that this limitation stems from the lack of a hierarchical learning process bridging low-level perception and high-level reasoning. To tackle this, they propose a three-stage progressive training framework that mirrors human cognitive development:\n\n1. Stage 1 : supervised fine-tuning on object localisation tasks.\n\n2. Stage 2 : fine-tuning on a new multimodal dataset covering spatial reasoning tasks across images, multi-view setups, and videos.\n\n3. Stage 3 : reinforcement learning with chain-of-thought supervision (GRPO) to strengthen reasoning quality.\n\nThey also introduce SpatialLadder-26k, a curated dataset (26k samples) systematically covering spatial reasoning tasks across modalities. The resulting SpatialLadder-3B model (based on Qwen2.5-VL-3B) achieves state-of-the-art performance on spatial reasoning benchmarks,  including VSI-Bench, SPBench-SI and SPBench-MV, outperforming GPT-4o and Gemini-2.0-Flash by some margins.\nThe authors claim that this demonstrates the value of progressive spatial training for robust spatial intelligence in VLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed SpatialLadder-26k dataset is carefully curated and covers a wide range of spatial tasks from object localisation to multi-view and video reasoning, providing a systematic and high-quality resource for studying spatial understanding in VLMs.\n\n2. The three-stage progressive framework is conceptually clean and easy to reproduce. It makes sense, in theory, to have training stages the way the authors designed.\n\n3. The paper reports extensive experiments across multiple benchmarks, including new ones introduced by the authors, with consistent metrics, ablations, and qualitative analyses that make the empirical findings clear and verifiable.\n\n4. The resulting model achieves nice gains over comparable open-source and even proprietary models on several spatial reasoning benchmarks, demonstrating that targeted fine-tuning on well-structured spatial data can significantly improve performance."}, "weaknesses": {"value": "1. Despite being framed as “progressive spatial reasoning,” the method largely reduces to a structured fine-tuning pipeline. Most of the gains appear to come from dataset design and task-aligned supervision rather than from a fundamentally new training principle or architectural innovation.\n\n2. The final stage adds negligible improvement, around one to two percent, and therefore does not convincingly demonstrate that the proposed reinforcement-based reasoning step meaningfully enhances spatial understanding beyond what supervised fine-tuning already achieves.\n\n3. While the authors ablate each stage separately, they never compare progressive versus joint training on all data. Without such a baseline, it remains unclear whether the sequential, staged structure is necessary or if a single fine-tuning step on the combined dataset would yield equivalent results.\n\n4. Both the dataset and the evaluation benchmarks share nearly identical task formats and question templates. This overlap makes it difficult to separate genuine reasoning improvements from specialization to the specific spatial QA style used during training."}, "questions": {"value": "1. Have the authors compared progressive stage-wise training against a single-stage joint fine-tuning using all Stage 1 + Stage 2 data to demonstrate that the “progressive” order is indeed beneficial?\n\n2. Given that Stage 3 yields only marginal gains, can the authors clarify what specific reasoning abilities are improved by the GRPO reinforcement phase, beyond formatting or answer consistency?\n\n3. Since the training and evaluation datasets share highly similar spatial QA formats and scene types, how do the authors ensure that the model’s gains reflect genuine spatial reasoning rather than memorization of task patterns?\n\n4. Qwen2.5-VL is a general-purpose multimodal model. Did the authors evaluate whether SpatialLadder retains its original non-spatial capabilities (e.g., captioning or OCR) after fine-tuning? This is assuming the fact that the authors did full fine tuning of the model and not LoRA, which mostly leads to catastrophic forgetting.\n\n5. Following from my previoous question, are all model parameters fine-tuned during the supervised and reinforcement phases, or are parameter-efficient methods (e.g., LoRA) used? How stable is training when applying full-model updates?\n\n6. Given that Stage 1 improves results only slightly, what evidence supports the claim that perceptual grounding through object localisation meaningfully benefits downstream reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AQPTwSCPEA", "forum": "KtrFXlvgrK", "replyto": "KtrFXlvgrK", "signatures": ["ICLR.cc/2026/Conference/Submission1481/Reviewer_d8EG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1481/Reviewer_d8EG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908731473, "cdate": 1761908731473, "tmdate": 1762915780204, "mdate": 1762915780204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SpatialLadder, a progressive training framework for enhancing spatial reasoning in vision-language models (VLMs). The approach builds on a new dataset, SpatialLadder-26k, which includes 26k multimodal samples (object localization, single-image, multi-view, and video reasoning). The model is trained in three progressive stages: (1) supervised fine-tuning for perception (object localization), (2) spatial understanding through seven spatial dimensions (direction, distance, size, etc.), and (3) reinforcement learning with GRPO for complex reasoning. The resulting SpatialLadder-3B model achieves strong performance on multiple spatial reasoning benchmarks (VSI-Bench, SPBench-SI, SPBench-MV), outperforming GPT-4o and Gemini-2.0-Flash on average by 20.8% and 10.1%, respectively, with moderate out-of-domain generalization gains (7.2%)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Extensive experimental validation, including ablations, training dynamics, attention visualization, and both in-domain and out-of-domain benchmarks.\n\n- The dataset is clearly defined and covers complementary modalities; the pipeline is systematic and reproducible.\n- Empirically strong; achieves state-of-the-art results on spatial reasoning tasks without architectural changes."}, "weaknesses": {"value": "- The novelty is limited. The method primarily combines known strategies (curriculum learning, GRPO-based RL, multimodal fine-tuning) rather than introducing a new algorithmic insight or theoretical formulation.\n\n- The claimed “progressive construction of spatial intelligence” is not quantitatively justified beyond accuracy gains, no analysis on intermediate representation transfer or reasoning trajectory quality.\n\n- The reinforcement learning component is weakly analyzed: reward sensitivity, stability, and ablations on the GRPO configuration are missing.\n\n- Comparisons to alternative architectures (e.g., InternVL, LLaVA-Next) with equivalent compute budgets are absent, weakening generality claims.\n\n- The dataset scale (26k) is relatively small and domain-constrained (indoor ScanNet scenes). This limits claims of “general spatial intelligence.”"}, "questions": {"value": "- How much of the performance gain comes from the new data versus the progressive training schedule?\n\n- Is Stage 3 RL critical, or can comparable performance be achieved by longer supervised training?\n\n- How sensitive is performance to the stage ordering or reward scaling?\n\n- Were out-of-domain benchmarks seen or related to the training domains (e.g., ScanNet overlap)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No major ethical risks. The dataset relies on public 3D scans and video sources. The paper follows standard academic ethics; minor concern may involve redistribution of third-party datasets."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e0rlmgQnke", "forum": "KtrFXlvgrK", "replyto": "KtrFXlvgrK", "signatures": ["ICLR.cc/2026/Conference/Submission1481/Reviewer_fKYQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1481/Reviewer_fKYQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762028279979, "cdate": 1762028279979, "tmdate": 1762915780031, "mdate": 1762915780031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}