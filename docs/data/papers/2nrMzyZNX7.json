{"id": "2nrMzyZNX7", "number": 15602, "cdate": 1758253038274, "mdate": 1763533715417, "content": {"title": "Perceive Fast, Think Slow: A Cognitive-Inspired Framework for Time Series Analysis", "abstract": "Time series modeling faces persistent challenges: fixed-window tokenization misaligns with natural event boundaries, uniform computation wastes capacity on simple patterns, and static architectures cannot adapt to diverse temporal dependencies. We propose **PeCo-TS**, a cognitive-inspired framework that instantiates the principle of “perceive fast, think slow” through three key innovations: (1) *event-driven dynamic-length tokenization* that aligns tokens with semantic boundaries and reduces redundancy, (2) a *Slow–Fast dual-pathway architecture* that separates rapid perception of fine-grained variations from slower abstraction of event-level structures, and (3) *Dual-Axis Adaptive (DA²) attention* that dynamically balances intra-series and inter-series dependencies via learnable gating. Extensive experiments across four diverse tasks—forecasting, classification, anomaly detection, and imputation—demonstrate the broad applicability of PeCo-TS, yielding consistent improvements over Transformer and linear baselines, including 5.6% lower forecasting MSE, 9.3% lower imputation error, higher classification accuracy across UCR/UEA benchmarks, and a 6.7% relative F1 gain in anomaly detection.  Beyond accuracy, PeCo-TS achieves favorable efficiency–performance trade-offs by leveraging event-level abstraction and complementary pathway synergy, while its learned boundaries align with real-world regime shifts, providing interpretability. These results establish PeCo-TS as a versatile backbone that unifies efficiency, adaptability, and semantic alignment for diverse time-series applications.", "tldr": "PeCo-TS is a cognitive-inspired framework that uses event-driven tokenization and a dual Fast–Slow pathway to achieve state-of-the-art accuracy and efficiency across diverse time-series tasks.", "keywords": ["Time-series modeling;  Multivariate time series; Event-driven segmentation; Dual-Axis Adaptive Attention"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50ce0011d9242f7e17b1ed4d569dc768f2f5df1d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Perception-Concept Transformer for Time Series (PeCo-TS), a dual pathway architecture for modeling event driven signals with efficiency and accuracy. Experiments across forecasting, classification, anomaly detection, and imputation show good performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Cognitive neuroscience perspective on architecture for modeling time series is interesting\n- Good empirical results\n- Considers many time series analysis tasks"}, "weaknesses": {"value": "- Missing important time series baselines, such as UniTS [1]. UniTS has been adapted to pretraining, demonstrates few shot fine tuning and prompt tuning, as well as performant across many tasks. Strong single-task performance has been shown as well. Authors should compare with this and other methods within paper.\n[1] S. Gao et al, \"UniTS: A Unified Multi-Task Time Series Model\", NeurIPS 2024\n- Comparisons with other adaptive tokenization methods, such as Pathformer [2] is missing. Authors should compare with this model and other adaptive tokenization methods in terms of performance and efficiency tradeoffs.\n[2] P. Chen et al, \"PATHFORMER: MULTI-SCALE TRANSFORMERS WITH ADAPTIVE PATHWAYS FOR TIME SERIES FORECASTING\", ICLR 2024"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h9okGvUBLq", "forum": "2nrMzyZNX7", "replyto": "2nrMzyZNX7", "signatures": ["ICLR.cc/2026/Conference/Submission15602/Reviewer_9xFx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15602/Reviewer_9xFx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761224054332, "cdate": 1761224054332, "tmdate": 1762925874708, "mdate": 1762925874708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces PeCo-TS, a framework for time series forecasting that builds a novel architecture based on a cognitive-inspired framework. The authors nicely motivate the need for this type of architecture in time series, where data are sparse and need different computational considerations across tokens and regions of the time series samples. The model is demonstrated on several benchmark time series datasets, and the authors nicely examine the interpretability component of the model."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work is nicely motivated from a cognitive science perspective, considering the complexities of time series and how processing is fundamentally different than other types of sequential data. \n- The authors consistently comment on the computational efficiency and considerations of their model. This is very important in time series data where real-world datasets may be very large and consist of very lengthy inputs.\n- The performance of the model is very strong compared to baseline methods. \n- The mechanistic insights was an interesting discussion in the results, backing up some of the intuitive positing in the methods."}, "weaknesses": {"value": "- I found the interpretability discussion lacking substance. I see the intuitive nature of how the architecture can be seen as being more interpretable, which is a positive, but there doesn't seem to be enough evidence to conclude that the internal components yield a model that is more interpretable.\n- The work only examines the model on a few benchmark datasets, and only in the forecasting setting."}, "questions": {"value": "- I would be interested to see performance on real-world time series datasets that are long-range. Also, does the model perform well on other types of time series tasks, such as classification or anomaly detection?\n- Building on this, can the method apply to irregularly-sampled time series data? This seems as if it would interfere with some of the boundary selection techniques."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MzLoonTaHu", "forum": "2nrMzyZNX7", "replyto": "2nrMzyZNX7", "signatures": ["ICLR.cc/2026/Conference/Submission15602/Reviewer_uct9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15602/Reviewer_uct9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963617104, "cdate": 1761963617104, "tmdate": 1762925873951, "mdate": 1762925873951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PeCo-TS, a cognitive-inspired time-series framework that operationalizes a ``perceive fast, think slow'' principle via three components: an event-driven, variable-length tokenization based on frequency-aware boundary detection; a Slow--Fast dual-pathway backbone (a high-resolution fast path with linear attention plus a slow path operating on event tokens); and a Dual-Axis Adaptive (DA2) attention that gates between intra-series (temporal) and inter-series (channel) dependencies. The authors claim consistent gains across forecasting, classification, anomaly detection, and imputation, along with efficiency benefits from event-level abstraction and interpretability via boundary alignment with regime shifts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Interesting problem formulation and coherent story: The paper directly targets three pain points of time-series Transformers---fixed-patch boundary misalignment, uniform compute allocation, and rigid channel mixing---and proposes a single framework whose components map cleanly to each issue: event-driven tokenization for semantic alignment, Slow--Fast pathways for non-uniform compute, and DA2 gating for adaptive intra-/inter-series dependence. The narrative is clear and supported by ablations (e.g., removing Fast/Slow leads to asymmetric performance drops), which helps attribute gains to each module.\n- Slow–Fast dual-pathway architecture with clear division of labor: The Fast path keeps high-resolution, near-diagonal dependencies (linear attention) while the Slow path abstracts event tokens with multi-layer processing; ablations show asymmetric but benefits such as fast for timing/anomalies and slow for long-range integration as mentioned before. The reprojection layer ties abstractions back to the fine scale, which seems to explain the margins at longer horizons and robust multi-task behavior.\n- DA2 mixes token-axis (temporal, per-channel) and channel-axis (cross-series) attentions through a learnable gate $\\pi\\in(0,1)$, strictly generalizing channel-independent and channel-shared extremes. The paper appears to support this with both theoretical stability/expressivity arguments and empirical evidence (dataset-specific evolution of $\\pi$) and shows gains over static designs in forecasting, classification etc. This gating idea has been applied to hybrid and test time training architectures for time series and its use is well motivated."}, "weaknesses": {"value": "- While the cognitive framing is compelling, several components (frequency-guided segmentation, multi-rate processing, cross-channel attention) echo ideas in multi-resolution tokenization, deformable/selective attention, and SlowFast-style designs. The paper would benefit from comparisons and controlled ablations against closer alternatives (e.g., learnable segmentation vs. token merging, vs. fixed multi-res), beyond standard baselines. There are also some more recent architectures such as state space models, hybrid architectures, and foundation models which have seen use in time series forecasting whose performance would be worth mentioning.\n- Claims about favorable accuracy–efficiency trade-offs would be stronger with hardware-based end to end metrics (params, FLOPs, and latency/memory) under identical training/inference regimes and long-context would be interesting. Reporting how $M$ scales with sequence length and dataset periodicity (and its variance across runs) would clarify real-world complexity. The reprojection overhead and DA2 cost should be included in the accounting.\n- The detector stacks FFT smoothing, temperature-softmax, cosine-comb exponent $\\gamma$, thresholding and NMS, followed by padding/masking. Ablations on the sensitivity to $\\tau,\\gamma$, robustness under nonstationary or multi-period signals, and gradient behavior at boundary flips would increase confidence. Reasons for over-/under-segmentation would also be interesting."}, "questions": {"value": "- Beyond global trends, do you observe layer-wise or position-wise diversity in $\\pi$ (e.g., early layers favor token-axis, later layers favor channel-axis)? Are there identifiability issues where both branches learn redundant roles? \n- Can you provide on-the-fly diagnostics ie: boundary confidence dispersion, over-/under-segmentation indicators and an adaptive fallback  when the learned boundaries are noisy or unstable? How does performance degrade if boundaries are deliberately perturbed or partially randomized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "P1LtePHv9j", "forum": "2nrMzyZNX7", "replyto": "2nrMzyZNX7", "signatures": ["ICLR.cc/2026/Conference/Submission15602/Reviewer_4qb4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15602/Reviewer_4qb4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762737607854, "cdate": 1762737607854, "tmdate": 1762925873287, "mdate": 1762925873287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}