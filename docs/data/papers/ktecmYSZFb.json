{"id": "ktecmYSZFb", "number": 22305, "cdate": 1758329384050, "mdate": 1759896873365, "content": {"title": "SciLitBench: Benchmark and Design Principles for LLM-Powered Systematic Literature Reviews", "abstract": "Systematic literature reviews are essential for science but remain labor-intensive. To benchmark and improve automation, we introduce SciLitBench, a new dataset of 42,980 curated abstracts, 2,311 full texts, and TODOXXX structured data elements (e.g. study-level metadata, PICO entities, outcome measures, and evidence) annotated and labeled for inclusion decisions and knowledge reasoning. Across 22 open-source large language models (LLMs), we uncover general design principles that make automation reliable under recall-skewed objectives (F$_2$). First, we observe clear scaling and prompt-design effects: explicit inclusion/exclusion prompting improves accuracy by up to +29\\%, while adding researcher ``thought traces'' yields a +28\\% gain. Second, we show that reliability under full-text screening depends sharply on the interaction between context length and model capacity. Motivated by this, we introduce a token-length–aware routing system that surpasses ensembles of strongest models ($F_2 = 0.949$ vs $0.938$). Finally, we demonstrate a human-in-the-loop, rubric-guided extraction workflow that separate field extraction from guideline adherence checking to align model outputs with domain standards given researcher feedback. Together, our benchmark and findings establish scaling, prompt design, thought traces, and adaptive routing as key principles for reliable, researcher-aligned automation of systematic reviews.", "tldr": "", "keywords": ["LLM", "benchmark", "dataset", "literature review", "screening", "data extraction", "abstract screening", "design principle", "systematic review"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a29f481c16ab169bdc4b4894eeeec2af6b55727c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SciLitBench, an open, multi-stage benchmark/protocol for LLM-assisted systematic literature reviews, spanning title/abstract screening, full-text screening, and structured extraction. It curates records and releases frozen splits for TA and FT items with rationales, plus large-scale semi-automated labels for the remainder. Methodologically, it studies prompt design, thought-trace demonstrations, and a token-length–aware routing ensemble, reporting strong F2 improvements and perfect-recall operating points. Figures 2–6 and Tables 2–3 summarize main empirical findings."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "S1) Clear protocol + frozen splits. Tasks, I/O contracts, and forezen sets for TA/FT are well specificed supporting replication. \n\nS2) Prompting yields good relative gains.\n\nS3) Open-Artifact orientation: Authors emphasize releasing prompts, outputs, scoring scripts, and guidelines to enable \"leaderboard-style\" comparison."}, "weaknesses": {"value": "W1) Incomplete placeholder stats. The abstract contains “TODOXXX structured data elements” (Abstract, lines 012–020). Similarly, Sec. 1 claims creating “45,291 binary labels and TODOXXX multi-class labels” (Sec. 1, line 086). This undermines reporting completeness. \n\nW2) Semi-automated label prop. risk -- The protocol auto-labels TAs and FTs using the best system with only \"sample validation\", which may propagate system biases/errors. \n\nW3) \"Perfect Recall\" comes with *very* low precision. For downstream TA application, the chosen combined voting (t_s=3, t_l=4) achieves recall = 1.0 but precision = 0.275 (Tab 2), implying high FP rate and heavy reviewer load despite the reported 90.5% workload reduction computation. \n\nW4) Limited discussion of training data overlap -- The benchmark topic (AI-for-reviews) likely appears in LLM pretraining corpora. Within pp. 3–6 there’s no audit/guardrail for pretraining overlap or leakage when evaluating open-source LLMs with fixed prompts; this threatens authors' validity claims."}, "questions": {"value": "Q1) Authors should revise this version with significant improvements to their presentation (replace \"TODOXXX\"). \n\nQ2) Data contamination audit -- For the open-source LLMs and frozen splits, can nyou report any steps/heuristics to assess the pretraining overlapwith your benchmark? Or at least a sensitivity check? \n\nQ3) Routing generalization -- The routing ensemble fits length-bin–specific logistic regressions (Tab 3). How robust are the learned weights across topics/length distributions, and what happens if bins or the length cap change? Please add an ablation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NvjxPnJZda", "forum": "ktecmYSZFb", "replyto": "ktecmYSZFb", "signatures": ["ICLR.cc/2026/Conference/Submission22305/Reviewer_et5a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22305/Reviewer_et5a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926418695, "cdate": 1761926418695, "tmdate": 1762942161382, "mdate": 1762942161382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SciLitBench, a benchmark for automating systematic literature reviews with large language models. It evaluates many LLMs on tasks like screening and data extraction, showing that careful prompt design and reasoning examples greatly improve reliability. The study highlights design principles for building trustworthy, efficient AI-assisted review systems."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper conducts extensive experiment with their curated dataset, somewhat bringing insights regarding performances of LLMs in the literature screening task.\n\n2. The experiments concerning scaling law are informative."}, "weaknesses": {"value": "1. This paper has notable presentation problem. For instance, the placeholder span \"TODOXXX\" appears multiple times, even in abstract; although the authors claim that an example will be presented in Appendix D, it is indeed not there; Figure 7 and 8 are shown without any description or analysis. Therefore, this work leaves me a impression that it still requires more time to be formulated as a publishable manuscript.\n\n2. Some analyses do not fully align with the observation. For instance, in Line 312, the authors claim that \"larger models consistently achieve higher F2 scores\", while the larger Qwen model results in worse performance compared with its smaller variants. The authors would better provide more detailed interpretation on the results as they are quite a lot of analysis remaining to be made. Similarly, the performance of Phi 3.5 is extremely poor, which requires further descriptions.\n\n3. As a benchmark paper, this study does not provide sufficient details regarding its data, e.g., which \"multiple databases\" that have been used in the querying, what the genre of literatures is, etc. This drawback considerably undermines the contribution of data curation."}, "questions": {"value": "Please refer to my aforementioned weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xx3zojsjy7", "forum": "ktecmYSZFb", "replyto": "ktecmYSZFb", "signatures": ["ICLR.cc/2026/Conference/Submission22305/Reviewer_w6N2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22305/Reviewer_w6N2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962384141, "cdate": 1761962384141, "tmdate": 1762942160991, "mdate": 1762942160991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SciLitBench, a large-scale benchmark designed to evaluate the automation of systematic literature reviews. The dataset includes 42,980 abstracts, 2,311 full texts, and structured annotations spanning study metadata, PICO elements, outcomes, and evidence labels. The authors benchmark 22 open-source LLMs under recall-weighted objectives and identify several key factors influencing reliability, including scaling behavior, explicit inclusion/exclusion prompting, and context-length."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper addresses a valuable problem domain (systematic review automation).\n\nOpen the dataset and evaluation rules\n\nIncludes long-context evaluations and structured extraction\n\nAttempts to establish practical design principles for reliable LLM screening"}, "weaknesses": {"value": "**Major Issues:**\nWhen presenting a new benchmark, it is standard to provide descriptive statistics that give the reader a clear sense of what the dataset covers. Unfortunately, the manuscript and appendix do not provide sufficient detail to understand the main contribution. Even after multiple readings, I still do not have a concrete grasp of the dataset or a clear understanding of how each task was evaluated. Additional clarity is needed.\n\nThe paper also lists four contributions; however, not all of them are thoroughly validated. For example, the human-in-the-loop annotation framework is presented as a contribution, but there is no dedicated section in the appendix rigorously evaluating its different versions or comparing it against prior work. This weakens the empirical support for the claimed contribution. Similarly, not really clear what contribution three lists.\n\n**Additional issues**\n1. The dataset topic scope is very narrow (AI-for-literature-review papers), reducing generalizability. Ideally, the benchmark would include more topics instead of a **single topic**. \n\n2. Semi-automated label extension using LLMs increases the risk of label propagation bias. What measures are authors taken to mitigate this bias?\n\n3. Similarly, the bias of the annotations can extend to the evaluation. For example, the use of proprietary frontier models to label the majority of the dataset risks circular validation.\n\n\n4. There is a lack of statistical significance tests or inter-annotator agreement reporting. Please at least consider adding confidence intervals to your results.\n\n5. Statements made in the manuscript implying trustworthy automation are premature, given small validation sets and known LLM hallucination/citation issues. \n\n**Minor Issues:**\n\n1.- There is extensive prior work benchmarking LLMs on systematic review tasks, long-context processing, and structured extraction pipelines. Please expand prior work to include more of these works and contrast how your contribution differs.\n\n2.- This work positions itself as enabling end-to-end systematic review automation, but the benchmark still only simulates isolated filtering/extraction steps rather than a true full-cycle review. This is ok, just make sure to be clearer in the intro and abstract.\n\n3.- Correct abstract (for example, remove TODO) and other mentions of TODOs in the manuscript."}, "questions": {"value": "Please address questions above ^ \n\nAlso, why are there so many mentions of the applications for systematic reviews in medicine if the single topic covered in this dataset is AI for literature review?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lXafw42Hl9", "forum": "ktecmYSZFb", "replyto": "ktecmYSZFb", "signatures": ["ICLR.cc/2026/Conference/Submission22305/Reviewer_JA7t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22305/Reviewer_JA7t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045414778, "cdate": 1762045414778, "tmdate": 1762942160694, "mdate": 1762942160694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}