{"id": "yB23AQUuCd", "number": 18258, "cdate": 1758285707963, "mdate": 1759897116062, "content": {"title": "Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation", "abstract": "Despite the increasing use of large language models for creative tasks, their outputs often lack diversity. Common solutions, such as sampling at higher temperatures, can compromise the quality of the results. Dealing with this trade-off is still an open challenge in designing AI systems for creativity.\nDrawing on information theory, we propose a context-based score to quantitatively evaluate value and originality. This score incentivizes accuracy and adherence to the request while fostering divergence from the learned distribution. We show that our score can be used as a reward in a reinforcement learning framework to fine-tune large language models for maximum performance. We validate our strategy through experiments considering a variety of creative tasks, such as poetry generation and math problem solving, demonstrating that it enhances the value and originality of the generated solutions.", "tldr": "Drawing on information theory, we propose a context-based score to quantitatively evaluate value and originality that can be used as a reward for tuning language models.", "keywords": ["Computational creativity", "large language models", "reinforcement learning", "natural language generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37dbf29e8091a14d31c5dc3546acaa37d21ae26b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Disclosure: Claude is used to refine the review.\n\nThis paper proposes CoVO, a context-based score derived from mutual information to assess value and originality in neural text generation. The score combines two components: value (log p(x|y), measuring how well the input can be inferred from output) and originality (-log p(y|x), measuring surprisal). CoVO is used as a reward signal in GRPO to fine-tune LLMs for creative tasks. Experiments on poetry generation, math problem solving, and NoveltyBench demonstrate that the method can improve diversity without sacrificing quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The tension between quality and diversity in LLM generation is important, and balancing value with originality is a meaningful objective.\n- Experiments span multiple domains. Testing on poetry, mathematics, and NoveltyBench shows effort to validate across different creative tasks.\n-  The proposed method is simple. Meanwhile, the paper provides detailed implementation guidance, e.g., how to compute the score with autoregressive models and integrate it with GRPO. \n- The release of GutenVerse dataset for poetry evaluation is a useful resource."}, "weaknesses": {"value": "- My main concern is that this paper compares with no existing work on improving creativity of language models, such as DivPO, DRA-GRPO, and DARLING.\n- The leap from mutual information to \"creativity\" lacks rigorous justification. I don't get the claim that log p(x|y) measures \"value\" - if y is relevant to x but y itself is ungrammatical/low-quality, won't log p(x|y) still be high?\n- Computing p(x|y) for autoregressive models requires a workaround (adding prompt q to make y' = y + q) and is not guaranteed to approximate the true posterior."}, "questions": {"value": "- Why is log p(x|y) evaluating \"value\"? If y is ungrammatical/low-quality but still relevant to x, would log p(x|y) still be high?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iD4ICR8EqJ", "forum": "yB23AQUuCd", "replyto": "yB23AQUuCd", "signatures": ["ICLR.cc/2026/Conference/Submission18258/Reviewer_RULH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18258/Reviewer_RULH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956158543, "cdate": 1761956158543, "tmdate": 1762927983336, "mdate": 1762927983336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CoVO (Context-based score for Value and Originality), an information-theoretic score designed to evaluate and encourage both value and originality in neural text generation. The authors derive CoVO from mutual information between model inputs and outputs and demonstrate how it can be used as a reward in reinforcement learning (via GRPO) to fine-tune large language models for creative tasks. Experiments are presented across three domains: poetry generation, mathematical problem solving, and the NoveltyBench benchmark. The authors claim that optimizing for CoVO enhances creativity-oriented metrics without sacrificing correctness."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Clear motivation: addressing the diversity–quality trade-off in creative LLM generation.\n- Application of CoVO within reinforcement learning is technically sound and compatible with modern LLM fine-tuning frameworks (e.g., GRPO).\n- The experiments span both creative (poetry) and analytic (math reasoning) domains, suggesting some generality."}, "weaknesses": {"value": "- Lack of comparative evaluation. The paper does not benchmark CoVO against existing novelty/diversity metrics (e.g., Diversity Is All You Need, intrinsic rewards, novelty search).\n- All experiments rely on similar foundation model families (llama). Experiments on a different family of foundation models would be useful (e.g., Qwen).\n- Qualitative analysis or examples demonstrating that outputs are actually more creative or original would be useful to see.\n- Reported improvements are small and not statistically significant."}, "questions": {"value": "- The CoVO objective may be gameable (Goodhart’s law). Did the authors see any reward hacking happening? How might we prevent such reward hacking of quantitative measures from happening?\n- Are there possible failure modes or adversarial cases where optimizing CoVO harms creativity instead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3qY9lBleFj", "forum": "yB23AQUuCd", "replyto": "yB23AQUuCd", "signatures": ["ICLR.cc/2026/Conference/Submission18258/Reviewer_Gznv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18258/Reviewer_Gznv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972045420, "cdate": 1761972045420, "tmdate": 1762927983007, "mdate": 1762927983007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CoVO, a context-based score rooted in mutual information theory, to quantify value as log p(x|y) and originality as -log p(y|x). This may promote divergence from the model's distribution in LLM-generated text. It employs CoVO as a reward in GRPO fine-tuning to enhance LLM creativity. Tested on poetry generation, math problem solving, and NoveltyBench tasks, the results demonstrate improvements: poetry shows higher tone adherence and lower reproduction rates; math yields better accuracy and diversity; NoveltyBench boosts novelty and quality.\n\nStrength:\n1. The paper is well written and easy to understand.\n2. Comprehensive empirical validation across varied tasks.\n\nWeaknesses:\n1. The concepts of value and originality are not theoretically defined.\n2. Using a reward model as a proxy is known to induce the reward hacking problem.\n\nGiven the vagueness in the task definition, I am leaning towards rejection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to understand.\n2. Comprehensive empirical validation across varied tasks."}, "weaknesses": {"value": "1. The concepts of value and originality are not theoretically defined. The use of P(x|y) and P(y|x) is unjustified. In addition, this also leads to the second issue.\n2. Using a reward model as a proxy is known to induce the reward hacking problem. The reward score can be optimized to a higher level without necessarily improving the proxied quality. Also, the metrics in evaluation (EAD, T-LCS, SBERT, etc.) might suffer from a similar problem as reward hacking (following Goodhart's law)."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9cCdlxgIOD", "forum": "yB23AQUuCd", "replyto": "yB23AQUuCd", "signatures": ["ICLR.cc/2026/Conference/Submission18258/Reviewer_ZVTj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18258/Reviewer_ZVTj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762834538669, "cdate": 1762834538669, "tmdate": 1762927982577, "mdate": 1762927982577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}