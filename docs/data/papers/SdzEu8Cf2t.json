{"id": "SdzEu8Cf2t", "number": 21371, "cdate": 1758316804065, "mdate": 1759896925583, "content": {"title": "Multimodal Generative Recommendation for Fusing Semantic and Collaborative Signals", "abstract": "Sequential recommender systems rank relevant items by modeling a user's interaction history and computing the inner product between the resulting user representation and stored item embeddings. To avoid the significant memory overhead of storing large item sets, the generative recommendation paradigm instead models each item as a series of discrete semantic codes. Here, the next item is predicted by an autoregressive model that generates the code sequence corresponding to the predicted item. However, despite promising ranking capabilities on small datasets, these methods have yet to surpass traditional sequential recommenders on large item sets, limiting their adoption in the very scenarios they were designed to address. We identify two key limitations underlying the performance deficit of current generative recommendation approaches: 1) Existing methods mostly focus on the text modality for capturing semantics, while real-world data contains richer information spread across multiple modalities, and 2) the fixation on semantic codes neglects the synergy of collaborative and semantic signals. To address these challenges, we propose MSCGRec, a Multimodal Semantic and Collaborative Generative Recommender. MSCGRec incorporates multiple semantic modalities and introduces a novel self-supervised quantization learning approach for images based on the DINO framework. To fuse collaborative and semantic signals, MSCGRec also extracts collaborative features from sequential recommenders and treats them as a separate modality. Finally, we propose constrained sequence learning that restricts the large output space during training to the set of permissible tokens. We empirically demonstrate on three large real-world datasets that MSCGRec outperforms both sequential and generative recommendation baselines, and provide an extensive ablation study to validate the impact of each component.", "tldr": "", "keywords": ["Generative Recommendation", "Generative Retrieval", "Residual Quantization", "Recommender Systems"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d572a494c1698945053b67c7706c84cc3c274b10.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a multimodal generative recommendation framework, MSCGRec, whose core ideas are:\n\n- Discretizing textual, visual, and collaborative signals into modality-specific codes and integrating them in a unified autoregressive framework;\n- Introducing RQ-DINO, a self-supervised residual quantization method to learn semantically meaningful visual discrete codes; and\n- Adopting constrained vocabularies and constrained beam search during training and inference to avoid wasting computation on impossible code continuations in large candidate spaces."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. RQ-DINO learns visual discrete codes that emphasize product-level semantics (e.g., category, function) rather than low-level visual details, which is more suitable for recommendation tasks. \n\n2. It uses a teacher–student distillation structure with residual quantization, maintaining discriminative capacity under a relatively small codebook size. \n\n3. Ablation results show that RQ-DINO achieves better recommendation performance than the simpler “feature extraction + post-quantization” pipeline."}, "weaknesses": {"value": "1. The motivation stated in the introduction is not well justified. Multimodal representation learning for generative recommendation has already been extensively explored and is widely accepted in GR literature. Likewise, collaborative signals have long been integrated into sequence modeling. Moreover, large-scale industrial systems have already run GR models on datasets with tens of millions of interactions, so this is no longer a novel entry point.\n\n2. Recent and representative GR methods such as [1] and [2] are missing, even though they reflect the latest technical advances in the field. Without them, the empirical comparison lacks persuasiveness.\n\n3. The paper does not compare RQ-DINO against standard residual quantization or VAE/VQ-VAE approaches. Additionally, there is no quantitative report on memory footprint, inference latency, or real-time cost of constrained beam search. These aspects are essential to support claims about scalability.SASRec \n\n4. The SASRec baseline does not appear to incorporate multimodal inputs (text or image features). Hence, part of the observed advantage may stem from richer multimodal information rather than from the generative modeling paradigm itself.\n\n5. The collaborative codes are derived from SASRec’s final item embeddings, but the pipeline description is vague. Details about how these embeddings are trained, whether they are frozen, and whether only training data are used are not provided. This ambiguity raises questions about both computational complexity and the fairness of comparison.\n\n6. The technical presentation is overly abstract and lacks sufficient formalization to ensure reproducibility. For instance, how are the constraints constructed and applied during training and decoding? How is the search space pruned, and how does complexity scale with catalog size? What are the specific hyperparameters (codebook size, number of quantization layers, loss formulation)? Without concrete equations, pseudocode, or implementation details, it is difficult to assess the practical feasibility and replicability of the method.\n\n[1] Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations. ICML 2024.\n[2] Contrastive Quantization based Semantic Code for Generative Recommendation. CIKM 2023."}, "questions": {"value": "Please refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NtbQ3Gboht", "forum": "SdzEu8Cf2t", "replyto": "SdzEu8Cf2t", "signatures": ["ICLR.cc/2026/Conference/Submission21371/Reviewer_tZ7k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21371/Reviewer_tZ7k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727917105, "cdate": 1761727917105, "tmdate": 1762941728657, "mdate": 1762941728657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **MSCGRec**, a generative recommender that represents items as discrete code sequences and explicitly **fuses text, image, and collaborative signals**. It is motivated by the memory burden of traditional sequential recommenders on large catalogs and by the observation that prior Gen-Rec models either over-index on text or fail to couple semantics with co-occurrence signals. MSCGRec tackles this by: (1) treating **collaborative features** (from a strong sequential model) as a *separate modality* alongside text and image; (2) introducing **RQ-DINO**, a self-supervised image quantization method aligned to semantics rather than reconstruction; and (3) a **constrained sequence learning** objective that normalizes only over *permissible* code paths, improving scalability. On large, modern datasets (Amazon-2023 Beauty/Sports; PixelRec), the method **matches or surpasses** strong sequential and generative baselines on most metrics, with ablations supporting the role of constrained training, positional encodings, and multimodal quantization. The work is significant as a concrete path toward **scalable generative + multimodal + collaborative** recommendation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **High Significance and Impact:** The paper addresses a core limitation of the current generative recommendation (Gen-Rec) paradigm: its failure to outperform strong sequential recommenders on large-scale datasets. By presenting a model that (mostly) matches or exceeds strong baselines like SASRec, this work represents a meaningful step toward the practical adoption of scalable generative models.\n2. **Novel and Effective Fusion:** The core idea of treating collaborative features (i.e., item embeddings from a sequential model) as just another \"modality\" is simple, elegant, and effective. This approach \"seamlessly integrates\" the power of collaborative filtering, which was a key weakness in previous semantic-only generative models.\n3. **Novel Contributions to Scaling:** The paper introduces two valuable techniques for handling large and complex item sets:\n   - **Constrained Sequence Learning:** Modifying the softmax loss to normalize *only* over the set of permissible (valid) code sequences is a key contribution. This cleverly prevents the model from wasting capacity on memorizing the vast space of *invalid* code combinations.\n   - **RQ-DINO:** The proposed self-supervised quantization method for images, which integrates Residual Quantization directly into the DINO framework, is a novel approach. It avoids a standard reconstruction loss to focus on capturing semantically meaningful information relevant for recommendation.\n4. **Strong Empirical Validation:** The authors use three large-scale, modern datasets (Amazon 2023, PixelRec) , which are noted to be \"an order of magnitude larger\"  than those in prior work. The model is benchmarked against a comprehensive set of strong sequential and generative baselines."}, "weaknesses": {"value": "1. **Lack of Statistical Rigor:** The paper reports no confidence intervals or standard deviations over multiple runs. This is a critical omission, as many of the performance improvements are marginal (e.g., on the Beauty dataset, MSCGRec's R@10 of 0.0315 is *lower* than SASRec's 0.0317 ). Without statistical tests, the claim of \"surpassing\" sequential models is not adequately supported.\n2. **Critical Missing Analysis on Collisions:** The model uses a very small codebook (3 levels, 256 entries each) for very large item sets (e.g., 407k in PixelRec ). This must result in massive code collisions. The paper states it uses an \"additional code level... to separate collisions\", but provides **no statistics** on this. It is impossible to judge if the model is learning a rich semantic hierarchy or just relying on a \"de-facto item ID\" at the final level, which would undermine the generative premise.\n3. **Missing Key Ablations:**\n   - **Decoding Target:** The model's performance is reported when decoding the *collaborative* modality's codes. It is unclear if this performance is due to true multimodal fusion or just \"piggybacking\" on the powerful pre-trained SASRec embeddings. An ablation showing performance when decoding the text or image codes is essential and missing.\n   - **Image SSL Choice:** For the Amazon datasets, which contain paired image-text data, the paper justifies using the image-only DINO  but never compares it against quantizing embeddings from a stronger, multimodal model like CLIP.\n4. **Unfair Baseline Comparison (ETEGRec):** The authors state that the ETEGRec baseline's implementation \"struggles to handle the frequent collisions\" and that they had to modify it by \"increasing its capacity\". Comparing a tuned, multi-component model against an ad-hoc, modified baseline is unfair and clouds the claim of superiority over this key SOTA generative model."}, "questions": {"value": "1. **Statistical Rigor:** Can you please provide the **mean and standard deviation** over multiple runs for the key results in Table 2? Specifically, are the gains over SASRec statistically significant?\n2. **Collision Rate Statistics:** This is critical. Please provide statistics on your codebook. For the 3 semantic/collaborative levels, what percentage of items are unique? What is the average number of items that share the same L1-L3 code prefix *before* the final \"collision level\"  is added?\n3. **Decoding Target Ablation:** What is the model's performance when the decoding target is changed from the collaborative codes  to the **text codes** or the **image codes**? This is essential to prove the model is truly fusing information.\n4. **Image Embedding Comparison:** For the Amazon datasets, can you provide a comparison of your RQ-DINO  approach against quantizing embeddings from a pre-trained **multimodal (image-text) model like CLIP**?\n5. **Baseline Fairness (ETEGRec):** Can you clarify the exact modifications made to ETEGRec? Can you provide results using a standard, well-tuned implementation to ensure a fair comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t8luwHq3hD", "forum": "SdzEu8Cf2t", "replyto": "SdzEu8Cf2t", "signatures": ["ICLR.cc/2026/Conference/Submission21371/Reviewer_b46m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21371/Reviewer_b46m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915715822, "cdate": 1761915715822, "tmdate": 1762941728184, "mdate": 1762941728184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MSCGRec is a generative recommender system designed to explain and overcome the performance gap between generative and sequential models on large-scale datasets. Existing generative recommenders often depend too heavily on text semantics and fail to fully exploit collaborative signals. MSCGRec addresses these issues by integrating multiple modalities—text, images, and collaborative features—and treating each as a distinct semantic source. The model introduces several innovations, including self-supervised image quantization with DINO, constrained sequence learning to prevent memorization of invalid codes, and dual position embeddings that capture structural relationships within code sequences."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper advances the emerging generative recommendation paradigm by introducing a principled multimodal framework tha integrates collaborative signals alongside semantic modalities treating them as complementary information sources rather than competing objectives.\n2. The paper is well-structured with clear motivation, comprehensive ablation studies, and transparent implementation details. The progression from problem identification to solution design is logical and easy to follow.\n3. The experimental evaluation demonstrates that MSCGRec achieves significant performance gains over existing generative methods and, critically, becomes the first generative recommender to surpass traditional sequential baselines on large-scale datasets, validating the paradigm's practical viability."}, "weaknesses": {"value": "1. The paper motivates generative recommendation as a solution to the memory and scalability challenges of traditional sequential models. However, MSCGRec fundamentally depends on SASRec embeddings as the collaborative modality, creating a circular dependency. If the generative approach still requires training a full sequential model as a prerequisite, the claimed advantages (reduced memory footprint, avoiding ANN search) become questionable. This undermines the core value proposition of the generative recommendation paradigm.\n2. The methodology lacks principled explanation for why this particular combination of components (DINO, RQ, SASRec embeddings, T5) should work synergistically. The paper reads more as an engineering effort that stacks multiple techniques rather than a theoretically grounded approach. Without formal analysis—such as information-theoretic bounds, convergence guarantees, or at minimum controlled ablations explaining the fusion mechanism—it's unclear whether the gains stem from the multimodal framework itself or simply from leveraging more information sources.\n3. Table 3(c) shows image-only performance is poor (Recall@10: 0.0173 vs 0.0315), indicating minimal contribution from the image modality. Why train a custom RQ-DINO encoder from scratch instead of using pre-trained CLIP embeddings? The paper provides no cost-benefit analysis to justify this added complexity given the modest gains.\n4. The multi-stage training pipeline and constrained beam search at inference introduce considerable complexity. The paper provides no discussion of total training time, inference latency, or wall-clock comparisons with baselines, making it impossible to assess the practical trade-offs between the modest accuracy gains and substantially increased computational overhead."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bISK9laFSt", "forum": "SdzEu8Cf2t", "replyto": "SdzEu8Cf2t", "signatures": ["ICLR.cc/2026/Conference/Submission21371/Reviewer_AiRL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21371/Reviewer_AiRL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919801570, "cdate": 1761919801570, "tmdate": 1762941727882, "mdate": 1762941727882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MSCGRec, a multimodal generative recommendation framework that unifies text, image, and collaborative filtering (CF) signals into a discrete token space, enabling a generative model to jointly leverage semantic and collaborative knowledge. The method introduces RQ-DINO for semantically stronger visual quantization and a constrained prefix-tree guided decoding strategy to prevent shortcut generation. Experiments on large-scale datasets show that MSCGRec achieves substantial improvements and is the first generative model to surpass strong sequential recommenders such as SASRec, demonstrating both effectiveness and efficiency gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper motivates the use of RQ for collaborative embeddings by arguing that CF signals inherently possess multi-level semantics. While the overall performance gains and ablations confirm the usefulness of incorporating CF as a discrete modality, there is no diagnostic analysis demonstrating that different RQ levels actually capture distinct semantic granularity (e.g., global clusters vs. fine-grained item distinctions). A more direct validation—such as layer-wise probing, codebook visualization, or semantic clustering across quantization levels—would substantially strengthen this core claim.\n\n2. The proposed RQ-DINO combines DINO-based self-supervised vision features with residual quantization, yielding more semantically meaningful visual codes and improving generative modeling. The prefix-tree guided generation effectively reduces “memorization of valid token combinations” and forces the model to perform more genuine preference reasoning—an important yet underexplored issue in generative recommenders."}, "weaknesses": {"value": "1. The paper assumes that CF embeddings exhibit hierarchical semantics suitable for residual quantization, yet provides no empirical diagnostics (e.g., layer-wise probing or semantic clustering) validating that RQ indeed captures different levels of collaborative structure. This weakens the core motivation for applying RQ to CF signals.\n\n2. Converting CF embeddings into discrete tokens may unintentionally encode implicit item identity, especially in high-capacity codebooks. This raises the concern that the model’s gains may partially stem from memorizing token-to-item mappings rather than genuinely modeling collaborative structure. Although the multimodal and masked-modality experiments provide some indirect evidence of robustness, a more explicit examination—e.g., shuffled-ID evaluation or unseen-item generalization—would be valuable to confirm that CF tokens do not collapse into disguised item IDs.\n\n3. The prefix-tree guided decoding improves performance and mitigates shortcut generation, as supported by ablation results. However, the paper lacks a deeper discussion of its impact on search space, calibration, and diversity. In particular, it is unclear whether hard constraints reduce the discovery of novel or serendipitous items, or whether soft constraint variants or entropy-aware decoding could achieve a better trade-off between correctness and flexibility."}, "questions": {"value": "Please see the weaknesses above.\n\n1. The motivation for applying RQ to collaborative embeddings relies on the assumption that CF contains multi-level semantics. Could authors provide direct evidence supporting this claim? For example, do different RQ levels correspond to different semantic granularities (e.g., global category vs. fine-grained item distinctions)? Or, Can authors provide codebook visualizations, per-level t-SNE plots, or probing classifiers to show that RQ captures progressively finer collaborative signals rather than redundant noise?\n\n2. Since CF embeddings inherently encode item similarity structure, quantizing them into discrete tokens risks encoding implicit item identity. Could authors provide evidence that the model is not relying on memorized token-to-item mappings? Two possible checks: (1) Train with item IDs randomly permuted but keep CF tokenization identical. If performance remains high, this would indicate identity leakage. (2) Can CF tokens generalize to items not seen in training?\n\n3. Prefix-tree constrained decoding improves correctness, but may reduce diversity or calibration. Could authors provide an analysis comparing on (1) Hard vs. soft constraints. (2) Diversity metrics. (3) Whether constrained decoding biases the model toward popular or well-represented code paths? A deeper study would clarify if this method sacrifices flexibility for accuracy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xD00727eKm", "forum": "SdzEu8Cf2t", "replyto": "SdzEu8Cf2t", "signatures": ["ICLR.cc/2026/Conference/Submission21371/Reviewer_VJT9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21371/Reviewer_VJT9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964977754, "cdate": 1761964977754, "tmdate": 1762941727228, "mdate": 1762941727228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}