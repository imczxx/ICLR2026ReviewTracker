{"id": "ofNbGPV6Ve", "number": 17303, "cdate": 1758274457684, "mdate": 1763751711921, "content": {"title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement Learning", "abstract": "Recent advances in image reasoning methods, particularly \"Thinking with Images\", have demonstrated remarkable success in Multimodal Large Language Models (MLLMs); however, this dynamic reasoning paradigm has not yet been extended to video reasoning tasks. In this paper, we propose Video-Thinker, which empowers MLLMs to think with videos by autonomously leveraging their intrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues throughout the inference process. To spark this capability, we construct Video-Thinker-10K, a curated dataset featuring autonomous tool usage within chain-of-thought reasoning sequences. Our training strategy begins with Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group Relative Policy Optimization (GRPO) to strengthen this reasoning capability. Through this approach, Video-Thinker enables MLLMs to autonomously navigate grounding and captioning tasks for video reasoning, eliminating the need for constructing and calling external tools. Extensive experiments demonstrate that Video-Thinker achieves significant performance gains on both in-domain tasks and challenging out-of-domain video reasoning benchmarks, including Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B substantially outperforms existing baselines such as Video-R1 and establishes state-of-the-art performance among 7B-sized MLLMs.", "tldr": "We introduce Video-Thinker, a novel approach that empowers MLLMs to think with videos by autonomously leveraging their intrinsic grounding and captioning capabilities to generate reasoning clues throughout the inference process.", "keywords": ["Video reasoning", "Multimodal large language model", "Thinking with videos"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2434c327ac439ca9afeb57c45297d6f58cbecc77.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Video-Thinker, a novel framework that extends the “Thinking with Images” paradigm to videos by enabling MLLMs to autonomously perform temporal reasoning through intrinsic “grounding” and “captioning” capabilities without relying on external tools. The authors introduce Video-Thinker-10K, a curated dataset of 10K samples with structured chain-of-thought annotations that include temporal localization, visual descriptions, and analytical reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Innovative “Thinking with Videos” Paradigm: The work successfully adapts dynamic visual reasoning to the more complex video domain by integrating temporal grounding and captioning directly into the reasoning chain, enabling MLLMs to autonomously navigate video content.\n\n2. Efficiently Curated Dataset: Video-Thinker-10K is thoughtfully constructed using a hindsight-curation pipeline that ensures reasoning traces are both relevant and effective, achieving strong performance with only 10K samples."}, "weaknesses": {"value": "1. Video-Thinker-SFT-7B underperforms Qwen2.5-VL-7B on many out-of-distribution benchmarks, with the primary performance gains attributed to reinforcement learning. However, since the construction of SFT data is highlighted as a key contribution of this work, a critical ablation study is missing: specifically, what would happen if RL were applied directly to Qwen2.5-VL-7B without the SFT stage? Such an experiment is essential to determine whether the SFT phase actually hinders final performance.\n\n2. The paper lacks evaluations on standard video understanding benchmarks such as Video-MME and MVBench, as well as comparisons against recent video reasoning models like VersaVid-R1 and VideoRFT. It remains unclear how much Video-Thinker improves upon these baselines, limiting the assessment of its overall effectiveness and competitiveness."}, "questions": {"value": "1. When generating captions for different time segments, how are the temporal boundaries (i.e., the start and end times) of each caption determined?\n\n2. From the provided examples, it appears that the model performs captioning on a per-segment basis across the entire video. What are the advantages of this segmented captioning approach compared to generating a single holistic caption for the entire video?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "duUqTwROu1", "forum": "ofNbGPV6Ve", "replyto": "ofNbGPV6Ve", "signatures": ["ICLR.cc/2026/Conference/Submission17303/Reviewer_Fvzf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17303/Reviewer_Fvzf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760604705643, "cdate": 1760604705643, "tmdate": 1762927240312, "mdate": 1762927240312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes an automatically generated dataset of 10k annotations applied to videos from existing video datasets. It shows that fine-tuning Qwen2.5-VL-7B-Instruct on this data by using a two-stage training strategy consisting of SFT followed by GRPO yields strong performance on various video understanding benchmarks. The automatically generated dataset is composed of two types of data: i) data with temporally assigned captions, and ii) data with global instead of temporal questions and answer pairs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a carefully created dataset of annotations yielding strong performance results on several video understanding benchmarks. The code is (or will be) made publicly available."}, "weaknesses": {"value": "The paper describes a data engineering approach to improving performance on a variety of video understanding benchmarks. While the performance appears to be strong overall, I do not find the paper particularly scientifically insightful or revealing. Specifically, I am not surprised that for the given set of video benchmark tasks (Video-Holmes, CG-Bench-Reasoning and VRBench), a careful selection of DeepSeek-R1-assisted and Gemini-assisted annotations on a careful selection of existing video datasets can improve the performance over the Qwen2.5-VL-7B-Instruct baseline and starting point. Importantly, I am a bit confused about some statements made in the paper (see questions below)."}, "questions": {"value": "Is the performance on Video-Holmes (but the question could apply similarly to the other benchmark results) based on the same test-set as the results on the official Leaderboard? Does the model described in this paper currently not appear there to retain anonymity of the submission and will appear it there after anonymity is lifted? \n\nI do not quite understand the statement in Line 320 onward: “For the in-domain evaluation, since the TutorialVQA (…) training set contains only 76 samples, we do not construct a corresponding test set. Instead, we derive held-out test sets from the five training datasets…” First, I do not understand how and why the limitation of TutorialVQA affects the choice of test-set selection for the other datasets. Can you elaborate? Second, I wonder whether the performance figures reported in the paper (for example, Table 1) are based on the identical train-test splits across all models or not. Can you please clarify?\n\nDo you expect the choice of source datasets, annotation scheme and training approach detailed in the paper to potentially degrade rather than improve performance on certain video-related tasks? Or do you expect these to be \"universally relevant\" to most if not all video-understanding benchmark tasks one can imagine? It would be nice to better understand the potential trade-offs and limitations besides the performance-improvements on existing benchmarks.\n\nHow were good values for the hyperparameters (such as beta, weight decay, data mix, etc.) determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IGBbc0yIhw", "forum": "ofNbGPV6Ve", "replyto": "ofNbGPV6Ve", "signatures": ["ICLR.cc/2026/Conference/Submission17303/Reviewer_Hmd7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17303/Reviewer_Hmd7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854403816, "cdate": 1761854403816, "tmdate": 1762927239878, "mdate": 1762927239878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “VideoThinker: Sparking Video Understanding with Reasoning” introduces VideoThinker, a unified framework that integrates multimodal large language model (MLLM) reasoning with video understanding. Its core innovation lies in enabling a system that not only analyzes video content but also “thinks”—reasoning about temporal dynamics, spatial consistency, and logical event sequences before producing final predictions. The framework consists of three main components: a Video Reasoner, which performs step-by-step multimodal reasoning based on an LLM backbone; a Video Analyzer, which interprets high-level reasoning outputs to extract structured video understanding; and a Video Evaluator, which provides feedback for iterative refinement. The authors also construct a Video-ReasonBench to evaluate reasoning ability in video understanding tasks and show that VideoThinker surpasses existing transformer- and diffusion-based baselines on both quantitative metrics and human assessments. The results demonstrate that incorporating explicit reasoning significantly enhances temporal comprehension and causal inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides an extensive comparison with several contemporaneous approaches such as Video-R1, Temporal-R1, and Time-R1. This helps readers clearly understand the distinctions and advantages of the proposed method under a similar technical framework (GRPO), enhancing the paper’s contextual clarity.\n\n2. The authors propose a new dataset tailored for video understanding and reasoning tasks, which effectively improves the efficiency and stability of reinforcement learning (RL) training. This contribution adds practical value and could benefit future research in the field.\n\n3. The proposed method demonstrates impressive generalization ability under OOD settings, indicating that the model captures robust reasoning and compositional skills beyond the training distribution."}, "weaknesses": {"value": "1. Although the paper compares with several GRPO-based methods, the baselines are relatively narrow in scope. Including more competitive and diverse video understanding models would strengthen the claim of GRPO’s effectiveness in video reasoning tasks.\n\n2. The approach supplements reasoning traces using large language models, which may introduce hallucinations or inaccurate information. It remains unclear whether the textual reasoning genuinely contributes to more accurate or meaningful reasoning steps; an ablation or validation study would clarify this.\n\n3. While the overall system design is well-structured, the use of GRPO itself is not highly novel in the current research landscape. The paper would benefit from emphasizing deeper algorithmic innovation or unique adaptation of GRPO specifically tailored to video reasoning."}, "questions": {"value": "If the authors can address or experimentally validate the weaknesses, especially by expanding the baseline comparisons, verifying the accuracy of LLM-based reasoning traces, and clarifying the novelty of the GRPO application, I would consider increasing my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HsxTRtlsl1", "forum": "ofNbGPV6Ve", "replyto": "ofNbGPV6Ve", "signatures": ["ICLR.cc/2026/Conference/Submission17303/Reviewer_LTjt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17303/Reviewer_LTjt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146012620, "cdate": 1762146012620, "tmdate": 1762927239367, "mdate": 1762927239367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper leverages existing open-source video datasets and utilizes the capabilities of large models to obtain a training dataset, Video-Thinker-10K, which includes question-answer pairs and chain-of-thought annotations. During the training process, the Video-Thinker-7B model was trained using the SFT+GRPO training strategy, outperforming several existing large model approaches on several common video QA datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written, and the specific prompt design for the dataset construction process is also well-explained. \n\nThe chain-of-thought (CoT) data annotation for video reasoning represents a notable contribution.\n\nThe phenomena observed during the chain-of-thought training process provide valuable insights."}, "weaknesses": {"value": "The paper's technical contribution is limited. \nThe CoT annotations for video labeling primarily rely on the capabilities of the DeepSeek and Gemini models. \n\nThe training process of Video-Thinker-7B lacks contrution, as it mainly adopts the conventional approach of SFT+GRPO."}, "questions": {"value": "1. During the data generation process, how can the hallucination phenomenon in the automatic annotation of large models be addressed?  \n2. In the training of Video CoT, what are the specific differences compared to GRPO training in Language or Image CoT?  \n3. Could more comprehensive training details be provided, such as the number of T?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NHn7AigiyM", "forum": "ofNbGPV6Ve", "replyto": "ofNbGPV6Ve", "signatures": ["ICLR.cc/2026/Conference/Submission17303/Reviewer_1Y79"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17303/Reviewer_1Y79"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173715385, "cdate": 1762173715385, "tmdate": 1762927238865, "mdate": 1762927238865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Repsonse to all reviewers"}, "comment": {"value": "We first summarize our response and the results of additional suggested experiments here. We have also responded to the specific concerns of each reviewer as individual comments below.\n\nAll reviewers recognized the value of our proposed \"Thinking with Videos\" paradigm. To address concerns regarding technical contribution, robust evaluation, and training mechanisms, we have conducted extensive new experiments:\n\n1.  ***Validation of Technical Contribution (Endogenous vs. External):*** To address concerns regarding technical novelty and contribution (Reviewer 1Y79, LTjt), we demonstrated the distinct superiority of our intrinsic training paradigm over currently emerging tool-use frameworks. We compared Video-Thinker-7B against pipelines utilizing larger models (including Qwen2.5-VL-72B) as external tools. Our 7B model significantly outperforms even the 72B-augmented pipeline (e.g., +9.26% on Video-Holmes). This result empirically substantiates our core contribution: internalizing grounding/captioning capabilities is far more effective than external function calls, which suffer from error propagation and context misalignment. These results are reported in Section 4.4 with a case study in Appendix G.2. We also have modified the introduction section to further clarify our contributions.\n\n2.  ***Extended Benchmarks and Baselines:*** We expanded our evaluation to include **5 additional strong baselines** (VersaVid-R1, VideoRFT, MiMo-VL, VR-Thinker, Video-RTS) and **3 new out-of-domain benchmarks** (VideoMME, SciVideoBench, VideoTT) as requested by Reviewers LTjt and Fvzf. Video-Thinker-7B achieves state-of-the-art performance across nearly all metrics, confirming its robust generalization capabilities beyond the initial dataset selection. All these results are available in Table 1.\n\n3.  ***Training Strategy Ablation:*** We conducted a \"Pure RL\" ablation (Reviewer Fvzf) to verify the necessity of our two-stage training. Results show that applying RL directly to Qwen2.5-VL-7B without SFT leads to significant performance degradation, confirming that SFT is essential for cold-starting the structured format following required for effective RL exploration. These results have been added in Table 1.\n\n4.  ***Data Quality and Hallucination Analysis:*** We performed a controlled study comparing synthetic vs. ground-truth captions (Reviewer 1Y79, LTjt). While synthetic captions naturally underperform human annotations, they still yield a **+9% accuracy gain** over the baseline. This confirms that our hindsight-curation mechanism effectively filters destructive hallucinations, ensuring the synthesized reasoning traces serve as a beneficial training signal. We have added these new results in Appendix E.\n\nFor your convenience, all modifications in the updated paper PDF are highlighted in blue color."}}, "id": "qJx0QVQ1RD", "forum": "ofNbGPV6Ve", "replyto": "ofNbGPV6Ve", "signatures": ["ICLR.cc/2026/Conference/Submission17303/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17303/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission17303/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763748954017, "cdate": 1763748954017, "tmdate": 1763752226460, "mdate": 1763752226460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}