{"id": "uIJyYkOgAy", "number": 16621, "cdate": 1758266847170, "mdate": 1759897229054, "content": {"title": "Can Large Language Models Match the Conclusions of Systematic Reviews?", "abstract": "Systematic reviews (SR), in which experts summarize and analyze evidence across\nindividual studies to provide insights on a specialized topic, are a cornerstone\nfor evidence-based clinical decision-making, research, and policy. Given the exponential growth of scientific articles, there is growing interest in using large\nlanguage models (LLMs) to automate SR generation. However, the ability of\nLLMs to critically assess evidence and reason across multiple documents to provide recommendations at the same proficiency as domain experts remains poorly\ncharacterized. We therefore ask: Can LLMs match the conclusions of systematic\nreviews written by clinical experts when given access to the same studies?\nTo explore this question, we present MedEvidence, a benchmark pairing findings\nfrom 100 SRs with the studies they are based on. We benchmark 24 LLMs on\nMedEvidence, including reasoning, non-reasoning, medical specialist, and models\nacross varying sizes (from 7B-700B). Through our systematic evaluation, we find\nthat reasoning does not necessarily improve performance, larger models do not\nconsistently yield greater gains, and knowledge-based fine-tuning degrades accuracy on MedEvidence. Instead, most models exhibit similar behavior: performance\ntends to degrade as token length increases, their responses show overconfidence,\nand, contrary to human experts, all models show a lack of scientific skepticism\ntoward low-quality findings. These results suggest that more work is still required\nbefore LLMs can reliably match the observations from expert-conducted SRs, even\nthough these systems are already deployed and being used by clinicians. We release our codebase\nand benchmark\nto the broader research community to further\ninvestigate LLM-based SR systems.", "tldr": "We introduce MedEvidence, a benchmark to test if LLMs can replicate expert systematic reviews.", "keywords": ["Benchmarks", "Multi-document Reasoning", "Medical AI"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f93ecf88d9c5f5faa4b48a315e9c582f27adf8ab.pdf", "supplementary_material": "/attachment/65c77fbf40f3ac6b80f80acb6b792f624fd46379.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduced MedEvidence, a benchmark based on human-annotated systematic reviews, to test the LLMs’ ability in question answering based on medical context. Various LLMs are then evaluated and compared on the benchmark."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* All test cases are manually curated based on existing Cochran meta-analyses.\n* A large number of LMs are evaluated on the benchmark."}, "weaknesses": {"value": "* The soundness of the benchmark is heavily based on the annotation quality, but there is no discussion about the annotators’ background.\n* The only task provided by MedEvidence so far is a multiple-choice/classification task, as the model has to answer one of five given treatment outcome effects. Please see the questions for details.\n* There are no numerical experimental statistics in the main paper. All results are presented in plots."}, "questions": {"value": "* Line 252-257: The categorization is directly based on DeepSeek, which is not rigorous enough. Also, in the remaining part of the paper, the authors did not utilize the categorization in the evaluation/analysis.\n* Figure 6: It is hard to distinguish the medically finetuned models from the others via the thickness of the margin. Making notes on the y-axis labels can be helpful (for example, adding * for medical models)\n* The results include a few surprising results. For example, reasoning models, larger models, and domain-adapted models do not improve performance. Case studies discussing potential causes for these results can be helpful."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "Not all papers sourced from PubMed are fully open-access; for instance, some may restrict commercial use. However, the licensing information for the papers included in MedEvidence is not discussed in the main text or appendix."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IFRewAbROF", "forum": "uIJyYkOgAy", "replyto": "uIJyYkOgAy", "signatures": ["ICLR.cc/2026/Conference/Submission16621/Reviewer_4Htb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16621/Reviewer_4Htb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760911832136, "cdate": 1760911832136, "tmdate": 1762926689560, "mdate": 1762926689560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents two contributions: (1) a dataset of systematic reviews from Cochrane, and (2) an assessment of LLM ability to generate a conclusion over the SR evidence strength."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "This is a well written paper with a scoped contribution that explicitly tests modern LLMs (incl. reasoning models) ability to synthesize evidence across systematic reviews; the analysis is solid and has high relevance for clinical settings where these models may be deployed."}, "weaknesses": {"value": "My main concern with this work is the lack of adequate comparison with prior work. It’s unclear whether the dataset itself is a novel contribution; there is a lack of interaction with prior work on evaluating LLM evidence synthesis and existing cleaned SR datasets:  \n\n[1] Three datasets built similarly: TrialReviewBench (https://arxiv.org/html/2407.00631v2), https://arxiv.org/abs/2008.11293, and https://aclanthology.org/2024.acl-srw.42/. \n\n[2] There is some prior work that finds similar conclusions regarding overconfidence in LLM responses, and lack of ability to synthesize evidence (https://pmc.ncbi.nlm.nih.gov/articles/PMC11613457/; https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2838106) \n\nI do believe this paper has a good set of experiments over models that benefits the community, especially since it’s clear that these modern models still have the same issues as prior iterations. I am happy to increase my score given the better grounding against prior work and datasets, and justification for why new labels/questions were generated for this task (see questions below)."}, "questions": {"value": "1. There is also a difference between general systematic reviews and randomized control trials (RCTs). Since this is sourced from Cochrane, I assume they are all RCTs? Could you confirm this? \n    (a) Often there are established research questions that are specific and provided in these reviews. Why are these questions not being used directly? \n\n2. For the dataset generation, there does not appear to be any assessment of annotator agreement or the quality of the question conversions, apart from the source concordance analysis. However, this measure alone is insufficient, it primarily reflects whether a question can be answered using a single source, which does not capture true evidence synthesis. How is quality is assessed among annotators? \n\n3. Could you clarify the motivation for mapping to *new* labels for the model to synthesize? I am concerned that *“X increases Y”* or *“X may reduce Y”* don’t imply “higher” or “lower” labels, since direction alone doesn’t indicate desirability or certainty of effect.\n\n4. Could you clarify the difference and novelty introduced by your dataset relative to the listed prior works? \n\n(Clarity) L246 typo: should be “use an LLM” not “use an LLMs”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZsQF4D0c1P", "forum": "uIJyYkOgAy", "replyto": "uIJyYkOgAy", "signatures": ["ICLR.cc/2026/Conference/Submission16621/Reviewer_zrHx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16621/Reviewer_zrHx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761004166954, "cdate": 1761004166954, "tmdate": 1762926689052, "mdate": 1762926689052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MedEvidence, a benchmark of closed-form QA items distilled from Cochrane systematic reviews, pairing each conclusion with the same source studies experts used. LLMs (reasoning/non-reasoning; medical-finetuned/generalist) are evaluated; key findings: reasoning/scale/medical-finetuning don’t reliably help, performance drops with long inputs, models are overconfident and insufficiently skeptical of low-quality evidence. Even frontier models trail a time-constrained expert baseline."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "S1) This is a well-posed task and this paper makes targeted contributions. The paper removes retrieval and long-summary grading by converting conclusions into closed-QA and evaluating exact match answers. \n\nS2) Evaluation is reasonbly transparent with limited uncertainty. Metrics (e.g. per-class recall, accuracy, evidence uncertainty w/ source concordance) reliably support key findings. Work is methodically sound. \n\nS3) Clear empirical takeaways -- I am largely satisfied with their takeaways of frontier models underperforming time-constrained experts, etc."}, "weaknesses": {"value": "W1) I think perhaps a reasonable weakness/questions here is re conceptual novelty and whether this paper is suited for ICLR. Essentially authors do a dataset+evaluation work with closed-class answers. While useful, it advances prior factuality/evidence-reasoning datasets incrementally (i.e. Table 1) and centers on mapping mapping SR conclusions to a QA rather than introducing new modeling/eval methods. \n\nW2) LLM-derived source concordance. While I think this is a reasonable thing to do for eval/metric purposes, I think there is a potential circularity/model bias element to it. And we're introducing those limitations into a key explanatory variable. \n\nW3) Evaluation pipeline (maybe) order-sensitive. When context overflows, and answers are refined over seqs of artile chunks; I am not sure how randomization effects play into the analysis of main text."}, "questions": {"value": "Q1) On source concordance -- Beyond using DeepSeek, did you validate concordance against human judgments on a subset to ensure this variable itself isn't model induced? \n\nQ2) For time-constrained expert line (Fig 4a), how many experts participated, how were specialities matched to questions, and what agreement (e.g., κ/ICC) did you observe?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "viSJ7dfCLT", "forum": "uIJyYkOgAy", "replyto": "uIJyYkOgAy", "signatures": ["ICLR.cc/2026/Conference/Submission16621/Reviewer_WvC2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16621/Reviewer_WvC2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913918857, "cdate": 1761913918857, "tmdate": 1762926688593, "mdate": 1762926688593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}