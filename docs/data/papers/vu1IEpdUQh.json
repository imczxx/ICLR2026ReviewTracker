{"id": "vu1IEpdUQh", "number": 14572, "cdate": 1758239237403, "mdate": 1763710520846, "content": {"title": "FlowSymm: Physics–Aware, Symmetry–Preserving Graph Attention for Network Flow Completion", "abstract": "Recovering missing flows on the edges of a network, while exactly respecting local conservation laws, is a fundamental inverse problem that arises in many systems such as transportation, energy, and mobility. We introduce FlowSymm, a novel architecture that combines (i) a group-action on divergence-free flows, (ii) a graph-attention encoder to learn feature-conditioned weights over these symmetry-preserving actions, and (iii) a lightweight Tikhonov refinement solved via implicit bilevel optimization. The method first anchors the given observation on a minimum-norm divergence-free completion. We then compute an orthonormal basis for all admissible group actions that leave the observed flows invariant and parameterize the valid solution subspace, which shows an Abelian group structure under vector addition. A stack of GATv2 layers then encodes the graph and its edge features into per-edge embeddings, which are pooled over the missing edges and produce per-basis attention weights. This attention-guided process selects a set of physics-aware group actions that preserve the observed flows. Finally, a scalar Tikhonov penalty refines the missing entries via a convex least-squares solver, with gradients propagated implicitly through Cholesky factorization. Across three real-world flow benchmarks (traffic, power, bike), FlowSymm substantially outperforms state-of-the-art baselines in RMSE, MAE and correlation metrics.", "tldr": "FlowSymm is an end-to-end graph neural model that completes missing flows by enforcing a divergence-free group-action prior, scoring corrections with attention, and refining with feature-conditioned Tikhonov regularization", "keywords": ["graphs", "networks", "flow graphs", "graph attention networks", "group action", "bilevel-optimization", "physics-aware graph neural networks"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b3e1d6f92765a7c0833a006f122585baa833c29.pdf", "supplementary_material": "/attachment/64543588602e2348957ff8821e15ddc0f5206c70.zip"}, "replies": [{"content": {"summary": {"value": "FlowSymm first anchors a balanced solution, then builds the feasible subspace defined by conservation and frozen observations, and updates only within that subspace via a group-action using learned weights over basis directions. A small Tikhonov-style refinement with implicit differentiation absorbs noise. Experiments on traffic, power, and bike datasets show improvements over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Cleanly hard-enforces conservation and frozen observations via a feasible subspace; updates are interpretable as combinations of physical modes; solid gains on traffic/power/bike."}, "weaknesses": {"value": "No test of basis-choice invariance; missing a predict-then-project baseline to isolate the benefit of the basis parameterization; limited analysis of noise/bias and λ trade-offs, with potential DOF overestimation when extra linear constraints exist."}, "questions": {"value": "1. Are results invariant to basis rotations within the same subspace A? Does the claimed “group action” imply basis-choice invariance?\n2.  Add a predict-then-project baseline, train a GNN to output unconstrained \\(z\\) and project \\(\\Delta=P_A z\\), to verify that gains come from the basis parameterization rather than projection alone. \n3. How do measurement noise/bias in \\(c\\) affect (i) the anchor \\(f^{(0)}\\), (ii) the subspace/basis \\(A,U\\), and (iii) final predictions? \n4. What is the learned \\(\\lambda\\) range, and how does it control the constraint residual \\(\\|Bf - c\\|\\) vs. reconstruction error?\n5. The “degrees of freedom” in §3.2 are only defined with respect to the “divergence-free + fixed observations.” But if an unknown cycle has a known injection, then the values on that cycle are actually determined. Does this overstate the DOF? Is that good or bad in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JdNc7yt28y", "forum": "vu1IEpdUQh", "replyto": "vu1IEpdUQh", "signatures": ["ICLR.cc/2026/Conference/Submission14572/Reviewer_yn4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14572/Reviewer_yn4E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760710501095, "cdate": 1760710501095, "tmdate": 1762924960459, "mdate": 1762924960459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript addresses edge-flow imputation under conservation on graphs. The proposed method comprises three key components: (1) an initial minimum-norm balanced completion that satisfies B f = c while keeping observed edges fixed; (2) a physics-aware group-action subspace for all observed edges from which an orthonormal basis U is constructed (truncated to k columns), and attention over this basis selects divergence-free corrections that cannot alter observed edges; (3) a lightweight Tikhonov refinement solved as a single SPD system with exact implicit differentiation via a Cholesky/CG solve. Experiments on Traffic, Power, and Bike benchmarks demonstrate consistent gains over a broad set of baselines, with ablations for basis size, attention, and the bilevel/implicit components."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear physics prior with strict guarantees: the anchor solution and group-action construction ensure corrections remain on the B f = c manifold and do not alter sensor edges.\n2. Interpretability: attention over ker(B) basis elements (zero on observed edges) admits a physical reading as redistributions consistent with conservation; reported sparsity aids inspection.\n3. Efficient inner solver with exact hypergradients: the Tikhonov layer yields an SPD system amenable to stable Cholesky/CG solves; implicit differentiation avoids unrolling.\n4. Thorough baselines and ablations across physics-free, physics-soft, and bilevel methods; ablations isolate the effect of k, group actions, attention, and bilevel training. Reported gains are consistent across RMSE/MAE/CORR and across domains (traffic, power, bikes)."}, "weaknesses": {"value": "1. The core components of the proposed model, such as the single regularization weight and the global attention, assume uniform noise and physics, limiting its flexibility in handling more complex, heterogeneous networks.\n2. While the paper claims physics-awareness, it only evaluates data-fit metrics (like RMSE) and fails to directly measure how well the final predictions actually adhere to the physical conservation laws.\n3. The comparison could be improved by including other relevant physics-informed baselines in addition to GNNs; meanwhile, key details about the basis vector sorting method are unclear."}, "questions": {"value": "1.\tA single global λ seems suboptimal if the noise isn't uniform. If some parts of the network are much noisier, how does the model adapt? Dose learning a different λ for each edge provide the flexibility needed to handle different noise levels?\n2.\tMany state of the art variants of GNN are selected as baseline, however, some other physics informed machine learning model (with similar ideas) may also perform well (thus be a good baseline candidate) on such tasks, e.g. [1] (transformer with interpretable basis), [2] (neural operator with learnable basis).\n3.\tThe proposed model is claimed to have a good balance in physics and noisy data, however, the performance on data is verified on better RMSE/MAE/CORR, then what about physics? E.g. is the Kirchhoff’s law respected better in the power case？\n4.\tAre the k basis vectors sorted by singular value? Does this mean you're assuming that large-scale flow patterns are important, while small, local ones can be ignored?\n5.\tYour global attention applies one uniform fix across the entire graph. How does it handle specific local regions that have completely different or more complex physics? Does the model fail to capture these important local dynamics?\n\n[1] Cao, Shuhao. \"Choose a transformer: Fourier or galerkin.\" NeurIPS (2021): 24924-24940.\n[2] Lu, Lu, et al. \"A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data.\" Computer Methods in Applied Mechanics and Engineering 393 (2022): 114778."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JOeRraQmQu", "forum": "vu1IEpdUQh", "replyto": "vu1IEpdUQh", "signatures": ["ICLR.cc/2026/Conference/Submission14572/Reviewer_MoiE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14572/Reviewer_MoiE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761692504332, "cdate": 1761692504332, "tmdate": 1762924959760, "mdate": 1762924959760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Given partial and noisy measurements on the edges of a graph, this paper addresses the problem of completing the missing measurements subject to conservation laws. This problem arises in applications such as transportation and power planning, where accurate measurements cannot be obtained for all edges. The authors’ approach has several steps. First, an initial solution satisfying conservation (with external sources) is obtained by solving a linear system. Then, this solution is refined through a learning procedure that incorporates measured features. Because the refinement is restricted to the null space of valid solutions, conservation is guaranteed to be preserved. The proposed approach is evaluated against competing methods on several tasks and demonstrates favorable performance."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The problem addressed in the paper is important and grounded in real-world applications where measurements are an expensive resource. The use of neural attention for feature-driven completion is compelling. State-of-the-art results presented on three datasets demonstrate strong potential."}, "weaknesses": {"value": "The biggest issue I have with this work is its **presentation**. While the exposition is mostly clear in terms of English, it feels unnecessarily complicated and often hinders understanding. As a non-expert, I found myself sifting through dense sentences overloaded with terminology, trying to distill the essence of each component of the work, including the problem itself, but especially the proposed approach. This issue is apparent as early as the abstract, which I understood better after reading the paper, yet still find quite convoluted. Ultimately, I must admit that I am not confident I fully understand the entirety of the proposed approach. I believe I have a reasonable high-level understanding of the procedure, but the presentation seems unnecessarily complex. Consequently, I believe the paper requires significant work, and I cannot recommend accepting it in its current form. \n\nLine 377 reads: *“physics-aware, symmetry-preserving group actions with attention-based encodings and feature-conditioned refinements.”* I find this statement overstated and misaligned with what I gathered from the manuscript.  \n\nTo illustrate the concerns above, and to raise a few additional ones, I would like to break down the outline of the proposed approach as presented in Section 1.2:  \n\n* **(i)** *“First, we reinterpret admissible divergence-free adjustments as elements of an Abelian group action; by constructing an orthonormal basis and retaining only the leading k basis vectors, we obtain a tractable latent space that scales linearly with the number of missing flows. Unlike typical equivariant GNNs, which embed spatial or permutation symmetries, our group-action basis directly spans the algebraic space of divergence-free adjustments tailored to the missing flows.”*  \n  To my understanding, after reading Sections 3.1 and 3.2, this step characterizes the subspace of solutions to the linear system over the missing edges, subject to the available measurements, in a form similar to $x = x_0 + Bz,$ where $x_0$ is a particular solution (e.g., minimum-norm) and the columns of $B$ form a (possibly truncated) basis for the null space of the linear system. If this interpretation is correct, I find the presentation misleadingly complicated (e.g., invoking Abelian groups and “physically informed bases”). It also raises some concerns; for instance, do exact solutions to the completion problem even exist in the presence of noisy measurements? In what sense does the null space provide a good characterization of admissible solutions in the noisy case? If, however, I have misinterpreted the proposed idea altogether, I encourage the authors to clarify and revise the text to avoid such misunderstandings.  \n\n* **(ii)** *“Second, we combine this basis with edge-wise GATv2 embeddings (Brody et al., 2022): the attention ... enabling the model to inject corrective flows ... preserving flow balance on the anchor manifold. This attention-guided selection departs from prior bilevel diagonal regularizers by coupling missing edges through context-dependent weights over physically interpretable basis actions, rather than treating each edge independently.”*  \n  My understanding is that an attention-based neural network is trained on edge features to select a solution within the subspace identified above. Again, I may be oversimplifying or misunderstanding, and I ask the authors to clarify as before.  \n\n* **(iii)** *“Third, we impose soft physical consistency through a feature-conditioned Tikhonov refinement and train all weights end-to-end via reverse-mode implicit differentiation.”*  \n  From Sections 3.4 and 3.5, my understanding is that the model is trained in an end-to-end manner on a regularized version of the formulation above, using implicit differentiation to backpropagate through SPD regularization. Please provide additional clarity on this part."}, "questions": {"value": "Please address the presentation issues and specific questions raised above.  \n\n**Additional minor comments and questions (not a comprehensive list):**  \n* **Line 106:** “while respecting these physical constraints” — does this refer to flow conservation constraints?  \n* **Section 3.3:** Could you please provide an explanation or intuition for how GATv2 couples the recovery of missing edges with the edge features $X$?  \n* **Line 284:** “SPD solver” — should this read “SPD solve”?  \n* **Section 4:** What are the features associated with each edge for the different problems?  \n* **Sections 4.1, 4.2:** Is it “ten baselines” or “eight baselines”?  \n* **Line 412:** “imptovements” — fix typo.  \n* **Line 449:** “meta-search over the divergence-free flow manifold” — overstated or overcomplicated; please revise."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V5kzIfEUEM", "forum": "vu1IEpdUQh", "replyto": "vu1IEpdUQh", "signatures": ["ICLR.cc/2026/Conference/Submission14572/Reviewer_ietj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14572/Reviewer_ietj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864407066, "cdate": 1761864407066, "tmdate": 1762924959133, "mdate": 1762924959133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FlowSymm, a solver that augments a graph attention backbone with symmetry-preserving, physics-aware corrections for partially observed flow graphs. This method for estimating missing flows in networks (e.g., traffic, power, bike-sharing) where only a subset of edges have sensors.\n\nIts main contributions are:\n1、It reinterprets admissible divergence-free adjustments as elements of an Abelian group action. \n2、It combines this basis with edge-wise GATv2 embeddings. An attention mechanism scores each basis vector in a context-aware manner, enabling the model to inject corrective flows where the local structure demands, while preserving flow balance.\n3、It imposes soft physical consistency through a feature-conditioned Tikhonov refinement and trains all weights end-to-end via reverse-mode implicit differentiation within a bilevel optimization framework.\n4、Extensive experiments on three real-world benchmarks show that FlowSymm consistently improves upon nine baselines, reducing RMSE by up to ten percent and yielding performance gains over the previous state-of-the-art."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a very well-written and clearly presented paper. The core idea—leveraging a group-action basis for divergence-free corrections in network flow completion. The methodology is explained with remarkable clarity, and the experimental results are thorough and convincing. The core innovation is the reformulation of the flow completion problem through the lens of group theory. While GNNs typically encode spatial symmetries or permutation symmetries, this work defines a new, problem-specific \"algebraic symmetry\" derived directly from the graph's incidence matrix and the sensor mask. \n\nIn addition, the mathematical derivation is sound, from the initial balanced anchor using the Moore-Penrose pseudoinverse to the construction of the projector P_A and the implicit differentiation for the bilevel problem. The method is built on a firm algebraic foundation.\n\nFinally, the experimental design is robust. The use of three distinct, real-world domains (Traffic, Power, Bike) demonstrates generalizability."}, "weaknesses": {"value": "1、The introduction does an excellent job of covering related work in physics-informed ML and equivariant GNNs. However, the transition to the specific \"algebraic symmetry\" of flow conservation could be slightly more explicit for a reader unfamiliar with this background.\n2、The derivation of the projector P_A is technically correct but may be dense for some readers. The step of subtracting the projector onto the observed edges within the balanced space is crucial but could be briefly motivated with an intuitive phrase."}, "questions": {"value": "Questions：The entire method, starting with the \"balanced anchor\"  , assumes the net nodal injection vector c is known. In many real-world scenarios (e.g., unmonitored power grid feeders, unobserved traffic sources/sinks), c is also partially unknown or highly uncertain. How sensitive is FlowSymm's performance to errors in c?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Nothing"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vofqEahAJu", "forum": "vu1IEpdUQh", "replyto": "vu1IEpdUQh", "signatures": ["ICLR.cc/2026/Conference/Submission14572/Reviewer_wnzU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14572/Reviewer_wnzU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998291714, "cdate": 1761998291714, "tmdate": 1762924958560, "mdate": 1762924958560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of rebuttal"}, "comment": {"value": "We sincerely thank you for your time, thorough evaluation, and constructive feedback. We are encouraged by the consensus that our problem is important, our method is novel, and our results show strong potential.\n\nWe have completed our revisions and uploaded an updated manuscript. Your comments have helped us improve both the clarity of our presentation and the empirical validation of our approach. We summarize the key updates and the additional experiments:\n\n**1. Clarifications on Terminology and Intuition**\n\n- Based on feedback regarding presentation (Reviewers ietj, wnzU), we have revised the Abstract, Introduction, and Method sections:\nWe explicitly clarified the distinction between our constraint-based \"algebraic symmetry\" (defined by conservation laws and sensor masks) and the spatial/geometric symmetries in traditional E(n)-equivariant GNNs.\n\n- We have clarified the role of the Abelian group structure while retaining the original terminology, and we now relate it more explicitly to concepts such as the null-space basis and solution subspace to better align with readers’ intuition about linear systems.\n\n- We added a step-by-step, intuitive explanation for the construction of the projector $P_\\\\mathcal{A}$, describing it as a process of filtering out components of the balanced flow that would violate sensor constraints.\n\n**2. New Experimental Results (Appendix E)**\n\n- We conducted three new sets of experiments to directly address questions regarding robustness and baselines. These are now detailed in the new Appendix E:\n    - **Sensitivity to Injection Vector $c$** (Reviewers wnzU, yn4E): We performed a sensitivity analysis by training on ground-truth $c$ and injecting noise at test time. While the anchor component $f^{(0)}$ drifts with noise as expected, our model's learned, feature-based correction ($\\\\Delta$) remains stable. Consequently, the final RMSE remains robust (e.g., increasing only from 0.057 to 0.060 on Traffic even with 50% noise).\n    - **\"Predict-then-Project\" (PnP) Baseline (Reviewer yn4E)**: We implemented the suggested PnP baseline, which predicts an unconstrained vector and projects it onto the feasible subspace. FlowSymm significantly outperforms this baseline (e.g., 0.02 vs. 0.13 RMSE on Power), and notably, the PnP baseline performs worse than a simple physics-agnostic MLP. This validates our architectural choice to learn coefficients within the latent basis rather than projecting onto it.\n   - **Physical Consistency Analysis (Reviewers MoiE, yn4E)**: We computed the final divergence residuals ($||B\\tilde{f} - c||_2$) for all models.\nResult: FlowSymm achieves the lowest divergence residual across all datasets while maintaining the lowest RMSE, proving that it achieves the best balance of data fidelity and physical adherence.\n\nWe have posted detailed, individual responses to each reviewer addressing their specific questions. We believe these revisions and additional analyses fully address the concerns. We remain open to further discussion and are happy to provide any additional clarifications during the discussion period.\n\nBest regards,\n\nThe Authors"}}, "id": "EtDLFFiVJ5", "forum": "vu1IEpdUQh", "replyto": "vu1IEpdUQh", "signatures": ["ICLR.cc/2026/Conference/Submission14572/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14572/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission14572/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763710429319, "cdate": 1763710429319, "tmdate": 1763710429319, "mdate": 1763710429319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}