{"id": "fI0AZpfe5e", "number": 5986, "cdate": 1757949613367, "mdate": 1759897941005, "content": {"title": "Enlightenment Period Improving DNN Performance", "abstract": "The start of deep neural network training is characterized by a brief yet critical phase that lasts from the beginning of the training until the accuracy reaches approximately 50\\%. During this phase, disordered representations rapidly transition toward ordered structure, and we term this phase the Enlightenment Period. Through theoretical modeling based on phase transition theory and experimental validation, we reveal that applying Mixup data augmentation during this phase has a dual effect: it introduces a Gradient Interference Effect that hinders performance, while also providing a beneficial Activation Revival Effect to restore gradient updates for saturated neurons. We further demonstrate that this negative interference diminishes as the sample set size or the model parameter size increases, thereby shifting the balance between these two effects. Based on these findings, we propose three strategies that improve performance by solely adjusting the training data distribution within this brief period: the Mixup Pause Strategy for small-scale scenarios, the Alpha Boost Strategy for large-scale scenarios with underfitting, and the High-Loss Removal Strategy for tasks where Mixup is inapplicable (e.g., time series and large language models). Extensive experiments show that these strategies achieve superior performance across diverse architectures such as ViT and ResNet on datasets including CIFAR and ImageNet-1K. Ultimately, this work offers a novel perspective on enhancing model performance by strategically capitalizing on the dynamics of the brief and crucial early stages of training. Code is available at https://anonymous.4open.science/r/code-A5F1/.", "tldr": "Achieving a statistically significant improvement in model (CNN,VIT,Time-Series,LLM)performance by modifying training parameters only during the first few epochs.", "keywords": ["mixup", "Optimization-methods", "training-orchestration"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/df524f5e8e712b369894b53be23cf717be47a1c2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the \"Enlightenment Period\" (ENP), defined as the initial training phase of a deep neural network from the start until accuracy reaches approximately 50%. The authors posit that during this period, Mixup data augmentation has a dual effect: a detrimental \"Gradient Interference Effect\" and a beneficial \"Activation Revival Effect\". They further theorize that this negative interference diminishes with increased model or data size, which they term the \"Gradient Interference Diminishing Effect\". Based on this trade-off, the paper proposes three strategies: the Mixup Pause Strategy for small-scale scenarios , the Alpha Boost Strategy for large-scale, underfitting models , and the High-Loss Removal Strategy for tasks where Mixup is inapplicable (e.g., time-series, LLMs). The authors claim these strategies achieve superior performance across diverse models like ViT and ResNet on datasets including CIFAR and ImageNet-1K."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper attempts to analyze the very early, chaotic phase of training and connect it to practical, data-driven strategies.\n2. The investigation of these strategies (or heuristics) is applied across multiple domains, including CV, time-series, and LLMs, which shows breadth."}, "weaknesses": {"value": "1. **Oversimplified Theoretical Model:** The core theoretical analyses of \"Gradient Interference\" (Section 2.2) and the \"Diminishing Effect\" (Section 2.3) are based on a simple linear model with a sigmoid activation ($\\hat{y}=\\sigma(\\theta^{\\top}x)$). This model does not capture the non-linear dynamics, layer interactions, or high-dimensional parameter spaces of modern ResNets and Vision Transformers. Conclusions drawn from this toy model (e.g., about gradient orthogonality) cannot be reasonably extrapolated to these deep architectures.\n2. **Marginal and Unconvincing Empirical Gains:** The reported performance improvements are modest (e.g., +0.10% on CIFAR-10, +0.4% on ImageNet-1K). Such minimal gains do not provide a compelling reason to adopt these strategies, especially given their added complexity.\n3. **Introduction of a Confounding Hyperparameter:** The proposed strategies introduce a new, highly sensitive hyperparameter: the \"ENP Duration\"15. Ablation studies in Table 3 and Table 7 show that an incorrect choice for this duration can degrade performance, sometimes significantly (e.g., ViT-S/Tiny-ImageNet performance drops from +1.57% to -0.69% when changing duration from 10 to 25 epochs). This makes the method impractical, as it merely replaces the tuning of one hyperparameter (Mixup $\\alpha$) with another, less-understood one (ENP duration).\n4. **Weak Evidence for the Activation Revival Effect:** The \"Activation Revival Effect\" (Section 2.4) is supported only by correlational evidence. Figure 4 shows that Mixup training results in fewer zero-value activations. This is an expected property of Mixup (which interpolates samples, pulling them away from saturated decision boundaries) and does not prove a causal \"revival\" mechanism that improves performance.\n5. **Writing and Presentation:** The overall writing of this paper is not good. Some figures are difficult to interpret. The 2D embeddings in Figure 1 and Figure 7 are low-resolution and do not provide clear, quantitative evidence for the claims about \"disorganized clusters\" or \"boundary-ambiguous characteristics\"."}, "questions": {"value": "1. **Order parameter:** In the introduction, the authors frame the \"Enlightenment Period\" (ENP) using phase transition theory and suggest that accuracy acts as a \"quasi-order parameter.\" However, this concept is not explained in detail or utilized in the subsequent theoretical analysis (Section 2). Can the authors clarify the role of the \"order parameter\" concept in your work? How does this analogy concretely inform the development of your theoretical effects (e.g., Gradient Interference) or practical strategies?\n\n2. **ENP Duration:** A critical, practical aspect of this work is determining the duration of the ENP. The authors suggest it ends \"around the point when the model accuracy reaches 50%\", but the experiments (e.g., Table 3, Table 7) appear to be a grid search over a number of epochs (e.g., 10, 15, 20).\n- How is the optimal ENP duration determined in practice? Is it a hyperparameter that must be tuned via a full grid search for every new model/dataset pair?\n- If the authors do use the 50% accuracy criterion, what is the performance? For instance, in the experiments, what epoch corresponds to ~50% accuracy, and how does applying the \"Mixup Pause\" strategy for that specific duration compare to the optimal duration grid search?\n\n3. **Assumption:** In Section 2.2 (L146), the derivation of the Mixup gradient relies on the assumption that $|f_i| > |f_j|$ (where $x_i$ is a positive sample and $x_j$ is a negative sample). Why should this be the case during early training, when the model's classifications are essentially random and scores are likely to be small and noisy for both classes?\n\n4. **Expectation of the gradient perturbation:** In Section 2.3.1 (L205), the authors assume that the expectation of the gradient perturbation, $\\mathbb{E}[\\delta^{(k)}]$, is approximately zero. Why is it the case?\n\n5. **Experimental validation:** In the experimental validation (Section 2.3.3), the authors define the gradient interference ratio $r$ using only the component of the Mixup gradient that is perpendicular to the vanilla gradient. This seems problematic. First, the mixup gradient is not entirely orthogonal as shown in Figure 2. Second, the norm of perpendicular component is usually smaller than that of the entire gradient, leading to a small value of $r$.\n\n6. **Small/large scale:** The authors propose a \"boundary\" between small-scale and large-scale scenarios (L336), which you define as \"ViT-T when trained on the 100% ImageNet-1K dataset.\" This seems arbitrary.\n- Can you provide a more principled justification for this boundary?\n- Out of your 16 experimental settings, it appears only *one* (ViT-T/ImageNet-1K) is considered \"large-scale,\" and all others are \"small-scale.\" Can you confirm this?\n- If so, how can you be confident in the \"Alpha Boost\" strategy when it has only been validated in a single experimental setting? Would it not be necessary to test this on other large-scale setups (e.g., larger models than ViT-T or larger datasets than ImagetNet-1K) to validate the claim?\n\n7. **Missing related works:** The High-Loss Removal strategy shares conceptual similarities with recent work on simplicity bias [1], which also identifies an early training phase to find hard samples and modify the data distribution. Can the authors please discuss the relationship between \"High-Loss Removal\" strategy and their approach? How does your ENP definition and method differ from their use of early-phase checkpoints to mitigate simplicity bias?\n\n8. **Theoretical proofs:** 2. The derivations for BENR (Appendix C.1) and ATD (Appendix C.2) appear to be based on non-rigorous examples.\n- The BENR derivation (Eq. 18-26) is based on a single concrete example of two positive samples and one negative sample. A proof by example is not rigorous. Can the authors provide a more general derivation for $BENR_{vanilla} > BENR_{mix}$?\n- In Appendix C.2, how to transition from Equation (28) (which compares the L2 norm of the *parameter updates*) to Equation (29) (which compares the *variance* of the *activation trajectory distance*)? Please provide the missing steps.\n\n9. **Alternative choice of distance:** The authors use Activation Trajectory Distance (ATD), defined as the L2 norm of the difference in activation vectors, to quantify representation change. Given that these are high-dimensional representations, L2 distance can be a noisy and potentially misleading metric. Have you considered using more robust, alignment-based similarity metrics like Centered Kernel Alignment (CKA) [2] to verify your findings about representation dynamics?\n\n[1] Nguyen, Tuan H., et al. \"Changing the training data distribution to reduce simplicity bias improves in-distribution generalization.\" Advances in Neural Information Processing Systems 37 (2024): 68854-68896\n[2] Kornblith, Simon, et al. \"Similarity of neural network representations revisited.\" International conference on machine learning. PMlR, 2019."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "shCKNCng9A", "forum": "fI0AZpfe5e", "replyto": "fI0AZpfe5e", "signatures": ["ICLR.cc/2026/Conference/Submission5986/Reviewer_iEY9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5986/Reviewer_iEY9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631054907, "cdate": 1761631054907, "tmdate": 1762918393733, "mdate": 1762918393733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers how to apply 'mixup data augmentation' effectively during model training over the initial enlightenment period (when the accuracy reaches ~50%) for obtaining DNN models with improved performance.  It bases on the observed dual effect of mixup data samples to control model training in that (1) pausing the use of mixup data for small scale datasets, (2) boosting mixup's 'alpha' values for large datasets with underfitting, and (3) removing high-loss data samples when mixup data augmentation is inapplicable.  Experimental evaluation results over five model architectures on four datasets are obtained to show performance gains of its mixup data use control upon model training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Providing mathematical proofs and experimental validations on the dual effect of mixup data samples to show that the negative effect of Gradient interference (that hinders performance) diminishes as the dataset size or the model parameter size rises, implying the potential gains via Mixup Pause for small-scale cases.  On the other hand, the positive effect of Activation Revival (that restores gradient updates for saturated neurons) benefits large-scale cases with little negative effect and with better gain potentials from mixup samples obtained by boosted alpha.  Experimental results are provided (1) to validate gradient interference and its diminishing effect and (2) to demonstrate the manifestation of activation revival for mixup samples with two different alpha values."}, "weaknesses": {"value": "The trained models obtained by following the considered Mixup strategies under different model architectures on datasets, unfortunately, exhibit negligible performance gains when compared with their baselines.  In Figure 5, for example, accuracy improvement by models trained with the mixup pause strategy exhibits less than 0.5% (and could become negative if the Mixup Pause lasts beyond 20 epochs.  The main results listed in Table 3 also indicate the accuracy gains of 0.45% (with Mixup Pause) and of 0.4% (with Alpha Boost) for PreactResNet50/Tiny-ImageNet and ViT-T/ImageNet-1K, respectively.  With Mixup Pause for ViT-S/Tiny-ImageNet, the best case improves accuracy by just 1.57%, with two other cases to have accuracy improved by 0.81% and reduced by 0.69%. \n\nThe results clearly fail to support its claimed Mixup use control strategies for model performance gains.  Due to its inadequacy in propping up its claimed strategies for DNN improvement, the paper is recommended for weak rejection, albeit to its sound theoretical treatment on the dual effects of Mixup samples during model training in the initial enlightenment period."}, "questions": {"value": "More real-world evaluation to demonstrate meaningful performance improvement via Mixup sample use strategies will markedly lift the paper's relevance and acceptance chance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ObwQbihmfX", "forum": "fI0AZpfe5e", "replyto": "fI0AZpfe5e", "signatures": ["ICLR.cc/2026/Conference/Submission5986/Reviewer_X5Kd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5986/Reviewer_X5Kd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843422778, "cdate": 1761843422778, "tmdate": 1762918393315, "mdate": 1762918393315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the “Enlightenment Period” (ENP), a brief early training phase, from the beginning of training until the accuracy reaches approximately 50%, during which representations transition from disorder to order and learning dynamics are highly volatile. The authors argue that Mixup within ENP yields two competing effects: (i) a Gradient Interference Effect that initially hinders boundary refinement, and (ii) an Activation Revival Effect that rescues saturated/“dead” activations. Building on this, they propose three simple scheduling strategies focused only on the ENP window: Mixup Pause (turn Mixup off early, for small-scale settings), Alpha Boost (use larger Mixup α early, for large/underfitting settings), and High-Loss Removal (temporarily drop high-loss samples in tasks where Mixup is inapplicable). The paper presents a linearized theoretical analysis, small synthetic/MLP studies, and vision experiments (ResNet/ViT on CIFAR, Tiny-ImageNet, ImageNet-1K) showing modest but consistent improvements, with ablations on ENP duration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The three strategies are simple to implement and slot naturally into existing training recipes; some reported gains (e.g., ViT-T on ImageNet-1K +0.4% top-1) are non-trivial given the minimal code changes.\n\n2. Positioning ENP as a short “phase transition” with accuracy as a quasi-order parameter provides an intuitive lens for practitioners to reason about early instabilities and data-augmentation timing.\n\n3. Results span multiple model families/datasets and include ablations on ENP duration, with evidence that over-extending ENP hurts—supporting the notion that the useful window is short."}, "weaknesses": {"value": "1. The main claim hinges on an accuracy-based 50% threshold that is ill-posed beyond balanced K-class classification and internally inconsistent. \n\n2. The defining statement—ENP lasts “until accuracy reaches ~50%”—is dataset/label-space dependent and not theoretically grounded. In binary classification, 50% is the chance level, so declaring “enlightenment” at chance accuracy is questionable and contradicts the intuition that ENP marks the onset of useful generalization. Conversely, for K-class tasks with K>2, 50% can be far above chance (1/K), making the same threshold arbitrary across tasks. \n\n3. The theory section models a binary classifier with sigmoid and linear score, yet the top-level definition of ENP uses a 50% accuracy marker that has a different meaning in binary vs multi-class settings; this mismatch is not reconciled.\n\n4. The gradient-interference analysis assumes a linear/sigmoid model and orthogonality/normalization heuristics, while the main wins are shown on ReLU CNNs and ViTs with complex optimizers/schedules and label-space structures. The paper does not quantify how much of the claimed effect persists once nonlinearity, modern training artifacts (label smoothing, CutMix/AutoAugment/erasing), and large-batch warmup are accounted for.\n\n5. “High-Loss Removal” is close in spirit to curriculum/hard-example scheduling. While the paper argues conceptual differences (temporary removal only during ENP, then restore), a fairer empirical comparison against modern loss-aware curricula, hard mining, focal loss, and sample reweighting baselines is missing; this makes it hard to attribute gains to ENP-specificity rather than general loss-aware sampling."}, "questions": {"value": "1. Do we have formal, task-agnostic theoretical evidence that “accuracy ≈ 50%” is the natural and universal boundary of ENP, rather than an empirical heuristic that varies with K, class imbalance, and noise label rates?\n\n2. How should ENP be identified for binary tasks, where 50% equals chance? Does ENP end at chance accuracy (which seems counterintuitive), or should the criterion be reframed (e.g., exceeding chance by a fixed margin, crossing a calibrated loss/MI/entropy threshold, or reaching a stable drop in gradient variance)?\n\n3. Do you view “Enlightenment” as related to Emergent Abilities [1] in LLMs (e.g., sudden capability jumps under scale/task prompts)? If not, could you clarify the differences—e.g., ENP as a training-time, short-lived, optimization-phase transition vs. emergent abilities as capability-level, scaling-law/task-threshold phenomena observed at evaluation?\n\n[1] Wei et. al., \"Emergent Abilities of Large Language Models\", TMLR 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k8b8m6yFbW", "forum": "fI0AZpfe5e", "replyto": "fI0AZpfe5e", "signatures": ["ICLR.cc/2026/Conference/Submission5986/Reviewer_bMWW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5986/Reviewer_bMWW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891678260, "cdate": 1761891678260, "tmdate": 1762918393091, "mdate": 1762918393091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the effect of the enlightenment period in the context of the Mixup method. The authors show that when the model and dataset are small, applying Mixup early in training leads to gradient interference, which disorganizes the embedding space. Consequently, avoiding Mixup during this stage yields better performance. However, as the model and dataset size increase, this interference during the enlightenment period diminishes, making Mixup more beneficial. Moreover, using a larger value of the Mixup Beta distribution parameter $\\alpha$ further improves final test performance by preventing zero-value activations. The paper also reports that, for time-series data, a similar principle applies: skipping high-loss samples during the enlightenment period can improve overall performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-organized and clearly written. In the introduction, the authors use 2D embedding visualizations to illustrate the difference between Mixup and vanilla training. Section 2 then analyzes this phenomenon in the context of the enlightenment period using a logistic regression setting, showing how gradient interference arises. The paper also highlights the benefit of the enlightenment period in reducing zero-value activations and supports this claim with corresponding experimental results.\n- The experiments are clearly described and include well-designed ablation studies. In particular, the ablations on the Mixup Pause strategy thoroughly examine the effects of the pause epoch and dataset size, while the Alpha Boost strategy is analyzed through ablations on the $\\alpha$ value. The paper also presents a convincing table showing that the optimal enlightenment period corresponds to around 50% training accuracy. Furthermore, to ensure statistical significance, the authors report $p$-values and average results over three random seeds for the ViT/ImageNet experiments."}, "weaknesses": {"value": "- The current analysis does not sufficiently explain why Mixup causes gradient interference specifically during the *enlightenment period*. The same reasoning could apply to both early and late phases of training, so it is unclear why interference occurs only in the early stage but becomes beneficial later. A more detailed explanation is needed to justify why this phase-dependent behavior emerges and what mechanism differentiates the early and late training dynamics.\n- The motivation regarding gradient interference is somewhat unclear. Lines 158–161 state that when input samples are normalized and high-dimensional, $x_i + x_j$ (Mixup gradient) and $x_j - x_i$ (vanilla gradient) are approximately orthogonal. The paper argues that this orthogonality causes interference with the useful direction of the vanilla gradient. However,\n    1. In practice, the Mixup ratio is sampled from a Beta distribution rather than fixed at 0.5, so the resulting directions cannot be perfectly orthogonal.\n    2. it is unclear why orthogonality itself should cause interference. If the two directions are truly orthogonal rather than partially aligned, the Mixup update would be in an independent direction, neither helping nor hindering the vanilla update.\n    3. The paper also claims that this orthogonality is amplified in higher dimensions (large model size), yet in Section 2.3.2 it argues that interference decreases as the model size increases. These two claims appear contradictory.\n    \n    A more consistent explanation might be that in low-dimensional settings, the two gradients are not perfectly orthogonal, so they interfere with each other, whereas in high-dimensional models they become more orthogonal and thus less interfering. Clarifying this reasoning would help readers better understand the motivation behind the analysis.\n    \n- The claim that Mixup gradients can “revive” dead neurons is interesting, but the explanation lacks rigor. It is not clearly explained why zero-value activations still increase significantly even when Mixup is applied from the beginning, nor why they are said to decrease rather than simply remain small. This conceptual gap needs to be addressed. Based on the current explanation, Mixup seems to maintain low zero-value activation rather than reduce it, and this distinction should be clarified.\n- In addition, the motivation for increasing $\\alpha$ is not clearly justified. Since $\\alpha$ in $\\mathrm{Beta}(\\alpha, \\alpha)$ controls the variance of the sampling distribution, a larger $\\alpha$ concentrates samples near 0.5, which reduces variance. It is unclear why this behavior would improve performance or better prevent dead neurons in enlightenment period. Furthermore, Wouldn’t the level of zero-value activations converge to a similar magnitude regardless? A more detailed explanation or supporting analysis would strengthen this argument.\n- The motivation for high-loss removal in the time-series experiments is insufficient. The logical connection to Mixup is unclear. Simply stating that high-loss examples corresponds to Mixup samples does not adequately justify why they should be removed. A stronger theoretical or empirical rationale is needed to support this design choice.\n- The title is overly general. Since the paper specifically studies the role of Mixup during the enlightenment period, it is problematic that the word *Mixup* does not appear in the title. The title should explicitly reflect the paper’s main focus."}, "questions": {"value": "- Is the Alpha Boost strategy applied only during the enlightenment period? Please clarify whether Figure 4 also reflects this setting. If so, what would happen if the strategy were applied throughout the entire training process? \n- In addition, it is unclear why increasing $\\alpha$ reduces the number of zero-activations specifically during the enlightenment period and then increases again afterward. Would the number of zero-activations remain lower if the strategy were applied throughout the entire training process? Clarification or additional experiments on this point would be helpful.\n- The paper provides ablation studies on data scale and pause epoch for the Mixup Pause strategy, but there is no ablation on model size. An additional analysis showing how model size affects the effectiveness of the Mixup Pause strategy would strengthen the experimental validation.\n- A more thorough comparison with related work is needed. The paper should discuss prior theoretical studies on Mixup [1, 2, 3] as well as literature on the critical period [4, 5]. In particular, [1, 2] argue that Mixup should be applied only in the early phase of training, which directly contradicts the main claim of this paper. Meanwhile, [5] identifies a critical period around 50% training accuracy as crucial for preventing the loss of plasticity, which appears closely related to the authors’ notion of the enlightenment period. Providing a more detailed comparison that highlights these similarities and differences would significantly strengthen the paper’s positioning and clarify how this work advances beyond the existing studies.\n\n\n**Minor Corrections**\n\n- Line 290: alpha → $\\alpha$\n- Line 364: ResNetShafiq & Gu (2022) → ResNet (Shafiq & Gu, 2022)\n- Lines 367, 371, 377, etc.: Fix spacing issues (e.g., AppndixD → Appendix D, Table3 → Table 3, Table 7and → Table 7 and, etc.)\n- Line 198: Eq.equation 3 → Equation 3\n- Line 209: $\\delta^{(k)}$ → $\\boldsymbol{\\delta}^{(k)}$ for consistency\n\n---\n**References**\n\n[1] The Benefits of Mixup for Feature Learning, In ICML 2023.\n\n[2] Over-Training With Mixup May Hurt Generalization, In ICLR 2023.\n\n[3] Provable Benefit of Mixup for Finding Optimal Decision Boundaries, In ICML 2023.\n\n[4] Critical Learning Periods Emerge Even in Deep Linear Networks, In ICLR 2024.\n\n[5] DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity, In NeurIPS 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CdEHbycs6q", "forum": "fI0AZpfe5e", "replyto": "fI0AZpfe5e", "signatures": ["ICLR.cc/2026/Conference/Submission5986/Reviewer_JpvS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5986/Reviewer_JpvS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5986/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985565169, "cdate": 1761985565169, "tmdate": 1762918392801, "mdate": 1762918392801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}