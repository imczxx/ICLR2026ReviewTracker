{"id": "mdw0vvRBEL", "number": 17499, "cdate": 1758276821996, "mdate": 1759897171200, "content": {"title": "Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction", "abstract": "The evaluation and post-training of large language models (LLMs) rely on supervision, but strong supervision for difficult tasks is often unavailable, especially when evaluating strong models. In such cases, models have been demonstrated to exploit evaluation schemes built on such imperfect supervision, leading to deceptive results. \n\nHowever, underutilized in LLM research, a wealth of mechanism design research focuses on game-theoretic *incentive compatibility* - eliciting honest and informative answers with weak supervision. \nDrawing from this literature, we introduce the peer prediction method for model evaluation and post-training. It rewards honest and informative answers over deceptive and uninformative ones, using a metric based on mutual predictability and without requiring ground truth labels. \n\nWe demonstrate the method's effectiveness and resistance to deception, with both theoretical guarantees and empirical validation on models with up to 405B parameters. We show that training an 8B model with peer prediction-based reward recovers most of the drop in truthfulness due to prior malicious finetuning, even when the reward is produced by a 0.135B language model with no finetuning.\n\nOn the evaluation front, in contrast to LLM-as-a-Judge which requires strong and trusted judges, we discover an inverse scaling property in peer prediction, where, surprisingly, resistance to deception is *strengthened* as the capability gap between the experts and participants *widens*, enabling reliable evaluation of strong models with weak supervision. In particular, LLM-as-a-Judge become worse than random guess when facing deceptive models 5-20$\\times$ the judge's size, while peer prediction thrives when such gaps are large, including in cases with over 100$\\times$ size difference.", "tldr": "We introduce a game theory-based method for LLM evaluation and post-training that's resistant to model deception, and without needing any access to ground truth labels.", "keywords": ["Language Model Evaluation", "AI Alignment", "AI Truthfulness and Deception", "Large Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eb429f33ffe942fdb01e8f675a5994da9feee7fc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the peer prediction mechanism from the field of mechanism design for model evaluation and post-training. It evaluates the response of a participant by checking how much it can help the expert to predict other participants. The authors conduct a theoretical analysis of the incentive compatibility and resistance to deception. The authors also conduct empirical experiments on different tasks including the usefulness in post-training, model ranking and resistance to deception. The authors further reveal the inverse relations between the capability gap and resistance to deception, indicating its usefulness when lacking a stronger LLM judger or ground-truth supervisions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces an interesting method for model evaluation and post-training.\n\n2. The authors conduct both theoretical analysis and empirical evaluations for different tasks to prove the effectiveness of their method.\n\n3. The paper includes the discussion of limitations and other questions, which is easy for the readers to better understand the scope of the method."}, "weaknesses": {"value": "1. There are gaps between the theoretical analysis and empirical evaluations:\n\na. In Line 258, the authors derive the theoretical proof based on the assumption that A is a finite set of possible answers. However, in their empirical evaluation, the settings are free-form responses, which are not a finite set of possible answers. \n\nb. Although the authors claim that the empirical results of scaling the number of experts validate Theorem 2, it seemingly uses a variant of the original method by introducing the weights of the experts. According to Figure 9, where $\\alpha=0$, the original form of the proposed method seems to contradict with Theorem 2.\n\n2. Lack the baseline of a stronger LLM-as-a-Judge. The experiments are mainly conducted under the setting that the expert is at the same level or weaker than the participants. However, for the LLM-as-a-Judge baseline, the authors should use stronger judgers (e.g. GPT-4 or Gemini) to see if there is any gap.\n\n3. In Section 4.2, the authors claim that peer prediction can help distinguish strong models. However, the candidates of different sizes (8B/70B/405B) are easy to distinguish. The author could consider models with similar sizes and see if the scores from the peer prediction are aligned with their performance on public benchmarks."}, "questions": {"value": "Additionally, \n\n1. If we follow the condition derived in Line 328 for the size of participants and experts, what are the concrete numbers of m,n to be used in practice?\n\n2. Could you provide more insights about why Inverse Scaling with Model Capability Gap would happen?\n\n3. Could you explain the reason that the effectiveness of the proposed method varies in different tasks (Page 16)?\n\n4. In the experiments, the authors use multiple clones of the same model for honest participants (Section 4.1 and 4.3). Does it actually mean that more weights are put on the honest participant, which naturally creates a bias?\n \n5. One claim of the advantage over major voting is that the proposed method does not require a truthful majority (Line 763). But it seems counterintuitive. Could you explain more about that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "953Bq9GX9y", "forum": "mdw0vvRBEL", "replyto": "mdw0vvRBEL", "signatures": ["ICLR.cc/2026/Conference/Submission17499/Reviewer_PGHr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17499/Reviewer_PGHr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428429359, "cdate": 1761428429359, "tmdate": 1762927381962, "mdate": 1762927381962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a novel method for model evaluation and post-training that possesses game-theoretic incentive compatibility and does not require ground truth labels. Three roles, source, expert and target, are playing in a game. Source and target roles provide answers to a given question. The expert is taught to predict the target’s answer conditioned on the source’s answer, and the source is taught to provide informative answers for accurately predicting the target’s. The expert is sampled from the set of expert agents. The target and the source are both sampled from the set of participant agents. In this way, the participant agents are updated to be informative. Even when some experts are weaker, or some participants provide deceptive or uninformative answers, the peer prediction-based post-training incentivizes truthful answering and distinguishes correct answers from incorrect answers."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is well written and easy to follow. The proposed framework is novel and effective, supported by theoretical results and empirical results.\nThe peer prediction method is incentive compatible and resistant to deception and strategic manipulation. \nThe framework also demonstrate peer prediction method is resilient to deception. It also supports recovery of truthfulness, i.e., the accuracy drop from deception training is recovered."}, "weaknesses": {"value": "The cost of the proposed framework may be a concern and needs to be further explained. Algorithm 1 requires n^2m rounds of iteration which may be a big amount of computation."}, "questions": {"value": "What’s the post-training cost with the peer prediction approach. The appendix A.1 only addresses the cost in evaluation phase. As shown in the main text, the scaling with participant population size and expert numbers provides better performance. It would be also helpful to provide cost-effectiveness ratio for better understanding of the approach. The cost-effectiveness ratio can be also compared to ensemble methods to demonstrate the advantages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fOgYrk1KOW", "forum": "mdw0vvRBEL", "replyto": "mdw0vvRBEL", "signatures": ["ICLR.cc/2026/Conference/Submission17499/Reviewer_JtmQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17499/Reviewer_JtmQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874296039, "cdate": 1761874296039, "tmdate": 1762927381600, "mdate": 1762927381600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for improving truthfulness using peer prediction without strong supervision. The authors provide formal properties and empirical evaluations for their method. The paper has relevant applications in scalable oversight and AI safety."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Experiments include a wide range of models spanning 135M to 405B parameters and 37K questions from 8 different datasets\n- Section 3 provides theoretical properties about the proposed method: truthfulness is a Bayesian Nash equilibrium, and approximate incentive compatibility can be achieved with a large pool of agents with representative priors"}, "weaknesses": {"value": "- Algorithm 1's computational cost scales quadratically with the number of agents, which can be impractical when trying to have a large enough agent pool to achieve approximate incentive compatibility \n- Since the main focus is on incentive compatibility, I would've liked to see a more significant discussion of collusion. While collusion is briefly touched upon in the appendix, I would like to see a more in-depth explanation in the main paper"}, "questions": {"value": "- The inverse scaling claim seems counterintuitive. Could the authors discuss possible confounding factors or experiments to prove otherwise? There must be a threshold where decreasing model size does not improve LLM-as-a-judge. What would that threshold be?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dGV7TsY6an", "forum": "mdw0vvRBEL", "replyto": "mdw0vvRBEL", "signatures": ["ICLR.cc/2026/Conference/Submission17499/Reviewer_vWCq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17499/Reviewer_vWCq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964101631, "cdate": 1761964101631, "tmdate": 1762927380793, "mdate": 1762927380793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of LLMs' deception behaviors, particularly in evaluating strong LLMs that can deceive weaker supervisors, like LLM-as-a-Judge. Inspired by works in mechanism design, it proposes a peer prediction mechanism where a (potentially weak) expert scores a participant not on answer quality, but on how much its answer helps the expert predict the answers of other participants. The authors provide theoretical guarantees for this method and demonstrate two key empirical results: the score can be used as a DPO reward to recover truthfulness in a deceptive model, and it exhibits an inverse scaling property."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written, and the core concepts and claims are explained clearly and supported with figures. The problem is well-motivated. I especially appreciate the FAQ section.\n2. Scalable oversight, especially weak-to-strong oversight and generalization, is an important problem. The authors' proposal to use ideas in mechanism design and game theory to achieve some degree of weak-to-strong oversight in deception mitigation seems novel.\n3. The main method is backed by a solid game-theoretic foundation and empirical results, including DPO training experiments and tests against heterogeneous and realistically deceptive models."}, "weaknesses": {"value": "1. The current mechanism does not adequately address collusion. Since the method is being pitched as an ad-hoc fix to the deceptive behaviors of existing models, collusion among the game participants seems likely.\n2. Adding participants could bring about a quadratic increase in the query costs to LLMs."}, "questions": {"value": "1. Would there be diminishing returns if we continue to add participants?\n2. The results presented by the authors have strong domain-dependent behaviors. Do the authors have any insights or intuitions into why this is the case? Does this mean that it is easier to mitigate deception in certain use cases? Does the method's domain-dependent effectiveness have anything to do with the model's base capabilities in these domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N7Vx0sUx4d", "forum": "mdw0vvRBEL", "replyto": "mdw0vvRBEL", "signatures": ["ICLR.cc/2026/Conference/Submission17499/Reviewer_CPYv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17499/Reviewer_CPYv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967578520, "cdate": 1761967578520, "tmdate": 1762927379536, "mdate": 1762927379536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}