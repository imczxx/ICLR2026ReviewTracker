{"id": "a1MteiJJ6g", "number": 23036, "cdate": 1758338477256, "mdate": 1759896834992, "content": {"title": "Generative Krylov Subspace Representations for Scalable Quantum Eigensolvers", "abstract": "Predicting ground state energies of quantum many-body systems is one of the most central computational challenges in quantum chemistry, physics, and materials science. Krylov subspace methods, such as Krylov Quantum Diagonalization (KQD) and Sample-based Krylov Quantum Diagonalization (SKQD), are promising approaches for this task on near-term quantum computers. However, both require repeated quantum circuit executions for each Krylov subspace and for every new Hamiltonian, posing a major bottleneck under noisy hardware constraints. We introduce Generative Krylov Subspace Representations (GenKSR), a framework that learns a classical, generative representation of the entire Krylov diagonalization process. To overcome the scalability limitations of Transformer-based models, GenKSR leverages a conditional generative model (CGM) built on the Mamba state-space architecture, enabling efficient modeling of large-scale quantum systems. By learning the distribution of measurement outcomes conditioned on Hamiltonian parameters and evolution time, GenKSR generates Krylov subspace samples for unseen Hamiltonians and for larger subspace dimensions than those used in training. This enables full energy reconstruction purely from the classical model, without additional quantum experiments. We validate our approach on simulations of Heisenberg models up to 15 qubits and on a 20-qubit XXZ chain using an IBM quantum processor. Our model successfully learns the noisy distribution from experimental data and generates a high-fidelity representation of the quantum process. This representation enables classical reproduction of experimental outcomes, supports reliable energy estimates for new, unseen Hamiltonians, and significantly reduces the need for further quantum computation.", "tldr": "We propose GenKSR, a generative Krylov subspace representation based on Mamba that reduces quantum experiments for scalable eigensolvers.", "keywords": ["Quantum computing", "generative models", "Krylov subspace"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31cac1b8802040dba3f7a2a931fa3dacd8843abc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces **Generative Krylov Subspace Representations (GenKSR)**, a framework that replaces repeated quantum circuit executions in eigensolver methods with a **classical generative model** trained on quantum measurement data. By using the **Mamba state-space architecture**, GenKSR efficiently learns the distribution of measurement outcomes conditioned on Hamiltonian parameters and evolution times, enabling scalable ground-state energy estimation entirely classically. The approach, which fall within the regime of quantum-classical hybrid approaches, is validated through simulations on up to 15 qubits and hardware experiments on a 20-qubit IBM processor, showing accurate predictions and strong generalization to unseen systems. Overall, GenKSR significantly reduces quantum resource requirements while maintaining high fidelity in modeling quantum dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written, as well as well structured. It gives a comprehensive introductions to the basics of the standard algorithms the authors are improving upon, i.e., KQD and SKQD. Moreover, the authors also clearly state the problem and give an appropriate introduction also for people with a strong background in quantum computing. The numerical experiments conducted in the paper clearly showcase the benefits of the proposed approach."}, "weaknesses": {"value": "The paper does not explicitly discuss the limitations of the proposed approach. \nFurthermore, during little to no comparison is made with respect to the standard algorithms (KQD, SKQD) which would be interesting to compare the propose ML-backed approach and the standard counterparts. In particular I believe a discussion in terms of computational resources, i.e., account for the resources needed to generate the training data to train the model, and the training of the model. In particular I wonder if this generates a substantial computational overhead. \n\nAdditionally see questions below."}, "questions": {"value": "Furthermore, it is not very clear to me whether a comparison between standards KQD, or SKQD, and the proposed approach is considered at all. I believe this would be important to add to the paper. At the moment a lot of the analysis focuses on comparing Transformer and Mamba architectures. This is indeed relevant but I believe there's should be a stronger focus on the advantages compared to standard approaches. \n\nAlso I wonder what is the degradation in performance when extrapolating the sampling  to a  much higher number of qubits and higher number of Krylov dimensions (compared to samples seen during training). \n\nI also find it a bit hard to follow the experimental section where classical shadowing is used as a benchmark. As far as I can tell this was not extensively introduced but to my understanding this should be a benchmark to compare the ML based approaches to a standard approach. Is this intuition correct?\nProvided that the above is correct, then Table 1 suggests that KQD with CS still remains the strongest baseline on average. \nI wonder if one can argue that the computational efficiency and transferability properties of the Mamba approach should make it preferable to the CS approach in some scenarios. \n\nIn lines 77-82 I'd recommend to also add Bayesian approaches which, despite not being strictly speaking generative models per se, showed some promising directions when combined with PQC, in particulars VQEs (see e.g., [Nicoli et al., NeurIPS 2023](https://proceedings.neurips.cc/paper_files/paper/2023/hash/3adb85a348a18cdd74ce99fbbab20301-Abstract-Conference.html))\n\nLine 189: I believe that the ground state should be $\\vert \\phi_0 \\rangle$ instead of $\\vert \\psi_0 \\rangle$?\n\nIn section 3.3, all the occurrences of $x$ and $x^*$ should be bold in order to be consistent with the notation introduced earlier as they represent a set of couplings of the Hamiltonian. Is that right?\n\nAs a recommendation, I believe a more in depth analysis and discussion of the generalisation capabilities of the proposed approach on both the qualitative and quantitative sense would be helpful to better assess the strengths of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b0TpTUgXck", "forum": "a1MteiJJ6g", "replyto": "a1MteiJJ6g", "signatures": ["ICLR.cc/2026/Conference/Submission23036/Reviewer_FX5y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23036/Reviewer_FX5y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650278727, "cdate": 1761650278727, "tmdate": 1762942485767, "mdate": 1762942485767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a generative modeling approach to Krylov diagonalization methods to find ground states of Hamiltonians. These methods usually use computational basis measurement from a QPU to reduce the size of Hamiltonian and then solve it. This paper suggests training an ML model from QPU data to act as a surrogate for the QPU. The claim is that such an ML model trained on a low dimensional Krylov subspace will generalize well to higher dimensional spaces."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well written and has a very interesting core idea. It uses recent advances in generative modelling to tackle a very pertinent problem in quantum algorithms. I also like that they included some results from IBM's NISQ machine."}, "weaknesses": {"value": "The main weakness I see is the lack of good experiments and comparisons with other methods. All the experiments in the paper are on 1D models. This is especailly concerning because the generative model used here also has a auto-regressive structure mimicking the 1D topology. Also, 1D models are pretty easily solvable using DMRG, making them uninteresting candidates for illustration. To make the results of the paper truly interesting, it has to be benchmarked on a suite of 2D problems. For the problem sizes that the authors are considering, (up to 25 qubits) the ground state computation in such models can be brute forced. I believe these calculations can even be pushed to higher qubit numbers using tensor networks (refer to ITensors.jl for some examples).\n\nAlso, the methods here are not compared to other classical ML methods to find ground states of such Hamiltonians, based on Neural Network Quantum State (implementations can be found in the NetKet library developed by Carleo and collaborators). For instance, in this framework an autoregressive neural net can be directly used to model the ground state and can be trained using Variational Monte-Carlo methods to estimate the ground state energy and other properties. Now these methods do not use any quantum data to train, but any ML method that claims to find ground states should at least be able to beat these methods.\n\nIt is also concerning in Fig 3 that the experiments are not pushed to large enough D values such that the energy error actually goes to zero."}, "questions": {"value": "1. In the Heisenberg chain experiments in the paper, what is the true value of the ground state enery and how does $\\Delta E$ compare relative to that?\n2. In the exact simulation, at values of D does the estimated energy start to converge to the ground state energy? \n3. In Fig 3(b), will the ML models achieve $\\Delta E = 0$ for larger values of D? If now, how large a D must be the Mamba model be trained on such that it will eventually achieve $\\Delta E = 0$ when tested for larger values of D?\n4. Can the Mamba model be enhanced in someway using a variational component to the training? Right now, this is just a surrogate model for the QPU generating bit strings, and it seems agnostic to the Hamiltonian at the level of the loss function. But can the loss function be enhanced with the information that these bitstrings should have low energy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Sw4uHcsytB", "forum": "a1MteiJJ6g", "replyto": "a1MteiJJ6g", "signatures": ["ICLR.cc/2026/Conference/Submission23036/Reviewer_yuRu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23036/Reviewer_yuRu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800544588, "cdate": 1761800544588, "tmdate": 1762942485222, "mdate": 1762942485222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Generative Krylov Subspace Representations (GenKSR), a framework that uses a Mamba-based generative model to learn the measurement distributions. Once trained on quantum data, this classical model can predict ground-state energies for new, unseen Hamiltonians and larger Krylov dimensions without further quantum experiments, aiming to reduce quantum resource costs and improve scalability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-structured, outlining the limitations of existing Krylov methods, introducing the proposed GenKSR framework, and systematically validating it with both simulation and real hardware experiment.\n\n2. The framework demonstrates a valuable extrapolation capability, successfully predicting energy convergence for larger Krylov dimensions  than those it was trained on.\n\n3. A significant strength is the validation of the model on a 20-qubit quantum processor, showing that GenKSR can learn a faithful representation of noisy experimental data and generalize to unseen Hamiltonian."}, "weaknesses": {"value": "1. The paper frames GenKSR as a new paradigm. However, the core methodology—training a conditional generative model on quantum measurement data to predict properties—is a well-established technique that is to learn a classical distribution of the quantum state from a number of measurement results. The novelty merely lies in its application to the Krylov diagonalization process (i.e., conditioning on Hamiltonian parameters $x$ and evolution time $t_l$). \n\n2. A justification for the work is the use of the Mamba architecture to overcome the $O(n^2)$ complexity of Transformers, thereby enabling scaling to \"large-scale quantum systems\". This is presented as a main contribution. However, the experiments presented do not support this claim. The largest system studied is a 20-qubit hardware experiment, and the paper explicitly states that on this system, \"both models (Mamba and Transformer]) achieve comparable accuracy\". The provided timing benchmarks (Figure 6) show negligible practical difference at this scale (The scale on the y-axis is relatively small). As presented, the choice of Mamba is a intuitive assertion rather than an empirically validated necessity for the problem scales investigated.\n\n3. The simulation study Table 1 reveals a critical issue: the generative models consistently underperform the AI-free \"Classical Shadow\" baseline. In almost all simulation scenarios, particularly those with higher shot counts (5000 and 10000) where statistical noise is reduced, the CS baseline achieves a lower RMSE than either AI model. Despite the reviewer's recognition that the generalization ability to unseen Hamiltonians of AI-based methods surpasses that of classical methods, this result raises a fundamental question about the very necessity of the AI-based methods in this framework."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NCh1piST2c", "forum": "a1MteiJJ6g", "replyto": "a1MteiJJ6g", "signatures": ["ICLR.cc/2026/Conference/Submission23036/Reviewer_DHK1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23036/Reviewer_DHK1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943449082, "cdate": 1761943449082, "tmdate": 1762942484363, "mdate": 1762942484363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GenKSR, a framework that learns a conditional generative model instantiated with a Mamba state-space architecture to model measurement distributions arising in Krylov Quantum Diagonalization and Sample-based KQD. Conditioned on Hamiltonian parameters and the Krylov time step, the CGM generates synthetic bitstrings that are fed into classical KQD/SKQD post-processing, enabling ground-state energy estimation for unseen Hamiltonians and extrapolation to larger Krylov dimensions without additional quantum executions. The paper validates GenKSR with noiseless simulations on Heisenberg chains up to 15 qubits and hardware SKQD on a 20-qubit subset of IBM fez, showing that the CGM reproduces the noisy measurement distribution and supports energy estimation at higher Krylov dimensions than seen in training. It also argues for Mamba’s near-linear sequence complexity versus Transformer baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and timely problem framing. The paper targets a bottleneck in NISQ-era Krylov methods: every new Hamiltonian and every higher Krylov dimension costs new quantum shots. Turning this into an offline-learn-once, infer-many pipeline is a sensible strategy.\n2. Experiments on real hardware. Simulated 15-qubit Heisenberg and a real 20-qubit IBM experiment provide a fairly convincing story that the approach is not simulator-only. The fact that they can learn the noise and regenerate it for test cases is a nice touch.\n3. Generalization to unseen Hamiltonians and larger Krylov dimensions.  Training on $D\\le 5$ and evaluating on up to D=15 is a nontrivial test. The paper shows that both Transformer and Mamba follow the correct shape of the energy curve, demonstrating effectiveness in generalization."}, "weaknesses": {"value": "1. The Hamiltonian families are closely related (1D Heisenberg/XXZ). It remains unclear how well GenKSR transfers across lattice geometries, boundary conditions, anisotropies, and initial states. An explicit OOD Hamiltonian evaluation would strengthen the generalization claim.\n2. Lack of ablations on conditioning. It is informative to disentangle contributions from (i) Hamiltonian encoder, (ii) time-step embedding forms, and (iii) Mamba depth/width.\n3. The complexity motivation for replacing Transformers by Mamba is valid for naive quadratic attention, but the paper does not compare against modern linear/sparse/structured-attention Transformers, which seems a bit outdated from ML's perspective."}, "questions": {"value": "1. In Sec. 3.2 you state: “The discrete evolution index $t_l$ is mapped into a dense vector via a learnable embedding layer.” This sounds like a standard learned lookup over the set of evolution indices present in training ( $t_l$ in your setup). However, your experiments later extrapolate to larger Krylov dimensions (D=15). Can you clarify how the embedding rows for $l > 5$ are initialized and whether they receive any gradient signals. I am confused how untrained rows can extrapolate effectively.\n2. Following the previous question, in ML we know “length / depth / horizon” generalization is usually brittle (sequence length in LLMs, longer rollouts in RL, deeper diffusion steps, etc.). How do you interpret the successful generalization to extrapolated Krylov dimensions suppose it actually works? \n3. Do you need to train a separate model for each Hamiltonian family? In this case, it still requires substantial quantum resources and undermines the advantages in resource saving.\n4. Does Transformer achieve better RMSE in larger qubits with less shots in Table 1? Is it possible to extrapolate to qubit number larger than 15 to see if Transformer outperforms in resource-restricted settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "axpLrOTgCc", "forum": "a1MteiJJ6g", "replyto": "a1MteiJJ6g", "signatures": ["ICLR.cc/2026/Conference/Submission23036/Reviewer_c4Tv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23036/Reviewer_c4Tv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970030777, "cdate": 1761970030777, "tmdate": 1762942484102, "mdate": 1762942484102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}