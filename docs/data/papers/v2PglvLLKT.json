{"id": "v2PglvLLKT", "number": 17657, "cdate": 1758278920430, "mdate": 1759897162306, "content": {"title": "The Shape of Adversarial Influence: Characterizing LLM Latent Spaces with Persistent Homology", "abstract": "Existing interpretability methods for Large Language Models (LLMs) often fall short by focusing on linear directions or isolated features, overlooking the high-dimensional, non-linear, and relational geometry within model representations. In this study, we focus on how adversarial inputs affect the internal representation spaces of LLMs—an aspect that remains poorly understood. We propose persistent homology (PH), a tool from topological data analysis, as a principled framework to characterize the multi-scale dynamics within LLM activations. Using PH, we systematically analyze six state-of-the-art models under two distinct adversarial conditions—indirect prompt injection and backdoor fine-tuning—and uncover a consistent topological signature of adversarial influence. Across architectures and model sizes, adversarial inputs induce \"topological compression\", where the latent space becomes structurally simpler, collapsing from varied, compact, small-scale features into fewer, dominant, and more dispersed large-scale ones. This topological signature is statistically robust across layers, highly discriminative, and provides interpretable insights into how adversarial effects emerge and propagate. By quantifying the shape of activations and neuron-level information flow, our architecture-agnostic framework reveals fundamental invariants of representational change, offering a complementary perspective to existing interpretability methods.", "tldr": "We use persistent homology to interpret how adversarial inputs reshape LLM representation spaces, resulting in a robust signature that provides multiscale, geometry-aware insights complementary to standard interpretability methods.", "keywords": ["Persistent Homology", "Interpretability", "Topological Data Analysis", "Representation Geometry", "Large Language Models", "AI Security", "Adversarial Attacks", "Sparse Autoencoders"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/71eb2758dc8a285f96d1ba1dcd3fc8fd611d4321.pdf", "supplementary_material": "/attachment/62d88cde47fa81740f71eb4fe649b87d9b1e4d85.zip"}, "replies": [{"content": {"summary": {"value": "The paper uses persistent homology (PH), from topological data analysis, to study how LLM activations change under adversarial conditions (prompt injection and backdoor “sandbagging”). It finds a consistent, layer wise topological signature distinguishing clean from poisoned activations, and proposes PH based summaries as practical signals for detection and analysis.  The core idea is to treat each layer’s activation vectors, for many inputs, as a point cloud, compute a Vietoris–Rips filtration and its barcode,  vectorize these barcodes into fixed length barcode summaries, and compare across layers/conditions. The findings are: (i) poisoned activations show fewer small scale features and later/longer lived large scale ones than clean, yielding a clear separation in PH features layer by layer, (ii) The PH summaries linearly separate clean vs. poisoned with near-perfect accuracy across multiple models and attacks, (iii) A local neuron level analysis pinpoints, where in depth adversarial effects concentrate, and ontrols (neuron permutation) remove the signal. Thus, PH barcodes + vectorized summaries provide architecture agnostic descriptors of the latent geometry."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(i)  The PH barcode summaries cleanly separate clean vs. poisoned activations across layers with simple linear models. This repeats across several LLMs (7B→70B) and two attack types. This is not only a classification, but rather a characterization of how geometry shifts: poisoned states show fewer small features and later/longer lived large scale ones, while clean states have many short lived features, (ii) There is a solid, reproducible pipeline beginning with subsample activations till PCA/CCA + logistic + SHAP, (iii A complementary local analysis embeds neurons across two layers, and applies PH to track where differences peak in depth, and variance heuristics can find informative layers even without labels."}, "weaknesses": {"value": "(i) Vietoris–Rips PH scales poorly, and the paper therefore subsamples activations, which may lose fine detail or bias results, (ii)  PH results depend strongly on normalization, distance metric, and whether one uses token vs pooled activations. Different choices could change the barcode summaries, (iii) Activation geometry can vary with dataset composition, prompt templates and topic distribution), hence the results might not generalize beyond the specific prompt suite or poison triggers used, (iv) Topological compression is a descriptive signature but the causal link to model failure modes or specific failure behaviors is not fully established. The paper provides strong empirical separation, but no theory explaining why adversarial triggers should systematically compress topology across architectures."}, "questions": {"value": "Suggestions: (i) Sweep the subsample sizes, distance metrics and token selection. Report the key barcode features and perform a stability analysis, (ii) Ensure that the differences are not just prompt length/format, by adding benign dummy insertions that mimic injection structure but carry no instruction, and re run PH, (iii) To anchor the phenomenon, prove for a simple model of mixture of Gaussians in high d,  that moving mass from many small clusters to a few larger, more separated clusters reduces counts of 1-bars and increases mean 0-bar death, formalizing topological compression."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UXvS1LNqZD", "forum": "v2PglvLLKT", "replyto": "v2PglvLLKT", "signatures": ["ICLR.cc/2026/Conference/Submission17657/Reviewer_inAf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17657/Reviewer_inAf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725130311, "cdate": 1761725130311, "tmdate": 1762927512648, "mdate": 1762927512648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the embeddings of clean inputs versus adversarially perturbed inputs using persistent homology. This is a  tool from topological data analysis that enbles to describe the topology and geometry of point clouds, in this case last token embeddings, in a principled manner. The authors show that clean and adversarial inputs exhibit very different topological properties, which goes beyond the typical analysis of linearly separating such samples. The results are demonstrated on a diverse set of models and appropriate choice of benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Investigating the geometry of the feature space of modern neural networks is crucial to better understand how models work and how they arrive at decisions. Taking the route of PH is an interesting direction and a valuable contribution.\n- The paper is clearly written. In particular, given that PH is not yet commonly used in interpretability/robustness research, the authors give good intuition for what barcodes do.\n- The experimental setting is well chosen and covers important failure modes in LLM security."}, "weaknesses": {"value": "- This sentence gives the impression you are the first to do this: “We propose persistent homology (PH), a tool from topological data analysis, as a principled framework to characterize the multi-scale dynamics within LLM activations.” I suggest phrasing it as using PH as a tool rather than proposing a framework, since related uses exist in other domains and your contribution is not a full PH framework.\n\n- In the background section, consider citing a review of complex constructions (e.g., Vietoris–Rips, Čech, alpha complexes).\n\n-  A formal treatment of PH would be appreciated in the appendix (optional).\n\n- The fact that the barcode summaries cluster adversarial vs. clean examples is not surprising if these samples are already linearly separable. However, it needs further elaboration to explain why this is important.\n\n- Even when you obtain a perfect AUC, adding your \"method\" to Table 1 would improve the presentation.\n\n\nNote: If my concerns are properly addressed I am inclined to raise my score to an accept.\n\n[1] Gardinazzi, Yuri, et al. \"Persistent topological features in large language models.\" arXiv preprint arXiv:2410.11042 (2024)."}, "questions": {"value": "- What do you mean by \"consistent topological behavior within the LLM latent space\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fcvwStcAAl", "forum": "v2PglvLLKT", "replyto": "v2PglvLLKT", "signatures": ["ICLR.cc/2026/Conference/Submission17657/Reviewer_NSud"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17657/Reviewer_NSud"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766823113, "cdate": 1761766823113, "tmdate": 1762927511999, "mdate": 1762927511999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper applies persistent homology (PH) from topological data analysis to study how adversarial inputs reshape latent representations in large language models (LLMs). The authors analyze six instruction-tuned models (3.8B–70B) under two adversarial conditions — indirect prompt injection (XPIA) and sandbagging / backdoor fine-tuning — using the TASKTRACKER and sandbagged datasets. They compute Vietoris–Rips filtrations on subsampled point clouds of last-token activations, vectorize persistence barcodes into 41-dimensional “barcode summaries,” and run global (layer-wise) and local (neuron-pair 2D embedding) analyses. The central empirical claim is that adversarial influence produces a reproducible “topological compression”: adversarial activations show fewer but larger-scale topological features (fewer H1 loops, higher mean death times of H0 components), a phenomenon that is discriminative (logistic classifiers/SHAP achieve near-perfect separation) and consistent across models and attack types. The paper also reports a neuron-level phase transition in topological complexity at intermediate layers (≈ layer 12 for Mistral 7B)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Novel methodology: First systematic application of PH to characterize adversarial effects in LLM latent spaces at both global and neuron levels. \n\nRobust empirical signal: Separation is reproducible across six models and two attack modes; discriminative power is high and interpretable via SHAP. \n\nMechanistic insight: Neuron-pair 2D embeddings and layerwise analysis identify where adversarial influence reconfigures information flow (phase transition in deeper layers). \n\nCareful controls: Normalization, permutation tests, adaptive-attack evaluation (LLMail-Inject) and baseline linear methods are included."}, "weaknesses": {"value": "Dependence on subsampling and last-token choice. PH is memory-intensive; authors subsample large numbers (k=1000, many subsamples), which is theoretically supported but may miss rare, high-impact topological features and makes replication costly. Also, using only the last-token embedding leaves open whether signatures generalize to alternative aggregation choices. \n\nInterpretation vs. causality. The paper convincingly documents correlational topological signatures but stops short of causal interventions (e.g., topology-aware regularization or targeted modifications to test whether changing topology alters adversarial susceptibility). Such experiments would strengthen the link between topology and vulnerability.\n\nGenerality to other threat models. The study focuses on XPIA and sandbagging/backdoor attacks. While LLMail-Inject adaptive examples were tested, evaluation against a wider array of adaptive, distributional, or model-poisoning attacks (and on more diverse prompts/tasks) would better establish universality. \n\nScalability & runtime. Practical adoption of PH-based monitoring in production LLM systems would require faster approximations or streaming variants; the paper acknowledges this but provides limited engineering pathways. It is also not clear how can the method scale to much larger production-grade models with 100B+ parameters. \n\nReferences. The authors miss citing a few critical works comparing their approach in other domains. For e.g. Extreme Image Transforms (EITs) [Crowder et al., 2022; Malik et al., 2023, Biol Cybernetics, Malik et al., 2023, arXiv] help with similar representations in vision for deep networks. Similarly Network Dissection [Bau et al., 2017, CVPR] and Locating and editing factual associations in gpt [Meng et al., 2022, NeurIPS] show layer-wise applicability to the final output of deep networks. \n\nPresentation. The individual sections of the paper are well written but the paper flow is not easy to follow when put together. The authors should consider reorganizing the sections to make the story flow better and for the reader to keep track of what is happening, without losing the focus from the main point. ."}, "questions": {"value": "How does the observed topological compression depend on the choice of token pooling (last token vs. mean-pooled vs. CLS-like embeddings)? Any preliminary experiments? \n\nCan the authors provide an ablation showing how sensitive the signature is to subsample size k and the number of subsamples K (e.g., do smaller k or fewer K materially change detection performance)? \n\nHave the authors attempted a simple topology-aware defense (e.g., penalize total persistence changes or normalize mean H0 death) to test whether changing topology reduces task drift or attack success? That would help evaluate causality.\n\nThe local 2D neuron embedding analysis finds a phase shift around mid-layers for Mistral — does the layer index of that transition correlate with model size/architecture across the six models? \n\nCould noisy or distributional natural shifts (non-adversarial OOD) produce similar topological signatures? That is, how specific is the signature to malicious adversarial influence vs. benign OOD?\n\nThe authors should consider releasing their code publicly for reproducibility by the community. \n\nCan the authors highlight the details of their experimental setup and the hardware/infrastructure used?\n\nThe authors should also consider showing a specific example across different models for the reader to visualize the method a little less abstractly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tXwfEVZhG9", "forum": "v2PglvLLKT", "replyto": "v2PglvLLKT", "signatures": ["ICLR.cc/2026/Conference/Submission17657/Reviewer_d2uJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17657/Reviewer_d2uJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007616058, "cdate": 1762007616058, "tmdate": 1762927511680, "mdate": 1762927511680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how adversarial interventions reshape the latent geometry of large language models (LLMs) using topological data analysis and persistent homology (PH) diagrams. The analysis is done across several instruction-tuned models and two attack types (prompt injection and backdoor fine-tuning), and PH barcodes are computed from layer activations, summarizing them into 41-dimensional feature vectors, and comparing clean versus adversarial conditions. The authors report a recurring pattern termed “topological compression”, fewer 1-dimensional bars and larger 0-bar death times for adversarial inputs. Logistic regression on PH summaries separates clean and adversarial activations with high accuracy, roughly matching linear baselines. A second, “local” analysis applies PH to neuron-pair across layers to examine layer-to-layer information flow, reporting differences in total 1-bar persistence and proposing PH variance as an unsupervised indicator of affected layers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Originality of the approach.**\n  Applying persistent homology to adversarial analysis in LLMs is novel to the best of my knowledge and interesting, bridging topological data analysis with model interpretability and robustness.\n\n* **Wide experimental coverage.**\n  The study spans multiple coverage, including very large scale models in the Appendix (70B) and two distinct adversarial mechanisms, showing qualitatively consistent trends across settings."}, "weaknesses": {"value": "* **Claims may not be fully supported by experiments.**\n  - The “topological compression” effect is observed descriptively but remains correlational. No controlled test distinguishes true topological simplification from simpler geometric or scaling changes: a simple example would be to match the scales of the original and poisoned features similarly as done in section 4.2. \n  - Linear baselines already achieve near-perfect separability (Table 1), aside from \"layer 0\", which is not showed for the PH stats  PH’s added value over simpler probes remains not fully clear to me in this setting. Moreover this could highlight that the binary task of distinguishing between poisoned and clean features might be too simple on this dataset and model pairs. \n  - The local analysis (Table 2) relies on a coarse metric (precision@k), without random baselines or multiple-comparison control. This weakens the claim that PH variance reliably identifies adversarially affected layers. A better metric to use would be spearman correlation. \n  - The results are not linked to behavioral or task-level outcomes (e.g. jailbreak success or refusal rates).\n\n\n* **Clarity and presentation quality.**\n  - Several results are difficult to interpret at first sight:  I think the authors should give priority for each figure to the result/plot that better highlights the current claim and put the remaining ones in the Appendix. For example Figure 8 show different information and is very dense, obscuring panel (d) which is the one that supports to the claim as (a), (b), (c) difference between poisoned and clean activation curves are not very visible. \n  - minor: Cross-model results are not included in the main text: I believe that a concise summary table in the main text would make the findings clearer and more convincing.\n\n\n* **Few ablations.**\nPH is computed only with one distance metric and one filtration type. No ablations on these design choices, or on subsample size, are provided, making it hard to assess stability, see [c,d] for discussions on how persistence diagrams can vary with metric choice. Scalability is not addressed beyond the subsampled settings; approximate PH algorithms such as witness complexes or streaming approaches [e] could test whether the reported effects persist at realistic layer scales. The 41-dimensional summarisation is also not discussed in comparison with alternative vectorisations such as persistence images [a]  or persistence landscapes [b], leaving uncertain whether the observed pattern depends on this particular encoding. I suggest to include at least a discussion on these choices should be included in the paper. \n\n_[a] Adams, H., et al. (2017) Persistence Images: A Stable Vector Representation of Persistent Homology. JMLR_\n\n_[b] Bubenik, P. (2015) Statistical Topological Data Analysis Using Persistence Landscapes. JMLR_\n\n_[c] Chazal, F., et al. (2015) Convergence Rates for Persistence Diagrams. JMLR_ \n\n_[d] Cohen-Steiner, D., Edelsbrunner, H. & Harer, J. (2007) Stability of Persistence Diagrams. Discrete & Computational Geometry 37(1):103–120._\n\n_[e] Kerber, M., Morozov, D. & Nigmetov, A. (2016) Geometry Helps to Compare Persistence Diagrams. J. Exp. Algorithmics 22(1):1–20._"}, "questions": {"value": "- Is there any motivation  not considering higher order bars (2-bars, 3-bars) etc?\n\n-  How exactly is the variance in Table 2 computed: across all samples or separately per condition?\n\n- Would the same compression pattern appear under different subsample sizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EPFL1Z5Sgq", "forum": "v2PglvLLKT", "replyto": "v2PglvLLKT", "signatures": ["ICLR.cc/2026/Conference/Submission17657/Reviewer_DxAW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17657/Reviewer_DxAW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051771026, "cdate": 1762051771026, "tmdate": 1762927510837, "mdate": 1762927510837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}