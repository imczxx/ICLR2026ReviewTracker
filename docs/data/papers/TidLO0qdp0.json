{"id": "TidLO0qdp0", "number": 6242, "cdate": 1757961561790, "mdate": 1759897927355, "content": {"title": "Unlearning Diffusion Policies with Relative Fisher Forgetting", "abstract": "Diffusion policies have recently advanced offline reinforcement learning (RL) by enabling expressive and multi-modal action generation. \nAs these models move closer to real applications, it becomes important to remove the influence of specific data, either for privacy reasons, to eliminate unsafe behaviors, or to meet regulatory requirements. Existing unlearning methods, however, cannot handle diffusion-based policies because training influence is spread across the denoising process and reinforced by critic values. In this paper, we present Relative Fisher Forgetting (RFF), the first framework for unlearning in diffusion-based offline RL. RFF removes unwanted data influence through two complementary components: actor unlearning with noise aware influence gradients that are scaled by relative Fisher importance, and critic unlearning that suppresses value estimates for forgotten trajectories. To ensure stability, RFF alternates actor and critic updates and introduces gradient norm control, retain set regularization, and convergence monitoring. Experiments on MuJoCo control benchmarks for both single-task and multi-task settings show that RFF reliably removes designated trajectories and behaviors while preserving performance on retained data, outperforming retraining and prior unlearning baselines in efficacy and efficiency.", "tldr": "", "keywords": ["Machine Unlearning; Offline Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/78e14484d1e1e61da9f136c024c7d4c2e6973eba.pdf", "supplementary_material": "/attachment/a9fb401ec1dbac7e586356e3333068db9eb834b0.pdf"}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical and novel problem of machine unlearning in diffusion-based offline reinforcement learning (RL) policies. The authors identify that existing unlearning methods are ineffective for diffusion policies because training influence is dispersed across the multi-step denoising process and reinforced by the critic's value estimates.\nTo solve this, the paper introduces Relative Fisher Forgetting (RFF), a principled framework to remove the influence of specific data ($\\mathcal{D}_f$) while preserving performance on retained data ($\\mathcal{D}_r$). RFF is a dual-component system that unlearns from both the actor and the critic simultaneously:\n1. Actor Unlearning: The policy ($\\epsilon_{\\theta}$) is updated using \"forgetting gradients\" derived from $\\mathcal{D}_f$. Crucially, these gradients are scaled per-parameter by the relative Fisher importance. This metric, computed from the empirical Fisher diagonals of the forget and retain sets, protects parameters vital for $\\mathcal{D}_r$ (preventing catastrophic forgetting) while aggressively updating parameters specific to $\\mathcal{D}_f$.\n2. Critic Unlearning: The value function ($Q_{\\phi}$) is updated with a hybrid loss. It continues to perform standard TD regression on the retain set $\\mathcal{D}_r$ while simultaneously applying a value suppression loss on the forget set $\\mathcal{D}_f$, (Eq. 3), explicitly de-incentivizing the forgotten behaviors.\n\nExperiments on MuJoCo benchmarks for both trajectory-level and behavior-level unlearning demonstrate that RFF effectively removes the influence of $\\mathcal{D}_f$ and significantly outperforms baselines (including retraining and SOTA unlearning methods like EraseDiff) in preserving utility on $\\mathcal{L}_r$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Significance & Novelty**: The paper is the first to systematically address the unlearning problem in the increasingly important domain of diffusion-based offline RL policies. This is a timely and critical contribution.\n2. **Technical Depth**: The method is technically strong. The key insight that naive updates are insufficient due to \"parameter-level\" knowledge entanglement is astute. The use of relative Fisher importance as an arbitrator to protect shared parameters is a highly effective solution.\n3. **Practical Efficiency**: The proposed unlearning procedure is fast and competitive with simpler (but lower-performing) baselines.\n4. **Strong Empirical Validation**: The experiments are comprehensive. Testing on both trajectory-level (data removal) and behavior-level (skill removal) unlearning demonstrates the method's robustness, with clear, convincing separation from all baselines."}, "weaknesses": {"value": "1. **Hyperparameter Sensitivity**: The method appears to be highly sensitive to hyperparameter tuning. Appendix Table 6 shows that key parameters, such as the actor learning rate ($\\eta$) and the critic suppression margin ($\\delta_Q$), vary by an order of magnitude (10x) across different tasks. This strongly suggests that RFF is not a 'plug-and-play' solution and requires careful, task-specific tuning to achieve the reported results.\n2. **Ambiguity of \"Convergence Adapter\" / Practicality**: The practicality of the method is further questioned by Algorithm 5 (Convergence Adapter). The pseudocode describes corrective steps in natural language (e.g., \"Action: decrease $\\eta$ or increase $\\beta$\") rather than as explicit algorithmic operations. This raises a significant concern: is this an automatic, self-tuning mechanism, or is it a manual tuning guide for a human expert? If the latter, it severely limits the method's reproducibility and practical deployability.\n3. **Scalability with $\\mathcal{D}_f$ Size**: The experiments are limited to relatively small forget sets (1%-5% for trajectory unlearning). It is unclear how RFF's approximations and performance scale as the size of $\\mathcal{D}_f$ increases (e.g., to 10%, 25%, or 50%).\n4. **Questionable Problem Framing**: As seen in Fig. 1c, the model trained only on $\\mathcal{D}_r$ still learns the 'forget' behavior. This makes RFF's success appear less about 'unlearning data' and more about 'concept ablation,' which is a different (though related) problem. The paper fails to adequately discuss the implications of this finding."}, "questions": {"value": "1. The most critical point of ambiguity is Algorithm 5. Could you please clarify if the \"Convergence Adapter\" is a fully automatic algorithm? If so, could you provide the exact logic (e.g., the update rule for $\\eta$)? Or, does it represent a manual tuning process guided by monitoring $D_{\\pi}$ and $D_Q$?\n2. Given the hyperparameter sensitivity shown in Table 6, how much tuning was required for each task? Does the \"Convergence Adapter\" automate this, or was this a manual search?\n3. How does RFF's performance (both forgetting efficacy and utility preservation) degrade as the size of the forget set $\\mathcal{D}_f$ increases? At what point would you hypothesize that full retraining on $\\mathcal{D}_r$ becomes the superior strategy?\n4. About the Retrain baseline's failure to forget (Fig. 1c), does this not imply that the forget behavior is learnable from $\\mathcal{D}_r$ alone? If so, how can you justify that RFF is performing 'data unlearning' (removing $\\mathcal{D}_f$'s influence) rather than 'behavior suppression' (which is a different objective)? Please justify the framing of this as an 'unlearning' problem in light of this result."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9dGk0Outyp", "forum": "TidLO0qdp0", "replyto": "TidLO0qdp0", "signatures": ["ICLR.cc/2026/Conference/Submission6242/Reviewer_PANp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6242/Reviewer_PANp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718721721, "cdate": 1761718721721, "tmdate": 1762918567555, "mdate": 1762918567555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of machine unlearning in offline reinforcement learning (RL), specifically for diffusion-model-based policies. The authors propose a framework called Relative Fisher Forgetting (RFF), trying to remove the influence of specific trajectories or behaviors from a sub-dataset without retraining. RFF works by combining two components: (1) an actor unlearning step that applies gradient updates to the diffusion policy network, scaled by a relative Fisher information weight to forget the targeted data, and (2) a critic unlearning step that suppresses the Q-values (value function) for the experiences that should be forgotten. The procedure alternates between actor and critic updates, with additional stabilization techniques (gradient clipping, regularization on retained data, etc.) to prevent catastrophic forgetting. Empirically, the paper demonstrates on MuJoCo benchmark tasks that RFF can reliably remove the effects of the unlearn subset  while maintaining the policy’s performance on the remaining (retained) data. The proposed approach reportedly outperforms prior unlearning baselines in both effectiveness (more thorough forgetting) and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The unlearning problem in offline diffusion RL is an interesting question that I have never thought off. It is a fair problem with potential use cases in the future when robotics become more common for human daily life.\n\nThe experimental results on MuJoCo offline RL tasks support the claims. The paper shows that after applying RFF, the policies stop exhibiting behaviors from the forbidden data (e.g., a certain trajectory or skill is unlearned), yet their performance on the remaining data distributions is largely preserved. The results demonstrate clear forgetting without a large hit to overall reward on the kept data, which validates the approach’s utility."}, "weaknesses": {"value": "The core algorithmic approach in RFF appears to draw heavily from existing ideas in machine unlearning and continual learning, and the main novelty lies in applying these known techniques to the diffusion-offline-RL setting. In essence, the method performs additional gradient descent steps on a subset of data with a Fisher information-based weighting – a strategy reminiscent of prior unlearning methods in supervised models and policy forgetting in RL. The inclusion of RL-specific losses (TD loss for the critic and DDPM loss for the policy) is a necessary adaptation, but it is a fairly incremental change. As a result, the paper’s contribution may be perceived as modest: it extends known unlearning mechanisms to a new context rather than introducing fundamentally new theory or algorithms.\n\nThe explanation and justification of the unlearning procedure lack depth in places. The paper provides a high-level intuitive reasoning (e.g. using Fisher information to protect important parameters) and cites plausible inspirations, but it does not delve deeply into theoretical analysis or ablation to illuminate why the method works so effectively.\n\nWhile the paper motivates the problem with scenarios like privacy and safety, it’s a bit unclear how practical or common these unlearning scenarios are in real-world RL deployments. The experiments are conducted on standard simulated control tasks with artificial unlearning tasks (e.g., forgetting a direction in Ant environment). It would strengthen the work to better demonstrate or application that such targeted unlearning would be needed in real applications and that this approach would handle them."}, "questions": {"value": "Backpropagating gradients through the denoising chain can be computational expensive. Although this is not the main focus of the paper, I wonder how you handle the backpropagation through the denoising steps and its computation cost.\n\nIs there any reason why you adopt the \"relative Fisher-weighted update rule\"? Why using equation 2 as opposed to other way of using the Fisher matrix?\n\nThe choice of $\\lambda$ in Equation 3 and its loss function design is also quite ad-hoc to me. I don't find a strong explanation to use this weighted loss for the critic unlearning (and the choice of the weight $\\lambda$)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FxkoFZMH2D", "forum": "TidLO0qdp0", "replyto": "TidLO0qdp0", "signatures": ["ICLR.cc/2026/Conference/Submission6242/Reviewer_L2Z7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6242/Reviewer_L2Z7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889330917, "cdate": 1761889330917, "tmdate": 1762918567188, "mdate": 1762918567188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for diffusion-based offline RL unlearning named Relative Fisher Forgetting (RFF). The framework combines the relative Fisher importance weighted actor unlearning gradient with the value suppression-based critic unlearning. Several techniques are also introduced for stable training. Experiments on MuJoCo benchmarks show that RFF outperforms baselines in both trajectory-level and behavior-level unlearning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper aims to solve the novel task of offline diffusion policy unlearning, which is an important task with real-world impact.\n2. The paper is well-written."}, "weaknesses": {"value": "1. One goal of unlearning is that we want the unlearned policy to behave as if $D_f$ had never been used. However, this does not strictly align with the objective of minimizing the Q-value in the $D_f$ region. For example, the Q-value in the $D_f$ region will likely be close to that of its neighborhood region, and not simply the smaller the better. However, in both training loss definition and the experiment evaluation the authors assume these two different goals are the same.\n2. Both the actor unlearning loss and the critic unlearning loss are introduced without adequate theoretical analysis. \n3. Minor issue: Figure 5 lacks legend."}, "questions": {"value": "1. In the actor unlearning loss, why use $\\hat a^0$ instead of the a_0 action denoised with multiple steps?\n2. In Table 1, why is the $\\mathcal{D}_r$ score of RFF lower than that of TrajDeleter in two of the three tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1JEtlnFLcD", "forum": "TidLO0qdp0", "replyto": "TidLO0qdp0", "signatures": ["ICLR.cc/2026/Conference/Submission6242/Reviewer_xmvt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6242/Reviewer_xmvt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990542212, "cdate": 1761990542212, "tmdate": 1762918566700, "mdate": 1762918566700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}