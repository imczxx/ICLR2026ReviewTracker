{"id": "vnipyA8c9V", "number": 9559, "cdate": 1758127543611, "mdate": 1763718244227, "content": {"title": "Intrinsic Entropy of Context Length Scaling in LLMs", "abstract": "There has been work discussing the impact of long context on Language Model performance: some find that long irrelevant context could harm performance, while some experimentally summarize loss reduction by relevant long context as Scaling Laws. This calls for a more thorough understanding on how long context impacts Language Modeling. In this work, we (1) propose to use Intrinsic Entropy for explaining the impact of context length on language modeling; and (2) conduct experiments on natural language and synthetic data, validating our proposed theoretical assumptions and deductions. Our theoretical framework can provide practical insights such as establishing that training dataset size dictates an optimal context length and bounds context length scaling for certain cases. We hope our work may inspire new long context Language Models, as well as future work studying Physics for Language Models.", "tldr": "We propose to use Intrinsic Entropy for understanding impact of context length on Language Modeling, and conduct experiments to validate theoretical assumptions and deductions with language and synthetic datasets.", "keywords": ["context length", "intrinsic entropy"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2275ffba03d4e4d9123cbfb00729cce77918ed67.pdf", "supplementary_material": "/attachment/d71ea016108f569a90eff80b3cf62323688d37c0.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a theoretical and empirical framework for\nunderstanding how context length affects the performance and scaling\nbehavior of large language models. The authors propose the concept of\nIntrinsic Entropy, which quantifies the information a model can encode\nwithin its internal (“intrinsic”) representation space given a\ncertain context length. They decompose cross-entropy loss into two\nparts — the Bayes Risk, representing the irreducible uncertainty from\nlimited context, and the Approximation Loss, representing imperfect\nmodel learning. From this perspective, the Bayes Risk decreases\nmonotonically with longer context since more information is available,\nwhereas the Approximation Loss tends to increase because longer\ncontexts enlarge the model’s effective input dimension and complicate\noptimization. The trade-off between these opposing effects gives rise\nto an optimal context length that minimizes total loss for a given\ndataset size.\n\nEmpirically, experiments using LLaMA-3.1 (8B and 70B) on OpenWebText\nsubsets and a synthetic parity dataset confirm the theory. The\nmeasured relationship between Intrinsic Entropy and Cross-Entropy Loss\nis nearly linear (correlation > 0.97), and the validation loss across\ncontext lengths follows a scaling law of the form H=C0 + C / l^gamma.\n. Moreover, when varying training-data size, the optimal context\nlength shifts upward — indicating that larger datasets justify\nproportionally longer contexts. The findings imply that excessively\nextending context windows can degrade performance when data are\ninsufficient, emphasizing the need for joint optimization of context\nlength, data size, and model capacity. This framework bridges\nstatistical physics–style scaling laws with information-theoretic\nreasoning, offering a principled explanation for when and why long\ncontexts help or harm LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel concept — Intrinsic Entropy — that\nlinks information-theoretic measures to model-internal representations\n(“intrinsic space”). This provides a principled bridge between\nentropy, Bayes risk, and approximation loss, enabling a quantitative\nexplanation for the existence of an optimal context length. By\nderiving analytical relationships the work extends traditional\nscaling-law theory (e.g., Kaplan et al. 2020) to the underexplored\ndimension of context length. This is conceptually powerful: it\nreframes the intuition that “longer is better” into a mathematically\ngrounded trade-off between information gain and representational\ncomplexity.\n\n\n\n\n2. Experimentally, the paper validates its theory using LLaMA 3.1 models\nand synthetic data, showing a near-perfect linear correlation (R^2 >\n0.97) between Intrinsic Entropy and Cross-Entropy loss, across varying\ncontext lengths. The authors further identify a data-dependent optimal\ncontext length, which scales predictably with dataset size, and\nreproduce this result across different datasets. This dual\ntheoretical–empirical consistency gives the framework strong\ncredibility and practical relevance: it offers a diagnostic tool for\noptimizing context length in training regimes and provides insight\ninto the physics-like scaling behavior of LLMs.  In short, the paper’s strength lies in its integration of\nfirst-principles reasoning, strong empirical validation, and broad\napplicability — offering both a new theoretical lens and actionable\ninsight into how LLMs handle long contexts."}, "weaknesses": {"value": "1. The paper’s theory hinges on several idealized assumptions that may\nnot hold in practice: The notion of a stable “Intrinsic Space” — a well-behaved latent\nmanifold shared across models — is assumed rather than\ndemonstrated. In reality, representations in transformer layers are\nnon-stationary and architecture-dependent, making it difficult to\nassert that a single intrinsic space can be defined across context\nlengths. The linearity assumptions — e.g., that Bayes Risk is linearly\nproportional to Intrinsic Entropy or Intrinsic Dimension — simplify\nthe analysis but are only loosely supported empirically. These\nrelationships might hold approximately in narrow regimes but could\nbreak under different architectures, data modalities, or tokenization\nschemes. While mathematically elegant, the framework risks being too abstract\nto describe the heterogeneous behaviors of real LLMs under diverse\nconditions.\n\n\n\n2. The experiments, though carefully designed, rely on a small number of\ndatasets and model families: Only LLaMA 3.1 (8B and 70B) and OpenWebText subsets are tested for\nnatural language, with one synthetic dataset for controlled\nvalidation. There is no downstream evaluation (e.g., reasoning or QA tasks) to\nverify whether the proposed “optimal context length” correlates with\nfunctional performance. The paper also lacks cross-model validation — results are not shown\nfor alternative architectures like Mistral or Gemini-style\nmixture-of-experts, which could challenge the universality of the\nscaling law. The findings, while compelling within the tested settings, remain\ntentative as a general principle across model types and data regimes.\n\n\n\n3. Although the “Intrinsic Entropy” construct is theoretically neat, it\nis not directly measurable without heavy computational overhead (e.g.,\neigenvalue decompositions of hidden representations across\ncontexts). Moreover, its connection to optimization dynamics and\narchitecture-level design remains abstract — the paper suggests\nguidelines for tuning context length but provides no concrete\nalgorithm or scaling heuristic to implement this in real-world\ntraining. The work’s insights are conceptually illuminating but operationally\ndifficult to apply for practitioners designing next-generation\nlong-context LLMs."}, "questions": {"value": "1. The entire theory rests on the existence of a stable Intrinsic Space\nwhere internal representations of data reside and where entropy can be\nmeasured. However, as the authors themselves admit, this space may\ndepend not only on the data but also on the neural network\narchitecture and training dynamics.  Can Intrinsic Entropy be defined independently of specific\narchitectures or layer representations, making it a universal property\nof language modeling itself?\n\n\n2. While the framework explains cross-entropy scaling, it is unclear\nwhether higher or lower Intrinsic Entropy predicts better reasoning,\nretrieval, or long-document comprehension. The authors validate on\nsynthetic and language data but do not link the proposed measure to\nfunctional metrics. Does Intrinsic Entropy correlate with actual task-level performance,\nor does it capture only internal statistical regularities?\n\n3. Can the optimal context-length principle generalize across models,\ndata types, and modalities? The observed law that the optimal context length increases with\ndataset size is based on a limited set of models (LLaMA-3.1) and\ncorpora (OpenWebText, synthetic parity). Whether this scaling\nrelationship extends to multimodal LLMs, retrieval-augmented\narchitectures, or dialogue models remains untested.  Is the “optimal context length” phenomenon universal across model\nfamilies and data modalities, or an artifact of specific transformer\ntraining regimes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UAWJO47mwf", "forum": "vnipyA8c9V", "replyto": "vnipyA8c9V", "signatures": ["ICLR.cc/2026/Conference/Submission9559/Reviewer_ULtS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9559/Reviewer_ULtS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761095826574, "cdate": 1761095826574, "tmdate": 1762921116446, "mdate": 1762921116446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new scaling law that takes context length into account. It has the form of\n$$L = C_0 + C/l^\\gamma + A(l)/D^{\\alpha(l)}$$\nWhere l is the context length. This form is a natural generalization of the scaling law suggested by Kaplan et. al. to the case of a variable context length. It captures the trade-off between reduced training sample efficiency and increased generation confidence that LLM experiences as the context length grows.\n\nThe authors study the form of $\\alpha(l)$ and show that it can be represented as $\\alpha(l) = c\\dim(l)$ with $dim(l)$ being the dimensionality of the latent space necessary to represent text of length l. Since larger text naturally may contain more information, the latent dimensionality grows with l, which means that $\\alpha(l)$ decreases. Thus, the suggested scaling law consists of two terms, one of which increases with l while the other one decreases, which implies the existence of an optimal value of l.\n\nThe results summarized here are shown rigorously in the presented work. It is worth noticing that the relation between the coefficient $\\alpha$ of the scaling law and the intrinsic dimension of the data was known in the literature. The authors push it further by leveraging the relationship between the intrinsic dimension and the context length."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents an interesting result, consisting of a non-trivial observation and a sound and consistent theory.\n\nThe authors provide a convincing set of evidence that supports their theoretical assumptions."}, "weaknesses": {"value": "The paper claims that the theoretical framework presented can provide practical insights, such as establishing that the size of the training dataset dictates an optimal context length and bounds context-length scaling. It seems to miss that, in practice, the context length is determined not to minimize the loss function but rather for other considerations, such as the typical context length in the target use cases and the capabilities of the computational hardware. If the prescribed “optimal” context length is larger than the typical context length in applications, the context length will still be kept within the application's demands due to computational constraints. If the prescribed “optimal” context length is shorter than the typical context length in applications, the context length will still be kept up to the application's demand, or the system would not be deployable.\n\nAnother argument against the practicality of the delivered result is that the real objective behind LLM training is not the cross-entropy values, but the evaluation results on downstream metrics. To prove the practicality of the results, the authors would need to conduct experiments that hold compute/memory constant and study task metrics (e.g., RAG accuracy, long-form QA F1, code completion pass@k) against context length"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HlXm4b76mK", "forum": "vnipyA8c9V", "replyto": "vnipyA8c9V", "signatures": ["ICLR.cc/2026/Conference/Submission9559/Reviewer_vDBt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9559/Reviewer_vDBt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663370381, "cdate": 1761663370381, "tmdate": 1762921116070, "mdate": 1762921116070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to investigates into LLM context length from model entropy perspective. The paper leverages cross entropy (and treated as Bayes Risk in the paper, for example in Figure 2 and explained in 2.2.2) and show that it empirically bears a linear relationship with intrisic entropy which is calculated as log of the eigenvalues. Furthermore, by approxmimating the model approximation error in section 2.3, the paper shows theoretically that there exists an optimal context length in section 3 and also demonstrate it empirically."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The direction which contructs a decomposition and shows that optimally there exists an optimal context length as shown in section 3 is very interesting. Eq6/7 effectively shows that D would imply an optimal L. There are several empirical results that demonstrate this; besides, the linear relationship between cross entropy and intrinsic entropy seems novel and insightful."}, "weaknesses": {"value": "I personally think that the mathematical construction lacks rigor. For example, there is no justification of equaling states with volume divided by dimensions. Although eq(3) is understandable, it is not an acceptable exposition since l is discrete. It is also problematic to directly treat crossentropy as Bayes Risk particularly there is no justification provided throughout the paper.\n\nThe paper is badly presented. The figures are not very readable most of the time with non informative titles (e.g. Figure 1, part1/part2/part3) and equations through 381-386. All these make papers more read as a draft instead of a formal conference submission. All the above significantly makes the reading process harder. For example, I am not able to understand the synthetic task presented in Figure 5 right part (with carefully reading its descriptions 372-377)/"}, "questions": {"value": "Figure 1 and Figure 7 shows the correlation between cross entropy and N. How are samples obtained for a particular N please? More in detail, how is correlation calculated for a particular (N, cross entropy) pair?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iiba7evsiG", "forum": "vnipyA8c9V", "replyto": "vnipyA8c9V", "signatures": ["ICLR.cc/2026/Conference/Submission9559/Reviewer_Mnty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9559/Reviewer_Mnty"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777856899, "cdate": 1761777856899, "tmdate": 1762921115810, "mdate": 1762921115810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the impact of context length on Language Model (LM) performance through a novel theoretical lens termed \"Intrinsic Entropy.\" The authors posit that the cross-entropy loss can be decomposed into Bayes Risk (the irreducible error of an optimal model) and Approximation Loss (the error from the trained model failing to match the optimal model). The core contribution is the proposal that the Bayes Risk is linearly related to the Intrinsic Entropy of the data manifold, which itself is a function of context length. They further argue that while longer contexts reduce Bayes Risk by providing more information, they simultaneously increase the intrinsic dimension, making the Approximation Loss harder to minimize, especially with limited data. This trade-off leads to an \"optimal context length\" for a given training dataset size, beyond which validation loss increases. The paper supports its claims with experiments on both natural language (OpenWebText with LLaMa and GPT-2) and a carefully constructed synthetic dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. While the decomposition of loss into Bayes Risk and Approximation Error is standard, and the use of an \"intrinsic space\" is inspired by prior work, the formulation of \"Intrinsic Entropy\" as a central concept to bridge context length and loss is novel. \n2. The idea that there is a fundamental trade-off leading to an optimal context length is a significant and non-obvious insight that challenges the simplistic view that \"more context is always better.\" \n3. The paper is generally well-structured and clear."}, "weaknesses": {"value": "My main concern is that existing work [1] has pointed out that PPL cannot serve as a standard for evaluating long-text performance. Therefore, experimental results on more diverse long-text benchmarks are necessary, such as RULER, HELMET, and Longbench v2.\n\n[1]Fang L, Wang Y, Liu Z, et al. What is Wrong with Perplexity for Long-context Language Modeling?[J]. arXiv preprint arXiv:2410.23771, 2024."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "maS0VxHiRu", "forum": "vnipyA8c9V", "replyto": "vnipyA8c9V", "signatures": ["ICLR.cc/2026/Conference/Submission9559/Reviewer_8bxz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9559/Reviewer_8bxz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822047069, "cdate": 1761822047069, "tmdate": 1762921115460, "mdate": 1762921115460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}