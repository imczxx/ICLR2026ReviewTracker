{"id": "ltnplSHUGm", "number": 7291, "cdate": 1758014489321, "mdate": 1759897861719, "content": {"title": "TEPO: A Transferable EDA Prediction Optimization Method Based on Learngene Characterization", "abstract": "This paper introduces TEPO, a novel multi-task learning framework to optimize Electronic Design Automation (EDA) in integrated circuit (IC) design by addressing increasing complexity and the limitations of traditional independent design task approaches. TEPO systematically decomposes design knowledge into gene knowledge and class knowledge, which are referred to as Learngenes. This framework employs a dual-pathway architecture with an adaptive gating mechanism, allowing for fine-grained control over knowledge activation and enhancing computational efficiency and interpretability. In the data input section,  the VIT-GNN fusion processor, which integrates Vision Transformer (ViT) features from layout images with Graph Neural Network (GNN) features from circuit topology, spatially aligning them onto a unified 256x256 grid to preserve both global visual patterns and local structural relationships. Our approach tackles four critical challenges in EDA: knowledge fragmentation, feature integration, transferability and data scarcity. The methodology involves pre-training an upstream model to extract Learngene, which is then used to initialize a downstream 12-layer Transformer model for various prediction tasks. Experiments are conducted on CircuitNet-N28, a dataset providing multi-modal features for Congestion, DRC violations, and IR-drop prediction tasks, as well as a new thermal prediction task. The transferability of learning genes not only performs well in existing categories but also shows a faster convergence speed in new task categories. The data required for its training is also less, it saves more computing costs while achieving the same performance.", "tldr": "A transferable predictive optimization method based on Learngene in the field of Electronic Design Automation", "keywords": ["Transfer Learning", "Electronic Design Automation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/99ce7608cd7c702eabccd85236e12d6c1fc06958.pdf", "supplementary_material": "/attachment/dd4db6c8160b5d0409321246a5dc971cdbc99171.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes TEPO, a transferable framework for EDA prediction. TEPO factorizes each layer’s weights into task-agnostic “gene knowledge” and task-specific “class knowledge” (via an SVD-style decomposition) and combines them through dual-path gating during transfer. On the feature side, layout images (ViT) and netlist/topology features (GNN) are spatially aligned and fed to a Transformer. Experiments cover standard backend targets (congestion/DRC/IR-drop) and include an additional Thermal task aimed at testing cross-category transfer. The manuscript reports faster convergence and lower final errors than Xavier initialization and several EDA baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear modular design. The weight factorization plus gating is conceptually coherent; the multi-modal alignment (layout + netlist) is a sensible engineering pipeline.\n- Transfer signal. Results indicate faster convergence and improved endpoints over vanilla initialization across multiple tasks, suggesting value as a pretraining/initialization strategy.\n- Practical motivation. Addressing multi-task transfer in data-constrained EDA settings is relevant to downstream design flows."}, "weaknesses": {"value": "- Missing efficiency and reproducibility details. Training time, inference runtime/latency, computing resources (hardware, memory), and model size/compute (parameters/FLOPs) are not reported, limiting assessment of the claimed efficiency and hindering reproduction.\n- Limited statistical reporting. The manuscript does not provide evaluations over multiple random seeds or dispersion measures (e.g., mean ± std, confidence intervals) and does not report significance testing, weakening the strength of the empirical claims.\n- Thermal label transparency. The Thermal ground truth appears to be taken directly from the referenced dataset (so transparency would largely inherit from the dataset), yet the paper does not explain how those labels are defined or verified within the dataset context; a short clarification would help readers assess fidelity and reproducibility.\n- Task coverage gap (timing). Timing prediction (e.g., slack/WNS/TNS) is a key objective in EDA because it directly guides placement optimization and impacts final performance and PPA quality. Recent timing-prediction frameworks such as E2ESlack [1] and PreRoutGNN [2] explicitly address pre-routing timing prediction using global pretraining and local delay learning. Moreover, cross-stage optimization work such as LaMPlace [3] highlights that improving timing correlation during placement leads to better downstream metrics, underscoring the importance of evaluating timing transfer."}, "questions": {"value": "1) Gene/Class ranks and gating. How are the per-layer ranks for “gene” vs. “class” chosen (fixed ratios or data-driven), at what granularity is gating applied (per-layer, per-head, or per-channel), and what sensitivity is observed on convergence, final accuracy, and compute/latency?  \n2) Thermal ground truth. If Thermal labels are inherited from the dataset, how are those labels defined (solver, boundary conditions, grid/resolution, dataset QA) and how is leakage between training and test designs or stages avoided?  \n3) Timing prediction. Since timing is one of the most critical tasks in EDA and directly guides placement for better downstream performance, why is timing not included among the evaluated tasks, and how would TEPO be expected to transfer to timing (e.g., pre-routing slack/WNS/TNS) compared with baselines such as E2ESlack [1], PreRoutGNN [2]?\n\n[1] Bodhe, S., Zhang, Z., Hamidizadeh, A., Kai, S., Zhang, Y., & Yuan, M. (2025). E2ESlack: An End-to-End Graph-Based Framework for Pre-Routing Slack Prediction. arXiv preprint arXiv:2501.07564.  \n\n[2] Zhong, R., Ye, J., Tang, Z., Kai, S., Yuan, M., Hao, J., & Yan, J. (2024, March). Preroutgnn for timing prediction with order preserving partition: Global circuit pre-training, local delay learning and attentional cell modeling. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 38, No. 15, pp. 17087-17095).\n\n[3] Geng, Z., Wang, J., Liu, Z., Xu, S., Tang, Z., Kai, S., ... & Wu, F. LaMPlace: Learning to optimize cross-stage metrics in macro placement. In The Thirteenth International Conference on Learning Representations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QZkVdV7rPz", "forum": "ltnplSHUGm", "replyto": "ltnplSHUGm", "signatures": ["ICLR.cc/2026/Conference/Submission7291/Reviewer_1vTs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7291/Reviewer_1vTs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761532458733, "cdate": 1761532458733, "tmdate": 1762919413187, "mdate": 1762919413187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TEPO, a transferable multi-task framework for EDA prediction. TEPO fuses ViT features from layout images with GNN features from circuit topology by spatially aligning both on a 256×256 grid, and  decomposes model weights into “gene” (shared) vs “class” (task-specific) components with adaptive gating to route knowledge per task. The system is pre-trained on congestion, DRC, and IR-drop and then transferred to downstream tasks, including a new thermal prediction task. Experiments on CircuitNet-N28 claim faster convergence, better data efficiency, and improved accuracy over random (Xavier) initialization and several SOTA baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. There is Clear motivation for early, transferable prediction across EDA tasks.\n2. The article has few writing errors and the charts are clear in meaning."}, "weaknesses": {"value": "1. TEPO underperforms RouteNet on IR-drop in Table 3 (0.027 vs 0.014), so “surpasses existing EDA models” needs qualification by task.\n2. CircuitNet-N28 provides a large amount of data on different designs. Only 100 designs were used for training and 20 designs for testing, which raises concerns about the effectiveness and scalability.\n3. The selected SOTA is not the current or recent single-task SOTA.\n4. This article lacks ablation experiments to prove the effectiveness of each part of the design.\n5. This multi-task learning method should have a large number of gradient conflicts among different tasks, but no solution to the gradient conflicts has been seen. So, I am skeptical that the experimental results are better than those of models specifically designed for a single task."}, "questions": {"value": "1.CircuitNet-N28, this dataset has no thermal ground truths. How did you obtain the labels for the experiment? Any validation vs. physics-based solvers?\n2.What is the exact feature difference loss between ViT patches and GNN nodes? Any alternatives tested (e.g., cross-attention alignment)?\n3.Table 2 lacks 30-sample results, is not matched with the description before,\n4. Please provide the mathematical form of σ_gene, σ_class, and g_task, and an ablation bewteen gating and simple additive mixing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hTnYXQRivM", "forum": "ltnplSHUGm", "replyto": "ltnplSHUGm", "signatures": ["ICLR.cc/2026/Conference/Submission7291/Reviewer_C7yg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7291/Reviewer_C7yg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832208158, "cdate": 1761832208158, "tmdate": 1762919412574, "mdate": 1762919412574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed a transferable EDA prediction optimization method by Learngene, which is a weights initialization method by leveraging singular value decomposition (SVD) on the pre-trained weights. By this initialization method, this work performs better than random (Xavier) initialization. Besides, this work used VIT and GNN for multi-modal fusion to improve quality."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This work first leveraged Learngene in the EDA prediction task.\n2. The performance of this work is better than random initialization."}, "weaknesses": {"value": "1. The experiments of this work are too weak to verify the transferability of their method. As the title of this paper suggests, this work targets the transferable learning problem in the EDA domain. However, the authors only compare their method with random (Xavier) initialization.\n1. The paper lacks a detailed explanation of data distribution, especially since the authors only select 120 samples (100 for training, 20 for testing) from a large-scale CircuitNet-N28 dataset (over 10K samples). Meanwhile, I think experimenting on the whole CircuitNet dataset will be more convincing.\n1. The experiment settings are wrong for the thermal prediction task. As shown in Tables 1/2/3, I can not understand why the value of temperature is used to evaluate the model performance. In my opinion, the authors should use MSE, which is used for Congestion, DRC, and IR-drop predictions. Meanwhile, the authors didn't mention how to generate the thermal data in the experimental section."}, "questions": {"value": "1. The title of this paper consists of \"prediction\" and \"optimization\". I think the paper only propose prediction method without optimization?\n2. The pre-training loss contains the losses of downstream tasks. Therefore, I think is a multi-stage training strategy rather than transfer learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7OJ4gL9Oqg", "forum": "ltnplSHUGm", "replyto": "ltnplSHUGm", "signatures": ["ICLR.cc/2026/Conference/Submission7291/Reviewer_RtUC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7291/Reviewer_RtUC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838649103, "cdate": 1761838649103, "tmdate": 1762919412205, "mdate": 1762919412205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TEPO, a transfer learning framework for chip design prediction tasks. It combines a GNN (circuit topology) and ViT (layout image) by aligning features on a shared 256×256 grid. It then decomposes model weights via SVD into “gene” (universal) and “class” (task-specific) components (“Learngene”). These components are used to initialize a downstream transformer for new tasks. TEPO is evaluated on CircuitNet-N28 for congestion, DRC, IR-drop, and a new thermal task, and claims better accuracy, faster convergence, and better data efficiency than Xavier initialization and several existing EDA models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The SVD-based split of weights into transferable “gene” vs task-specific “class” knowledge is an interesting, explicit formulation of reusable IC design knowledge.\n* The multimodal fusion of graph (netlist/topology) and layout image features is reasonable for EDA and is implemented end-to-end.\n* TEPO shows faster convergence and better sample efficiency, including on a task (thermal) not seen during pretraining.\n* Tables and curves suggest TEPO beats Xavier init and standard GNN/EDA baselines (GCN, GAT, RouteNet, etc.) in final accuracy and convergence speed."}, "weaknesses": {"value": "1. **Learngene identifiability is under-justified.**\n   The paper assumes “top 512 singular directions = universal” and “bottom 256 = task-specific,” but gives no theory or ablation to prove that split is actually semantic, unique, or robust.\n\n2. **Gating is under-specified.**\n   The paper refers to a gating mechanism that turns gene/class knowledge on or off per task, but never gives the actual gating function, loss, or how leakage between the two is prevented.\n\n3. **Limited baselines and no statistics.**\n   TEPO is only compared against Xavier for transfer initialization, and does not compare to other transfer / flow-tuning / knowledge-transfer frameworks in EDA. All results are single numbers: no std devs, no significance, no repeated trials.\n\n4. **Single dataset, tiny data regime, possible leakage.**\n   All results are on CircuitNet-N28 with ~100 train / 20 test designs. There’s no cross-validation, no discussion of overfitting control, and no evaluation on a different process node or dataset. This weakens generalization claims.\n\n5. **Reproducibility gaps.**\n   Key training details (optimizer, LR, batch size, epochs, early stopping, seeding) are missing. The fusion step (“Flatten by Position” putting GNN node features onto a 256×256 grid to align with ViT output) is described conceptually, but collision / aggregation rules are not specified, so it’s not fully reproducible.\n\n6. **Missing related work positioning.**\n   The paper does not seriously compare or contrast with recent multimodal / transfer / foundation-style EDA models (e.g., NetTAG, DeepGate4, FlowTuner, etc.), which weakens novelty framing.\n\n**Key questions for authors**\n\n* Show ablations: why 512/256? What happens if you change the split?\n* Give the exact gating equation and training procedure.\n* How did you prevent overfitting with only ~100 designs?\n* How exactly are GNN features mapped onto the 256×256 layout grid when multiple nodes land in the same cell?\n* Do results transfer to any dataset or node other than N28?\n* Please report variance / error bars and full hyperparameters."}, "questions": {"value": "refer weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EQbiB8qM5k", "forum": "ltnplSHUGm", "replyto": "ltnplSHUGm", "signatures": ["ICLR.cc/2026/Conference/Submission7291/Reviewer_RfUN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7291/Reviewer_RfUN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7291/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991995522, "cdate": 1761991995522, "tmdate": 1762919411750, "mdate": 1762919411750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}