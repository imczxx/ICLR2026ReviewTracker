{"id": "f2VxF4QIx1", "number": 19385, "cdate": 1758295865558, "mdate": 1759897042317, "content": {"title": "Benchmarking Overton Pluralism in LLMs", "abstract": "We introduce the first framework for measuring Overton pluralism in large language models—the extent to which diverse viewpoints are represented in model outputs. We (i) formalize Overton pluralism as a set-coverage metric (OvertonScore), (ii) conduct a large-scale U.S.-representative human study (N=300; 15 questions; 8 LLMs), and (iii) develop an automated benchmark that closely reproduces human judgments. On average, models achieve OvertonScores of 0.2 – 0.37, with OpenAI's o4-mini performing best; yet all models remain far below the theoretical maximum of 1.0, revealing substantial headroom for improvement. Because repeated large-scale human studies are costly and slow, scalable evaluation tools are essential for model development. Hence, we propose an automated benchmark that achieves high rank correlation with human judgments ($\\rho=0.88$), providing a practical proxy while not replacing human assessment. \nBy turning pluralistic alignment from a normative aim into a measurable benchmark, our work establishes a foundation for systematic progress toward more pluralistic LLMs.", "tldr": "We introduce OvertonScore and the first benchmark for measuring pluralism in LLMs, combining a large-scale human study with an automated LLM-as-a-Judge framework.", "keywords": ["Pluralism", "Overton pluralism", "pluralistic alignment", "benchmark"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61a937452721a447c92acf6ef8cc44fd736e8c09.pdf", "supplementary_material": "/attachment/e078ddd421c90b6fb4eb80a93d5eae0575612d25.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces OvertonScore, a set-coverage metric for \"Overton pluralism\", how many distinct, reasonable viewpoints a single model response simultaneously represents on a subjective question. The Overton window W(x) is induced from humans by clustering participants into viewpoint groups via agreement/disagreement judgments. A viewpoint is marked “covered” when its cluster’s mean perceived-representation score for a model response is ≥4/5. Using a U.S.-representative study (N=300; 15 questions) the authors evaluate 8 LLMs, finding adjusted OvertonScores around 0.2–0.37 with o4-mini best on average, well below the 1.0 ceiling. For scalability, they build an automated judge (best: Gemini 2.5 Pro with few-shot + user free-response), which reproduces model rankings with Spearman ρ≈0.88."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Human-grounded viewpoint: W(x) comes from participant clustering rather than model outputs\n\nInformative findings: Demonstrates substantial headroom (low scores even at the “best-across-models” upper bound) and a trade-off between neutrality and pluralistic coverage.\n\nAutomated evaluation that tracks humans: The judge achieves MAE ≈ 0.66 and ρ ≈ 0.88 on model ranking, making iterative evaluation feasible."}, "weaknesses": {"value": "For benchmark design:\n\n* Benchmark breadth: 15 questions is small for a general benchmark; topical and cultural coverage is limited.\n\n* U.S.-centric panel: Viewpoint discovery and coverage judgments are U.S.-specific; cross-cultural validity is unclear.\n\nFor evaluation details:\n\n* The 8 evaluated models may omit newer frontier systems (e.g., GPT-5, Grok-4), which could shift conclusions.\n\n* Differences across chatbot vs. reasoning vs. agentic models are not explored; these may behave differently on pluralism.\n\n* Coverage could increase with higher temperature.\n\n* Overton coverage is inherently prompt-dependent (e.g., prompts that explicitly request multiple perspectives)."}, "questions": {"value": "Did you try prompts that explicitly instruct models to “cover multiple reasonable perspectives”? How do scores change?\n\nCan you include newer frontier models to test whether conclusions hold?\n\nPlease also see the Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Seh1NnroqT", "forum": "f2VxF4QIx1", "replyto": "f2VxF4QIx1", "signatures": ["ICLR.cc/2026/Conference/Submission19385/Reviewer_6u64"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19385/Reviewer_6u64"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761385150088, "cdate": 1761385150088, "tmdate": 1762931309582, "mdate": 1762931309582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper assesses the degree to which LLMs generate responses which faithfully capture the entire range of preferences of human users on political subjects, the Overton window. The define a coverage metric for assessing model pluralism, and score several leading models in a human evaluation. They then extend the human evaluation with an automated assessment that achieves high correlation to human judgements with less overhead."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper stands out to me as a strong contribution in originality, quality, clarity and significance. The operationalisation of pluralistic alignment via the overton window, and specifically the extent to which surveyed humans agree that their view is represented, is innovative and human-centric. The methods employed are well-described and thorough. The implications for the community are valuable in highlighting the current lack of pluralism across models."}, "weaknesses": {"value": "1. The human sample is limited in size and coverage. Although the authors clearly recognize this, a sample of only 300 people is likely to underrepresent less common views, which are key to an Overton-type analysis. Similarly, the questions and participants are US-centric.\n2. There are fundamental trade-offs between Overton coverage and response length. An optimal model probably does not include every possible viewpoint on every topic, which means a perfect score on this benchmark is not always desirable."}, "questions": {"value": "1. Did the participants all complete the study at the same time? How did the early participants get responses to agree/disagree with?\n2. Why do you default to the viewpoint-weighted (\"unweighted\") score rather than the participant-weighted score? \n3. How do you propose balancing a desire to hear all views with practical constraints on how long a good response should be?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MhsYwXujX7", "forum": "f2VxF4QIx1", "replyto": "f2VxF4QIx1", "signatures": ["ICLR.cc/2026/Conference/Submission19385/Reviewer_zRFp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19385/Reviewer_zRFp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728026935, "cdate": 1761728026935, "tmdate": 1762931308843, "mdate": 1762931308843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark for measuring Overton pluralism - the extent to which LLM outputs simultaneously represent multiple reasonable perspectives. The authors formalize OVERTONSCORE as a set-coverage metric, conduct a human study with 300 US participants evaluating 8 LLMs, and develop an automated LLM-as-judge benchmark achieving ρ=0.88 correlation with human judgments.\n\nNovelty. The claim of introducing \"the first framework for measuring Overton pluralism\" is overstated. Modular Pluralism (Feng et al., 2024) already measures Overton pluralism quantitatively using NLI-based coverage and human win rates, achieving 68.5% improvements. VITAL (Shetty et al., 2025) benchmarks all three pluralism modes including Overton coverage. The genuine novelty lies in the specific methodology: participant voting-based clustering, representation ratings, and human-validated automation. This is a better, more rigorous version of something that exists, not the first of its kind.\n\nSignificance. The paper addresses an important problem but overstates its contribution. The claim that methods \"are not evaluated directly... due to a lack of benchmarks\" contradicts existing work -- Modular Pluralism and VITAL explicitly evaluate Overton pluralism. The actual gap is in standardization and human validation, not evaluation per se. Furthermore, algorithmic monoculture research (Zhang et al., 2025) shows 21 LLMs align with only 41% of human preferences due to fundamental response homogeneity. The paper's findings (0.2-0.37 coverage) confirm this systemic issue, but better benchmarks alone may be insufficient without addressing underlying alignment paradigms. Significance is further limited by: (1) US-only scope, (2) Overton being one of three pluralism types with context-dependent importance, and (3) modest scale compared to PRISM (1,500 participants, 75 countries) and Model Slant (10,007 respondents).\n\n* Shangbin Feng, Taylor Sorensen, Yuhan Liu, Jillian Fisher, Chan Young Park, Yejin Choi, and Yulia Tsvetkov. 2024. Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 4151–4171, Miami, Florida, USA. Association for Computational Linguistics.\n* Anudeex Shetty, Amin Beheshti, Mark Dras, and Usman Naseem. 2025. VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 22954–22974, Vienna, Austria. Association for Computational Linguistics.\n* Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, Tim Althoff, and Yejin Choi. 2024. Position: a roadmap to pluralistic alignment. In Proceedings of the 41st International Conference on Machine Learning (ICML'24), Vol. 235. JMLR.org, Article 1882, 46280–46302.\n* Lily Hong Zhang, Smitha Milli, Karen Jusko, Jonathan Smith, Brandon Amos, Wassim Bouaziz, Manon Revel, Jack Kussman, Yasha Sheynin, Lisa Titus, Bhaktipriya Radharapu, Jane Yu, Vidya Sarma, Kris Rose, Maximilian Nickel (2025). Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset. arXiv:2507.09650"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-  Novel clustering methodology: Using participant voting patterns to identify viewpoints is innovative and more faithful to human understanding than semantic similarity or NLI approaches.\n- Rigorous statistical framework: Proper significance testing with OLS models, fixed effects, and cluster-robust standard errors enables principled model comparison.\n- Validated automation: LLM-as-judge achieves strong correlation (ρ=0.88) with human judgments and shows small fairness disparities (η² < 0.004), providing practical utility for iterative development.\n- Confirms systemic gap: Models achieve only 0.2-0.37 coverage (even pooled: 0.62), consistent with broader algorithmic monoculture findings showing fundamental limitations in current LLMs.\n- Clear formalization: Set-coverage metric provides intuitive operationalization with both weighted and unweighted variants.\n- Actionable insights: Identifies o4-mini as significantly above average (p=0.043) and DeepSeek V3 below (p=0.017)."}, "weaknesses": {"value": "- False \"first\" claim: Modular Pluralism and VITAL already measure Overton pluralism. The contribution is methodological refinement (human validation + clustering), not pioneering measurement.\n- Contradictory \"lack of benchmarks\" statement: Page 1 claims methods aren't evaluated due to lacking benchmarks, but Modular Pluralism explicitly evaluates Overton pluralism improvements.\n- Limited scope: US-only, 15 questions, 300 participants. PRISM has 75 countries; Model Slant has 10,007 respondents; Community Alignment has 15,000 across 5 countries. Overton windows are culturally situated -- generalizability is severely limited.\n- Modest clustering quality: Silhouette score of 0.358 indicates only moderate separation, questioning whether \"distinct viewpoints\" are genuine or algorithmic artifacts.\n- Unclear practical importance: Overton pluralism is one of three types (Sorensen et al., 2024b note distributional/steerable get more attention). Context-dependent value not established.\n- May address wrong problem: Algorithmic monoculture research shows root cause is response homogeneity in alignment processes, not measurement inadequacy. Better benchmarks alone may be insufficient.\n- Question set overlap with Model Slant: o4-mini ranks most pluralistic here but second-most politically slanted there. This relationship needs investigation -- potential confounds from verbosity/style not addressed.\n- Automated benchmark limitations: Systematic over-rating of Claude 3.7 Sonnet (+0.104 in Table 1) and significant subgroup differences suggest generalization issues.\n- Binary coverage threshold: Rating ≥4 = covered is arbitrary; no sensitivity analysis provided.\n- Missing ablations: No analysis of how coverage varies by question difficulty, cluster size, or agreement levels."}, "questions": {"value": "- How does your representation-rating approach differ substantively from Modular Pluralism's human win-rate evaluation for Overton pluralism?\n- Can you provide systematic comparison between your OVERTONSCORE rankings and Model Slant rankings? Do high pluralism and perceived slant correlate? Could verbosity/hedging confound both?\n- How sensitive are rankings to clustering hyperparameters (silhouette=0.358)? Did you validate clusters qualitatively?\n- How do results change with coverage thresholds of 3.5 or 4.5 instead of 4.0?\n- Given algorithmic monoculture research showing fundamental response homogeneity limits preference learning even with diverse datasets, how does better measurement address this? Would your benchmark be more useful for evaluating data collection strategies than model selection?\n- Best-across-models achieves only 0.62 -- does this suggest ensemble approaches or that single pluralistic models are infeasible?\n- How do you distinguish valuable pluralism from harmful false balance when some viewpoints are epistemically inferior?\n- What is computational cost vs. human evaluation? How frequently should automated benchmarks be re-validated?\n- Do you expect same viewpoint clusters cross-culturally? How would you adapt methodology for non-US populations?\n- Which metric should be primary -- weighted or unweighted? How prevent gaming where models optimize for largest clusters while ignoring minorities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZPsAaUXEIO", "forum": "f2VxF4QIx1", "replyto": "f2VxF4QIx1", "signatures": ["ICLR.cc/2026/Conference/Submission19385/Reviewer_Zw8e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19385/Reviewer_Zw8e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053143332, "cdate": 1762053143332, "tmdate": 1762931308265, "mdate": 1762931308265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed methods to benchmark Overton pluralism, measuring the proportion of represented perspectives in LLM responses. They conducted a user study, which resulted in the development of an automated evaluation of overtoon pluralism that can be used for model development, as it highly correlates with human data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-presented and explained, outlining the concept of Overton pluralism and how it is measured and evaluated. \n\n- The paper is accompanied by a dataset, where users were asked to evaluate statements in free form or from selected views, as well as rate models’ responses. I believe that the experimental protocol is thoughtfully designed for the narrow setting it targets: participants provide both free-form statements and Likert ratings, as well as pairwise agreement votes, which are then used for clustering."}, "weaknesses": {"value": "- The study’s focus is very narrow and is actually dataset and task-dependent. It does not tell us much about the model’s abilities or how these scores can be generalised or used for developing models. The entire benchmark is built on 15 questions, drawn from a US-focused political dataset. \n\n- I believe that the paper’s contribution is limited for this type of conference, and can be better suited for a workshop. \n\n- The automated benchmark is presented as a tool for model selection, but the paper only demonstrates correlation with human results. It does not demonstrate a concrete development loop where this metric actually guides model improvement or selection."}, "questions": {"value": "I do not have questions for the authors, as they have clearly stated the limitations of their work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qT4tNd65Lc", "forum": "f2VxF4QIx1", "replyto": "f2VxF4QIx1", "signatures": ["ICLR.cc/2026/Conference/Submission19385/Reviewer_fPY8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19385/Reviewer_fPY8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762260468622, "cdate": 1762260468622, "tmdate": 1762931307889, "mdate": 1762931307889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}