{"id": "FNHVYCrgph", "number": 13515, "cdate": 1758218830808, "mdate": 1763097954348, "content": {"title": "AirV2X: Unified Air-Ground Vehicle-to-Everything Collaboration", "abstract": "While multi-vehicular collaborative driving demonstrates clear advantages over single-vehicle autonomy, traditional infrastructure-based V2X systems remain constrained by substantial deployment costs and the creation of \"uncovered danger zones\" in rural and suburban areas. We present AirV2X-Perception, a large-scale dataset that leverages Unmanned Aerial Vehicles (UAVs) as a flexible alternative or complement to fixed Road-Side Units (RSUs). Drones offer unique advantages over ground-based perception: complementary bird's-eye-views that reduce occlusions, dynamic positioning capabilities that enable hovering, patrolling, and escorting navigation rules, and significantly lower deployment costs compared to fixed infrastructure. Our dataset comprises 6.73 hours of drone-assisted driving scenarios across urban, suburban, and rural environments with varied weather and lighting conditions. The AirV2X-Perception dataset facilitates the development and standardized evaluation of Vehicle-to-Drone (V2D) algorithms, addressing a critical gap in the rapidly expanding field of aerial-assisted autonomous driving systems. The dataset and development kits are open-sourced at https://anonymous.4open.science/r/AirV2X-Perception-BBA7.", "tldr": "We present AirV2X-Perception, a large-scale dataset enabling drone-assisted autonomous driving as a flexible and cost-effective alternative or complementary to traditional V2X dataset.", "keywords": ["Autonomous Driving", "Collaborative Perception", "Low altitude economy"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/eb69283f361c2c18903bc179fe5484d744df44cb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores a new V2X paradigm for autonomous driving by leveraging vehicle-to-drone (V2D) communication. Whereas prior work primarily studies V2V or vehicle–RSU communication, the authors propose using UAVs as mobile, lower-cost infrastructure that can provide an aerial overview and line-of-sight advantages. To support this study, they introduce a synthetic dataset built with CARLA and AirSim that simulates coordinated vehicle–drone sensing and communication. Experiments compare V2D against conventional V2X setups and indicate that aerial fusion can improve performance (e.g., under occlusions) while potentially reducing infrastructure cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- `Novel V2X perspective:` Proposes vehicle-to-drone (V2D) collaboration as a cost-effective alternative to fixed RSUs. The aerial viewpoint offers wide FOV and line-of-sight advantages, which plausibly improves perception in occluded scenarios.\n\n- `Reproducible data pipeline:` Provides a clear CARLA+AirSim co-simulation procedure and 6.73 hours of data, enabling the community to study V2D and to recreate/extend the dataset with the described workflow.\n\n- `Empirical support:` 3D object detection experiments indicate that incorporating aerial views can improve perception capability relative to conventional V2X configurations."}, "weaknesses": {"value": "- `Necessity and task coverage:` The benefit of an aerial viewpoint is demonstrated primarily in 3D detection. It is unclear that this advantage transfers to other key perception tasks (online mapping, traffic light recognition, lane/topology extraction, HD map maintenance). Suggest broadening the evaluation to BEV segmentation and lane/centerline extraction.\n\n- `Lack of end-to-end, closed-loop evaluation:`\nThe paper does not assess whether V2D measurably improves driving quality. Without closed-loop CARLA tests (e.g., Driving Score, route completion, collision/infraction rates), the impact on autonomy is speculative. Recommend integrating aerial fusion into an end-to-end or modular planning stack and reporting improvements under occlusion-heavy routes.\n\n- `Simulation-only evidence and dataset scope:`\nThe 6.73-hour CARLA+AirSim dataset is useful but synthetic; domain gap to real UAV imagery, weather, lighting, wind-induced motion blur, and sensor noise is unquantified. No real-world validation or transfer experiments are provided.\n\n- `Operational feasibility and total cost of ownership:`\nWhile drones are cheaper per unit than RSUs, recurring costs (battery swaps, maintenance, fleet management, airspace compliance, pilot-in-command requirements) and limited endurance may reduce practical viability. A cost–benefit analysis versus fixed RSUs or hybrid deployments would strengthen the economic argument.\n\n- `Overall assessment:`\nThe V2D idea is intriguing and potentially impactful, but stronger evidence is needed: broader task coverage, closed-loop driving metrics, realistic comm/latency constraints, and at least preliminary real-world validation or rigorous sim-to-real analysis."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qnQxOY8KWL", "forum": "FNHVYCrgph", "replyto": "FNHVYCrgph", "signatures": ["ICLR.cc/2026/Conference/Submission13515/Reviewer_W6Xh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13515/Reviewer_W6Xh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761373014555, "cdate": 1761373014555, "tmdate": 1762924124849, "mdate": 1762924124849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "kjRO794X6a", "forum": "FNHVYCrgph", "replyto": "FNHVYCrgph", "signatures": ["ICLR.cc/2026/Conference/Submission13515/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13515/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763097953502, "cdate": 1763097953502, "tmdate": 1763097953502, "mdate": 1763097953502, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AirV2X-Perception, a new large-scale, simulated dataset for autonomous driving that focuses on air-ground collaborative perception. The primary contribution is the dataset itself, which, for the first time at this scale, unifies three types of connected agents: ground vehicles, fixed infrastructure (RSUs), and aerial agents (UAVs). Beyond the dataset, the authors provide a comprehensive benchmark of six representative V2X perception algorithms. This benchmark analyzes performance on 3D object detection and BEV semantic segmentation, with a deep dive into robustness against environmental conditions, drone navigation patterns, LiDAR degradation, and spatio-temporal errors."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper's primary contribution is the first large-scale dataset to systematically integrate vehicles, RSUs, and drones, which it clearly positions against prior work. The dataset features impressive scale, agent complexity, and environmental diversity.\n2. The paper also provides an extensive and valuable benchmark of perception algorithms. The analysis of drone-specific navigation dynamics (hover, patrol, escort) is a novel and valuable feature.\n3. The overall presentation is clear and well-motivated."}, "weaknesses": {"value": "Major Concerns: the sim2real gap. This manifests in two key areas:\n\n1. The specified drone LiDAR configuration (Table 2) is unconventional. A 60-degree vertical FOV pointing exclusively downwards does not correspond to common, commercially available spinning LiDARs. If this setup is purely theoretical, it diminishes the dataset's utility for developing algorithms intended for real-world hardware.\n2. While the three drone navigation modes (hover, patrol, escort) are conceptually clear (Section 3.2), the paper omits quantitative flight parameters. Crucial details such as typical/max flight altitude, speed, and the specific constraints for the 'escort' (e.g., following distance, relative altitude) and 'patrol' (e.g., waypoint generation logic) modes are not provided. This lack of detail hinders the assessment of the simulation's realism.\n3. Real-world V2X performance is heavily constrained by bandwidth, packet loss, and variable latency. The current benchmark only models a binary \"with or without temporal asynchronization\", which is not a sufficient proxy for these complex network dynamics. A more realistic benchmark would involve simulating these constraints quantitatively to truly test which algorithms are robust to data loss or high latency.\n\nMinor Concerns:\n\n1. While Table 9 presents 'Sync/Async' and 'Async/Async' results, the paper provides no details on the specific temporal asynchronization parameters (e.g., mean/max latency, distribution of delays) that were simulated, which makes the conclusions about robustness difficult to interpret."}, "questions": {"value": "1. What was the rationale for selecting the 360°H x -30°/-90°V UVA LiDAR configuration? More importantly, is this sensor configuration based on a real-world, commercially available product, or is it a purely theoretical setup?\n2. Could the authors provide the specific flight parameters used for the drones, such as the typical altitude for \"hover,\" the altitude and speed ranges for \"patrol,\" and the relative following distance/altitude for \"escort\"?\n3. Given that bandwidth constraints and packet loss are critical challenges in real-world V2X, why were these quantitative network constraints not simulated?\n4. For the \"Async\" experiments in Table 9, what specific inter-agent latency or temporal asynchronization parameters were used? How were these values chosen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YsDLah0d78", "forum": "FNHVYCrgph", "replyto": "FNHVYCrgph", "signatures": ["ICLR.cc/2026/Conference/Submission13515/Reviewer_XxuA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13515/Reviewer_XxuA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994873653, "cdate": 1761994873653, "tmdate": 1762924124397, "mdate": 1762924124397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article introduces a UAV dataset for V2X systems, innovatively utilizing bird's-eye-views to address the lack of Road-Side Units in most areas. It achieves a complete simulation dataset integrating multiple scenarios and environments, and designs three types of UAV tasks—Hover, Patrol, and Escort—to adapt to different needs. It supports tasks like 3D object detection and BEV semantic segmentation. Furthermore, it benchmarks existing cooperative perception methods, filling a data gap in aerial-assisted autonomous driving systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The dataset includes urban and rural distributions, lighting condition distributions, and weather distributions, and also contains three types of drone missions. Its comprehensive design makes it suitable for robust algorithm evaluation.\n- The paper provides a complete evaluation of this dataset for several cutting-edge algorithms, analyzes the performance of various algorithms under different conditions and the reasons behind their performance, and examines the trade-offs between performance and accuracy.\n- The paper systematically compares AIRV2X-Perception with multiple V2X datasets, demonstrating its significant advantages and potential in real-world applications."}, "weaknesses": {"value": "- **First, it is worth noting that the author included an arXiv link https://arxiv.org/abs/2506.19283 and a Hugging Face link https://huggingface.co/datasets/xiangbog/AirV2X-Perception/viewer?views%5B%5D=train in the anonymous link. This may violate the anonymity rules, and it need the chairs to decide.**\n- The paper mentions that RSUs, due to economic constraints, are only installed at high-traffic intersections and critical urban junctions, while drones, due to their low cost and high dynamic capabilities, can be deployed in various areas. However, in its performance comparison, the article only compared data from three categories: Vehicle + Infra + Drone, Vehicle + Infra, and Vehicle only. This fails to demonstrate the independent performance of the Drone in areas where RSUs are lacking.\n- As a simulated dataset, it may not fully cover all real-world scenarios, requiring real-world transfer testing to determine its generalization ability."}, "questions": {"value": "- Could the authors discuss whether some models could be transferred to other datasets (such as OPV2V or V2XSim) for a more intuitive comparison?\n- A primary motivation for the paper is that the construction cost of current RSUs is higher than that of drones, but it does not consider the operating costs of drones and the losses incurred in communication and other engineering aspects. Please demonstrate whether the total cost of drones is truly lower than that of RSUs.\n- Section 4.4 discusses three drone navigation strategies and concludes that the Patrol mode yields the best results. Is this generalizable? Should different strategies be chosen in different environments and building clusters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GaApb7ezxS", "forum": "FNHVYCrgph", "replyto": "FNHVYCrgph", "signatures": ["ICLR.cc/2026/Conference/Submission13515/Reviewer_4FbW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13515/Reviewer_4FbW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997920546, "cdate": 1761997920546, "tmdate": 1762924124042, "mdate": 1762924124042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}