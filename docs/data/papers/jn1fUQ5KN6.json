{"id": "jn1fUQ5KN6", "number": 10820, "cdate": 1758182609939, "mdate": 1759897626729, "content": {"title": "Layer-wise Knowledge Distillation from a Pretrained Network Improves Hypernetwork Convergence", "abstract": "Hypernetworks that generate weights of another network often exhibit lower test accuracy and slower convergence due to implicit weight updates. The recently proposed HyperLight framework (Magnitude Invariant Parameterisations, MIP) addresses this convergence issue by bounding the scale of the hypernetwork's input encoding using sine-cosine transforms and by introducing additive weights.\nPreliminary experiments revealed that when deeper primary networks are fully hypernetised, MIP achieves lower test accuracy compared to a canonically trained network. This paper investigates layer-wise knowledge distillation methods for hypernetwork training by bridging the hypernetised layers with a pretrained Teacher network of the same architecture. Nine layer-wise KD methods (Feature-KD) -- AB, AT, CwD, FitNets, FSP, FT, JacobianKD, RKD, and SP -- were evaluated on the shufflenetv2_0x5 architecture for the CIFAR-100 classification task. The two best-performing methods, AB-KD (Activation Boundary) and AT-KD (Attention Transfer), were further evaluated on nine additional deep networks, including ShuffleNet, ResNet, MobileNet, VGG, and Reparameterised VGG. Experiments reveal that AT and AB methods applied to MIP hypernetworks improve performance even for fully hypernetised deeper networks such as VGG19. For example, AB-KD with MIP achieved a test accuracy of 72.65%, only 1.22% lower than the canonically trained Teacher at 73.87%, compared to the MIP baseline accuracy of 11%.", "tldr": "Layerwise knowledge distillation improves hypernetwork convergence of Magnitude Invariant Parameterisations, with AT and AB methods enabling deep architectures like VGG19 to approach canonical training performance.", "keywords": ["Hypernetwork", "knowledge distillation", "hypernet convergence", "classification"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b7130c7da0863c1745560c51567e51debf6b5af7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates layer-wise knowledge distillation (KD) methods to improve hypernetwork convergence and test accuracy. Hypernetworks that generate weights for primary networks struggle with convergence and underperformance relative to canonically trained networks, especially when deeply hypernetized. The paper systematically evaluates 9 existing layer-wise KD methods (AB, AT, CwD, FitNets, FSP, FT, JacobianKD, RKD, SP) applied to the MIP (Magnitude Invariant Parameterisation) hypernetwork framework on CIFAR-100. Results show Attention Transfer (AT) and Activation Boundary (AB) KD significantly improve MIP, e.g., AB-KD achieves 72.65% accuracy on fully hypernetized VGG19 (vs. 11% baseline MIP, 73.87% canonically trained teacher), but most other methods fail or regress."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Comprehensive empirical study**: Testing 9 KD methods systematically is thorough. Table 1 comparisons are informative for practitioners interested in hypernetworks.\n\n2. **Dramatic performance improvement on pathological case**: VGG19 MIP improving from 11% → 72.65% with AB-KD is striking and demonstrates the method's value for extreme compression scenarios.\n\n3. **Reproducibility**: Detailed hyperparameters, algorithms, and dataset specifications enable reproduction. Code release promised.\n\n4. **Multiple architectures**: Testing on 10 different network types (ResNets, VGGs, RepVGGs, ShuffleNets, MobileNets) shows some generalization within CNN domain.\n\n5. **Ablations on weighting strategy**: Comparing uniform vs. dynamic λ weighting (Table 1) provides practical guidance."}, "weaknesses": {"value": "1. **Severely limited scope**:\n   - **Single dataset (CIFAR-100)**: No ImageNet, STL-10, Tiny-ImageNet, or other benchmarks. CIFAR-100 is small; hypernetwork convergence dynamics may differ drastically on larger datasets.\n   - **Niche problem**: Fully hypernetized deep networks is not a mainstream use case. MIP's original applications (Bayesian ConvNets, medical imaging) don't require full hypernetization.\n   - **One setting only**: All experiments on image classification with CNNs. No other domains (NLP, graphs, etc.).\n\n2. **Lack of mechanistic understanding**:\n   - **Why do FitNets/CwD catastrophically fail?** FitNets regression of -7.04% is alarming but unexplained. Are they incompatible with hypernetwork gradients?\n   - **Why is uniform λ better than dynamic λ?** Only hand-wavy \"tug-of-war\" explanation (line 402-404). No analysis, no ablation on Gaussian spike design.\n   - **No analysis of what AT/AB capture** that other methods don't. Just empirical results.\n\n3. **Incomplete experimental design**:\n   - **No computational cost analysis**: KD adds 9 auxiliary losses. What's the training time overhead? Memory impact? This is critical for practitioners.\n   - **Weak baseline comparison**: Only compares to MIP no-KD. What about other hypernetwork improvements (HyperFan, HyperInit)? Or simpler baselines like standard training with dropout/batch norm?\n   - **No statistical significance testing**: Is 4.63% improvement significant? Standard deviations reported but no p-values.\n   - **Incomplete ablations**: Why use 9 methods? Why not test on other KD methods (e.g., VID, RealKD, more recent approaches)?\n\n4. **Mysterious design choices**:\n   - Why [16,64,128] hidden dimensions for MIP? Why downscale to [16,16,16]? No justification or ablation.\n   - Why use validation-based model selection for CIFAR-100? Standard is full training for N epochs.\n   - Why only 40 epochs? VGG19 Figure 2 shows convergence is still occurring.\n   - Factor Transfer requires 20 epochs of pretraining—no justification.\n\n5. **Generalization concerns**:\n   - **VGG19 exceptional case**: AB-KD achieves 72.65% (1.22% below teacher). But this is one datapoint. ResNet56 results are less impressive (3.62% gap).\n   - **RepVGG paradox**: AB-KD *worse* than baseline on repvgg_a0/a2 (Table 2, -5.98%, -8.35%). Suggests method fragility. Why? No analysis.\n   - **Smaller networks**: Results are mixed. AT/AB don't uniformly help all architectures.\n\n6. **Questionable claims and framing**:\n   - Abstract claims \"first systematic study\" of layer-wise KD for hypernetwork convergence, but the problem is so specialized this claim is somewhat trivial.\n   - Paper positions this as solving a major problem, but hypernetwork convergence is not blocking progress in ML.\n   - Title emphasizes \"improves convergence\" but mostly shows test accuracy improvement; convergence is only shown in plots.\n\n7. **Missing practical guidance**:\n   - When should practitioners use this method?\n   - What's the memory/compute tradeoff?\n   - Does it work beyond CIFAR-100?\n   - How sensitive are results to hyperparameter choices (β, λ weights, etc.)?"}, "questions": {"value": "1. **Generalization**: Can you provide results on ImageNet or at least a larger dataset like STL-10 or Tiny-ImageNet? How different are conclusions?\n\n2. **RepVGG failure**: Why does AB-KD fail dramatically on RepVGG architectures (-5.98%, -8.35%)? This suggests the method is architecture-dependent. How do you reconcile this with claims of broad applicability?\n\n3. **Failure analysis**: FitNets/CwD regress 5-9%. Why? Are they incompatible with the implicit gradient flow in hypernetworks? Can you provide gradient analysis?\n\n4. **Computational cost**: What is the wall-clock training time increase from adding layer-wise KD? Memory overhead? This is critical for practitioners deciding whether to adopt the method.\n\n5. **Baseline methods**: Why not compare to other hypernetwork improvements (HyperFan, HyperInit)? Why not test other recent KD methods (VID, RealKD)?\n\n6. **Statistical rigor**: Are the improvements (e.g., 4.63% for AT on ShuffleNet) statistically significant? Can you provide confidence intervals and significance tests?\n\n7. **Dynamic λ design**: Why the specific Gaussian spike formula (Eq. 30)? Have you ablated the σ, Λ_min parameters? Why is uniform uniformly better?\n\n8. **Scope justification**: What is the real-world use case for fully hypernetized deep networks on CIFAR-100? Can you motivate beyond compression?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RHckw0bAhX", "forum": "jn1fUQ5KN6", "replyto": "jn1fUQ5KN6", "signatures": ["ICLR.cc/2026/Conference/Submission10820/Reviewer_s4Af"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10820/Reviewer_s4Af"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761189525457, "cdate": 1761189525457, "tmdate": 1762922024404, "mdate": 1762922024404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical and well-known problem of convergence instability in hypernetworks, which generate the weights for a primary network. The authors observe that even recent advanced frameworks like Magnitude Invariant Parameterisation (MIP/HyperLight) fail catastrophically when applied to deep, fully hypernetised architectures without residual connections (e.g., VGG19 accuracy drops to 11%). To solve this, the paper proposes a unified framework, LayerKD-HN, which uses a pre-trained, architecturally identical \"teacher\" network to provide layer-wise supervision to the hypernetised \"student\" network via knowledge distillation (KD). The core hypothesis is that this provides shorter, more stable gradient paths and clearer optimization targets, mitigating the gradient conflicts (\"tug-of-war\") that destabilize training.The authors conduct a large-scale empirical study, systematically evaluating nine different layer-wise KD methods across ten diverse network architectures.The key findings are that Attention Transfer (AT) and Activation Boundary (AB) distillation are exceptionally effective, with AB-KD dramatically rescuing the VGG19 model's accuracy from 11.00% to 72.65%, bringing it within 1.22% of the canonically trained teacher network."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This work's main strength lies in its novel, well-motivated idea backed by extensive and compelling empirical evidence.\nNovel and Impactful Problem-Solution Pairing: The paper creatively repurposes an existing family of techniques (layer-wise KD) to solve a different, highly relevant, and challenging problem (hypernetwork convergence). This is a significant conceptual contribution that moves beyond using KD for just compression or performance boosting, framing it as a crucial optimization tool.\nExtremely Rigorous and Broad Empirical Evaluation: The systematic comparison of nine distinct KD methods across ten different CNN architectures is a standout feature.This comprehensive study provides a valuable service to the community by identifying which distillation strategies are most suitable for this challenging training regime. This level of rigor greatly increases the credibility and generality of the findings.   \nThe performance improvements are not incremental but substantial and, in some cases, transformative. The ability to take a VGG19 model from a near-random 11% accuracy to 72.65% is a powerful demonstration of the method's efficacy. This result alone proves that the proposed framework can make previously intractable hypernetwork configurations viable."}, "weaknesses": {"value": "Despite its strengths, the paper has several weaknesses that limit its overall impact and scientific depth.\nInsufficient Contextualization and Missing Key Baselines: The paper almost exclusively benchmarks against the MIP/HyperLight framework without any KD.However, the field of hypernetwork convergence includes other important and orthogonal lines of work, most notably principled initialization methods like HyperFan and HyperInit.These methods tackle the exact same stability problem from a different angle (initialization vs. in-training regularization). A thorough comparison is necessary to understand if the proposed LayerKD-HN offers benefits beyond what can be achieved with better initialization alone, or if they are complementary. Without this comparison, the paper's claim to \"improve hypernetwork convergence\" is limited to the context of the MIP baseline, not the broader state-of-the-art.   \n\nLack of Insight into Why Certain Methods Excel: The paper is an excellent report on what works (AT and AB) but provides very little analysis on why they are so effective while others (like FitNets) fail spectacularly.The most successful methods (AT, AB) transfer more abstract, structural knowledge (spatial attention, activation boundaries) rather than exact feature values.This suggests a deeper principle—that low-variance, simplified supervisory signals are crucial for stabilizing a chaotic training process—but this insight is not explored or articulated by the authors.   \n\nUnexplored Interaction Between KD Method and Architecture: Table 2 presents a fascinating result that is not discussed: AB-KD is the best method for VGG models (without residual connections) but performs worse than the baseline on RepVGG models (with residual connections), where AT-KD excels.This strongly suggests that the choice of the optimal KD method is architecture-dependent. This is a significant finding in its own right, but the paper misses the opportunity to analyze this interaction, which could have led to deeper insights about the interplay between explicit architectural stabilizers (residual connections) and external training stabilizers (KD)."}, "questions": {"value": "The paper's related work section mentions initialization-based convergence methods like HyperFan and HyperInit. Could the authors elaborate on why these were not included as experimental baselines? A direct comparison seems crucial for positioning the contribution of this work within the state-of-the-art. Furthermore, have the authors considered if their LayerKD-HN framework is complementary to these initialization schemes (e.g., applying LayerKD-HN on a HyperFan-initialized network)?   \n\nThe empirical results clearly show that KD methods transferring abstract knowledge (AT, AB) are far superior to those matching raw feature values (FitNets).Do the authors have a more concrete hypothesis for this phenomenon? Could it be that for an unstable training dynamic like that of a hypernetwork, a \"low-variance\" or \"simplified\" KD signal is key, and that trying to match noisy, high-dimensional feature maps is counterproductive?   \n\nTable 2 shows a strong interaction effect: AB-KD is highly effective for VGG but detrimental for RepVGG, while AT-KD is consistently beneficial for both.What is the authors' interpretation of this? Could it be that the strong, binary constraints of AB-KD act as a necessary stabilizer for networks lacking residual connections, but become an overly restrictive prior for networks that already have stable gradient flow?   \n\nThe proposed method requires an additional forward pass through a teacher network during training. Could the authors quantify the computational overhead (in terms of training time and memory) compared to the MIP baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6KpwQAcJuU", "forum": "jn1fUQ5KN6", "replyto": "jn1fUQ5KN6", "signatures": ["ICLR.cc/2026/Conference/Submission10820/Reviewer_MeM6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10820/Reviewer_MeM6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814295793, "cdate": 1761814295793, "tmdate": 1762922024027, "mdate": 1762922024027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a study and evaluation of how layer-wise knowledge distillation from a fully converged neural network can improve the convergence of hypernetwork training. The study investigates nine (9) layer-wise distillation methods and their impact on the convergence of training for one hypernetwork architecture (HyperLite) trained on one dataset (CIFAR100).\n\nA full set of all (9) layer-wise knowledge distillation is evaluated to generate a ShuffleNetv2.0x5 model, and a subset of two (2) layer-wise knowledge distillation is evaluated to generate ShuffleNet, ResNet, MobileNet, VGG, and Parameterised VGG models. Results indicate that layer-wise knowledge distillation does help with convergence and of hypernetwork training and performance of the generated neural network.\n\nI very much appreciate the idea and think that hypernetworks are a great area to study. However, I have multiple concerns given the current state of this submission. My concerns are centred around the motivation of layer-wise knowledge distillation, the lack of methodological novelty, and the experimental setup. I will outline each of my concerns in the weakness section below."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- **(S1)**: I appreciate the work in the area of hypernetworks, as they have great potential to improve initialization or compression of larger neural networks\n- **(S2)**: This paper is easy to read and follow, given its systematic build-up."}, "weaknesses": {"value": "- **(W1)**: Motivation: I have difficulties understanding the utility of using layer-wise knowledge distillation using a fully converged teacher model. Do I understand correctly that the proposed method does require a fully converged model of the exact same neural network model (=architecture) trained on the exact same dataset as the teacher for distillation? Why would I do this? One would need to fully train a neural network to use the weights (or activations) as information for the training of a hypernetwork to generate the exact same neural network weights. The proposed method would make more sense when used in an out-of-distribution setup, where the teacher would have another architecture or would be trained on another dataset. In the current setup, it is difficult to see its utility.\n\n- **(W2)**: Lack of novelty: This work provides an investigation of layer-wise knowledge distillation for hypernetwork training. The idea to use distillation is already seen in hypernetworks; the hypernetwork architecture used is known, and the layer-wise distillation methods are also known. There is minor methodological novelty in this setup. Regardless of this, the findings of the investigation are interesting, but looking at the findings, I have some concerns about the experimental setup.\n\n- **(W3-1)**: Experimental setup: This work only evaluates one kind of hypernetwork (HyperLite) trained on one dataset (CIFAR100). To understand if the proposed method does generalize well, one would need to show its utility on multiple datasets and more than one kind of hypernetwork.\n\n- **(W3-2)**: Experimental setup: Since knowledge distillation is known in the context of hypernetwork training, it would be important to see how \"regular\" knowledge distillation performs in comparison to layer-wise knowledge distillation. This baseline is missing."}, "questions": {"value": "- **(Q1)**: Do you have any intuition about what would happen if you would set lambda_0 = 0, i.e., totally neglecting the \"classification loss\" of the generated neural network and only train with the distillation loss of the teacher network?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JI6Isos7xI", "forum": "jn1fUQ5KN6", "replyto": "jn1fUQ5KN6", "signatures": ["ICLR.cc/2026/Conference/Submission10820/Reviewer_gkqC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10820/Reviewer_gkqC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901543461, "cdate": 1761901543461, "tmdate": 1762922023736, "mdate": 1762922023736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates hypernetwork convergence particularly for layer-wise knowledge-distillation setting. The paper discussed the issues with recent approaches and extended them with attention-transfer and activation-boundary methods. It performed extensive experiments across nine layer-wise knowledge distillation methods across nine different neural network architectures. The experiments demonstrated the superiority of proposed extensions, particularly for fully hypernised deep networks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper investigates an important direction- both hypernetworks and knowledge distillation are becoming increasingly important in the current era of large models\n+ The paper performs extensive experiments across different NN architectures and different KD methods\n+ The experimental results supports the paper’s claims in the CIFAR 100 classification task"}, "weaknesses": {"value": "- The paper only explores convolutional networks and does not explored transformer-based models, whereas the most recent large models are mostly transformer-based. Furthermore, it is not clear if the advantages of LayerKD-HN will also hold for transformer-based models\n- The presentation of the paper could be significantly improved. At its current form of the paper, too many concepts seems floating around and it is hard to keep track of the logic flow. \n- Despite investing an important direction, the paper itself does not propose any significant novel method. It is more about combining things and see how they work in this case. \n- The paper extends the MIP method and hence depends too much on the MIP paper. However, an ICLR paper should be self-contained."}, "questions": {"value": "- How does the methods perform for transformer-based models?\n- Why only CIFAR-100 classification task is used for benchmarking."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7R6Kop4QMX", "forum": "jn1fUQ5KN6", "replyto": "jn1fUQ5KN6", "signatures": ["ICLR.cc/2026/Conference/Submission10820/Reviewer_3QYY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10820/Reviewer_3QYY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985291035, "cdate": 1761985291035, "tmdate": 1762922023409, "mdate": 1762922023409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}