{"id": "WliHWqTfAb", "number": 16519, "cdate": 1758265524592, "mdate": 1759897235693, "content": {"title": "GeoDiv: Framework for Measuring Geographical Diversity in Text-to-Image Models", "abstract": "Text-to-image (T2I) models are rapidly gaining popularity, yet their outputs often lack geographical diversity, reinforce stereotypes, and misrepresent regions. Given their broad reach, it is critical to rigorously evaluate how these models portray the world. Existing diversity metrics either rely on curated datasets or focus on surface-level visual similarity, limiting interpretability. We introduce GeoDiv, a framework leveraging large language and vision-language models to assess geographical diversity along two complementary axes: the Socio-Economic Visual Index (SEVI), capturing economic and condition-related cues, and the Visual Diversity Index (VDI), measuring variation in primary entities and backgrounds. Applied to images generated by models such as Stable Diffusion and FLUX.1-dev across $10$ entities and $16$ countries, GeoDiv reveals a consistent lack of diversity and identifies fine-grained attributes where models default to biased portrayals. Strikingly, depictions of India, Nigeria, and Colombia are disproportionately impoverished and worn, reflecting underlying socio-economic biases. These results highlight the need for greater geographical nuance in generative models. GeoDiv provides the first systematic, interpretable framework for measuring such biases, marking a step toward fairer and more inclusive generative systems.", "tldr": "We propose GeoDiv, a framework that formally defines and quantifies geographical diversity in generative models.", "keywords": ["geographical diversity"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51ee7a55dc73f0129123ce4d0281c81ce10244f5.pdf", "supplementary_material": "/attachment/dcd2a253086c4653586feb6eb8baf257c5000a32.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces GeoDiv, a framework for measuring geographical diversity in Text-to-Image (T2I) models . The authors argue that existing diversity metrics are often uninterpretable or rely on curated datasets, failing to capture nuanced geographical and socio-economic biases. GeoDiv leverages LLMs and VLMs to assess diversity along two primary axes: the Socio-Economic Visual Index (SEVI), which quantifies cues like 'Affluence' and 'Maintenance', and the Visual Diversity Index (VDI), which measures variation in entity and background appearance. Applying this framework to 160,000 images generated by four T2I models across 10 entities and 16 countries, the authors found a consistent lack of diversity and significant socio-economic biases. Notably, depictions of countries such as India, Nigeria, and Colombia were found to be disproportionately impoverished and worn compared to others, demonstrating the framework's utility in identifying specific, interpretable biases."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**1. A Novel Evaluation Framework**\n\nThe paper introduces GeoDiv, a novel and well-structured framework for the critical task of measuring geographical diversity in text-to-image models. By decomposing the complex concept of \"geo-diversity\" into two clear and complementary axes, the framework provides a highly interpretable and systematic way to diagnose model biases. This multi-dimensional approach is a significant contribution over existing monolithic or uninterpretable diversity metrics.\n\n**2. A Methodologically Rigorous and Nuanced Definition of Diversity**\n\nA standout strength of this work is its thoughtful and rigorous approach to defining socio-economic diversity. The authors adeptly navigate the paradox of whether to evaluate against real-world inequality or an idealized \"fair\" world. Their solution, which separately measure the *breadth* of representation (Diversity Score) and the *central tendency* of that representation (Mean Score), is an elegant and powerful way to identify stereotyping without demanding unrealistic portrayals.\n\n**3. Comprehensive Validation and Demonstrated Practical Application**\n\nThe paper's claims are supported by a large-scale empirical study and rigorous validation. The authors not only generated a massive dataset of 160,000 images but also conducted extensive human studies to validate that their VLM-based metrics correlate well with human judgments. Crucially, the paper includes a proof-of-concept experiment, demonstrating how the diagnostic insights from GeoDiv can be directly used to guide a simple prompt-based mitigation strategy and measurably improve diversity.\n\n**4. Rich, Granular, and Actionable Insights from the Analysis**\n\nThe paper does not just present high-level scores; it provides a rich and granular analysis of the results that yields specific, actionable insights. For example, it uncovers concrete biases like the overrepresentation of \"stone\" for Egyptian houses and \"dirt roads\" for Nigeria, and identifies an interesting trade-off in the FLUX.1 model between aesthetic polish and visual diversity. This level of detailed analysis demonstrates the diagnostic power of the GeoDiv framework and provides concrete targets for model improvement.\n\n**5. Clear Writing and High-Quality Presentation**\n\nThe paper is well-written and clearly structured, presenting a complex, multi-component framework in an intuitive and easy-to-follow manner. The arguments are logically laid out, and the claims are well-supported by the evidence presented. Furthermore, the figures and tables are of high quality, effectively visualizing the data and making the key findings—such as the country-level SEVI disparities—easy to understand."}, "weaknesses": {"value": "**1. Limited Scope of Evaluated Entities and Countries**\n\nWhile the study is large in scale, its conclusions are based on a curated set of 10 common entities and 16 countries. These entities are mostly globally common objects, and the countries, while diverse, still represent a fraction of global diversity. The framework's effectiveness and the specific biases it uncovers might not generalize to more culturally-specific entities (e.g., traditional clothing, regional architecture) or to the many countries not included in the analysis.\n\n**2. Attribute Definitions are Constrained by LLM Capabilities**\n\nThe Visual Diversity Index (VDI) relies on an ensemble of LLMs to generate the question-answer sets that define the visual attributes for each entity. This approach is fundamentally limited by the collective \"imagination,\" knowledge, and inherent biases of the LLMs used. The framework may fail to generate questions about important, culturally-nuanced visual features that are outside the LLMs' training data distribution, potentially leading to an incomplete or skewed measurement of diversity.\n\n**3. Lack of Reported Inter-Annotator Agreement for Human Validation**\n\nThe paper's validation of its core SEVI and VDI metrics relies on human studies, but it fails to report standard Inter-Annotator Agreement (IAA) metrics. While the authors mention using three annotators and resolving disagreements via majority vote or averaging, they do not quantify the level of agreement among these human annotators. This omission makes it difficult to fully assess the consistency and reliability of the human-generated ground truth data used to validate the framework's components."}, "questions": {"value": "**1.** Could the authors discuss the limitations of this VDI approach, particularly the risk that the LLM's own biases and limited knowledge may prevent it from generating a truly comprehensive and culturally aware set of visual attributes?\n\n**2.** The high failure rate of the visibility check for certain questions (e.g., over 50%) is a concern. Could the authors comment on how this aggressive filtering might affect the validity of the final VDI scores? Does removing a large portion of the images for a given question introduce a form of selection bias into the measurement?\n\n**3.** For the human studies used to validate the framework, could the authors please report the IAA scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KKJQY4tPAP", "forum": "WliHWqTfAb", "replyto": "WliHWqTfAb", "signatures": ["ICLR.cc/2026/Conference/Submission16519/Reviewer_UsfU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16519/Reviewer_UsfU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760565244509, "cdate": 1760565244509, "tmdate": 1762926608255, "mdate": 1762926608255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GeoDiv, an interpretable framework using LLMs and VLMs to measure geographical diversity in text-to-image (T2I) models, addressing limitations of existing metrics. It assesses images along two axes: the Socio-Economic Visual Index (SEVI) for affluence/maintenance, and the Visual Diversity Index (VDI) for entity/background variety. The authors validate GeoDiv against human judgments, confirming its reliability. Applying GeoDiv reveals significant geographical biases in T2I models, such as stereotypical portrayals of certain countries."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper addresses the important and novel problem of quantifying geographical bias in T2I models by decoupling diversity into interpretable socio-economic and visual axes.\n2. The proposed GeoDiv framework is rigorously validated against human judgments, building trust in its reliability.\n3. The framework produces strong empirical results that highlight specific biases and demonstrate its advantages over existing diversity metrics.\n4. The paper is clearly written, well-organized, and effectively presented with helpful figures."}, "weaknesses": {"value": "The paper's main weakness lies in its narrative structure, which could more clearly delineate the problem from the proposed solution. While the paper successfully demonstrates (1) that T2I models exhibit geographical bias and (2) that the GeoDiv framework is a valid VLM-based method to measure this, the presentation intertwines these two major points.\n\nA potentially stronger narrative might be:\n1. First, establish the core problem: Demonstrate unequivocally that significant geographical bias exists in T2I model outputs. This could potentially leverage some of the human-annotated scores upfront to ground the problem in human perception, making the need for an automated metric clear.\n2. Then, introduce GeoDiv as the solution: Present the framework as the proposed method to systematically and interpretably measure the established bias. The strong correlation between VLM and human scores (Sec 4.2) then serves as the crucial validation for this specific methodological contribution.\n\nThis revised structure would create a clearer separation between motivating the problem and validating the proposed tool, potentially strengthening the paper's overall argument by first concluding \"T2I image generation is biased (as perceived by humans)\" before concluding \"and our VLM-based GeoDiv is a reliable way to measure this.\" The current organization mixes the validation of the tool (correlation) with the findings from the tool (bias results) without this clear sequential flow."}, "questions": {"value": "I have a few questions, primarily related to the calculation and presentation of the Visual Diversity Index (VDI), which seem connected to the paper's organizational structure:\n- Placement of VDI Calculation: Section 3.1 introduces VDI and mentions using VQA, but the actual calculation method (Normalized Hill Number, Eq. 1) appears later, after Section 3.2. Could the authors confirm this placement is intentional and perhaps clarify the VDI pipeline within Section 3.1 itself for better flow?\n- Role of Q&A Pairs: If the final VDI score is based on the diversity of the VQA model's output distribution (measured by the Hill Number), what is the specific role of generating detailed Question-Answer pairs beforehand? How do these pre-defined pairs contribute to the final diversity score if the score reflects the diversity of the VQA outputs themselves?\n- VDI Metric in Table 1: Table 1 lists \"Accuracy\" under the VDI column. Given VDI is ultimately a 0-1 diversity score, could the authors clarify what this \"Accuracy\" refers to? Is it measuring the accuracy of the VQA model's intermediate categorical answers against human annotations (as part of the framework validation), rather than the final VDI score itself? Clarifying this distinction would be helpful.\n\n\nMoreover, one high-level question. Due to GDPR, the open-sourced generative models might not be trained on EU countries' images. Do you think this can be a potential reason why \"European countries (Italy, Spain, UK) exhibit more uniform contexts\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c1Wl1Z9bSX", "forum": "WliHWqTfAb", "replyto": "WliHWqTfAb", "signatures": ["ICLR.cc/2026/Conference/Submission16519/Reviewer_oxKU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16519/Reviewer_oxKU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761692557058, "cdate": 1761692557058, "tmdate": 1762926607718, "mdate": 1762926607718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates biases that arise from geographic diversity in text-to-image generative models. The authors propose GeoDiv, a framework designed to assess geographic representation through two complementary metrics: the Socio-Economic Visual Index (SEVI) and the Visual Diversity Index (VDI). Experiments are conducted on multiple diffusion models, including Stable Diffusion (v2.1, v3, v3.5) and FLUX.1-dev."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important issue of geographical diversity and socio-economic bias in text-to-image models, advancing beyond traditional demographic or visual diversity metrics toward a more region-aware fairness perspective.\n- The study builds a large-scale benchmark comprising 160,000 generated images across 16 countries, 10 object categories, and 4 text-to-image models, enabling a comprehensive analysis of model-, country-, and attribute-level biases, further validated through strong correlations with human ratings (ρ≈0.7–0.8 for SEVI)."}, "weaknesses": {"value": "- Novelty: \n   - The framework’s core methodology lacks novelty. The idea heavily relies on prompt-based probing and LLM/VLM-assisted scoring, which have been explored in prior works such as [1] and [2], making the contribution more of an application to a new bias dimension rather than a methodological advance.\n   - The proposed indices (SEVI and VDI) add interpretability but do not introduce fundamentally new techniques beyond existing entropy- or attribute-based bias quantification methods. Probing variations using LLM and VQA have been extensively explored in works like [1] and [2].\n\n- The framework’s reliance on closed-source evaluators such as Gemini-2.5 and Qwen2.5-VL introduces potential evaluator bias and limits reproducibility, while its high computational and monetary cost (~$68 per entity–country pair) further constrains accessibility and scalability for large-scale or continuous audits.\n\n\n[1] TIBET: Identifying and Evaluating Biases in Text-to-Image Generative Models\n\n[2] Generated Bias: Auditing Internal Bias Dynamics of Text-to-Image Generative Models"}, "questions": {"value": "- L160: \" Manually defining a comprehensive set of attributes for each entity is infeasible, so we leverage multiple LLMs to generate candidate question-answer sets, and consolidate them into a unified list using a neutral LLM.\" What impact does using multiple LLMs have compared to a single model, and what is the associated computational overhead?\n\n- L216: \"Consider a question qk (related to either SEVI or VDI attributes), having a set of possible answers denoted by Ak.\"How is the answer set Ak defined and quantified for each question? Since the full space of possible answers is likely intractable, are you instead working with a sampled or limited subset of Ak? If so, how is the quality or representativeness of this subset ensured?\n\n- L27: \"where Pk is the answer distribution for qk, and H(·) denotes Shannon entropy.\" How exactly is Pk defined in this context? Does it represent a probability distribution over all possible answers or over token-level probabilities produced by the model? If it is over answers, how is this distribution estimated or normalized, given the discrete and potentially open-ended nature of responses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZuF9b45BSR", "forum": "WliHWqTfAb", "replyto": "WliHWqTfAb", "signatures": ["ICLR.cc/2026/Conference/Submission16519/Reviewer_WAhk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16519/Reviewer_WAhk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045002626, "cdate": 1762045002626, "tmdate": 1762926607263, "mdate": 1762926607263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents GeoDiv to measure geographical diversity in text-to-image (T2I). It is driven by LLM/VLM and focusses on (a) SEVI (Socio-Economic Visual Index), and (b) VDI (Visual Diversity Index). The paper reports (1) systematic geographic/socioeconomic biases (e.g., India, Nigeria, Colombia show as poor), (2) reduced diversity in entities and backgrounds for newer models. These conclusions are arrived from 160k images from four diffusion models across 16 countries and 10 entities.\n\nOverall, while the paper seeks to address an important problem, concerns with what exactly is being measured, and lack of statistical rigor significantly reduces the reliability of the paper's findings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Auditing geo-diversity of generative models is important, but has not received sufficient attention, apart from a few research publications and media articles. \n\n- Analysis was performed on a large-scale set of generated images. The dataset involving 160k synthetic images across 16 countries × 10 entities, prompts, QA sets, and annotations could be valuable to the community.\n\n- Human validation with reasonable Spearman correlation (about 0.7) for the evaluation."}, "weaknesses": {"value": "- The paper mentions bias and stereotypes interchangeably. However, these two concepts are different from each other as noted by OASIS. And, in the context of generative models, we are largely interested in stereotypes. Furthermore, measuring stereotypes depends on the baseline diversity of concepts in the real world, and deviations from this baseline. However, the current paper does not take this baseline into account, so it is unclear what exactly is being measured, and if the conclusions are meaningful.\n\n- There is heavy use of LLM/VLM to both define and measure attributes. The model's predictions themselves are potentially biased. These biases are not evaluated or accounted for. \n\n- There is heavy use of LLM/VLM to both define and measure attributes. The paper provides point estimates of quantified metrics, and it not clear how robust or reliable these are to slight variations of the proposed approach. For example, different models (partially addressed), different prompts, number of images per attribute used for analysis, etc.\n\n- While the proposed SEVI and VDI are interpretable metrics, they lack statistical grounding."}, "questions": {"value": "- There have been studies into geographical stereotypes in generative models with similar conclusions (e.g., see [a] below). Could the authors clarify the contribution of the paper beyond this?\n\n[a] https://restofworld.org/2023/ai-image-stereotypes"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XxVJvVHu89", "forum": "WliHWqTfAb", "replyto": "WliHWqTfAb", "signatures": ["ICLR.cc/2026/Conference/Submission16519/Reviewer_oi9r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16519/Reviewer_oi9r"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762278688541, "cdate": 1762278688541, "tmdate": 1762926606819, "mdate": 1762926606819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}