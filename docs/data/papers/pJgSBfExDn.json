{"id": "pJgSBfExDn", "number": 22049, "cdate": 1758325318831, "mdate": 1759896888839, "content": {"title": "Interpretable Transformer Regression for Functional and Longitudinal Covariates", "abstract": "We consider scalar-on-function prediction from functional covariates that may be measured sparsely and irregularly over time with noise, which is common in longitudinal studies. We propose a dual‑attention Transformer that operates on a discretized time grid with missing‑value masks and trains end‑to‑end without any imputation. The model couples time‑point attention, which encodes local and long‑range temporal structure, with inter‑sample attention, which shares information across similar subjects. We derive prediction error bounds and prove consistency under a random‑effects framework that accommodates sparse/irregular sampling, measurement error, and label noise. In simulations across varying sparsity levels, our method outperforms 19 strong baselines (ensemble, statistical/functional, deep learning methods, tabular Transformers, and pre-trained models such as TabPFN) in regimes with $\\leq 50$\\% observations and remains competitive in denser settings, highlighting the importance of end‑to‑end missingness‑aware modeling. The learned attention weights are interpretable, revealing predictive time windows and cluster structure. In real‑world data, our approach achieves the best prediction and classification performance, surpassing leading imputation methods paired with competitive learners. This underscores that explicitly modeling sparsity is preferable. In summary, the dual‑attention mechanism is interpretable, consistently identifying predictive time windows and cohort clusters that align with domain knowledge. The proposed Transformer also outperforms state‑of‑the‑art methods while preserving robustness and interpretability.", "tldr": "", "keywords": ["Dual-attention Transformer", "interpretable attention", "functional data", "longitudinal data", "irregular sampling plan"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/90748e82731f8ba0a1cca4dd6686e6d175f480d3.pdf", "supplementary_material": "/attachment/6ad29c520e000ac0d2cb3489261e154650e75b31.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes the Interpretable Dual-Attention Transformer (IDAT), which is a model for predicting scalar outcomes from sparse, irregular, and noisy longitudinal data. Specifically, IDAT integrates time-point attention to capture temporal structure and inter-sample attention to share information across similar subjects. The methods is trained end-to-end without imputation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Sound approach:** IDAT unifies temporal (time-point) and cross-sample (inter-sample) attention mechanisms. This design allows the model to capture both individual trajectory dynamics and shared structures across subjects, and it seems to work well in practice.\n\n- **Mesh size insight:** Personally, I found th mesh size tradeoff provides helpful insights how to balance computational efficiency with prediction bias."}, "weaknesses": {"value": "- The paper is very **straight forward**. The paper basically uses a grid discretization for functional data input.\n\n- **Dual attention** = known parts, new combo: Time-point self-attention and cross-sample (nearest-neighbor-like) pooling are both established ideas. IDAT simply combines them in one encoder and trains them  end-to-end with masks. \n\n- **Important parts of the paper (e.g., ALL results on simulated data in Sec. 4.1, all Lemmas) are relegated to the appendix.** I understand that not all results can always make it to the main body of the paper. However, here it seems the authors have moved too much important parts to the supplements.\n\n- **Performance is very regime-specific:** The strongest gains are reported when only < 50% of time points are observed, which is fair since this is the regime the method is designed for. However, as sparsity decreases, it seems like the advantage will narrow outside of the very sparse regime\n\n- **Standard consistency and Lipschitz arguments:** The main results are architectural Lipschitz/approximation lemmas and two theorems: encoder consistency and a training-MSE generalization bound under standard assumptions (boundedness, stable SGD, $p/n \\to 0$ ). \nThese are mathematically sound but completely expected for modern attention+MLP networks. My issue here is that these insights do not provide information /  do not translate into design guidance (e.g., how to pick layers, heads, grid size beyond asymptotics)\n\n- **Variance reduction claim:** Inter-sample attention reduces embedding noise by $O(B^{−1/2})$ when subjects are similar. However, this is a mini-batch averaging effect. The result does not quantify when the bias from pooling outweighs variance gains (of note, the paper concedes this trade-off empirically).\n\n- **Presentation:** the paper would benefit a lot from improving presentation and writing. As mentioned earlier, many components that appear important are moved to the supplements for no obvious reason."}, "questions": {"value": "- How does IDAT fundamentally differ from existing attention architectures that combine temporal and relational modeling (such as TabNet, Set Transformers, or hierarchical temporal attention models)? What isthe novelty beyond combining two known mechanisms?\n\n- Why is inter-sample attention considered novel?\n\n- The paper provides consistency and Lipschitz-type generalization proofs, but these are standard for attention networks. Specifically, what new theoretical insight do these results provide about dual-attention?\n\n- The variance reduction lemma assumes similarity among samples. Can the authors provide any conditions or diagnostics for when inter-sample attention introduces bias instead, for example in in heterogeneous populations?\n\n- Regarding the variance reduction claims, how does the train-test mismatch around the Y-token affect consistency in practice?\n\n- The interpretability claim rests on “weights acting as regression coefficients.” Is it possible to somehow interpret these weights quantitatively or only qualitatively as saliency maps?\n\n- Sinusoidal positional encodings do not extrapolate well. How does IDAT handle unseen time grids? Would it be posssible to incorporate relative or learned encodings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xWnae55MeO", "forum": "pJgSBfExDn", "replyto": "pJgSBfExDn", "signatures": ["ICLR.cc/2026/Conference/Submission22049/Reviewer_63go"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22049/Reviewer_63go"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760534636683, "cdate": 1760534636683, "tmdate": 1762942035390, "mdate": 1762942035390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method, the Interpretable Dual-Attention Transformer (IDAT), for scalar-on-function regression where the functional covariate is observed sparsely and irregularly over time. IDAT discretizes the time axis into a grid and uses explicit missing-value masks, avoiding imputation and enabling end-to-end training. It introduces two attention mechanisms: (1) Time-Point Attention, which encodes both local and long-range temporal structure within a subject's trajectory, and (2) Inter-Sample Attention, which borrows information across similar subjects in a batch. The learned attention weights are interpretable and reveal predictive time windows and cohort clusters. The paper also provides theoretical analysis of prediction error bounds and consistency. Experiments on both simulated and real-world datasets are conducted to evaluate the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation for addressing scalar-on-function regression with sparse, irregular, and noisy longitudinal data is clearly described.\n2. A new solution that combines time-point and inter-sample attention in a Transformer framework is proposed, directly targeting the challenges of missingness and heterogeneity.\n3. Theoretical analysis for the error bound and consistency of the training and testing phases MSE is provided.\n4. Experiments comparing the proposed method with more than 19 baselines on both simulated and real-world datasets are conducted to evaluate its effectiveness."}, "weaknesses": {"value": "1. The experiments lack an ablation study to illustrate the importance of each module in the model, such as time-point attention and inter-sample attention, respectively.\n2. The hyperparameters used in the experiments should be described clearly. In addition, a hyperparameter sensitivity analysis should be provided for important hyperparameters, such as batch size and grid size.\n3. The method for simulating different levels of sparsity should be described clearly. It is recommended to also simulate different missing patterns, if not already done."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V2FRQORkIP", "forum": "pJgSBfExDn", "replyto": "pJgSBfExDn", "signatures": ["ICLR.cc/2026/Conference/Submission22049/Reviewer_ZLuK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22049/Reviewer_ZLuK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896697559, "cdate": 1761896697559, "tmdate": 1762942034910, "mdate": 1762942034910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes IDAT (Interpretable Dual-Attention Transformer), a method for a scalar-on-function regression over time. IDAT uses a dual attention module (time-point attention and inter-sample attention), and, thus, it can handle sparsely and irregularly measured longitudinal data. Furthermore, the learned attention weights are interpretable. The authors also provided the theoretical guarantees for the consistency of  IDAT. Finally, the paper compares the new method against multiple other baselines and demonstrates its effectiveness for the scalar-on-function regression."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a comprehensive theory on the consistency of the proposed method. Also, the experimental evaluation is very extensive (it includes 19 baselines and multiple datasets)."}, "weaknesses": {"value": "I have outlined a couple of main concerns for the paper:\n- **Lack of assumptions for missingness**. I lacked the assumptions on the missigness (or measurement intensity), namely, whether the observation times are informative of the outcome. If yes, shouldn’t the missingness mask be included as an input itself? \n- **Limited contribution**. I my opinion, the main method of the paper is a simple combination of multiple standard Transformer-based approaches. Yes, technically, IDAT might be new in this specific setting of scalar-on-function regression. Yet, overall, I cannot pinpoint a single non-trivial or unique approach that was used in this setting. \n\nI am curious to hear the authors’ response, and I am open to further discussion.  \n\nOther minor concerns include the following:\n- I found the abstract hard to read, as it is overloaded with very specific, tiny details and misses the broader picture.\n- Same for the theoretical part, the implications for the theoretical statements were missing."}, "questions": {"value": "- What is the main motivation to use the inter-sample attention (especially given that the data is i.i.d.)? I understand that they might help to reveal cluster patterns in data, but don’t they increase the variance of the scalar-on-function regression?\n- Line 23. “The learned attention weights are interpretable...” Isn’t it true for all the Transformers?\n- I wonder whether the stream of literature on marked point processes is relevant or could be adapted to this work (e.g., [1])?\n\nReferences:\n- [1] Panos, Aristeidis. \"Decomposable transformer point processes.\" Advances in Neural Information Processing Systems 37 (2024): 88932-88955."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "P829ZhxQuW", "forum": "pJgSBfExDn", "replyto": "pJgSBfExDn", "signatures": ["ICLR.cc/2026/Conference/Submission22049/Reviewer_jqgT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22049/Reviewer_jqgT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762227959833, "cdate": 1762227959833, "tmdate": 1762942034023, "mdate": 1762942034023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}