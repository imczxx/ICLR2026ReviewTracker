{"id": "uDQ4pvbkxc", "number": 16725, "cdate": 1758268105951, "mdate": 1759897222534, "content": {"title": "VAM: Value-Attention Merging for KV Cache Optimization in LLMs", "abstract": "Efficient key-value (KV) cache management is essential for large language models (LLMs) performing long-text inference. Traditional methods, which retain all original KV pairs, lead to high memory usage and degraded performance due to outdated contextual representations. While existing solutions predominantly focus on cache eviction or compression to reduce memory and computation, they largely neglect the issue of semantic degradation in the cache itself. In this paper, we identify two critical limitations in long-context inference—Progressive Clustering and Context Degradation—which cause the model to lose global contextual awareness over time. To address these issues, we propose VAM, a plug-and-play KV cache optimization algorithm that dynamically merges attention outputs into value states. Unlike cache compression methods that aim to reduce cache size, VAM specifically targets the preservation of contextual semantics in the cached representations, thereby improving the model’s ability to retain and utilize long-range dependencies. VAM is lightweight, easy to integrate, and complementary to existing compression strategies. Experiments on LongBench tasks across LLaMA and Mistral models (7B–70B) show consistent improvements of 0.36–6.45 in absolute score (0.64\\%–4.26\\% relative), and up to 8.33\\% when combined with state-of-the-art KV compression methods, demonstrating VAM's effectiveness in enhancing long-sequence inference quality. Our code is available at https://anonymous.4open.science/r/vam-torch-386B/.", "tldr": "VAM preserves context semantics in KV cache for better long-text inference, complementing compression methods and boosting LLM performance.", "keywords": ["Large Language Model", "KV Cache", "Self Attention"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03586a75f2d0f12f19ac0571d19c5133f78b00e1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a method that optimizes the KV Cache's representation (specifically, its preservation of contextual semantics), which enhances the generation quality of the original model after kv cache compression."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow.\n- The visualization is clear."}, "weaknesses": {"value": "- The paper lacks sufficient theoretical justification and robust experimental validation.\n- The observed performance gains are marginal and appear unstable across different settings.\n- The visualization analysis is based on an overly old model (Llama2-7B). Directly applying the conclusions drawn from this limited analysis to other models severely compromises confidence.\n- The core idea—directly applying reweighting to the Value Cache before storage—is overly simplistic and lacks a formal theoretical analysis. Furthermore, similar methodologies have been previously proposed in existing work, notably CAM[1].\n- The experiments on the LongBench benchmark are missing results for the complete set of datasets (a full 16 datasets are typically expected to against different base methods).\n- The proposed \"Progressive Clustering\" observation appears to be an inherent property of model generation, analogous to the principle that a more detailed or better-crafted prompt leads to more deterministic output.\n\nReferences:\n\nCaM: Cache Merging for Memory-efficient LLMs Inference. ICML 2024"}, "questions": {"value": "- The proposed method is similar to existing work, CAM. Detailed explanation of the differences and compare is required.\n- Please supplement the manuscript with a complete set of results for all 16 datasets in the LongBench."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hBgktHEPwH", "forum": "uDQ4pvbkxc", "replyto": "uDQ4pvbkxc", "signatures": ["ICLR.cc/2026/Conference/Submission16725/Reviewer_1RsN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16725/Reviewer_1RsN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760926270651, "cdate": 1760926270651, "tmdate": 1762926777459, "mdate": 1762926777459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method to integrate contextual information into the stored value states at inference time. Experiments on a variety of tasks show that this improves performance slightly. The simplicity of this method makes it easy to test and deploy, though I think the benefit is a bit incremental. I think it may be worthwhile to explore more deeply as to why the value states seem fairly robust to this perturbation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple method which does not need any training.\n2. Consistent improvement across models and tasks.\n3. Visualizations of current limitations are very helpful."}, "weaknesses": {"value": "1. While consistent, the improvement that VAM adds on top of LLMs is fairly marginal. There seems to be a slightly larger benefit when used in conjunction with sparse attention methods, which I think may warrant further investigation.\n2. How does VAM perform on long generation tasks like CoT math?\n3. Table 2: I think it would be beneficial to include a row for the full KV cache for comparison.\n4. See questions"}, "questions": {"value": "1. How does this work with multiquery/group query attention? Wouldn't a separate value state need to be stored for each query head, negating half of the memory benefit of these methods?\n2. Have you tried fine tuning LLMs with VAM? I wonder if this can help with the training process. \n3. How does VAM affect the output token distribution? I'm wondering if the logits become more/less concentrated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JHokVKz9Q1", "forum": "uDQ4pvbkxc", "replyto": "uDQ4pvbkxc", "signatures": ["ICLR.cc/2026/Conference/Submission16725/Reviewer_PNdu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16725/Reviewer_PNdu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761424417270, "cdate": 1761424417270, "tmdate": 1762926777148, "mdate": 1762926777148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VAM (Value-Attention Merging), a plug-and-play algorithm for optimizing the KV cache in LLMs during long-text inference. The authors identify a limitation of standard KV caching: Context Degradation (token representations become increasingly localized) and Context Degradation (as the sequence lenth grows, the model increasingly prioritizes recent tokens). To mitigate these issues, VAM dynamically updates the value cache by merging the original cache vector with the attention output of the first layer, offering more contextual information into the cache.\nExperiment on the LongBench benchmark, showing consistent performance improvements  on LLaMA and Mistral models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a critical and problem in long-context LLM inference (missing contextual information under long-context settings).\n- The method is training free and easy to implement\n- VAM is a plug-and-play method, can be combined with other KV cache acceleration methods to further improve their performance."}, "weaknesses": {"value": "- The experiments are unconvincing: the models used are outdated, with most experiments on older models like LLaMA-2-7B and Mistral-7B, whose context windows are much smaller than models used in other KV cache papers (e.g., LLaMA-3.1-8B-1024K, Phi-3-Mini-128K...). Secondly, the RULER benchmark should be tested, since it is a more accepted dataset for long-context scenarios with context lengths up to 256k.\n- Motivation: The author use t-SNE visualization on the LLaMA2-7B model to demonstrate the Context Degradation phenomenon, which is not convincing. The authors should verify that this phenomenon is widespread on a variety of different model architecture, especially those with a large context window.\n- Overhead Analysis: The absence of concrete latency or memory overhead measurements is a notable omission. Claims of \"negligible overhead\" must be substantiated with statistics (e.g., tps, latency)."}, "questions": {"value": "- There is a typo in the caption of Figure 4, the method is VAM, not VAMP?\n- For Table2, instead of only 5 tasks, why not report results on other tasks of LongBench?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iPRY2xz9OW", "forum": "uDQ4pvbkxc", "replyto": "uDQ4pvbkxc", "signatures": ["ICLR.cc/2026/Conference/Submission16725/Reviewer_jVFV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16725/Reviewer_jVFV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904231559, "cdate": 1761904231559, "tmdate": 1762926776731, "mdate": 1762926776731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is interesting in the way it tries to make KV cache not just smaller but smarter, the motivation makes sense, but the design is still very heuristic. \n\nThe proposed method wants to make value representations more contextual instead of static. \nThe idea to merge the current attention output into each value vector is simple and appealing, but it also assumes that the two vectors live in the same neural space, which is not strictly true, so the addition is more of a practical shortcut than a well grounded mathematical operation.\n\nThis work shows good results on LongBench but doesn’t test on reasoning heavy or extreme length sets like RULER or InfiniteBench, so the claim of general long context benefit feels limited. also, all tests are on llama and mistral, which share the same cache interface. it doesn’t include Qwen or DeepSeek, whose fused attention kernels would make this merging step non-trivial.\n\nThe merge behaves like a residual update or an exponential moving average, letting each cached value carry some global semantic bias. it improves coherence within single long sequences, but if applied blindly across different modes of reasoning or tool use, it could blur boundaries between functions or roles. In an agentic setting with function calls or json interactions, this could easily mix unrelated context and weaken state tracking.\n\nThe method is plug-and-play and can combine with any compression method, however,  the weak part is the lack of deeper explanation, limited coverage, and a strong reliance on linear addition that may not hold under architectural diversity. \n\nOverall, it’s a empirical approach, light and effective for classic long-context tasks, but it probably needs a more structured or gated variant to stay useful in agentic LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is simple, plug-and-play, and doesn’t need retraining. \n2. It improves cache quality with almost no cost and works well with other KV compression approaches. \n3. Results on LongBench are consistent, and visual evidence supports its effect."}, "weaknesses": {"value": "1. It’s limited in scope and theory. the method is only tested on llama and mistral\n2. The addition of attention output to values assumes both lie in the same neural space, which isn’t strictly true. \n3. The fixed α makes it inflexible, and over merging can blur semantics. \n4. In agentic or multi-modal scenarios, it could mix unrelated context and break function separation."}, "questions": {"value": "1. How exactly does the linear merge avoid subspace misalignment between value and attention outputs? \n2. Would a projection or learned adapter make it more stable across architectures? \n3. Why only test on llama and mistral—would models with fused attention kernels behave differently? \n4. Does the merge still help when the context exceeds LongBench’s length?\n5. How would the method perform in agentic settings?\n6. Is the semantic benefit still consistent when tokens switch roles, like between reasoning and tool calling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c7MCKMSM5G", "forum": "uDQ4pvbkxc", "replyto": "uDQ4pvbkxc", "signatures": ["ICLR.cc/2026/Conference/Submission16725/Reviewer_8frj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16725/Reviewer_8frj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001479469, "cdate": 1762001479469, "tmdate": 1762926776373, "mdate": 1762926776373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}