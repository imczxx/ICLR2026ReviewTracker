{"id": "HZRz9R1Xjo", "number": 20628, "cdate": 1758308350578, "mdate": 1759896967128, "content": {"title": "Do Depth-Grown Models Overcome The Curse Of Depth? An In-Depth Analysis", "abstract": "Gradually growing the depth of Transformers during training cannot only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half-also known as the Curse of Depth (Sun et al., 2025, Csordás et al., 2025). Using depth-wise analyses, we show that growth via gradual middle stacking yields more effective utilization of model depth, changes in the residual stream structure, and formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how gradual growth of model depth can lead to formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.", "tldr": "We analyse how growing Transformers gradually in depth improves reasoning by boosting depth utilization and inducing modular computational circuits.", "keywords": ["stacking", "language model", "reasoning", "efficient training", "depth analysis"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88be5a3dd32f169d51e19f2f8fe72dc9df5028e4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper performs an analysis of MIDAS a gradual stacking technique to grow transformers whilst training them. The paper reproduces the MIDAS experiments on SmolLM at 360M and 1.7B scales.\nThe paper introduces LIDAS, slightly different to MIDAS, as when intruding a new 4 layer block B in-between B’ and B’’, takes the last two layers of B’ and first two layers of B’’. Where as MIDAS would just set B=B’. I view LIDAS less as a flashy novel method and more of a sanity check for the authors analysis experiments, although the authors find LIDAS is competitive with MIDAS.\n\nThe paper performs an in depth analysis of MIDAS and LIDAS:\n1. Depth grown transformers utilise their depth more efficiently \n2. Depth grown models are more robust to block level reordering but less robust to layer level reordering or removal.\n3. Block grown models exhibit cyclic patterns within the layers\n4. MIDAS and LIDAS weights are more symmetric."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper independently reproduces prior results, something often overlooked in ML research\n- The papers presentation is very clear.\n- The analysis presented is deep and through."}, "weaknesses": {"value": "- The analysis is limited to one model, SmolLM-1.7b. Although this is an expensive analysis to perform.\n- The benchmarks used are a little limited, perhaps the FineWeb-Edu benchmarks could be useful here.\n- Initialisation and learning rate scheme not described for baseline, this is known to impact a models ability to use deeper layers effectively (https://arxiv.org/pdf/2505.01618).\n\nMinor: The figures are slightly out of order with the text, e.g. Figure 1 is a long way away from where it is described. Reorganising slightly would make the presentation perfect."}, "questions": {"value": "1. What learning rate and initialisation scaling methods were used during training? For example, Dey et al. (https://arxiv.org/pdf/2505.01618) show better learning rate scaling can lead to more effective training.\n2. Figure 1 shows that depth grown models use their later layers, but this needed? For example it now looks like earlier layers are used less, does growing move the problem or solve the problem?\n3. The paper discusses weights being more symmetric when using M/LIDAS, do we necessarily want symmetric weights in our trained language models?\n4. Is there a link between M/LIDAS vs regular training and how pruning of weights post training can be conducted? For example, regular training may allow practitioners to prune more efficiently?\n\nIn the rebuttal I would be most interested in hearing about the initialisation and learning rate scaling for the baseline, then answers to the above questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IKTd8Dh5bd", "forum": "HZRz9R1Xjo", "replyto": "HZRz9R1Xjo", "signatures": ["ICLR.cc/2026/Conference/Submission20628/Reviewer_UGkv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20628/Reviewer_UGkv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760549464125, "cdate": 1760549464125, "tmdate": 1762934030307, "mdate": 1762934030307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Transformers are known not to fully exploit the deeper layers, known as the Curse of Depth. Recent depth growing techniques adaptively increase the depth of a model and can obtain more gain from the depth. This study investigates the Curse of Depth through detailed analysis comparing non-growing and growing models. The detailed analysis reveals three main observations. (i) The removal of deeper layers has significantly greater impact for depth-grown models than non-grown ones, indicating the exploitation of deeper layers in depth-grown models. (ii) The depth-grown models are more robust against computational block permutations. (iii) Depth-grown models present cyclic patterns in the attention sublayer's contribution over layers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Extensive experiments are conducted on the impact of deeper layers, comparing standard depth-fixed models and recent depth-growing models. \n- Besides, LIDAS, a variant of MIDAS method, is proposed, which attains superior performance in reasoning-intensive tasks.\n- The presentation is easy to follow; the hypothesis, evidence, results, and interpretation are presented clearly."}, "weaknesses": {"value": "This study experimentally collects observations on depth-non-growing and depth-growing models. While I appreciate them, one of the major weaknesses of this work is that the connection between these observations is unclear, and the practical takeaway from them is limited. \n\nDepth-fixed models do not fully take advantage of the depth, and deeper layers can be dropped with a subtle cost in performance. This has been known already, and the experiments collect related observations from layer-wise analysis with a contrast to depth-growing models. The introduction [l.053] writes \n\n> However, a clear mechanistic understanding of these gains has so far been missing. \n\nbut I don't feel this paper fully addresses these points. The experiments strengthen the known fact but do not offer how to boost the depth-growing models (if some understanding is obtained, this should be done to some extent, which validates the understanding). \n\nThe proposed method, LIDAS, is also kind of independent; it's not designed based on the observations. While LIDAS performs better than MIDAS in some tasks, the reason is not explained clearly or theoretically. \n\nI'm afraid to say that this work is preliminary or a \"concatenation\" of several results without any conclusive remarks. Each of the experiments reports a solid observation, and I appreciate it. The main concern is the final takeaway (and its impact) built upon them in training Transformer models,"}, "questions": {"value": "Please address the weaknesses. Particularly, what new understanding of depth-fixed and depth-growing models is obtained from experiments, and how is it tested? Note that this is not asking about how the experiments to obtain new observations are performed (these are already well presented), but asking what new understanding/hypothesis is obtained from the observations, and how their correctness is justified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QYld2wGK0j", "forum": "HZRz9R1Xjo", "replyto": "HZRz9R1Xjo", "signatures": ["ICLR.cc/2026/Conference/Submission20628/Reviewer_ZzFz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20628/Reviewer_ZzFz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879844291, "cdate": 1761879844291, "tmdate": 1762934029749, "mdate": 1762934029749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies depth‑growth training (e.g., MIDAS) as a remedy for the “curse of depth” observed in pre‑LN Transformers and proposes LIDAS, a layer‑wise middle‑duplication variant. Using SmolLM‑v1 at 360M and 1.7B parameters, the authors show that grown models (i) increase late‑layer utility according to multiple diagnostics (depth score, early‑exit overlap, tuned‑lens accuracy), (ii) exhibit permutable computational blocks in the middle of the network, and (iii) match or exceed MIDAS on reasoning while keeping NLL stable. Training compute is reported to decrease by ≈23% (1/1.29×) under the paper’s schedule."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Mechanistic depth. Converging diagnostics (tuned‑lens, early‑exit, swap/reverse/skip) make a coherent case that growth increases effective depth usage.\n- Actionable variant. LIDAS is a lightweight change that preserves or improves reasoning without harming NLL (i.e., token-level negative log-likelihood/perplexity on held-out text), indicating no regression in general language modeling quality).\n- Reproducibility. Setups and intervention protocols are described clearly; the narrative is easy to follow."}, "weaknesses": {"value": "- Compute accounting / fairness. Main comparisons fix steps, not FLOPs. Since growth changes training compute, a FLOPs‑matched baseline (e.g., truncating baseline steps to ≈77%) is needed to support “efficiency–performance” claims. Error bars (multi‑seed) are also missing on the headline numbers.\n- Cross‑method context. The paper positions growth as a remedy for pre‑LN “curse of depth,” yet omits direct comparisons to LayerNorm scaling baselines. A small 2×2 factorial (Pre‑LN vs Mix‑LN) × (no growth vs LIDAS) would clarify whether growth is orthogonal or redundant with normalization tricks.\n- Scale & breadth. Evidence is limited to 360M/1.7B and emphasizes reasoning; generalization to ≥7B/13B, instruction‑tuned, code, and knowledge‑heavy QA remains uncertain. Systems issues that may arise at scale (optimizer‑state copying, pipeline‑parallel rebalancing at growth boundaries) are not explored."}, "questions": {"value": "- Backbone generality. How do findings transfer to post‑LN/parallel‑residual and MoE backbones?\n- Curricula interplay. Are growth and length curriculum or token‑drop schedules complementary or redundant?\n- Failure modes. Any tasks where growth hurts (knowledge‑heavy QA, code), and diagnostic clues as to why?\n- Large‑scale stability. At ≥13B, do you observe optimizer‑state copy overheads or the need for short local warm‑ups at growth boundaries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VGqbXYnVTc", "forum": "HZRz9R1Xjo", "replyto": "HZRz9R1Xjo", "signatures": ["ICLR.cc/2026/Conference/Submission20628/Reviewer_h9LG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20628/Reviewer_h9LG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988793096, "cdate": 1761988793096, "tmdate": 1762934029322, "mdate": 1762934029322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a mechanistic investigation into why progressive layer growth improves downstream reasoning performance in Transformer LMs. Starting from MIDAS, the authors develop a layer-wise growth variant (LIDAS) and present a series of depth diagnostics (1) early-exit behavior, skip/swap/reverse perturbations, and block-wise contribution/similarity analyses and (2) showing that growth increases late-layer functional utilization and mitigates the Curse of Depth in pre-LN models.\n\n## Soundness  2 (fair)\nThe analyses are methodologically sound, and the triangulation of probing, perturbation, and contribution metrics is compelling. \nThe primary limitation is that only 2 models of SmolLM family are studied, which is not sufficient to support the argument. Also, several claims are correlational, especially the interpretation that permutation robustness *implies* deeper utilization, which is plausible but not causally demonstrated.\n\n## Presentation 3 (good)\nThe paper is exceptionally well written and well structured. The progression from motivation $\\rightarrow$ analyses $\\rightarrow$ implications is very clear. Some plots (Fig. 3, Fig. 4) assume background context that could be made more self-contained.\n\n## Contribution - 3 (good)\nThe novelty lies in *mechanistic understanding*, not architecture: the work explains **why** depth growth improves performance, beyond reporting that it does. This deepens the conceptual grounding of growth-based reasoning improvements and is well suited for poster-level acceptance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Mechanistic insight into internal computation rather than only surface-level gains.\n- Multiple convergent forms of evidence (early-exit, swap/reverse, contribution metrics).\n- Clear link to Curse of Depth and demonstration of how growth reactivates late layers.\n- Well-scoped and well-written with a strong explanatory narrative.\n- Practical relevance for growth-based strategies in reasoning-oriented LMs."}, "weaknesses": {"value": "### 1. Limited architectural generality\nAll experiments are on SmolLM (360M / 1.7B), a single pre-LN, short-context family. Since the contribution is mechanistic in nature, it remains unclear whether the observed “resurrected depth utilization” is a *property of staged growth itself*, or a *property of this architecture family*. \n\n### 2. Causal link between permutation robustness and “depth utilization” remains implicit\nFor example, section 4.2 shows that grown models are more robust to block-level reordering, and **claims** this as evidence of deeper utilization. However, it needs more detailed explanations to bridge this casual relation.\n\n### 3. Performance gaps are small and lack statistical framing\nIn Table 1, most LIDAS vs MIDAS gains are <1pp; without multi-seed variance this is difficult to interpret; if without confidence intervals or multi-seed reporting, the differences are too small to determine whether those small gains reflect a stable trend or training stochasticity. Since LIDAS is an architectural refinement, the size and stability of differences matter for how much weight the architecture (vs. the growth mechanism) contributes to the overall story. \n\n### 4. The 360M + math-cooldown Primitive result is an unexplained outlier\nIn table 1, the improvement between LIDAS and MIDAS is dramatically larger, however, there's no explanation. (The paper itself only states that the cause of the improvements between grown model and baseline is unclear.) \nThis weakens the confidence that the measured gains derive cleanly from the proposed mechanism rather than an interaction or confound.\n\n### 5. Block-size may be an unexamined confound\nThe periodicity observed in section 4.3 is shown only for block size b=4. If b=2 or b=8 produces a different cycle structure, part of the reported effect may be hyperparameter-induced. A small ablation would confirm whether the pattern is intrinsic or scheduler-driven.\n\n### 6. Need more explanation on the causal relation between symmetry and depth depth utilization \nSection 4.4 mentions that LIDAS is more block-wise symmetrical than MIDAS. However, it does not explain precisely why this offers an advantage in terms of in-depth utilization. Need more detailed explanation to build a causal link. \nAlso, better if there is a causal link between this symmetry and the mechanism of LIDAS."}, "questions": {"value": "In Fig. 1 and Fig. 9, the difference between the two variants is subtle (In Fig.1, it seems that LIDAS is not as good as MIDAS). Is the benefit primarily symmetry/stability, or does it yield distinct behavior in downstream layer activation? Clarifying *where* LIDAS helps would sharpen the architectural takeaway.\n\nAdditionally, please address the weaknesses stated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A1GtEdF2Ja", "forum": "HZRz9R1Xjo", "replyto": "HZRz9R1Xjo", "signatures": ["ICLR.cc/2026/Conference/Submission20628/Reviewer_2ves"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20628/Reviewer_2ves"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042008806, "cdate": 1762042008806, "tmdate": 1762934028139, "mdate": 1762934028139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}