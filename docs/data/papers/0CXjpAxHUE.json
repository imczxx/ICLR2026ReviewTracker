{"id": "0CXjpAxHUE", "number": 15017, "cdate": 1758246906466, "mdate": 1759897335276, "content": {"title": "Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression", "abstract": "Large Language Model (LLM) training often processes vast text corpora in a single pass, leaving much available data underutilized. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws. Concretely, given a $K$-epoch training on $N$ samples, how many fresh samples would one-pass training require to match the same performance? We quantify this using the \\textit{effective reuse rate} of the data, $E(K, N)$, which we define as the factor by which the dataset must grow under one-pass training to match the test loss of multi-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \\approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($\\Theta(\\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings complement a recent empirical study by [Muennighoff et al. (2023)](https://arxiv.org/abs/2305.16264), which found that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \\textit{i.e.}, $E(K, N) \\approx K$ for $K \\le 4$ in our notation. \n    Supported by further empirical validation with LLMs, our results reveal how this behavior depends on the underlying data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.", "tldr": "Theoretical analysis of multi-epoch scaling in linear regression", "keywords": ["Deep learning theory", "Multi-epoch training", "Data-reuse", "Optimization", "Scaling law", "Large language model"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51ee840632db57dc0e2199f27424cf25620fc996.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a theoretical framework for understanding multi-epoch data reuse in the context of linear regression and its implications for data-scaling laws in large model training. It shows that larger datasets can be repeated more times effectively. Simulation and LLM pretraining experiments confirm the theory’s predictions."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Valuable theoretical insights: The discussion of how larger datasets (N) allow more effective reuse is both novel and practically relevant.\n2. Solid theoretical foundation under two regimes: The analysis is carefully constructed for both strongly convex and Zipf-distributed settings.\n3. The experiment empirically confirms the theory’s predictions."}, "weaknesses": {"value": "1. Limited discussion of “depends on the data size and distribution.”\nWhile the paper acknowledges that E(K, N) depends on both dataset size and distribution, the explanation remains mostly theoretical. More quantitative or illustrative examples—especially for large-scale LLM pretraining where data heterogeneity and long-tail effects dominate—would make this claim more convincing."}, "questions": {"value": "1. Theoretically, the paper suggests that a dataset of size N can be effectively “amplified” by a factor of log N through multi-epoch training.\nIf that interpretation is correct, why do modern LLMs only have 4 with billions of tokens?\nDoes this discrepancy imply that current LLMs operate beyond the idealized regime assumed in the theory (e.g., due to non-convexity, heavy-tailed data, or curriculum effects)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DwPuDWrQK4", "forum": "0CXjpAxHUE", "replyto": "0CXjpAxHUE", "signatures": ["ICLR.cc/2026/Conference/Submission15017/Reviewer_H78L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15017/Reviewer_H78L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760694740549, "cdate": 1760694740549, "tmdate": 1762925347841, "mdate": 1762925347841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretical analysis of multi-epoch training for large-scale models, a common practice when high-quality training data is limited. The authors introduce a metric called the \"effective reuse rate,\" $E(K, N)$, to quantify how many additional \"fresh\" data samples one-pass training would need to match the performance of training for K epochs on a dataset of size N. Through a detailed analysis of Stochastic Gradient Descent (SGD) in linear regression, they demonstrate two key regimes: 1) for a small number of epochs (K), the benefit is nearly linear (E(K, N) ≈ K), meaning each pass is almost as good as seeing new data; 2) as K increases, the benefit saturates. Crucially, they prove that the saturation point itself grows with the dataset size N (e.g., logarithmically or as a power of N). This central finding—\"larger datasets can be repeated more\"—challenges previous empirical work that suggested the reuse rate was independent of N. The authors validate this theoretical insight with both synthetic data simulations and pre-training experiments on a large language model."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a principled and rigorous theoretical framework to analyze the widely used but poorly understood practice of multi-epoch training. The introduction of the \"effective reuse rate\" E(K, N) is a clear and valuable conceptual contribution. The key finding that the benefit of data reuse scales with the dataset size (N) is a significant insight. It provides a concrete guideline for practitioners: one can and should repeat larger datasets more times before expecting diminishing returns. This directly refutes a simpler assumption from prior empirical work, making a clear and important contribution to the field of scaling laws.\n\n2. While the core theoretical results are derived in the simplified setting of linear regression, the authors do an excellent job of validating their main qualitative finding with an actual LLM pre-training experiment (Section 6.3). This strengthens the paper's claims significantly and shows that the core intuition derived from theory holds in a much more complex, real-world scenario.\n\n3. The paper is well-written and structured."}, "weaknesses": {"value": "1. The primary limitation is the gap between the theoretical setting (linear regression with SGD) and the practical setting of interest (Transformer-based LLMs trained with AdamW). Linear models cannot capture the complex, non-linear feature learning that occurs in deep networks. While the qualitative findings transfer, the exact quantitative predictions (e.g., the saturation point scaling as Θ(log N)) may not hold for Transformers. This is a standard and often necessary simplification in theoretical work, but it's an important caveat.\n\n2. The theory is developed for Mean Squared Error (MSE) loss, which is standard for regression. However, LLMs are almost universally trained using a cross-entropy loss. These two loss functions have different properties, and it's not immediately obvious if the scaling dynamics would be identical. \n\n3. While the inclusion of LLM experiments is a major strength, their scope is naturally limited by computational cost. The experiments use a 0.3B parameter model. While this provides strong evidence, it does not definitively prove the same scaling behavior would be observed in much larger, state-of-the-art models, where different phenomena might emerge."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xIVqKegPGT", "forum": "0CXjpAxHUE", "replyto": "0CXjpAxHUE", "signatures": ["ICLR.cc/2026/Conference/Submission15017/Reviewer_2N8u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15017/Reviewer_2N8u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921144301, "cdate": 1761921144301, "tmdate": 1762925347360, "mdate": 1762925347360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges prior work that assumed the effective reuse rate of data is independent of dataset size. Through rigorous theoretical analysis in linear regression, the authors demonstrate that larger datasets can be trained for more epochs before experiencing diminishing returns. Specifically, they show that the effective reuse rate E(K,N) depends not only on the number of epochs K, but critically on the dataset size N, which is a factor overlooked in previous empirical scaling laws."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents rigorous theoretical analysis with precise characterizations of the effective reuse rate E(K,N) under both strongly convex and Zipf-distributed settings.\n2. The central insight that larger datasets can be repeated more times is clearly articulated and challenges existing assumptions in the field.\n3. The theoretical predictions are thoroughly validated through two complementary approaches: controlled simulations on synthetic data and large-scale LLM pretraining experiments (up to 200B tokens), both of which strongly support the main hypothesis."}, "weaknesses": {"value": "1. The overall conclusions are very similar to the previous work \"Improved scaling laws in linear regression via data reuse\". \n2. And the paper still lacks sufficient practical evidence from LLMs. It is well established that LLM performance differs significantly between large and small models. A more meaningful experiment would be to scale across different model sizes and examine how the effective reuse rate varies with model capacity."}, "questions": {"value": "1. Could the authors provide a clearer distinction between this work and prior theoretical studies, especially Lin et al. (2025)? While the paper mentions providing \"o(1) relative error\" versus \"Θ(K)\" bounds, it would be helpful to understand what new insights or capabilities this precision enables.\n2. Could the authors clarify the practical utility of these theoretical findings? Specifically, how should practitioners use the E(K,N) ≈ log(N) saturation result to inform training decisions, given that most modern LLMs train for fewer than 5 epochs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SzJw0akHp1", "forum": "0CXjpAxHUE", "replyto": "0CXjpAxHUE", "signatures": ["ICLR.cc/2026/Conference/Submission15017/Reviewer_V6sv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15017/Reviewer_V6sv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943026404, "cdate": 1761943026404, "tmdate": 1762925346895, "mdate": 1762925346895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the question: how large of a dataset is required for one-pass training to match the loss of a dataset of size N trained for K epochs?\n\nThey theoretically characterize the scaling behavior for SGD in linear regression in two settings: strong convexity and Zipf-distributed data. In each settings, there are two phases, one phase where K is small and data can be repeated without harm to the performance, and one where K is large and reused data plateaus in usefulness. The point where this phase transition occurs depends on the setting (strongly convex vs Zipf-distributed data) and the data distribution.\n\nIn contrast to recent empirical work, their analysis supports a functional form where the number of times you can repeat the dataset grows with the size of the dataset. In other words, the practical takeaway is that larger datasets can be repeated more.\n\nThey perform LLM pretraining experiments where they take different size datasets, train them for 100 epochs, extract the loss after varying numbers of epochs, and compare to a 200B dataset trained for one epoch. The experiments validate the small K regime where data reuse doesn't hurt performance significantly, and that the larger datasets can be repeated more."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This is a very nice paper. The core question in the paper is important and well-framed and finding an interesting but tractable theoretical analysis is a valuable contribution. Solving the linear regression problem in both the strongly convex and Zipf distribution settings is valuable and illustrated the dependence on the data distribution exponent. The proof sketch gave nice intuition about the approach and which techniques were used to bound which terms. The LLM experiments give useful validation of the key takeaways (illustrating the small K regime where data reuse is not harmful, and showing that the effective reuse ratio increases with the dataset size)."}, "weaknesses": {"value": "All of the LLM experiments use a constant learning rate schedule with AdamW, rather than some form of learning rate decay (e.g. cosine) as is required for competitive performance in practice. This is a reasonable limitation of a primarily theoretical paper as using a time-horizon-dependent learning rate schedule would require training separate models for every different number of epochs, requiring substantially more compute.\n\n(Similarly, they use the same peak learning rate for all the training runs, and this should likely be tuned for each dataset size and number of epochs, but again this would require substantial compute.)\n\nIn particular, there may be an interaction between the learning rate decay and the bias-variance decomposition (i.e. the learning rate decay at the end of training reduces the gradient noise and \"reveals\" how much the model learned from the repeated data).\n\nTo capture the effects of learning rate decay without requiring significantly more compute, one approach would be to load the existing checkpoints (perhaps from a small number of steps before the end of training), then perform linear learning rate decay to zero across a small number of steps. This would produce a \"trapezoidal\" learning rate schedule for each setting without needing to train a model from scratch for each distinct number of epochs. Then the final decayed losses could be plotted / analyzed as is already done in Figure 2."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O7JXCqHYeE", "forum": "0CXjpAxHUE", "replyto": "0CXjpAxHUE", "signatures": ["ICLR.cc/2026/Conference/Submission15017/Reviewer_tXco"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15017/Reviewer_tXco"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181592600, "cdate": 1762181592600, "tmdate": 1762925346452, "mdate": 1762925346452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}