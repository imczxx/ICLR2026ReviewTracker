{"id": "sbapQyunYK", "number": 8369, "cdate": 1758080316653, "mdate": 1759897789304, "content": {"title": "PARDiff: Bridging Autoregressive and Diffusion Models for Order-Agnostic Graph Generation", "abstract": "Graph generation has long struggled with the trade-off between structural fidelity and permutation robustness: autoregressive models excel in expressivity but break under node-order sensitivity, while diffusion models offer invariance at the cost of directional coherence. We introduce PARDiff, a Progressive AutoRegressive Diffusion framework that unifies these strengths through block-wise, order-agnostic generation guided by learned structural decomposition. Unlike prior heuristics, PARDiff jointly predicts block sizes, ranks nodes, and applies an equivariant diffusion process to each block, aligning AR directionality with diffusion robustness. This reframes graph synthesis as probabilistic reasoning over learned topological partitions, enabling scalable, semantically faithful, and order-agnostic generation across molecular and non-molecular domains without auxiliary features. Experiments show state-of-the-art results on diverse benchmarks, while its modular, latency-aware design supports real-time applications like drug–drug interaction analysis, positioning PARDiff as a paradigm shift in structured generative modeling.", "tldr": "", "keywords": ["Autoregressive Graph Generation", "Diffusion Process", "Transformer"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b373cb18d421b84a4b614fa4e301d63cd2fecc21.pdf", "supplementary_material": "/attachment/4992eced7ba4211ab42acd8cc16708e66776a7a5.pdf"}, "replies": [{"content": {"summary": {"value": "LLM generated paper."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "NO"}, "weaknesses": {"value": "Generated wrong stuffs."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "88GYNr7SxR", "forum": "sbapQyunYK", "replyto": "sbapQyunYK", "signatures": ["ICLR.cc/2026/Conference/Submission8369/Reviewer_DdtL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8369/Reviewer_DdtL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762235984030, "cdate": 1762235984030, "tmdate": 1762920277714, "mdate": 1762920277714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Upon reviewing the manuscript, I discovered a profound degree of content overlap and high similarity with an independently published paper titled \"PARD,\" accessible via arXiv (https://arxiv.org/pdf/2402.03687). This \"PARD\" paper is accepted by NeurIPS 2024.\n\nCrucially, the current submission fails to cite the \"PARD\" paper, despite the substantial equivalence in content.\n\nThis situation strongly suggests a potential violation of academic ethics, specifically in the form of dual submission or academic plagiarism/self-plagiarism.\n\nI request that you formally investigate this case by reviewing the content of both submissions and their publication status to ascertain whether a violation of the academic code of conduct has occurred."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "None,"}, "weaknesses": {"value": "I have prepared a precise, concise list detailing some of the apparent similarities between the submission (which I'll refer to as \"PARDiff\") and the \"PARD\" paper to facilitate your assessment: Please note that this is not an exhaustive list of all potential overlaps, but rather highlights the most evident commonalities.\n\nIntroduction Overlap: Lines 50-60 of the PARDiff submission are substantially similar to the last two paragraphs of the Introduction section in the PARD paper.\nMethodology/Section 3.1 Similarity: Lines 90-124 of PARDiff exhibit significant overlap with Section 3.1 of the PARD paper.\nTheoretical Duplication: Theorem 1 in the PARDiff submission is identical to Proposition 3.1 as presented in the PARD paper.\nArchitectural/Blockwise Similarity: Section 2.1.1 of PARDiff is substantially similar to the \"Autoregressive Blockwise Generation\" component within Section 3.2 of the PARD paper.\nSection 4 Similarity: Section 2.4 of the PARDiff submission aligns directly with the content of Section 4 of the PARD paper."}, "questions": {"value": "I have prepared a precise, concise list detailing some of the apparent similarities between the submission (which I'll refer to as \"PARDiff\") and the \"PARD\" paper to facilitate your assessment: Please note that this is not an exhaustive list of all potential overlaps, but rather highlights the most evident commonalities.\n\nIntroduction Overlap: Lines 50-60 of the PARDiff submission are substantially similar to the last two paragraphs of the Introduction section in the PARD paper.\nMethodology/Section 3.1 Similarity: Lines 90-124 of PARDiff exhibit significant overlap with Section 3.1 of the PARD paper.\nTheoretical Duplication: Theorem 1 in the PARDiff submission is identical to Proposition 3.1 as presented in the PARD paper.\nArchitectural/Blockwise Similarity: Section 2.1.1 of PARDiff is substantially similar to the \"Autoregressive Blockwise Generation\" component within Section 3.2 of the PARD paper.\nSection 4 Similarity: Section 2.4 of the PARDiff submission aligns directly with the content of Section 4 of the PARD paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JedkeNVG53", "forum": "sbapQyunYK", "replyto": "sbapQyunYK", "signatures": ["ICLR.cc/2026/Conference/Submission8369/Reviewer_wAS7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8369/Reviewer_wAS7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762251878148, "cdate": 1762251878148, "tmdate": 1762920277356, "mdate": 1762920277356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Upon reviewing the manuscript, I discovered a profound degree of content overlap and high similarity with an independently published paper titled \"PARD,\" accessible via arXiv (https://arxiv.org/pdf/2402.03687). This \"PARD\" paper is accepted by NeurIPS 2024.\n\nCrucially, the current submission fails to cite the \"PARD\" paper, despite the substantial equivalence in content.\n\nThis situation strongly suggests a potential violation of academic ethics, specifically in the form of dual submission or academic plagiarism/self-plagiarism.\n\nI request that you formally investigate this case by reviewing the content of both submissions and their publication status to ascertain whether a violation of the academic code of conduct has occurred."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "None,"}, "weaknesses": {"value": "I have prepared a precise, concise list detailing some of the apparent similarities between the submission (which I'll refer to as \"PARDiff\") and the \"PARD\" paper to facilitate your assessment: Please note that this is not an exhaustive list of all potential overlaps, but rather highlights the most evident commonalities.\n\nIntroduction Overlap: Lines 50-60 of the PARDiff submission are substantially similar to the last two paragraphs of the Introduction section in the PARD paper.\nMethodology/Section 3.1 Similarity: Lines 90-124 of PARDiff exhibit significant overlap with Section 3.1 of the PARD paper.\nTheoretical Duplication: Theorem 1 in the PARDiff submission is identical to Proposition 3.1 as presented in the PARD paper.\nArchitectural/Blockwise Similarity: Section 2.1.1 of PARDiff is substantially similar to the \"Autoregressive Blockwise Generation\" component within Section 3.2 of the PARD paper.\nSection 4 Similarity: Section 2.4 of the PARDiff submission aligns directly with the content of Section 4 of the PARD paper."}, "questions": {"value": "I have prepared a precise, concise list detailing some of the apparent similarities between the submission (which I'll refer to as \"PARDiff\") and the \"PARD\" paper to facilitate your assessment: Please note that this is not an exhaustive list of all potential overlaps, but rather highlights the most evident commonalities.\n\nIntroduction Overlap: Lines 50-60 of the PARDiff submission are substantially similar to the last two paragraphs of the Introduction section in the PARD paper.\nMethodology/Section 3.1 Similarity: Lines 90-124 of PARDiff exhibit significant overlap with Section 3.1 of the PARD paper.\nTheoretical Duplication: Theorem 1 in the PARDiff submission is identical to Proposition 3.1 as presented in the PARD paper.\nArchitectural/Blockwise Similarity: Section 2.1.1 of PARDiff is substantially similar to the \"Autoregressive Blockwise Generation\" component within Section 3.2 of the PARD paper.\nSection 4 Similarity: Section 2.4 of the PARDiff submission aligns directly with the content of Section 4 of the PARD paper."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JedkeNVG53", "forum": "sbapQyunYK", "replyto": "sbapQyunYK", "signatures": ["ICLR.cc/2026/Conference/Submission8369/Reviewer_wAS7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8369/Reviewer_wAS7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762251878148, "cdate": 1762251878148, "tmdate": 1763135970754, "mdate": 1763135970754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Review \n\nThe paper appears to be a close derivative of the previous work [1], shows strong signs of being generated by a large language model, and potentially breaks double-blindness by exposing author names. Therefore, I recommend that the paper be rejected and, if the suspicions are confirmed, that the authors face appropriate consequences.\n\nIssue 1: Plagiarism\nThe submission reproduces core ideas and results from PARD (arXiv:2402.03687v3) while no citation to PARD appears in PARDiff's references. Concretely:\n- The block-wise AR factorization and the structural-order ranking via with the same algorithm (PARDiff Sec. 2.1.1, p.3-4; PARD Eq. (4)-(5), p.5).\n- The \"equivariant-orbit collapse\" theorem and the annealing/energy argument for symmetry breaking (PARDiff Sec. 2.2.1-2.3, p.5-6; PARD Sec. 3.3-3.4, p.6-7).\n- The causal parallel training and identical masked bilinear product (PARDiff Sec. 2.4, p.7; PARD Sec. 4.2, Eq. (10), p.8; Appx. A.10)\n- Matching Algorithms 2-4 for block-size prediction, diffusion training, and generation (PARDiff p.4-6; PARD Appx. A.8 p.18-19)\n- The same datasets/splits/metrics with reproduced baseline rows in Tables 1-3 (PARDiff p.8-9; PARD p.9-10). \n\nIssue 2: LLM generation\nThere are some strong indicators that the paper may have been generated using a large language model:\n\n- The paper cites a non-existent reference for ConGress, which was introduced as part of [2].  The URL is invalid and there is no such publication in the ICRL 2023 proceedings. The authors refer to it as:\n\t- \"Chen Cai and Yusu Wang. Congress: Conditional graph generation via score-based diffusion. In International Conference on Learning Representations (ICLR), 2023. URL https:// [openreview.net/forum?id=ycyWpR0Uxn](http://openreview.net/forum?id=ycyWpR0Uxn).\"\n\n- DiGress is cited twice, both times referring to the same arxiv link. The second version adds M. Bronstein, who is not an author:\n\t- Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. arXiv preprint arXiv:2209.14734, 2022a.\n    - Clement Vignac, Jiaxuan You, Jure Leskovec, and Michael M. Bronstein. Digress: Discrete denoising diffusion for graph generation. In International Conference on Learning Representations (ICLR), 2023. URL [https://arxiv.org/abs/2209.14734](https://arxiv.org/abs/2209.14734).\n\n- In Table 1. with QM9 results the row titled \"Dataset (Optimal)\" is strange. The reported \"optimal\" values are exceeded by the proposed method, which is logically inconsistent. The \"Mol\" column (\"molecular accuracy\" - not explained what it means) assigns the dataset itself a score of 87%, implying the dataset \"predicts itself,\" which is meaningless and unexplained.\n\nIssue 3: violation of double-blindness\nThere is a name of the author of [1] in the LICENSE file of the submission. While not definitive, the name could correspond to an author of this submission. It is also possible that this is an artifact of copying the LICENSE or other metadata files from the original PARD repository, which would itself further confirm plagiarism.\n\nReferences:\n[1] - Zhao, L., Ding, X., & Akoglu, L. (2024). PARD: Permutation-invariant Autoregressive Diffusion for Graph Generation. arXiv:2402.03687v3.\n[2] - Vignac, C., Krawczuk, I., Siraudin, A., Wang, B., Cevher, V., & Frossard, P. (2023). DiGress: Discrete denoising diffusion for graph generation. In International Conference on Learning Representations (ICLR 2023)"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "."}, "weaknesses": {"value": "."}, "questions": {"value": "."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SNzrxsORgS", "forum": "sbapQyunYK", "replyto": "sbapQyunYK", "signatures": ["ICLR.cc/2026/Conference/Submission8369/Reviewer_PndM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8369/Reviewer_PndM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762268487779, "cdate": 1762268487779, "tmdate": 1762920276983, "mdate": 1762920276983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PARDiff, a progressive autoregressive–diffusion framework for graph generation. The key idea is to bridge the controllability of autoregressive (AR) models and the permutation-invariance of diffusion models by dynamically decomposing graphs into topological blocks, predicting the block order autoregressively, and generating each block via a shared equivariant discrete diffusion process. This allows the model to generate large graphs in a scalable, order-informed manner while avoiding permutation bias. Experiments on molecular and synthetic graph datasets demonstrate improved controllability and structural coherence compared to prior AR-only, diffusion-only, and hybrid approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The ability to improve scalability, structure-awareness, and expressivity in graph generative models is valuable for molecular design and structured graph synthesis tasks."}, "weaknesses": {"value": "The block decomposition module plays a central role but is not fully interpretable. The model may produce suboptimal or unstable block partitions, which directly affect generation quality.\n\nThe training process involves multiple coupled learning components (block prediction, autoregressive ordering, diffusion generation), which increases implementation complexity and may hinder reproducibility.\n\nAlthough more efficient than full-node autoregression, inference still requires step-wise block generation, which can be slow for very large graphs.\n\nThe paper mainly evaluates datasets where block-like or community structures are naturally present. It is unclear how well the model performs on graphs that lack such modularity."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "It is indicated by other reviewers, this paper seems generated by the LLM."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0mzUxAFhwL", "forum": "sbapQyunYK", "replyto": "sbapQyunYK", "signatures": ["ICLR.cc/2026/Conference/Submission8369/Reviewer_W42x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8369/Reviewer_W42x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762601735019, "cdate": 1762601735019, "tmdate": 1762920276600, "mdate": 1762920276600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}