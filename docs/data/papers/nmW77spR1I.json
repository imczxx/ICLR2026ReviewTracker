{"id": "nmW77spR1I", "number": 1911, "cdate": 1756964144835, "mdate": 1759898179179, "content": {"title": "Vision-Language-Action Pretraining from Large-Scale Human Videos", "abstract": "Existing Vision-Language-Action models (VLA) struggle with complex manipulation tasks requiring high dexterity and generalization, primarily due to their reliance on synthetic data with significant sim-to-real gaps or limited teleoperated demonstrations.\nTo address this bottleneck, we propose leveraging human hands as a ``manipulator template'', capitalizing on the rich dexterity and scalability present in web data of human manipulation.\nOur approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks.\nAdditionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. \nTo support our paradigm, we develop a comprehensive data curation pipeline that integrates heterogeneous sources --- including motion capture, VR, and RGB-only videos --- into a large-scale dataset with millions of motion-based instructional instances.\nWe empirically show the excellence of our model in hand motion generation and instruction following, and it also scales well with model and data sizes.\nImportantly, we observe the expected gains in robotic dexterous manipulation as physical instruction tuning is applied.", "tldr": "", "keywords": ["vision-language-action models", "robotics", "learning from video"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc93ae9ec5c3784607233343acdb52a026078e11.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Being-H, a dexterous Vision-Language-Action (VLA) model trained through Physical Instruction Tuning, which integrates large-scale human video data with explicit 3D hand motion modeling. The approach leverages a newly curated dataset (UniHand) and introduces a part-level motion tokenizer for millimeter-level precision."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- [S1] Physical Instruction Tuning effectively extends visual instruction tuning to the physical domain, an interesting idea that unifies VLA, physical space alignment, and post-training adaptation for robotic tasks.\n- [S2] The authors conduct comprehensive analysis, including quantitative and qualitative experiments (simulation and real-robot tasks)."}, "weaknesses": {"value": "- [W1] The manuscript, in its current form, lacks sufficient clarity and is not ready for publication. The inconsistent notation and disorganized presentation of formulas significantly hinder understanding. Substantial revision and careful polishing are required to improve the logical flow and overall readability. I also have several specific questions regarding unclear points and expect the authors to address them thoroughly.\n\n- [W2] Scalability limitations of Physical Instruction Tuning. Although the authors frame their method as scalable, it still depends on datasets with explicit 3D hand motion annotations, which are costly and non-trivial to obtain. This reliance contradicts the claimed scalability and restricts the applicability of the approach to specialized domains where such annotations exist.\n\n- [W3] Complex pipeline with modest gain. The training and alignment processes (including GRQ-based motion tokenization and physical space alignment) add substantial system complexity, yet the improvements over strong baselines are relatively modest compared to existing VLA model, such as GR00T."}, "questions": {"value": "- [Q1] What are $\\mathcal{X}_Q$ and $y_i$? How are they related to motions $\\mathbf{m}$?\n- [Q2] Do $\\mathbf{m}$ and $m$ indicate the same thing?\n- [Q3] What is the exact form of $\\mathcal{L}_{\\text{recon}}$ in Equation 3? \n- [Q4]  What is the exact form of $\\mathcal{L}_{\\text{commit}}$ in Equation 3? The manuscript does not describe the details about this objective."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zRaaQ8gfRY", "forum": "nmW77spR1I", "replyto": "nmW77spR1I", "signatures": ["ICLR.cc/2026/Conference/Submission1911/Reviewer_QutY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1911/Reviewer_QutY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615707600, "cdate": 1761615707600, "tmdate": 1762915943727, "mdate": 1762915943727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a paradigm that leverages large-scale human videos for VLA pretraining. The core method involves pretraining a model to generate detailed human hand motions from videos, and then adapting it to control a robot hand via post-training. The pretrained VLA successfully transfers human dexterity from internet videos to robots, demonstrating superior performance and data efficiency on real-world manipulation tasks compared to prior methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper's strength is its well-motivated idea to leverage human videos as a scalable resource for robot manipulation, a novel pretraining pipeline featuring part-level motion tokenization for high-fidelity control, and demonstrated strong performance with superior data efficiency on dexterous real-world tasks."}, "weaknesses": {"value": "1. The approach is heavily reliant on the MANO hand model, which may not perfectly capture the full complexity and contact dynamics of real-world manipulation, potentially limiting the fidelity of the transferred skills. I suggest having some analytical experiments to assess the impact of potential errors in MANO on the entire pipeline.\n\n2. The chosen tasks in the experiment section are not uniquely dependent on dexterous hands and could largely be accomplished with parallel grippers. The paper does not include functional grasping and in-hand manipulation tasks (e.g., reorienting a pen, spinning a key, or precise tool use) that would rigorously demonstrate the necessity of a dexterous hand and the specific advantages of the learned fine-grained finger control. This narrow task scope limits the claim of achieving general dexterous manipulation.\n\nI would like to improve my score if the authors address my concerns during rebuttal."}, "questions": {"value": "Please refer to the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AFKx7qvmnR", "forum": "nmW77spR1I", "replyto": "nmW77spR1I", "signatures": ["ICLR.cc/2026/Conference/Submission1911/Reviewer_dB8W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1911/Reviewer_dB8W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838506890, "cdate": 1761838506890, "tmdate": 1762915943289, "mdate": 1762915943289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Being-H, a large-scale vision-language-action (VLA) pretraining framework for dexterous robot manipulation.\nThe key idea is to treat human hand motion as a transferable prior for dexterous robot. Being-H first trains a transformer model on 2.5 M human video–text–motion triplets (the proposed UniHand-2.5M dataset) using MANO-based 3D hand parameters tokenized via a part-level grouped residual quantizer (GRQ). The pretrained model aligns vision, language, and hand motion tokens and is later adapted to robots through a lightweight projection and regression head for robot action prediction. Evaluations show that Being-H outperforms prior VLA baselines such as GR00T and InternVL3 on hand motion modeling, RoboCasa simulated tasks, and real-world dexterous manipulation, achieving higher success rates with only a fraction of teleoperation data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Leveraging human-hand motion as pretraining data for dexterous robot control is a promising idea that directly targets the challenge of transferring human manipulation priors to robotic hands.\n\n2. The work conducts large-scale multimodal pretraining on millions of human video–text–motion pairs and demonstrates clear empirical gains on hand motion modeling, simulated and real-world dexterous manipulation experiments.\n\n3. The paper provides thorough experiments and analyses for the pretraining stage, including ablations on quantization design, model scale, and data efficiency, which make the technical contribution solid and well-validated."}, "weaknesses": {"value": "1. The discussion of the embodiment gap between human and robot hands is not sufficiently clear. It remains unclear what specific aspects of human-hand pretraining help dexterous robot control, how large the embodiment gap actually is, and under what conditions the transfer succeeds or fails. The current framework behaves largely as a black box that relies on large-scale data to yield useful priors, without a mechanistic explanation.\n\n2. The real-world evaluation appears incomplete and under-documented. For example, in Table 4, it is unclear how many trials were used to compute the success rate and how post-training performance scales with different amounts of robot data (e.g., 10, 50, or 100 demonstrations). It would also be important to compare against strong imitation-learning baselines such as Diffusion Policy, not just VLA-style models. Moreover, the paper does not provide any real-robot videos in the supplementary material, which makes it difficult to assess qualitative behavior or reproducibility."}, "questions": {"value": "The paper appears to focus only on hand-centric representation learning and is applied to dexterous robotic hands. It would be helpful to clarify this scope explicitly in the title, so readers understand that the method is focusing on (dexterous) hand.\n\nIn Table 3, the performance on the Insertion task seems higher for GR00T than for Being-H."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AMlrYEFz2L", "forum": "nmW77spR1I", "replyto": "nmW77spR1I", "signatures": ["ICLR.cc/2026/Conference/Submission1911/Reviewer_8ymu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1911/Reviewer_8ymu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940777786, "cdate": 1761940777786, "tmdate": 1762915942830, "mdate": 1762915942830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a dexterous VLA training framework from large-scale human hand manipulation videos. It introduces a hand tokenization to tokenize the human hand for pretraining. Next, a post-training on the detextrous hand data is adapted. To support the training, a large-scale dataset is introduced. The experiments are conducted in different tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written. The reviewer enjoys reading the introduction part of the paper, where some substantial challenges are discussed. \n2. The whole training pipeline is well-designed, insightful and reasonable. \n3. The dataset contribution is a great bonus, which is meaningful to the community. \n4. The completeness of the paper is very good, with sufficient experiments (including real-world experiments) and supplementary material."}, "weaknesses": {"value": "1. Based on the knowledge of the reviewer, there is still a large embodiment gap between human hands and robot manipulators, especially when the grasping ways or contact points are distinct between human and robot manipulators. In these extreme cases, does the proposed framework, especially the proposed hand motion representation/tokenizer, still work well?  \n2. Beyond the physical adaptation from the spatial perspective, is it possible to add a physical adaptation module considering the embodiement transfer (from human hand representation to robot manipulator representation) during post-training, to improve the better utilization of the pretrained priors?\n3. Some failure case analysis and limitations should be discussed for future work.   \n4. The so-called \"*physical space* alignment for 3D reasoning\" or \"physical tuning\" is much overclaimed, as \"physical\" generally means a lot (not just depth or camera, also including reflection, force, material, dynamics, and so on). However, the proposed alignment is substantially a data distribution normalization/alignment for better pretraining. This should be addressed in the final version.  \n5. The ablation of post-training, such as scaling up the post-training data, using different post-training datasets (different gaps with pretraining dataset, also evaluate if can generalize to different post-training dataset) or networks, is not provided, which is similarly important for providing a clearer picture of the proposed framework."}, "questions": {"value": "The reviewer expects some discussions about the questions raised in the weaknesses, and is glad to keep the rating if they are well-discussed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dgis4yrBak", "forum": "nmW77spR1I", "replyto": "nmW77spR1I", "signatures": ["ICLR.cc/2026/Conference/Submission1911/Reviewer_qigd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1911/Reviewer_qigd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990692450, "cdate": 1761990692450, "tmdate": 1762915941578, "mdate": 1762915941578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}