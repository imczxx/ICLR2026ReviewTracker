{"id": "BeTZGuerU0", "number": 22884, "cdate": 1758336631632, "mdate": 1759896841323, "content": {"title": "Kernel-based Robust Markov Subsampling for Regularized Nonparametric Regression with Contaminated Data", "abstract": "Large-scale data with contamination are ubiquitous in biomedicine, economics and social science, but\nits statistical learning often suffers from computational bottlenecks and robustness.\nSubsampling offers an efficient solution by sampling a representative subset of uncorrupted data from full dataset, thereby reducing computational costs while enhancing robustness. Existing subsampling methods, like leverage- and gradient-based approaches,\nfocus on parametric models and fail under nonparametric models or severe contamination.\nTo address these limitations, we propose a kernel-based robust Markov subsampling (KRMS) method for nonparametric regression with\ncontaminated data in reproducing kernel Hilbert space (RKHS). By dynamically adjusting Markov sampling probabilities based on\nthe ratio of residuals to kernel norms of predictors, our method simultaneously suppresses contaminated observations\nand prioritizes informative observations, enabling robust learning from contaminated datasets. Theoretically, we establish the asymptotic properties of the estimators, including consistency and asymptotic normality, and generalization bounds under RKHS regularization, providing the first unified framework for robust subsampling in nonparametric settings.\nSimulations and real-data applications demonstrate KRMS’s superiority over existing methods,\nparticularly for high contamination levels. Our approach bridges a critical gap in scalable and\nrobust statistical learning, with broad applicability to large-scale.", "tldr": "Robust subsampling for nonparametric regression with contaminated data", "keywords": ["Contaminated data; Nonparametric regression; Robust subsampling"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a8e1c0cc7aed68f991ea69dc8425ea4ecce8283.pdf", "supplementary_material": "/attachment/901ce28f599c7110e2380819cc5fd3dca1426950.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a penalized nonparametric kernel regression method under data contamination and introduce a Markov sub-sampling method, specifically Algorithm 1: Robust Kernel-based Markov Sub-sampling. The core of the method relies on a Metropolis-Hastings rejection scheme based on the residual kernel-norm score in (3)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors are commended for providing a comprehensive theoretical analysis, including convergence rates, asymptotic distributions, and generalization error analysis.\n- The proposed sub-sampling method effectively reduces the proportion of contamination in the data from $\\theta$ to $\\theta^\\prime$, where $0 \\le \\theta \\le \\theta^\\prime$, thereby enhancing the robustness and effectiveness of the regression."}, "weaknesses": {"value": "- The experimental analysis, while covering both synthetic datasets (linear and nonlinear) and real-world datasets (financial and air quality), lacks a clear baseline comparison. Specifically, experiments under uncontaminated conditions are missing, which are essential to validate the effectiveness of the proposed method under varying contamination probabilities $\\theta$.\n- The concept of distribution $P^\\prime$ being a \"cleaner\" version of the initially contaminated distribution $P$ is not sufficiently quantified. It remains unclear how much \"cleaner\" $P^\\prime$ is compared to $P$, and whether there is any theoretical guarantee regarding the value of θ′ achieved by the algorithm.\n- There is no analysis of the computational complexity of the proposed algorithm 1, which is critical for assessing its scalability and practical applicability."}, "questions": {"value": "1.  Are there theoretical guarantees regarding the extent of contamination reduction, i.e., the value of $\\theta^{\\prime}$ achieved by the algorithm?\n2. Could the authors provide a theoretical or empirical analysis of the time complexity of the proposed algorithm?\n3. The experimental section is placed in the appendix, likely due to space constraints. However, given the heavy reliance on experimental validation in this work, would it be possible to restructure the paper to integrate the experiments into the main body for better readability and emphasis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WFMQ3xY0p3", "forum": "BeTZGuerU0", "replyto": "BeTZGuerU0", "signatures": ["ICLR.cc/2026/Conference/Submission22884/Reviewer_mooN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22884/Reviewer_mooN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761022020234, "cdate": 1761022020234, "tmdate": 1762942424542, "mdate": 1762942424542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel Kernel-based Robust Markov Subsampling (KRMS) method for nonparametric regression with contaminated data, addressing critical challenges in computational scalability and statistical robustness for large-scale datasets. The authors propose a residual kernel-norm scoring mechanism within a Reproducing Kernel Hilbert Space (RKHS) framework, which dynamically adjusts Markov sampling probabilities to suppress contaminated observations while prioritizing informative ones. Theoretically, the work establishes asymptotic properties including consistency, asymptotic normality, and generalization bounds under Huber's contamination model $P = (1-\\theta)F + \\theta Q$. Empirical evaluations demonstrate KRMS's superiority over existing subsampling methods, particularly under high contamination levels. This research bridges a significant gap in scalable robust nonparametric learning, offering a unified framework with broad applicability to contaminated, non-i.i.d. data in scientific domains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Methodological Innovation: KRMS represents the first subsampling approach specifically designed for contaminated data in nonparametric regression settings, leveraging RKHS geometry to effectively separate contaminated observations that would be indistinguishable in original feature spaces.\n\nTheoretical Rigor: The paper provides comprehensive theoretical guarantees including uniform ergodicity of the Markov chain (Theorem 1), consistency of estimators (Theorems 2-3), functional Bahadur representation (Theorem 4), asymptotic normality (Theorems 5-6), and generalization bounds (Theorem 7), establishing a solid foundation for the method."}, "weaknesses": {"value": "The algorithm lacks convergence guarantees for the parameter estimation procedure, particularly for the recursive updating of $\\alpha^{(\\kappa)}$ in Algorithm 1, which is especially concerning under high contamination where initial estimates may be poor. The initialization $\\alpha^{(1)} = \\alpha^{(0)} + 0.2$ is arbitrary without justification, potentially affecting reproducibility and convergence behavior. Experimental limitations include insufficient parameter selection guidelines for critical values like subsample size $n_0$, burn-in period $t_0$, maximum iterations $T_0$, and stopping criterion $\\xi_0$, with no sensitivity analysis provided. The exclusive use of Gaussian kernel $K(x, t) = \\exp\\{-(x-t)^2/4\\}$ without exploring alternatives or analyzing bandwidth parameters limits understanding of kernel dependence. The experimental scale with only $N = 10,000$ observations and $p = 4$ dimensions fails to demonstrate scalability to truly massive datasets or high-dimensional settings where contamination effects would be more pronounced. Additionally, the method is restricted to continuous responses without discussion of extensions to classification problems.\n\nComparisons lack state-of-the-art robust nonparametric regression methods and deep learning approaches, with existing comparisons primarily against parametric methods creating an unfair advantage. No systematic sensitivity analysis is provided for contamination level $\\theta$, subsample size $n_0$, regularization parameter $\\lambda$, or kernel parameters. Theoretical assumptions present several concerns: Condition 1's uniform ergodicity requirement may not hold in practice with complex dependencies; Conditions 3-4's smoothness assumptions are overly restrictive for real-world applications; and the contradiction between Condition 1's Markov dependence and Condition 2's i.i.d. errors is not addressed. The distributional gap between theoretical analysis (assuming $P'$) and algorithm operation (on $\\tilde{D}$) lacks rigorous justification. Proof validity issues include the unjustified convergence rate $O_p(\\ln n/n^2)$ in Theorem 4 under contamination, and the impractical simultaneous conditions $\\lambda = o(1)$ and $(-\\ln \\lambda)^{1/2}/\\omega \\sim n^{1/(2m+1)}$ in Theorems 5-6. Notational inconsistencies (e.g., $H_\\omega(s,t)$ vs. $K_\\omega(s,t)$ in Appendix B) and ambiguous definitions (e.g., initial $H(X)$ specification) further reduce clarity."}, "questions": {"value": "Refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BINtwxflex", "forum": "BeTZGuerU0", "replyto": "BeTZGuerU0", "signatures": ["ICLR.cc/2026/Conference/Submission22884/Reviewer_ikcK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22884/Reviewer_ikcK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558386309, "cdate": 1761558386309, "tmdate": 1762942423758, "mdate": 1762942423758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes KRMS, a robust subsampling framework designed for non-parametric regression with contaminated data. Specifically, KRMS introduces a residual kernel-norm score, which operates in the reproducing kernel Hilbert space and effectively identifies outliers. Moreover, this paper provides theoretical guarantees for the KRMS estimator, establishing its consistency, asymptotic normality, and generalization bounds."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The presentation is clear and well-organized.\n- The paper presents comprehensive theoretical analyses for the proposal.\n- The paper provides both simulated and real-world experiments, demonstrating superior performance."}, "weaknesses": {"value": "- While the theoretical analysis is solid and thorough, the methodological contribution appears limited, relying mainly on the kernel-trick-based residual score.\n- Introducing the residual score into kernel space is an interesting idea. Nevertheless, it would be much better if the paper included an empirical ablation study comparing the kernel-trick version with its linear-space counterpart."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "if8OnsucJx", "forum": "BeTZGuerU0", "replyto": "BeTZGuerU0", "signatures": ["ICLR.cc/2026/Conference/Submission22884/Reviewer_SVvk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22884/Reviewer_SVvk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988292318, "cdate": 1761988292318, "tmdate": 1762942423351, "mdate": 1762942423351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes KRMS, a Metropolis–Hastings (MH) subsampling scheme for kernel ridge regression under Huber contamination. The authors prove uniform ergodicity of the induced chain (finite state space), establish consistency and pointwise asymptotic normality in a symmetric periodic Gaussian RKHS, and derive a generalization bound for u.e.M.c. samples. Simulations and two “real‑data” illustrations are reported."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The target problem is an important regime: robust, scalable learning in RKHS under contamination.\n\nThe paper provides clear, intuitive heuristic: prefer points with small residual relative to their “kernel similarity magnitude”."}, "weaknesses": {"value": "1. The main weaknesses lie in the “cleaner” stationary distribution $\\mathcal{P}'$, which lacks clear explanation. And the paper states \\mathcal{P}' has less contamination $\\theta'<\\theta$. However, this statement is not proved.  The chain with acceptance $\\min \\\\{1,w(z_t)/w(z^*) \\\\}$ has stationary distribution proportional to $1/w(\\cdot,\\alpha)$ (with $\\alpha$ frozen), not obviously to a mixture $1-\\theta')F+\\theta'Q$ with $\\theta'<\\theta)$ The paper asserts convergence to a distribution with reduced contamination yet provides no identification of the MH target beyond ergodicity, nor any argument that $1/w$ upweights uncontaminated draws in the sense of a reduced mixture weight. Theorem 1 (irreducible+aperiodic on a finite set ⇒ uniform ergodicity) does **not** characterize the limit distribution. Consequently, Theorem 7’s bound—with a leading $48M^2\\theta'$ term—remains vacuous.\n\n2. In Algorithm 1, the paper sets $\\alpha^{(1)} = \\alpha^{(0)} + 0.2$. Please justify the logic here.\n\n3. In Algorithm 1, to evaluate $w(z,\\alpha)$, the paper needs to compute the weighted sum of $n$ $K(x_i,x_j)$, which results a complexity of $n$. This contradicts the claim of the complexity $O(T_0(n_0^2 p+n_0^3))$. \n\n4. It is hard to comprehend the numbers in Tables 1-6. It is better to emphasize key numbers."}, "questions": {"value": "na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gmX1pxeq82", "forum": "BeTZGuerU0", "replyto": "BeTZGuerU0", "signatures": ["ICLR.cc/2026/Conference/Submission22884/Reviewer_rXQQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22884/Reviewer_rXQQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988740049, "cdate": 1761988740049, "tmdate": 1762942423146, "mdate": 1762942423146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}