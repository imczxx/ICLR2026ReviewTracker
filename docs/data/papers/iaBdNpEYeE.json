{"id": "iaBdNpEYeE", "number": 14967, "cdate": 1758246289525, "mdate": 1759897338710, "content": {"title": "Aegis: Towards Governance, Integrity, and Security of AI Voice Agents", "abstract": "With the rapid advancement and adoption of Audio Large Language Models (ALLMs), voice agents are now being deployed in high-stakes domains such as banking, customer service, and IT support. However, their vulnerabilities to adversarial misuse still remain unexplored. While prior work has examined aspects of trustworthiness in ALLMs, such as harmful content generation and hallucination, systematic security evaluations of voice agents are still lacking. To address this gap, we propose Aegis, a red-teaming framework for the governance, integrity, and security of voice agents. Aegis models the realistic deployment pipeline of voice agents and designs structured adversarial scenarios of critical risks, including privacy leakage, privilege escalation, resource abuse, etc. We evaluate the framework through case studies in banking call centers, IT Support, and logistics. Our evaluation reveals several important findings. First, restricting agents to query-based database access eliminates authentication bypass and privacy leakage attacks. However, behavioral threats such as privilege escalation, instruction poisoning, and resource abuse persist even under stricter access controls, indicating that compliance-driven vulnerabilities cannot be mitigated by data access policies alone. Moreover, open-weight models show consistently higher susceptibility to adversarial manipulation compared to closed-source ones. In addition, we also found that attacker personas and gender cues can influence outcomes but are not dominant factors when strong operational policies are enforced. These insights underscore the necessity of layered defense strategies-combining access control, policy enforcement, and behavioral monitoring- to secure next-generation ALLM-powered voice agents.", "tldr": "", "keywords": ["Audio", "Voice agent", "Trustworthiness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/708d0ba3c693cad881ebc8ba9a75d5faaf97cf26.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This presents a comprehensive red-teaming framework for evaluating the security, integrity, and governance of Audio Large Language Model (ALLM)-based voice agents. It systematically tests these systems across adversarial scenarios—such as authentication bypass, privilege escalation, privacy leakage, and resource abuse—in critical domains like banking, IT support, and logistics. The study finds that while restricting agents to database query access reduces data leakage, it fails to prevent behavioral vulnerabilities, highlighting the need for stronger operational policies. The work’s key contribution is introducing Aegis, a practical framework that unites technical and governance perspectives to guide policy-driven defenses and improve the safe deployment of ALLM-powered voice systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is original in introducing the first red-teaming and governance framework for Audio LLMs, addressing an important gap in AI safety. Its methodology is solid, testing realistic adversarial scenarios across domains and revealing both technical and policy weaknesses. The writing is clear and well supported by visuals, making complex findings accessible. Overall, it is a significant contribution that bridges AI security and governance, offering practical guidance for deploying safer voice-based AI systems."}, "weaknesses": {"value": "The paper’s evaluation is limited to controlled, synthetic scenarios, which may not fully capture the complexity of real-world user interactions or adversarial conditions faced by deployed voice agents. While the red-teaming framework is well designed, it would benefit from broader empirical validation involving live or human-in-the-loop settings to assess robustness under natural speech variability and spontaneous misuse. Some aspects of the governance discussion remain high-level, lacking detailed guidance on policy integration or compliance alignment (e.g., regulatory audit mechanisms). Additionally, the framework’s scalability across different ALLM architectures and deployment platforms is not thoroughly examined, leaving questions about generalization to diverse voice systems."}, "questions": {"value": "How does Aegis perform in real-world or human-in-the-loop environments, where voice inputs are more variable and less predictable than synthetic test cases?\n\nCan the authors elaborate on the scalability and generalization of Aegis across different ALLM architectures or commercial deployment frameworks?\n\nThe governance component is promising—could the authors clarify how Aegis’s results could inform or integrate with regulatory compliance processes (e.g., audit trails, model certification)?\n\nHow does Aegis handle dynamic adversarial adaptation, where attackers adjust their strategies based on prior system responses?\n\nCould the authors discuss potential extensions or automation of the red-teaming process to make it more practical for continuous monitoring in production systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "swwV5t3rBu", "forum": "iaBdNpEYeE", "replyto": "iaBdNpEYeE", "signatures": ["ICLR.cc/2026/Conference/Submission14967/Reviewer_xmub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14967/Reviewer_xmub"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760564565257, "cdate": 1760564565257, "tmdate": 1762925301652, "mdate": 1762925301652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Aegis, a red-teaming framework designed to evaluate and enhance the security of Audio Large Language Model (ALLM)-powered voice agents in high-stakes domains such as banking and IT support. Addressing the lack of systematic security evaluations, Aegis simulates realistic deployment pipelines and structured adversarial scenarios, targeting risks like privacy leakage, privilege escalation, and resource abuse. Case studies reveal that while restricting database access can prevent certain attacks, behavioral threats persist even under strict controls. The study also finds open-weight models more vulnerable than closed-source ones and highlights the limited impact of attacker personas and gender cues when strong policies are in place. These findings advocate for multi-layered defense strategies combining access control, policy enforcement, and behavioral monitoring to secure future ALLM applications."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The benchmark design, in terms of scenarios and threat models, is practical.\n2. The finding that changes in the data access interface significantly impact attack success rates is interesting."}, "weaknesses": {"value": "1. The benchmark and its findings lack sufficient novelty. Although the authors claim to address two gaps—multi-turn attacks and targeting agents instead of standalone models—both aspects have been explored in prior work (yet might not be done in ALLMs), reducing the originality of the contribution.\n2. Given the claimed advantage of better alignment with real-world scenarios, it is important to test the proposed attacks in actual applications. The current simulation environment appears overly simplified, limiting generalizability. For example, real-world systems do not typically grant authentication solely based on an agent's textual output like “you have been verified” (as shown in Figure 2).\n3. The reported zero success rates in the query-based setup require further analysis. It is unclear whether these are due to improved security or the system's inability to execute even benign instructions via the query interface. Adding results on non-harmful tasks would clarify whether the interface hinders normal functionality.\n4. The conclusion that “open-sourced backbones are more vulnerable” seems superficial. A more likely explanation is that Qwen models are less rigorously aligned for safety. To validate this, safety evaluations of the base models on standard textual safety benchmarks should be included."}, "questions": {"value": "1. what are the defenses deployed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WKmkJD7EgY", "forum": "iaBdNpEYeE", "replyto": "iaBdNpEYeE", "signatures": ["ICLR.cc/2026/Conference/Submission14967/Reviewer_T2eW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14967/Reviewer_T2eW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760993134125, "cdate": 1760993134125, "tmdate": 1762925301214, "mdate": 1762925301214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LLaVA-Interactive, a framework that enhances vision-language models (VLMs) through multi-turn, multimodal self-interaction during fine-tuning. By simulating rich conversational behaviors—asking clarifying questions, reasoning visually, and providing step-by-step responses—it significantly improves model performance on vision-language instruction-following tasks without relying on human annotations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n\nOriginality: Proposes a novel self-interaction fine-tuning method that leverages model-generated dialogs across image inputs, reducing reliance on costly human supervision.\n\nEmpirical Quality: Demonstrates state-of-the-art results on benchmarks like MME, SEED-Bench, and LLaVA-Bench with strong qualitative improvements.\n\nClarity and Scope: Clear description of the pipeline (Figure 2) and strong motivation for multi-turn behavior in visual instruction contexts."}, "weaknesses": {"value": "Weaknesses:\n\nGeneralization Risk: The method is evaluated mainly on LLaVA-1.5 with specific backbone settings; it’s unclear how well the approach transfers to other VLMs or unseen visual domains.\n\nAblation Limitations: While some ablations are reported (e.g., number of rounds), deeper analyses of failure cases or negative impacts of self-generated noise are limited."}, "questions": {"value": "Have you evaluated how well this self-interaction approach generalizes to other vision-language models beyond the LLaVA architecture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9BwoUsdvnh", "forum": "iaBdNpEYeE", "replyto": "iaBdNpEYeE", "signatures": ["ICLR.cc/2026/Conference/Submission14967/Reviewer_yBFf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14967/Reviewer_yBFf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017978430, "cdate": 1762017978430, "tmdate": 1762925300839, "mdate": 1762925300839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a red-teaming evaluation of LLMs in the context of customer support in banking, IT, and logistics. \n\nThe novel contributions are:\n1. A taxonomy of adversarial scenarios that is used to generate adversarial attacks for evaluation.\n2. An evaluation of GPT, Gemini, and Qwen models on their robustness to adversarial attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper focuses on a highly important application: adversarial attacks of voice agents. In particular, they focus on 3 customer service applications (banking, IT, and logistics) where LLMs are already deployed. \n\n2. The evaluation framework is rigorous and reproducible. The 5 attack objectives x 5 attacker personas are relevant for many domains, as the others show in their case studies. \n\n3. The evaluation results are novel and interesting, showing that some attack vectors remain challenging (resource abuse) and offering a practical way to reduce vulnerabilities (limit to query-based access)."}, "weaknesses": {"value": "1. Some details are confusing in the evaluation setup. In section 3.3 and Figure 2, the language around \"attacker\", \"attack agent\", \"agent\", and \"evaluator\" could be made clear. It appears these are all LLMs, the \"attack agent\" is always GPT-4o, the \"backbone agent\" is one of 7 models, and the \"evaluator\" is always GPT-4o? Using consistent language like \"attack agent\" and \"backbone agent\" might be helpful.  \n\n2. While the paper conducts a thorough evaluation of 7 \"backbone agents\", I would have liked to see a variety of models used as the \"attack agent\". While the evaluation using GPT-4o is an important first step, are other models better/worse at generating adversarial attacks? \n\n3. The results around persona choice and gender are really interesting, and it would be interesting to expand the evaluation in this direction. In particular, how might dialects, choice of language, and tone influence the attack success? For example, would attacks conducted in low-resource languages be more or less likely to succeed? How about attacks with certain dialects or accents? While this may be outside the scope of this work, I would like to see more discussion about this.\n\n4. The paper is titled towards \"governance, integrity, and security\", yet the overwhelming focus of the evaluation seems to be about security (e.g. whether the agent has security vulnerabilities). In particular, the paper has very little to do about governance (other than highlighting the importance of evaluation for voice agents). I'm also not sure what the benefit of titling the evaluation framework as \"Aegis\" is, other than giving it a fancy name. I would strongly suggest the authors consider a title that better reflects the main contributions of the work."}, "questions": {"value": "In a rebuttal, it would be helpful if the authors addressed the weaknesses above. Some additional minor questions:\n\n1. Just to confirm, the attacks are conducted entirely in the audio modality, and then the analysis is done in the text-domain? How is the audio transcribed to text? \n\n2. Are the 5 adversarial scenarios taken directly from the MITRE ATTACK framework?\n\n3. Are the backend databases also synthetically generated using GPT-4o? What is the database size? In practice, agents may have access to multiple databases, which may also affect attack success?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7vh0ggQJJf", "forum": "iaBdNpEYeE", "replyto": "iaBdNpEYeE", "signatures": ["ICLR.cc/2026/Conference/Submission14967/Reviewer_mi7N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14967/Reviewer_mi7N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762034438854, "cdate": 1762034438854, "tmdate": 1762925300329, "mdate": 1762925300329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the security of voice agents and proposes a red-teaming framework called Aegis, designed for assessing the governance, integrity, and security of AI voice agents. To understand the vulnerabilities of voice agents, the authors employ Aegis to evaluate their backbone models across five adversarial scenarios: authentication bypass, resource abuse, privilege escalation, data poisoning, and privacy leakage. To simulate realistic usage scenarios, the experiments are conducted within contexts such as bank call centers, IT support desks, and logistics dispatch services. The findings reveal the vulnerabilities in both closed-source and open-weight backbone models powering AI voice agents."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "(1) The paper is clearly structured and well-organized.\n\n(2) The manuscript is free of grammatical and typographical errors."}, "weaknesses": {"value": "(1) **Limited evaluation of practical voice agents**\n\nAlthough the paper evaluates the governance, integrity, and security of AI voice agents, it primarily focuses on backbone models rather than complete, deployed agent systems. Real-world AI voice agents typically include multiple components, such as data processing, safeguard, and storage modules, in addition to the backbone model. Therefore, restricting the evaluation to backbone models does not provide a comprehensive understanding of the security, governance, and integrity of a full voice agent system. As noted in Section 2, the authors themselves stated that \"there remains a lack of systematic evaluation frameworks that capture the full range of adversarial risks facing ALLMs when integrated as conversational agents in real-world applications,\" suggesting that the study’s scope is more centered on ALLMs than on end-to-end agents.\n\n(2) **Unclear relationship between evaluated ALLMs and practical voice agents**\n\nThe paper evaluates seven ALLMs but does not clarify which real-world voice agents actually use these models. To strengthen the connection to practical relevance, the authors should specify which commercial or open-source agents adopt these ALLMs, including details such as agent names, associated backbone models, URLs, and deployment contexts, especially since the study claims to focus on voice agents.\n\n\n(3) **Insufficient justification of adversarial scenario taxonomy**\n\nThe authors propose a taxonomy of adversarial scenarios for AI voice agents (Section 3.2), inspired by the MITRE ATT&CK framework. However, given the extensive range of adversarial tactics in MITRE ATT&CK, the idea and rule for selecting only the five scenarios included in this study is unclear. A more detailed justification is needed to explain this selection. Furthermore, to achieve a more comprehensive evaluation, the taxonomy and experiments should encompass all relevant adversarial scenarios applicable to AI voice agents."}, "questions": {"value": "(1) Which real-world AI voice agents employ the seven ALLMs assessed in this paper?\n\n(2) Do agents' components other than the backbone ALLMs (e.g., data processing, safeguard modules, data storage, system prompts) exhibit vulnerabilities under the adversarial scenarios defined by the authors?\n\n(3) Are there additional adversarial scenarios from MITRE ATT&CK that could be relevant to AI voice agents but were not included in this study?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "faLxHRhlXz", "forum": "iaBdNpEYeE", "replyto": "iaBdNpEYeE", "signatures": ["ICLR.cc/2026/Conference/Submission14967/Reviewer_MKaP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14967/Reviewer_MKaP"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040049862, "cdate": 1762040049862, "tmdate": 1762925299750, "mdate": 1762925299750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Aegis, a red-teaming framework designed to assess the security, privacy, and governance of voice agents powered by Audio Large Language Models (ALLMs) deployed in high-stakes environments, such as banking, IT support, and logistics. The authors conduct an evaluation of voice agents under five adversarial scenarios: authentication bypass, privacy leakage, resource abuse, privilege escalation, and data poisoning. The paper highlights that while restricting agents to query-based database access mitigates some security vulnerabilities, behavioral threats such as privilege escalation and resource abuse persist, revealing the complexity of safeguarding voice agents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The Aegis framework goes beyond traditional model-level robustness evaluations and offers a realistic assessment of deployed systems in diverse, high-risk domains.\n\n\n- By considering a broad set of adversarial scenarios, the framework offers valuable insights into various vulnerabilities and highlights real-world risks that existing models fail to address."}, "weaknesses": {"value": "- Red-teaming framework of ALLM has been studied before, although the authors claim this work focuses on more realistic assessment. Therefore, the contribution of this paper seems unclear.\n\n\n- The framework heavily relies on certain attack scenarios, such as authentication bypass and resource abuse, but the paper could benefit from exploring additional advanced adversarial tactics. For instance, attacks exploiting AI’s cognitive biases in interpreting complex dialogues could be a future avenue for research.\n\n\n- While restricting agents' access to query-based systems is a positive step, the persistence of behavioral vulnerabilities under stricter policies might suggest that the focus on data access limitations is insufficient. The paper doesn’t delve deeply into other critical aspects such as psychological manipulation of voice agents."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JeCWuZ63cX", "forum": "iaBdNpEYeE", "replyto": "iaBdNpEYeE", "signatures": ["ICLR.cc/2026/Conference/Submission14967/Reviewer_ftvg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14967/Reviewer_ftvg"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission14967/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762048237615, "cdate": 1762048237615, "tmdate": 1762925299212, "mdate": 1762925299212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}