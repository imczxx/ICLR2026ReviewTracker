{"id": "oFw1hA6eAj", "number": 3656, "cdate": 1757492301374, "mdate": 1759898076538, "content": {"title": "OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision", "abstract": "Multimodal large language models (MLLMs) have shown strong vision–language reasoning abilities but still lack robust 3D spatial understanding, which is critical for autonomous driving. This limitation stems from two key challenges: (1) the difficulty of constructing accessible yet effective 3D representations without expensive manual annotations, and (2) the loss of fine-grained spatial details in VLMs due to the absence of large-scale 3D vision–language pretraining. To address these challenges, we propose OccVLA, a novel framework that integrates 3D occupancy representations into a unified multimodal reasoning process. Unlike prior approaches that rely on explicit 3D inputs, OccVLA treats dense 3D occupancy as both a predictive output and a supervisory signal, enabling the model to learn fine-grained spatial structures directly from 2D visual inputs. The occupancy prediction are regarded as implicit reasoning processes and can be skipped during inference without performance degradation, thereby adding no extra computational overhead. OccVLA achieves state-of-the-art results on the nuScenes benchmark for trajectory planning and demonstrates superior performance on 3D visual question-answering tasks, offering a scalable, interpretable, and fully vision-based solution for autonomous driving.", "tldr": "", "keywords": ["Autonomous driving", "Occupancy", "VLA"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f705d90b9544535aff64f11d5ee8cc6d720d3f08.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a novel framework called OccVLA, which integrates 3D occupancy supervision into a VLA model. The model is trained to predict latent occupancy representations, while the occupancy reasoning can be skipped during inference. Experimental results demonstrate that OccVLA achieves strong performance on the nuScenes planning and QA tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tOccVLA presents an effective approach to integrating occupancy supervision and demonstrates promising results on the nuScenes planning and QA tasks.\n\n2.\tThe method introduces no additional inference cost.\n\n3.\tThe visualization in Figure 5 shows that OccVLA can generate meaningful occupancy representations."}, "weaknesses": {"value": "1.\tIn OccVLA training, both occupancy supervision and CoT supervision are used jointly. It would be clearer to isolate the contribution of each component and provide detailed ablation studies. Additionally, it is unclear which model serves as the baseline that excludes only the OccVLA-nuScenes training.\n\n2.\tIt remains unclear how occupancy prediction is removed during inference and how much the resulting misalignment between training and testing affects performance. Furthermore, is this skipping process a default setting used in all experiments or an optional acceleration setting?\n\n3.\t(Minor) It looks like the authors modify the line space significantly. The captions of Figure 1 and Figure 5 are nearly overlap with the following text.\n\n4.\t(Minor) There is a spelling error in Line 299, where the word “choozaizuose” is misspelled."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fC4ZoFRXsU", "forum": "oFw1hA6eAj", "replyto": "oFw1hA6eAj", "signatures": ["ICLR.cc/2026/Conference/Submission3656/Reviewer_ho2c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3656/Reviewer_ho2c"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761212480108, "cdate": 1761212480108, "tmdate": 1762916901574, "mdate": 1762916901574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OccVLA, a Vision-Language-Action model designed to integrate 3D spatial understanding critical for autonomous driving. It addresses the limitation of current Multimodal Large Language Models (MLLMs) by treating dense 3D occupancy as an implicit supervisory signal, allowing the model to learn fine-grained spatial structures directly from 2D input. Crucially, the 3D occupancy prediction is an intermediate reasoning step that can be skipped during inference, ensuring no added computational overhead or latency degradation. The comprehensive experiments demonstrate the model's effectiveness across various tasks, including VQA, grounded perception, and autonomous driving planning on standard benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The concept of utilizing implicit 3D occupancy as a self-supervisory signal represents a significant contribution to spatial reasoning in autonomous driving.\n2. The unified architecture is capable of handling diverse multimodal tasks (perception, reasoning, and action planning) within a single, consistent framework."}, "weaknesses": {"value": "1. The core concept bears significant resemblance to the approach proposed in previous works ([*] Ross: Reconstructive Visual Instruction Tuning, ICLR2025), yet the paper fails to discuss or provide a comparative analysis against these related works.\n2. The paper introduces the \"Occupancy Transformer\" as a core component, yet it fails to clearly articulate its novel architectural differences compared to a standard transformer block.\n3. The description of the Latent Occupancy Prediction module is incomplete in several crucial aspects. Firstly, the motivation for choosing VQ-VAE over alternative modern generative models, such as diffusion-based variants, is insufficiently discussed. Secondly, many critical architectural hyperparameters are missing, most notably the specific number of queries used for the transformer-based prediction.\n4. The evaluation of the planning capability is limited to open-loop metrics on the nuScenes dataset, lacking essential simulation or closed-loop experiments.\n5. A significant concern remains that the overall complexity and integrated architecture—even without explicitly activating the 3D occupancy prediction—still introduces an inherent latency overhead compared to simpler, pure VLA baselines. A detailed latency profile comparing the final OccVLA model (with the module skipped) against relevant efficient baselines and against its own full model variant (with the occupancy module included) is essential for validation.\n6. Line 299 contains a clear typo (\"choozaizuose\")."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yTFcyTlzHV", "forum": "oFw1hA6eAj", "replyto": "oFw1hA6eAj", "signatures": ["ICLR.cc/2026/Conference/Submission3656/Reviewer_E7ic"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3656/Reviewer_E7ic"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761300085731, "cdate": 1761300085731, "tmdate": 1762916900200, "mdate": 1762916900200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes OccVLA, a novel Occupancy Vision-Language-Action framework that integrates dense 3D occupancy prediction into the VLM backbone, enabling the model to learn fine-grained spatial understanding directly from 2D images. This approach treats occupancy prediction as an implicit reasoning process that can be skipped during inference for zero computational overhead, achieving state-of-the-art results on nuScenes for trajectory planning and 3D visual question-answering."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is highly motivated. Using a dense 3D signal to help the VLM/MLLM improve the spatial understanding capability is a novel idea.\n2. The paper is easy to follow.\n3. The methods demonstrate SOTA performance on two benchmarks."}, "weaknesses": {"value": "I have major concerns in the paper's incorrect statements and experiments. \n1) The authors make an invalid claim regarding existing VLM supervision. The statement in the introduction section that \"supervision relies on 3D annotations described in text (e.g., coordinates or bounding boxes), which are inherently weak and sparse\"  lacks supporting evidence. While bounding boxes are inherently sparser than dense occupancy, the term 'weak' is judgmental and should either be supported by a metric-based comparison or replaced with a more neutral term.\n2) The argument against the scalability of 3D bounding box supervision is weakened by recent advances in auto-labeling. The critique that prior methods are constrained by a lack of scalability due to the need for \"extensive manual labeling\" is outdated. Given that the proposed OccVLA-nuScenes dataset relies on automated pipelines to generate occupancy, the authors should acknowledge that recent advancements have also enabled high-quality auto-labeling for 3D detection/bounding box supervision, thereby allowing both dense (occupancy) and sparse (box) supervision methods to scale.\n3) The characterization of the EMMA baseline is inaccurate and potentially misleading. The paper claims that the state-of-the-art method EMMA relies on costly supervision (3D/BEV coordinates & 3D bounding box) that \"limits its scalability\". This is incorrect for the base model, which achieves strong results via self-supervision on motion planning trajectories alone. The authors should clarify the specific EMMA variant they are comparing against and justify why its data requirements fundamentally limit scalability more than their own approach, which requires dense, automatically generated 3D occupancy data.\n4) The related work section and experimental evaluation lack important context. (1) The authors should discuss recent and relevant work such as S4-Driver[1], which also focuses on improving Large Language Models for driving by enhancing spatio-temporal visual representation and lifting 2D information to 3D. (2) The nuScenes planning benchmark is rapidly becoming outdated for challenging scenarios. To demonstrate the robust performance of OccVLA on the latest benchmarks and its capability in long-tail scenarios, the authors are strongly encouraged to provide results on the current standard, such as the Waymo Open Dataset End-to-End Driving (WOD-E2E) challenge[2].\n\n[1]S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation\n[2] WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios"}, "questions": {"value": "**Verification of Foundational Claims**\nThe paper's narrative regarding the limitations of existing supervision requires justification, especially concerning scalability and the quality of ground truth.\n- Scalability of Bounding Box Supervision: The claim that models requiring 3D Bounding Boxes are not scalable due to human effort is debatable, given recent advances in high-quality auto-labeling techniques for 3D detection. To validate the paper's core motivation, the authors must address the reliance on automated pipelines for both dense and sparse supervision. I request that the authors either:\n1)  Provide quantitative evidence that the auto-labeling quality of the occupancy grid map is significantly superior to modern auto-labeled 3D detection bounding boxes, or that occupancy GT requires no auto-labeling at all.\n2) Acknowledge the scalability argument and adjust the paper's storyline accordingly.\n- Unsupported Claim on Supervision Quality.  The paper states that 3D annotations described in text are \"inherently weak and sparse.\" While they are sparser than dense occupancy, the term \"weak\" lacks a clear technical definition or reference. I request that the authors either provide evidence to justify the use of \"weak\"\n\n**Comprehensive Evaluation**\n- The analysis of related work is incomplete. I believe a crucial comparison with S4-Driver is necessary.\n- Outdated Benchmark : The nuScenes planning benchmark is becoming outdated. To demonstrate the method's real-world generalization to challenging long-tail scenarios, I strongly recommend evaluating performance on  Waymo Open Dataset End-to-End Driving (WOD-E2E) challenge. If the method cannot produce good occupancy grid maps or competitive planning results in WOD-E2E, this indicates a severe limitation in its generalization or scalability that must be clearly discussed as a weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xBQHeC6aCo", "forum": "oFw1hA6eAj", "replyto": "oFw1hA6eAj", "signatures": ["ICLR.cc/2026/Conference/Submission3656/Reviewer_QBwy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3656/Reviewer_QBwy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836990519, "cdate": 1761836990519, "tmdate": 1762916899908, "mdate": 1762916899908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OccVLA, a framework that integrates 3D occupancy representations into vision-language models for autonomous driving. The key innovation is treating occupancy prediction as both an output and supervisory signal, which can be skipped during inference. The authors evaluate on nuScenes for trajectory planning and 3D visual question answering, achieving competitive results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Using occupancy as implicit supervision rather than explicit input is creative and addresses computational efficiency concerns during inference.\n2.The paper evaluates on multiple tasks (motion planning, VQA, occupancy prediction) and provides extensive comparisons with relevant baselines.\n3.The automated data pipeline for generating meta-actions and CoT annotations could be valuable for the community.\n4.Achieves state-of-the-art on trajectory planning (0.28m average L2 error) and superior performance on NuScenes-QA (59.5% accuracy).\n5.The paper includes useful ablations on occupancy supervision and ego trajectory input."}, "weaknesses": {"value": "1.The core architecture (cross-attention between occupancy queries and visual features) is relatively standard. The main contribution is using occupancy as supervision, which feels incremental.\n2.The occupancy prediction results (~10% mIoU) are not comprehensively evaluated against specialized occupancy prediction methods\n3.Several grammatical errors and awkward phrasings (e.g., \"choozaizuose\" in line 299)\n4.The paper structure could be improved - the three-stage training is mentioned but not clearly motivated\n5.All experiments are on nuScenes. How does the approach generalize to other datasets or driving scenarios?"}, "questions": {"value": "1.Can you provide actual inference time measurements comparing with/without occupancy prediction?\n2.Can you ablate the value of λ_occ and provide more justification for your choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0tLiElVPqK", "forum": "oFw1hA6eAj", "replyto": "oFw1hA6eAj", "signatures": ["ICLR.cc/2026/Conference/Submission3656/Reviewer_5Rfz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3656/Reviewer_5Rfz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3656/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936128685, "cdate": 1761936128685, "tmdate": 1762916899673, "mdate": 1762916899673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}