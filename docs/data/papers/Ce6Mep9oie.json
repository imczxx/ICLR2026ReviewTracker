{"id": "Ce6Mep9oie", "number": 2351, "cdate": 1757062128101, "mdate": 1759898154274, "content": {"title": "Continuous Autoregressive Language Models", "abstract": "The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process. We argue that overcoming this bottleneck requires a new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autoregressive Language Models (CALM), a paradigm shift from discrete next-token prediction to continuous next-vector prediction. CALM uses a high-fidelity autoencoder to compress a chunk of K tokens into a single continuous vector, thereby reducing the number of generative steps K-fold. This paradigm shift necessitates a new modeling toolkit; therefore, we develop a comprehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain without access to explicit probabilities. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the performance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable pathway towards ultra-efficient language models.", "tldr": "We replace discrete next-token prediction with continuous next-vector prediction as a paradigm shift to accelerate the training and inference of LLMs.", "keywords": ["Continuous Autoregressive Language Models", "Autoencoder", "Next-Vector Prediction"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8ffeeb58650b266e199d5cd04a0dd8522adb1fd.pdf", "supplementary_material": "/attachment/d3995db3fec816acbd1b52fc24946340490a4d3e.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce an architecture change to transformers, that allow to run the transformer backbone on multiple tokens. I.e. a compression module is introduced that merges the embeddings of k tokens into 1, after standard transformer, the hidden state is decoded thru a series of noise inductions and a variational AE that directly outputs the k next tokens. Like that, it contributes towards more efficient systems as well as an interesting shift in LLM modeling objectives."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- in principle interesting idea, in a relevant field. the compression aspect as well as shift to monte carlo based loss instead of CE.\n- good writing, easy to follow\n- plausible construction towards the experiments"}, "weaknesses": {"value": "Major weakness that leads to my reject is the lack of comparisons and failed demonstration of language modeling practicality. Yes you demonstrated somewhat stable scaling law when comparing Brier-Score performance, but: \n\nW1 there is no actual language modeling performance shown anywhere, such as exact-match decoding on token on a corpus. it is not clear that the brier score is useful for language modeling and a simple correlation plot to CE in appendix is not enough. the experiment section in main paper imho is pretty delusive/ insufficient.\n\nW2 while cited, there is no comparison to other multi-token methods like Gloeckle. hierarchical approaches, that also compress the input, have also been there.\n\nW3 if i were to abstract W1 and W2 and 'just focus on the 'paradigm shift'', there are further ablations missing:\n- 3.1 why / how many of those stochastic MLP's do i need? just to get a 'less brittle' distribution? theoretically they should not be required?\n- 3.2 why is the compressor deterministic? \n- 3.3 can one remain in a fully distributional transformer, i.e. re-inject the 'continuous prediction' for otherwise it's really just a multi-token predictor?\n- 3.4 i feel like the paper failed to pitch the benefit for the continuous/ likelihood-freeness \n\n\nMinors\n- the 'energy based head' is really just a few noisy linear layers followed by the anyway distributional  VAE - it feels a bit overselling."}, "questions": {"value": "plans to address the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N6Dc6GQbEe", "forum": "Ce6Mep9oie", "replyto": "Ce6Mep9oie", "signatures": ["ICLR.cc/2026/Conference/Submission2351/Reviewer_FBcT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2351/Reviewer_FBcT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844098045, "cdate": 1761844098045, "tmdate": 1762916202807, "mdate": 1762916202807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method to train language models by predicting vectors that represent a window of k tokens. This allows the model to generate k tokens per execution, therefore reducing the compute needed per token. The authors argue that individual tokens carry very minimal amount of information and per token log-likelihood estimates is a limiting factor to achieving better information density. Therefore, they propose to build an auto-encoder to compress sets of contiguous tokens to vector representations and reconstruct them back to token space. The main language modeling task is therefore to predict these vectors in an auto-regressive manner. The rest of the paper describes the challenges of using usual log likelihood estimates for training such a model along with proposed likelihood free estimator. The authors also propose a likelihood free evaluation and temperature sampling method for decoding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A new vision of looking into LLM training.\n2. Thorough process to define new loss functions, metrics and decoding mechanism for the proposed LM."}, "weaknesses": {"value": "1. Comparisons with existing multi-token prediction methods\n2. Accuracy of generative benchmarks\n3. Although not explicitly but some papers like 1. Medusa: https://arxiv.org/abs/2401.10774, 2: Your LLM knows the future, https://arxiv.org/pdf/2507.11851 encode this set of token behavior in the model embedding in its current form. So a section explaining this correlation can be useful."}, "questions": {"value": "1. How does the training loop look like?\n2. What would happen if I train the architecture (fig:2) end to end on general CE-Loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jIgOyHP6NH", "forum": "Ce6Mep9oie", "replyto": "Ce6Mep9oie", "signatures": ["ICLR.cc/2026/Conference/Submission2351/Reviewer_fiNd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2351/Reviewer_fiNd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952888192, "cdate": 1761952888192, "tmdate": 1762916202608, "mdate": 1762916202608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Continuous Autoregressive Language Models (CALM), which replace discrete token-by-token prediction with continuous next-vector prediction.\nCompressing multiple tokens into a single vector results in efficiency in generation.\nTo enable CALM, the paper introducies a likelihood-free, continuous generative framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- CALM reduces both train and inference FLOPs compared to a vanilla transformer\n- the generation quality also seems to be better (according to Brier Score)"}, "weaknesses": {"value": "- my main concern is limited evaluations: \n    - How does CALM perform on standard LM evals like HellaSwag, PIQA etc.?\n    - More importantly, i'm curious about in-context recall abilities of CALM. It has been observed that many efficient architectures match vanilla transformer in perplexity, simple LM evals etc., but they lack the ability to recall specific tokens from the past. How does CALM perform on tasks from the EVAPORATE suite: https://huggingface.co/collections/hazyresearch/evaporate-suite ? or needle-in-haystack tasks from RULER benchmark? or even on toy datasets like MQAR [1, 2]?\n\n- perhaps a few comparison with baselines is also missing I think (authors need not run all baselines if they can be differentiated conceptually :) )\n    - How is CALM different from Large Concept Models [3]?\n    - should Block Transformer or MegaByte be a baseline? even though conceptually they are different from CALM, but since the end-goal is same as CALM i.e. achieving lower FLOPs during inference and training, should these be compared?\n\n[1]: Zoology: Measuring and Improving Recall in Efficient Language Models\n\n[2]: Simple linear attention language models balance the recall-throughput tradeoff\n\n[3]: Large Concept Models: Language Modeling in a Sentence Representation Space"}, "questions": {"value": "- do you train a seperate auto-encoder for different values of K ?\n- how does CALM perform on in-context recall tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "23ybz8XxU7", "forum": "Ce6Mep9oie", "replyto": "Ce6Mep9oie", "signatures": ["ICLR.cc/2026/Conference/Submission2351/Reviewer_yTzS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2351/Reviewer_yTzS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060129284, "cdate": 1762060129284, "tmdate": 1762916202445, "mdate": 1762916202445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to model language \"hierarchically\" by auto-encoding sentence chunks to a latent sequence which is then modeled with an energy transformer.  This allows for likelihood-free language model training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method goes beyond standard next-token prediction and allows for hierarchical modeling of language.\n- The proposed methodology for training and evaluation is sound."}, "weaknesses": {"value": "See questions."}, "questions": {"value": "- I'm not sure what the energy interpretation is really doing. The generative head can be thought of as a simple part of the decoder with auxiliary random variables yes? To be clear, an energy function allows for gradient-based sampling which is not done, there is no contrastive divergence or integration during training. What you have seems to be a multi-stage decoder that handles dependencies across the sequence and the generative head is trained via \"supervision\" through reconstruction. Also, you did sort of choose a likelihood when you chose the squared loss: it's a gaussian log-likelihood. Maybe fewer mentions of likelihood-free training will be useful.\n\n\n- With brier LM, because you're monte-carlo estimating for each token (unlike log-likelihood which is directly evaluated), you have to produce error bars.\n\n- Where are non-LM evaluations? Like question-answering, retrieval, and long-context understanding (see benchmarks here: https://arxiv.org/abs/2402.18668) ? Brier-LM being correlated with cross-entropy for standard transformers is not sufficient to justify that you trained a good language model, that too on WikiText-103. Further language dataset evaluations are also necessary.\n\n- The authors do not perform sufficient comparisons and related work needs to be refined. You point out that large concept models and MegaByte both face challenges and say that diffusion based generation is slow, but there was no comparison to see how slow. You also say their Auto-encoder is \"computationally heavy and fragile\" but you also use an auto-encoder, with the main difference being chunking."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8MbJDthfVr", "forum": "Ce6Mep9oie", "replyto": "Ce6Mep9oie", "signatures": ["ICLR.cc/2026/Conference/Submission2351/Reviewer_Wiky"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2351/Reviewer_Wiky"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762270018495, "cdate": 1762270018495, "tmdate": 1762916202306, "mdate": 1762916202306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}