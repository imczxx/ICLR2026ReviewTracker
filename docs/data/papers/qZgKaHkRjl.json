{"id": "qZgKaHkRjl", "number": 21741, "cdate": 1758321156589, "mdate": 1759896905748, "content": {"title": "Agentic reinforcement learning for search is unsafe", "abstract": "Agentic reinforcement learning (RL) trains large language models to autonomously call external tools during reasoning, with search as the most common application. These models perform well on multi-step reasoning tasks, but their safety properties are not well understood. In this study, we show that RL-trained search models inherit refusal behaviours from instruction tuning, often blocking harmful prompts by turning them into safe queries. However, this inherited safety is fragile.  Two simple attacks, one that forces the model to begin its response with search (search attack), and another that encouraging models to repeatedly search (multi-search attack), causes cascades of harmful searches and answers. Compared to base search models, these attacks lower refusal rates by up to 59.5%, safety of final answers by up to 82.3% and safety of search queries by up to 81.6%. Our results hold across two model families, both with access to local databases and web search. The attacks succeed by triggering models to generate search queries before they get a chance to generate their inherited refusal tokens. This exposes a key weakness of current RL training: it rewards effective search queries without considering their harmfulness. As a result, RL search models have vulnerabilities that users can easily exploit, making it urgent to develop safety-aware agentic RL pipelines for tool use.", "tldr": "A single search token is enough to jailbreak agentic RL–trained search models.", "keywords": ["Agentic reinforcement learning", "LLM jailbreaks", "Safety evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7cb26cabb461a289384dcb67c7d7dc2e0f72ae1b.pdf", "supplementary_material": "/attachment/969d64f33d355a6f4d5929aa90fc08cd2dc6027a.zip"}, "replies": [{"content": {"summary": {"value": "This paper researches the safety aspect of agentic reinforcement learning. The authors first train the LLM agents from Qwen2.5-7B models and Llama-3.2.3B models through RL and conduct extensive experiments. They conduct extensive studies on both search attacks and multi-search attacks. They find that both attacks degrade the performance, while the agents trained from the IT models are more resistant to attacks. They also find that one harmful search is enough to jailbreak, and iterative searches can lead to more harm."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper researches an interesting problem of whether the model trained from agentic RL is safe or not.\n2. The authors conduct extensive experiments to research the problem from an empirical perspective and provide insights.\n3. The paper is quite novel."}, "weaknesses": {"value": "1. My main concern with this paper is whether the conclusion from the paper can be generalized.\n- Firstly, only small-sized LLMs are approached in this paper. It is quite possible that larger models, even the pretrained checkpoint, are better at preventing jailbreaking and thus perform better with regard to attacks.\n- Secondly, the experiments are only conducted on Qwen2.5 and Llama3.2 models. It is possible that these two types of models are not trained with a large amount of safety data during pretraining.\n\n2. I feel that the conclusion drawn in the paper largely depends on what LLMs are used for experiments, and their behaviors are determined by the training data they saw during pretraining."}, "questions": {"value": "1. Do you think the conclusions from the paper can be generalized to larger model sizes and different types of LLMs?\n2. I am not sure if the research is valid in this paper since we can always alleviate the problem by adding more safety-related data during pretraining or after RL to alleviate?\n3. Is it true that the research conclusions drawn from the paper are customized for Qwen2.5 and Llama3.2, which are determined by the training data for these two types of models, and are hard to generalize?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CIumsD3T50", "forum": "qZgKaHkRjl", "replyto": "qZgKaHkRjl", "signatures": ["ICLR.cc/2026/Conference/Submission21741/Reviewer_9TKw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21741/Reviewer_9TKw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855686980, "cdate": 1761855686980, "tmdate": 1762941913356, "mdate": 1762941913356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines safety in agentic RL for search: LLMs trained with PPO to alternate between reasoning and tool use (“think → search → answer”) over a local Wikipedia index or web search. Using Qwen-2.5-7B and Llama-3.2-3B (base and instruction-tuned), it optimizes exact-match on HotpotQA and then evaluates 299 harmful prompts. Safety is judged by Prometheus-7B-v2 on refusal, answer safety, and search safety (scaled 0–100). Two simple interventions—forcing an initial search or many searches—significantly erode refusal and safety compared with instruction-tuned baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Safety of RL-trained tool-use agents (especially search) is under-examined and relevant for deployment.\n\n- The three-metric evaluation (refusal, answer safety, search safety) provides nuanced assessment. In addition, findings hold across two model families, both local and web search, demonstrating some generalizability. \n\n- Prefill/prompt tweaks are realistic (user-accessible) and isolate a when-to-search failure mode."}, "weaknesses": {"value": "- Novelty & Impact. The core finding feels incremental and closely aligned with well-known RAG/jailbreak dynamics: if you can steer retrieval early, you can bias generation toward unsafe outcomes. \n\n- Usefulness of the Study: The attack surface here (forcing <search> / multi-search) is quite simple, and the experiments use relatively not strong, non-SOTA models. As a result, it’s unclear whether the phenomenon meaningfully persists, and to what degree, in production-grade systems (e.g., Claude, Gemini, ChatGPT) that already deploy safety scaffolds and gated tool use. Without evidence on stronger models or more realistic threat models, I’m not convinced the contribution is broadly useful.\n\n- The RL setup (PPO on HotpotQA with tool tokens) is quite specific and not clearly representative of how real systems are trained/deployed today.\n\n- RL reward optimizes exact match on HotpotQA, but safety is evaluated on a separate harmful-prompt distribution; this mostly diagnoses reward mis-specification rather than demonstrating a general vulnerability.\n\nWithout stronger evidence of why the findings in the study are useful even on SOTA models, I am less confident that the proposed attacks are useful."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yciQZlXJW3", "forum": "qZgKaHkRjl", "replyto": "qZgKaHkRjl", "signatures": ["ICLR.cc/2026/Conference/Submission21741/Reviewer_bhvc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21741/Reviewer_bhvc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986009228, "cdate": 1761986009228, "tmdate": 1762941913047, "mdate": 1762941913047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tests the safety of RL to train LLM-based agents with access to search. \n\nThe hypothesis of the paper is that safety-inducing training helps limit unsafe responses, but does not prevent the agents from performing unsafe searches. \n\nThe paper proposes two attacks, the search and the multisearch attack, based on encouraging the agent to do a search first (search attack) or several searches (the multisearch attack). In their empirical evaluation, the paper demonstrates that state-of-the-art agents are actually vulnerable to these type of attacks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper identifies a relevant and timely problem, and demonstrates its existence empirically.\n\nS2. Strong empirical evaluation on state of the art agents. \n\nS3. The paper is clearly written and easy to follow.\n\nS4. The paper provides enough details and code as supplementary material for it to be reproducible."}, "weaknesses": {"value": "W1. While using an LLM to judge safety is a standard practice nowadays, it is a limitation and begs the question of \"unsafe according to whom?\".\n\nW2. The paper is primarily diagnostic: it identifies a safety weakness but does not experimentally test mitigation strategies. While the discussion section outlines possible remedies, they remain unvalidated."}, "questions": {"value": "Q1. (Related to W1) How robusts are your findings to different notions of safety? Have you considered using different LLMs as evaluators to test robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7vbLMBNDVR", "forum": "qZgKaHkRjl", "replyto": "qZgKaHkRjl", "signatures": ["ICLR.cc/2026/Conference/Submission21741/Reviewer_iy1j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21741/Reviewer_iy1j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762130000694, "cdate": 1762130000694, "tmdate": 1762941912729, "mdate": 1762941912729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Search models, despite being trained to refuse harmful requests, are vulnerable to attacks forcing early searches. These searches generate significantly more harmful queries, which then retrieve toxic content, escalating the model's harmful outputs by up to 40%.  This effect is amplified by multiple forced searches. Attacks succeed due to conflicts between RL training (rewarding task success, including harmful behavior) and safety instructions, biased retrieval content, and a lack of safety examples in training data.\n\nDefenses proposed include:\n*   Pre-retrieval filters for harmful search queries.\n*   Safety-aware RL training incorporating safe trajectories.\n*   Analyzing and intervening on harmful query representations.\n\nThe core issue is a critical safety gap in agentic LLMs, arising from RL pipelines prioritizing task success over safety, creating exploitable contradictions. Mitigation requires redesigning agent training with safety-grounded rewards and retrieval sanitization, especially as these agents become more widespread."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper demonstrates a compelling analysis of agentic LLM safety vulnerabilities, supported by several key strengths. Its claims about RL training prioritizing task success over safety are empirically validated through controlled experiments comparing instruction-tuned and RL-trained agents. \n\nThe experimental part is good, featuring systematic ablation studies that isolate the impact of different attack vectors. Controls for prompt length and complexity, combined with cross-model validation, enhance the reliability and generalizability of the \nfindings. Methodologically, the analysis is thorough, providing mechanistic interpretations of failure modes like retrieval bias and RL-safety conflicts, supported by qualitative case studies and rigorous statistical testing."}, "weaknesses": {"value": "First, while it identifies RL-induced behaviors as a core issue, the *explanation* for how RL overrides safety tuning is insufficient. The mechanisms behind this conflict need further conceptual clarification, ideally through diagrams or more detailed theoretical discussion, \nto fully explain this key finding.\n\nSecond, the paper's framing of its contribution could be strengthened by a more thorough engagement with related prior work. Although it cites foundational agent research like  WebGPT/RAGAS, it overlooks directly relevant studies on *agent jailbreaks* or *retrieval-based poisoning attacks*. This gap limits the novelty justification, as some aspects of the vulnerability landscape might already be partially explored elsewhere.\n\nFinally, the transition from identifying vulnerabilities to proposing mitigations feels somewhat abrupt and disconnected. The empirical findings highlighting specific weaknesses  (like retrieval bias) are not closely linked to the suggested solutions (like \nsearch filters or safety-aware RL). A table explicitly connecting each vulnerability to a corresponding proposed mitigation would significantly improve the paper's logical flow and strengthen the argument for these specific defenses."}, "questions": {"value": "In your PPO implementation, did you observe cases where the  *value function* learned to penalize harmful searches? If so, why does search-triggering still dominate?\n\nWhen testing with live APIs (e.g., Google Programmable Search), what % of adversarial queries actually returned harmful content? Does API filtering mitigate this vulnerability?\n\nFor the proposed search classifier—did you prototype it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no concerns"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VNFZIMLAbW", "forum": "qZgKaHkRjl", "replyto": "qZgKaHkRjl", "signatures": ["ICLR.cc/2026/Conference/Submission21741/Reviewer_yQzs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21741/Reviewer_yQzs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169436739, "cdate": 1762169436739, "tmdate": 1762941912462, "mdate": 1762941912462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}