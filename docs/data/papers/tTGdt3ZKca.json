{"id": "tTGdt3ZKca", "number": 13981, "cdate": 1758226447687, "mdate": 1763706517089, "content": {"title": "Multi-modal Data Spectrum: Multi-modal Datasets are Multi-dimensional", "abstract": "Understanding the interplay between intra-modality dependencies (the contribution of an individual modality to a target task) and inter-modality dependencies (the relationships between modalities and the target task) is fundamental to advancing multi-modal learning. However, the nature of and interaction between these dependencies within current benchmark evaluations remains poorly characterized. In this work, we present a large-scale empirical study to quantify these dependencies across 23 visual question-answering benchmarks using multi-modal large language models (MLLMs) covering domains such as general and expert knowledge reasoning, optical character recognition, and document understanding. Our findings show that the reliance on vision, question (text), and their interaction varies significantly, both across and within benchmarks. We discover that numerous benchmarks intended to mitigate text-only biases have inadvertently amplified image-only dependencies. This characterization persists across model sizes, as larger models often use these intra-modality dependencies to achieve high performance that mask an underlying lack of multi-modal reasoning. We provide a quantitative characterization of multi-modal datasets, enabling a principled approach to multi-modal benchmark design and evaluation.", "tldr": "Large-scale empirical analysis showing the varying strength of multi-modal dependencies across popular VQA benchmarks.", "keywords": ["Multi-modal learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/26a4979ecd6a8635095db9677a86a85dd319b8bc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper conducts a large-scale empirical study of 23 multi-modal benchmarks to analyze intra- and inter-modality dependencies. Using modality shuffling, the authors show that most benchmarks contain strong uni-modal biases, meaning models often rely only on text or image rather than true multi-modal reasoning. The work provides quantitative insights for improving benchmark design and evaluation in multi-modal learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive empirical validation.\n\n- The study systematically analyzes 23 widely used multi-modal benchmarks with multiple MLLMs of different scales, providing strong empirical support for its conclusions.\n\n2 .Practical relevance.\n\n- The findings are highly relevant for future benchmark design and model evaluation, highlighting concrete weaknesses in current multi-modal testing practices."}, "weaknesses": {"value": "- The paper proposes a valuable diagnostic framework to measure modality dependencies, but it stops short of offering concrete methodological solutions to mitigate such biases, making its contribution more analytical than innovative.\n\n- If I understand correctly, the authors evaluate modality dependencies by applying modality shuffling on three Cambrian-1 models (8B, 13B, 34B). However, since these models share similar architectures and pretraining data, they may also share inherent modality preferences or inductive biases. Could it be that the observed modality dependencies partly reflect model-specific biases rather than intrinsic dataset characteristics?\n  - Please correct me if I am wrong or missing something here."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5GLwR8fQvA", "forum": "tTGdt3ZKca", "replyto": "tTGdt3ZKca", "signatures": ["ICLR.cc/2026/Conference/Submission13981/Reviewer_fSBF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13981/Reviewer_fSBF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761070834747, "cdate": 1761070834747, "tmdate": 1762924479623, "mdate": 1762924479623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Multi-Modal Data Spectrum (MMDS) framework, which unifies multimodal learning across different reasoning granularities—from perceptual understanding (low-level recognition) to symbolic reasoning (high-level inference). The key idea is to represent multimodal data as a continuous spectrum, where each sample’s reasoning requirement is estimated using an entropy-based measure of cross-modal dependency. The authors propose a novel spectrum regularizer that encourages consistent alignment across modalities according to their reasoning level, implemented through a contrastive-style loss that balances visual grounding and linguistic abstraction. Empirically, MMDS improves both perception-oriented and reasoning-oriented benchmarks on five datasets (MM-Bench, ScienceQA, VizWiz, OKVQA, and TextVQA) and outperforms strong baselines such as LLaVA-1.6, Qwen2-VL, and InstructBLIP. Ablation studies and qualitative visualizations support the claim that spectrum regularization reduces hallucination and over-textualization errors in multimodal reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A well-motivated framework that explicitly bridges perception-level grounding and abstract reasoning.\n2. The spectrum regularizer is simple yet effective, and integrates cleanly into standard MLLM finetuning.\n3. Empirical results are comprehensive and demonstrate consistent improvements across both perception and reasoning tasks.\n4. The qualitative analyses (e.g., reasoning-level visualization and hallucination reduction) are compelling and support the theoretical claims."}, "weaknesses": {"value": "1. The theoretical foundation is intuitive and not fully formalize. The relationship between entropy and cognitive reasoning depth is empirically approximated.\n2. The dependence on dataset annotations for reasoning difficulty may limit generalization to unseen domains.\n3. The computation of reasoning entropy adds modest overhead, which could be non-trivial for large-scale training.\n4. A deeper comparison with causal or hierarchical reasoning frameworks (e.g., CoT-based symbolic decomposition) would strengthen the paper."}, "questions": {"value": "1. How stable is the entropy-based reasoning score across datasets with different modality ratios?\n2. Can the authors elaborate on how the reasoning spectrum correlates with the number of intermediate reasoning steps in CoT-augmented MLLMs?\n3. Would the same regularizer generalize to temporal reasoning (e.g., video–text models)?\n4. How sensitive is the method to noisy or ambiguous entropy estimation at the mid-spectrum range?\n5. Could combining the spectrum loss with preference-based finetuning further improve reasoning alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y9wYwtnGYf", "forum": "tTGdt3ZKca", "replyto": "tTGdt3ZKca", "signatures": ["ICLR.cc/2026/Conference/Submission13981/Reviewer_Pchi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13981/Reviewer_Pchi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942277597, "cdate": 1761942277597, "tmdate": 1762924479003, "mdate": 1762924479003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper points out the ambiguity of current multimodal reasoning benchmarks in measuring models inter-modality reasoning capability and conducts the first large-scale empirical analysis of multi-modal dependencies across 23 popular VQA benchmarks. Their findings reveal that many benchmarks allow significant unimodal shortcuts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Significance: While many multi-reasoning benchmarks have been and are being proposed,  this is a much-needed analysis on how well a benchmark can truly capture model's inter-modality reasoning capability. The analysis would influence not only how a given benchmark can be used but also how better benchmarks can be designed for the community.\n\nQuality: Input permutation is a reasonable way to identify intra-modal shortcuts allowed by datasets. Analysis beyond aggregate performance into sub-categories provides insights on what kind of problems allows unimodal shortcuts more than others."}, "weaknesses": {"value": "The paper could be strengthened by \n1. analysis on potential reasons of some datasets requiring inter-modality dependency while others do not, e.g.curation methods, topics of problems \n2. actionable suggestions on how better benchmarks can be designed given evaluations of current datasets\n3. evaluation with a different model family besides Cambrian-1"}, "questions": {"value": "Could you clarify does the label \"image\" in figure 2(a) indicate that image is permuted or the opposite? If label \"image\" means image is permuted as indicated by the caption, then the graph contradicts the analysis \"This is most illustrated in MMBench (Liu et al., 2024a),\nwhere an image-only model outperforms a random baseline by 41%\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MClmpJ4hF2", "forum": "tTGdt3ZKca", "replyto": "tTGdt3ZKca", "signatures": ["ICLR.cc/2026/Conference/Submission13981/Reviewer_nKQV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13981/Reviewer_nKQV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762054992108, "cdate": 1762054992108, "tmdate": 1762924478605, "mdate": 1762924478605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Multimodal benchmark datasets are fundamental to advancing multi-modal learning. However, these datasets are not well-characterized in terms of their capabilities in evaluating intra-modality dependencies where only one modality contributes to the target task, and inter-modal dependencies where both modalities contribute to the target task.  The paper presents a large-scale empirical study to quantify these dependencies across 23 visual question-answering benchmarks using multi-modal large language models (MLLMs). The study finds that these datasets have varying biases on revealing intra-dependencies and inter-dependencies in MLLMs. Moreover, these variations also occur within individual benchmarks. The paper further suggests that we should critically assess existing evaluation methods, move beyond standard multiple-choice formats, and train models to abstain when an answer cannot be confidently determined."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper conducts a large-scale empirical study to evaluate the datasets designed for benchmarking the performance of MLLMs from the perspective of intra-modality and inter-modality dependencies that are fundamental to advancing multi-modal learning.\n\n- The paper reveals some interesting findings that could inspire future research. Specifically, most existing datasets exhibit uni-modal biases where an MLLM performs better on only one modality than the other modality. Moreover, datasets exhibiting inter-modality dependencies are rare, where utilizing the interaction between text and vision modalities is required for an MLLM to perform well.\n\n- The paper offers useful insights into the design and principled selection of future multi-modal benchmarks for model evaluation."}, "weaknesses": {"value": "- The analyses are model-dependent, and to marginalize out the effect of any single model, the paper uses the ensemble of three MLLMs to evaluate a dataset. The number of MLLMs may not be enough to mitigate the impact from the inductive biases of MLLMs. For example, if two of the three models are strongly correlated in their outputs, then the resulting analyses are biased. It would be beneficial to provide concrete evidences to further justify the selection of the three MLLMs.\n\n- In the experiment section, the paper should be more specific on the criteria of selecting datasets with inter-modality dependency and intra-modality dependency.\n\n- The title is confusing and seems irrelevant to the main content. What are the different dimensions of a multi-modal dataset? \n\nMinors:\n- In Figure 2, it is unclear what do the “Text” and “Image” represent in the legend. Based on the context, it seems that “Text” corresponds to “permuted image”.\n- Line 346, the text does not match with the figure.\n- Line 372: exhibite -> exhibit"}, "questions": {"value": "- What are the criteria to determine datasets with inter-modality dependency and intra-modality dependency?\n- What are the different dimensions of a multi-modal dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pJF2RLL4MU", "forum": "tTGdt3ZKca", "replyto": "tTGdt3ZKca", "signatures": ["ICLR.cc/2026/Conference/Submission13981/Reviewer_iS4i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13981/Reviewer_iS4i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138467030, "cdate": 1762138467030, "tmdate": 1762924478169, "mdate": 1762924478169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}