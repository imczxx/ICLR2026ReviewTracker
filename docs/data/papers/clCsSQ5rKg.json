{"id": "clCsSQ5rKg", "number": 10846, "cdate": 1758183236025, "mdate": 1759897624967, "content": {"title": "NeuMa, Born to Work", "abstract": "Grounded in the evolutionary principle that resource constraints favor structural solutions for complex computation, we propose a neuro-centric framework that reframes the success of State Space Models (SSMs), e.g. Mamba, as an unconscious convergence to an incomplete model of the hippocampus—the brain's canonical circuit for sophisticated computations such as pattern separation and completion. Motivated by this, we introduce NeuroMamba (NeuMa), a novel architecture that consciously and faithfully implements the canonical hippocampal circuit, including the dentate gyrus (DG), Cornu Ammonis 3 (CA3), and Cornu Ammonis 1 (CA1), using foundational SSM blocks. Enabled by custom kernels, our design bridges the gap between biological plausibility and practical efficiency. Experiments demonstrate that NeuMa achieves superior performance and learning efficiency on synthetic benchmarks. More critically, it exhibits profound biological fidelity by spontaneously replicating the \"orthogonalized state machine\" dynamics of the biological hippocampus. Finally, we validate its capacity for real-world scientific discovery by developing a generative agent for piezoelectric catalysis that achieves superior performance in this complex, low-resource domain, thereby showcasing a new path for AI architecture design rooted in neuroscience.", "tldr": "We reframe Mamba as an incomplete model of the hippocampus and demonstrate that a more neuro-faithful architecture unlocks superior algorithmic reasoning, emergent robustness, and profound biological fidelity.", "keywords": ["State Space Models", "Computational Neuroscience", "Hippocampal Modeling", "Interpretability", "Architectural Priors"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50462e129b9287a504163b7223ce661545a64d8f.pdf", "supplementary_material": "/attachment/a2ecf359bc475d91326c5bba6dcd339f63cbdc98.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces NeuroMamba (NeuMa), a biologically inspired state-space model designed as a circuit-level analog of the hippocampal DG–CA3–CA1 circuits. The authors argue that existing selective SSMs (e.g., Mamba) implicitly approximate this structure but lack key functional pathways (dentate input and CA3 direct output). NeuMa explicitly implements these modules to achieve both engineering efficiency and biological interpretability. The paper provides (1) synthetic benchmark results showing improved learning stability, (2) neuroscientific validation showing that NeuMa reproduces hippocampal decorrelation dynamics, and (3) a “real-world validation” stage involving fine-tuning a pretrained NeuMa model (NeuMa-Chem) for CO_2 reduction catalysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Conceptual novelty\\\nThe paper presents an interesting perspective by interpreting state-space models through the lens of hippocampal circuitry. The proposed DG–CA3–CA1 modular structure offers a biologically inspired inductive bias that could improve interpretability and potentially influence model dynamics.\n\n- Empirical evaluation on synthetic tasks\\\nOn algorithmic benchmarks such as Selective Copying and Induction Heads, NeuMa achieves comparable or somewhat better convergence behavior than Mamba. The results suggest that the proposed structure can enhance stability in some settings.\n\n- Neuroscientific analysis\\\nThe experiments simulating hippocampal learning dynamics show qualitative similarities between NeuMa’s internal states and observed CA1 decorrelation patterns.\n\n- Ablation study\\\nThe ablations are generally helpful for understanding the role of each sub-module. The comparisons support the claim that DG, CA3-Out, and CA1 gating each contribute to model stability and performance, although the evidence is mainly qualitative."}, "weaknesses": {"value": "- Ambiguous contribution of each biological module\\\nThe paper introduces DG, CA3, and CA1 as functionally distinct components but does not provide a systematic analysis or theoretical justification of their respective computational roles. The ablation study, while informative, does not clearly demonstrate how each module contributes differently to learning dynamics or model performance. As a result, the improvement over Mamba appears somewhat ad hoc and difficult to attribute to specific mechanisms.\n\n- Lack of quantitative rigor in the neuroscience comparison\\\nThe reported resemblance between NeuMa’s internal dynamics and hippocampal recordings is largely qualitative. The choice of thresholds or correlation metrics used to claim “orthogonalized dynamics” is not well justified and could appear tuned to fit the narrative. A more quantitative or statistically grounded comparison would be needed to establish genuine biological correspondence.\n\n- Unclear relevance of the “scientific discovery” example to NeuMa’s claimed capabilities\\\nThe real-world validation section describes a CO₂ catalysis result with significant yield improvement, but it is not explained how this outcome specifically demonstrates NeuMa’s architectural advantages. It remains uncertain whether other architectures such as Transformer or standard Mamba could achieve similar predictive results under comparable training conditions. As currently written, this section feels detached from the main contribution and does not convincingly support NeuMa’s unique capabilities.\n\n- Empirical validation remains speculative\\\nIt is not clearly stated whether the catalytic results came from the authors’ own laboratory experiments, from simulation, or from reanalysis of prior literature. Without transparent methodology and reproducible data, the “real-world validation” claim lacks credibility."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "naQEIbTMZn", "forum": "clCsSQ5rKg", "replyto": "clCsSQ5rKg", "signatures": ["ICLR.cc/2026/Conference/Submission10846/Reviewer_MqWf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10846/Reviewer_MqWf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707588174, "cdate": 1761707588174, "tmdate": 1762922047489, "mdate": 1762922047489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an extension of the Mamba State Space architecture that is inspired by contemporary neuroscientific models of the hippocampus. The architecture introduces additional pathways that feed into an adapted SSM module. The experiments evaluate the architecture on multiple benchmark tasks in comparison to Mamba and include an ablation study into the role of the individual modules."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Establishing potential architectural links between neural circuitry and state-space models is thought-provoking and may lead to new insights for the design of next-generation architectures. The work is well motivated and makes interesting connections to the neuroscientific literature\nDespite the more complex architecture, Table 2 demonstrates GPU-based training with comparable efficiency to Mamba."}, "weaknesses": {"value": "The manuscript is using a model of the hippocampus as \"blueprint\" for the proposed machine learning architecture but it does not clearly separate between sources of inspiration, hypothesis, and empirical validation of the resulting approach. \n\nSection 3 makes it difficult to understand what is established Mamba practice and what is novel. The mix of neuroscientific language (e.g. \"mossy fiber\") and machine learning terminology (e.g. \"projection\") makes it hard to discern what is description and what is motivation or inspiration. \n\nLeaving aside the fact that the evoked contemporary theories of hippocampal computation are far from settled, even if we knew exactly how the hippocampus worked, it would not follow that the same architecture would be a good idea to implement on a modern GPU. It is indeed unlikely that the von Neumann cache hierachy illustrated in Figure 3 suits hippocampal computation (and vice versa). In fact, section 3.3 notes that the models recurrent relations are difficult to parallize and require custom kernels. This means that the performance gains of a proposed architecture must justify the downside of difficult parallelization. However, the presented experimental results, while promising, do not provide a rigorous comparison with prior work. The only baseline in Mamba but there is no comparison with alternative hybrid approaches like Jamba (Lenz+2025) and their respective baselines.\n\nMoreover, the ablation study does not shed much light on the function of the circuit. Notably, removing the DG pathway slightly improved performance but it is not clear why. The offered hypothesis that the module is not required for simple signal filtering is one possibility but further experimentation would be required to firmly establish when including the module is a good idea.\n\nFinally, Section 4.3.3. teases that a \"success of this approach has opened a new research avenue\" but leaves the  \"specifics of which [to be] detailed in a forthcoming publication\". Without further evidence, it is not possible to validate this claim and I recommend removing it until it can be fully explained in the planned publication.\n\n**Additional comments**\n\n- The work cites  Mamba as Gu & Dao, 2023 but it was published in 2024. Please ensure that you cite the published version over preprints.\n- Not to get too hung up on words but to \"propose a paradigm shift\" (in the Kuhn sense?) might be overstating the case for what amounts to an architectural innovation of an existing deep learning model.\n- I recommend to consider changing the title to be more descriptive of the work. It is currently impossible to guess from the title what the work is about."}, "questions": {"value": "The introduction states that the \"trial-and-error process [of AI design], while powerful, frequently lacks a foundation in first principles and leaves interpretability a persistent challenge\". However, is that not how evolutionary search processes work? I am not sure what \"first principles\" went into the design of the hippocampus."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AOZi4uv5dM", "forum": "clCsSQ5rKg", "replyto": "clCsSQ5rKg", "signatures": ["ICLR.cc/2026/Conference/Submission10846/Reviewer_qnSj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10846/Reviewer_qnSj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770001077, "cdate": 1761770001077, "tmdate": 1762922047087, "mdate": 1762922047087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a development to the Mamba model, inspired by the neural architecture of the hippocampus. The CA1 and CA3 pathways are equated to the SSM block - CA3 is the recurrent block which utilises input-dependent A (rec) and B (inp) matrices & maintains a hidden state, while CA1 essentially utilises matrix C to multiplicatively gate the CA3 output. DG is an entirely new pathway that does not exist in current Mamba formulations, and projects an additive input-dependent contribution to the SSM block. \n\nThe model performs better on toy problems involving pattern recognition when under constraints such as noise and distractors. The model also reproduces a hippocampal finding in which responses are decorrelated when taking two paths with different futures (as in key splitter cell literature). Finally, the authors provide preliminary evidence of the model assisting with scientific discovery, although this seems slightly tangential to the main scope of the paper."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The model displays impressive performance gains on toy problems over the standard Mamba architecture. \n- The design itself is original, as are the parallels drawn to the hippocampal architecture. \n- Improved model can also be implemented with the optimised parallel scan\n- Ablation studies provide coarse evidence for the necessity of the components of the new model over baseline Mamba\n- Diagrams of the model (fig 2, 3) are clear and helpful."}, "weaknesses": {"value": "I would like to see a more comprehensive explanation for the rationale behind model design decisions, so that it is more normative. It is true that it is inspired by hippocampus, but it is not clear how principled this translation is, or what each pathway contributes in the normative sense. There are coarse explanations in 3.1, but these are descriptive and apply more to the biological hippocampus rather than the proposed model. I think explicitly comparing the equations of the proposed model to Mamba could help, both for explanation/elucidating the functions of the pathways but also for the reader who may not have the Mamba equations at their fingertips. This might reveal principled similarities and differences: for example, does mf_t (equation for mf should also be provided in text) just perform the effect of adding identity matrix I (or some rotation) to B, since mf is a projection of x? Another example is the D matrix I believe Mamba has on the output, which doesn't appear in the model.\n\nTo really argue the model is an improvement over Mamba I think it would be nice to see the performance on a benchmark rather than just toy examples. There is the CO2 application you touch on (a cool result for sure but possibly too far from the core ideas of the paper), but to convince the reader I think you need to test on one (or several) well-known image or text-based benchmark(s). In the 140M benchmark I could only see metrics relating to efficiency and not performance. \n\nI am skeptical of the results in figure 5c and 7; it's not clear to me why performance increases above the training performance at extremely long sequence lengths. I also don't understand why performance varies so significantly, for example Mamba has 0%, 100%, 0% performance at seq len 10^4-10^5 on induction head level 3 task. Many points also seem to be exactly equivalent between models (e.g. seq len 10^4-10^5 green line is flat and also red has same value in middle) which seems strange, as if there are very limited discrete values (perhaps multiples of vocab size 1/16) that accuracy can take - surely accuracy should be essentially continuous if it is an average over many batches/timesteps? I would also like to see error bars on these plots between separate runs, given how unstable the curves are. Some of the ablation studies in figure 7 indicate better performance without the proposed pathways (instability could be learning rate related). I am also surprised that bigger models perform worse even on training data.\n\nIf you wish to argue that the model recapitulates hippocampus (you don't necessarily have to argue this, it's fine to just use HPC as design inspiration) which figure 6 alludes to, then it would be nice to see some analysis of the separate pathways - do they exhibit similar characteristics in their responses to the HPC pathways they were based on?"}, "questions": {"value": "- Description of the induction head tasks should be in the main text/figure in my opinion, since it's one of the core results and it's not clear from the current figure (5a) - the reader can only really understand by visiting the appendix\n- Would be helpful to compare to mamba equations\n- Equation for mf_t\n- Fig 6 requires some clarification - 6b the dotted lines could be labelled; also not clear what 6b shows, is it the correlation between hippocampal activity on near and far runs?\n- What does the better match in fig6 say about the algorithmic processes in NeuMa compared to Mamba? Why is Mamba incapable of using this representation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XacmCMYLlT", "forum": "clCsSQ5rKg", "replyto": "clCsSQ5rKg", "signatures": ["ICLR.cc/2026/Conference/Submission10846/Reviewer_Sj3q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10846/Reviewer_Sj3q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850530671, "cdate": 1761850530671, "tmdate": 1762922046034, "mdate": 1762922046034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}