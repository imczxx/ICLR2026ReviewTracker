{"id": "qSak1Hjfdq", "number": 226, "cdate": 1756731776068, "mdate": 1763752537586, "content": {"title": "All-day Multi-scenes Lifelong Vision-and-Language Navigation with Tucker Adaptation", "abstract": "Deploying vision-and-language navigation (VLN) agents requires adaptation across diverse scenes and environments, but fine-tuning on a specific scenario often causes catastrophic forgetting in others, which severely limits flexible long-term deployment. We formalize this challenge as the all-day multi-scenes lifelong VLN (AML-VLN) problem. Existing parameter-efficient adapters (e.g., LoRA and its variants) are limited by their two-dimensional matrix form, which fails to capture the multi-hierarchical navigation knowledge spanning multiple scenes and environments. To address this, we propose Tucker Adaptation (TuKA), which represents the multi-hierarchical navigation knowledge as a high-order tensor and leverages Tucker decomposition to decouple the knowledge into shared subspaces and scenario-specific experts. We further introduce a decoupled knowledge incremental learning strategy to consolidate shared subspaces while constraining specific experts for decoupled lifelong learning. Building on TuKA, we also develop a VLN agent named AlldayWalker, which continually learns across multiple navigation scenarios, achieving all-day multi-scenes navigation. Extensive experiments show that AlldayWalker consistently outperforms state-of-the-art baselines.", "tldr": "We propose Tucker Adaptation (TuKA) for VLN agents lifelong learning with multi-hierarchical knowledge in a high-order tensor, achieving all-day multi-scenes lifelong VLN.", "keywords": ["Tensor Decomposition", "Vision-and-Language Navigation", "Lifelong Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8dbeb9cefa49aab2cc58e907a34e9bda1e792ee.pdf", "supplementary_material": "/attachment/63db47718aee8e9106f36e1254d81e870b7b472a.zip"}, "replies": [{"content": {"summary": {"value": "This paper formulates the All-Day Multi-Scenes Lifelong VLN (AML-VLN) problem, that is, agents learn sequential scenarios without forgetting. The authors propose: 1) TuKA (high-order tensor adapter) to model multi-hierarchical knowledge; 2) DKIL (incremental learning strategy) to mitigate forgetting. Experiments on the Allday-Habitat benchmark (24 tasks) show AlldayWalker outperforms LoRA variants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Formalizing AML-VLN clarifies the \"scene-environment-lifelong\" gap in VLN research.\n2. Allday-Habitat provides a unified testbed for lifelong VLN, saving future work from benchmark construction.\n3. Experiments cover diverse baselines (10 LoRA variants) and key ablation (tensor order, DKIL losses), ensuring basic reliability."}, "weaknesses": {"value": "1. novelty is limited: TuKA repurposes Tucker decomposition (a classic technique) for VLN adapters—there is no novel mathematical insight (e.g., improved decomposition efficiency, dynamic rank adjustment). DKIL’s \"decoupled learning\" is a standard combination of Elastic Weight Consolidation loss and orthogonality loss, with no new mechanism to handle cross-scene/environment knowledge transfer.\n\n2. Experiment design is incomplete: \n(1) No generalization test to unseen scenes/environments (critical for lifelong learning and VLN task)—the paper only tests sequential tasks from pre-defined sets.\n(2) No ablation of \"tensor vs. matrix MoE\": A direct comparison between TuKA and a matrix-based MoE with the same hierarchy (scene+environment branches) would confirm if tensors add unique value.\n(3) Real-world deployment is superficial: The DeepRobotDog experiment lacks details (e.g., navigation success rate in real low-light vs. simulation, failure cases).\n\n3. Practicality is unproven: No inference latency/memory data: High-order tensor operations may be too slow for real-time robot navigation (e.g., edge devices), but the paper ignores this.\n\n4. Some related and important works are missing citations:\n[1] NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation\n[2] Test-time Adaptive Vision-and-Language Navigation\n[3] Learning Vision-and-Language Navigation from YouTube Videos\n[4] Magic: Meta-ability guided interactive chain-of-distillation for effective-and-efficient vision-and-language navigation"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7bsLpSfv5D", "forum": "qSak1Hjfdq", "replyto": "qSak1Hjfdq", "signatures": ["ICLR.cc/2026/Conference/Submission226/Reviewer_gySm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission226/Reviewer_gySm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761365587373, "cdate": 1761365587373, "tmdate": 1762915475117, "mdate": 1762915475117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the continual learning problem in Vision-and-Language Navigation (VLN) by formalizing it as the All-Day Multi-Scenes Lifelong VLN (AML-VLN) problem. It proposes Tucker Adaptation (TuKA) — a parameter-efficient, multi-hierarchical adapter that represents navigation knowledge as a high-order tensor. By leveraging Tucker decomposition, TuKA disentangles shared, scene-specific, and environment-specific subspaces, enabling continual adaptation with reduced interference across sequential VLN tasks. The method is integrated into the AlldayWalker agent, which employs a decoupled knowledge incremental learning strategy to preserve prior knowledge while adapting to new conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper is well-written and clearly organized, with a solid motivation and clear presentation of the continual learning setting in VLN.\n2.\tAddresses the underexplored problem of continual learning in vision-and-language navigation — a task that naturally involves sequential adaptation yet has received limited attention in prior research.\n3.\tProposes TuKA, a Tucker-based adapter that decouples scene- and environment-specific experts. This structure enables task-wise adaptation while mitigating interference, effectively reducing catastrophic forgetting.\n4.\tProvides comprehensive experiments, including ablations, scalability analyses and comparisons with multiple adapter-based baselines.\n5.\tEnables LLM-based embodied agents to perform continual learning, achieving higher SR/SPL and lower forgetting rates than other continual learning frameworks."}, "weaknesses": {"value": "1.\tThe current experiments appear to evaluate tasks within the same building. Since a central goal of continual learning is to enable agents to transfer previously acquired knowledge to unseen environments, it would be valuable to include evaluations in completely new buildings or environments to more clearly demonstrate TuKA’s generalization ability beyond the trained domain.\n2.\tContinual learning results can be sensitive to the order in which tasks are learned. It would be helpful to clarify whether you have experimented with different training orders — for example, by shuffling or reversing the task sequence — to examine how such variations affect TuKA’s performance and forgetting behavior.\n3.\tTuKA structurally separates experts into scene- and environment-specific components, but it would be helpful to include experiments verifying whether each expert actually learns the characteristics of its corresponding task.\n4.\tIn L806–807, it is mentioned that “they often suffer from catastrophic forgetting of previously learned scenarios, severely limiting their robustness in dynamic real-world settings.” Adding a discussion on Test-Time Adaptation (TTA) [1-2] as one possible way to address this issue would make the paper more complete.\n5.\tI will consider raising the score if these points are clarified.\n[1] Gao, et al., \"Fast-Slow Test-Time Adaptation for Online Vision-and-Language Navigation.\", ICML 2024.\n[2] Kim, et al., \"Test-Time Adaptation for Online Vision-Language Navigation with Feedback-based Reinforcement Learning.\", ICML 2025."}, "questions": {"value": "1.\tCould you include experiments in completely unseen environments?\n2.\tCould you discuss how much TuKA’s performance might depend on the training sequence?\n3.\tCould you report how the base StreamVLN policy performs under the same AML-VLN setup? Since TuKA is built upon StreamVLN, this comparison would clarify  how much improvement TuKA provides over the baseline model itself.\n4.\tAs new environments or scenes are introduced, the number of factors would also increase — does this lead to improved generalization performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CHUR2hsyQN", "forum": "qSak1Hjfdq", "replyto": "qSak1Hjfdq", "signatures": ["ICLR.cc/2026/Conference/Submission226/Reviewer_Xr3z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission226/Reviewer_Xr3z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892439865, "cdate": 1761892439865, "tmdate": 1762915474983, "mdate": 1762915474983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to All"}, "comment": {"value": "Dear reviewers and area chairs:\n\nWe extend our gratitude to all the reviewers and area chairs for dedicating their time and effort to evaluating our paper. We also thank the reviewers for their positive and insightful comments, which can help us improve our work.\n\nWe are encouraged that:\n\n* All the reviewers (W2Xo, edmj, Xr3z, and gySm) agree that our work explores an **underexplored new problem**, which requires a VLN agent to continually learn a sequence of navigation tasks across diverse scenes and diverse environments, thus achieving all-day multi-scenes navigation. The reviewer W2Xo also thinks that **our exploration is novel and important for future research**.\n\n* All the reviewers recognize that we **establish a new benchmark** with **significant modifications of the existing simulator** for the proposed AML-VLN task with multiple scenes and diverse environments.\n\n* Reviewer W2Xo, Reviewer edmj, and Reviewer Xr3z think that our proposed high-order tensor adaptation design (TuKA) provides **effective decoupling representation learning**. The reviewer Xr3z also thinks that our work with a **solid motivation**.\n\n* All reviewers recognize that our model achieves **state-of-the-art performance under comprehensive experiments**.\n\nWe appreciate the opportunity to discuss and refine our AlldayWalker. We have responded to all reviewers individually to address the concerns, and the following is a brief summary:\n\n* For Reviewer W2Xo and edmj, we clarify the two critical challenges and highlight the technical contributions.\n\n* For Reviewer W2Xo, we add more navigation scenario training tasks.\n\n* For Reviewer W2Xo, we provide additional experiments with the NaVid baseline agent.\n\n* For Reviewer W2Xo, we clarify the degradation imaging models, and our AML-VLN task setting.\n\n* For Reviewer W2Xo, we provide additional experiments for the retrieval mechanism.\n\n* For Reviewer edmj and Xr3z, we provide a comparison and discussion with TTA VLN.\n\n* For Reviewer edmj, we clarify the MoE-LoRA (HydraLoRA) knowledge representation.\n\n* For Reviewer edmj, we clarify the MoE-LoRA (HydraLoRA) knowledge representation, the representational capacity of our TuKA, and Tab3-4 settings.\n\n* For Reviewer Xr3z and gySm, we provide additional experiments for completely unseen environments.\n\n* For Reviewer Xr3z, we provide additional experiments for the training sequence, verifying the expert matrix, comparison with the base StreamVLN policy, and generalization improvement.\n\n* For Reviewer gySm, we further clarify our contribution.\n\n* For Reviewer gySm, we provide additional experiments for matrix-based MoE LoRA with the same hierarchy comparison, and latency with memory comparison.\n\n* For Reviewer gySm, we clarify the real-world deployment.\n\nWe thank all reviewers and area chairs again!\n\nBest,\n\nAuthors of Paper #226"}}, "id": "8TxodtsG62", "forum": "qSak1Hjfdq", "replyto": "qSak1Hjfdq", "signatures": ["ICLR.cc/2026/Conference/Submission226/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission226/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission226/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763748246193, "cdate": 1763748246193, "tmdate": 1763748246193, "mdate": 1763748246193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an interesting idea of a multi-scene lifelong learning approach for Vision-and-Language Navigation (VLN) tasks. The authors introduce Tucker Adaptation (TuKA), a parameter-efficient method that represents multi-hierarchical knowledge as a high-order tensor and applies Tucker decomposition to decouple task-shared and task-specific knowledge.\nThey further propose a decoupled knowledge incremental learning strategy to support continual learning of multi-hierarchical knowledge.\nBased on TuKA, the authors develop a lifelong VLN agent, AlldayWalker, which achieves improved navigation performance compared to LoRA-like approaches on the modified VLN benchmark, demonstrating the value of third-order tensor adaptation for continual representation learning for VLN tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper introduces a parameter-efficient adaptation method (TuKA) that leverages Tucker decomposition to decouple and represent multi-hierarchical knowledge in a high-order tensor, enabling more expressive representation learning.\n\n* The integration of TuKA into a lifelong VLN agent (AlldayWalker) demonstrates the potential of high-order tensor adaptation for continual learning in navigation tasks.\n\n* The authors make meaningful modifications to existing VLN simulators, allowing more systematic evaluation of adaptation performance under diverse out-of-distribution conditions."}, "weaknesses": {"value": "* Line 76: The challenges (i) and (ii) appear to be repeated. Please clarify or consolidate to avoid redundancy.\n\n* The proposed method appears to directly apply Tucker decomposition to the VLN task. The paper should more clearly articulate what unique ideas or design choices provide additional contributions beyond standard Tucker-based factorization.\n\n* The paper omits relevant discussions of recent test-time adaptation approaches for VLN, such as FSTTA (ICML 2024) and FeedTTA (ICML 2025). In particular, FeedTTA also investigates catastrophic forgetting issues in adaptation, which seems closely related to the current work. A comparison or at least a discussion of similarities and differences would help clarify the contribution.\n\n* The authors state that VLN knowledge can be decomposed into navigation skills, scene-specific, and environment-specific knowledge, whereas MoE-LoRA (HydraLoRA) cannot handle more than two-dimensional hierarchies. However, MoE-like methods can, in principle, combine multiple specific experts across more than two dimensions. It is unclear whether it is correct to categorize MoE-LoRA (Eq. 1) as strictly “two-dimensional,” since it composes K experts with potentially different feature dimensionalities. As the main contribution hinges on representational advantages over existing methods, this aspect should be clarified more rigorously.\n\n* Eq. (2) shows the intended decomposition of VLN tasks into shared encoders, scene knowledge, and environment knowledge. However, it is unclear how this decomposition is actually enforced during adaptation learning.\nThe conceptual difference between Eq. (1) and Eq. (2) seems to be in the additive or multiplicative composition of the experts (task-specific knowledge). The authors should provide stronger justification or empirical support demonstrating that Eq. (2) yields better representational capacity than Eq. (1).\n\n* The results in Table 3 are difficult to interpret in terms of the effectiveness of the proposed components. For instance, adding U1 or U2 does not necessarily improve SR, and there is no baseline result for “Sd-G only,” which makes it harder to assess the contribution of U1 and U2.\n\t​\n* Table 4 does not clearly specify which tasks or scenarios the agent was trained and tested on.\nIt remains unclear how these results validate that L_ewc and L_co effectively prevent catastrophic forgetting.\nThe test settings and interpretation of the results should be described in more detail.\n\n\nI would currently lean toward a borderline reject, but I would be open to increasing the rating if the rebuttal provides clarifications on the questions and concerns."}, "questions": {"value": "Please refer to the weakness section where I listed questions as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mvsXKXzR1a", "forum": "qSak1Hjfdq", "replyto": "qSak1Hjfdq", "signatures": ["ICLR.cc/2026/Conference/Submission226/Reviewer_edmj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission226/Reviewer_edmj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953564438, "cdate": 1761953564438, "tmdate": 1762915474894, "mdate": 1762915474894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focus on all-day long multi-scenes lifelong learning for vision-language-navigation. The authors collected 24 scenes including different buildings and using different light intensity to simulate day and night, the author also scattered the environments using a scattering model. This work proposes a continual learning method in order to learn new knowledge without catastrophic forgetting. The method leverages tensor Tucker decomposition to decouple a high-order tensor for multi-hierarchical knowledge. Besides, this work also proposed a decoupled knowledge incremental learning strategy to consolidate shared subspaces while constraining the task-specific knowledge. This work conducts experiments on the collected 24 scenes and compared with different LoRA-based continual learning methods and achieves the best performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis article explores continual learning in vision-language navigation which is novel and important for future research.\n2.\tThe authors represent multi-hierarchical knowledge as high-order tensor and proposes an effective method to decouple task-shared and task-specific representations. The method could also successfully learn new task-specific knowledge incrementally without catastrophic forgetting.\n3.\tExtensive experiments are conducted by the authors which reveals the effectiveness of their method."}, "weaknesses": {"value": "1.\tThe two critical challenges presented in the introduction (Lines 75–78) appear very similar, which may cause confusion for readers. The authors are encouraged to clarify the distinction between them.\n2.\tThe authors collected only 24 scenes for the multi-scene lifelong VLN benchmark setting. Adding more scenes would make the results more convincing; otherwise, the authors should provide justification for why these 24 scenes are sufficient (such as the number of tested episodes).\n3.\tThis work is based solely on the StreamVLN agent. It would be preferable if the authors could conduct additional experiments using other baseline agents to better demonstrate the general effectiveness of the proposed continual learning method."}, "questions": {"value": "1.\tHow are the intensities of scattering, low-light, and overexposure controlled to ensure that these environments are visually distinct yet not overly degraded to disturb visual observation?\n2.\tCould the authors provide visual examples of the observations under different the three conditions to illustrate this distinction?\n3.\tAdditionally, do cases exist where two environments within the same scene differ only slightly in illumination? If so, are they still treated as separate tasks, and could such overlap potentially weaken the challenge of continual learning?\n4.\tDuring inference, TuKA retrieves the appropriate scene and environment experts by matching CLIP features via cosine similarity. How robust is this retrieval mechanism when the test scene partially resembles multiple training scenes in texture or layout? Has the paper evaluated how retrieval ambiguity or mismatched expert selection affects overall navigation performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RwY9EHCHL9", "forum": "qSak1Hjfdq", "replyto": "qSak1Hjfdq", "signatures": ["ICLR.cc/2026/Conference/Submission226/Reviewer_W2Xo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission226/Reviewer_W2Xo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055826015, "cdate": 1762055826015, "tmdate": 1762915474757, "mdate": 1762915474757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}