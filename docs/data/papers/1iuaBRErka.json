{"id": "1iuaBRErka", "number": 6991, "cdate": 1758004301822, "mdate": 1762963555131, "content": {"title": "CoKV: Optimizing LLM Inference with Game-Theoretic Adaptive KV Cache", "abstract": "Large language models (LLMs) have achieved remarkable success in various aspects of human life. However, one of the major challenges in deploying these models is the substantial memory consumption required to store key-value pairs (KV), which imposes significant resource demands. Recent research has focused on KV cache budget allocation, with several approaches proposing head-level budget distribution by evaluating the importance of individual attention heads. These methods, however, assess the importance of heads independently, overlooking their cooperative contributions within the model, which may result in a deviation from their true impact on model performance. In light of this limitation, we propose CoKV, a novel method that models the cooperation between heads in model inference as a cooperative game. By attributing the contribution of each head within the model, CoKV can more effectively allocate the cache budget in KV cache techniques such as eviction and quantization. Extensive experiments demonstrate the effectiveness of CoKV on long-context benchmarks (e.g., LongBench, NIAH, and RULER) and mathematical reasoning benchmarks (e.g., GSM8K and MATH) across multiple model families, including Qwen, Llama, and Mistral. Code is provided in \\url{https://anonymous.4open.science/r/CoKV-40AC}.", "tldr": "", "keywords": ["Game theory", "Model inference", "Large Language Models"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ddefc83c5649cafb17e18ff43d88717c121cf746.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CoKV, which models the cooperation among attention heads during inference as a cooperative game, allowing the analysis of each head's collaborative contribution to determine its importance. Based on this importance measure, the authors introduce a new cache eviction algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of analyzing the cooperative relationships among attention heads appears reasonable and well-motivated."}, "weaknesses": {"value": "1. The experimental setup, especially the training process of CoKV, is unclear. Specifically, the authors state:\n\"We randomly split each dataset into a very small validation dataset and a test dataset. The hyperparameter \\alpha is selected from {1, 5, 10, 15, 20, 30, 40} based on the Sliced Shapley value computed on the corresponding validation set.\" However, it is not clear where the training set is. My understanding is that CoKV follows a training-based framework: there should be a training set used to learn the head scores, a validation set to select \\alpha, and finally a test set to report the results. Did the authors train directly on the test set and then select \\alpha using the validation set?\n\n2. Following the previous point, it is also unclear how the authors report the LongBench scores. For example, in the Qasper dataset, there are about 200 samples. Were all 200 samples used to compute the final score?\n\n3. CoKV seems to require training and hyperparameter tuning for each dataset individually. Does this approach have practical value in real-world applications?\n\n4. The experimental design is somewhat confusing. The paper places significant emphasis on analyses based on masking important heads and comparing performance drops. In my view, such experiments serve more as analysis rather than direct evidence of the method's superiority. In contrast, the more detailed and valuable experiments-such as those on LongBench and Ruler-should be presented in the main text."}, "questions": {"value": "The paper introduces several hyperparameters (e.g., \\alpha, s, M). It would be helpful if the authors could provide a sensitivity analysis of these parameters and clarify the criteria for their selection in practical applications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UvYBX9PvjZ", "forum": "1iuaBRErka", "replyto": "1iuaBRErka", "signatures": ["ICLR.cc/2026/Conference/Submission6991/Reviewer_eVGh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6991/Reviewer_eVGh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760686042349, "cdate": 1760686042349, "tmdate": 1762919209368, "mdate": 1762919209368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "vVOGdUOZof", "forum": "1iuaBRErka", "replyto": "1iuaBRErka", "signatures": ["ICLR.cc/2026/Conference/Submission6991/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6991/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762963554306, "cdate": 1762963554306, "tmdate": 1762963554306, "mdate": 1762963554306, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CoKV, a game-theoretic adaptive KV-cache optimization framework for large language model (LLM) inference.\nThe key insight is that prior KV-budget allocation methods treat each attention head independently, ignoring cooperative or redundant interactions between heads. CoKV models these relationships as a cooperative game, computing each head’s contribution using an approximate Sliced Shapley Value (SSV) to determine cache allocation for both KV eviction and quantization.\n\nExperiments across Qwen-3-32B, Llama-3-8B-Instruct, and Mistral-7B on LongBench, RULER, GSM8K, and MATH show that CoKV achieves comparable or slightly better accuracy than HeadKV-R2 and Ada-KV while reducing peak memory by ≈ 38 % and inference latency to ≈ 25 % of the FullKV baseline.\nThe authors also report that CoKV can identify negative-contribution heads, improving interpretability and pruning safety."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel perspective:\nIntroduces a cooperative-game formulation of head importance—conceptually interesting and mathematically grounded.\n\n2. Theoretical rigor:\nDerives approximation bounds for the proposed SSV estimator, connecting sampling complexity with estimation variance.\n\n3. Versatile applicability:\nWorks for both KV eviction and quantization, with only a single offline head-importance estimation step.\n\n4. Empirical consistency:\nAcross diverse models and tasks, performance drop < 3 % while halving memory; decoding latency ≈ 25 % of FullKV.\n\n5. Interpretability:\nAbility to detect redundant or harmful heads offers a clear analytical advantage over heuristic-based approaches."}, "weaknesses": {"value": "1. Limited performance gain:\nAccuracy improvements are modest (≈ +1–2 points over HeadKV-R2 ); \n2. Offline cost:\nThe SSV estimation requires multiple inference passes on a validation set; although lightweight, it adds extra preprocessing.\n3. Evaluation scope:\nAll experiments are head-level; token- or layer-level extensions are not demonstrated.\n4. Ablation gaps:\nMissing analysis on sensitivity to coalition size H and validation-set size; unclear scalability to 70B-class models."}, "questions": {"value": "How does CoKV interact with FlashAttention 2 and PagedAttention caching mechanisms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ankFXdid1O", "forum": "1iuaBRErka", "replyto": "1iuaBRErka", "signatures": ["ICLR.cc/2026/Conference/Submission6991/Reviewer_qWq7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6991/Reviewer_qWq7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761475196425, "cdate": 1761475196425, "tmdate": 1762919208997, "mdate": 1762919208997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a cooperative game-theoretic approach for modeling interactions among attention heads in large language models. By quantifying each head's collaborative contribution, CoKV enables more accurate and efficient KV cache budget allocation, including eviction and quantization. Experiments on Longbench validate its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and easy to follow.\n\n2. The results on LongBench demonstrate that CoKV can more effectively identify the importance of attention heads."}, "weaknesses": {"value": "1. The CoKV method searches for hyper-parameters separately on each dataset but does not specify the selected values or provide any robustness analysis.\n\n2. The training process of CoKV introduces substantial computational overhead, especially since it requires grid search over different hyper-parameters.\n\n3. As shown in Figure 6, the training outcomes of CoKV vary significantly across different datasets, which may limit its practicality for real-world deployment.\n\n4. The experiments are limited to evaluations on LongBench and should include additional benchmarks such as RULER and mathematical reasoning datasets for a more comprehensive assessment."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lt1HNI4GdU", "forum": "1iuaBRErka", "replyto": "1iuaBRErka", "signatures": ["ICLR.cc/2026/Conference/Submission6991/Reviewer_61vm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6991/Reviewer_61vm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636789969, "cdate": 1761636789969, "tmdate": 1762919208442, "mdate": 1762919208442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose the adaptive method to allocate the budget for different attention heads by using the cooperative contributions within the model. Extensive experiments demonstrate the effectiveness of CoKV on long-context benchmarks (e.g., LongBench, NIAH, and RULER) and mathematical reasoning benchmarks (e.g., GSM8K and MATH) across multiple model families, including Qwen, Llama, and Mistral."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The adaptive budget allocation is useful by using the cooperative contributions within the model, the head importance is measured using the Sliced Shapley Value (SSV) with the complementary contributions. Then the KV Cache is compressed with adaptive budget allocation, Heads with higher SSV are allocated more cache size or bit width to retain KV pairs prior to the local window. \n\nThe experiments show good results: For Qwen3-32B, CoKV achieve 98.83% of the performance of the full KV when retains an average of 1024KV pairs(12.8%). Also Experiments on mathematical reasoning datasets also demonstrate that CoKV possesses strong cross-task capabilities."}, "weaknesses": {"value": "A lot of works, just like AdaKV, the adaptive budget allocation is very mature. Maybe the Attention recall based method is enough, so this should give the theoretical analysis of the comparison between the proposed method and Attention recall.\n\nAnd the experiments on long-cot tasks is not enough, since the performance of reasoning is very sensitive to token eviction."}, "questions": {"value": "1. What is the advantage of this method, compared with the method based Attention recall?\n2. In the future, the native sparse attention may be popular, how can your method adapt to these methods?\n3. How this method adapted to PageAttention, for system implementation?\n4. How can this method used with quantization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nLNe2R3oFp", "forum": "1iuaBRErka", "replyto": "1iuaBRErka", "signatures": ["ICLR.cc/2026/Conference/Submission6991/Reviewer_P5da"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6991/Reviewer_P5da"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922925324, "cdate": 1761922925324, "tmdate": 1762919208061, "mdate": 1762919208061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}