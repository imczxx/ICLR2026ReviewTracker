{"id": "L2kWjJ3a1W", "number": 12332, "cdate": 1758207112541, "mdate": 1759897516397, "content": {"title": "Decision Transformers As Zero-Shot Learners via Text-Behavior Alignment", "abstract": "Offline meta-reinforcement learning (meta-RL) aims to train agents that can generalize to unseen tasks using pre-collected data from related tasks. Recent approaches leverage the scalability of transformer architectures to model behavior sequences and support task adaptation using target task demonstrations. However, such data is often unavailable in real-world settings, where the task objective may be known but cannot be easily demonstrated. In contrast, humans routinely interpret and perform new tasks based solely on natural language instructions. In this work, we explore the potential of using natural language task descriptions to enable zero-shot task adaptation in offline meta-RL without requiring any data from the target task. We propose the Text-Guided Decision Transformer (TG-DT), a framework that enables zero-shot generalization by grounding policy learning in natural language. TG-DT learns a shared embedding space between task descriptions and behavioral trajectories via a dual contrastive and matching-based objective, ensuring robust alignment. A transformer-based policy is then conditioned on these aligned representations to generate task-appropriate actions. At test time, TG-DT synthesizes policies for unseen tasks using only their text descriptions and can optionally leverage a description-guided data sharing strategy to enhance adaptation. Experiments on standard offline meta-RL benchmarks, including MuJoCo and Meta-World, demonstrate that TG-DT achieves strong generalization to unseen tasks.", "tldr": "", "keywords": ["Offline Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/680f38f77fd3e011a70e0704d52d526a83ecf757.pdf", "supplementary_material": "/attachment/da4e71aee8e6d98795acbab1f588f8970668e6b0.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a way to train a decision transformer with textual descriptions. The problem setting is as follows: we are given trajectories with text labels, and we want to train a multi-task policy that can handle unseen task descriptions at test time, mainly in a zero-shot manner. To do this, the authors first encode text descriptions into a latent space using contrastive learning with behaviors. With the learned text embeddings, they train a decision transformer on the dataset trajectories. This method is called the text-guided decision transformer (TG-DT). The authors evaluate TG-DT on several HalfCheetah, Ant, and Meta-World tasks, comparing its performance with previous DT and language-conditioned RL methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The problem setting is important and the proposed method is reasonable.\n* The paper provides several ablation studies about each component of the method, which helps understand their relative importance."}, "weaknesses": {"value": "* I think the biggest weakness of this paper is its empirical evaluation. The authors only evaluate their method on \"classic\" tasks like HalfCheetah, Ant, and Meta-World. They are not the most \"natural\" language-based tasks either, and it is unclear how this method works on *actual* language-based benchmarks, such as CALVIN, LIBERO, or the one used in UniPi. Evaluating on classic tasks would be fine if the paper were more theory-oriented, but since this paper is purely empirically motivated and based on scalable transformer learning, I believe it is important to demonstrate its ability on tasks that the community currently finds more relevant.\n* Moreover, even on these \"classic\" tasks, their performance gain seems marginal (Tables 1 and 2). This makes the additional complexity introduced in this paper questionable. I suspect this might again stem from the simplicity of the evaluation tasks.\n* I'm not sure the term \"offline meta-RL\" is appropriate. To me, the problem setting considered in this paper is simply language-conditioned RL (and/or multi-task RL), with a focus on generalization toward unseen tasks. Meta-RL typically involves additional online rollouts or task-specific offline datasets for quick *adaptation*, while this paper mainly focuses on the zero-shot setting (I know the paper also has few-shot adaptation experiments, but they are based on naive fine-tuning with no specific meta-RL techniques, unlike typical meta-RL methods like MAML, PEARL, etc.). Are there similar previous offline RL works that refer to this setting as \"meta-RL\"? If not, I'd encourage the authors to revise the problem statement and terminology to prevent unnecessary misunderstanding.\n* (Relatively minor) The writing could generally be further improved. For example, the big picture of the method wasn't very clear at the beginning of Section 3, so TBC and TBM seem a bit ad hoc until one understands how the learned embeddings are actually used in Section 3.3. An algorithm box could help. Figure 1 is also hard to parse. From the figure, it seems the text and behavior datasets are separated (I think they're paired -- correct me if I'm wrong), and it is not immediately clear how a decision transformer plays a role in this method (from the figure, it appears that the dataset trajectory itself (in particular, without text embedding) is the input to the DT, which is not the case I suppose)."}, "questions": {"value": "I don't have additional questions beyond the ones raised in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RnugelyTAk", "forum": "L2kWjJ3a1W", "replyto": "L2kWjJ3a1W", "signatures": ["ICLR.cc/2026/Conference/Submission12332/Reviewer_ex7f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12332/Reviewer_ex7f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761285989173, "cdate": 1761285989173, "tmdate": 1762923256111, "mdate": 1762923256111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TG-DT, a framework that leverages textual task descriptions to enable zero-shot generalization to unseen tasks. The method builds upon Decision Transformer and introduces dual contrastive and matching-based objectives to jointly train a text encoder, behavior encoder, and text–behavior encoder–decoder. Experiments on Cheetah-dir, Cheetah-vel, Ant-dir, ML10, and ML45 show that TG-DT achieves improvements over baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and easy to follow.\n - The problem of generalizing to new tasks without additional demonstrations is relevant and important for data-efficient robot learning."}, "weaknesses": {"value": "- Although the problem setting is meaningful, the novelty appears limited. There has been substantial recent progress in aligning natural language with embodied behaviors (e.g., vision-language-action models such as Otter [https://arxiv.org/pdf/2503.03734] and related cross-modal alignment frameworks). The conceptual contribution of TG-DT, aligning text descriptions with behaviors, may therefore be seen as incremental unless its advantages are more concretely demonstrated.\n - The experimental results show only modest gains over the baselines (e.g., as shown in Figure 3). Given the simplicity of the evaluated locomotion and meta-RL benchmarks, it is unclear whether the approach would scale to more complex tasks. The significance of the improvement is therefore limited.\n - The paper does not sufficiently explain how textual task descriptions are constructed and shared across multiple tasks, particularly when the task goals differ conceptually. Additional clarification and concrete examples of task descriptions would be helpful. Details about how templates are applied and how text variation affects generalization are needed for reproducibility."}, "questions": {"value": "- The paper provides a task description template, but how is the same template applied across tasks with different objectives? Could you provide concrete examples of task descriptions used in each benchmark?\n - Do you foresee ways to improve the performance of TG-DT? If current benchmarks are saturated, have you considered evaluating on more complex or realistic task suites where text grounding may provide clearer advantages?\n - How sensitive is TG-DT to the richness or specificity of text descriptions? For example, does adding more descriptive language improve performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CgJPSXtf5L", "forum": "L2kWjJ3a1W", "replyto": "L2kWjJ3a1W", "signatures": ["ICLR.cc/2026/Conference/Submission12332/Reviewer_Y7e1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12332/Reviewer_Y7e1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994143528, "cdate": 1761994143528, "tmdate": 1762923255536, "mdate": 1762923255536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a decision transformer method guided by text to address the offline meta-reinforcement learning problem. The proposed Text-Guided Decision Transformer (TG-DT) aligns text and trajectory representations via a dual alignment mechanism, and then conditions a DT-style policy on the aligned text embedding to act on unseen tasks using only their textual descriptions. A description-guided data-sharing heuristic is optionally used to fine-tune the model with trajectories from semantically similar training tasks. Experiments on MuJoCo and Meta-World demonstrate the zero-shot and few-shot abilities of TG-DT compared with DT variant baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces an interesting idea of incorporating text descriptions into meta-RL tasks to guide the DT trajectory.\n- The proposed dual alignment mechanism between language descriptions and trajectory representations is novel.\n- Empirical results on MuJoCo and Meta-World support the effectiveness of the proposed method in zero-shot and few-shot settings."}, "weaknesses": {"value": "- The design of the proposed modules lacks detailed justification. The dual alignment mechanism and the description-guided data-sharing are not well motivated or clearly explained in the methodology section.\n- The experimental results show that the proposed method achieves only marginal improvements over the baselines while potentially incurring substantial computational overhead. Moreover, the paper does not provide any comparison of computational costs between the proposed method and the baselines.\n- For zero-shot settings, some baselines such as PDT and HDT are not designed for this setting, leading to unfair comparisons. For few-shot settings, although the proposed method fine-tunes the model on new tasks, it still shows unsatisfactory performance compared with the baselines.\n- The results lack the standard deviation, making it difficult to evaluate the true performance of the proposed method."}, "questions": {"value": "- Could you explain in more detail the design of the dual alignment mechanism and the description-guided data-sharing? What is the motivation for using these two modules?\n- Could you provide the computational cost of the proposed method compared with the baselines?\n- Could you elaborate on why the proposed method shows unsatisfactory performance in few-shot settings compared with the baselines even after fine-tuning?\n- Could you provide the standard deviation of the results to illustrate the performance variance?\n- In Tables 1 and 2, what do the “5 runs” refer to? Does it mean 5 different random seeds or 5 different trajectories?\n- During inference, the proposed method encodes text descriptions using a language model. How are these language descriptions obtained? Do they rely on prior information about the unseen tasks?\n- Could you discuss more related works such as [1] and [2], which also leverage language models for meta-RL tasks? What are the key differences between your method and these approaches?\n\n[1] *Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer*, 2024\n\n[2] *LLM-Driven Policy Diffusion: Enhancing Generalization in Offline Reinforcement Learning*, 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G342J3F8EA", "forum": "L2kWjJ3a1W", "replyto": "L2kWjJ3a1W", "signatures": ["ICLR.cc/2026/Conference/Submission12332/Reviewer_wPGH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12332/Reviewer_wPGH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113408914, "cdate": 1762113408914, "tmdate": 1762923254978, "mdate": 1762923254978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}