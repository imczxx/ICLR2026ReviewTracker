{"id": "RvCkgg7pdt", "number": 15065, "cdate": 1758247361956, "mdate": 1763639124907, "content": {"title": "Less is More: Towards Simple Graph Contrastive Learning", "abstract": "Graph Contrastive Learning (GCL) has shown strong promise for unsupervised graph representation learning, yet its effectiveness on heterophilic graphs, where connected nodes often belong to different classes, remains limited. Most existing methods rely on complex augmentation schemes, intricate encoders, or negative sampling, which raises the question of whether such complexity is truly necessary in this challenging setting. In this work, we revisit the foundations of supervised and unsupervised learning on graphs and uncover a simple yet effective principle for GCL: mitigating node feature noise by aggregating it with structural features derived from the graph topology. This observation suggests that the original node features and the graph structure naturally provide two complementary views for contrastive learning. Building on this insight, we propose an embarrassingly simple GCL model that uses a GCN encoder to capture structural features and an MLP encoder to isolate node feature noise. Our design requires neither data augmentation nor negative sampling, yet achieves state-of-the-art results on heterophilic benchmarks with minimal computational and memory overhead, while also offering advantages in homophilic graphs in terms of complexity, scalability, and robustness. We provide theoretical justification for our approach and validate its effectiveness through extensive experiments, including robustness evaluations against both black-box and white-box adversarial attacks.", "tldr": "", "keywords": ["Contrastive learning", "simple model", "feature noise", "structural noise"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e3fec8165cb3e3b69ca7eda8ccaa05374dfe3cb0.pdf", "supplementary_material": "/attachment/dcafe02022185e1e80f22513542b5af9bc605e40.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a dual-view contrastive learning framework for graph representation learning, where a GCN and an MLP respectively encode structural and feature views. The two embeddings are aligned via a simple cosine-similarity objective without requiring data augmentation or negative sampling. A learnable scalar adaptively balances the two views. Experiments on standard homophilic and heterophilic benchmarks show that the proposed model achieves competitive or superior performance with significantly lower complexity, highlighting that simplicity can rival or surpass more elaborate GCL designs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a remarkably minimal yet effective framework, questioning the necessity of complex data augmentations and negative sampling in graph contrastive learning.\n- It demonstrates consistent improvements over recent baselines, particularly under challenging non-homophilic settings, highlighting the method’s robustness and generality."}, "weaknesses": {"value": "- The framework essentially reuses standard contrastive alignment (cosine similarity between two encoders), which limits its novelty.\n- The analysis does not clearly disentangle the contributions of individual components such as the learnable view-weight parameter, encoder depth, or loss formulation, making it hard to attribute the observed gains.\n- Although the paper emphasizes simplicity and scalability, all experiments are conducted on small benchmark graphs (e.g., Cora, Texas). Evaluations on larger real-world non-homophilous datasets [1] would be necessary to substantiate claims of efficiency and scalability.\n\n[1] Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods, NeurIPS'21"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tOjh1FnLw7", "forum": "RvCkgg7pdt", "replyto": "RvCkgg7pdt", "signatures": ["ICLR.cc/2026/Conference/Submission15065/Reviewer_pexd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15065/Reviewer_pexd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760934099824, "cdate": 1760934099824, "tmdate": 1762925383575, "mdate": 1762925383575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an approach to graph contrastive learning, particularly for heterophilic graphs. Their proposed GCN-MLP model uses a standard GCN to capture the structural view and a simple MLP to capture the feature view. By contrasting these two views, which are argued to have less correlated noise sources, the model achieves strong performance without requiring any data augmentation or negative sampling, leading to significant gains in efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core insight of decoupling and contrasting \"feature noise\" and \"structural noise\" is both elegant and intuitive. It provides a principled argument against the need for complex, heuristic-based data augmentations, representing a significant departure from mainstream GCL research.\n\n2. The empirical results are compelling. The model achieves SOTA performance on several challenging heterophilic benchmarks while, as shown in Table 6, being orders of magnitude faster and more memory-efficient than its competitors.\n\n3. The paper's clarity is one of its strongest points. The proposed GCN-MLP model is straightforward to understand and implement, which greatly enhances its potential for adoption. The authors do an excellent job of justifying the simplicity of their design choices.\n\n4. The method shows superior robustness against adversarial attacks. This is a well-explained and significant benefit of the dual-view design, as the MLP-based view is inherently resilient to topological perturbations."}, "weaknesses": {"value": "The main limitation of the paper, which the authors acknowledge, is its performance on certain homophilic graphs (e.g., Cora). On these graphs, where feature and structure information are highly correlated, the benefit of separating them is less pronounced, and the model can be slightly outperformed by specialized methods. While this is a reasonable trade-off, it might be worth briefly discussing whether a simple mechanism could be introduced to adaptively weigh the views based on the graph's homophily level."}, "questions": {"value": "1.Have the authors experimented with other GNN backbones for the structural encoder? Is there a specific reason why the smoothing property of GCN and the feature-transforming nature of MLP create such an effective pairing for noise decoupling?\n\n2.The final representation is a weighted average of the two views, controlled by a hyperparameter β. How sensitive is the model's performance to the choice of β? Figure 1 shows impressive results, but it would be helpful to understand the tuning effort required to achieve them.\n\n3.The framework is presented for unsupervised learning. Have the authors considered its application in a semi-supervised setting? For example, could this contrastive objective serve as a powerful pre-training or regularization task to further boost performance when a few labels are available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GA5mMuZX4I", "forum": "RvCkgg7pdt", "replyto": "RvCkgg7pdt", "signatures": ["ICLR.cc/2026/Conference/Submission15065/Reviewer_fKkq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15065/Reviewer_fKkq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761404569224, "cdate": 1761404569224, "tmdate": 1762925383035, "mdate": 1762925383035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a GCL method that aligns GCN with node-attribute based MLP and integrates the features of the two channels (i.e., GCN encoder and MLP encoder)  as the final node representation. The performance of the proposed method  is evaluated on node classification task for homophilious/heterophilic graphs, and the robustness of the model against intentional  and random structural perturbations is assessed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written, easy to follow and  technically correct. The experiments can be reproducible."}, "weaknesses": {"value": "*  high similar works SimMLP (arxiv:2412.03864) and DGCL (Briefs in bioinformatics, 2024) have been published a year ago, which also align GNN-structured view with node-attribute invariant MLPs.  \n* Some claims such as \"uncover a simple yet effective principle for GCL:...\" have been the common ground in GNN communities for years, e.g., earlier study of heterophilic graph learning by Zhu et al. (AAAI'20) reveals the advantages of MLPs on heterophilic graphs.\n* Common tasks including link prediction/ graph classification / node clustering are missing, which can be more sensitive to structural information."}, "questions": {"value": "* The robustness of the proposed GCL model is conducted on attacking structure. What if node attributes  are adversarially attacked, e.g. using metattack?\n* Besides the previous works mentioned in \"Weakness\", GECL(Xiao et al, 2024) also adopts the similar framework: Aligning MLPs encoder with GNNs. How your model distinguish from the previous ones needs to be clarified.\n* What is the role of weighted sum of two channels? Do you use the similar feature fusion approach for the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eLHx7vF7n2", "forum": "RvCkgg7pdt", "replyto": "RvCkgg7pdt", "signatures": ["ICLR.cc/2026/Conference/Submission15065/Reviewer_UouA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15065/Reviewer_UouA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570397842, "cdate": 1761570397842, "tmdate": 1762925382499, "mdate": 1762925382499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on heterophilic graph contrastive learning. Motivated by the increasing complexity of existing methods, the authors propose a simple yet effective model. Leveraging the natural low correlation between node features and graph structure in heterophilic graphs, the model uses GCN and MLP to create two complementary views for contrastive learning. The GCN-MLP model achieves competitive representation performance without any augmentation and at low cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "i. This model adopts a straightforward design, leveraging the characteristics of heterophilic graphs. From a practical standpoint, the motivation is clear and reasonable. The effort to solve the problem with a simpler model is appealing.\nii. The proposed GCN-MLP model reduces computational costs while maintaining competitive representation performance. The experimental results clearly support this advantage."}, "weaknesses": {"value": "i. The relationship between the two main components, GCN and MLP, should be described more clearly. It is important to emphasize that the proposed method is not simply a combination of existing architectures.\nii. Some statements lack rigor:\na) On p.2, the phrase “akin to a law-of-large-numbers effect” is not rigorous and should be rephrased in a more formal manner.\nb) All symbols should be clearly defined before being used. On p.3, the notation R^F introduces F without explanation. \niii. Parameters described as empirical require further justification. For example, L. If the proposed model is not only a simple combination of MLP and GCN, the reuse of this empirical setting (L=1) should be justified. Additionally, experiments should be conducted to demonstrate the impact of this parameter on the performance.\niv. Minor presentation issue: Sentences such as “This completes our brief review of GCL.” should be removed, as they do not provide any substantive information.\nv. In the theoretical analysis (Observation 1), the term “average correlation” is repeatedly used. However, it lacks a formal definition or explanation in the main text. Although it is briefly mentioned in the appendix (“if we use E^k as a measure of the average correlation...”), a formal definition would help clarify its meaning ."}, "questions": {"value": "i. In the experiments, the proposed model shows competitive representation performance without relying on data augmentation or negative sampling. Could the authors provide experimental analysis to verify whether using these techniques would further improve performance, or if the model already reaches its optimal capacity without these techniques?\nii. Could the authors consider including an additional experiment to evaluate the effectiveness of the GCN-MLP model on downstream tasks beyond node classification, to further demonstrate its performance and broader applicability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dWBn790a1G", "forum": "RvCkgg7pdt", "replyto": "RvCkgg7pdt", "signatures": ["ICLR.cc/2026/Conference/Submission15065/Reviewer_EBbx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15065/Reviewer_EBbx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956709118, "cdate": 1761956709118, "tmdate": 1762925382139, "mdate": 1762925382139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}