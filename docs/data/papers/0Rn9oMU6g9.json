{"id": "0Rn9oMU6g9", "number": 12866, "cdate": 1758211033633, "mdate": 1759897480664, "content": {"title": "Subspace-Boosted Model Merging", "abstract": "Model merging enables the combination of multiple specialized expert models into a single model capable of performing multiple tasks. However, the benefits of merging an increasing amount of specialized experts generally lead to diminishing returns and reduced overall performance gains. In this work, we offer an explanation and analysis from a task arithmetic perspective; revealing that as the merging process (across numerous proposed merging heuristics) continues for more and more experts, the associated task vector space experiences rank collapse. By intervening on this task vector rank collapse through our newly introduced Subspace Boosting, which operates on the singular value decomposed task vector space, we maintain task vector ranks to raise merging efficacy on up to 20 expert models by large margins of more than 10%. Moreover, we showcase how a Higher-Order Generalized Singular Value Decomposition can be leveraged to further quantify task similarity, offering a new interpretable perspective on model merging.", "tldr": "", "keywords": ["model merging"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5c4f6c70fa19ffda5d474c47667fda886f4bbf95.pdf", "supplementary_material": "/attachment/90bae80007d8a37d35e572a7e1766b7073db3595.zip"}, "replies": [{"content": {"summary": {"value": "The work tackles task-vector-based model merging, where the differences between task-specific finetunings and their base models (task vectors) are aggregated to obtain one single multi-task model that can perform all the tasks at once. In particolar, the paper investigates the problem of rank collapse when merging many tasks: the more tasks are added, the higher the singular values for the most important directions, contracting the spaces into a few dominant directions. The work proposes clamping the singular values to avoid some of the singular directions overshadowing the others. It also proposes higher-order SVD to obtain a space that is shared across the tasks, making different task-specific weights more comparable in the latter. The method is tested on ViT-B/32, ViT-B/16 and ViT-L/14 on the common 8-, 14, and 20-tasks benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is interesting and intuitive. Current SVD-based methods either do not consider any common subspace (TSV [1]) or obtain one just by summing the task vectors before the SVD takes place (Iso-C [2]). Producing this one through Higher-Order SVD seems principled and effective. The method is applicable to any existing task-arithmetic variant, including e.g. TIES and Consensus Merging. It is also more efficient than comparable SVD-based methods, which is by itself not a big thing in standard model merging but could be useful in applications requiring iterative merging (e.g. federated learning). Being data- and tuning-free, the method is broadly applicable and in line with the considered literature.\n- The paper is well written and easy to read, with clear figures that are immediate to grasp. The formalization is intuitive and the paper structure is well-thought.\n- The results are competitive with the state-of-the-art, and the experimental evidence is extensive and considers all the relevant baselines, model architectures and benchmarks."}, "weaknesses": {"value": "- The evidence for the main motivation, i.e. the rank collapse, is somewhat limited: in layer 10, one of the most significant, the trend is actually the opposite: N14 and N20 have higher stable rank than N8. This suggests that it has more to do with the particular composition of the merging set rather than the number of tasks alone. For a similar analysis to be performed, one should try to average over subsets of increasing cardinality, possibly many of them so to rule out the difference in composition.\n- The novelty is somewhat limited when compared to Iso-C [1]. The main contribution seems to be solving the rank collapse phenomenon, but this seems to be already tackled by Iso-C/Iso-CTS [1], although in a simpler manner. Simplicity however has its benefits, as [1] does not require a beta hyperparameter.\n- From the performance standpoint, the approach only outperforms the current state-of-the-art in the ViT-B-32 setting, while remaining below Iso-CTS for larger architectures.\n- The alignment method is not well motivated and discussed. I don’t fully understand what’s the intuition of e.g. MNIST and EuroSAT being most aligned to SUN397. How does this alignment measure correlate with the interference measures proposed in TSV and Iso-C? The subspace alignment studied in Iso-C would be particularly interesting as from my understanding it also measures similarity in the singular vector space, although differently.\n- The whole “interpretable merging” point seems fairly oversold. It is not immediately clear to me what added interpretability stems from the proposed alignment matrices. I might have missed the point, and I would be happy to be convinced otherwise on this aspect.\n\nGiven the current strengths and weaknesses, I am inclined to reject the paper: from the performance standpoint, it does not distance itself by comparable baselines that are also similarly motivated, and the remaining benefits (e.g., interpretability) are not properly explored and discussed.   \n\n[1 Marczak, Daniel, et al. \"No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces.\" ICML 2025.\n\n[2] Gargiulo, Antonio Andrea, et al. \"Task singular vectors: Reducing task interference in model merging.\" *Proceedings of the Computer Vision and Pattern Recognition Conference*. 2025."}, "questions": {"value": "- What is the proper way to look at the alignment matrix? what are its immediate implications? how does it correlate with other existing measures? (see weakness 4).\n- Figure 3 does not specify the architecture, the task nor the layer.\n- Why is A used in place of Delta (common choice in previous literature?) This seems like a peculiar choice, given that the alignment matrix is termed **A**.\n- I don’t fully understand the I_{>1} notation. I get that it refers to the common components, but why is it expressed in this way?\n- M is also somewhat confusing as might lead to think of a matrix instead of a scalar. Also some papers use it for the merged model.\n- What sort of interpretability can we derive from the alignment matrices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uv5ArrVFZv", "forum": "0Rn9oMU6g9", "replyto": "0Rn9oMU6g9", "signatures": ["ICLR.cc/2026/Conference/Submission12866/Reviewer_RgdQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12866/Reviewer_RgdQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761384417378, "cdate": 1761384417378, "tmdate": 1762923658420, "mdate": 1762923658420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies \"rank collapse\" in the task vector space as a fundamental limitation in model merging, which explains why performance gains diminish as more expert models are combined. To solve this, the authors propose Subspace Boosting, a training-free method that uses Singular Value Decomposition (SVD) to decompose task vectors and explicitly enhance underutilized dimensions, thereby maintaining the effective rank and significantly improving merging efficacy by over 10% on vision and language tasks. Additionally, the paper introduces the use of Higher-Order Generalized SVD (HO-GSVD) as a novel framework to quantify task similarity, offering a new interpretable perspective on model merging and enabling principled expert selection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Unlike prior works that only observed diminishing performance with more merged experts, this study provides a mechanistic explanation from a task vector space perspective: as more experts are merged, task vectors suffer from rank collapse.\n\n- The proposed Subspace Boosting addresses rank collapse in a highly practical manner. Operating via singular value decomposition (SVD) on merged task vectors, it boosts underutilized small singular values to maintain effective rank."}, "weaknesses": {"value": "- The third part quantifies Rank collapse only relying on \"Stable Rank\" and \"Cumulative Energy Rank\" (for example, Formula 2 defines stable rank as the ratio of the sum of squares of singular values to the square of the maximum singular value). However, the universality of these two indicators for the \"correlation degree of model fusion performance\" has not been fully demonstrated. The manuscript only demonstrates the negative correlation between the stable rank and performance through experiments of the ViT-B/16 model (Figures 2 and 3), but does not verify: \n     - a . In different model architectures (such as the language model T5), whether the stable rank can still effectively reflect the impact of rank collapse on performance - for instance, the dimension of the weight matrix and the layer structure of T5 are significantly different from those of ViT, the distribution rules of singular values may be different, and the \"effective rank\" representation ability of the stable rank may fail; \n     - b. In extreme scenarios (such as fusing two models or fusing models with highly similar tasks), will there be a counterexample of \"low rank but high performance\" in the stable rank? If so, it indicates that this metric cannot be used alone as a criterion for determining rank collapse.\n\n- Subspace enhancement relies on the hyperparameter β (lift threshold) to determine the singular value cutoff point to be enhanced. However, this paper only found through experiments that the performance is stable when β∈{0,0.01,0.02} (Table 3a), without providing a theoretical explanation when the task type changes, does the optimal value range of β remain stable? If β needs to be re-tuned according to the scene, the practicality of the method will significantly decline, but the documentation has not verified this boundary condition.\n\n- The experiment only selects \"same mode, same type\" tasks.  All visual tasks are classification tasks and do not include non-classification tasks such as detection and segmentation. All language tasks are QA and NLP classification tasks (such as sentiment analysis), and do not include generation or translation tasks. The \"cross-type task fusion\" scenario - such as fusing classification and detection tasks - has not been verified to see if subspace enhancement can still improve performance. If the task vector conflicts of cross-type tasks are more significant, the method's effectiveness may drop significantly, and the generalization of existing conclusions is limited."}, "questions": {"value": "- Whether Subspace Boosting can simultaneously improve performance on both visual and language sub-tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V5Y3fkxgoq", "forum": "0Rn9oMU6g9", "replyto": "0Rn9oMU6g9", "signatures": ["ICLR.cc/2026/Conference/Submission12866/Reviewer_Eqcf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12866/Reviewer_Eqcf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927056417, "cdate": 1761927056417, "tmdate": 1762923658007, "mdate": 1762923658007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reveals the rank collapse limitation in existing model merging methods. To address this issue, the authors propose a technique called subspace boosting, whose core idea is to boost the singular values below a certain cutoff threshold."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work identifies a critical limitation in existing model merging approaches, rank collapse, and provides empirical evidence to support this finding.\n\n2. The paper is well-organized and easy to follow."}, "weaknesses": {"value": "1. **Potential error amplification.** Directly boosting the singular values below the cutoff point may introduce noise or bias. The authors should provide further discussion to justify the rationality of the proposed subspace boosting technique.\n\n2. **Hyperparameter sensitivity.** The method requires manual tuning of the cutoff hyperparameter, which may limit its practicality and robustness in real-world applications.\n\n3. **Unclear connection between HO-GSVD and rank collapse.** While HO-GSVD offers a new and interpretable perspective on model merging, it is unclear how it directly relates to the core motivation of preventing rank collapse. This conceptual gap may cause readers to lose focus on the main contribution."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wGJhrFA41O", "forum": "0Rn9oMU6g9", "replyto": "0Rn9oMU6g9", "signatures": ["ICLR.cc/2026/Conference/Submission12866/Reviewer_NH2N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12866/Reviewer_NH2N"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935175986, "cdate": 1761935175986, "tmdate": 1762923656997, "mdate": 1762923656997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Subspace Boosting, a method developed to improve the effectiveness of model merging. The authors analyze the degradation in performance that occurs as the number of merged models increases, attributing it to a reduction in the effective dimensionality of the task-vector space, where variance becomes concentrated in a few dominant directions.\nSubspace Boosting is implemented on top of the Higher-Order Generalized Singular Value Decomposition (HO-GSVD) framework. Additionally, the authors introduce the Alignment Matrix, which measures relationships between task vectors within the shared subspace.\nExperiments on Vision Transformer (ViT-B/32, ViT-B/16, and ViT-L/14) models trained on 8, 14, and 20 tasks compare Subspace Boosting against the ISO-C and TSV baselines and show the performance obtained applying this method to some model-merging baselines ( TIES-Merging, Task Arithmetic, Consensus Merging)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The topic is relevant to the community, addressing a problem in multi-task and model-merging research.\n- The proposed Subspace Boosting method is conceptually clear and appears computationally efficient.\n- The connection between singular value structure and model merging dynamics is insightful. The analysis of shared versus task-specific subspaces through the singular-value structure is particularly interesting."}, "weaknesses": {"value": "- Some inclarities about the tables and exepriment report (see questions). \n- The notion of “rank collapse” that is central in the paper could benefit from a more formal explanation.\n- Algorithm 1 applies a standard SVD step. I think it is misleading to put it in the approach instead of the algorithm of the Subspace Boosting. \n\nMinor\n- In Figure 2 (a–c), the y-axis label “Value” should likely be “Stable rank value”, right?\n- In line 268, “n” is associated with the shape of V and aslo to the  number of merged tasks, is this notation correct?"}, "questions": {"value": "1. Table 1 vs. Table 4: The results for your method differ between Tables 1 and 4. Could you clarify why this is the case?\n2. In table 4 Subspace Boosting 4 uses LiNeS while other baselines do not, why, is the comparison fair?\n3. Table 10,  Random selection: In Table 10, you average over 20 random selections when merging 8 out of 20 models. Why not also report the standard deviation to show the variance of random selection?\n4.  In Figure 3, the largest singular values seem to scale roughly linearly with the number of merged experts (e.g., almost 0.05 for 4 experts,  close to 0.10 for 8, and almost 0.20 for 20).\nThis appears consistent with what one would expect if the overall Frobenius norm of the merged matrix increases linearly with the number of merged task vectors, meaning the curves could differ mainly by a global scaling factor rather than by a change in shape.\nDid you normalize the merged weight matrices (e.g., by dividing by the Frobenius norm or the largest singular value) before plotting the singular-value distributions in Fig. 3?\nIf not, could the apparent steepening of the spectra be explained by such scaling rather than by true “rank collapse”?\n5. $\\beta$ is tuned over the set {0, 0.01, 0.02}. How this range was chosen, and whether a broader search might affect results?\n6. You mention that Subspace Boosting is faster than competing methods. Could you explain which component (e.g., decomposition, projection step, or optimization) contributes most to this improvement?\n7. Have you considered including the Singular Task Interference metric (Gagiurlo et al.) as an additional as a way to interpret task similarity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yPMVm1yEcz", "forum": "0Rn9oMU6g9", "replyto": "0Rn9oMU6g9", "signatures": ["ICLR.cc/2026/Conference/Submission12866/Reviewer_AddN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12866/Reviewer_AddN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942193189, "cdate": 1761942193189, "tmdate": 1762923656533, "mdate": 1762923656533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}