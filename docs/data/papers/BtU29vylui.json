{"id": "BtU29vylui", "number": 8662, "cdate": 1758093985675, "mdate": 1759897771087, "content": {"title": "MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use", "abstract": "Recent advances in Agentic Intelligence have highlighted the importance of agentic tool use in Large Language Models (LLMs), especially when interacting with users. During multi-turn interactions, the dynamic, uncertain, and stochastic nature of user demands challenges agents to iteratively refine their understanding of user needs through communication while invoking tools to resolve queries, rather than simply calling tools for results. Existing reinforcement learning (RL) approaches for tool use lack the integration of genuinely dynamic users during the RL training process. To bridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use), a novel reinforcement learning framework that, for the first time in the field of agentic tool use, integrates LLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable autonomous learning of models to communicate with users efficiently and use various tools to solve practical problems in dynamic multi-turn interactions. Evaluations on several benchmarks demonstrate that MUA-RL-32B outperforms or matches much larger open-source models such as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking setting (see Figure 1).", "tldr": "", "keywords": ["Large Language Models", "Agentic Tool Use", "Reinforcement Learing", "Multi-turn User-interacting"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d1723b19ab232132a2939650d8148442daac9d7.pdf", "supplementary_material": "/attachment/cb45d6add6ff7c89e6103af5d62d48d38239de0f.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces MUA-RL, a reinforcement learning framework designed for multi-turn, user-interactive agentic tool use. The authors conduct experiments demonstrating that, after reinforcement learning training, MUA-RL-32B achieves improved performance across several benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The focus on multi-turn and user-interactive settings addresses essential aspects for developing more autonomous agents, introducing greater complexity to the learning process.\n\n2. The paper includes evaluations on multiple benchmarks, such as $\\tau^2$-Bench, BFCL-V3 Multi-Turn, and ACEBench."}, "weaknesses": {"value": "1. Although the paper claims that MUA-RL represents a novel reinforcement learning framework, its novelty is not convincingly demonstrated. The task formulation, reward design, and training methodology appear relatively conventional, and the distinctions from existing approaches (such as [1,2,3]) are not clearly presented. \n\n2. The $\\tau$-bench  are used as both the training and test dataset.\n\n3. The paper introduces two agentic data synthesis pipelines, but their effectiveness is not validated through ablation studies, leaving uncertainty about their contributions.\n\n[1] WebGPT: Browser-assisted question-answering with human feedback.\n\n[2] AGILE: A Novel Reinforcement Learning Framework of LLM Agents.\n\n[3] Multi-turn Reinforcement Learning from Preference Human Feedback."}, "questions": {"value": "1. If it is possible to obtain real trajectories using the MCP server, what is the motivation for generating LLM-simulated tool execution? What advantages do synthetic trajectories offer compared to real trajectories obtained from the MCP server?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wrtMiazyVO", "forum": "BtU29vylui", "replyto": "BtU29vylui", "signatures": ["ICLR.cc/2026/Conference/Submission8662/Reviewer_DZFv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8662/Reviewer_DZFv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793426944, "cdate": 1761793426944, "tmdate": 1762920479594, "mdate": 1762920479594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an on-policy reinforcement learning loop that incorporates tool-use and interaction. Using their method named MUA-RL, the authors train a 32B model to outperform or match Deepseek and a large Qwen model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Integration with MCP tool server with reinforcement learning and user interaction \n2. Creating an RL loop on three interesting interactive tasks with a user: Tao bench, Berkeley Function-Calling Leaderboard, ACEBench Agent\n3. Performs an ablation comparing non-thinking and cold start \n4. The paper ablates different user LLMs that have a different LLM backend"}, "weaknesses": {"value": "1. The paper states that it introduces “a novel multi-turn user-interacting reinforcement learning framework that incorporates LLM-simulated users into the reinforcement learning rollouts.” (line 74-75) However, similar ideas have been explored in prior work. For example:\n   a. LMRL-Gym [1]: Presents a framework for developing and evaluating human simulators, and integrates them into the LLM and RL   training loop\n   b. Offline RL with Simulated Users [2,3]: Previous studies have incorporated user simulators into offline RL settings, including for interactive coding tasks\n   c. SOTOPIA-RL [4]: Demonstrates related approaches for simulating human interactions in reinforcement learning contexts.\n2. Concern about long term impact and framing: the paper frames as the primary contribution as placing tool use and user simulation in the same RL loop. I don’t see this as fundamentally different from those that were studied in the above papers and as such don’t see why this work should \n\n[1] Abdulhai, Marwa, et al. \"Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models.\" arXiv preprint arXiv:2311.18232 (2023).\n[2] Hong, Joey, Sergey Levine, and Anca Dragan. \"Zero-shot goal-directed dialogue via rl on imagined conversations.\" arXiv preprint arXiv:2311.05584 (2023).\n[3] Zhou, Yifei, et al. \"Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks.\" arXiv preprint arXiv:2503.15478 (2025).\n[4] Yu, Haofei, et al. \"Sotopia-RL: Reward Design for Social Intelligence.\" arXiv preprint arXiv:2508.03905 (2025)."}, "questions": {"value": "- What is the significant novelty of including tool use in addition to a user centric approach?\n- When doing the synthetic data generation pipeline for creating a dataset for further fine-tuning before RL fine-tuning, do the authors observe a difference between using only the LLM generated tool call responses versus the ones where the response comes from a genuine MCP server?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hBvooDtgRu", "forum": "BtU29vylui", "replyto": "BtU29vylui", "signatures": ["ICLR.cc/2026/Conference/Submission8662/Reviewer_8aYg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8662/Reviewer_8aYg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952705033, "cdate": 1761952705033, "tmdate": 1762920479015, "mdate": 1762920479015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MUA-RL, a reinforcement learning framework that incorporates LLM-simulated users into RL rollouts for multi-turn agentic tool use. For cold-start SFT training, the authors construct two data synthesis pipelines, i.e., using LLM-simulated and real MCP server tool responses. For the RL process, the authors conduct a detailed analysis of training dynamics. MUA-RL-32B matches or outperforms much larger models like DeepSeek-V3-0324 and Qwen3-235B-A22B across multiple benchmarks, demonstrating strong performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work proposes a framework to integrate LLM-simulated users into RL rollouts for agentic tool use, addressing the critical gap of dynamic user interactions in existing RL methods.\n2. MUA-RL-32B matches or outperforms much larger models (DeepSeek-V3-0324, Qwen3-235B-A22B) across multiple benchmarks, demonstrating remarkable efficiency gains.\n3. The paper provides a detailed analysis of the training dynamics. This analysis is insightful and better demonstrates the MUA-RL process."}, "weaknesses": {"value": "1. This work only uses retail and airline datasets from TAU1-Bench for RL training, which is in a similar distribution as the testing environment, potentially limiting generalization to other domains.\n2. This proposed method relies on GPT-4o as the user simulator during training. This is costly and may not be scalable."}, "questions": {"value": "1. How does the framework prevent LLM-simulated users from generating unreasonable behaviors during training? What quality control mechanisms ensure the simulated users provide realistic interactions that won't lead to overfitting on artificial patterns?\n2. Why was the non-thinking version of the model adopted instead of the thinking version?\n3. What is the cost of LLM invocation when generating the cold-start data and in the RL process for simulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wcQZpyyLqU", "forum": "BtU29vylui", "replyto": "BtU29vylui", "signatures": ["ICLR.cc/2026/Conference/Submission8662/Reviewer_jMed"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8662/Reviewer_jMed"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998661781, "cdate": 1761998661781, "tmdate": 1762920478443, "mdate": 1762920478443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MUA-RL, a reinforcement learning (RL) framework that enables large language models (LLMs) to learn agentic tool use through multi-turn user interactions. The authors argue that existing reinforcement learning (RL) approaches for tool use lack the integration of genuinely dynamic users during the RL training process, and that this limits an agent’s ability to adapt to real-world conversations. Hence, the paper integrates LLM-simulated users into the reinforcement learning loop, enabling the agent to iteratively refine its understanding of user intent across multiple conversational turns. The method consists of a cold-start SFT phase, using two agentic data synthesis pipelines for high-quality cold-start: one with LLM-simulated tool responses, and another with real MCP server tool responses.\n\nThe authors report that MUA-RL-32B outperforms or matches much larger open-source models such as DeepSeek-V3-0324 and Qwen3-235B-A22B, and even rivals GPT-4o and GPT-4.1 on multiple tool-use benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation is very compelling. Specifically, the authors note that users often adjust their questions and expectations based on the model’s responses\n- The paper moves beyond static datasets or scripted dialogues and frames this problem of agent and user co-evolution \n- The method MUA-RL combines synthetic cold-start data generation with interactive RL training, producing a more realistic environment for the agent.\n- The evaluation spans four major multi-turn benchmarks—TAU1-Bench, TAU2-Bench, BFCL-V3 Multi Turn, and ACEBench Agent\n- The paper provides analysis of training dynamics"}, "weaknesses": {"value": "- The paper uses simulated users as a proxy for human behavior, which is reasonable, but lacks empirical validation with actual human users. This could cause overfitting to synthetic conversational patterns. I suggest the authors do a user study with real human users to benchmark their method, as the title of the paper is also specifically addressing users. There needs to be validation of the LLM-simulated users as well. \n- There is no discussion of the safety risks of the method and how it affects real users"}, "questions": {"value": "- The paper discusses using approximately 1600 trajectories across 9 scenarios for the cold-start training. More details about domain coverage and diversity would strengthen reproducibility claims.\n- Integrating large user simulators like GPT-4o-2024-11-20 into the RL loop can be quite expensive. The authors do not provide estimates of training compute or efficiency.\n- The reward is defined as binary. Did the authors experiment with other reward structures?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "There needs to be a discussion of safety concerns regarding the method and its effect on users"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OjToP5gcVC", "forum": "BtU29vylui", "replyto": "BtU29vylui", "signatures": ["ICLR.cc/2026/Conference/Submission8662/Reviewer_A4Zo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8662/Reviewer_A4Zo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762900399830, "cdate": 1762900399830, "tmdate": 1762920478134, "mdate": 1762920478134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}