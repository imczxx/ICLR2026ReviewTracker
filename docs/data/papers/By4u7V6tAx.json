{"id": "By4u7V6tAx", "number": 16327, "cdate": 1758263242756, "mdate": 1759897247369, "content": {"title": "Closing the Train-Test Gap in World Models for Gradient-Based Planning", "abstract": "World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of tasks chosen at inference time. Compared to traditional MPC procedures, which rely either on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose an improved method for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. Moreover, we demonstrate an improvement over the search-based CEM method on an object manipulation task in 10\\% of the time budget.", "tldr": "We identify a train-test gap when using gradient-based planning with world models and propose techniques to close it.", "keywords": ["world models", "model predictive control", "gradient-based planning", "meta-learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a863e3c5b594316cf24409ed320d26a5c602a5f4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework to improve gradient-based planning with learned world models by addressing the train-test gap. While existing world models achieve accurate next-state prediction, they often fail during long-horizon planning due to covariate shift and non-smooth loss landscapes, causing instability. The proposed solution introduces two training regimes built on a pretrained latent world model. Online World Modeling (OWM) mitigates distributional drift by iteratively collecting trajectories generated by the planner, executing them, and adding the resulting transitions into the training data, similar to DAgger. Adversarial World Modeling (AWM) improves local smoothness by training the model under perturbed latent states and actions, encouraging robustness to small input deviations. Using this refined world model, gradient-based planning directly optimizes action sequences in latent space toward a goal embedding. Experiments on three tasks demonstrate that both OWM and AWM reduce the simulator-model rollout discrepancy, enabling GBP to achieve higher success rates and faster convergence than standard teacher-forced training, and getting results similar to CEM while being significantly faster. However, the proposed regimes are conceptually simple and not novel, and are evaluated only on DINO-WM, so it is unclear whether the improvements generalize to other world-model architectures or tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Better results compared to baselines\n\n- Two simple methods for dataset aggregation"}, "weaknesses": {"value": "- Novelty is very limited; the paper simply applies existing concepts (DAgger and adversarial training) into world model training and makes some further trivial changes (e.g., SGD to Adam optimizer)\n\n- The method Adversarial World Modeling is not well-explained. In Algorithm 3, there are points that need to be clarified further. First, the trajectories that are added to the trajectory dataset are the original trajectories, not the ones that are updated. Even if the model is trained with the new trajectory (tao prime), the targets that we use $z_{t+1}'$ are also perturbed inputs/actions. The adversarial training should be done such that $f_\\theta(a_t’, z_t’)$ is similar to $z_{t+1}$. In this version, it is $z_{t+1}’$. I believe training with a non-perturbed target is the right choice.  \n\n- Nearly half of the performance improvement seems to come from switching to Adam in planning, rather than the dataset augmentation methods that are the main contributions of this paper\n\n- The details of the experimentation are missing: The way the initialization network is trained, details of the baseline CEM implementation, under which setup the data is collected for the graphs of Figure 3, what is meant by training $f_\\theta$ on algorithm 2, 3 (do we take a batch or do 1 epoch), and architectural details of the networks. Some hyperparameter values (batch size, horizon length etc.) and hardware used are also not mentioned in the paper. Paragraph 4.3 is unclear, there should be more data about the used model\n\n- Notational inconveniences:\n  - (Mentioned above) Not generating and adding a new trajectory to the dataset in Algorithm 3.\n  - There is no $p_\\theta$ (line 257, 315) defined. I suppose it is $f_\\theta$\n\n- The only world model used is DINO-WM; it is unclear whether the proposed training regime would improve performance that much in other world models. In addition, the only non-gradient-based planner is CEM; MPPI or other planners could also be tested.\n\n- It would also be a plus for the paper, if they have included more qualitative analysis for justifying the practical concept, including some scenes from their environments or having failure cases alongside successful trajectories.\n\n- The number of tasks used in experimentation is three. It would be better to test the proposed methods on diverse tasks from potentially different frameworks/environments."}, "questions": {"value": "- Elaborate what exactly is being compared in Figure 3. Which optimizer is used for each curve? Further discussion of these results is also needed; e.g. why does teacher forcing start with a significantly higher model loss difference?\n\n- Instead of mentioning some hyperparameter values in the text, please include a single table with all hyperparameters that are used in your setup.\n\n- In Algorithm 3, because of the way lambda parameters are chosen, don't we have clipping on most of the iterations? When this happens, does applying a perturbation in the direction of the gradient mean anything? Did you try not doing a gradient step and observe the results with just clipping applied? I suspect the results won't differ much.\n\n- Some qualitative analysis (e.g. with example expert and generated trajectories) would strengthen your claims"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NzxBPmQWgJ", "forum": "By4u7V6tAx", "replyto": "By4u7V6tAx", "signatures": ["ICLR.cc/2026/Conference/Submission16327/Reviewer_yvUu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16327/Reviewer_yvUu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913973345, "cdate": 1761913973345, "tmdate": 1762926464061, "mdate": 1762926464061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the train–test gap between world-model training and test-time planning. World models are usually trained for one-step next-state prediction, but during gradient-based planning (GBP) they are unrolled for many steps, causing compounding errors and off-distribution states.\nThe authors propose two methods to mitigate this: \n1. Online World Modeling (OWM), a DAgger-style approach: after planning, the predicted action sequence is executed in the true simulator to obtain corrected next states, which are added to the training set to update the world model.\n2. Adversarial World Modeling (AWM): a latent-space FGSM-style attack. Perturb latent states and actions in the direction of largest prediction error and retrain on these adversarial samples.\n\nExperiments on small 2-D tasks show moderate success-rate gains for GBP and some improvement over the Cross-Entropy Method at a fraction of its compute time."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The “train–test gap” framing is intuitive and addresses a real limitation of current world-model planning.\n- Both OWM and AWM are straightforward, well-explained extensions of DAgger and adversarial training to latent dynamics.  \n- Demonstrates that smoother, more robust dynamics enable faster differentiable planning."}, "weaknesses": {"value": "- **Limited experimental scope:** evaluated only on a limited number of toy 2-D domains; no high-dimensional or real-robot settings, despite claims about scalability.    \n- **Incremental novelty:** both methods are direct adaptations of known techniques; no new architectural or theoretical contribution.\n- **Missing baselines:** lacks comparison with modern hybrid planners (e.g. **TD-MPC2**).        \n- **Unclear wall-time reporting:** runtime results only shown for one task.\n- **Writing clarity:** several explanations are confusing and could be better structured. Some citations seem off.\n- **Limited generality:** OWM assumes simulator reset access which are impractical in real environments."}, "questions": {"value": "1. How does AWM differ works that use adversarial perturbations to diversify state visitation?. You mention \"In a similar spirit to our approach, (Zhang et al., 2025) introduce an adversarial attack method to encourage diverse state visitation distribution in a model-based RL setting.\" but that points to a survey paper.\n2. Why report wall-clock efficiency only for PushT? Do other tasks show similar ratios vs. CEM?\n3. If the simulator (h) is available (as required by OWM), why not plan directly in it instead of through a learned world model?\n4. Could the approach extend to offline-only settings without simulator resets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WpPdRMBIdD", "forum": "By4u7V6tAx", "replyto": "By4u7V6tAx", "signatures": ["ICLR.cc/2026/Conference/Submission16327/Reviewer_P9Wi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16327/Reviewer_P9Wi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988334352, "cdate": 1761988334352, "tmdate": 1762926463669, "mdate": 1762926463669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies why gradient based planning with learned world models underperforms strong search based planners and proposes two training time data augmentation strategies to close the resulting train test gap between next state prediction training and action sequence optimization at test time.\nThe setting uses a latent world model built on a frozen DINOv2 encoder and a ViT transition model trained with teacher forcing on offline expert trajectories. At test time, actions are optimized by backpropagating through the world model to minimize latent distance to a goal. The authors observe that gradient based planning drives the model into out of distribution latent regions and exploits modeling errors. They introduce two finetuning procedures that expand the effective training distribution to better match the distributions induced by planning:\n- Online World Modeling: run gradient based planning between the first and last latent states of expert trajectories, execute the planned actions in a trusted simulator to obtain corrected rollouts, and aggregate these corrected trajectories to further train the transition model.\n- Adversarial World Modeling: without running a simulator, generate adversarial perturbations in latent and action space around training minibatches using a one step FGSM style update with adaptive radii and clip the perturbations to create hard examples for robust finetuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear identification of a practical train test gap for world models used with gradient based planning and a concrete diagnosis that planning induces out of distribution trajectories and adversarial solutions in latent space.\n- Two simple and implementable finetuning strategies that are compatible with existing latent world models. Online World Modeling leverages a simulator to correct planned rollouts. Adversarial World Modeling promotes robustness with lightweight one step perturbations in latent and action space.\n- Empirical improvements across three tasks from the DINO World Model suite. The paper reports sizable success rate gains over a teacher forcing baseline for both open loop and MPC settings and shows that gradient based planning with Adam benefits most from the proposed finetuning.\n- Evidence that the gap between simulator and world model rollouts during planning is reduced, which directly supports the paper’s motivation to align training and test time distributions."}, "weaknesses": {"value": "- Limited experimental breadth. Only three simulated tasks are considered and all come from the same DINO World Model setup. The work would be more convincing with additional domains or harder long horizon tasks.\n- Comparisons to alternative strong planning baselines are incomplete. Prior hybrid methods that interleave CEM and gradient steps are not compared. There is no comparison to iLQR or DDP on lower dimensional variants, and MPPI is not included despite being common in this setting.\n- The adversarial finetuning design uses heuristic choices such as adaptive batch standard deviation radii, fixed scaling factors, and one step updates. The paper lacks ablations that justify these choices or explore sensitivity to $\\lambda_a$ and $\\lambda_z$ and the clipping ranges."}, "questions": {"value": "- What are the exact simulator characteristics used for Online World Modeling. Is it deterministic or stochastic. How much wall clock time per rollout. How does performance change as simulator latency increases.\n- How many expert trajectories and what diversity were used to train the base teacher forcing model. Please provide dataset sizes per task and the number of finetuning steps for Online and Adversarial methods.\n- For Adversarial World Modeling, why choose one step FGSM style perturbations instead of a small number of PGD steps. Did you test two to three steps and if so did robustness and planning performance change.\n- How sensitive are results to the perturbation scales. Please include ablations over $\\lambda_a$ and $\\lambda_z$ and the adaptive radius choice versus fixed radii. Also report the per dimension magnitudes of the perturbations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xR1YCnBK1Z", "forum": "By4u7V6tAx", "replyto": "By4u7V6tAx", "signatures": ["ICLR.cc/2026/Conference/Submission16327/Reviewer_rdsc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16327/Reviewer_rdsc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988599267, "cdate": 1761988599267, "tmdate": 1762926463333, "mdate": 1762926463333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to improve gradient-based planning methods. Gradient-based planning methods use a differentiable world model. Given a start observation, goal observation, and planning horizon, they sample an initial set of actions to take and perform stochastic gradient descent to try to find actions that result in reaching the goal from the start observation through the differentiable world model.\n\nThe paper proposes two methods to improve gradient based planning: online world modeling and adversarial world modeling. Online world modeling looks at the difference between what the world model predicted and the actual result and finetunes the model based on this. Adversarial world modeling looks for small perturbations in expert trajectories to try to fool the world model into making mistakes and then correcting these mistakes.\n\nThe results show that gradient based planning improvements perform better than not using them and in one instance, they are competitive with the CEM method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper has a straightforward motivation and gives a clear definition of gradient based planning. The two proposed methods are well justified. The proposed methods show improvements over doing gradient-based planning without them."}, "weaknesses": {"value": "This paper does not define key terms and algorithms and has ambiguity in tables and figures. As a result, someone from a planning background, that does not specialize in planning for robotics, will struggle to understand key points. Here is a lists:\n-\tThe interaction between CEM and other methods is not clear from Table 1.\n-\tThe CEM algorithm for planning is not defined, but it is a key comparison.\n-\tThe planning domains are not defined. Some images of them may help.\n\nThe results are only competitive with a non-gradient based method (CEM) in one case. However, it should be noted that gradient-based planning without the proposed modifications is also competitive for this case. Therefore, its hard to say that the methods the authors introduced are responsible for this similarity in performance."}, "questions": {"value": "Why is CEM significant and how does the planning algorithm work?\nIs CEM separate from the other methods in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y14rV3UcNB", "forum": "By4u7V6tAx", "replyto": "By4u7V6tAx", "signatures": ["ICLR.cc/2026/Conference/Submission16327/Reviewer_PBP2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16327/Reviewer_PBP2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762316245661, "cdate": 1762316245661, "tmdate": 1762926462977, "mdate": 1762926462977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}