{"id": "xTP2g8e986", "number": 6867, "cdate": 1757998947734, "mdate": 1759897887351, "content": {"title": "WarpFace: Revisiting Face Reenactment via Self-Supervised Motion Learning in Diffusion Models", "abstract": "Face reenactment enables personalized motion transfer between identities and serves as a fundamental task in human animation. While recent diffusion-based approaches have achieved impressive visual quality, they often depend on human-centric inductive biases (e.g., landmark detectors), which limits their flexibility and scalability. In contrast, self-supervised GANs have demonstrated that meaningful motion representations can emerge directly from raw videos. This motivate us to introduce a novel self-supervised diffusion framework for face reenactment that eliminates the need for domain-specific priors. Our key insight is that diffusion models inherently encode rich motion cues, but naive extraction often leads to semantic collapse, where motion representations lose discriminability. To address this, we propose \\textbf{WarpFace} with two core components: (1) Warping-enhanced Cross-Attention (\\textit{WarpCA}), which incorporates geometry-aware warping within the attention mechanism to enable robust motion learning while preventing semantic collapse; and (2) a Multi-Group Motion Encoder (\\textit{MGME}) that disentangles motion into structured subspaces for fine-grained control. Extensive experiments demonstrate that our method achieves expressive and accurate reenactment without relying on manual annotations or human-specific pretrained priors.", "tldr": "We propose a self-supervised diffusion framework for face reenactment that requires no human-specific priors, achieving expressive and controllable motion transfer.", "keywords": ["Face Animation", "Motion Extraction", "Diffusion Application"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2e948a144aa5e79f0fb385cd48feaf83e4c535bb.pdf", "supplementary_material": "/attachment/ce8dea0aca20777237f2260b3ce7731c185e5ecd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a self-supervised motion learning framework for face reenactment. To transfer the motion pattern without human annotations, this paper aims to exploit the rich motion cues encoded in the diffusion model. This paper proposes two components, including warping-enhanced cross-attention to capture geometry information and a multi-group motion encoder for fine-grained motion control. The experiments are conducted on self- and cross-reenactment setups."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Self-supervised learning for face reenactment has potential for better scaling the models.\n2. The overall paper is easy to follow."}, "weaknesses": {"value": "1. The experiment evaluation contains no comparisons with state-of-the-art video diffusion models (e.g., Wan2.1). As current video generative models become more and more versatile and powerful, it would be beneficial to compare the proposed method with the SoTA video foundation models to further support the contributions. \n2. In Figure 2, the latent motion conditions are encoded from a single image. Given that the motion information is temporally dependent, how can this method derive the motion information simply from a static image?\n3. In Figure 2, the source and driven images are sampled from the same identity. How to ensure the cross-identities transfer?\n4. How to ensure the appearance and motion are properly disentangled? No objective ensures the appearance and motion patterns are decoupled. It might cause degraded results when the motion patterns transfer to different identities that are not similar to the source one."}, "questions": {"value": "1. Can the proposed method be applicable to the facial images that are not precisely aligned in the middle of the image?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gzMP2HZhTb", "forum": "xTP2g8e986", "replyto": "xTP2g8e986", "signatures": ["ICLR.cc/2026/Conference/Submission6867/Reviewer_PYDH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6867/Reviewer_PYDH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761057052571, "cdate": 1761057052571, "tmdate": 1762919120565, "mdate": 1762919120565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents WarpFace, a self-supervised diffusion framework for face reenactment that eliminates the need for domain-specific priors such as facial landmarks or 3D models. The key insight of the method is to exploit the latent motion space encoded in the diffusion process itself, improving control over facial dynamics while maintaining high-quality outputs. The paper introduces Warping-enhanced Cross-Attention (WarpCA) and Multi-Group Motion Encoder (MGME) to enable motion disentanglement. Extensive experiments show that the method outperforms several existing techniques in terms of identity preservation and image quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces an interesting approach by using latent diffusion models to capture motion information without the need for external priors like 3D face models or facial keypoints. This is a novel way to achieve self-supervised motion learning.\n\n2. The proposed WarpCA and MGME modules effectively address issues like semantic collapse and motion expressiveness. The geometry-aware warping in WarpCA ensures that motion features are not degraded during the denoising process, making the method robust for reenactment tasks. MGME provides better control over facial dynamics through multi-group motion representation, improving expressiveness."}, "weaknesses": {"value": "1. While the method demonstrates solid results, the experimental improvement over existing methods like DiffusionAct and X-NEMO is relatively small in certain metrics. Some key metrics  are still worse than some established methods, which indicates that the improvement is not as dramatic as expected from a novel framework.\n\n2. The paper mentions that SeMo and X-NEMO were reproduced by the authors. However, the authors do not provide sufficient explanation or comparison with the original reported results from the respective papers. It is unclear how the authors ensured that their reproduced results are comparable to the original. \n\n3. It relies on iterative denoising which can hinder real-time applications. The computational cost of training and inference, especially with the multi-group motion encoding, might limit the practical deployment of the system for large-scale applications."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CAHy1IILtC", "forum": "xTP2g8e986", "replyto": "xTP2g8e986", "signatures": ["ICLR.cc/2026/Conference/Submission6867/Reviewer_TjYk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6867/Reviewer_TjYk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890789181, "cdate": 1761890789181, "tmdate": 1762919119231, "mdate": 1762919119231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "WarpFace introduces a self-supervised diffusion-based framework for face reenactment that requires no facial landmarks or 3D priors. Its core innovations include a geometry-aware cross-attention module (WarpCA), which predicts optical flow and occlusion maps to perform explicit spatial alignment within diffusion U-Net layers—effectively preventing semantic collapse—and a Multi-Group Motion Encoder (MGME) that decomposes facial motion into multiple orthogonal subspaces, enforcing bidirectional consistency through a symmetry constraint. WarpFace achieves superior identity preservation and visual quality over prior GAN and diffusion approaches on benchmarks such as HDTF and VFHQ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "WarpFace’s main strength lies in its elegant integration of classical geometric warping with modern diffusion-based generation, achieving a self-supervised and interpretable face reenactment framework that requires no landmarks or 3D priors. The proposed geometry-aware cross-attention (WarpCA) effectively prevents semantic collapse by aligning spatial features within diffusion U-Net layers, while the multi-group motion encoder (MGME) introduces a structured, orthogonal latent motion space with bidirectional consistency."}, "weaknesses": {"value": "1. The overall pipeline largely mirrors traditional portrait animation frameworks: feature extraction → warping → occlusion fusion → decoding, while replacing the backbone with SD 1.5. However, the proposed method appears difficult to outperform LivePortrait baseline (Table 2). \n2. Since the paper strongly claims that the method explicitly models geometric transformations, the predicted flow map F and occlusion map O should be clearly visualized to substantiate this claim.\n2. It's strongly claimed that the method explicitly models geometric transformations, then flow map F and occlusion map O should be visualized.\n3. If WarpCA indeed performs explicit geometric warping, applying such operations across multiple U-Net layers could risk distorting local structures and causing feature over-warping; corresponding feature visualizations are needed to verify this effect.\n4. The model is not tested under extreme poses, large rotations, or heavy occlusions, where explicit geometric modeling should provide the most benefit.\n5. As the official code of X-NEMO is publicly available, the authors should use the released implementation for fair and consistent comparison."}, "questions": {"value": "please check the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8xg1HkmBra", "forum": "xTP2g8e986", "replyto": "xTP2g8e986", "signatures": ["ICLR.cc/2026/Conference/Submission6867/Reviewer_RG3p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6867/Reviewer_RG3p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982680147, "cdate": 1761982680147, "tmdate": 1762919118517, "mdate": 1762919118517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}