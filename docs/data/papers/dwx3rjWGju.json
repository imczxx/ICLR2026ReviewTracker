{"id": "dwx3rjWGju", "number": 12725, "cdate": 1758209783123, "mdate": 1759897491224, "content": {"title": "Adaptive Preconditioners Trigger Loss Spikes in Adam", "abstract": "Loss spikes commonly emerge during neural network training with the Adam optimizer across diverse architectures and scales, yet their underlying mechanisms remain poorly understood. In this work, we investigate the fundamental causes of Adam spikes. While previous explanations attribute these phenomena to sharper loss landscapes at lower loss values, our analysis reveals that it is Adam's adaptive preconditioners that trigger spikes during training. We identify a key mechanism where the second moment estimate becomes insensitive to current gradients when using large $\\beta_2$ values. This insensitivity can push the maximum eigenvalue of the preconditioned Hessian beyond the stability threshold $2/\\eta$ for sustained periods, manifesting as dramatic loss spikes. We theoretically and experimentally characterize five distinct stages of spike evolution and propose a predictor for anticipating spikes based on gradient-directional curvature. We further validate our mechanism and demonstrate practical mitigation strategies from small fully connected networks to large-scale Transformers. These findings provide new theoretical insights for understanding and controlling loss spike behavior in Adam optimization.", "tldr": "", "keywords": ["loss spike", "Adam", "training instability"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63cd953c3f55469bbba4ad0aad1654d0af92b66c.pdf", "supplementary_material": "/attachment/35292f0ed6aab270fc86a3eb86b12b89897b9c14.zip"}, "replies": [{"content": {"summary": {"value": "This work studies the loss spikes and why they appear during training. This work demonstrates that the main cause of such a phenomenon is Adam's preconditioner, particularly high values of $\\beta_2$. The authors provide a 5-stage explanation behind loss spikes and validate them on some problems like 1D quadratics, MLP and Transformer models on synthetic data, and Transformer on real data. The authors also provide a metric to detect the loss spikes that computes the curvature along the gradient direction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- This work provides a 5-stage explanation for loss spikes when using Adam. The authors provide an indicator $\\lambda_{grad}(\\hat{H}_t)$ of the loss spike. Once this quantity becomes larger than $2/\\eta$, then there is a spike. \n\n- The authors validate the proposed scheme for MLP and Transformer models on synthetic data, and the Llama-based model on real data.\n\n- The authors provide the intuition why the proposed measure $\\lambda_{grad}(\\hat{H}_t)$ might be a good indicator based on 2nd-order Taylor polynomial.\n\n- The authors verify their 5-stage description of the loss spikes using a simple 1D quadratic function.\n\n- The authors also provide various types of spikes, depending on how the model performs afterward."}, "weaknesses": {"value": "- Missing experiments on real data. Only one of the experiments is done on real data. Why is this so? Is there any particular reason to test the algorithms on synthetic data, not real data?\n\n- Comments on Section 4:\n    - I believe using the quadratic function is not the best model to explain the spike phenomenon. This is especially important since the claims do not generalize to GD (EoS does not happen on quadratics for GD). Therefore, a model where both GD and Adam exhibit EoS should be considered when one studies loss spikes. \n    \n    - It is unclear from the presentation why the sharpness measures for momentum and preconditioning sum up. I encourage the authors to provide more evidence that this is indeed the case by considering a general case, when both preconditioning and momentum are used.\n\n    - I find it weird to talk about divergence when the method achieves the loss 1e-33 or smaller. This is already too good and will never happen in real experiments. The observed behavior might also happen due to numerical instabilities, e.g., division of two numbers close to 0.\n\n    - The results of section 4.2 are already discussed in Appendix D of [1]. They discuss that for significantly small step sizes $\\eta,$ Adam's sharpness hovers below the stability threshold $2/\\eta$. However, if one freezes the preconditioner, then the algorithm enters EoS \"properly\" and spikes appear. Their intuition is that the gradients generated by first-order momentum can still be large, forcing the preconditioner to grow always. This results in the absence of a classic EoS stage. To make this point clear, the behavior observed in Figure 3 happens because of both momentum terms, not just the second-order term and the corresponding choice of $\\beta_2$. I encourage the authors to test what happens when $\\beta_1=0$ (RMSprop) and with a larger stepsize $\\eta$ for the toy problem in Figure 3, to observe similar things to Appendix D of [1] or disprove their claims. If the authors want to make precise statements, they should somehow remove the influence of $\\beta_1$. In my view, the spikes appear for $\\beta_2=0.9$ and large enough $\\eta$, which says that the appearance of spikes is a more complex mechanism, which involves both momentum parameters and stepsize, and it cannot be simplified to just the value of $\\beta_2$ alone.\n\n    - While (6) is clearly derived from a 2nd-order Taylor expansion, the derivations for the predictor for Adam are missing. \n    \n    - The connection between $\\lambda\\_{\\max}(\\hat{H}\\_t)$ and $\\lambda\\_{grad}(\\hat{H}\\_t)$ is unclear. In Figure 3, the authors demonstrate that $\\lambda\\_{\\max}(\\hat{H}\\_t)$ is a good predictor of spike, while in Section 4.3, they suddenly move to $\\lambda\\_{\\max}(\\hat{H}\\_t)$. Why? This is not explained. In fact, the reason is because $\\lambda\\_{\\max}(\\hat{H}\\_t)$ is an upper bound on $\\lambda\\_{grad}(\\hat{H}\\_t)$, and the difference between them might be large when there are several unstable eigendirections (in 1D case this of course does not happen). $\\lambda\\_{grad}(\\hat{H}\\_t)$ is a better predictor because it measures the curvature along the current update direction. Such a choice of the predictor is also highly correlated with the fact that the preconditioned gradient aligns well with one of the unstable top eigenvectors (see discussion in [1]), so $\\lambda\\_{grad}(\\hat{H}\\_t)$ measures the curvature along the current unstable direction. \n\n    - The fact that $\\beta_2$ influences the spikes was also shown in prior work [3], see also a blogpost [4], but the results from [3] are not discussed in this work. They obtain that the effective sharpness of norm-RMSprop (standard RMSprop as well) grows both due to the increase of the standard sharpness and the decrease of the gradient norm. Once the algorithm crossed the threshold $2/\\eta$, the stabilization mechanism forces the gradient norm to increase while the standard sharpness to decrease. This results in a decrease in the effective sharpness."}, "questions": {"value": "- Lower-loss-as-sharper (LLAS) structure corresponds to the progressive sharpening phase of the training, i.e., before GD enters EoS (see caption 6 in [2]). This is one of the primary reasons why it might fail to explain what happens at EoS. It is not so trivial, and I encourage the authors to give a clear explanation for this. \n\n- Does Figure 3 (a) make sense? The divergence might happen because of the rounding errors. The loss is already super small, the method converged, and the gradients are 0. So my guess is that that kind of behavior might also happen because of the rounding errors in computing the ratio of two small numbers.\n\n- Can the authors ensure their assumption that the Hessian does not change much? \n\n- What happens in Figure 3 if the stepsize $\\eta$ is larger? Does $\\beta_2=0.9$ still give a stable loss decrease?\n\n- Typo in line 204.\n\n- The authors refer to some specific values in the plots. Therefore, they should add a grid to make the figure easier to read.\n\n- Confusing labels in the figures. For example, Figure 3a has a y-label to be the train loss, while the authors also plot the gradient norm. I encourage the authors to present their results more cleanly, so that this kind of thing does not confuse readers. Similar question regarding the captions. For example, in the caption of Figure D12, the authors refer to an orange line that is not present in the figure. \n\n- How do the authors manage to obtain different types of spikes in Figure D2? What changes between the setups so that Adam exhibits various spikes w.r.t. generalization/test loss?\n\n- Can the authors provide more details behind the predictor in the stochastic setting? I believe the authors were lucky that their predictor predicts well (maybe there is a little amount of noise), and it can be broken by using data where the stochastic Hessian significantly differs from the full Hessian. \n\n\n[1] Cohen, Jeremy M., et al. \"Adaptive gradient methods at the edge of stability.\" arXiv preprint arXiv:2207.14484 (2022).\n\n[2] Cohen, Jeremy M., et al. \"Understanding optimization in deep learning with central flows.\" arXiv preprint arXiv:2410.24206 (2024).\n\n[3] Cohen, Jeremy M., et al. \"Understanding optimization in deep learning with central flows.\" arXiv preprint arXiv:2410.24206 (2024).\n\n[4] https://centralflows.github.io/part2/"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GSCBts8z6V", "forum": "dwx3rjWGju", "replyto": "dwx3rjWGju", "signatures": ["ICLR.cc/2026/Conference/Submission12725/Reviewer_xauY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12725/Reviewer_xauY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576275510, "cdate": 1761576275510, "tmdate": 1762923548644, "mdate": 1762923548644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies loss spikes in Adam.  They emphasize that a key contributor to loss spikes in Adam (not present for gradient descent) is when gradients shrink, causing the the second moment accumulator to shrink, causing the effective step sizes to grow until the top eigenvalue of the preconditioned Hessian exceeds the linearized stability threshold of Adam.  They also emphasize that instabilities do not always cause the train loss to visibly spike; instead, visible spikes in the the train loss only happen when the Rayleigh quotient of the gradient with the preconditioned Hessian exceeds $2/\\eta$.  They show that cutting $\\beta_2$ and increasing $\\epsilon$ can ameliorate loss spikes (though they caution that it is not always clear if the spikes are good or bad)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper conducts thorough experiments on an interesting and important topic (loss spikes in Adam).  The suggestion that cutting $\\beta_2$ can ameliorate loss spikes may be useful for practitioners."}, "weaknesses": {"value": "One weakness of the paper is novelty w.r.t prior works, notably https://arxiv.org/abs/2207.14484 (which was cited).  That paper previously conducted the stability analysis of Adam that is given here as Proposition 2.  Nevertheless, that paper did not emphasize how gradient norm shrinkage leads to sharpening of the preconditioned Hessian, nor did it have Theorem 1, nor did it discuss how some instabilities lead to spikes in the loss whereas others do not."}, "questions": {"value": "* The $2/\\eta$ line seems to be different between Figure 7(b) and 7(c).  In 7(b), it's around 4.2, whereas in 7(c), it seems to be larger than that.  What's the source of the discrepancy?\n* In Figure 7(b), the Hessian (purple) seems to grow over the course of the spike, which seems surprising to me.   To be clear, this is not necessarily contradicted by any prior works, but I'd still expect that if you froze the preconditioner at step 11210 and monitored the preconditioned Hessian _with that fixed preconditioner_ over the next few hundred steps, it would decrease during the spike.\n* The central flows paper (https://arxiv.org/abs/2410.24206) may be of interest.  For example, the five-stage characterization from your Section 5 is discussed [in the blog post here](https://centralflows.github.io/part2/).  Within that paper's notation, if $x_t$ denotes the magnitude of oscillation along the top eigenvector, I suspect that the loss spikes (as contrasted with instabilities that do not cause the loss to noticeably spike) occur when $\\max_t x_t^2$ is large, which is different from the time-average $\\mathbb{E}_t[x_t^2]$ being large."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CCH3E0Mu0N", "forum": "dwx3rjWGju", "replyto": "dwx3rjWGju", "signatures": ["ICLR.cc/2026/Conference/Submission12725/Reviewer_ewU8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12725/Reviewer_ewU8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878055196, "cdate": 1761878055196, "tmdate": 1762923548254, "mdate": 1762923548254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the phenomenon of loss spikes in the Adam optimizer. The authors propose that these spikes are caused by the adaptive preconditioning mechanism of Adam, especially its slow adaptation in the second moment estimate, with theoretical analysis and experimental results. The paper identifies a five-stage mechanism describing spike evolution, introduces a predictor for spike onset based on gradient-directional curvature, and validates both the mechanism and mitigation strategies from basic function approximation to large transformer models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a clear and convincing mechanistic model explaining how Adam’s adaptive preconditioner leads to spike formation, supported by precise mathematical characterization, direct derivations, and step-by-step proofs. The proposed five-stage framework provides an intuitive yet theoretically grounded view of the phenomenon.\n\n2. Extensive experiments across diverse neural architectures, such as quadratic functions, small MLPs, CNNs, and transformer models, demonstrate the ubiquity and consistent behavior of loss spikes, providing strong empirical support for the theoretical analysis.\n\n3. The introduction of a gradient-directional curvature predictor significantly improves the accuracy of spike onset detection and offers a practical tool for real-time monitoring and stabilization during training."}, "weaknesses": {"value": "1. The paper lack experiments comparing Adam with other optimizers or its common variants, such as RMSProp, AdaGrad, or AdamW. Including such baselines would help clarify whether the observed spike dynamics are unique to Adam or shared across adaptive methods.\n\n2. The large-scale experiments rely on synthetic data or controlled training conditions. As a result, the generality of the conclusions for diverse, real-world datasets remains to be convincingly demonstrated.\n\n3. The use of gradient-directional curvature for real-time monitoring is computationally demanding in high-dimensional models, as accurate Hessian-vector products and eigenvalue tracking are non-trivial, especially for large transformer architectures.\n\n4. While the proofs are mathematically sound, the main theorems rely heavily on quadratic or convex assumptions."}, "questions": {"value": "1. What are the empirical effects of using alternative normalization strategies (suck as LayerNorm) on spike incidence, especially in conjunction with Adam?\n\n2. How do the conclusions change when applying Adam variants specifically designed to address training instabilities (such as AdamW)? Can the proposed mitigation strategies and insights be extended to these optimizers, or do their design differences alter the underlying spike mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "okUncNof3H", "forum": "dwx3rjWGju", "replyto": "dwx3rjWGju", "signatures": ["ICLR.cc/2026/Conference/Submission12725/Reviewer_wQk8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12725/Reviewer_wQk8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989331027, "cdate": 1761989331027, "tmdate": 1762923547894, "mdate": 1762923547894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work  tries to  explain the mechanism behind Adam's loss spikes, tracing their cause to the optimizer's adaptive preconditioners.It identifies a critical delay in the second-moment's response to gradients, which sustains these instabilities.  It also discovers a simple fix mehtod that reducing the β2 parameter,  and this is also validated experimentally."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work addresses a significant and longstanding issue in deep learning: the phenomenon of loss spikes during Adam optimization. This problem frequently troubles practitioners but has, until now, been poorly understood. The paper's focus on providing a mechanistic explanation for these instabilities is therefore timely and valuable."}, "weaknesses": {"value": "- Lack of Theoretical Novelty and Applicability: The theoretical analysis presented in the paper lacks novelty. The core condition for stability, $\\eta \\le \\frac{2}{\\lambda_{\\max} (H_t)}$, is a well-known and fundamental result in optimization theory. While the authors attempt to extend this by using a time-varying $\\lambda_{\\max} (H_t)$, their analysis still relies on the assumption that the Hessian $H_t$ remains constant and positive-definite. This severely limits the applicability of their theory to non-convex deep learning scenarios, where the Hessian is neither constant nor positive-definite. Consequently, the analysis does not provide new insights for understanding or resolving loss spikes in practical settings.\n\n- Impractical Proposed Monitoring Scheme: The proposal to use $\\lambda_{\\max} (H_t)$ as a real-time monitor for loss spikes is computationally prohibitive and thus unrealistic for modern deep learning. The computational complexity for calculating the spectral norm is $O(n^3)$, and the storage requirement for the Hessian is $O(n^2)$, where $n$ is the number of parameters. For example, in the LLaMA-187M experiment in Figure 8, storing the full Hessian would require approximately $10^{16}$ bytes (10 Petabytes) of memory, which is far beyond the capacity of current hardware. It is unclear how the authors accomplished this in their experiments, and the methodology for this computation is not explained, raising concerns about the feasibility and reproducibility of this approach."}, "questions": {"value": "- Proposition D.2: The proof assumes the Hessian $H_t$ and the gradient $g_t$ are constant, which is an unrealistic assumption for non-convex optimization, which undermines the generalization of the result.\n\n- Line 179: The approximation $\\delta \\theta_{t+1} \\simeq \\delta \\theta_{t} - \\eta \\nabla F(\\delta \\theta_{t} )$ appears problematic. A standard  expansion would typically yield a form like $\\delta \\theta_{t+1} \\simeq \\delta \\theta_{t} -\\eta \\nabla F(\\theta_t)$ or similar. This step requires clarification and justification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TP78tHEY7V", "forum": "dwx3rjWGju", "replyto": "dwx3rjWGju", "signatures": ["ICLR.cc/2026/Conference/Submission12725/Reviewer_kJgM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12725/Reviewer_kJgM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762174659302, "cdate": 1762174659302, "tmdate": 1762923547594, "mdate": 1762923547594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}