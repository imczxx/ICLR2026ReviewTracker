{"id": "pye62xAoi4", "number": 10674, "cdate": 1758179240298, "mdate": 1759897636123, "content": {"title": "Value Shaping: Bias Reduction in Bellman Error for Deep Reinforcement Learning", "abstract": "The Bellman error plays a crucial role as an objective function in deep reinforcement learning (DRL), serving as a proxy for the value error. However, this proxy relationship does not guarantee exact equivalence between the two, as the Bellman error inherently contains bias that can lead to unexpected optimization behavior. In this paper, we investigate the relationship between the value error and the Bellman error, and analyze why the Bellman error is not a reliable proxy due to its inherent bias. Leveraging the linear structure of the Bellman equation, we propose a method to compensate for this bias by adjusting the reward function—while ensuring that such modifications do not alter the optimal policy. In practice, we initialize two parallel Bellman iteration processes: one for estimating the bias and the other for updating the value function with minimal bias. Our method effectively learns a low-bias Q-function, making it broadly applicable and easily integrable into existing mainstream RL algorithms. Experimental results across multiple environments demonstrate that our approach improves RL efficiency, achieves superior performance, and holds promise as a fundamental technique in the field of reinforcement learning.", "tldr": "Our goal is to enhance the sample efficiency of deep reinforcement learning algorithms by addressing potential biases in the Bellman error.", "keywords": ["Bellman error", "bias reduction", "affine reward transformation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/09079418ebc95b53d650def81c4401de8fa95bfd.pdf", "supplementary_material": "/attachment/2621bd56c4baf2cf0ba0dc577c1e0ac3a9b30e82.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a reinforcement learning algorithm that linearly transforms the reward function to improve sample efficiency.\nSpecifically, the authors propose a method to compensate for the bias in the target value of the Q-function.\nIn the proposed approach, two functions are used to approximate the Q-function.\nIf my understanding is correct, the bias in the value function is estimated as the mean of the Bellman error computed from one of the approximated Q-functions, and this estimated bias is then used to modify the target Q-value for the other approximated Q-function.\nThe proposed method was evaluated on benchmark tasks in Gymnasium with MuJoCo environments."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The idea of transforming the reward function is an interesting research direction for improving the sample efficiency of RL algorithms.\n\n- The proposed method is agnostic to specific policy update algorithms and can be combined with various RL methods.\n\n- The approach appears simple and easy to implement, provided that the necessary details are clearly described."}, "weaknesses": {"value": "- The presentation is not well organized and is difficult to follow in some parts.\n\n- The performance improvement appears limited.\n\nAlthough the paper discusses the difference between the value error and the Bellman error, it is unclear how the proposed method addresses the challenge of estimating the value error. In lines 236–237, the paper states: \"It is additionally noteworthy that  $\\epsilon^*_{Q}=\\Delta_{Q}$.\"\n\nHowever, the definition of $\\epsilon^*_Q$ seems to be missing, and it is unclear how this relationship is mathematically justified.\n\nMoreover, the description of the proposed algorithm is ambiguous.\nIf I understand correctly, the bias in the value function is estimated as the mean of the Bellman error,\n$\\hat{b} = r_i + \\gamma Q_1 (s'_i, a'_i) - Q_1 (s_i, a_i)$,\nand the target value for $Q_2$ is computed as\n$r_i + \\gamma Q_2 (s'_i, a'_i) + \\hat{b}$.\nHowever, I am not entirely sure if this interpretation is accurate.\nIf it is, I do not think the proposed algorithm can correctly reduce the bias, since the bias in $Q_1$ differs from that in $Q_2$, and the Bellman error is generally not equivalent to the value error.\nThe authors should revise the manuscript to clarify the proposed algorithm and its theoretical motivation.\n\nAnother weakness lies in the empirical results.\nIn Figure 1, the performance gains from the proposed method are marginal in Humanoid-v4, Ant-v4, HalfCheetah-v4, Hopper-v4, and Swimmer-v4.\nFurthermore, to substantiate the proposed method, it would be necessary to show that the value error is actually reduced.\nAlthough empirical Q-values are plotted in Figure 2, it would be more informative to plot the difference between the estimated Q-values and the empirical Q-values (i.e., the empirical value error).\nThe current experimental results do not sufficiently demonstrate the claimed advantages of the proposed approach."}, "questions": {"value": "- If my understanding is correct, the bias in the value function is estimated as the mean of the Bellman error\n$\\hat{b} = r_i + \\gamma Q_1 (s'_i, a'_i) - Q_1 (s_i, a_i)$,\nand the target value for $Q_2$ is computed as\n$r_i + \\gamma Q_2 (s'_i, a'_i) + \\hat{b}$.\nPlease confirm whether this interpretation is accurate or correct me if I am mistaken.\n\n- In lines 236–237, the paper states: \"It is additionally noteworthy that  $\\epsilon^*_{Q}=\\Delta_{Q}$.\"\nSince the definition of $\\epsilon^*_Q$ seems missing, please clarify its meaning and provide the mathematical justification for this statement.\n\n- Although Figure 2 plots the empirical Q-values, I suggest plotting the empirical value error instead—approximated as the difference between the estimated and empirical Q-values. Could you provide such a figure to support your claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6s6anknI8k", "forum": "pye62xAoi4", "replyto": "pye62xAoi4", "signatures": ["ICLR.cc/2026/Conference/Submission10674/Reviewer_M7qB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10674/Reviewer_M7qB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761034814581, "cdate": 1761034814581, "tmdate": 1762921924038, "mdate": 1762921924038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main objective of this work is to estimate and correct the bias present in the Bellman error, which arises because deep reinforcement learning (DRL) algorithms optimize this quantity instead of the true value error. To address this discrepancy, the authors propose a value shaping approach that introduces a bias correction term into the Q-target. Authors evaluate their approach on Mujoco envirnoments, by combining the proposed approach, MSR, with TD3, RRS and TD7."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "NA"}, "weaknesses": {"value": "Major weaknesses:\nIn general, the paper is quite hard to follow (not due to the theoretical concepts, but due to the writing).\n1) Proposition 3.1: This result is already presented in [1] (Theorem 1). You can report this result to improve readability, but you have to specify that this result has already been presentd and proved in [1]. In fact, this cannot be considered as your contribution.\n2) Sections 4.1 and 4.2 are quite hard to follow: you introduce b as a result of Proposition 4.3 and you define $\\espilon^*_Q$, which is the value error. So, from line 234 to 237, you are just adding notation which might be just confusing. Why eq. 7 is an assumption you make? Equation 6 is the proof that the value error has an additional term.\n3) Is there a typo in equation 10? $||f(r)|-|g||\\le |f(r)|+|g|$?\n4)  The overall discussion in section 4.3 is quite confusing. Once you define $\\espilon^*_Q$ in Eq. 7, you can immediately derive $\\hat{b}=-\\epsilon_Q$, but to derive it you introduce the otimization problem in 11.\n5) Since the entire discussion is quite confusing, the derivation of equation (13) is not clear to me. It seems that there is a missing explanation there.\nConcerning the results:\nA) The results reported in Figure 1 do not show a significant improvement with MRT\nB) Depending on the environment/algorithm, the gain might be close to 0, e.g. MRT+TD3 does not improve TD3 in ant-v4.\nC) Again, the fact that the Q-values are lower in the early training steps is just (highly) evident in humanoid, halfcheetah, and swimmer, but just for TD7.\nD) in 418 \"...leading to smaller update magnitudes, which in some cases also results in higher accuracy\" is quite vague.\nE) In 423: \" ...Q-values updates are relatively stable\". This depends on the environment and the RL algorithm. In Figure 2, b, c, and d don't show this effect.\nF) Same in Figure 3: concerning the Bellman error, the effect of MRT is not the same for all methods and environments.\nminor:\n1) The name of the method is MRT, where R stands for reward, but then you hightlight that your approach is value shaping rather than reward shaping.\n2) 248-249: \"we can adjust the parameter of to make\". remove \"of\"\n3) 266: \"This (eq[8]) transfromed Bellmand error …\" Use instead: \"The proposed e_Q', the transformed Bellman error, can be …\"\n[1] Fujimoto, Scott, et al. \"Why should i trust you, bellman? the bellman error is a poor replacement for value error.\" arXiv preprint arXiv:2201.12417 (2022)"}, "questions": {"value": "please clarify the weaknesses highlighted"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kfPuCgLWac", "forum": "pye62xAoi4", "replyto": "pye62xAoi4", "signatures": ["ICLR.cc/2026/Conference/Submission10674/Reviewer_NhmY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10674/Reviewer_NhmY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761302725343, "cdate": 1761302725343, "tmdate": 1762921923592, "mdate": 1762921923592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work investigates a method to reduce the bias in the Bellman error estimation to improve the convergence of policy evaluation methods. The method, called MRT, leverages the fact that an affine translation of the reward function can be implemented without changing the optimal policy of the problem at hand. However, instead of directly learning the affine parameters, the method estimates the Bellman error and subtracts it from the regression target. Experiments are conducted on the MuJoCo benchmark.\n\nWhile the work focuses on a relevant topic in RL, it does not contain any theoretical contribution, the algorithm is not clearly presented, and the introduced method does not lead to significant improvements. Therefore, the recommendation for this work is \"2: reject, not good enough\"."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "I. The work focuses on a relevant limitation of value-based methods: the Bellman error is a biased estimate of the value error. Reducing the bias promises improvement of policy evaluation methods.\n\nII. The proposed idea is creative, starting from an interesting perspective."}, "weaknesses": {"value": "A. The presented method is not clearly explained and is misleading at times. This is especially true for section 4.3, which I suggest rewriting entirely.\n\n   i. While the method is motivated from a reward-shaping point of view, the method ends up following the value-shaping paradigm. This shift happens when the bias estimator is set to $- \\mathbb{E}[\\epsilon_Q]$, on Line 306. First, this result is not justified, and second, it seems that the constraint of the fact that the reward-shaping should be an affine transformation of the original reward function is not taken into account.\n\n   ii. The pseudo-code is unclear as most of the lines are written in text, which increases the ambiguity.\n\n   iii. Equation 10 is wrong. Indeed, $\\epsilon'_Q = -1$ and $\\epsilon^*_Q = 0$ is a counterexample invalidating Equation 10. Furthermore, it is not justified why minimizing the left term is relevant. Indeed, on Line 260, it is only stated that: \"we can minimize the following [Equation 10]\".\n\n   iv. The proof of Proposition 4.3 is in the appendix but is not mentioned in the main text.\n\n   v. In Proposition 3.1, $p^{\\pi}$ is not defined.\n\n   vi. Line 162, the definition of the modified reward is repeated in Line 165.\n\n   vii. There seems to be an assumption on the reward definition that is not clearly stated in Line 197.\n\n   viii. The presentation of the idea of bias propagation in Remark 4.2 is unclear.\n\n   ix. The term \"irrelevant bias\" on Line 214 is not defined.\n\n   x. The statement Line 240 is wrong: \"We know that the reward can be linearly transformed without affecting the convergence of the optimal policy\". Linearly transforming the reward does not change the optimal policy but might affect convergence to it. In fact, it is the primary motivation for transforming the reward.\n\n   xi. Many parentheses are missing around citations, which breaks the ready flow. For example, Line 32, \"Bellman (1966)\" should be in parentheses.\n\nB. The experimental analysis demonstrates no significant gains with respect to the baseline, and the presentation can be improved.\n   \n   i. All curves comparing an algorithm and the presented method are overlapping, which leads to the conclusion that the algorithm is not worth incorporating into existing RL pipelines.\n\n   ii. Figure 2 reports the empirical Q-values. This comparison is not relevant as the proposed algorithm implicitly changes the reward function.\n\n   iii. Similarly to the plot on performance, the plot on Bellman error (Figure 3) does not show any improvement with respect to the baselines.\n\n   iv. The information on the figures is not visible. Increasing the font size and the line width would help.\n\nC. Some related works are missing.\n\n   i. Value shaping for representation learning: Dabney, Will, et al. \"The value-improvement path: Towards better representations for reinforcement learning.\" AAAI 2021.\n\n   ii. Value shaping for minimizing the error propagation: Vincent, Théo, et al. \"Iterated Q-Network: Beyond One-Step Bellman Updates in Deep Reinforcement Learning.\" TMLR 2025.\n\n   iii. Value shaping for convergence acceleration: Farahmand, Amir-Massoud, and Mohammad Ghavamzadeh. \"PID accelerated value iteration algorithm.\" ICLM 2021.\n\n   iv. The work Hao Sun, et al. \"Exploit reward shiftingin value-based deep-rl: Optimistic curiosity-based exploration and conservative exploitation vialinear reward shaping.\" NeurIPS 2022 is cited twice, once in Line 603, and once in Line 608. Please remove one version to avoid double citation."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MHnWuxzrb3", "forum": "pye62xAoi4", "replyto": "pye62xAoi4", "signatures": ["ICLR.cc/2026/Conference/Submission10674/Reviewer_d3e2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10674/Reviewer_d3e2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761469208708, "cdate": 1761469208708, "tmdate": 1762921923032, "mdate": 1762921923032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a MRT method to address the estimation error issue. Experiments across Mujoco environment show that MRT improves sample efficiency, reward, and value estimation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. estimation bias is a fundamenal issue of RL\n2. the idea is novel"}, "weaknesses": {"value": "1. the theoretical fundation presented in the paper has notable limitations. For Remark 4.1, auther use the exact example as Fujimoto 2022 paper, however this example list an unrealistic example of estimation bias. The estimation bias measures between estimated value and true value over all state and action pairs and it is a stochastic variable. Assuming a fixed additive bias of \"+1\" across all state-action pairs oversimplifies the nature of estimation bias which only represent the corner case. It fails to capture its statistical properties based on such corner cases to make an significant conclusion of \"estimation error is a poor replacement for value error\" is problematic. Similar oversimplifications appear in Remark 4.2 and Assumption 4.4, where the propagation and compensation of bias are modeled without accounting for its stochastic properties. These assumptions undermine the rigor of the proposed relationship between TD error and value error. As a result, the connection between Bellman error and value error remains insufficiently justified and requires further theoretical reevaluation.\n\n2. Figure 1, espically for TD3, With MRT, the performance still within the  shaded area which is really hard to see if there exist performance improvment.\n\n3.  Many theoretical parts which is identical to Fujimoto's paper (Proposition 3.1, Remark 4.1, Remark 4.2)."}, "questions": {"value": "1. Figure 1-3, the shaded area is confidence interval or std value? please specifiy in the caption.\n2. As TD3 state it is focused on overestimation bias but can induce underestimation bias, does this method apply to underestimation bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D1PTcTUWuZ", "forum": "pye62xAoi4", "replyto": "pye62xAoi4", "signatures": ["ICLR.cc/2026/Conference/Submission10674/Reviewer_Esde"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10674/Reviewer_Esde"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962712313, "cdate": 1761962712313, "tmdate": 1762921922572, "mdate": 1762921922572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}