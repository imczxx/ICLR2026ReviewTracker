{"id": "4lAQBVU2Za", "number": 15968, "cdate": 1758257840576, "mdate": 1759897270175, "content": {"title": "Safe Multi-Objective Reinforcement Learning via Multi-Party Pareto Negotiation", "abstract": "Safe multi-objective reinforcement learning (Safe MORL) seeks to optimize performance while satisfying safety constraints. Existing methods face two key challenges: (i) incorporating safety as additional objectives enlarges the objective space, requiring more solutions to uniformly cover the Pareto front and maintain adaptability under changing preferences; (ii) strictly enforcing safety constraints is feasible for single or compatible constraints, but conflicting constraints prevent flexible, preference-aware trade-offs.\nTo address these challenges, we cast Safe MORL within a multi-party negotiation framework that treats safety as an external regulatory perspective, enabling the search for a consensus-based multi-party Pareto-optimal set. We propose a multi-party Pareto negotiation (MPPN) strategy built on NSGA-II, which employs a negotiation threshold $\\varepsilon$ to represent the acceptable solution range for each party. During evolutionary search, $\\varepsilon$ is dynamically adjusted to maintain a sufficiently large negotiated solution set, progressively steering the population toward the $(\\varepsilon_{\\text{efficiency}}, \\varepsilon_{\\text{safety}})$-negotiated common Pareto set.\nThe framework preserves user preferences over conflicting safety constraints without introducing additional objectives and flexibly adapts to emergent scenarios through progressively guided $(\\varepsilon_{\\text{efficiency}}, \\varepsilon_{\\text{safety}})$. Experiments on a MuJoCo benchmark show that our approach outperforms state-of-the-art methods in both constrained and unconstrained MORL, as measured by multi-party hypervolume and sparsity metrics, while supporting preference-aware policy selection across stakeholders.", "tldr": "The paper proposes a multi-party negotiation framework for safe multi-objective reinforcement learning, allowing for a Pareto front of policies that balance efficiency and safety constraints.", "keywords": ["Multi-party Multi-objective Reinforcement Learning; Constrained Reinforcement Learning; Multi-objective Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68aeb7d5488f7db65c4e0b50f1450ad65705a8c1.pdf", "supplementary_material": "/attachment/fd90151eaf1af8c461ac8f6c4ea8a00ff15aa722.zip"}, "replies": [{"content": {"summary": {"value": "This work addresses safe multi-objective reinforcement learning within a multi-party negotiation framework, rather than treating safety as an additional objective. This approach enables the search for a consensus-based, multi-party Pareto-optimal set without enlarging the objective space. Within this framework, the authors propose a multi-party Pareto negotiation (MPPN) strategy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Formulating safe multi-objective reinforcement learning as a multi-party negotiation problem is, to the best of my knowledge, novel, interesting, and practically valuable.\n\n2. The overall method design is generally reasonable and coherent."}, "weaknesses": {"value": "1. Algorithm 2 appears to update policy parameters solely through DE mutation, without using policy gradients or value-based guidance. This raises questions about efficiency and whether the learning signal may be too sparse, so it is also unclear if this approach can still be considered “reinforcement learning.” The observed improvements may primarily result from the multi-party-specific evaluation metrics, whereas baseline methods are not designed for a multi-party setting.\n\n2. It would be helpful to present the learned behaviors and analyze how they relate to multi-party Pareto optimality.\n\n3. The policy indices in Figure 2 do not seem to correspond with the text description in lines 180–185."}, "questions": {"value": "1. Does it update policy parameters solely through DE mutation?\n2. How could policy gradients be incorporated to provide denser and more informative updates?\n3. Does the method learn meaningful multi-party Pareto behaviors in complex environments, such as humanoid tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L97nFMQUbS", "forum": "4lAQBVU2Za", "replyto": "4lAQBVU2Za", "signatures": ["ICLR.cc/2026/Conference/Submission15968/Reviewer_5Y92"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15968/Reviewer_5Y92"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15968/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656608246, "cdate": 1761656608246, "tmdate": 1762926177732, "mdate": 1762926177732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reconceptualizes Safe MORL as a multi-party negotiation problem, where the safety objectives and efficiency objectives are treated as separate multi-objective decision parties rather than as additional objectives in a single objective space.\nBuilding on this idea, they develop a negotiation-driven evolutionary framework, MPPN-MORL, which integrates multi-party Pareto negotiation into policy search without increasing the dimension of the objective space. The algorithm incorporates an ε-dominance criterion to enable negotiation into evolutionary search. The idea offers a novel and well-motivated perspective on safe MORL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a novel conceptual formulation of safe MORL as a multi-party negotiation process, which provides a fresh perspective on balancing safety and efficiency.\n\nThe proposed MPPN-MORL framework is well-motivated, integrating negotiation principles with evolutionary policy search in a coherent way.\n\nThe use of an ε-dominance based negotiation rule and differential evolution operators is clearly described and logically connected to the goal of efficient compromise among objectives (see however below). \n\nIt is possible that the algorithm respects user-specified preferences over both performance and safety, preserves diversity in the solution set, and promotes fairness across parties."}, "weaknesses": {"value": "The paper claims adaptability, diversity, and fairness in the learned policy set, but these aspects are not directly analyzed or supported by quantitative experiments. Including such evidence would strengthen the empirical evaluation. \n\nThe scalability and computational cost of negotiation among multiple parties are not extensively discussed, which may limit understanding of its practical applicability. \n\nFig. 1 may be excellent for use in a talk, but in a collection containing several contributions on MORL, there is no need to start from this level.\n\nDefinition 3.1 does not define Pareto Dominance, but Pareto Dominance w.r.t a DM. This should be stated in the beginning in brackets.\n\nIn Table 1, the proposed method is referred to as “MPNN”, while the paper elsewhere uses “MPPN.” Please check whether this is a typo. \n\nPlease clarify what the $x$- and $y$-coordinates in Figure 5 represent, including their units and scales? It is important as it is not clear from the caption or text how Figure 5 is obtained or what it is intended to show.\n\nA problem is the need to choose two ε thresholds (for performance and safety) which is at odds with the idea of MORL where the  decision whether a criterion is more less strict is left for the user for after the optimization, while here a related decision is to be made before the start of MORL, so that it is questionable whether the MORL framework has to be used here in the first place or whether already a scalarization is sufficient. I understand that there is theoretical difference between the preference coefficients and the ε thresholds, but it will be difficult to explain this to any users."}, "questions": {"value": "The paper notes that MPPN-MORL has certain limitations in terms of solution distribution. Could the authors elaborate on what causes this limitation, and whether it relates to the ε-dominance mechanism or the negotiation dynamics? \n\nHow would you treat safety constraints that cannot be expressed as objectives?\n\nWouldn't in the case of a safety-critical application a hierarchical approach be useful? I.e. why should unsafe regions be explored at all? If this is in some cases justifiable, then such a justification needs to be discussed already here. \n\nCan you define a Multi-party Pareto Front?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The algorithm promises safety, diversity and fairness, which may not necessarily meet the criteria of any possible application. Although no problem is visible at this stage, a note of caution be may useful especially as the assessment of these properties is improvable (see above) already for the applications that are considered here."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vKIqcCYT4q", "forum": "4lAQBVU2Za", "replyto": "4lAQBVU2Za", "signatures": ["ICLR.cc/2026/Conference/Submission15968/Reviewer_mHFd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15968/Reviewer_mHFd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15968/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912643320, "cdate": 1761912643320, "tmdate": 1762926176486, "mdate": 1762926176486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new framework for Safe Multi-Objective Reinforcement Learning that treats efficiency and safety as separate decision-making parties in a multi-party negotiation process rather than as combined objectives or hard constraints."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel conceptual reformulation: Framing Safe MORL as a multi-party negotiation problem is original.\n\n- Clear algorithmic description: The dynamic adjustment of ε to control exploration vs. exploitation is intuitive and aligns with practical safety–efficiency trade-offs."}, "weaknesses": {"value": "I feel the authors want to discuss multiple things, which are entangled together: safety, different decision makers, adaptability. It would be more meaningful to separate these challenges, and discuss what is the key motivation and novelty.\n\nThe negotiation is just a selection of hyperparameters. Note that in the training process, there is no \"negotiation\" between different agents.\n\nSimulations are quite limited to few-dimension simulations, while the performance of proposed techniques are unclear on high-dimensional Pareto front.\n\nLittle theoretical insights or guarantees are provided for this method.\n\nFor real-world safe RL, there are hard constraints which can never violate, which shall be discussed and compared to other approaches."}, "questions": {"value": "- I think the first challenge raised by the paper, \"incorporating safety as additional objectives enlarges the objective space, requiring more solutions to uniformly cover the Pareto front and maintain adaptability under changing preferences\" is a quite mixed one. How to show the proposed method can achieve both coverness and adaptability, while achieving safety?\n\n- Can the authors explain more about the \"perspective\" of Pareto front? Because in the Pareto front, it is already discussed about the different weighted combinations of preferences. Then why is that different from the different decision makers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DodmzHBvf5", "forum": "4lAQBVU2Za", "replyto": "4lAQBVU2Za", "signatures": ["ICLR.cc/2026/Conference/Submission15968/Reviewer_UHKa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15968/Reviewer_UHKa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15968/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762408767445, "cdate": 1762408767445, "tmdate": 1762926175656, "mdate": 1762926175656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a negotiation-based framework for safe multi-objective reinforcement learning, where efficiency and safety are modeled as two decision parties."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of viewing safety vs. performance as a negotiation problem is interesting and conceptually novel.\n\n2. The paper is well-written and structured, with detailed experimental comparisons.\n\n3. Includes ablation and multiple environments."}, "weaknesses": {"value": "1. The paper is technically dense and hard to follow for readers unfamiliar with MORL or NSGA-II.\n\n2. The theoretical justification for why ε-dominance negotiation leads to better Pareto solutions is weak.\n\n3. The experiments, though numerous, are limited to simulated MuJoCo control tasks and do not test broader generality.\n\n4. The innovation appears to be an incremental combination of existing techniques (Pareto negotiation + NSGA-II) rather than a fundamental new theory."}, "questions": {"value": "1. How does the negotiation mechanism differ in practice from traditional Pareto dominance used in NSGA-II?\n\n2. Is there any formal analysis or convergence guarantee that supports the ε-dominance negotiation mechanism?\n\n3. Could the authors provide empirical evidence (e.g., ablation on ε decay rate or negotiation threshold) to show its quantitative effect?\n\n4. How well would the proposed framework scale to higher-dimensional or discrete-action environments?\n\n5. Could the authors clarify what is fundamentally new beyond integrating NSGA-II with multi-party negotiation?\n\n6. Are there any new theoretical insights or properties that emerge uniquely from the proposed formulation?\n\n7. Could the authors include statistical tests or confidence intervals to confirm the significance of improvements?\n\n8. Why does the proposed method have weaker MPSP (diversity) performance, and how could that be improved?\n\n9. Can the authors analyze trade-offs between global convergence and solution diversity more clearly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VZYYmq5a83", "forum": "4lAQBVU2Za", "replyto": "4lAQBVU2Za", "signatures": ["ICLR.cc/2026/Conference/Submission15968/Reviewer_NTqb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15968/Reviewer_NTqb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15968/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762562134863, "cdate": 1762562134863, "tmdate": 1762926174910, "mdate": 1762926174910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}