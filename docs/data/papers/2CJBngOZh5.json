{"id": "2CJBngOZh5", "number": 9741, "cdate": 1758137299257, "mdate": 1759897701192, "content": {"title": "Preferential dynamic modeling with forward-backward smoothing", "abstract": "Estimating a secondary signal (e.g., behavior) from neural activity over time is central to both causal online decoding and non-causal offline inference in neuroscience. Existing two-signal latent state-space modeling methods typically either support causal prediction of the secondary signal from the primary signal or non-causal inference (smoothing), but rarely both; here we extend one analytical linear method (PSID) and one nonlinear deep learning method (DPAD) beyond causal prediction to also support non-causal inference, yielding a more universally applicable family of methods for diverse applications. We provide theoretical derivations extending PSID to enable optimal filtering (i.e., estimate sample $k$ given samples $1$ to $k$) and optimal smoothing (i.e., estimate sample k given samples $1$ to $N$, where $N > k$) of the secondary signal. We show that, in the PSID setting, the presence of a secondary signal increases identifiability. This allows us to uniquely learn the quantities needed for the optimal Kalman update via a reduced-rank regression step that augments the standard SVD-based PSID algorithm, yielding our first contribution, PSID with filtering. We next design a forward-backward construction for smoothing, yielding our second contribution, PSID with smoothing. For nonlinear prioritized modeling, we extend DPAD to a bidirectional variant that combines forward and backward hidden states at readout to perform smoothing, yielding our third contribution, DPAD with smoothing. In simulations, we validate that PSID with filtering and smoothing reach ideal performance. In non-human primate motor cortex data, PSID with smoothing consistently improves over PSID with filtering, which improves over one-step-ahead prediction with standard PSID, with all three cases outperforming a linear neural dynamic modeling (NDM) baseline. Finally, we test DPAD with smoothing on three Neural Latents Benchmark (NLB) datasets, where it achieves the top behavior-decoding result on at least one dataset and near-top performance in behavior decoding and held-out neural prediction on all three. Together, these methods form a family with wide-ranging applications, from causal online decoding to offline inference, in both linear and nonlinear settings.", "tldr": "", "keywords": ["Subspace identification", "Dynamical modeling", "Neuroscience", "RNN", "Supervised learning"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff49552c1704e62be9f48f4a1aa6648f9338f669.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper extends PSID (linear) and DPAD (nonlinear) to support both filtering and smoothing of a secondary signal from a primary time series, thereby bridging causal and non-causal inference in two-signal system identification. The authors provide analytical derivations for the linear case and introduce a bidirectional RNN variant for the nonlinear case. Experiments on simulations, primate motor cortex data, and the Neural Latents Benchmark (NLB) demonstrate performance competitive with top-ranking methods such as Ctrl-TNDM and LangevinFlow, achieving top behavior-decoding accuracy on the MC_RTT dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well written, clearly structured, and easy to follow.\n    \n- Strong empirical validation across multiple datasets and settings, with consistent improvements over relevant baselines."}, "weaknesses": {"value": "- Theoretical contributions are presented in a textbook style, making it hard to separate novel insights from background material. Appendix e.g. lacks formal structuring (e.g., propositions or theorems) that would clarify assumptions and originality."}, "questions": {"value": "- Can the authors summarize their key theoretical contributions more formally (e.g., in a compact proposition) to distinguish them from standard Kalman filtering results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jwaqJ2QZ12", "forum": "2CJBngOZh5", "replyto": "2CJBngOZh5", "signatures": ["ICLR.cc/2026/Conference/Submission9741/Reviewer_55zD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9741/Reviewer_55zD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928849515, "cdate": 1761928849515, "tmdate": 1762921237316, "mdate": 1762921237316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the paper proposes the framework extensions of two existing dual-signal latent state-space modeling methods. The first is a linear method named Preferential Subspace Identification (PSID), and the second one is the non-linear Dynamic Prioritized Analysis of Dynamics (DPAD). The goal of this proposed framework is to enable optimal filtering and smoothing of a secondary signal e.g., behavior, $z_k from the primary neural activities. As for PSID, the authors use the Reduced Rank Regression (RRR) step to do the filtering extension. They also extend the PSID with forward-backward smoothing. Then for the DPAD, they use the bidirectional RNN (Bi-RNN) deep model architecture to implement the smoothing. By comparison, the original PSID and DPAD methods are focused on one-step-ahead prediction.\n\nFor the experiment parts, the authors validate their proposed framework with simulation data and the widely-adopted NLB benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper writing flow is concrete and easy-to-follow.\n2. The paper provides a comprehensive framework addressing the filtering and smoothing tasks in two wide-adopted models, linear (PSID) and nonlinear (DPAD). These problems are actually quite important in computational neural data modeling."}, "weaknesses": {"value": "1. The overall framework in the submitted manuscripts is like a combination of the incremental model enhancement of two existing methods, which is pretty heuristic and intuitive. There is even no name of the proposed framework.\n2. Besides the a bit engineering method combinations, there seems to be no real algorithm novelty to the neural and behavior analysis community.\n3. Some components of the proposed framework, like bi-directional RNNs, are actually a bit old-fashioned. The framework overall is actually looks like a theoretically unvalidated construction."}, "questions": {"value": "I have no more questions, other concerns please see my weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GBdzI59K8o", "forum": "2CJBngOZh5", "replyto": "2CJBngOZh5", "signatures": ["ICLR.cc/2026/Conference/Submission9741/Reviewer_24bz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9741/Reviewer_24bz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967533182, "cdate": 1761967533182, "tmdate": 1762921236815, "mdate": 1762921236815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops methods for filtering and smoothing two-signal SSMs, i.e., where observable $y\\_k$ and $z\\_k$ depend on latent $x\\_k$ and the goal is to predict or infer $z$ from $y$. The proposed methods extend linear (PSID) and nonlinear (DPAD) methods to smoothing using forward-backward approaches. For PSID, they also add a method for system identification for optimal filtering that learns only the part of the $x,y$ dynamics that is identifiable from $z$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strong theoretical grounding.\nGood experimental results on synthetic data and on neural decoding tasks against established baselines."}, "weaknesses": {"value": "The paper claims the extensions of PSID have certain optimality properties but this is not proved. It is claimed that solving eq 7 (setting aside sampling error from a finite training set) yields the optimal filter for predicting $z_k$ from $y\\_{1:k}$, but this is not quite proved. A similar question applies to PSID with smoothing, but for that algorithm there is also the more primary question of whether it yields the optimal (i.e., Bayesian or minimum-variance unbiased) estimate of $z\\_k$ when the system parameters are known. I expect this would be easy to prove and would be important to add to the paper."}, "questions": {"value": "I work a lot with SSMs and Kalman-type methods but had not seen models where the system and observation noise are dependent. Can you add a brief explanation or motivating example for why we should expect such dependence?\n\nI was unfamiliar with DPAD and wonder whether it could benefit from tracking uncertainty ($P$). If I understand correctly the only latent state in the RNN is $x_k$, which is a mean estimate, whereas optimal filtering also requires covariance. Can the model be extended in this way, or does it somehow track uncertainty as is?\n\neq 12a: $A$ should be $I$\n\nDuplicate \\labels for eqs 19 and 25?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qA7b0qjpJZ", "forum": "2CJBngOZh5", "replyto": "2CJBngOZh5", "signatures": ["ICLR.cc/2026/Conference/Submission9741/Reviewer_2Qcp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9741/Reviewer_2Qcp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976060919, "cdate": 1761976060919, "tmdate": 1762921236209, "mdate": 1762921236209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends PSID (linear) and DPAD (nonlinear) methods to support optimal filtering and smoothing of a secondary signal (e.g., behavior) from a primary neural signal. The authors derive theoretical updates, introduce a forward-backward smoothing framework, and validate the methods on both simulated and real neural datasets, showing improved decoding accuracy over existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Clear problem formulation for prediction, filtering, and smoothing in two-signal settings.\n* Solid theoretical derivation for extending PSID to filtering via reduced-rank regression.\n* Comprehensive evaluations on simulations, primate data, and NLB benchmarks.\n* Unified framework covering both linear and nonlinear models for causal and non-causal inference."}, "weaknesses": {"value": "* There seems to be no discussion on computational cost or real-time applicability.\n* The current abstract is a bit long."}, "questions": {"value": "* Whether the forward and backward RNNs can be replaced with GRU or LSTM, since they could provide more smoothness.\n* What's the definition of $\\\\boldsymbol \\\\epsilon_k$? Does it have to be Gaussian?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EnwYA1vnUa", "forum": "2CJBngOZh5", "replyto": "2CJBngOZh5", "signatures": ["ICLR.cc/2026/Conference/Submission9741/Reviewer_KZ2E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9741/Reviewer_KZ2E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982108668, "cdate": 1761982108668, "tmdate": 1762921235175, "mdate": 1762921235175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}