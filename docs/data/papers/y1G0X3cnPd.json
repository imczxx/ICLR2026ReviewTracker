{"id": "y1G0X3cnPd", "number": 14602, "cdate": 1758239725021, "mdate": 1759897360066, "content": {"title": "CHyLL: Learning Continuous Neural Representations of Hybrid Systems", "abstract": "Learning the flows of hybrid systems that have both continuous and discrete time\ndynamics is challenging. The existing method learns the dynamics in each discrete\nmode, which suffers from the combination of mode switching and discontinuities\nin the flows. In this work, we propose CHyLL (Continuous Hybrid System\nLearning in Latent Space), which learns a continuous neural representation of a\nhybrid system without trajectory segmentation, event functions, or mode switching.\nThe key insight of CHyLL is that the reset map glues the state space at the\nguard surface, reformulating the state space as a piecewise smooth quotient manifold\nwhere the flow becomes spatially continuous. Building upon these insights\nand the embedding theorems grounded in differential topology, CHyLL concurrently\nlearns a singularity-free neural embedding in a higher-dimensional space\nand the continuous flow in it. We showcase that CHyLL can accurately predict\nthe flow of hybrid systems with superior accuracy and identify the topological invariants\nof the hybrid systems. Finally, we apply CHyLL to the stochastic optimal\ncontrol problem.", "tldr": "We learn discontinuous flow in continuous space with gurantees.", "keywords": ["Hybrid Systems", "Topology", "Geometry", "Neural ODE"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48a9c67907f1722e0b949cea456e5cf9a7347fdb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose chyLL, a method to learn continuous representations of hybrid systems in latent space. The method ensures the representations become continuous in a higher-dimensional latent space, then fit a latent ode there and decode back, avoiding mode labels, event functions, or segmentation. The contribution is the gluing/conformal/anticollapse losses to learn this manifold from time-series alone, with demos (bouncing ball, torus/klein bottle topology checks, and a control task)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clear and well-motivated, and the pipeline and objectives are easy to follow. \nThe route explored is worthwhile as a method to modeling hybrid dynamics that generalizes across systems. \nThe proposal of learning a glued quotient manifold for hybrids via gluing + conformal losses is nice, and the training setup seems sensible (e.g. curriculum, anti-collapse, LM projection)."}, "weaknesses": {"value": "1. Although latent encoding for ODEs is not a new avenue, the quotient/gluing idea is a nice addition, but as the authors hint at in the paper, learning the correct 'glued' space might be hard in principle, and the lack of guarantees can be concerning. \n\n2. The experimental scope is a little narrow, with toy problems/examples, and only a few comparative methods. The results for the ball juggling with MPPI experiment also show a deep Koopman baseline achieving a lower mean tracking cost, without much of a detailed explanation. \n\n3. It's hard to comment on the scalability and robustness of the approach since there are no results under sensor noise/partial observability, many-guard/mode systems, or higher-dimensional robotics experiments."}, "questions": {"value": "2. The authors should expand further on the limitation of the proposed method, and its failure mode, with respect to existing literature.\n\n1. Can you provide any diagnostic or bound (e.g., jump norms across detected guards, encoder/decoder Jacobian conditioning near resets) that indicate the learned latent is truly continuous, or conditions under which it fails?\n\n2. How sensitive is performance to the gluing/conformal/anti-collapse weights and latent dimension? It would be valuable to include a sweep or at least failure modes when turning each off.\n\n3. Can you expand further on why deep Koopman wins on mean cost, and whether CHyLL improves with different horizon/MPPI settings or controller?\n\n4. How do you think the method would behave under more realistic sensor noise or partial observability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "w13jY0TYEG", "forum": "y1G0X3cnPd", "replyto": "y1G0X3cnPd", "signatures": ["ICLR.cc/2026/Conference/Submission14602/Reviewer_wC3u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14602/Reviewer_wC3u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761327934306, "cdate": 1761327934306, "tmdate": 1762924984423, "mdate": 1762924984423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CHyLL presents a groundbreaking method for learning hybrid system dynamics directly from time-series data, eliminating the need for mode switching or event detection. Its core innovation lies in reformulating the discontinuous state space into a continuous quotient manifold—the hybrifold—by topologically gluing guard surfaces via reset maps. A dual-phase training strategy separately optimizes the continuous encoder/flow and the discontinuous decoder. Evaluations on systems like the bouncing ball and torus show CHyLL's superior prediction accuracy and its ability to identify correct topological invariants."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This article is quite abstract, drawing on profound mathematical theories to guide the methodology design. It addresses a very practical problem and has achieved promising results in the selected experimental examples."}, "weaknesses": {"value": "1. Although this article cites many theorems and employs sophisticated mathematical frameworks, its contributions are primarily concentrated on methodological design, with the theoretical contributions not being sufficiently sound. Perhaps the authors could further incorporate theoretical analysis or proofs regarding their methods (such as the design of the loss Eq. 4).\n2. The experiments in the paper all use simulated data from simple examples. There is an absence of benchmarking on publicly available real-world datasets."}, "questions": {"value": "1. Although profound mathematical theorems are cited in the paper, I have concerns regarding their rigor. For instance, Theorem 1 states that the manifold is piecewise smooth, while Theorem 2 requires the manifold to be C^r—what is the relationship between these two conditions? Additionally, I seem to be unable to find the precise definition of 'piecewise smooth'.\n2. Could there be performance evaluations on some real-world public datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LYinebh4HZ", "forum": "y1G0X3cnPd", "replyto": "y1G0X3cnPd", "signatures": ["ICLR.cc/2026/Conference/Submission14602/Reviewer_EUQm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14602/Reviewer_EUQm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747518418, "cdate": 1761747518418, "tmdate": 1762924983913, "mdate": 1762924983913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of learning hybrid dynamical systems, that is the systems combining continuous flows and discrete mode switches. Diversly from existing approaches, authors propose method CHyLL (Continuous Hybrid System Learning in Latent Space) that   learns directly from time-series data avoiding major scalability bottlenecks of explicit segmentation or event detection. The central insight is topological: hybrid systems’ discontinuities arising from mode switching can be “glued” to form a piecewise smooth quotient manifold, on which the overall flow becomes spatially continuous. CHyLL operationalizes this known theoretical result, by: (1) learning a singularity-free neural embedding of the quotient manifold in a higher-dimensional latent space, (2) concurrently learning the continuous flow within this embedded space, and (3) enabling prediction and control of hybrid dynamics without explicitly modeling discrete modes or resets. Experiments show that CHyLL can reconstruct hybrid system flows with high accuracy, recover topological invariants, and be applied to stochastic optimal control problems."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "(1) I find paper generally well written, besides few minor issues listed bellow. I like the clarity of motivation, transparency of novelties and  appreciate the balance in presenting both intuition and technical complexity.\n\n(2) Up to my knowledge, CHyLL appears genuinely novel in its topological formulation of hybrid system learning. The reinterpretation of mode switches as gluing operations forming a quotient manifold and the learned embedding that makes hybrid flows continuous is non-standard path. While some ideas overlap with Neural ODE extensions and manifold-learning methods, no prior work explicitly connects hybrid system topology, latent manifold embedding, and continuous neural flow learning in a single approach.\n\n(3) I believe that making the embedding theorem (Simic et al. 2005) from differential topology operational within Neural ODEs can inspire new methods that up to known were not able to successfully tackle hybrid systems.\n\n(4) Experimental setup is generally appropriate."}, "weaknesses": {"value": "(1) Presentation:\n- Section 3: I find that introduction of main concepts like guards and resets should be smoother. Before jumping to formal Definitions, authors can use Figure 2 to introduce these concept first informally to build the intuition. e.g. in simple terms, what is the role of $q$.\n- Section 2: I feel that related work section would be easier to parse after the intuition and notation on hybrid systems is current Section 3.\n- Levenberg-Marquardt: Due to unclear Experimental conclusions (lack of std),  this aspect can be either avoided or emphasised. Computational overhead of using it should be discussed. \n\n(2) Experiments: Standard deviations across trials is missing in experiments of Section 6.2. This makes it hard to conclude which version of the proposed method (w/o LM) is better performing, and diminishes the impact of the conclusions. \n \n(3) Minor:\n- should read \"data\" in line 182\n- \"be\" lacking in line 189\n-  notation should be ${\\cal L}_c(\\theta)$ in line 299\n\nWhile my current score reflects the above weaknesses, I am happy to revise it if the rebuttal is successful."}, "questions": {"value": "Given the good performance of DynamicAE of Lusch et al. 2018 in experiment of Section 6.3, what do authors think of adding additional  discussion on combining CHyLL with Koopman-based method. Namely, instead of using parametrising the vector filed and using Eq. (5) to evolve the latent dynamics, Koopman approach would learn linear evolution. Also, since DynamicAE is known to fail in modelling evolution in longer time-horizon, one can think of combining the CHyLL with representation learning for Koopman/Transfer operators, e.g. of references bellow, to learn appropriate representations of hybrid systems. \n\nHan et al. Deep learning of Koopman representation for control, IEEE CDC2020\nKostic et al. Learning invariant representations of time-homogeneous stochastic dynamical systems, ICLR2024\nKostic et al. Neural conditional probability for uncertainty quantification, NeurIPS2024\nJeong et al. Efficient Parametric SVD of Koopman Operator for Stochastic Dynamical Systems, arXiv preprint 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nadNHlWlQZ", "forum": "y1G0X3cnPd", "replyto": "y1G0X3cnPd", "signatures": ["ICLR.cc/2026/Conference/Submission14602/Reviewer_488A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14602/Reviewer_488A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906112657, "cdate": 1761906112657, "tmdate": 1762924983533, "mdate": 1762924983533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}