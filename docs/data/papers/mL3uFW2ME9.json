{"id": "mL3uFW2ME9", "number": 20703, "cdate": 1758309186427, "mdate": 1763702766119, "content": {"title": "The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration", "abstract": "As large language models (LLMs) become integral to multi-agent systems, new privacy risks emerge that extend beyond memorization, direct inference, or single-turn evaluations. In particular, seemingly innocuous responses, when composed across interactions, can cumulatively enable adversaries to recover sensitive information, a phenomenon we term compositional privacy leakage. \nWe present the first systematic study of such compositional privacy leaks and possible mitigation methods in multi-agent LLM systems. First, we develop a framework that models how auxiliary knowledge and agent interactions jointly amplify privacy risks, even when each response is benign in isolation. Next, to mitigate this, we propose and evaluate two defense strategies: (1) Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent by anticipating how their outputs may be exploited by adversaries,\nand (2) Collaborative Consensus Defense (CoDef), where responder agents collaborate with peers who vote based on a shared aggregated state to restrict sensitive information spread.\nCrucially, we balance our evaluation across compositions that expose sensitive information and compositions that yield benign inferences.\nOur experiments quantify how these defense strategies differ in balancing the privacy-utility trade-off. \nWe find that while chain-of-thought alone offers limited protection to leakage (39% sensitive blocking rate), our ToM defense substantially improves sensitive query blocking (up to 97%) but can reduce benign task success. CoDef achieves the best balance, yielding the highest Balanced Outcome (79.8%), highlighting the benefit of combining explicit reasoning with defender collaboration. \nTogether, our results expose a new class of risks in collaborative LLM deployments and provide actionable insights for designing safeguards against compositional, context-driven privacy leakage.", "tldr": "", "keywords": ["Multi-agent privacy", "Compositional attacks", "Collaboration", "Defense", "LLM Agent"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/25d6a2b398550a3d3063e384a2ac2b8d8cd5e023.pdf", "supplementary_material": "/attachment/a15cbec88649be859c2aabbb8d2e669dd140bb03.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the concept of compositional privacy leakage in multi-agent LLM systems, where sensitive information emerges through the combination of individually innocuous outputs from multiple agents. The authors develop a systematic evaluation framework modeling adversary-defender interactions as a POMDP, where defenders hold partial, non-sensitive data that can be composed by adversaries to infer private attributes. Two defense mechanisms are proposed: (1) Theory-of-Mind (ToM) Defense, where defenders anticipate adversarial intent by simulating the questioner's knowledge state, and (2) Collaborative Consensus Defense (CoDef), where defenders vote based on shared aggregated state. Experiments across 119 scenarios with multiple LLMs (Qwen3-32B, Gemini-2.5-pro, GPT-5) show that while baseline CoT provides limited protection, ToM substantially improves blocking but reduces benign utility, whereas CoDef achieves better balance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies compositional privacy leakage as a distinct threat in multi-agent systems that extends beyond memorization or single-agent risks. This has high practical value.\n2. The evaluation setup is well designed. By providing adversaries with optimal plans $P^*$ and measuring plan execution success separately from inference accuracy, the authors isolate whether privacy failures arise from information flow versus reasoning errors. This methodological rigor enables objective comparison across defense mechanisms.\n3. The findings clearly demonstrate that simple CoT reasoning is insufficient for defending against compositional attacks, and that collaborative defense mechanisms provide superior privacy-utility trade-offs compared to single-agent ToM reasoning. These are valuable insights on guiding the development of safe multi-agent systems."}, "weaknesses": {"value": "1. The data construction process is unclear. The paper writes \"We construct structured scenarios specifying entities, private data, sensitive targets, and adversary plans\", but provides insufficient detail about this critical process. CoT results is bad might because the model does not think these targets are sensitive. Section 3 needs substantial expansion on scenario generation methodology, including examples of how sensitive vs. benign targets are differentiated.\n2. The analysis is somewhat superficial. While the paper demonstrates that certain defenses fail, it provides limited insight into why they fail. Is the failure due to lack of privacy awareness about compositional leakage? Is it due to insufficient information for reasoning (especially for non-collaborative methods)? For example, in a related study [1], they found models can identify privacy-sensitive cases but fail to act accordingly in agentic tasks due to complicated objectives. I think similar analysis is needed here for shedding insights on how to improve these models.\n\n\n[1] PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action, Shao et al., NeurIPS 2024"}, "questions": {"value": "1. How are benign and sensitive queries being sourced?\n2. How are sensitive fragment combinations computed for \"CoT + Sensitive Set\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TZD8jGcYms", "forum": "mL3uFW2ME9", "replyto": "mL3uFW2ME9", "signatures": ["ICLR.cc/2026/Conference/Submission20703/Reviewer_8tHH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20703/Reviewer_8tHH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883321025, "cdate": 1761883321025, "tmdate": 1762934079386, "mdate": 1762934079386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank the reviewers for their thoughtful and constructive feedback. We appreciate their recognition of the timeliness of studying **“compositional privacy risks in multi-agent systems, an important but understudied and practically significant problem”** (SSXU, XcEa, 8tHH). Reviewers highlighted the clarity and novelty of the attack formulation, emphasizing that **“the contextual-privacy–style leakage mechanism is well defined, intuitive, and empirically strong”** (mhXs, XcEa, 8tHH). They also appreciated our formalization of compositional privacy leakage and the introduction of a systematic multi-agent evaluation framework, which they found **“useful for structuring future investigations of inference-time privacy risks”** (mhXs, XcEa, 8tHH). \nReviewers further noted that our proposed defenses, Theory-of-Mind reasoning and Collaborative Consensus Defense (CoDef), represent **“coherent and meaningful approaches to mitigating these attacks”** (mhXs, XcEa). They highlighted that CoDef, in particular, **“achieves a compelling privacy-utility balance, offering substantial improvements in blocking sensitive queries while maintaining strong benign response rates”** (XcEa, 8tHH). Additionally, we are thankful that they appreciated the **“clear experimental design, including the separation of planning and inference accuracy and the multi-model evaluation”** (mhXs, 8tHH). We also thank the reviewers for acknowledging the contribution of **“categorizing reasoning-depth levels and demonstrating their correlation with improved privacy protection”** (XcEa, 8tHH)."}}, "id": "hgrqIbmSCj", "forum": "mL3uFW2ME9", "replyto": "mL3uFW2ME9", "signatures": ["ICLR.cc/2026/Conference/Submission20703/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20703/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20703/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763697735467, "cdate": 1763697735467, "tmdate": 1763697735467, "mdate": 1763697735467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates an emerging inference-time privacy issue in multi-agent systems, which is that sharing seemingly innocuous response during agent interactions can allow an adversary to aggregate them and reveal sensitive information. They term this phenomenon \"compositional privacy leakage,\" formally defined it, and proposed two mitigations: 1) ToM defense, where defender agents try to infer the agent's malicious intent on its own; 2) Collaborative Consensus Defense (CoDef), where defender communicates query and answer histories with each other and collectively vote for whether to block a request. They evaluated the baseline CoT methods as well as the proposed defense methods, showing that ToM preserves privacy at the cost of denying benign requests, while CoDef achieves a better balance. They also categorized the reasoning depth into four levels and found a correlation between richer reasoning and stronger compositional privacy protection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies an important privacy issue of the compositional privacy leakage, similar to the concept of quasi-identifier, in the context of multi-agent systems. It contributes to research on inference-time privacy leakage in multi-agent systems, which is an understudied area.\n- The paper provides a formal definition of the compositional privacy leakage problem, and proposes two defense methods. They both more effectively blocked the sensitive requests. The CoDef method substantially improved the blocking success rate while maintaining a similar level of benign success rate to the baselines."}, "weaknesses": {"value": "- In the threat model and the defense methods, the defender agents and adversary agents are explicitly separated. There seems to be an assumption that the defender agents are trusted by all other defender agents, and another agent is explicitly regarded as an adversary. This does not reflect the risks in a realistic multi-agent setup, where any agent could be compromised. Making this assumption may introduce bias that boost the performance in this task, while lacking applicability in real settings where such identities can't be assumed.\n- The current description of compositional privacy leakage seems to emphasize scenarios where sensitive attributes only become visible after multiple structured datasets are joined. For example, linking tables on shared IDs or quasi identifiers, similar to classic reidentification attacks where an anonymized dataset is deanonymized using auxiliary public records. That framing is important but not especially new. It also underplays what is unique about LLMs. LLMs can compose unstructured, semantically rich evidence such as emails, documents, chat logs, and meeting notes, and then infer sensitive facts about specific people without any explicit shared keys. In other words, the leakage is not just a matter of table A being joined with table B. It is that the model can synthesize implicit meaning across heterogeneous text and surface latent information. The latter feels like the real frontier risk, and it is not fully captured by the current definition."}, "questions": {"value": "- Why are the benign success rates also low in the several CoT baselines (some even lower than the defense methods)?\n- Can you clarify what types of compositional privacy leakage are captured in your evaluation?\n- Can you address the question regarding the assumption of the roles of defender and adversary, and how it might affect the validity of your results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hMiBxOqnDG", "forum": "mL3uFW2ME9", "replyto": "mL3uFW2ME9", "signatures": ["ICLR.cc/2026/Conference/Submission20703/Reviewer_XcEa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20703/Reviewer_XcEa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961871308, "cdate": 1761961871308, "tmdate": 1762934078728, "mdate": 1762934078728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper defines privacy leakage that accumulates over time through benign outputs and can be inferred by an adversary. The paper uses a game setup and discusses different defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The attack is clear and works really well, it is a novel result in the area of Contextual Privacy\n- The additional formulation is useful to enable a systematic framework \n- Defenses introduce coherent methods to mitigate the attacks\n- Evaluation is clear and uses synthetic data."}, "weaknesses": {"value": "- Synthetic data might not fully represent situations that will occur in real life with the agents.\n- The defense mechanisms might still be vulnerable to context hijacking attacks\n- I think this can be framed as model-level defenses which is a valuable contribution but yet (as all defenses) has its own limitations, it would be great to have them addressed in the paper\n- Also, there is this neighboring domain of LLM censorship [1] that is worth discussing. \n- An adversary could also cause \"overthinking\" attacks to slow down the system. \n- It might be interesting to see the overhead of preventing the leakage and ways to stop early\n\n[1] LLM Censorship: A Machine Learning Challenge or a Computer Security Problem? [icml'24]"}, "questions": {"value": "Overall, paper is great, addressing weakness brought above would be the most helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8CeHt8Se2h", "forum": "mL3uFW2ME9", "replyto": "mL3uFW2ME9", "signatures": ["ICLR.cc/2026/Conference/Submission20703/Reviewer_mhXs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20703/Reviewer_mhXs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976070352, "cdate": 1761976070352, "tmdate": 1762934077957, "mdate": 1762934077957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses compositional privacy leakage in multi-agent systems, where individual outputs from multiple agents can collectively reveal sensitive information. The authors formalize this risk and evaluate two defense strategies — a Theory-of-Mind (ToM) defense, in which agents model an adversary’s intent, and a Collaborative Consensus Defense (CoDef), where agents jointly decide whether to respond to a query."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Compositional privacy risk is an important topic in privacy."}, "weaknesses": {"value": "Although I understand what the authors are *trying* to convey in this paper, the evaluation setup feels unrealistic and overly artificial. The paper also gives the impression of being hastily organized and written, resulting in missing or unclear details. The assumption of a “black-box adversary with auxiliary knowledge” is underspecified and vaguely defined — does this auxiliary knowledge include personal identifiers? If so, I disagree with the authors’ characterization of such data as public information, as it makes the overall setup overly simplified and less credible.\n\nMoreover, several examples in the paper suggest that the defenders already hold and disclose highly sensitive information that should never be shared with any external entity. For instance, providing an employee ID–to–(employee name, department) mapping from personal records to the attacker would itself constitute a serious privacy violation. Many of the defenders’ data sources are already sensitive by nature, which further undermines the realism of the evaluation.\n\nIn addition, the paper lacks a clear explanation of how leakage is identified or measured. The appendix is also difficult to follow, with duplicated sections and disorganized ordering. As a result, I find it challenging to fully understand the results or accept the validity of such an artificial experimental setup.\n\nThere is no clear winner for defense. The results for the defense methods are mixed: Self-Voting performs best on Qwen, CoDef achieves the highest performance on Gemini-2.5-pro, and ToM performs best on GPT-5."}, "questions": {"value": "- Does ToM defenses sacrificing benign success mean the LLM is pessimistic in predicting the other’s intents?\n- Is the leakage detection based on string matching or entailment? What does the s* look like?\n- How do humans perform on this task?\n- Line 144: What is A_j and A_i?\n- If the correct plan is already provided, what is the attacker’s role? Do they simply generate natural language queries based on that plan?\n- How do you ensure the information held by each defender are not sensitive?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3vIXLFhewI", "forum": "mL3uFW2ME9", "replyto": "mL3uFW2ME9", "signatures": ["ICLR.cc/2026/Conference/Submission20703/Reviewer_SSXU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20703/Reviewer_SSXU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992506430, "cdate": 1761992506430, "tmdate": 1762934077536, "mdate": 1762934077536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}