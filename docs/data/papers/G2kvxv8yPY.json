{"id": "G2kvxv8yPY", "number": 1357, "cdate": 1756875422715, "mdate": 1759898213176, "content": {"title": "A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model", "abstract": "We propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs). Our algorithms are based on a hybrid exploration-generative reinforcement learning (RL) model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion, i.e., by having access to a \"simulator.\" By employing known classical and new quantum algorithms for approximating optimal policies under a generative model within our learning algorithms, we show that it is possible to avoid several paradigms from RL like \"optimism in the face of uncertainty\" and \"posterior sampling\" and instead compute and use optimal policies directly, which yields better regret bounds compared to previous works. For finite-horizon MDPs, our quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps $T$, thus breaking the $O(\\sqrt{T})$ classical barrier. This matches the time dependence of the prior quantum works of Ganguly et al. (arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other parameters like state space size $S$ and action space size $A$. For infinite-horizon MDPs, our classical and quantum bounds still maintain the $O(\\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we propose a novel measure of regret for infinite-horizon MDPs with respect to which our quantum algorithms have $\\operatorname{poly}\\log{T}$ regret, exponentially better compared to their classical counterpart. Finally, we generalise all of our results to compact state spaces.", "tldr": "We propose and formalise a hybrid exploration-generative reinforcement learning model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion. This allows improve regret bounds.", "keywords": ["reinforcement learning", "Markov decision process", "generative model", "quantum computation", "finite-horizon", "infinite-horizon"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58a0f45baa537de1b01494f71a227d20e8754fc0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The manuscript deals with regret bounds of online RL in MDPs with finite horizons and MDPs with infinite horizons. Classical and quantum algorithms in the special case with oracles are considered. This is a purely theoretical work, covering a total of 49 pages."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Very careful preparation with comprehensive discussion of the literature.\n* Innovative approach to this established research topic.\n\nFurther comments:\\\nAlthough it is unusual to mention selected publications in the abstract, I think this is good here because it makes it very concrete and carefully presented.\n\nThe use of many footnotes is rather unusual. However, I think it fits in with the very careful style of the work.\n\nOne could criticize the enormous length of the appendix, but I think it is well done, because the main text can be understood well without the appendix, and the appendix provides useful additional information."}, "weaknesses": {"value": "I can't identify any clear weaknesses.\n\nPerhaps your own contribution could be mentioned in one central place and described in more concrete terms there.\n\nFurther comments:\n\nIn “One of the most famous measures is that of regret,” I don't like the word “famous.”\n\nSimilarly, in “The secret of the improved performance,” I don't like the word “secret.”\n\nIt's probably a matter of taste, but I think “Conclusion” is more accurate than “Conclusions.”\n\n„a RL“ -> „an RL“"}, "questions": {"value": "* Which relevance do the results have for further research?\n* Which relevance could the results potentially have for future applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AmBpuFtdBN", "forum": "G2kvxv8yPY", "replyto": "G2kvxv8yPY", "signatures": ["ICLR.cc/2026/Conference/Submission1357/Reviewer_qj3A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1357/Reviewer_qj3A"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760989062418, "cdate": 1760989062418, "tmdate": 1762915746742, "mdate": 1762915746742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes quantum and classical algorithms for online reinforcement learning in both finite-horizon and infinite-horizon MDPs. The authors design quantum versions of variance-reduced backward induction and robust value iteration, achieving query complexities that improve the best classical bounds by factors of $\\sqrt{A}$ and smaller H-dependence. They introduce an online “exploration–generative” protocol and prove regret bounds that are minimax-optimal for classical algorithms and enjoy an exponential-in-T advantage for a newly defined expected regret when quantum oracles are used. Results are extended to continuous state spaces via Hölder-smooth discretization."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper provides solid theoretical proofs for its results. \n2.\tFor the online learning problem of MDPs, when we introduce quantum algorithm, it is difficult to define proper regret. The authors introduce a novel model with classical exploration phases and classical/quantum generative phases to solve this difficulty."}, "weaknesses": {"value": "1.\tIn section 2 (to compute optimal policies), the authors consider undiscounted version of MDPs (both finite-horizon and infinite-horizon). In my understanding, the discounted version is more important and the quantum algorithm in this version has already been proposed. The motivation and technique challenges of the undiscounted version are not clearly explained. \n2.\tIn section 3 (online learning version), the authors introduce a novel model which splits the interaction into two types of phases: classical exploration phases and classical/quantum generative phases. Although this idea solves the difficulty to define proper regret of the quantum algorithm, it undermines the most critical challenge in online learning: the algorithm must balance exploration and exploitation. This significantly reduces the problem's difficulty. I don’t think it is reasonable. \n3.\tIn section 3, the authors require that the length of generative phase is at most $O(\\tau)$ where $\\tau$ is the length of the previous exploration phase. We must have some limitations of generative phase, but such a restriction appears arbitrary and lacks justification."}, "questions": {"value": "1.\tRelated to weakness 1: Is there any special motivation to consider quantum algorithm for finite-horizon and infinite-horizon undiscounted MDPs? Comparing to the quantum algorithms in previous work for infinite-horizon discounted MDPs, what is the main technique challenges for undiscounted version?\n2.\tRelated to weakness 2: In practical scenarios, is there any motivation for proposing such model?\n3.\tRelated to weakness 3: why should we require that the length of generative phase is at most $O(\\tau)$？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nVSGQN5F7N", "forum": "G2kvxv8yPY", "replyto": "G2kvxv8yPY", "signatures": ["ICLR.cc/2026/Conference/Submission1357/Reviewer_zBSn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1357/Reviewer_zBSn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745824285, "cdate": 1761745824285, "tmdate": 1762915746554, "mdate": 1762915746554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose both classical and quantum online algorithms for reinforcement learning (RL) tasks. These algorithms are rooted in an exploration-generative RL framework, wherein the agent can occasionally interact freely with the environment by utilizing access to a simulator of the environment. The authors claim that their proposed algorithms address and mitigate common RL challenges, such as “optimism in the face of uncertainty” and “posterior sampling.”"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and engaging. The discussed related work is extensive and provides a good overview on similar research."}, "weaknesses": {"value": "However, the applicability of the proposed algorithms is strictly confined to RL scenarios where a generative model with comprehensive knowledge of the state and action spaces—as well as the reward function—is available. Moreover, it assumes the transition probabilities can be queried through an oracle. Such oracles are not novel in RL research and are frequently associated with methods categorized as model-based RL. Past research has already explored quantum oracles extensively, particularly focusing on achieving quantum advantage via quantum sampling.\n\nA key point of concern is the lack of clarity regarding the benefits of separating the exploration phase (classical) from the policy learning phase (classical/quantum). If the agent has access to a flawless oracle, it is unclear why a classical exploration on the MDP (Markov Decision Process) would be necessary. Additionally, the rationale behind limiting access to the oracle (referred to as “the true MDP”) needs to be elaborated. For the paper to be more impactful, it should include a clear justification for the algorithm’s design choices as well as an explanation of how, in practical settings, the oracles representing the true MDP might be obtained.\n\nMinor Issues:\nThe citation format in the abstract, “Ganguly et al. (arXiv’23) and Zhong et al. (ICML’24),” is unconventional and should follow standard citation styles.\nPage 9: There is a typo in “a RL.” It should be corrected to “an RL.”"}, "questions": {"value": "Why is the access to the oracle limited?\nWhy is the oracle not used to learn an optimal policy offline?\nHow can in practical settings oracles representing the true MDP be obtained?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u2WOx2VDRr", "forum": "G2kvxv8yPY", "replyto": "G2kvxv8yPY", "signatures": ["ICLR.cc/2026/Conference/Submission1357/Reviewer_Mu5H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1357/Reviewer_Mu5H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837003835, "cdate": 1761837003835, "tmdate": 1762915746337, "mdate": 1762915746337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the context of quantum RL, this paper studies both computing near-optimal policies under a generative model for (generalized) tabular MDPs, and\nonline reinforcement learning (regret minimization) in finite-horizon and infinite-horizon average-reward MDPs. The basic setting for this generalized tabular MDP is that the state space $\\mathcal{X}$, although continuous, can be approximated with an $\\epsilon$-net by the Holder continuous condition, reducing the problem to a standard tabular RL.\n\nFor the generative model setting, this paper proposes a novel algorithm with a quantum maximum finding subroutine based on the classical value iteration algorithm, which reduces the sample complexity from $O(A)$ to $O(\\sqrt{A})$ in the finite horizon MDP. The usage of quantum mean estimation also provides a quadratic improvement with regards to $\\epsilon$, which has been investigated in the literature [1]. For infinite-horizon average-reward MDP, this paper revises the classical (extended) value iteration in favor of quantum mean estimation, and achieves a sample complexity of $\\tilde{O}(\\Lambda |\\mathcal{S}_n| \\sqrt{A} / (1 - \\nu) \\epsilon)$ in the case that the Bellman operator is a $\\nu$-contraction.\n\nFor the general online exploration setting, this paper studies the regret minimization with a new formulation. The whole exploration process is divided into two phase: the exploration phase and the generative phase. The agent is allowed to interact *freely* with the quantum MDP in the generative phase, as if there is a generative model and without incurring any regret. In the exploration phase, the agent executes current policy and incurs corresponding regret. The total length of a generative phase must be the same order of previous exploration phase. This paper proposes a unified algorithm for both finite horizon MDP and infinite horizon MDP, where the agent uses the algorithms of generative model setting to approximate the optimal policy in the generative phase, and the execute this policy in the exploration phase. The regret for the finite horizon setting is $\\tilde{O}(\\min(HSA, H^2S\\sqrt{A} \\log T))$, improving on the order of $S, A$ of previous works. The regret for the infinite horizon setting is $\\tilde{O}(\\Lambda \\sqrt{T} + \\Lambda S \\sqrt{A} \\log^2 T)$, where the term $\\Lambda \\sqrt{T}$ can be omitted if an expected regret is used so as to achieve the $\\mathrm{poly} \\log T$ regret.\n\n[1]. Zhong et al., Provably efficient exploration in quantum reinforcement learning with logarithmic worst-case regret."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The application of quantum computing (e.g., the quantum mean estimation subroutine) to online exploration of RL is an interesting and important problem. This paper pushes the boundary of this problem.\n\n2. As far as I know, this is the first paper investigating quantum RL in the setting of infinite-horizon average-reward MDP. It shows that the sample complexity can also be improved quadratically with the help of quantum mean estimation.\n\n3. The idea of applying quantum maximum finding subroutine in the hoeffding-style value iteration algorithm is novel, saving a $\\sqrt{A}$ in the regret bound and sample complexity."}, "weaknesses": {"value": "1. There is a major problem in the formulation of this \"exploration-generation\" two-phase procedure, which assumes that the agent can use the oracles as *a generative model* in the generation phase *without incurring any regret*. There are indeed many works in the literature of classical RL using this idea of \"lazy update\" to design sample-efficient algorithms such as [1, 2], but none of these works assume the access to a generative model nor assume the data collection phase incurs no regret. They can use this lazy update because *they use the data from the \"exploration\" phase to estimate the value function directly*, instead of collecting new data to estimate the value function in a new \"generation\" phase. In this work, however, the agent is able to use a generative model to collect grand new data in the generation phase, which means the agent is able to explore the MDP in an **arbitrary** state-action distribution. On the other hand, the core assumption of online RL for unknown environments is that the agent should create favorable state-action distribution themselves by taking good policies to find the high-rewarded state and action. The basic assumption here is, you can **NOT** collect any data from the states that you never know how to get there in an online exploration problem (e.g., see all of the references from line 365 to 367 in this paper), which is also the case for any real-world tasks. Therefore, there is a large gap between the real online exploration RL and the two-phase model proposed in this paper. All the results of Section 3 shall be RL with a generative model instead of online exploration.\n\n2. For the results of the quantum algorithms in the infinite horizon MDP, there is an extra contraction measure $\\nu$ of the Bellman operator in the sample complexity, which does not appear in classical RL literature like current SOTA [5]. The sample complexity bound has a $(1 - \\nu)^{-2}$ dependency for the classical setting and a dependency of $(1 - \\nu)^{-1}$ for the quantum setting. This term mainly comes from the pipeline of value iteration of Algorithm 5. What is the reason to introduce $\\nu$ here? Is it because if one uses a conventional decomposition of regret like [6] then an extra martingale term of $O(\\sqrt{T})$ will appear? This contraction term somehow makes it harder to evaluate the sample complexity bound of Result 2.\n\n[1]. Zhong et al., Provably efficient exploration in quantum reinforcement learning with logarithmic worst-case regret.\n\n[2]. Jacksch et al., Near-optimal Regret Bounds for Reinforcement Learning.\n\n[3]. Fruit et al., Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning.\n\n[4]. Ayoub et al., Model-Based Reinforcement Learning with Value-Targeted Regression. \n\n[5]. Zhang et al., Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes.\n\n[6]. Bartlett et al., REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs."}, "questions": {"value": "1. What is the data collected in the exploration phase used for in Algorithm 1?\n\n2. As the essential setting of this paper is tabular MDP, why is the paper using a continuous state space $\\mathcal{X}$ with a $1/n$-covering $|\\mathcal{S}_n|$ but all of the bounds (except for $\\mathcal{X} = [0, 1]^D$) depends on $|\\mathcal{S}_n|$? It is quite a common sense that one can use the $\\epsilon$-net trick to apply the results of tabular RL to a continuous space, but this does not lead to any improvement since the size of the $\\epsilon$-net has the same order as the original space $\\mathcal{X}$. The analysis for the discretizaiton error of $\\mathcal{S}_n$ in the Holder continuity seems to only overly complicate the paper. A right example here is the Theorem 1 of [4], which uses a log-covering number as the complexity measure. \n\n3. Is is possible to use the reduction from an average-reward MDP to a discounted reward MDP mentioned in [5] to get rid of the contraction measure $\\nu$?\n\n4. Why the paper is consistently using \"backward induction algorithm\" to name the well-know value itration algorithm in RL?\n\n5. What is $\\mathcal{L}0$ in line 1993?\n\n6. It would be better to provide extra sections in the appendix to introduce the algorithms and summarize the results since the algorithms are not given in the main context, instead of mixing the algorithms and the full proof into a single section.\n\n[4]. Ayoub et al., Model-Based Reinforcement Learning with Value-Targeted Regression. \n\n[5]. Zhang et al., Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mZvmGHc14G", "forum": "G2kvxv8yPY", "replyto": "G2kvxv8yPY", "signatures": ["ICLR.cc/2026/Conference/Submission1357/Reviewer_6JTM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1357/Reviewer_6JTM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762757195306, "cdate": 1762757195306, "tmdate": 1762915746191, "mdate": 1762915746191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}