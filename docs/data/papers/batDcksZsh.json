{"id": "batDcksZsh", "number": 12800, "cdate": 1758210411042, "mdate": 1759897484355, "content": {"title": "Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices", "abstract": "Deploying machine learning models on compute-constrained devices has become a key building block of modern IoT applications. In this work, we present a compression scheme for boosted decision trees, addressing the growing need for lightweight machine learning models. Specifically, we provide techniques for training compact boosted decision tree ensembles that exhibit a reduced memory footprint by rewarding, among other things, the reuse of features and thresholds during training. Our experimental evaluation shows that models achieved the same performance with a compression ratio of 4–16x compared to LightGBM models using an adapted training process and an alternative memory layout. Once deployed, the corresponding IoT devices can operate independently of constant communication or external energy supply, and, thus, autonomously, requiring only minimal computing power and energy. This capability opens the door to a wide range of IoT applications, including remote monitoring, edge analytics, and real-time decision making in isolated or power-limited environments.", "tldr": "Compressing boosted decision tree ensembles by efficient reuse of features and threshold values combined with a bitwise encoding.", "keywords": ["TinyML", "Boosting", "Decision Trees", "Microcontrollers", "IoT"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3ee489a34b27db3ce6f602cb1f1bfa398e665144.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces compressed boosted decision trees for constrained devices. The authors reuse values across thresholds, leaves, etc. and create a compact memory layout. The results indicate that the proposed methods are competitive in accuracy while being significantly better in comression."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clear to read\n2. The motivation and the constraints specified as per use cases and practical designs pique the interest\n3. The proposed method is well-described and the method appears very intuitive\n4. The results and analysis are rigorous and show good performance"}, "weaknesses": {"value": "1. I am not from this exact area but I am a bit surprised if there aer not more related works. Neural network pruning, quantization and deployment on edge devices is common. But, perhaps even for tree-based methods there are more such related work. Atleast, a couple of examples that come to my mind.\n\n• ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices\nChirag Gupta, Arun Sai Suggala, Ankit Goyal, Harsha Vardhan Simhadri, Bhargavi Paranjape, Ashish Kumar,\nSaurabh Goyal, Raghavendra Udupa, Manik Varma and Prateek Jain\nInternational Conference on Machine Learning (ICML), 2017\n\n• Resource-efficient Machine Learning in 2 KB RAM for the Internet of Things\nAshish Kumar, Saurabh Goyal and Manik Varma\nInternational Conference on Machine Learning (ICML), 2017\n\n\n2. Continuation to above -- I think the absence of more baselines esp. in optimized tree implementations is a weakness."}, "questions": {"value": "1. Can you expand related work to include more methods for optimizing tree-based methods on constrained devices?\n2. Can you compare your work empirically to those papers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GslKxpLgpu", "forum": "batDcksZsh", "replyto": "batDcksZsh", "signatures": ["ICLR.cc/2026/Conference/Submission12800/Reviewer_gDno"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12800/Reviewer_gDno"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817544832, "cdate": 1761817544832, "tmdate": 1762923607995, "mdate": 1762923607995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This research introduces a novel compression scheme tailored for boosted decision tree ensembles to address the challenge of deploying machine learning models on compute-constrained IoT devices. The approach focuses on training compact models by strategically encouraging the reuse of features and thresholds. Experimental results demonstrate a substantial memory footprint reduction of 4–16x compared to standard LightGBM models without compromising performance. This optimization is crucial for enabling autonomous, low-power IoT applications such as remote monitoring, edge analytics, and real-time decision-making in isolated or energy-limited environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Penalizing the use of new features/thresholds encourages reuse across trees. The idea of feature reuse and threshold reuse is interesting and simple yet useful for sustainable ML. Introduction of a new loss function. \n\nThe work is very useful for doing efficient ML on resource-constrained devices.  \n\nGood sensitivity analysis. \n\nI found the writing to be decent, and the paper is well structured. \n\nThe performance gains with minimal memory consumption compared to SOTA methods."}, "weaknesses": {"value": "The part on memory layout based on encoding the information in a bit-wise manner is not novel. \n\nNo actual implementation on MCUs, which makes this a purely algorithmic work. A bit more analysis, including power consumption and energy efficiency, is required. \n\nDomains requiring distinct rules (e.g., heterogeneous datasets) do not allow threshold reuse without performance loss.\n\nThe idea is good, but the utility of it is limited. For example, it does not make sense to train ML models on tiny MCU devices with 32 KB of RAM. The authors should motivate the real use case scenarios where their proposed method will be useful in the real world."}, "questions": {"value": "What are some real-life scenarios where one will need to do training on MCUs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nWcjPSG1IA", "forum": "batDcksZsh", "replyto": "batDcksZsh", "signatures": ["ICLR.cc/2026/Conference/Submission12800/Reviewer_BpWP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12800/Reviewer_BpWP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924902066, "cdate": 1761924902066, "tmdate": 1762923607795, "mdate": 1762923607795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Trees on a Diet (ToaD), a method for compressing GBDT for resource-constrained devices. It employs a regularization strategy that encourages the reuse of features and thresholds during training, coupled with a specialized, bit-wise memory layout. This layout uses global lookup tables and a pointer-less array representation to minimize storage. Evaluated on eight tabular datasets, ToaD compresses models that are 4–16× smaller than baseline LightGBM while maintaining comparable accuracy under tight memory constraints."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The work enables complex models like GBDTs to run on severely memory-constrained microcontrollers.\n2. The proposed pointer-less array-based memory layout is highly suitable for microcontroller deployment, minimizing memory footprint and avoiding inefficient pointer chasing.\n3. The method is orthogonal to many existing compressions like pruning and quantization, and easy to integrate into existing work\n4. The experimental evaluation is comprehensive and sound"}, "weaknesses": {"value": "1. Potential inference latency overhead. The decoding process involving bit-level manipulations and lookups in global arrays may inherently more computationally expensive than the direct pointer-based traversal used in standard implementations. The paper would be significantly strengthened by an end-to-end latency evaluation.\n2. Linear penalty is not motivated theoretically (e.g., from a Bayesian perspective) or empirically against other potential forms. Would a logarithmic penalty, which might better model diminishing costs, be more effective? \n3. Several figures (e.g., Figure 6)  is so small that makes the text are hard to read. Furthermore, in Figure 5, the label is partially covered."}, "questions": {"value": "1. The search over 32,076 configurations per dataset is computationally intensive. Do you have any insights or heuristics for navigating the hyperparameter more efficiently in practice?\n2. For larger models that must reside in main memory (or SSD) and are evaluated on systems with hierarchical caches, could the non-sequential, lookup-heavy memory access pattern of your method lead to frequent cache misses or I/O bottlenecks compared to the more sequential access of standard array-based tree representations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "moCgosojJR", "forum": "batDcksZsh", "replyto": "batDcksZsh", "signatures": ["ICLR.cc/2026/Conference/Submission12800/Reviewer_YmzG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12800/Reviewer_YmzG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933533383, "cdate": 1761933533383, "tmdate": 1762923607434, "mdate": 1762923607434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this article, the authors propose Trees on a Diet (ToaD), a training‑time compression framework for Gradient-Boosted Decision Tree (GBDT) ensembles, targeting microcontrollers and, more generally, resource-constrained edge/embedded devices.\nThe core ideas are: *i)* adding linear regularizers that penalize the introduction of new features and new thresholds across the ensemble to encourage reuse during tree growth, and *ii)* deploying a pointer‑less bit‑wise memory layout with global lookup tables for feature thresholds and global leaf values shared across all trees.\nThe results show that ToaD matches the competitor's performance, reporting 4 to 16 times lower memory usage for the same accuracy in the most relevant memory range (≤128 KB)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work tackles a concrete real-world problem: DTs on microcontrollers, where RAM and flash budgets are very limited.\n\nThe article is well-written, structured, and easy to follow.\n\nThe proposed method is described in sufficient detail.\nSpecifically, the introduction of ensemble-level penalties using features and thresholds is a simple yet effective idea to induce parameter reuse, which complements post-training pruning/quantization.\nFurthermore, the design choice to store the threshold bit-width and numeric type per feature provides a flexible precision/size trade-off."}, "weaknesses": {"value": "The primary motivation for this work is the deployment of GBDT on resource-constrained devices, where memory is a critical constraint, as well as latency (and energy consumption).\nThe authors provide results of memory savings, but do not present any experimental results on inference speed (and energy consumption).\nWithout this analysis, the practical utility of ToaD for real-time edge applications remains unproven.\n\nThe authors state that the $RF$ is “the ratio between the global number of values and the sum of the nodes and leaves” (line 371).\nThus, if values are reused effectively, the number of global values becomes smaller, while the number of nodes and leaves remains fixed; therefore, a good reuse would produce $RF<1$.\nAt line 374, the interpretation in the text says the opposite, implying that $RF$ should be (#nodes + #leaves)/(#global values), *i.e.*, the inverse of the statement above.\n\nMemory for baselines is computed under a simplified node model that includes two child pointers, whereas ToaD benefits from a pointer‑less encoding and global sharing.\nThis risks giving ToaD an advantage in the comparison.\n\nExperiments use a single 80/20 split per dataset with large sweeps and report the best points within memory limits; however, no statistical significance values are presented.\n\nThe authors cite several other relevant works on tree compression and optimization (*e.g.*, Koschel et al., 2023, and Buschjäger & Morik, 2023) in their related work section, but do not include them in the experiments."}, "questions": {"value": "The authors should:\n1) Provide inference latency and energy consumption (per prediction) on representative MCUs (*e.g.*, the mentioned ARM Cortex‑M4 @ 48 MHz) for ToaD versus baselines.\n2) Clarify the reuse factor formula to align with the intended interpretation.\n3) Report memory results under a unified layout, or at least discuss the potential advantages of ToaD against baseline models.\n4) Report mean±std for accuracy at each memory budget, and compare against other relevant presented works only mentioned in the state-of-the-art."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d1NBpoycU0", "forum": "batDcksZsh", "replyto": "batDcksZsh", "signatures": ["ICLR.cc/2026/Conference/Submission12800/Reviewer_GwkK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12800/Reviewer_GwkK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985869240, "cdate": 1761985869240, "tmdate": 1762923607199, "mdate": 1762923607199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}