{"id": "JxXy3YGSln", "number": 4036, "cdate": 1757588767413, "mdate": 1759898056609, "content": {"title": "FineRMOE: Dimension Expansion for Finer-Grained Expert with Its Upcycling Approach", "abstract": "Fine-grained expert design has hitherto been restricted to the intermediate dimension of MoE layers, with its potential at the output dimension remaining largely unexplored, primarily due to the accompanying dimension discrepancy in subsequent computations after the MoE layers. Drawing on the power of multi-head attention, we pioneer the FineRMoE (FineR-grained MoE) architecture to expand fine-grained expert design across both intermediate and output dimensions for further enhancing expert specialization. FineRMoE introduces a bi-level sparsity paradigm: a sparse sum layer produces dimension-reduced candidate vectors for each token through its activated experts, and a sparse concatenation layer subsequently reassembles a dimension-restored output by selectively concatenating the chosen candidate vectors. Despite the bi-level sparsity, we devise a specialized routing mechanism that uses only a single router network to govern both expert activation and candidate selection, eliminating the extra computational cost of adopting two distinct routers. Meanwhile, to obviate the prohibitive cost of training FineRMoE from scratch, we adopt the upcycling paradigm for efficient expert construction and training. Nonetheless, existing upcycling methods are tailored to single-layer additive-fusion MoE architectures, and therefore not applicable to FineRMoE. To this end, we propose an upcycling method, which is compatible with prevailing ones, to accomplish FineRMoE in a cost-effective manner. By enabling flexible partition and expansion of pre-trained FFNs along the intermediate and output dimensions, the upcycling method promotes a broad adaptability in converting dense models into MoE models. Experimentally, we build the FineRMoE, in which 2 experts are sparsely activated out of 128 experts, based on Qwen2.5 with sizes of 0.5B, 1.5B and 7B via the proposed upcycling method. After continued training on 50B tokens, in comparison with baselines, FineRMoE exhibits superior performance across ten standard benchmarks, as well as remarkable efficiency in both parameters and inference. Extensive experiments validate the effectiveness of our FineRMoE architecture and upcycling method.", "tldr": "", "keywords": ["Mixture-of-Experts", "Finer-Grained Expert", "Upcycling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39265f215bab780e39253c698a286498f43b89c0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes FineRMoE, a Mixture-of-Experts (MoE) model that introduces fine-grained sparsity not only in the intermediate dimension but also in the output dimension.\nThe method aims to improve both parameter and training efficiency by refining sparsity across multiple levels.\n\nIn upcycling from a dense model to an MoE model, FineRMoE shows less than one-point improvement on average across 10 benchmark tasks, and the authors claim that the model shows good performance compared to existing upcycling methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written overall, and the figures and pseudocode make the method easy to follow.\n\n- The method implemented within Megatron-LM, which is a strong practical advantage."}, "weaknesses": {"value": "1. The results are incremental, the improvement is modest (less than one point on average over 10 benchmarks).\n\n2. The paper lacks sufficient details for reproducibility, such as dataset specification, hyperparameters (e.g., load balancing loss, warmup steps), and environment details (e.g., Megatron-LM version).\n\n3. There is a minor typo in Algorithm 1 (line 219): ${G_I}$: intermediate expansion rate should be ${R_I}$: intermediate expansion rate.\n\n4. Since the main claim is efficiency for upcycling under limited compute, the authors should report actual GPU hours and FLOPs to verify the computational advantage.\n\n5. Although FineRMoE aims to improve efficiency, the proposed router mechanism introduces additional overhead, which may offset the theoretical gains in real training scenarios.\n\n6. Table 3 indicates that the model’s performance strongly depends on the hyperparameters (especially granularity settings).\n\n7. The overall improvement may not justify the added complexity, the cost-effectiveness of the method remains unclear.\n\n8. A comparison with prior fine-grained upcycling work (e.g., arXiv:2410.07524) would be informative."}, "questions": {"value": "1. Benchmark selection: Why did you choose the particular benchmarks used for evaluation?\nThe Qwen 2.5 paper (arXiv:2412.15115) includes MATH, HumanEval, and MMLU-Pro. Since results vary across tasks, could benchmark choice affect the perceived improvement?\n\n2. Why not Qwen 3? Experiments are limited to Qwen 2.5. Is there a reason for not testing on Qwen 3? Would the proposed method remain effective on stronger or more modern base models?\n\n3. Scratch training feasibility: The paper claims that FineRMoE can also be trained from scratch. Could you demonstrate this on a smaller setup (e.g., fewer tokens or smaller model) to show practicality for users? This would be valuable since many current models (e.g., Qwen 3, DeepSeek V3) are trained from scratch rather than upcycled.\n\n4. Router design: Would using two separate routers (one for the intermediate and one for the output) yield better performance than the shared single-router setup? Have you investigated this variant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hToeWDvCgu", "forum": "JxXy3YGSln", "replyto": "JxXy3YGSln", "signatures": ["ICLR.cc/2026/Conference/Submission4036/Reviewer_WeGh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4036/Reviewer_WeGh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761027625544, "cdate": 1761027625544, "tmdate": 1762917144946, "mdate": 1762917144946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FineRMoE, a fine-grained Mixture-of-Experts (MoE) architecture that decomposes the MLP block along both the intermediate and output dimensions.\nIn addition, the authors introduce a generalized upcycling method that extends beyond existing copy, allowing pretrained dense models to be converted into FineRMoE efficiently.\nExtensive experiments on multiple Qwen2.5 model scales (0.5B–7B) demonstrate that the method consistently improves downstream performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. High reproducibility: all training details, datasets, and hyperparameters are explicitly reported, increasing the paper’s credibility.\n2. Robust empirical validation: experiments span multiple model sizes and show consistent gains.\n3. Practical relevance: the proposed method can be readily applied to existing pretrained dense LLMs.\n4. Experimental evidence: In the reported experiments, the model’s performance drops after CT, yet the proposed method achieves improvement, which strongly supports its effectiveness."}, "weaknesses": {"value": "1. Missing comparison with Drop-Upcycling: Although cited as a related method, Drop-Upcycling is not included in Sec. 4.1 baseline comparisons. The omission leaves unclear whether FineRMoE’s gains hold against the strongest existing upcycling techniques.\n2. Unclear effectiveness when CT does not degrade: In cases where CT does not cause performance drops (e.g., with weaker pretrained models such as Llama-3, or using higher quality datasets), it remains unclear whether FineRMoE would still outperform standard CT.\nDemonstrating such results would clarify whether the gains arise from robustness to degradation or from improvements."}, "questions": {"value": "1. Have you analyzed the model’s downstream performance when equal computational cost (not token count or FLOPs but GPU hours) is enforced during continual training (CT)? Conducting comparisons under the actual GPU time required for training would allow evaluation of the method’s usefulness while also accounting for potential differences in training throughput efficiency across model architectures.\n(Using FLOPs alone does not necessarily ensure fairness between MoE and dense models, as their computational efficiency and hardware utilization characteristics differ.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ho9uUwNFmx", "forum": "JxXy3YGSln", "replyto": "JxXy3YGSln", "signatures": ["ICLR.cc/2026/Conference/Submission4036/Reviewer_j9xD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4036/Reviewer_j9xD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742858409, "cdate": 1761742858409, "tmdate": 1762917144660, "mdate": 1762917144660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new architecture for MoE models, introducing a bi-level sparsity paradigm for the sparse experts. Specifically, in the second stage, it integrates experts using concatenation rather than the conventional summation approach. The paper claims this allows the information from each expert to be output without being mixed together. Since standard upcycling methods are not applicable for initializing this new architecture, a compatible upcycling approach is also developed. The experimental results are inconsistent; the proposed method sometimes outperforms and sometimes underperforms the baseline and common upcycling techniques."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "To my knowledge, no well-known Sparse MoE (SMoE) models have adopted concatenation as an internal mechanism. The experimental results from this exploration could potentially aid future development in this area."}, "weaknesses": {"value": "- The most important problem with this paper is that it doesn't discuss a strong necessity for introducing the additional structure (concat) into sparse experts. The paper describes the qualitative features of the proposed method (e.g., that concatenation allows different information to coexist without being mixed, unlike summation), but it fails to mention a specific situation that *must* be solved by the proposed method rather than by other implementations. This makes it difficult to distinguish whether the method was proposed to solve a genuine problem or simply because it was unexplored.\n- I can’t find any significant improvements on the proposed method from the experimental results. They show only very similar results between the baseline and the proposed method by aggregating winner/loser subtasks, suggesting that the difference of average is still within the margin of error. As standard MoE (C32A2) marked similar results as well, I doubt that the training was actually insufficient to make meaningful comparison."}, "questions": {"value": "Related to the weaknesses, please provide a more robust discussion comparing this to other MoE methods. As written above, it is not enough to distinguish the technical difference between theirs, but it is necessary to make some discussion about how especially the proposed method resolves something difficult in other methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hMBwxpsjpC", "forum": "JxXy3YGSln", "replyto": "JxXy3YGSln", "signatures": ["ICLR.cc/2026/Conference/Submission4036/Reviewer_p2SQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4036/Reviewer_p2SQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003117054, "cdate": 1762003117054, "tmdate": 1762917144418, "mdate": 1762917144418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends fine-grained expert design in MoE models from the intermediate dimension to the output dimension. The proposed method adopts a bi-level sparsity paradigm: a sparse sum layer generates dimension-reduced candidate vectors via sparsely activated fine-grained experts, while a sparse concatenation layer restores the output dimension by selectively concatenating these candidates, with a single router network controlling both expert activation and candidate selection to avoid dual-router overhead. Experimentally, the proposed method built on Qwen2.5 (0.5B, 1.5B, 7B) with 128 experts (2 activated per token) via this upcycling method, trained on 50B tokens, outperforms baselines across 10 benchmarks in performance, parameter efficiency, and inference efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Extended Fine-Grained Design: the proposed method innovatively extends fine-grained expert design from the intermediate dimension to the output dimension of MoE models, addressing the long-standing issue of dimensional inconsistency that limited output-dimension specialization in previous MoEs, thus enhancing expert redundancy reduction and specialization.\n* Generalized Upcycling Method: The proposed upcycling approach resolves incompatibilities between existing upcycling techniques (for single-layer, weighted-sum MoEs). It efficiently initializes shared and sparse experts using pre-trained FFN weights (via copying or splitting) and remains compatible with mainstream upcycling methods (e.g., FFN replication), reducing training costs significantly.\n* Superior Performance & Efficiency: Experiments on Qwen2.5 (0.5B, 1.5B, 7B) show the proposed method outperforms baselines (dense models, other MoEs like C32A2) across 10 benchmarks in average performance, while achieving better parameter efficiency (outperforming larger-parameter MoEs) and inference efficiency (lower prefill latency, higher decoding throughput)."}, "weaknesses": {"value": "* The proposed method’s performance depends on the proper tuning of four hyperparameters. Improper configurations may lead to suboptimal expert specialization or increased computational overhead, adding complexity to model deployment.\n* This paper lacks experiments to validate the effectiveness of FineRMoE in Reinforcement Learning scenarios. Throughout the experimental sections, the evaluations are exclusively conducted on ten standard benchmarks covering knowledge, reasoning, code, and math, with no design or results of experiments involving RL tasks. As a result, the adaptability and performance of the FineRMoE architecture and its upcycling method in RL-related applications remain unproven."}, "questions": {"value": "* Could you provide more specific details regarding the load balancing loss?\n* Why does the continued training (CT) dense model exhibit slightly worse performance compared to the pre-trained model?\n* Furthermore, could the CT model’s performance be improved with additional training data—and if so, would this also affect the performance gap between the MoE and FineRMoE models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QcCu4LYyhm", "forum": "JxXy3YGSln", "replyto": "JxXy3YGSln", "signatures": ["ICLR.cc/2026/Conference/Submission4036/Reviewer_oeAo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4036/Reviewer_oeAo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762090712155, "cdate": 1762090712155, "tmdate": 1762917144028, "mdate": 1762917144028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}