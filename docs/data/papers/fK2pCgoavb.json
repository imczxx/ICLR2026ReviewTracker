{"id": "fK2pCgoavb", "number": 1623, "cdate": 1756898260283, "mdate": 1759898197996, "content": {"title": "DiMeR: Disentangled Mesh Reconstruction Model with Normal-only Geometry Training", "abstract": "We propose DiMeR, a novel geometry-texture disentangled feed-forward model with 3D supervision for sparse-view mesh reconstruction. Existing methods confront two persistent obstacles: (i) textures can conceal geometric errors, i.e., visually plausible images can be rendered even with wrong geometry, producing multiple ambiguous optimization objectives in geometry-texture mixed solution space for similar objects; and (ii) prevailing mesh extraction methods are redundant, unstable, and lack 3D supervision. To solve these challenges, we rethink the inductive bias for mesh reconstruction. First, we disentangle the unified geometry-texture solution space, where a single input admits multiple feasible solutions, into geometry and texture spaces individually. Specifically, given that normal maps are strictly consistent with geometry and accurately capture surface variations, the normal maps serve as the only input for geometry prediction in DiMeR, while the texture is estimated from RGB images. Second, we streamline the algorithm of mesh extraction by eliminating modules with low performance/cost ratios and redesigning regularization losses with 3D supervision. Notably, DiMeR still accepts raw RGB images as input by leveraging foundation models for normal prediction. Extensive experiments demonstrate that DiMeR generalises across sparse‑views-3D, single‑image-3D, and text‑to‑3D tasks, consistently outperforming baselines. On the GSO and OmniObject3D datasets, DiMeR significantly reduces Chamfer Distance by more than 30%.", "tldr": "", "keywords": ["LRM", "3D Reconstruction", "3D Generation", "Image-to-3D"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97644cf3e6f8543e2b545836e15c2dabf6ba3680.pdf", "supplementary_material": "/attachment/763952f1db864bdf37d67c45fb1b93edd7e50bbf.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes DiMeR, a 3D mesh reconstruction framework that disentangles geometry and texture to reduce training ambiguity. It predicts geometry solely from normal maps while learning texture from RGB images in a separate branch. The authors further improve the pipeline with techniques like Eikonal and PBR expectation losses, and demonstrate significant performance gains across sparse-view, single-image, and text-to-3D tasks. The overall design is clean, stable, and achieves clear improvements over prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a novel framework that reconstructs geometry purely from normal maps, effectively disentangling geometry and texture spaces, (partially) resolving the ambiguity between shape and appearance in RGB-supervised 3D reconstruction.\n- The method is well-engineered, integrating improvements like Eikonal regularization and a PBR expectation loss. The latter is especially novel as it introduces varying lighting and material as a way of disentangling geometry and appearance.\n- The authors conducted extensive experiments on GSO and OmniObject3D, showing substantial improvements over prior methods. Comprehensive ablation studies confirm the effectiveness of each design choice, demonstrating strong technical rigor."}, "weaknesses": {"value": "- In the Single-Image-to-3D and Text-to-3D experiments, the authors only use StableNormal and Lotus to generate normals, while models like Zero123++ can already output RGB and normal maps jointly. To validate robustness, the paper should test multiple normal sources (e.g., Zero123++, Era3D, Kiss3DGen) and report Chamfer/F1 variations under different normal qualities.\n- The model is trained with ground-truth normals but tested with predicted normals, which may lead to a training-testing gap. The authors are encouraged to train with noisy or predicted normals to better reflect real-world robustness.\n- Missing noise sensitivity analysis. The paper should include an additional experiment that perturbs GT normals with controlled noise levels and compares quantitative and qualitative results, to show how geometry accuracy degrades under realistic prediction errors."}, "questions": {"value": "- The specular rendering loss in the proposed PBR expectation term may suffer from gradient instability when the roughness parameter is small, as specular reflections can become highly discontinuous. Could the authors clarify how they stabilize this loss or whether any regularization is applied?\n- It would also be helpful to provide more details about the PBR setup — specifically, the BRDF model used, the range of roughness and metallic parameters, and the nature of the lighting conditions (e.g., sharp high-frequency environment maps vs. smooth diffuse ones).\n- Additionally, from the released code, it seems that only a single environment map is used for rendering; could the authors confirm whether multiple environments were actually sampled during training, as stated in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fMFvV4V3wl", "forum": "fK2pCgoavb", "replyto": "fK2pCgoavb", "signatures": ["ICLR.cc/2026/Conference/Submission1623/Reviewer_SAKn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1623/Reviewer_SAKn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722695872, "cdate": 1761722695872, "tmdate": 1762915835263, "mdate": 1762915835263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a disentangled dual-branch framework (DiMeR) for 3D mesh reconstruction. Unlike previous approaches that jointly train the texture and geometry space, DiMeR separates the geometry and texture learning using only normal maps and RGB inputs. It proposes geometry-specific losses and overall achieves large performance gains on two benchmarks for sparse-view, single-view and text-to-3D reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is quite clear and interestingly stated in Figure 2. The disentanglement is clean and effective. The performance improvement in Table 3 demonstrates the effectiveness of the proposed approach.\n2. The pruning of heavyweight FlexiCubes components achieves faster training and fewer overhead on GPU memory (in Table 5), which allows higher resolution without accuracy loss\n3. The evaluation is conducted comprehensively, including different modules in the ablations and comparison with previous baselines on 3 applications over two benchmarks."}, "weaknesses": {"value": "The architecture is a bit like the combination of LRM with FlexiCubes. The backbone largely follows existing LRM-style architecture. Most novelty lies in the training strategy and disentanglement of texture and geometry. But overall, the adaptation with obvious improvement, especially on geometry, is promising."}, "questions": {"value": "1. Is there any approach to measure how sensitive DiMeR's geometry performance is to errors in normal prediction?\n2. Since the geometry and texture branches are disentangled, how is consistency ensured during joint inference? Is it totally influenced by the normal map consistency with the RGB input, or any corner cases when the predictions of these two branches are not aligned well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B2yds3JwXg", "forum": "fK2pCgoavb", "replyto": "fK2pCgoavb", "signatures": ["ICLR.cc/2026/Conference/Submission1623/Reviewer_jDrM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1623/Reviewer_jDrM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790969832, "cdate": 1761790969832, "tmdate": 1762915835084, "mdate": 1762915835084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DiMeR, a feed-forward mesh reconstruction framework that explicitly disentangles geometry and texture: the geometry branch takes only surface normal maps; the texture branch takes RGB and is supervised by appearance losses. The geometry branch still accepts raw RGB at inference by first predicting normals with recent \"foundation\" normal estimators (e.g., Lotus, StableNormal). The authors also simplify FlexiCubes by removing deformation/weight MLPs and swap its regularizers for eikonal + GT-SDF supervision, allowing a higher resolution SDF grid at similar cost. Experiments on GSO and OmniObject3D report better geometry quality versus InstantMesh and PRM for sparse-view, single-image, and text-to-3D pipelines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper effectively illustrates the texture-hides-geometry problem in Figure 2, motivates normal-only geometry prediction to avoid multi-solution conflicts; the design is well illustrated and methodically grounded.\n2. Clear technical contributions e.g. replacing FlexiCubes regularizers with eikonal + GT-SDF supervision and adding PBR expectation losses. These modifications are technically sensible and addresses known stability issues.\n3. Empirical gains on GSO / OmniObject3D. DiMeR shows good amount of CD improvements. Single-image comparisons also improve over InstantMesh / PRM. Results remain strong even with predicted normals.\n4. Comprehensive evaluation. Tests across multiple tasks (sparse-view, single-image, text-to-3D). Ablations isolate the benefit of normal-only input and 3D regularization + PBR losses; FlexiCubes pruning demonstrates little accuracy loss but notable memory/latency savings. These provide valuable insights."}, "weaknesses": {"value": "1. Novelty relative to contemporaries is moderate. Prior work has: 1) disentangled geometry/texture for 3D generation (e.g., Fantasia3D/CLAY), 2) used normals as strong geometry cues (PRM, photometric-stereo based LRM), and 3) pursued feed-forward reconstruction with triplanes/meshes (LRM family, InstantMesh, MeshLRM, MeshFormer). The paper should delineate what is fundamentally new beyond combining these threads, clearer positioning vs MeshLRM/MeshFormer would help.\n2. Fairness/controls in single-image pipeline. For single-image-to-3D, the pipeline uses Zero123++ -> Lotus normals -> DiMeR and claims all reconstruction baselines use the same Zero123++ views; however, Trellis is generative and not strictly comparable and meaningful here. Other relevant pipelines such as MeshLRM is not mentioned or included for comparison.\n3. Grid Resolution. DiMeR trains with 192^3 SDF grids \"benefiting from our enhancement\"; prior LRM-style baselines often default to 128^3 to the best of my knowledge. Please clarify whether your reported gains persist at equal grid resolution and whether CD improvements correlate with resolution. Ideally, a lower resolution variant should be included for completeness and would give a better understanding of the performance gain.\n4. Normal FM dependency and failure modes: more qualitative failure cases and error-to-CD curves would provide more insights and help understand about the robustness of the model."}, "questions": {"value": "1. Please add discussion on what DiMeR does that PRM cannot, and how your normal-only geometry branch differs from the recent \"normal bridging\" idea in Hi3DGen and mesh-LRM variants—ideally with side-by-side ablations where applicable.\n2. Add discussions about resolution-control and sensitivity to normal FM outputs as mentioned in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mlAAWNrrFF", "forum": "fK2pCgoavb", "replyto": "fK2pCgoavb", "signatures": ["ICLR.cc/2026/Conference/Submission1623/Reviewer_QJa6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1623/Reviewer_QJa6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050163719, "cdate": 1762050163719, "tmdate": 1762915834955, "mdate": 1762915834955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a disentangled dual-branch model for 3D mesh reconstruction that separately learns geometry and texture. The geometry branch predicts shape using only normal maps, while the texture branch learns appearance from RGB images. This separation reduces ambiguity in cases where textures might hide geometric errors. This paper also improves mesh extraction with a simplified version of FlexiCubes, combined with 3D supervision and PBR-based regularization. The method achieves better results over existing approaches such as PRM and InstantMesh on both quantitative and qualitative metrics across sparse-view, single-image, and text-to-3D reconstruction tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The motivation of the paper is clear. The paper is well-written and the proposed method is easy to follow and the overall presentation is clear.\n \n* The paper is solving a relevant problem. Geometry ambiguity introduced due to texture is a common problem in optimization based 3D reconstruction pipelines.\n\n* Comprehensive evaluation is performed over several tasks."}, "weaknesses": {"value": "* Novelty: While the idea of learning disentangled representations for geometry and texture using different input modalities (normals for geometry and RGB images for texture) is sound, the overall approach of using normal supervision to regularize reconstruction is very standard with already known components.\n\n* Experimental Results: Overall, the results presented in both the main paper and the supplementary material appear relatively simple compared to the quality achieved by recent state-of-the-art mesh reconstruction methods.\n\n* No analysis and discussions on failure cases and limitations has been done in the paper."}, "questions": {"value": "Please follow the weakness section. \n\n* Why is Trellis omitted from the comparison in the sparse-view reconstruction task in Table 1? Since Trellis is capable of predicting meshes from sparse-view images, it seems reasonable to include it for a fair comparison.\n\n* In Table 2, why is only a single image provided as input to methods like Trellis, while DiMeR receives sparse-view inputs generated from Zero-123? Wouldn’t it be more consistent and fair to use the same input setup for all methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YIYxKD64Tg", "forum": "fK2pCgoavb", "replyto": "fK2pCgoavb", "signatures": ["ICLR.cc/2026/Conference/Submission1623/Reviewer_68mg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1623/Reviewer_68mg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104528452, "cdate": 1762104528452, "tmdate": 1762915834835, "mdate": 1762915834835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}