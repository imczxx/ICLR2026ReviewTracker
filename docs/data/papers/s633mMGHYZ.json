{"id": "s633mMGHYZ", "number": 7442, "cdate": 1758022189678, "mdate": 1763130898868, "content": {"title": "MPS: A Multi-Perspective Benchmark For Assessing Spurious Correlations in Text Classification", "abstract": "Text classification is especially susceptible to diverse spurious correlations, such as those related to word-frequency and concept-level patterns. Nevertheless, there is a lack of a comprehensive and standardized benchmark for evaluating the robustness of models against these spurious correlations. To address this crucial issue, we present MPS (Multi - Perspective Benchmark For Assessing Spurious Correlations in Text Classification). To construct this benchmark, we collect eight widely used text classification datasets and introduce five categories of spurious correlations for each of them, producing 40 variants of datasets for comprehensively evaluating spurious correlations in diverse settings.We then extensively evaluate various text classification models and state-of-the-art anti-spurious correlation methods on this benchmark, which uncovers the vulnerabilities of these models and methods to diverse spurious correlations. A follow-up comparative analysis on this benchmark is performed to assess the performance of these anti-spurious correlation methods and humans in diverse settings.", "tldr": "", "keywords": ["NLP; Spurious Correlations; Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/92881fef663020323fd02d3a75fe472035c1190d.pdf", "supplementary_material": "/attachment/39d3a4f93df94a01d3045d9745a5488f7450905e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a multi-perspective benchmark for systematically evaluating how text classifiers rely on spurious correlations, covering five types of spurious cues -- sentence-level, core-word, negation-based, question-based, and word-frequency biases -- across eight popular NLP datasets. The authors propose two new evaluation metrics derived from worst-group accuracy, to measure robustness degradation and potential improvement. Through extensive experiments on eight widely used datasets and multiple model families, they find that existing models and mitigation methods struggle to handle all types of spurious correlations, with question-based biases emerging as particularly challenging."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The introduction of the Question-Based Spurious correlation (QBS) category highlights a previously underexplored yet impactful source of model bias.\n\n(2) The proposed Œ¥ and Œî metrics, based on worst-group accuracy, allow fine-grained quantification of robustness degradation and potential improvement.\n\n(3) The paper evaluates a wide range of models (traditional, MLMs, LLMs) and robustness methods, revealing consistent vulnerabilities across approaches."}, "weaknesses": {"value": "(1) The idea of benchmarking spurious correlation learning in text classification is not new -- it builds directly on prior frameworks (e.g., Shortcut Maze) without introducing fundamentally new insights. The paper frames itself as the ‚Äúfirst comprehensive benchmark‚Äù, which is inaccurate given prior large-scale studies.\n\n(2) Unlike earlier work that manipulates correlation intensity (e.g., Œª in Shortcut Maze), MPS uses only binary Balanced vs. Imbalanced splits, reducing granularity in robustness measurement.\n\n(3) Although claiming to be comprehensive, the paper omits four spurious types -- synonym, register, author style, and concept correlation -- that were defined in Shortcut Maze.\n\n(4) The new question-based spurious type lacks clear operational boundaries. some questions may genuinely reflect task semantics rather than bias.\n\n(5) The main paper would benefit from a summary figure or table defining, explaining, and exemplifying each spurious type.\n\n(6) Because MPS constructs artificially balanced and imbalanced splits, it may not fully capture naturally occurring spurious correlations present in real-world data.\n\n(7) The redistribution used to create these splits may unintentionally alter label distributions and subgroup sizes, introducing confounding factors beyond the intended spurious correlations. Consequently, performance gaps between splits might reflect class imbalance or distributional shifts, rather than true model sensitivity to spurious features."}, "questions": {"value": "(1) Could the authors explicitly articulate how MPS advances beyond prior benchmarks such as Shortcut Maze? In particular, how do the proposed five spurious types and eight datasets provide new analytical insights rather than simply broader coverage? A clearer statement of conceptual novelty would help calibrate expectations.\n\n(2) In constructing Balanced and Imbalanced splits, how do the authors control for changes in label distributions, subgroup sizes, or topic diversity that could introduce confounding effects?\n\n(3) The human study is an interesting addition‚Äîcould the authors elaborate on participant demographics, task instructions, and agreement rates?\n\n(4) Since MPS focuses solely on text classification, do the authors foresee extending the benchmark to generation or reasoning tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qF8rMaGN6O", "forum": "s633mMGHYZ", "replyto": "s633mMGHYZ", "signatures": ["ICLR.cc/2026/Conference/Submission7442/Reviewer_AtUH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7442/Reviewer_AtUH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761061796100, "cdate": 1761061796100, "tmdate": 1762919556822, "mdate": 1762919556822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "The collective comments from the three reviewers."}, "comment": {"value": "We sincerely thank the three reviewers for their thorough and insightful comments on our manuscript. While we will provide point-by-point responses to each reviewer separately, we would first like to clarify several key aspects that may not have been sufficiently articulated in the initial version of the paper. These points are elaborated in detail below.\n\n1.In the first round of experiments, we evaluated several anti-spurious-correlation loss functions using BERT and Qwen as backbones. However, for the Qwen backbone, measurements were only conducted for the NFL loss function family and downsampling. In this revision, we have expanded the benchmark by incorporating DFR and LLR loss functions to ensure a more comprehensive evaluation. Additionally, we have completed the measurements for the Qwen backbone across these methods, with the exception of JTT, which is not compatible with the Qwen architecture.\n\n2.Our benchmark employs a novel evaluation framework for assessing spurious correlations. The definition of W-ACC (Worst-group Accuracy) was initially presented in the appendix of the first-round submission, where its heavily symbolic notation may have caused some ambiguity. This version rectifies that issue. It is important to note that our implementation of worst-group accuracy is not the conventional one; rather, it represents an optimized version of the metric. The detailed definition can be found in Appendix A.4 of the paper.Since our variant constitutes an optimization that remains fundamentally grounded in the group-based framework, we continue to refer to it as W-ACC.\n\n3.For the evaluation of standard pre-trained models on the benchmark, while the results for SCS-type spurious correlations are presented in the main text, the experimental outcomes for the remaining four types of spurious correlations are provided in the appendix. It should be noted that the color intensity in the table cells is designed to be interpreted through vertical comparison."}}, "id": "rPysQf4wmG", "forum": "s633mMGHYZ", "replyto": "s633mMGHYZ", "signatures": ["ICLR.cc/2026/Conference/Submission7442/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7442/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7442/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763129887631, "cdate": 1763129887631, "tmdate": 1763129887631, "mdate": 1763129887631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MPS, a benchmark built from 8 text classification datasets to evaluate model robustness under five spurious correlation types, namely SCS, CCS, NBS, QBS, and WFS. It defines two analysis metrics, which are Œ¥ (change in worst-group accuracy when balancing a given shortcut type) and Œî (headroom from worst-group under shortcut to overall accuracy after balancing), to quantify each shortcut‚Äôs impact. Across models and mitigation methods, results show no single method is robust across all five types, and the newly defined QBS is particularly challenging."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper‚Äôs core strength lies in its scope and standardization. A unified benchmark across eight datasets and five shortcut types), with paired balanced/imbalanced splits, and clear robustness metrics (W-ACC, Œ¥/Œî) that isolate and quantify shortcut effects. Empirically, it‚Äôs broad and careful, covering classic baselines, pretrained encoders, multiple mitigation families, and LLM backbones, plus a human reference, yielding actionable findings."}, "weaknesses": {"value": "1. The distinction between sentence-level concepts (SCS) and core-word concepts (CCS) is not fully transparent from the main text.\n2. It is not clear how static attributes of SCS are selected and the relationship with their corresponding labels.\n3. Some ambiguous parts, like the usage of W-ACC and Œ¥, are listed in the following Questions section."}, "questions": {"value": "1. Table 7‚Äôs descriptions do not make the distinction between SCS and CCS sufficiently clear. Could you provide precise definitions and concrete, dataset-specific examples for each to clarify the difference?\n2. When Œ¥ < 0 (e.g., strong results on an imbalanced split), how do you conclude that the model exploits spurious correlations rather than demonstrating genuine understanding?\n3. How to explain the large negative Œ¥ of TCM on Ag News in Table 1?\n4. Could the low W-ACC simply because of small sample sizes in certain (ùë¶, ùëé) groups, rather than true vulnerability to the spurious attribute."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hfUEDGxv90", "forum": "s633mMGHYZ", "replyto": "s633mMGHYZ", "signatures": ["ICLR.cc/2026/Conference/Submission7442/Reviewer_3pFQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7442/Reviewer_3pFQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629510960, "cdate": 1761629510960, "tmdate": 1762919556459, "mdate": 1762919556459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MPS (Multi-Perspective Benchmark For Assessing Spurious Correlations in Text Classification), a comprehensive benchmark for evaluating model robustness against spurious correlations in text classification. The authors systematically categorize spurious correlations into five types: SCS (Sentence-level Concept Spurious), CCS (Core-word Concept Spurious), NBS (Negation-Based Spurious), QBS (Question-Based Spurious), and WFS (Word-Frequency-based Spurious). Using 8 widely-used datasets, they create 40 dataset variants and conduct extensive evaluations of various models (MLMs, LLMs, traditional ML) and anti-spurious correlation methods. The work includes human performance comparisons and introduces novel metrics ($\\delta$ and $\\Delta$) to quantify spurious correlation effects."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The five-type taxonomy of spurious correlations (SCS, CCS, NBS, QBS, WFS) provides a clear framework for analysis. QBS (Question-Based Spurious correlations) appears to be a genuinely new contribution that existing methods struggle with.\n2. Testing 12 models and 5 anti-spurious correlation methods across 40 dataset variants represents substantial empirical work. The inclusion of human performance baselines (Table 4) provides valuable context and reveals interesting patterns (e.g., humans outperform models on emotional tasks but under-perform on contextual classification)."}, "weaknesses": {"value": "1. The 10% manual verification rate is too low for a benchmark paper. The paper should provide: (a) inter-annotator agreement scores, (b) error analysis of LLM mistakes, (c) validation on a larger sample or full validation on at least one dataset. Sample sizes, annotator selection criteria, training procedures, and inter-annotator agreement are not provided. This makes it difficult to assess the reliability of human performance claims.\n2. The Imbalanced/Balanced construction procedure needs algorithmic detail. What constitutes \"overwhelmingly dominant\"? What are the exact class distribution targets? Without this, reproduction is difficult.\n3. The paper doesn't ablate key design choices. For example: How sensitive are results to the choice of 6 spurious attributes for SCS/CCS? What about the training epoch selection strategy?\n4. The paper evaluates existing methods but doesn't propose new solutions. While benchmark papers need not introduce new methods, some guidance on promising directions would strengthen the contribution."}, "questions": {"value": "1. Can you provide pseudocode or precise algorithmic descriptions for constructing Imbalanced and Balanced subsets? What specific thresholds define \"overwhelmingly dominant\"?\n2. Beyond the 10% validation, can you provide error analysis? What types of mistakes does Llama 3.1 make in concept annotation? How do error rates vary across datasets?\n3. What are the sample sizes for human evaluation? How many annotators? What was their expertise level? What was inter-annotator agreement?\n4. Have you tested whether models trained to be robust on one spurious correlation type show improved robustness on other types?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q8pYcrxaEW", "forum": "s633mMGHYZ", "replyto": "s633mMGHYZ", "signatures": ["ICLR.cc/2026/Conference/Submission7442/Reviewer_Tbyw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7442/Reviewer_Tbyw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762067345866, "cdate": 1762067345866, "tmdate": 1762919555976, "mdate": 1762919555976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}