{"id": "mOJgZWkXKW", "number": 22191, "cdate": 1758327501462, "mdate": 1759896881264, "content": {"title": "Log-Linear Attention", "abstract": "The attention mechanism in Transformers is an important primitive for accurate and scalable sequence modeling. Its quadratic-compute and linear-memory complexity however remain significant bottlenecks. Linear attention and state-space models enable linear-time, constant-memory sequence modeling and can moreover be trained efficiently through matmul-rich parallelization across sequence length. However, at their core these models are still RNNs, and thus their use of a fixed-size hidden state to model the context is a fundamental limitation. This paper develops log-linear attention, an attention mechanism that balances linear attention's efficiency and the expressiveness of softmax attention. Log-linear attention replaces the fixed-size hidden state with a logarithmically growing set of hidden states. We show that with a particular growth function, log-linear attention admits a similarly matmul-rich parallel form whose compute cost is log-linear in sequence length. Log-linear attention is a general framework and can be applied on top of existing linear attention variants. As case studies, we instantiate log-linear variants of two recent architectures---Mamba-2 and Gated DeltaNet---and find they perform well compared to their linear-time variants.", "tldr": "We introduce a tensor attention framework and propose log-linear attention, which expands beyond fixed-size hidden states to achieve log-linear complexity.", "keywords": ["subquadratic architecture", "triton kernel", "structured matrices"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a07cfec921bd3d8ab6897ed8b0a463c34379d391.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Log-Linear Attention is an extension of Linear Attention to multiple timescales growing logarithmically along with a Fenwick-tree scheme. This way it introduces more memory capacity reserved for short time scales, which in turn enables a clearer separation of the longer-term memory (higher levels) for long-scale information. Through input-dependent temporal coefficients this time-scale separation can be achieved. The method shows slight performance gains on synthetic long-context and memory-capacity tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- fast parallel and hardware-aware implementation\n- captures typical inductive bias on shorter time-scales (recency bias)"}, "weaknesses": {"value": "- theoretically unclear why the extended memory can be effectively used, except for not \"bloating\" the long-term memory at the highest level with short time-scale information that can be store in lower levels\n- mild improvements on benchmarks\n- unclear scaling behavior"}, "questions": {"value": "- How does your method relate to other existing hierarchical sequence architectures like WaveNet that uses dilated convolutions [1]?\n- How would this combine with compression / focusing of the inputs via exponential gating as in xLSTM [2] which has shown to be beneficial for long-context tasks in [3]?\n- Given the different temporal scales are only separated by the different temporal coefficients $\\lambda_t^{(l)}$, how can the effective network capacity exceed the one of pure linear attention based on the foundational theory of Hopfield network capacity? For a querying mechanism that should work across all temporal scales (as in MQAR), shouldn't the \"space benefit\" vanish to normal linear attention (potentially there is less noise on the shorter time-scales in Log-Linear Attention)?\n\n\n[1] van den Oord et al. (2016): WaveNet: A generative model for raw audio\n\n[2] Beck et al. (2024): xLSTM: Extended Long Short-Term Memory\n\n[3] Beck et al. (2025): xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B2rUFG5TCC", "forum": "mOJgZWkXKW", "replyto": "mOJgZWkXKW", "signatures": ["ICLR.cc/2026/Conference/Submission22191/Reviewer_YX7t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22191/Reviewer_YX7t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761611740785, "cdate": 1761611740785, "tmdate": 1762942109351, "mdate": 1762942109351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies the fixed state size of currently best performing linear attention variants with gating, such as Mamba-2 and Gated DeltaNet as the main limitation to handle information in a long context. \nBy introducing log-linear attention - a framework with a logarithmically growing set of hidden states that can be applied to existing linear attention variants - it provides a middle ground between standard attention with linear growing memory and linear attention with a fixed state size, which is independent of sequence length.\nLog-linear attention is based on the insight that efficient attention variants depend on the structure of the structure of the masking matrix in the attention operation, and replaces existing masking structures with a hierarchical one. \nWith log-linear variants of Mamba-2 and Gated Delta Net, the authors demonstrate the general applicability of the log-linear attention framework. \nIn their experiments on small scale language modeling setups and synthetic tasks the log-linear variants show mild but consistent improvements over the standard linear RNN variants. \nIn training throughput and runtime benchmarks the authors demonstrate benefits of the log-linear Mamba-2 variant over Flash-attention 2 at longer context and only small runtime overheads compared to the default Mamba-2 implementation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation\n- Well written\n- The overview of related variants and the view of efficient attention mechanisms as different parametrizations of structured masking matrices is great. It shows how this naturally results in the idea & implementation for log-linear attention.\n- To the best of my knowledge log-linear attention is a novel method for expanding the state size.\n- The paper provides simple pure PyTorch implementations and shows experiments with optimized kernels (even though code for these is missing) achieving runtime benefits over existing methods"}, "weaknesses": {"value": "- Only small performance improvements over linear counter parts / base methods across several tasks (admitted by authors)\n- The authors place log-linear attention as middle ground between standard attention and linear attention in terms of memory state size: Hence I would expect an exemplary calculation of the memory consumption of log-linear attention, standard linear attention and KV-cache for various sequence lengths and reasonable model sizes\n- No code provided for Mamba2 and Gated Delta Net log linear attention variants\n- The paper would further benefit from more details on the efficient kernel implementations (including code) for Mamba-2 \n\n\nDespite these weaknesses, the paper has a clear motivation, is very well written, contributes new insights on efficient attention variants and their implementation, as well as a novel method for expanding the state size of linear attention variants, which outweighs the weaknesses. Therefore I recommend acceptance."}, "questions": {"value": "- L.392-393: Why does the linear layer on top of the hidden states add 3% additional parameters to Mamba-2 and only 0.4% to Gated Delta Net?\n- Description of the P matrix L.236 - 245 would help understandability\n- L. 337, 997: Could the authors elaborate on the MVA pattern for Mamba-2 and/or provide references to descriptions of this?\n- Does there exist an optimized kernel implementation for the log-linear variant of Gated Delta Net?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TzBFbDKerL", "forum": "mOJgZWkXKW", "replyto": "mOJgZWkXKW", "signatures": ["ICLR.cc/2026/Conference/Submission22191/Reviewer_Zys3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22191/Reviewer_Zys3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828368561, "cdate": 1761828368561, "tmdate": 1762942109126, "mdate": 1762942109126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a method to improve the modeling performance of modern SSMs/ Linear RNNs such as Mamba and Gated DeltaNet, whose recurrent state can be formulated as $S_{t} = \\alpha_t * S_{t-1} + (KV)_t$. To get final output, these algorithms multiply query $Q_t$ by a single state matrix $S_t$, which aggregates information about all previous KV states up to step $t$. Log-linear attention instead disaggregates $S_t$ into $O(\\log T)$ states of the same size, each containing information about a disjoint contiguous subsequence of  $\\{0, …, t\\}$. Then the query gets multiplied independently by each state and also by an input-dependent scalar coefficient $\\lambda_i, \\; i \\in \\{0, …, O(\\log T)\\}$, corresponding to that state. Finally, the results sum up. A pure linear attention variant is equivalent to log-linear attention where $\\lambda_i=1$ for all i.\n\nThe partitioning of the sequence proceeds according to Fenwick tree scheme, where each subsequence can have at most $2^i$ consecutive timesteps, and the shortest subsequences is located at the latest step t, while longer subsequences’s boundaries go toward the sequence start.\n\nI feel excited about this novel method (see strengths) and vote for its acceptance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* Potent sub-quadratic runtime alternatives to Transformers is an important open area of research, and this work provides a promising way to improve modeling quality of such architectures, hopefully bringing us closer to an algorithm capable of fully replacing Transformer in autoregressive language modeling.\n\n* The proposed method is coherent and intuitive: if we want to increase long-range performance in comparison with pure linear-time algorithms such as Mamba and DeltaNet, it is plausible that we have to execute a higher relative amount of computations, than for short sequences.\n\n* Log-linear attention is a meta-algorithm, which is compatible with many linear-time alternatives to softmax attention.\n\n* I believe the proposed approach has a potential to be extended and generalized by using other partitioning schemes which could bring further performance gains.\n\n* I’d like to specifically praise comprehensive and fair comparisons which don’t shy away from presenting both positive and negative results. They help to build an honest and complete picture of the method’s strong and weak sides and possible areas of application.\n\n* The validation shows meaningful improvements relative to pure linear counterparts on a large subset of tested benchmarks, effectuated by log-linear extension."}, "weaknesses": {"value": "My judgement is that the paper doesn't have major problems. There are some minor issues mostly related to exposition/ formatting which I listed below. \n\n1. Did you perform any measurements of the memory footprint of the algorithm during inference (prefill, decode) and training workloads? A comparison for different sequence lengths with vanilla Mamba-2 and Gated DeltaNet, as well as with FlashAttention would be helpful. I understand that it’s likely to be $O(\\log(T))$ times greater than aforementioned algorithms, but that’s an expected trade-off, which is easily tolerated given log-linear attention’s superior modeling quality. Nonetheless, these numbers would be important for finding out the most fitting conditions to use the algo.\n\n2. It’s not clear to me from the paper how the log-linear part of the algorithm modifies the underlying linear part, both in chunk-wise parallel and recurrent algos. For example, what happens when two states for neighboring filled buckets get merged after a recurrent step? Since the states are created independently, $S_{[t_1:t_2)} + S_{[t_2:t_3)} \\neq S_{[t_1:t_3)}$, although the underlying algorithm requires precisely $S_{[t_1:t_3)}$. Similar ambiguities emerge when considering chunk-wise form. \n\n3. There is no formal definition of the exact functional form of $\\lambda_i$s. Are they simply linear projections of input vector query $q_t$? Or are they calculated using the same laws as alphas (i.e. in a different manner for each underlying architecture)? How does the algorithm handle that the number of lambdas $max(i)$ is not bounded from above and can extrapolate beyond the maximal value during pre-training?\n\n4. Minor typos/ formatting problems:\n* Line 189 – why is the right bound $t$ open? From the text it follows that the t-th token itself is a part of the partition.\n\n* Line 213 – it would be clearer to mention explicitly that $b_t^{(i)}$ denotes the starting position of partition i, it was somewhat hard to infer for me at first glance.\n\n* Lines 237-246 – there’s no caption of this figure.\n\n* Line 291 – I believe it’s $\\lceil T/C \\rceil$.\n\n* Line 763 – image and table captions are overlaid.\n\n* Table 3 – There’s no mention of the table in the text, and it takes an attentive reader to understand that it’s the summary of Table 6."}, "questions": {"value": "1. Can you come up with a theoretical explanation why log-linear attention offers performance improvements in comparison with pure linear variants?\n\n2. Did you try any other partition schemes besides Fenwick Tree partitioning? I could think of other schemes, with overlapping and disjoined partitions. There could even be some schemes that recover $O(T)$ complexity (e.g., proceed as usual until sequence length is X, then keep placing the oldest tokens into the outermost bucket instead of creating new levels of hierarchy). As such, why did you choose this specific scheme?\n\n3. A follow-up question: could there be some trade-off, where this algorithm could run in linear time at the expense of an arbitrary higher memory consumption?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NeYEPIrrLZ", "forum": "mOJgZWkXKW", "replyto": "mOJgZWkXKW", "signatures": ["ICLR.cc/2026/Conference/Submission22191/Reviewer_xdQ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22191/Reviewer_xdQ7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971023501, "cdate": 1761971023501, "tmdate": 1762942108953, "mdate": 1762942108953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}