{"id": "0694m9ixnv", "number": 7123, "cdate": 1758008662115, "mdate": 1759897871663, "content": {"title": "LM-mixup: Text Data Augmentation via Language Model based Mixup", "abstract": "Instruction tuning is crucial for aligning Large Language Models (LLMs), yet the quality of instruction-following data varies significantly. While high-quality data is paramount, it is often scarce; conversely, abundant low-quality data is frequently discarded, leading to substantial information loss. Existing data augmentation methods struggle to augment this low-quality data effectively, and the evaluation of such techniques remains poorly defined. To address this, we formally define the task of *Instruction Distillation*: distilling multiple low-quality and redundant inputs into high-quality and coherent instruction-output pairs. Specifically, we introduce a comprehensive data construction pipeline to create *MIXTURE*, a 144K-sample dataset pairing low-quality or semantically redundant imperfect instruction clusters with their high-quality distillations. We then introduce *LM-Mixup*, by first performing supervised fine-tuning on *MIXTURE* and then optimizing it with reinforcement learning. This process uses three complementary reward signals: quality, semantic alignment, and format compliance, via Group Relative Policy Optimization (GRPO). We demonstrate that *LM-Mixup* effectively augments imperfect datasets: fine-tuning LLMs on its distilled data, which accounts for only about 3% of the entire dataset, not only surpasses full-dataset training but also competes with state-of-the-art high-quality data selection methods across multiple benchmarks. Our work establishes that low-quality data is a valuable resource when properly distilled and augmented with *LM-Mixup*, significantly enhancing the efficiency and performance of instruction-tuned LLMs.", "tldr": "", "keywords": ["Instruction distillation", "LM mixup"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/063db25688cafc17b63b0a73cc99a225f64ae83e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Instruction Distillation, a new paradigm for improving the quality of low-quality instruction-following data. The authors propose a dataset called MIXTURE that maps multiple low-quality or redundant text inputs to a distilled high-quality target. Building on this dataset, they develop LM-Mixup, a reinforcement learning framework that fine-tunes language models using GRPO with three rewards. The method aims to transform low-quality, redundant, or noisy samples into information-dense outputs. Experimental results show that LM-Mixup outperforms SFT and several strong data selection baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper formalizes the Instruction Distillation task, clearly articulating the goal of transforming multiple imperfect inputs into a single high-quality instruction-output pair.\n\n- Extensive experiments demonstrate consistent improvements across both in-domain (MIXTURE) and out-of-domain (OpenLLM leaderboard) benchmarks."}, "weaknesses": {"value": "1. Limited Practical Applicability. The proposed setting assumes the availability of a large proportion of low-quality or redundant data, which may not reflect many real-world instruction-tuning scenarios where most data are already of moderate or acceptable quality. Consequently, the practical scope of Instruction Distillation could be narrower than implied, and its benefits may diminish when applied to more balanced or higher-quality datasets.\n\n2. The paper’s exposition is at times vague and difficult to follow. For example, the details of the distillation process.\n\n\n3. The experimental comparisons omit several intuitive and important baselines:\n\n- Simple repair via LLM rewriting: directly improving low-quality samples using an LLM without mixup or GRPO.\n\n- High-quality-only training: fine-tuning solely on the curated high-quality subset to measure the added value of the mixup process.\n\n- Alternative data augmentation methods: such as paraphrasing or backtranslation.\n\nIncluding these baselines would provide a clearer understanding of where LM-Mixup stands relative to simpler or more established augmentation techniques."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H1wFP40ufY", "forum": "0694m9ixnv", "replyto": "0694m9ixnv", "signatures": ["ICLR.cc/2026/Conference/Submission7123/Reviewer_s6k7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7123/Reviewer_s6k7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536136746, "cdate": 1761536136746, "tmdate": 1762919291482, "mdate": 1762919291482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new task: instruction distillation, i.e., combining multiple low-quality instructions into a high-quality instruction. The authors then create a dataset for this task, where they trains a model with GRPO. They prove that the trained model is useful by applying it to improve the low-quality training data of other models. They observe an improvement on the performance when replacing the low-quality training data with distilled ones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The task studied is meaningful and significant, and it would be great to have a high-quality dataset for the task\n2. It's novel to train a model to do this specific task"}, "weaknesses": {"value": "1. I believe the task itself is not novel and there are papers, e.g., https://arxiv.org/pdf/2503.00034, that have discussed this problem. Yet these papers are not mentioned in the related works.\n2. I do not fully understand the experiment setting. Please see question 1. This can potentially weaken the results of the experiments.\n3. Although the authors have already explained it in section 5, LLM rating bias is still a valid concern. Specially, we see in the experiments that 50% low quality + 50% high quality data performs better than 30% low quality + 70% high quality. It's a sign that the rating system possibly has some issues. The authors' explanation is that \"some specific portion of low quality data is good\", but it doesn't make sense as \"50% mixup + 50% high quality\" improves over \"50% low quality + 50% high quality\". (mixup data should be of high quality by definition)"}, "questions": {"value": "1. LM-Mixup maps multiple low-quality examples into one high-quality example. So when you say things like \"MIXUP 50% + ORI 50%\", do you mean the 50% low-quality examples are condensed into fewer high-quality examples, or more low-quality examples are condensed into 50% high-quality examples, or somehow you transform 50% low-quality examples into 50% high-quality examples?\n2. Can you give a proper explanation to the question raised in weakness 3, i.e., you said \"Properly mixed low-quality data can even outperform high-quality-only baselines\",  but then why can mixup (which should produce high-quality data) improve performance if that's the case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8dqhL4443S", "forum": "0694m9ixnv", "replyto": "0694m9ixnv", "signatures": ["ICLR.cc/2026/Conference/Submission7123/Reviewer_qBwT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7123/Reviewer_qBwT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761538763092, "cdate": 1761538763092, "tmdate": 1762919289909, "mdate": 1762919289909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LM-mixup, a method for augmenting low-quality instruction data by distilling multiple imperfect inputs into high-quality outputs using a language model fine-tuned with reinforcement learning. The authors construct Mixture, a 144K-sample dataset, and train LM-mixup using GRPO with multi-dimensional rewards. Experiments show that training on a small mixup-augmented subset (∼3% of full data) can match or exceed full-dataset training and compete with data selection baselines on OpenLLM benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Practical Value: Demonstrates that low-quality data can be effectively repurposed to reduce dataset size and cost.\n\nComprehensive Experiments: Ablation studies and multiple benchmark evaluations strengthen empirical claims.\n\nReproducibility: Detailed training pipelines and dataset construction steps are provided."}, "weaknesses": {"value": "Limited Academic Depth: The method feels like a well-engineered application of existing tools rather than a theoretical or algorithmic advance.\n\nWeak Motivation: The paper does not sufficiently articulate why this particular form of \"instruction distillation\" is needed or how it differs philosophically from prior data augmentation or curation work.\n\nPresentation Flaws: Inconsistent referencing and occasionally overly technical writing reduce readability and scholarly rigor."}, "questions": {"value": "Could you better motivate the instruction distillation task in terms of its conceptual novelty beyond existing data augmentation or curation paradigms?\n\nHave you considered comparing with simpler augmentation strategies (e.g., back-translation, paraphrasing) to better isolate the benefit of your multi-source fusion approach?\n\nThe reference formatting is inconsistent. Could you revise it to meet standard citation conventions?\n\nHow does LM-mixup ensure that distilled samples do not simply memorize or overfit to the high-quality targets, especially given the use of RL with reference-based rewards?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4xzSp8wRGS", "forum": "0694m9ixnv", "replyto": "0694m9ixnv", "signatures": ["ICLR.cc/2026/Conference/Submission7123/Reviewer_5cxv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7123/Reviewer_5cxv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884336724, "cdate": 1761884336724, "tmdate": 1762919289383, "mdate": 1762919289383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}