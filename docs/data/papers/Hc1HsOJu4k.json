{"id": "Hc1HsOJu4k", "number": 23660, "cdate": 1758346843929, "mdate": 1759896802699, "content": {"title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling", "abstract": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) [OpenAI, 2024, DeepSeek-AI et al., 2025a, Zeng et al.,2025]. The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates elements from multiple subjects outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.", "tldr": "We find that one well selected or designed math sample can better elicit multidisciplinary reasoning ability of large language models. compared to training with datasets of magnitudes larger.", "keywords": ["Reinforcement Learning", "Multidisciplinary Reasoning", "Data Efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a1e080f2dc98dff14ae9f1ad8a9a8c02efc085f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method for generating a single math sample that, when post-trained with RL on, improves performance across other domains like biology, chemistry."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The method proposed by the others does increase performance across other domains as shown by Figure 1."}, "weaknesses": {"value": "While the work of this paper is not my area of expertise, I find several issues with the current work. First of all, it is quite dense and hard to read. The authors keep referring to LIMR throughout the paper, and claim that they select the samples with the lower LIMR scores (line 162), but this method is never explained or introduced in the current manuscript, hence I am unsure what exactly this means.\n\nSecond, as far as I can understand, the authors use \"salient skill identification\" to construct their samples, but I am not sure how exactly they define the skills. Could the authors explain more here? It seems to be that there is an LLM employed to identify the skills needed to solve a particular problem, but how are these skills defined?\n\nThirdly, the experimental setup makes no mention of hyperparameter tuning. Have the authors chosen a fixed set of hyperparams without sweeping? If so, how was this decision made?\n\nLastly, I am not sure I understand exactly what the authors do once they generate the sample. Lines 204-205 mention that they use GRPO and \"augment the polymath sample into the batch of 128\" - I thought the point of this work was to train on a single sample, generated using their framework. If so, what are the other 127 samples in the batch?"}, "questions": {"value": "I have posed my questions in the weaknesses section."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Template does not follow ICLR template (margins are too narrow)."}, "rating": {"value": 0}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UNFjO5kwID", "forum": "Hc1HsOJu4k", "replyto": "Hc1HsOJu4k", "signatures": ["ICLR.cc/2026/Conference/Submission23660/Reviewer_wLoj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23660/Reviewer_wLoj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037984670, "cdate": 1762037984670, "tmdate": 1762942751728, "mdate": 1762942751728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that using one single training example (polymath sample or synthetic data), can improve Qwen2.5-7B on multiple domains beyond only domain, which shows better cross-domain transfer capability than common math dataset like MATH and LIMR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is simple and effective; the results are good and reasonable. If RLVR on a single-domain (MATH) dataset brings limited gains or even causes forgetting in other domains, fewer-example RLVR may perform better for cross-domain improvement.\n2. The writing and pipeline are clear, and the evaluation covers a comprehensive set of categories.\n3. Also mention that training-free in-context learning can yield some improvement."}, "weaknesses": {"value": "1. As mentioned in [1], the results should not be reported only on the Qwen2.5-7B model. Other one-shot RLVR–related works [2,3,4] also consider other models, like Llama-3 3B or 8B, and maybe other SFT models like OpenThinker3-1.5B. It’s not necessary to beat RLVR with MATH/LIMR on all models, but we should at least see significant improvement from using polymath/synthetic data. I also wonder whether the data transfer to other models, or whether we have to select data for each different model.\n2. How does the selected data compare to the training data used in previous work [2,3,4]? I think it’s important to show the advantage of the data-construction pipeline in the paper by comparing with them.\n3. How did you get the 1,500 random samples from SuperGPQA? I note that Qwen2.5-7B can get about 25–28% overall performance on the SuperGPQA benchmark, but only 15.7% in your report. Is this mainly affected by the prompt, the selected subset, or are they from the hard part? Similar issues may exist in GPQA Diamond. Although it’s fine to compare under the same evaluation pipeline, the gap is too large and needs explanation.\n\nI think these questions are critical, and would like to increase my score if they are fixed.\n\n\n[1] Shao, Rulin, et al. \"Spurious rewards: Rethinking training signals in rlvr.\" arXiv preprint arXiv:2506.10947 (2025).\n[2] Wang, Yubo, et al. \"Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem.\" arXiv preprint arXiv:2506.03295 (2025).\n[3] Wang, Yiping, et al. \"Reinforcement learning for reasoning in large language models with one training example.\" arXiv preprint arXiv:2504.20571 (2025).\n[4] Gao Z, Chen L, Luo H, et al. One-shot entropy minimization[J]. arXiv preprint arXiv:2505.20282, 2025."}, "questions": {"value": "Are you using the same compute for MATH/LIMR training and for one-shot training? Would a larger dataset require more compute to converge? Maybe we should include tables/figures showing accuracy vs. training steps to verify whether the results are converging."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "enGre29LVD", "forum": "Hc1HsOJu4k", "replyto": "Hc1HsOJu4k", "signatures": ["ICLR.cc/2026/Conference/Submission23660/Reviewer_4tkZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23660/Reviewer_4tkZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762330989519, "cdate": 1762330989519, "tmdate": 1762942751447, "mdate": 1762942751447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"polymath learning,\" demonstrating that a single, strategically selected math reasoning sample can improve LLM performance across diverse domains (physics, chemistry, biology) through reinforcement learning, often outperforming training on thousands of samples. The authors find that optimal samples exhibit salient algebra and precalculus skills, and that synthetically engineered problems integrating multidisciplinary knowledge achieve the best results, suggesting a shift from data scaling to precision \"sample engineering\" for more efficient reasoning enhancement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Important research problem.\nThe paper proposes a meaningful problem to study: the data efficiency in current RL scaling for LLMs. The paper proposes an attemptive method to deal with the problem, which sheds some light on this important direction.\n\n2. Clear Writing.\nThe writing is easy to follow, and the methods and experiments are clearly presented."}, "weaknesses": {"value": "1. Unreasonably Low Math500 Performance after GRPO Training. \nAfter GRPO fine-tuning, the Qwen2.5-7B model achieves only 37.2 accuracy on Math500, which is far below the expected score. This discrepancy raises concerns about the validity of the experiment results.\n\n2. Lack of Robustness Verification for the Proposed Method\nThe paper does not provide sufficient evidence of the robustness and statistical reliability of the proposed sample selection method. A convincing validation would require multiple repeated experiments (e.g., 100 independent trials) and report the mean and variance of the final performance."}, "questions": {"value": "1. Unsubstantiated Claim about Low-LIMR Preference\nThe authors argue that “high LIMR samples lead to over-specialization in mathematics” and therefore choose low-score (≈0.6) LIMR samples, yet provide no controlled experiments to establish a causal relationship or robustness. \n\n2. Limited Model Diversity and Poor External Validity\nTraining and evaluation are both performed exclusively on Qwen2.5-7B-Base. The study does not examine how results generalize across different model sizes or architectures. This narrow setup restricts the external validity and robustness of the conclusions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Y9e8o5zyYJ", "forum": "Hc1HsOJu4k", "replyto": "Hc1HsOJu4k", "signatures": ["ICLR.cc/2026/Conference/Submission23660/Reviewer_xEUz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23660/Reviewer_xEUz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762399964918, "cdate": 1762399964918, "tmdate": 1762942750880, "mdate": 1762942750880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether reinforcement learning on a single training sample can enhance reasoning in large language models across multiple domains. The authors introduce Polymath Learning, a framework for one-shot RL training designed to extract cross-domain reasoning improvements from a single carefully chosen math problem."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper shows that RL can be extremely efficient on one single example.\n2. This paper demonstrates RL on a well-designed cross-domain reasoning problem can improve model's reasoning performance across domains."}, "weaknesses": {"value": "1. The evaluation is done on a randomly sampled subset (100 problems for each subject) from multiple benchmarks, this seems not a standard evaluation and makes it hard to compare the experimental results with prior works.\n2. Only one Qwen base model is tested, it's unclear how the method generalizes to other model families.\n3. The sample selection relies on the LIMR score, which actually requires a complete RL training on the full dataset. Therefore, the proposed method is not as efficient as it claims to be \"one-shot RL\"."}, "questions": {"value": "1. Can the authors provide a principled explanation on why one-shot RL can be better than RL on the whole dataset?\n2. Can the authors include the reward curve across training steps (e.g., mean reward) to illustrate how the model’s behavior evolves during one-shot training?\n3. Are the results reported in Table 3 statistically significant? For example, are they at least beyond the 2-sigma or 95% confidence level?\n4. Can the authors clarify what insight Figure 2 is intended to convey?  \n5. Why does RL on a single training example not lead to severe overfitting? Recent works (e.g., [1], [2]) show that reinforcing correct samples can reduce output entropy and lead to overconfidence, and that RL often refines a model’s prior knowledge rather than improving its intrinsic capabilities. Could the observed one-shot improvements arise from implicit refinement of pre-existing knowledge rather than genuine reasoning enhancement?\n\n[1] The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning. NeurIPS 2025\n\n[2] Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model? NeurIPS 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cepUesSCG8", "forum": "Hc1HsOJu4k", "replyto": "Hc1HsOJu4k", "signatures": ["ICLR.cc/2026/Conference/Submission23660/Reviewer_yKH5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23660/Reviewer_yKH5"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762930678024, "cdate": 1762930678024, "tmdate": 1762942750589, "mdate": 1762942750589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}