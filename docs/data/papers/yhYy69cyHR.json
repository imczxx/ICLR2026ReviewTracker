{"id": "yhYy69cyHR", "number": 9889, "cdate": 1758146602882, "mdate": 1759897689002, "content": {"title": "Meta-Reinforcement Learning for Compiler Optimization: A Kernel-Embedded CompilerLLM with Verified Assumptions and Practical Guarantees", "abstract": "Modern compilers carry out sophisticated transformation passes but they predominantly use static heuristics to come to optimisation decisions. In this work, we introduce GMPO which treats each compilation instance as a different task that is located on the similarity manifold and utilizes a kernel embedding for experiential knowledge transfer among related programs. We propose a Cross-Group Meta-Normalization (CG-MN) scheme that aggregates the gradient information from intra-batch neighbours specified by a normalised similarity operator and design a surprise-aware reward modulation mechanism for selectively amplifying learning signals for rather atypical yet successful compiler transformations. All theoretical assertions are expressed under express assumptions and within the sphere of a conservative scope. Specifically: (i) a result of a batch-centred mean square variance reduction of gradient descent (CG-MN) is provided, in which the demeaned component converges to zero as the square of the magnitude of the non-trivial eigenvalue. (ii) a local kernel local dynamics (KL) constrained performance lower bound is provided for natural gradient dynamics. (iii) independence performance accentuating on PAC. GMPO is provided with a 7 billion parameter code model trained on 5894 C programs while operating at the assembly level in a validator guard constrained action space programming with peephole rewrites, instruction substitutions, address mode changes and basic block local scheduling. On a held-out suite of 250 programs, we achieve compilation success for 246/250 examples (98.4%), test passes for 244/250 examples (97.6%), and a median speedup of 1.53x compared to GCC-O3 using a protocolized measurement pipeline. Performance from ablation experiments shows that CG-MN and surprise modulation are both used to improve overall performance.", "tldr": "", "keywords": ["Meta-Normalization", "Reinforcement Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e10d63eae9d1834a94aa36b1ec5c06fe8e4b9106.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces GMPO which is a meta-learning compiler framework that enables experiential knowledge transfer among related programs through kernel embeddings. It also proposes Cross-Group Meta-Normalization (CG-MN)scheme to aggregate gradient information from similar samples and a surprise-aware reward modulation technique to emphasize atypical but successful transformations. As a base model authors used a 7B parameter code model and showed on a test set of 250 programs GMPO achieves 98.4% compilation success, 97.6% test pass rate, and a 1.53× median speedup over GCC-O3."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. GMPO achieves good results compared to the other code models on compilation success, test pass rate and speedup over GCC-O3.\n\n2. Comparison with SOTA LLMs are provided.\n\n3. Ablation of different components are provided."}, "weaknesses": {"value": "Refer to the next section, \"Questions\", which provides a detailed list of weaknesses.\n- The experimental setup and results section is weak.  \n- Authors use a corpus of 5,894 C programmes. Nowhere is it mentioned how and where these are collected.\n- Nowhere is it mentioned how the test cases of the programs are generated?"}, "questions": {"value": "1. Presentation of the paper and also writing of the paper are not good. \n\n2. The experimental setup and results section is weak. The proposed framework is only evaluated on 250 programs. This is a very small test set.\n\n3. Authors use a corpus of 5,894 C programmes. How are these programs collected?\n\n4. Why only 250 programs for testing? How is this split constructed?\n\n5. Comparison with other LLMs is provided, but there are different rule-based tools that also perform program optimization. Comparison with such tools is not provided in the paper.\n\n6. Figures 1 and 2 are difficult to follow. It is hard to understand what each module is doing from the figure.\n\n7. Multiple typos like \"cheques\" should be \"checks\".\n\n8. I find the paper hard to read and follow.\n\n9. How are the test cases of the programs generated? \n\n10. How are kernel embeddings generated? Are these embeddings generated for both the C and the assembly language?\n\n11. For static and dynamic analysis, which tools are used? It is not mentioned in the paper. \n\n12. How are data dependence and hazard verifications performed?\n\n13. It is not clear how the model optimizes the assembly code. Also, why both C and the assembly code are provided to the model is not clear.\n\n14. Section 5.2 gives hints of using LoRa; however, the configurations used are not provided. For example, what is the rank value (r) that is used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YsXgsxOMX2", "forum": "yhYy69cyHR", "replyto": "yhYy69cyHR", "signatures": ["ICLR.cc/2026/Conference/Submission9889/Reviewer_Gbs5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9889/Reviewer_Gbs5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944103023, "cdate": 1761944103023, "tmdate": 1762921353072, "mdate": 1762921353072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GMPO, a meta-reinforcement learning framework for compiler optimization. The key idea is to model each program as an MDP and enable cross-program knowledge transfer through a kernel embedding that measures program similarity. The method introduces two technical components: (1) Cross-Group Meta-Normalization (CG-MN) for variance-reduced gradient aggregation among similar programs; and (2) a surprise-aware reward modulation mechanism that promotes rare but successful code transformations. A validator ensures the semantic safety of edits at the assembly level."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The pipeline emphasizes transparency and validation, which is commendable in an area prone to unverifiable heuristics.\n\n2. Empirical protocol and appendices are detailed, and the reported numerical results (compilation success, speedup) are promising if reproducible."}, "weaknesses": {"value": "1. The claimed contributions are mostly repackaged from existing techniques (kernel smoothing, PPO-style updates, variance reduction) with limited conceptual novelty.\n\n2. The theoretical analyses (spectral contraction, KL bounds, PAC-Bayes) are elegant but disconnected from empirical practice.\n\n3. The evaluation setup is non-standard: results are reported on a private dataset, without comparison to common benchmarks.\n\n4. The reported 1.53× speed-up over GCC-O3 appears unusually large and lacks variance analysis or reproducibility evidence."}, "questions": {"value": "1. How reproducible are the results? Are the 5894 programs and validator scripts public? Without open data or artifacts, the claimed speedups are unverifiable.\n\n2. Why was GCC-O3 chosen as the sole baseline? How does GMPO perform against learned compilers such as LLM-Compiler or RL-based optimizers on standard benchmarks?\n\n3. Can the authors provide quantitative evidence that CG-MN reduces gradient variance or improves sample efficiency?\n\n4. Given that the system operates purely at the assembly level with test-based validation, how does it generalize to other architectures or non-deterministic environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "36r2FI0Gs1", "forum": "yhYy69cyHR", "replyto": "yhYy69cyHR", "signatures": ["ICLR.cc/2026/Conference/Submission9889/Reviewer_u1AX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9889/Reviewer_u1AX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970845057, "cdate": 1761970845057, "tmdate": 1762921352787, "mdate": 1762921352787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes methods to improve optimization, specifically by trying to find surprising or interesting optimizations that nonetheless pass validation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. One interesting part about this paper is that it's not relying entirely on LLMs to perform optimizations (like many papers nowadays), but using more traditional techniques from compilers, including transformations that should preserve semantics and optimizing over them."}, "weaknesses": {"value": "1. This paper is not complete. There are obvious issues with the writing such as \"Meta-optimization\" and \"Meta-reinforcement learning\" being repeated many times in the intro, the \"Meta-Learning Theory.\" section in the related work being empty, etc.\n2. Clarity could use improvement. In many places (e.g. the abstract) the writing was either to jargon-filled or incomplete for me to understand well.\n3. The description in section 4.2 and beyond is not clear enough for me to fully understand the method. There are many places that are unclear, but for instance it is not stated how `k_cfg`, `k_data`, `k_inst` etc. are calculated.\n4. There is no comparison with other baselines on the task of optimizing programs, such as Shypula et al.\n\nLearning Performance-Improving Code Edits\nShypula et al. 2025."}, "questions": {"value": "1. I am confused about why the test pass rate is less than 100%. Shouldn't all transformations be semantics-preserving, so the test pass rate should be 100%?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jSygdx9eSZ", "forum": "yhYy69cyHR", "replyto": "yhYy69cyHR", "signatures": ["ICLR.cc/2026/Conference/Submission9889/Reviewer_K6jh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9889/Reviewer_K6jh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762082619895, "cdate": 1762082619895, "tmdate": 1762921352305, "mdate": 1762921352305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}