{"id": "ZDuyNJI56H", "number": 12190, "cdate": 1758206225059, "mdate": 1759897526279, "content": {"title": "OFMU: OPTIMIZATION-DRIVEN FRAMEWORK FOR MACHINE UNLEARNING", "abstract": "Large language models deployed in sensitive applications increasingly require the\nability to unlearn specific knowledge, such as user requests, copyrighted materi-\nals, or outdated information, without retraining from scratch to ensure regulatory\ncompliance, user privacy, and safety. This task, known as machine unlearning,\naims to remove the influence of targeted data (forgetting) while maintaining per-\nformance on the remaining data (retention). A common approach is to formu-\nlate this as a multi-objective problem and reduce it to a single-objective prob-\nlem via scalarization, where forgetting and retention losses are combined using\na weighted sum. However, this often results in unstable training dynamics and\ndegraded model utility due to conflicting gradient directions. To address these\nchallenges, we propose OFMU, a penalty-based bi-level optimization framework\nthat explicitly prioritizes forgetting while preserving retention through a hierar-\nchical structure. Our method enforces forgetting via an inner maximization step\nthat incorporates a similarity-aware penalty to decorrelate the gradients of the for-\nget and retention objectives, and restores utility through an outer minimization\nstep. To ensure scalability, we develop a two-loop algorithm with provable conver-\ngence guarantees under both convex and non-convex regimes. We further provide\na rigorous theoretical analysis of convergence rates and show that our approach\nachieves better trade-offs between forgetting efficacy and model utility compared\nto prior methods. Extensive experiments across vision and language benchmarks\ndemonstrate that OFMU consistently outperforms existing unlearning methods in\nboth forgetting efficacy and retained utility.", "tldr": "We propose OFMU, a penalty-based bi-level optimization framework for machine unlearning that prioritizes forgetting while preserving utility, with provable convergence and state-of-the-art performance on large language models and vision tasks.", "keywords": ["machine unlearning", "large language models", "privacy", "bi-level optimization", "convergence analysis", "Trustworthy Machine Learning", "Gradient-Based Methods"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9c866fec7eaf7f57db249e349d33944eb74e0159.pdf", "supplementary_material": "/attachment/cf9b2c02628c803b021bf51ab628aecd6b2025da.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes OFMU, a bi-level optimization framework for machine unlearning in large language models. The key idea is to formulate unlearning as a hierarchical optimization problem where the inner loop maximizes forgetting (with a similarity-aware gradient decorrelation penalty) and the outer loop minimizes retain loss. The authors develop a penalty-based reformulation to avoid expensive nested optimization, provide convergence analysis for both convex and non-convex settings, and validate their approach on TOFU, WMDP (language) and CIFAR-10/100 (vision) benchmarks.\n\nThe method shows consistent but modest improvements over existing baselines like Gradient Ascent, NPO, and RMU, particularly in maintaining stability on hard-to-forget samples and achieving better balance between forgetting quality and model utility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: Well-motivated framework\n\nThe hierarchical formulation clearly captures the asymmetry between forgetting (must succeed) and retention (restore afterward). The similarity-aware penalty that decorrelates forget and retain gradients addresses a real problem in existing scalarization approaches.\n\nS2: Solid theoretical grounding\n\nLemmas 1-3 provide rigorous convergence analysis for both convex and non-convex settings. The penalty reformulation (Equation 7) is theoretically justified and avoids the computational burden of fully solving the inner problem at each iteration.\n\nS3: Consistent empirical performance\n\nOFMU shows stable improvements across multiple benchmarks and scenarios. I particularly like Figure 1's analysis showing reduced coupling between sample difficulty and utility loss (though this might be from appendix content—focusing on main text results). The method avoids the catastrophic utility collapse of Gradient Ascent while achieving better forgetting than utility-focused methods like RMU.\n\nS4: Comprehensive evaluation\n\nThe paper evaluates across both language (TOFU, WMDP) and vision (CIFAR-10) tasks with multiple metrics capturing different aspects of unlearning quality."}, "weaknesses": {"value": "W1: Modest and potentially insignificant improvements\n\nLooking at Table 1, the gains are small and inconsistent. For forget05 on LLaMA-2, OFMU gets FQ=0.13 vs NPO's 0.09, but NPO actually beats OFMU on forget10 (0.42 vs 0.41). More critically, the paper provides no error bars or significance tests in main results tables. Without confidence intervals, it's impossible to judge whether these differences are meaningful or just noise.\n\nW2: Computational cost completely unaddressed\n\nThe method requires Hessian-vector products (Equation 9) at each outer iteration, plus T inner gradient ascent steps. This is clearly more expensive than single-loop baselines, yet the paper provides zero analysis of wall-clock time, memory usage, or FLOPs. For a method claiming \"scalability\" (abstract), this is a major omission that undermines practical applicability.\n\nW3: Limited scale and missing ablations\n\nThe largest model tested is LLaMA-2-7B. Given that the method involves second-order computations, scalability to 70B+ models is questionable but unexplored. Additionally, key design choices lack justification—why T inner steps specifically? How sensitive is performance to β and ρ schedules? These are mentioned but not systematically studied in the main paper."}, "questions": {"value": "Q1: Statistical significance\n\nCan you provide confidence intervals or statistical tests for the main results in Tables 1-3? Given the small effect sizes, this is essential to determine if improvements are real or within noise margins.\n\nQ2: Computational overhead\n\nWhat is the actual training time and memory consumption of OFMU compared to baselines like GA and NPO? How many additional gradient computations does the Hessian-vector product require per iteration?\n\nQ3: Scalability validation\n\nHave you tested on models larger than 7B parameters? If not, what are the expected computational bottlenecks that would prevent scaling to 70B+ models used in production?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SPmAVqRMhp", "forum": "ZDuyNJI56H", "replyto": "ZDuyNJI56H", "signatures": ["ICLR.cc/2026/Conference/Submission12190/Reviewer_9aX8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12190/Reviewer_9aX8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761453168271, "cdate": 1761453168271, "tmdate": 1762923139033, "mdate": 1762923139033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "`This paper proposes a bi-level optimization formulation for LLM unlearning, where the inner level optimizes the LLM paramater to unlearn forget data while  decorrelating forget data and retain data gradient and the outer level restores the unlearned model performance on retain data. To avoid heavy bi-level optimization, this paper proposes an iternating optimization scheme on mini-batch for stable training. This paper presents theoratical analysis and also shows performance improvement on LLM unlearning and image classifier unlearning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The bi-level optimization framework is reasonable and easy to follow.\n* The involvement of decorrelating penalty term is well motivated and promising."}, "weaknesses": {"value": "* Potential retain data robustness issue. The similarity-aware penalty relies on the cosine similarity between $\\nabla_\\theta L_f$ and $\\nabla_\\theta L_r$, which assumes meaningful gradient directions from the retain data. What's the effect of different retain data on the unlearning training?\n* Concerning performance on WMDP. Table 3 in Appendix presents the performance for WMDP. From my understanding on WMDP dataset, Bio Acc. and Cyber Acc. are measuring how the unlearned LLM performs on the desired knowledge to remove (Bio/Cyber), but Table mentions higher the better, is this really correct?"}, "questions": {"value": "* Is the outer level optimization always fine-tuned to divergence? Algorithm 1 suggests only 1-step for outer loop.\n* How is the penalty schedule determined in this work?\n* Given that the framework operates in bi-level optimization, involving runtime information can better allow users to estimate its cost."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ko9hZVyGOc", "forum": "ZDuyNJI56H", "replyto": "ZDuyNJI56H", "signatures": ["ICLR.cc/2026/Conference/Submission12190/Reviewer_R451"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12190/Reviewer_R451"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968525530, "cdate": 1761968525530, "tmdate": 1762923138590, "mdate": 1762923138590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OFMU, an LLM unlearning method based on bi-level optimization. Specifically, the authors formulate unlearning as a bi-level problem, where the lower level maximizes the forget loss and negative cosine similarity between the forget and retain gradients. The upper level minimizes the retain loss. For efficient optimization, the authors propose to convert the original problem into a single-level problem and leverage a two-loop optimization scheme to update the model in an efficient and stable way. The authors also provide theoretical analyses to show the convergence guarantee of the proposed method. Finally, experiments on both language and vision benchmarks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper formulates unlearning as a bi-level optimization problem, which addresses the issues in prior works where the forget and retain objectives are difficult to balance. The authors further propose a practical algorithm to solve the optimization problem.\n2. Experiments on both language and vision tasks show that the proposed method outperforms the strong baselines.\n3. The paper is well-written. The method is motivated and explained well."}, "weaknesses": {"value": "1. The method introduces additional costs due to the two-loop optimization and the computation of the Hessian-vector product. How's the speed of the method compared to simple baselines such as GA or NPO?\n2. The method introduces additional hyperparameters such as the penalty parameter and the regularization parameter $\\beta$. How robust is the performance with respect to these hyperparameters?"}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LR489GxQoY", "forum": "ZDuyNJI56H", "replyto": "ZDuyNJI56H", "signatures": ["ICLR.cc/2026/Conference/Submission12190/Reviewer_jsTy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12190/Reviewer_jsTy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992051384, "cdate": 1761992051384, "tmdate": 1762923138052, "mdate": 1762923138052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a bi-level method to tackle the unlearning problem, treating forgetting as inner maximization and retention as outer minimization. A brief convergence theory is provided. Two models and 4 datasets are tested empirically."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work tackles an important problem with a relatively new angle. The motivation is clear. The experiments are extensive. I think the 2-level approach makes sense, and the theoretical analysis supports it."}, "weaknesses": {"value": "Some parts of this paper is not self-consistent and makes it hard to trust.\n\n1. RMU (Risk Minimization Unlearning) is mis-cited.\nRMU an important baseline throughout this work. However, it is attributed to Bu et al in Section 7.4.11 but also attributed to Li et al in Section 5.2. Both citations are wrong. Bu's method is termed NGDiff and has nothing to do with risk. Li's paper is actually about WMDP benchmark, not an optimization method. Also in Section 5.1, WMDP is again mis-cited to Bu et al. \n\nIn the end, what is RMU? Did you really implement NGDiff?\n\n2. Tables are not trust-worthy.\n\nI am highly suspicious of Table 1 to 4. First of all, the bold texts seem to suggest OFMU is the best, but this is simply misleading. In Table 5, **bold texts clearly mean the best performing method**. However, in Table 1 to 4, the authors always highlight OFMU even if it is not the best under many metrics. There are so many false emphasis so I will only point out a few: Table 1, LLaMA-3.2-1B-Instruct, FTR (RMU is better but OFMU is bolded); Table 2, Random Forgetting, RA and TA columns (FT and GA and IU are much better but OFMU is bolded). \n\nSecondly and more importantly, Table 1 didn't not use fair baselines for comparison. The FQ column is almost all zeros and MU column is very low except OFMU, which is highly unlikely in the normal setting. I cannot list all entries in this big table that are suspicious to me, but I require clarity before I finalize my review score. What did I miss?\n\n3. Learning rate concerns\n\nThere is only one learning rate given in Section 7.4.12. But your 2-level algorithm needs two learning rates, inner and outer. Why not give both learning rates? Did you discuss the extra cost of tuning learning rates? If one sweeps 5 values for one learning rate, one will need 5*5=25 values to sweep for two learning rates, which can be very inefficient and impractical."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MmU4xLUvUN", "forum": "ZDuyNJI56H", "replyto": "ZDuyNJI56H", "signatures": ["ICLR.cc/2026/Conference/Submission12190/Reviewer_vkLf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12190/Reviewer_vkLf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040945121, "cdate": 1762040945121, "tmdate": 1762923137679, "mdate": 1762923137679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}