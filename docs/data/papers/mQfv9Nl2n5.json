{"id": "mQfv9Nl2n5", "number": 17540, "cdate": 1758277351492, "mdate": 1759897168517, "content": {"title": "QUAD: Q-Gradient Uncertainty-Aware Guidance for Diffusion policies in Offline Reinforcement Learning", "abstract": "Diffusion-based offline reinforcement learning (RL) leverages Q-gradients of noisy actions to guide the denoising process. Existing approaches fall into two categories: (i) backpropagating the Q-gradient of the final denoised action through all steps, or (ii) directly estimating the Q-gradient of noisy actions. The former suffers from exploding or vanishing gradients as the number of denoising steps increases, while the latter becomes inaccurate when noisy actions deviate substantially from the dataset. In this work, we focus on addressing the limitations of the second category. We introduce QUAD, an uncertainty-aware Q-gradient guidance method. QUAD employs a Q-ensemble to estimate the uncertainty of Q-gradients and uses this uncertainty to constrain unreliable guidance during denoising. By down-weighting unreliable gradients, QUAD reduces the risk of producing suboptimal actions. Experiments on the D4RL benchmark show that QUAD outperforms state-of-the-art methods across most tasks.", "tldr": "We introduce QUAD, an uncertainty-aware Q-gradient guidance for diffusion-based offline RL. By down-weighting unreliable gradients, QUAD achieves state-of-the-art performance on D4RL benchmarks.", "keywords": ["Offline RL", "Diffusion Policy"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/948e873b087220e703950a5a363ce5ffe66c3ebd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper considers offline RL, and proposes to weigh Q-gradient estimates based on their uncertainty. They show that this leads to clear improvements on the D4RL benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Offline RL is an important topic of high practical relevance.\n- The paper is well-written and clearly presented.\n- The Introduction and Preliminaries does a good job at positioning the paper against previous work while, at the same time, presenting the neecessary background. \n- Figures 1 and 2 are pedagogical and aestethically pleasing.\n- Results show improvements on D4RL."}, "weaknesses": {"value": "**Critical: The core assumption is not true in general, nor backed up with experimental evidence**\n\nOn l.202-210 you state that \"It is reasonable to assume that $\\hat{g}$ provides an unbiased estimate of $g^*$\". *I strongly disagree with this statement*. This is a critical flaw, since the rest of paper builds on the alignment loss in eq. (12) that stems from this assumption.\n\nIn general, the unbiasedness of an estimator $\\hat{y}(x)$ does *not* imply that its gradient $\\nabla_x \\hat{y}(x)$ is an unbiased or accurate estimator of $\\nabla_x y(x)$. Unbiasedness is a *pointwise* property, and interchanging differentiation and expectation, i.e. $\\nabla_x \\mathbb{E}[\\hat{y}(x)] = \\mathbb{E}[\\nabla_x \\hat{y}(x)]$, requires additional regularity conditions such as smoothness and dominated convergence. Even when this interchange is valid, gradient estimators can exhibit high variance or bias in practice. \n\nAt the very least, I would expect an empirical examination of whether the decomposition in eq. (11) is valid.\n\n**Minor: Missing AntMaze baseline**\n\nOn most AntMaze tasks, QUAD performs worse than what is reported by Zhang et al., \"Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning\", NeurIPS (2024)."}, "questions": {"value": "- Can you provide convincing evidence (theoretical or experimental) supporting the decomposition in eq. (11)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6lSII9mQwP", "forum": "mQfv9Nl2n5", "replyto": "mQfv9Nl2n5", "signatures": ["ICLR.cc/2026/Conference/Submission17540/Reviewer_J5PX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17540/Reviewer_J5PX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821531368, "cdate": 1761821531368, "tmdate": 1762927413221, "mdate": 1762927413221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors build on a diffusion actor-critic (DAC) style algorithm by introducing an uncertainty quantification (UQ) mechanism to weight an auxiliary loss. In essence, they estimate the Q-function gradient (the gradient of the critic’s Q-value with respect to the action) using an ensemble of Q-networks. This yields an estimate of the mean Q-gradient and its variance for each state-action sample. A per-sample weight $\\lambda(s,a)$ is then derived based on the ensemble’s uncertainty: samples with higher variance in the Q-gradient are assigned a lower weight, and those with more confident (lower variance) Q-gradients receive a higher weight. The weight $\\lambda$ is applied to the Q-gradient alignment loss in the DAC algorithm, with the intention of down-weighting unreliable guidance. In theory, the optimal weight comes from minimizing a mean-squared error risk function, resulting in an inverse-variance weighting scheme. In practice, the authors approximate this optimal $\\lambda$ using the ensemble’s empirical variance and a small regularizer for stability. Finally, this weighted guidance term is incorporated into the diffusion policy training objective (essentially adding $\\lambda(s,a)$ times the Q-gradient term to the diffusion model’s loss). The overall approach is a simple fix on top of the DAC framework: it modulates the influence of the Q-gradient guidance by the uncertainty of that guidance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The experimental results indeed show a little benefit to the original DAC algorithm without using UQ. This justifies the hypothesis that adding UQ to handle bias-variance tradeoff can help with the learning."}, "weaknesses": {"value": "While this method may improve the original DAC algorithm’s performance by tuning the guidance strength per sample, it amounts to a relatively incremental improvement. Essentially, the authors introduce a well-known statistical technique – weighting by inverse uncertainty – into the existing algorithm. This is a straightforward UQ method rather than a novel RL or diffusion modeling insight. The derivation of the optimal weight $\\lambda^(\\sigma^2)$ is a direct application of bias–variance trade-off analysis. In fact, the solution simply implements inverse-variance shrinkage, a classic approach where high-variance estimates are systematically down-weighted while low-variance (confident) ones are given full weight. This idea of down-weighting unreliable estimates is not new and has long been used in various domains for risk-sensitive learning.\n\nThe contribution here is therefore technically modest: it adds an ensemble-based uncertainty estimation and a weighting formula on top of a prior diffusion RL algorithm. Such a “simple fix” does not substantially expand the theory or capabilities of diffusion models or reinforcement learning algorithms. It offers a practical tweak to improve stability or performance of DAC, but its novelty and conceptual depth are limited."}, "questions": {"value": "Is there any reason not to compare with offline model-based RL algorithms, e.g., MOPO and MOReL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IlCzQcxT3I", "forum": "mQfv9Nl2n5", "replyto": "mQfv9Nl2n5", "signatures": ["ICLR.cc/2026/Conference/Submission17540/Reviewer_7TdU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17540/Reviewer_7TdU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893681737, "cdate": 1761893681737, "tmdate": 1762927412818, "mdate": 1762927412818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents QUAD, a method that enhances diffusion-based offline reinforcement learning by incorporating uncertainty-aware Q-gradient guidance during policy denoising. In diffusion-based offline RL, Q-guidance steers the denoising trajectory toward high-value actions. However, directly estimating Q-gradients for noisy intermediate actions often leads to unreliable guidance. To address this issue, QUAD explicitly models the uncertainty of Q-gradients using a Q-ensemble and adaptively down-weights unreliable gradients throughout the denoising process."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper focuses on a critical issue in diffusion-based policy denoising: when estimating Q-gradients for intermediate noisy actions, these actions may lie far from the dataset distribution, leading to unreliable value guidance. To address this, the paper provides a principled theoretical derivation of optimal uncertainty weighting based on mean-squared error (MSE) minimization. Empirically, QUAD achieves SOTA or near-SOTA performance across 18 D4RL tasks, and with its uncertainty-aware formulation, it significantly reduces training variance compared to the backbone algorithm DAC."}, "weaknesses": {"value": "1.\tThe diffusion policy’s optimization objective (Eq. 8 in the paper) assumes that $Q_\\phi(s, a_t)$ provides meaningful gradients at all timesteps. However, since the critic is trained only on $a_0$ (or near-dataset actions), the gradients for earlier steps are effectively unanchored. Consequently, DAC’s Q-guidance remains reliable only near the final denoising steps (small $t$, low noise) and becomes almost random in the early stages. Although QUAD attempts to down-weight such guidance when the uncertainty of the Q-gradient is high, it does not fundamentally resolve the extrapolation problem. In other words, it cannot yield more accurate estimates when noisy actions are far from the data distribution. While this uncertainty weighting reduces the influence of unreliable gradients, it also weakens guidance precisely when the denoising process requires stronger directional information to reach high-return regions. QUAD’s contribution lies in mitigating this inconsistency through uncertainty weighting, but the core limitation remains—the critic’s validity is unproven for noisy actions.\n2.\tQUAD trades off computational efficiency for robustness and still relies on heuristic, ensemble-based uncertainty estimation. Training a large Q-ensemble and computing per-sample gradient variance substantially increase computational cost due to multiple forward and backward passes. Moreover, evaluation requires sampling multiple candidate actions, which is inefficient for multi-step diffusion policy sampling.\n\n3.\tThe uncertainty estimation based on ensemble variance is relatively crude and heavily depends on the diversity of ensemble members. The paper provides no in-depth analysis of the reliability or calibration of the estimated uncertainty.\n\n4.\tQUAD assumes independence between the oracle gradient $g^*$ and the stochastic noise term $\\xi$ when deriving the variance decomposition (Eq. 28), an assumption that may not strictly hold in practice."}, "questions": {"value": "1.\tHow does QUAD compare to bootstrapped ensembles or dropout-based uncertainty estimation in similar settings?\n2.\tWhy we can assume $\\hat{g}$ to be an unbiased estimate of $g^*$?\n3.\tWhat is the rationale behind Eq. 11 and Eq. 12?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TGuajnAQYZ", "forum": "mQfv9Nl2n5", "replyto": "mQfv9Nl2n5", "signatures": ["ICLR.cc/2026/Conference/Submission17540/Reviewer_fTmh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17540/Reviewer_fTmh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948272259, "cdate": 1761948272259, "tmdate": 1762927412510, "mdate": 1762927412510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the diffusion policy’s challenge of backpropagating the Q-gradient of the final denoised action through all diffusion steps, the paper proposes a novel approach that directly maximizes $Q(s, a_t)$ on the noisy action $a_t$ with a new weighting function $\\lambda(s, a^t)$. The authors provide both theoretical guarantees and empirical evidence to support their claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The empirical experiments are solid and demonstrate strong performance across benchmarks. The proposed reweighting schedule on $\\lambda(s, a^t)$ is an interesting idea that effectively reduces the influence of inaccurate $Q(s, a^t)$ estimates, improving the stability of training."}, "weaknesses": {"value": "My main concern lies in the theoretical analysis. The proof flow lacks rigor and contains several gaps, and the notations are loosely defined, making it difficult to follow the derivation precisely. Please see my questions"}, "questions": {"value": "- In Eq. (10), why can we assume that $\\hat{g}$ is an unbiased estimator of $g^*$? This seems to introduce a large gap — in this case, the expectation of stochastic term is not necessarily zero.\n- Please clarify lines 202 and 207: what is the expectation taken over? It seems that it should be $\\mathbb{E}_{\\phi_k}[ξ] = 0$. What is the distribution of $\\phi_k$?\n- In Eq. (14) and line 224, please specify which random variable the expectation is taken with respect to.\n- Have you considered other kind of risk function, it may provide different property than mse risk function.\n- In Eq. (17), note that $v$ is a function of $\\theta$, which implies that $v^2$ must be recomputed for each update of $\\theta$.\n- The paper’s key challenge is estimating $\\sigma$ and $v$. However, since these quantities appear to rely on the entire batch of data and $K$ Q-functions for each update of $\\theta$ and $\\phi$, I am concerned that this may be computationally too expensive in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iB5IvuypAy", "forum": "mQfv9Nl2n5", "replyto": "mQfv9Nl2n5", "signatures": ["ICLR.cc/2026/Conference/Submission17540/Reviewer_U7Za"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17540/Reviewer_U7Za"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155099943, "cdate": 1762155099943, "tmdate": 1762927412129, "mdate": 1762927412129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}