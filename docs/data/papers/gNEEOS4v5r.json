{"id": "gNEEOS4v5r", "number": 10794, "cdate": 1758182085415, "mdate": 1763742244977, "content": {"title": "Enhanced DACER Algorithm with High Diffusion Efficiency", "abstract": "Due to their expressive capacity, diffusion models have shown great promise in offline RL and imitation learning. Diffusion Actor-Critic with Entropy Regulator (DACER) extended this capability to online RL by using the reverse diffusion process as a policy approximator, achieving state-of-the-art performance. However, it still suffers from a core trade-off: more diffusion steps ensure high performance but reduce efficiency, while fewer steps degrade performance. This remains a major bottleneck for deploying diffusion policies in real-time online RL. To mitigate this, we propose DACERv2, which leverages a Q-gradient field objective with respect to action as an auxiliary optimization target to guide the denoising process at each diffusion step, thereby introducing intermediate supervisory signals that enhance the efficiency of single-step diffusion. Additionally, we observe that the independence of the Q-gradient field from the diffusion time step is inconsistent with the characteristics of the diffusion process. To address this issue, a temporal weighting mechanism is introduced, allowing the model to effectively eliminate large-scale noise during the early stages and refine its outputs in the later stages. Experimental results on OpenAI Gym benchmarks and multimodal tasks demonstrate that, compared with classical and diffusion-based online RL algorithms, DACERv2 achieves higher performance in most complex control environments with only **five diffusion steps** and shows greater multimodality.", "tldr": "", "keywords": ["Online Reinforcement Learning", "Efficiency", "Diffusion Model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a988ea9bb7eef42100fb866eec32c88ae4c4bcb6.pdf", "supplementary_material": "/attachment/f62b2e2446079b3af176e14fe668ab1f535b0307.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes DACERv2, an enhanced version of the diffusion-based online reinforcement learning algorithm, DACER. The authors identify a key limitation in DACER: a trade-off between performance and efficiency, where a large number of diffusion steps is required for high performance, leading to high computational cost.\nTo address this, DACERv2 introduces two main contributions:\n1. **A Q-Gradient Field Objective ($\\mathcal{L}_g$)**: This is an auxiliary loss term that provides intermediate supervision at each step $t$ of the denoising process. It is motivated by the connection between the optimal soft-Q policy (a Boltzmann distribution) and the score function via Langevin dynamics, aiming to align the policy's score function $S_{\\theta}(s, a_t, t)$ with the normalized Q-gradient $\\nabla_{a_t} Q(s, a_t)$.\n2. **A Temporal Weighting Mechanism ($w(t)$)**: This mechanism modulates the strength of the Q-gradient objective based on the diffusion timestep $t$. This is designed to resolve the inconsistency between the time-independent Q-gradient field and the time-dependent nature of the diffusion denoising process.\nThe authors claim that this new combined objective ($\\mathcal{L}_\\pi = \\mathcal{L}_q + \\eta \\mathcal{L}_g$) allows DACERv2 to achieve state-of-the-art (SOTA) performance on complex MuJoCo benchmarks using only 5 diffusion steps. This results in significant improvements in both training and inference efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear and Significant Problem**: The paper addresses a critical, practical limitation of diffusion policies—their poor computational efficiency due to the high number of sampling steps.\n2. **Strong Empirical Results**: The primary claim of achieving SOTA performance with only $T=5$ steps is backed by comprehensive experiments. The efficiency gains shown in Table 1 are dramatic and highly compelling (e.g., >3.5x faster inference than DACER). The ablations in Figure 4 clearly isolate the impact of the two key contributions ($\\mathcal{L}_g$ and $w(t)$), empirically validating their necessity for the observed performance."}, "weaknesses": {"value": "1. **Lack of Theoretical Novelty and Justification**: The paper's theoretical support is weak on two fronts.\n    - **Heuristic Contribution**: The paper's primary novel contribution, the $\\mathcal{L}_g$ auxiliary loss, is a pure heuristic. It is motivated by analogy but lacks any formal proof or analysis showing that the combined objective ($\\mathcal{L}_\\pi = \\mathcal{L}_q + \\eta \\mathcal{L}_g$) leads to a better, faster, or more stable convergence to the optimal policy.\n    - **Non-Novel Theorem**: The main theoretical result presented (Theorem 1 in Appendix A) appears to be a restatement of a standard, known result (i.e., that maximizing value under a global entropy constraint yields a Boltzmann policy with a global temperature). This theorem only justifies the baseline DACER objective and does not represent a novel contribution of this work.\n2. **Potentially High Hyperparameter Sensitivity ($\\eta$)**: The hyperparameter $\\eta$ (the auxiliary loss weight) is clustered into two groups (Table 5), but the values are $1.0$ and $0.01$—a 100-fold difference. This implies that the algorithm's performance is highly sensitive to this choice. The paper offers no insight into what task properties (e.g., dimensionality) necessitate such a drastic change, which is crucial for applying this method to new environments."}, "questions": {"value": "1. Regarding the hyperparameter $\\eta$ in Table 5: The optimal value differs by 100x ($1.0$ vs. $0.01$) across tasks. What properties of the environment (e.g., dimensionality, task complexity) dictate this choice? How sensitive is the algorithm to this parameter?\n2. The paper's main theoretical support, Theorem 1, appears to be a standard result for justifying the soft-Q objective. Given this, can the authors provide any novel theoretical analysis for their actual contribution, $\\mathcal{L}_g$? For instance, can it be shown that $\\mathcal{L}_g$ acts as a variance reduction term, or that the combined objective $\\mathcal{L}_\\pi$ has superior convergence properties compared to optimizing $\\mathcal{L}_q$ alone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tNZ5bcMZCh", "forum": "gNEEOS4v5r", "replyto": "gNEEOS4v5r", "signatures": ["ICLR.cc/2026/Conference/Submission10794/Reviewer_dk8y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10794/Reviewer_dk8y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731067341, "cdate": 1761731067341, "tmdate": 1762922009567, "mdate": 1762922009567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Comparing against Langevin Soft Actor Critic  from ICLR 2025 and DeepMind Control Suite experiment suggestion"}, "comment": {"value": "Dear authors,\n\nWe wanted to point out that our ICLR 2025 paper on Langevin Soft Actor Critic [1] should be used as a baseline. In LSAC, we use Langevin Monte Carlo (LMC) based Thompson Sampling and distributional critic to achieve sample efficiency as well as multi-modal behavior through parallel tempering based LMC. Given one of the main motivations for this submission is achieving multi-modal and expressive behavior in the policy, we believe our LSAC paper is highly relevant as an alternative baseline to diffusion policy based approaches.\n\nIn our work, we used `DSAC-T` (Duan et al., 2023), `QSM` (Psenka et al., 2024), `DIPO` (Yang et al., 2023), `SAC` (Haarnoja et al., 2018a), `TD3` (Fujimoto et al., 2018), `PPO` (Schulman et al., 2017), `TRPO` (Schulman et al., 2015), `REDQ` (Chen et al., 2021),  `DrQ-v2` (Yarats et al., 2022)) and model-based (`Dreamer` (Hafner et al., 2020)) as baselines and we got superior results than these baselines in MuJoCo tasks and DeepMind Control Suite tasks. We want to **emphasize** that `LSAC` outperforms baselines `QSM`, `DIPO`, `PPO`, `SAC` and `DSAC` which were also used as baseline in this submission. Note that in MuJoCo `v-3`, which is same as what the authors here used, we used `10 seeds` and `1 million` time steps. We encourage the authors to use `10 seeds` for more robust and conclusive evidence. Our codebase and **all the data** are **publicly** available here https://github.com/hmishfaq/LSAC\n\n### **Suggestion for further experiment in DeepMind Control Suite (DMC) (Tassa et al., 2018)**\n\nWe would also like to suggest the authors to run their algorithm in DeepMind Control Suite (DMC) (Tassa et al., 2018) environments which are standard benchmarking environments for continuous control tasks. As mentioned above, you can readily find the data for `10 seeds` over `3e6` training steps in `12` environments for `LSAC`, `DSAC`, `DIPO`, `Dreamer`, `DrQ-v2`, `TD3`, `PPO`, `SAC` and `TRPO` in our codebase https://github.com/hmishfaq/LSAC . \n\nWe hope you will find these suggestions and pointers to baseline data helpful in regard to improving your paper.\n\nThanks!\n\n[1] Ishfaq, Haque, et al. \"Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning.\"  ICLR 2025"}}, "id": "gHBenqloks", "forum": "gNEEOS4v5r", "replyto": "gNEEOS4v5r", "signatures": ["~Haque_Ishfaq1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Haque_Ishfaq1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10794/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763260866045, "cdate": 1763260866045, "tmdate": 1763260866045, "mdate": 1763260866045, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DACERv2, an improved version of DACER that learns a Q-gradient field for fast action denoising. Observing that the denoising process in diffusion relies on a time-dependent score function, DACER-v2 also introduces such dependency by scaling the Q-gradient with a crafted exponential decay function. Finally, DACER-v2 is evaluated on tasks from Gym-MuJoCo and demonstrates improved performance as compared to several diffusion policy baselines."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The overall idea is clearly presented and straightforward to implement. The proposed method is efficient both in terms of training and inference, which makes it preferable for deployment in embodied scenarios."}, "weaknesses": {"value": "The idea of aligning the score networks with the gradient of Q-value functions has been extensively investigated in QSM [1], DAC [2], QGPO [3], iDEM [4], and [5]. One contribution of DACER-v2 seems to be the time-based weighting. However, this is purely heuristic and theoretically unjustified. On the other hand, QGPO, iDEM, and [5] also estimate the time-dependent score, and their estimations are exact in theory. Therefore, the novelty and insight of this paper are limited. \n\nBesides, this paper lacks a related work section to familiarize the readers with the frontier literature. For example, given that the proposed algorithm is termed DACER-v2, it is necessary to include detailed introductions about DACER and demonstrate how v2 improves the v1 algorithm. \n\n[1] Learning a Diffusion Model Policy from Rewards via Q-Score Matching.\n\n[2] Diffusion Actor-Critic: Formulating Constrained Policy Iteration as Diffusion Noise Regression for Offline Reinforcement Learning. \n\n[3]: Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning. \n\n[4]: Iterated Denoising Energy Matching for Sampling from Boltzmann Densities. \n\n[5]: Sampling from Energy-based Policies using Diffusion."}, "questions": {"value": "How many environment frames/steps does one iteration correspond to? \n\nThe authors mentioned that the Q-gradient prediction is an auxiliary objective (line 208). However, I don’t see any further introduction about the actual objective of the diffusion policy in the paper. Could the authors make this clear?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U4P2bly8QN", "forum": "gNEEOS4v5r", "replyto": "gNEEOS4v5r", "signatures": ["ICLR.cc/2026/Conference/Submission10794/Reviewer_VwAt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10794/Reviewer_VwAt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860645435, "cdate": 1761860645435, "tmdate": 1762922009082, "mdate": 1762922009082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DACERv2, an online RL algorithm built upon DACER and employs a new Q-gradient field matching objective in the policy learning loss. The proposed method enables action sampling with five diffusion steps and outperforms the included baselines on OpenAI Gym environments. The authors also introduce a temporal weighting function that adjusts gradient magnitude across diffusion timesteps."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed algorithm achieves strong performance on state-based OpenAI Gym environments, with higher training and inference efficiency than most baselines.\n2. The paper is well-written."}, "weaknesses": {"value": "1. The score function in a standard diffusion SDE is the score function of the perturbed distribution $\\int q_{t|0}(a_t|a_0) \\frac{e^{\\frac{1}{\\alpha}Q(s, a_0)}}{Z(s)}da_0$ and is not in the form of Equation (9). Moreover, the non-annealed Langevin dynamics used in this paper may suffer from slow mixing, as shown in [1].\n2. The method proposed in this paper is a straightforward combination of the QSM [2] policy training loss (with a newly introduced weighting function) and the DACER policy training loss, and the analysis is insufficient to explain why this combination boosts performance without gradient-conflict issues.\n3. The argument in Lines 698-699 is not well supported. If the optimal action with the largest Q value is a single point, then maximizing the Q-value will result in a delta distribution, not a multimodal policy. The multimodal property is more likely due to the entropy regularization and the expressive capacity of diffusion models. \n\n[1] Song Y, Ermon S. Generative modeling by estimating gradients of the data distribution[J]. Advances in neural information processing systems, 2019, 32.\n\n[2] Psenka, Michael, et al. Learning a diffusion model policy from rewards via Q-score matching. Proceedings of the 41st International Conference on Machine Learning. 2024."}, "questions": {"value": "1. In the training curves in Figure 3, why does DACERv2 improve more slowly than most baselines in the early stage, especially on Ant-v3, HalfCheetah-v3, Walker2d-v3, and Swimmer-v3?\n2. The argument in Lines 321-323 is not logically supported. Why does the auxiliary intermediate supervisory signal enable action sampling with fewer diffusion steps? A similar Q-gradient in QSM still requires 20 sampling steps.\n\nIf the authors can address the concerns above, I would be willing to increase the overall score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ngol0wc2F1", "forum": "gNEEOS4v5r", "replyto": "gNEEOS4v5r", "signatures": ["ICLR.cc/2026/Conference/Submission10794/Reviewer_n3U9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10794/Reviewer_n3U9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973132112, "cdate": 1761973132112, "tmdate": 1762922008644, "mdate": 1762922008644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the **DACER v2** algorithm, an improved version of **DACER**, which incorporates a **Q-gradient field** and a **temporal weighting mechanism**. Experiments on **OpenAI Gym** benchmarks and **multi-modal tasks** demonstrate that **DACER v2** achieves superior performance and stronger multi-modality using only **five diffusion denoising steps**, outperforming other online diffusion RL algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a highly important and timely research question, especially as diffusion models are becoming increasingly dominant in the fields of **imitation learning**, **reinforcement learning**, and **Vision-Language-Action (VLA)** modeling.\n\n2. The paper is **well-written** and **easy to follow**, presenting its ideas and methodologies clearly.\n\n3. The **DACER v2** algorithm demonstrates **strong performance** compared to other **online diffusion RL** methods."}, "weaknesses": {"value": "### Major Weaknesses:\n\n1. The authors claim that the **DACER v2** algorithm focuses on improving the diffusion efficiency of the original **DACER**. Accordingly, one would expect **DACER v2** to achieve comparable performance with fewer diffusion denoising steps compared to **DACER** using the full number of steps. However, the experimental results show that **DACER v2** not only maintains efficiency but also exhibits **stronger multi-modality** and **better sample efficiency** with fewer denoising steps. The authors are encouraged to explain the source of this additional performance gain in more detail.\n\n2. The main experimental figure (**Figure 3**) presents total average return plotted against **iterations**. Could the authors clarify what these iterations represent? Are they equivalent to **environment timesteps**? If not, please explain the rationale behind using this setting and consider including additional plots showing **total average return versus environment timesteps** for clearer comparison.\n\n3. In **Section 4.2**, the authors claim that *“in real-time industrial control tasks, the inference time should be less than 1 millisecond to meet control requirements.”* It would be helpful to provide additional evidence or references to substantiate this statement. From my perspective, an inference time of **1.6 ms** (as achieved by the original **DACER** algorithm) already appears sufficient for most robotic control tasks, and further reducing it to **0.6 ms** may offer only marginal benefits. Since this point directly relates to the **motivation of the paper**, a clearer justification would strengthen the argument.\n\n4. There exists a wide range of **diffusion acceleration methods**, such as **DDIM** and **Consistency Models**. In the introduction, the authors claim that these acceleration techniques trade performance for efficiency. It is highly recommended that the authors evaluate **DACER** combined with a diffusion acceleration method during inference and compare the results with **DACER v2**, to more clearly demonstrate the advantages of the proposed approach.\n\n5. The authors are encouraged to include a discussion or experimental comparison with **Diffusion Policy Policy Optimization (DPPO)**, as it represents a closely related and widely used approach in online diffusion RL.\n\n\n### Minor Weaknesses:\n\n1. The authors are recommended to discuss some highly related works:\n\n**Diffusion Acceleration:**\n\nSong, Jiaming, Chenlin Meng, and Stefano Ermon. \"Denoising diffusion implicit models.\" arXiv preprint arXiv:2010.02502 (2020).\n\nSong, Yang, et al. \"Consistency models.\" (2023).\n\n**Online RL with Diffusion Policy:**\n\nYuan, Xiu, et al. \"Policy decorator: Model-agnostic online refinement for large policy model.\" arXiv preprint arXiv:2412.13630 (2024).\n\nAnkile, Lars, et al. \"From imitation to refinement-residual rl for precise assembly.\" 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2025.\n\nWagenmaker, Andrew, et al. \"Steering Your Diffusion Policy with Latent Space Reinforcement Learning.\" arXiv preprint arXiv:2506.15799 (2025).\n\n**Offline RL with Diffusion Policy:**\n\nHansen-Estruch, Philippe, et al. \"Idql: Implicit q-learning as an actor-critic method with diffusion policies.\" arXiv preprint arXiv:2304.10573 (2023).\n\nPark, Seohong, Qiyang Li, and Sergey Levine. \"Flow q-learning.\" arXiv preprint arXiv:2502.02538 (2025).\n\n**I am more than willing to raise my scores if the authors adequately address my concerns**"}, "questions": {"value": "1. (Related to Major Weakness 1) Where do the observed improvements in **multi-modality** and **sample efficiency** originate from? Is **DACER v2** a strictly superior algorithm compared to the original **DACER** ?\n\n2. (Related to Major Weakness 2) What do the **iterations** in **Figure 3** represent?\n\n3. (Related to Major Weakness 3) Why do the authors believe that an inference time of **less than 1 millisecond** is required to meet **real-time industrial control** requirements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XAJOqR32mh", "forum": "gNEEOS4v5r", "replyto": "gNEEOS4v5r", "signatures": ["ICLR.cc/2026/Conference/Submission10794/Reviewer_eLPu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10794/Reviewer_eLPu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983072521, "cdate": 1761983072521, "tmdate": 1762922008263, "mdate": 1762922008263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}