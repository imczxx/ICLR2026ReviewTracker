{"id": "znnA2Opw6v", "number": 9548, "cdate": 1758127030089, "mdate": 1763685369583, "content": {"title": "KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning", "abstract": "Knowledge editing and machine unlearning are two popular approaches for large language models (LLMs) to stay up-to-date. However, the knowledge updating mechanism of LLMs remains largely unexplored due to insufficient, isolated, and small-scale evaluation. For instance, are LLMs similar to humans in modifying certain knowledge? What differs editing and unlearning as training data increases? This paper proposes KnowledgeSmith, a unified framework to systematically understand the updating mechanism of LLMs. We first cast editing and unlearning as instances of one constrained optimization problem. Then, we propose an automatic dataset generator that provides structured interventions across multiple graph levels and data scales, enabling controlled studies of how different modification strategies propagate through model knowledge. Extensive experiments demonstrate nuanced insights over knowledge propagation, plasticity scaling, consistency, and robustness. For instance, our results show that LLMs do not exhibit similar updating as humans for different levels of knowledge, and there exists consistency-capacity trade-off. We hope our findings can offer suggestions to the design of more reliable and scalable strategies.", "tldr": "", "keywords": ["Knowledge Editing", "Machine Unlearning", "Knowledge Graph"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d290850544978d5f51518ff4238bb532fa09576.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This article proposes a unified framework KnowledgeSmith to systematically study the knowledge update mechanism in Large Language Models (LLMs), with a particular focus on the performance and differences between knowledge editing and knowledge forgetting methods. The theoretical modeling of the paper is rigorous, the experimental design is meticulous, and the results are highly inspiring. The author not only proposed new evaluation metrics such as Consistency Collapse and Conflict Rate, but also constructed a structured and scalable knowledge intervention evaluation benchmark, filling the gap in current research on the lack of systematic understanding of knowledge update mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel Unified Framework: The paper introduces KnowledgeSmith, a conceptually novel and theoretically grounded framework that unifies knowledge editing and machine unlearning as instances of the same constrained optimization problem, offering a new lens for understanding and comparing these distinct but related knowledge updating mechanisms.\n- Innovative Benchmark Generation: It proposes a highly original and automatic pipeline for generating large-scale, structured evaluation benchmarks from existing knowledge graphs . This method allows for systematic probing of knowledge updates across different hierarchical levels (root, intermediate, leaf) and data scales, addressing major limitations of previous static and isolated fact-based evaluations.\n- Insightful Empirical Findings: The extensive experiments yield several significant and nuanced insights into LLM knowledge updating behavior, including propagation asymmetry (over-spreading vs. under-spreading) , plasticity limits related to hierarchy , the consistency-capacity trade-off , subject-dependent update resistance , and a unified taxonomy of failure modes."}, "weaknesses": {"value": "- Limited Scope of Investigated Update Methods: The study primarily focuses on AlphaEdit for knowledge editing and ReLearn  for machine unlearning. While these represent state-of-the-art approaches, the conclusions drawn about knowledge updating mechanisms (e.g., propagation asymmetry , consistency collapse ) might be specific to these particular algorithmic paradigms (locate-then-edit vs. retraining-based unlearning). The framework's generalizability across a broader spectrum of editing techniques (e.g., memory-based methods like SERAC , gradient-based methods like MEND ) or different unlearning approaches (e.g., gradient ascent ) remains underexplored, potentially limiting the universality of the observed phenomena.\n- Reliance on Multiple-Choice Question Format: The automatic benchmark generation pipeline exclusively creates multiple-choice questions derived from knowledge graph triples . While this format facilitates standardized evaluation and leverages existing datasets like MMLU, it may not fully capture the nuances of knowledge representation and retrieval in LLMs. Evaluating updates solely through multiple-choice accuracy might overlook impacts on generative capabilities, reasoning chains, or the model's ability to handle ambiguity, potentially offering an incomplete picture of the update's true effect.\n- Over-reliance on GPT-4o for Benchmark Generation and Insufficient Validation : The KnowledgeSmith framework heavily relies on GPT-4o for crucial generation steps, including initial KG construction, template generation, and potentially probe/distractor creation . This dependency raises concerns regarding reproducibility and potential biases inherent in the generator model, which could influence the benchmark's structure and content. While Appendix A.6 mentions quality control including external validation and manual spot checks , the paper lacks quantitative analysis from systematic human studies assessing the quality, factual accuracy, and potential hallucination rates specifically within the GPT-4o generated components (e.g., generated KG relations, question templates, or multiple-choice distractors), making it difficult to fully gauge the reliability of the benchmark itself.\n- Focus on Static Knowledge Updates : The current experimental design primarily investigates the effects of single-step, isolated knowledge editing or unlearning interventions based on a static KG structure. It does not explicitly address the challenges of dynamic knowledge updating in a continual learning setting, where knowledge evolves over time, and updates arrive sequentially. Consequently, the framework does not evaluate crucial aspects such as catastrophic forgetting across multiple sequential updates, the interaction between consecutive edits/unlearning operations, or how the observed mechanisms (like propagation or consistency trade-offs) might compound or change in a lifelong learning scenario"}, "questions": {"value": "- Does the observed propagation asymmetry (over-spreading in editing vs. under-spreading in unlearning ) stem primarily from the specific algorithms chosen (AlphaEdit/ReLearn), or is it a more fundamental characteristic inherent to the editing versus unlearning tasks themselves, regardless of the method?\n- Could the exclusive reliance on multiple-choice questions mask certain effects of knowledge updates, and how might findings, particularly the consistency-capacity trade-off , differ if evaluated using open-ended generation tasks?\n- Can the authors provide quantitative results from human evaluations assessing the factual accuracy, clarity, and distractor plausibility of the GPT-4o generated benchmark components (KG relations, question templates, multiple-choice options) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y3Ie1mmPQ9", "forum": "znnA2Opw6v", "replyto": "znnA2Opw6v", "signatures": ["ICLR.cc/2026/Conference/Submission9548/Reviewer_68hr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9548/Reviewer_68hr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536596388, "cdate": 1761536596388, "tmdate": 1762921107710, "mdate": 1762921107710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article proposes a unified framework KnowledgeSmith to systematically study the knowledge update mechanism in Large Language Models (LLMs), with a particular focus on the performance and differences between knowledge editing and knowledge forgetting methods. The theoretical modeling of the paper is rigorous, the experimental design is meticulous, and the results are highly inspiring. The author not only proposed new evaluation metrics such as Consistency Collapse and Conflict Rate, but also constructed a structured and scalable knowledge intervention evaluation benchmark, filling the gap in current research on the lack of systematic understanding of knowledge update mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel Unified Framework: The paper introduces KnowledgeSmith, a conceptually novel and theoretically grounded framework that unifies knowledge editing and machine unlearning as instances of the same constrained optimization problem, offering a new lens for understanding and comparing these distinct but related knowledge updating mechanisms.\n- Innovative Benchmark Generation: It proposes a highly original and automatic pipeline for generating large-scale, structured evaluation benchmarks from existing knowledge graphs . This method allows for systematic probing of knowledge updates across different hierarchical levels (root, intermediate, leaf) and data scales, addressing major limitations of previous static and isolated fact-based evaluations.\n- Insightful Empirical Findings: The extensive experiments yield several significant and nuanced insights into LLM knowledge updating behavior, including propagation asymmetry (over-spreading vs. under-spreading) , plasticity limits related to hierarchy , the consistency-capacity trade-off , subject-dependent update resistance , and a unified taxonomy of failure modes."}, "weaknesses": {"value": "- Limited Scope of Investigated Update Methods: The study primarily focuses on AlphaEdit for knowledge editing and ReLearn  for machine unlearning. While these represent state-of-the-art approaches, the conclusions drawn about knowledge updating mechanisms (e.g., propagation asymmetry , consistency collapse ) might be specific to these particular algorithmic paradigms (locate-then-edit vs. retraining-based unlearning). The framework's generalizability across a broader spectrum of editing techniques (e.g., memory-based methods like SERAC , gradient-based methods like MEND ) or different unlearning approaches (e.g., gradient ascent ) remains underexplored, potentially limiting the universality of the observed phenomena.\n- Reliance on Multiple-Choice Question Format: The automatic benchmark generation pipeline exclusively creates multiple-choice questions derived from knowledge graph triples . While this format facilitates standardized evaluation and leverages existing datasets like MMLU, it may not fully capture the nuances of knowledge representation and retrieval in LLMs. Evaluating updates solely through multiple-choice accuracy might overlook impacts on generative capabilities, reasoning chains, or the model's ability to handle ambiguity, potentially offering an incomplete picture of the update's true effect.\n- Over-reliance on GPT-4o for Benchmark Generation and Insufficient Validation : The KnowledgeSmith framework heavily relies on GPT-4o for crucial generation steps, including initial KG construction, template generation, and potentially probe/distractor creation . This dependency raises concerns regarding reproducibility and potential biases inherent in the generator model, which could influence the benchmark's structure and content. While Appendix A.6 mentions quality control including external validation and manual spot checks , the paper lacks quantitative analysis from systematic human studies assessing the quality, factual accuracy, and potential hallucination rates specifically within the GPT-4o generated components (e.g., generated KG relations, question templates, or multiple-choice distractors), making it difficult to fully gauge the reliability of the benchmark itself.\n- Focus on Static Knowledge Updates : The current experimental design primarily investigates the effects of single-step, isolated knowledge editing or unlearning interventions based on a static KG structure. It does not explicitly address the challenges of dynamic knowledge updating in a continual learning setting, where knowledge evolves over time, and updates arrive sequentially. Consequently, the framework does not evaluate crucial aspects such as catastrophic forgetting across multiple sequential updates, the interaction between consecutive edits/unlearning operations, or how the observed mechanisms (like propagation or consistency trade-offs) might compound or change in a lifelong learning scenario"}, "questions": {"value": "- Does the observed propagation asymmetry (over-spreading in editing vs. under-spreading in unlearning ) stem primarily from the specific algorithms chosen (AlphaEdit/ReLearn), or is it a more fundamental characteristic inherent to the editing versus unlearning tasks themselves, regardless of the method?\n- Could the exclusive reliance on multiple-choice questions mask certain effects of knowledge updates, and how might findings, particularly the consistency-capacity trade-off , differ if evaluated using open-ended generation tasks?\n- Can the authors provide quantitative results from human evaluations assessing the factual accuracy, clarity, and distractor plausibility of the GPT-4o generated benchmark components (KG relations, question templates, multiple-choice options) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y3Ie1mmPQ9", "forum": "znnA2Opw6v", "replyto": "znnA2Opw6v", "signatures": ["ICLR.cc/2026/Conference/Submission9548/Reviewer_68hr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9548/Reviewer_68hr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536596388, "cdate": 1761536596388, "tmdate": 1763689968306, "mdate": 1763689968306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed KnowledgeSmith, a unified framework and benchmark for studying knowledge updating in large language models via editing and unlearning. By framing both as constrained optimization problems and evaluating 13 LLMs on a hierarchical knowledge-graph benchmark, the study reveals asymmetric propagation, including editing over-spreads and unlearning under-spreads, a consistency–capacity trade-off, and shared failure modes. While the approach is conceptually rather than technically novel, it provides a valuable, systematic analysis of how LLMs modify and maintain knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This is the 1st work to a unified theoretical formulation linking knowledge editing and unlearning under a single constrained optimization framework.\n- The paper proposes a structured, hierarchical benchmark based on knowledge graphs, enabling fine-grained evaluation across root, intermediate, and leaf concepts.\n- The paper provides a comprehensive empirical analysis of 13 LLMs and four domains, revealing consistent phenomena such as propagation asymmetry and the consistency–capacity trade-off."}, "weaknesses": {"value": "The benchmark construction heavily relies on GPT-based data generation, which raises concerns about reproducibility and annotation reliability. Although Appendix A describes prompt structures and includes multi-stage validation with manual spot checks and cross-checks against encyclopedic sources, the extent of human verification and generation parameters is not fully specified, limiting strict reproducibility."}, "questions": {"value": "- Could the authors provide more details about the benchmark generation pipeline to improve reproducibility? For example, how consistent are the outputs across different runs? How extensive was the manual verification process mentioned in Appendix A — approximately what proportion of generated items were manually checked, and by how many annotators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cJku5Od25o", "forum": "znnA2Opw6v", "replyto": "znnA2Opw6v", "signatures": ["ICLR.cc/2026/Conference/Submission9548/Reviewer_hQBy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9548/Reviewer_hQBy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895934996, "cdate": 1761895934996, "tmdate": 1762921107418, "mdate": 1762921107418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper regards knowledge ”editing\" and \"forgetting\" as a unified constrained optimization problem and studies the performance of these two tasks by constructing multi-level data automatically generated based on a knowledge graph.\nThey mainly discovered phenomena such as Propagation Asymmetry, Plasticity Scaling and Branch-dependent Limits, and consistence-capacity Trade-off. And finally, a theoretical analysis was conducted from the perspective of singular value decomposition."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "This paper treats knowledge \"editing\" and \"forgetting\" as a unified constrained optimization problem. \nThis perspective is not very novel since some prior work has viewed unlearning as a subset of editing. However, the author conducts extensive empirical analyses from this unified viewpoint.\nThe experiments cover 13 models across 6 families of LLMs, ranging from 1B to 123B parameters. \nThese analyses provide valuable insights and offer useful suggestions for the current chaotic development of these two fields."}, "weaknesses": {"value": "This work presents a unified perspective, and the author conducts extensive comparative experiments on editing and unlearning.  However, it seems that the framework does not offer additional guiding functions, and after reading the paper, I find it hard to grasp the significance of this unified framework.  \nMoreover, this framework closely resembles the editing framework;  if we consider editing in a broader sense, forgetting could be seen as a special case of editing with the \"target empty.\"  \nLastly, the paper respectively employs one method for both editing and unlearning, but the paradigms of methods in these two areas differ significantly, and this difference is not mentioned in the analysis."}, "questions": {"value": "1. Do you think different editing or unlearning methods have a significant impact on the analysis results? Why?\n2. What advantages does your framework have over the view that \"the model editing method is regarded as a strong baseline for unlearning\", or what's your opinion on this issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ACRlYzYoYI", "forum": "znnA2Opw6v", "replyto": "znnA2Opw6v", "signatures": ["ICLR.cc/2026/Conference/Submission9548/Reviewer_7TGp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9548/Reviewer_7TGp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762364180984, "cdate": 1762364180984, "tmdate": 1762921107015, "mdate": 1762921107015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}