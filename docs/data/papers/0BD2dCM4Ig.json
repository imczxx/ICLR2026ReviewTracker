{"id": "0BD2dCM4Ig", "number": 2528, "cdate": 1757136379041, "mdate": 1763275020400, "content": {"title": "Two Sides of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models", "abstract": "Graph foundation models (GFMs), inspired by the success of LLMs, are designed to learn the optimal embedding function from multi-domain text-attributed graphs (pre-training) for the downstream cross-task generalization capability (fine-tuning). During our investigation, graph vector quantized-masked autoencoder (gVQ-MAE) stands out among the increasingly diverse landscape of GFM architectures. This is attributed to its ability to jointly encode topology and textual attributes from multiple domains into discrete embedding spaces with clear semantic boundaries. Despite its potential, domain generalization conflicts cause imperceptible pitfalls. In this paper, we instantiate two of them, and they are just like two sides of the same GFM optimization coin - Side 1 Model Degradation: The encoder and codebook fail to capture the diversity of inputs (e.g., social networks and molecular graphs); Side 2 Representation Collapse: The hidden embedding and codebook vector fail to preserve semantic separability due to constraints from narrow representation subspaces. These two pitfalls (sides) collectively impair the decoder and generate the low-quality reconstructed supervision, causing the GFM optimization dilemma during pre-training (coin). Through empirical investigation, we attribute the above challenges to Information Bottleneck and Regularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) - (1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic fusion strategy and a mixture-of-codebooks with domain-aware routing to improve information capacity. (2) Regularization Tinker for Optimization Coin, which utilizes two additional regularizations to further improve gradient supervision in our proposed Information Tinker. Notably, as a flexible architecture, MoT adheres to the scaling laws of GFM, offering a controllable model scale. Compared to SOTA baselines, experiments on 22 datasets across 6 domains demonstrate that MoT achieves significant improvements in supervised (1.4%), few-shot (3.1%), and zero-shot (3.3%) scenarios.", "tldr": "We propose MoT to address optimization pitfalls in GFMs and achieve SOTA cross-domain generalization.", "keywords": ["Foundation Model; Graph Pre-training; Vector Quantization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/2abdb6e4ce624e14ff91cd45148798851f75c236.pdf", "supplementary_material": "/attachment/fedf3aa3843901c06791f68b4f82d7bafef5c686.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies graph foundation model, and proposes a MoE module together with a graph self-supervised learning-based regularization term to enhance the performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The general layout of the paper is good \n2. the performance improvement is ok"}, "weaknesses": {"value": "In general, I think the direction studied in this paper is not very promising, and the paper's writing is very confusing. \n\n1. In the abstract, the definition of GFM is problematic. Why does GFM have to be working on text-attributed graphs? Many GFMs are working beyond TAGs, like [1,2]. \n2. Similarly, in the introduction part, the author directly goes to gVQ-MAEs? What is gVQ-MAE? I know there are some works like GFT that use the VAE as \"transferable vocabulary tokens\". This is just a small portion of GFM research, and I don't see any of these works presenting promising results. For node classification tasks, I remember GFT can't even beat the GNN baselines trained from scratch. So, I doubt the value of studying this line of work. \n3. What's the pitfall of gVQ-MAEs? I can only see Figure 2. You should first use one section to introduce these phenomena and then go to the latter method sections. \n4. For line 132,  you should cite relevant papers using token IDs in GFMs instead of citing this paper using token Ids for knowledge distillation. \n5. In Line 144, I'm very confused about what's \"model degradation\"? You should first have a preliminary section talking about these two problems. \n6. The appearance of Section 3 is very confusing. Why we come up with these questions? You should first state the motivation. \n7. the theoretrical part is some very general math formulation which doesn't even touch the graph structure. It doesn't help motivate your proposed methods\n8. In terms of KG, is link classification a reasonable task? Usually we deal with knowledge completion in KG, in which you need to compare pairwise information with all candidates. \n\n\n\n[1] Galkin M, Yuan X, Mostafa H, et al. Towards foundation models for knowledge graph reasoning[J]. arXiv preprint arXiv:2310.04562, 2023.\n[2] MÃ©ndez-Lucio, O., Nicolaou, C.A. & Earnshaw, B. MolE: a foundation model for molecular graphs using disentangled attention. Nat Commun 15, 9431 (2024). https://doi.org/10.1038/s41467-024-53751-y\n[3] Wang Z, Zhang Z, Chawla N, et al. Gft: Graph foundation model with transferable tree vocabulary[J]. Advances in Neural Information Processing Systems, 2024, 37: 107403-107443."}, "questions": {"value": "What's the merit of using vector quantization in the graph foundation model? As from [1], I acknowledge that these kinds of IDs can enhance efficiency. Based on my experience with cross-domain tasks like recommendation (which uses semantic IDs), I believe the common conclusion is that these IDs are not transferable. \n\n[1] Luo, Yuankai, et al. \"Node identifiers: Compact, discrete representations for efficient graph learning.\" arXiv preprint arXiv:2405.16435 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OopTbQalPe", "forum": "0BD2dCM4Ig", "replyto": "0BD2dCM4Ig", "signatures": ["ICLR.cc/2026/Conference/Submission2528/Reviewer_35LW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2528/Reviewer_35LW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760634424368, "cdate": 1760634424368, "tmdate": 1762916268953, "mdate": 1762916268953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "83kGCkURLI", "forum": "0BD2dCM4Ig", "replyto": "0BD2dCM4Ig", "signatures": ["ICLR.cc/2026/Conference/Submission2528/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2528/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763275019498, "cdate": 1763275019498, "tmdate": 1763275019498, "mdate": 1763275019498, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies graph foundation models (GFM) through the lens of graph oriented GFMs. The authors first identifies current bottlenecks in training gVQ-MAEs and propose several tricks to improve the optimization and regularization of the training process. Empirical results are competitive."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The intersection of graph representation learning and foundation models such as LLMs is a promising area of research."}, "weaknesses": {"value": "First of all, I would like to point out a critical flaw in this paper.\n\n## The theoretical statements are wrong in a FUNDAMENTAL SENSE\n\nUpon primary investigations of the theory statements in section 4.3 as well as proofs provided in appendix E, I found the proofs to be fundamentally flawed. Specifically:\n- In theorem 4.2, the intuition is correct since MoT incorporates a larger function class than that of vanilla edge representation construction via allowing node-to-edge dual message passing. However, upon checking the proof it seems that equation 15 in appendix E is NOT a valid lower bound, for example take $q(\\mathcal{E} | Z) \\equiv 100$ as a constant encoder, then we simply arrive at $I(Z, \\mathcal{E}) \\ge \\log 100 + H(\\mathcal{E})$ which violates the fundamental inequality in information theory that mutual information is upper bounded by entropy. Additionally, I checked the reference [1] and there seems to contain no such variational lower bounds of MI (lower bounding MI is non-trivial and often obtained lower bounds are not very meaningful). If the authors did find a place that validates this lower bound please state it in a clear fashion, **I think this bound is incorrect, and therefore the MI gap proof is likely to be incorrect as well**------it seems that the logic in the proof thereafter are somewhat messy.\n- In theorem 4.3, the statement itself is problematic: As there are altogether $M$ codebooks with each containing $K$ codewords, the entropy of any distribution supported on this collection is upper bounded by $\\log(MK)$ which further upper bounds MI. It is therefore strange to see the authors using $\\log(MK)$ as an MI lower bound. Upon investigation of the proofs in section E.2, once again it seems everywhere contains mistakes------In equation 23, what does *the probability of correct domain-specific mapping* mean? it appears pretty awkward to me that a uniform probability somehow emerges as some correct choice without formal justification. A more critical error comes right after in equation 25 that uses the inequality $I(S; Z) \\ge H(S) - H(S|Z)$, but **this is a well known equality that defines MI**. [2]\n\nThat said, I think the two aforementioned errors are rather critical, as even researchers with entry-level knowledge of information theory would identify the incorrectness of such statements, which makes me seriously suspect that LLMs are abused in writing such hallucinatory statements without careful fact checking. Furthermore, I noticed that there is no disclosure of LLM usage segments in the appendix of this paper. \n\n## Novelty issues\nThe technique of edge-wise semantic fusion described in equation 2 is actually a well-known technique in graph representation learning community that utilizes the edge-to-node duality, or line graph in the undirected graph case. Such adaptation are present even in pioneer works of GNN developments [1] and has later turned into broader protocols [2, 3]. Therefore I do not think this idea is new.\n\n## Presentation issues\nWhile there are plenty of summaries appeared at the start of paragraphs, I think the contents of the paper are organized in a non-natural way. For example, in section 3.2 the authors presented empirical investigation with improvements brought by the proposed methods, but without description of the method. I think perhaps this is a little bit harsh."}, "questions": {"value": "The most important question is: **How did you get your theory results?**\n\n[1]. Alemi, Alexander A., et al. \"Deep variational information bottleneck.\" arXiv preprint arXiv:1612.00410 (2016).  \n[2]. Cover, Thomas M. Elements of information theory. John Wiley & Sons, 1999.  \n[3]. Chen, Zhengdao, Lisha Li, and Joan Bruna. \"Supervised Community Detection with Line Graph Neural Networks.\" International conference on learning representations. 2020.  \n[4]. Jo, Jaehyeong, et al. \"Edge representation learning with hypergraphs.\" Advances in Neural Information Processing Systems 34 (2021): 7534-7546.  \n[5]. Wu, Ruofan, et al. \"Grande: a neural model over directed multigraphs with application to anti-money laundering.\" 2022 IEEE International Conference on Data Mining (ICDM). IEEE, 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "v9eTWyGtes", "forum": "0BD2dCM4Ig", "replyto": "0BD2dCM4Ig", "signatures": ["ICLR.cc/2026/Conference/Submission2528/Reviewer_QYw5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2528/Reviewer_QYw5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760973158800, "cdate": 1760973158800, "tmdate": 1762916268727, "mdate": 1762916268727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates critical optimization challenges in graph foundation models (GFMs), particularly focusing on graph vector-quantized masked autoencoders. It identifies two interrelated pitfalls that arise during multi-domain pre-training: failure to capture input diversity and loss of semantic separability. They propose Mixture-of-Tinkers (MoT), which integrates an Information Tinker (edge-wise semantic fusion and mixture-of-codebooks) and a Regularization Tinker (contrastive alignment and load-balancing constraint). In addition, the study provides theoretical analyses that connect MoT with the Information Bottleneck principle. The extensive experiments on 22 datasets across 6 domains demonstrate state-of-the-art results under supervised, few-shot and zero-shot settings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents preliminary experiments that reveal inherent issues concerning model degradation and representation collapse in existing GFMs.\n2. The proposed MoT framework provides clear mitigation strategies including information and regularization tinkers. The study provides a theoretical analysis for the Mixture-of-Tinkers from the perspective of the Information Bottleneck principle.\n3. The paper provides a broad range of empirical evidence demonstrating that MoT outperforms existing GFM baselines across multiple domains and training paradigms."}, "weaknesses": {"value": "1. While MoT achieves clear performance gains, the proposed framework is an incremental improvement built upon VQ-VAE and MoE variants. The two regularization objectives are closely related to the commitment loss and expert-load loss used in prior works.\n\n2. The adaptation of the Information Bottleneck (IB) in Definition 4.1 appears inconsistent with prior IB methods [1][2]. Instead of constraining the latent space to retain only minimal sufficient information within the input graph data, the objective in Definition 4.1 simultaneously maximizes mutual information with domain-specific semantics $S$ and graph topology $\\mathcal{E}$. It appears to overemphasize domain-dependent semantic/topological information and weaken the theoretical connection to the proposed framework.\n\n3. Most experiments focus solely on the accuracy. The paper lacks deeper analyses of representation quality (e.g., the similar experimental results in Fig. 2) to substantiate its claim of mitigating representation collapse.\n\n\n[1] Graph Information Bottleneck, 2020.\n\n[2] Deep learning and the information bottleneck principle, 2015."}, "questions": {"value": "1. How is the *domain generalization conflict* formally defined in this work? Furthermore, what distinguishes the proposed Information Bottleneck formulation for GFMs from conventional graph IB methods? It would be helpful to include additional analyses or visualizations (e.g., KL Divergence Heatmap in Fig. 2) to validate the claimed theoretical connection.\n\n2. Introducing the Mixture-of-Codebooks (MoC) may cause additional challenges of load imbalance or codebook collapse. Beyond accuracy-based metrics,  other metrics (e.g., codebook utilization and expert activation) are necessary for a more insightful analysis of the MoC.\n\n3. The functionality of the MoC appears achievable through simpler and more established vector quantization methods. Could the proposed Mixture-of-Codebooks be simplified or replaced with a more computationally efficient quantization variant (e.g., RQ-VAE or OPQ) in the ablation study?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ooLFrjUZy7", "forum": "0BD2dCM4Ig", "replyto": "0BD2dCM4Ig", "signatures": ["ICLR.cc/2026/Conference/Submission2528/Reviewer_swej"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2528/Reviewer_swej"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840490883, "cdate": 1761840490883, "tmdate": 1762916268487, "mdate": 1762916268487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "I am unable to recommend acceptance for this paper. My primary reason is that the paper, in its current form, is not reviewable due to severe issues with clarity, structure, and presentation. While the proposed method may have technical merit, the current manuscript is unintelligible, making it impossible to assess the validity of the approach or the significance of the results.\n\nMy core concerns are centered on the quality of the writing and presentation:\n- The paper fails to provide sufficient context for the problem being addressed. Section 3, in particular, introduces experiments without a clear explanation of their purpose, the hypotheses being tested, or how they connect to the paper's broader claims. The reader is left to guess the motivation behind the experimental design.\n- The figures and diagrams provided are overly dense and they do not serve as an effective aid for understanding. The captions are minimal and fail to explain the figures or what the reader should conclude from them. A good figure should be largely understandable on its own, and these are not.\n- The paper's structure does not follow a logical narrative. Most paragraphs, instead of building an argument, are a dense collection of bullet points. This style of writing makes it incredibly difficult to follow the authors' reasoning or understand the flow of the proposed method.\n\nIt is entirely possible that the research described in this paper is solid and contains valuable contributions. However, in the absence of an intelligible presentation, I cannot provide a technical assessment."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Look at summary"}, "weaknesses": {"value": "Look at summary"}, "questions": {"value": "Look at summary"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WyHQT1OPHJ", "forum": "0BD2dCM4Ig", "replyto": "0BD2dCM4Ig", "signatures": ["ICLR.cc/2026/Conference/Submission2528/Reviewer_Ap7s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2528/Reviewer_Ap7s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929182196, "cdate": 1761929182196, "tmdate": 1762916268357, "mdate": 1762916268357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}