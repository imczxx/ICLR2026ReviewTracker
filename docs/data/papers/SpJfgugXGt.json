{"id": "SpJfgugXGt", "number": 9736, "cdate": 1758137165785, "mdate": 1759897701484, "content": {"title": "Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime", "abstract": "Data selection plays a crucial role in data-driven decision-making, including in large language models (LLMs), and is typically task-dependent. Properties such as data quality and diversity have been extensively studied and are known to enhance model performance. However, it remains unclear whether there exist other quantitative and general principles of data selection that can consistently improve performance, especially for complicated tasks.  In this paper, we demonstrate that selecting more uniformly distributed data can improve training efficiency while enhancing performance. Specifically, we establish that more uniform (less biased) distribution leads to a larger minimum pairwise distance between data points, denoted by $h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training dynamics of gradient descent (GD). Moreover, we theoretically show that the approximation error of neural networks decreases as $h_{\\min}$ increases. Our analysis introduces a convergence framework for GD beyond the Neural Tangent Kernel (NTK) regime, applicable to a broad class of architectures, including transformers, without requiring Lipschitz smoothness. This framework further provides theoretical justification for the use of residual connection and function composition in deep neural architectures. In the end, we conduct comprehensive experiments for supervised fine-tuning across various settings, including different optimization strategies, model sizes, and training datasets. The results consistently demonstrate that selecting data by maximizing pairwise distance significantly accelerates training and achieves comparable or better performance in LLMs across diverse datasets. Code and Datasets are available at the link: https://anonymous.4open.science/r/data-uniformity-1A5C.", "tldr": "", "keywords": ["data selection", "data uniformity", "optimization", "gradient descent", "neural tangent kernel", "large language model", "approximation", "LLM efficiency", "finetuning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd699121de5fb1d56754ce4126ac12baba39f653.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper analyzes how data uniformity—measured by the minimum pairwise distance among samples—affects neural network training efficiency and approximation ability. It develops a convergence framework beyond the NTK regime and proposes a simple greedy data selection method that maximizes pairwise distances. Experiments on several datasets and LLaMA models show faster convergence and comparable or better performance using fewer tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper establishes a clear link between geometric data uniformity and training convergence beyond the NTK regime.\n* This work demonstrates that more uniform subset selection can reduce training cost while maintaining or improving performance."}, "weaknesses": {"value": "* The performance gains of the uniform over random selection are relatively minor (Figure 1), raising doubts about the practical significance and scalability.\n* The evaluation scope is limited to only a few benchmarks and llama-1 models; broader and more diverse downstream tasks and base models would be necessary to substantiate the generality of the approach.\n* The proposed data-uniformity procedure requires computing or approximating all pairwise distances among samples---an $O(N^2)$ operation---but the paper does not analyze its computational overhead.\n* In Figure 4, the training and validation losses of the 16K full dataset display abnormal behavior."}, "questions": {"value": "pls see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UPXuL6Pe2T", "forum": "SpJfgugXGt", "replyto": "SpJfgugXGt", "signatures": ["ICLR.cc/2026/Conference/Submission9736/Reviewer_9Va8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9736/Reviewer_9Va8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904284127, "cdate": 1761904284127, "tmdate": 1762921232914, "mdate": 1762921232914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores whether selecting more uniformly distributed training data improves neural network training. It formalizes uniformity via the minimum pairwise distance $h_{\\min}$, proves that larger $h_{\\min}$ accelerates gradient descent beyond the NTK regime and reduces approximation error, and introduces a greedy distance-based sampling strategy to increase uniformity. Experiments on both toy regression tasks and LLaMA fine-tuning (e.g., WizardLM, LESS) show that uniform subsets can reach similar or better performance than larger random subsets while achieving faster convergence."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n\n* The paper tackles data selection, an increasingly important topic for efficient LLM training.\n* Comprehensive theory analysis, provides a general convergence result beyond NTK assumptions and links data geometry to dynamics and approximation error."}, "weaknesses": {"value": "Weaknesses\n\n* The paper uses max-min distance sampling as the core uniformity criterion. However, pure maximum distance does not necessarily guarantee globally uniform coverage. For example, if the data contains two distant dense clusters, the greedy selection may oscillate between these clusters and ignore other regions of the space. Please correct me if this interpretation is incorrect.\n\n* The proposed selection strategy is closely related to prior work on distance-based uniform sampling. For example, the method in [1] shares many similarities with this paper, which also claim uniformity is important and also improve data uniformity via greedy distance selection while introducing extra constraints to alleviate the cluster oscillation problem mentioned above. It would strengthen the paper to explicitly compare and discuss differences from this line of work.\n\n* In several settings, the accuracy of uniform sampling is very close to or worse than random sampling. Also the loss curve of uniform sampling also similar to random sampling. \n\n* When comparing partial and full datasets, are the models trained for the same number of iterations? If training the full dataset longer, does it eventually surpass the uniform subset? Clarification of additional results would be helpful.\n\n* Some curves (e.g., Figure 4) show that the full dataset appears to stop training after only a few epochs while others run to 100 epochs. I assume this is due to instability or large loss spikes. Could the authors provide an explanation for this behavior?\n\n* How does the method behave when rare but important examples exist? Uniformity-based selection may inadvertently discard such samples. A discussion or experiment on rare scenarios would strengthen the paper.\n\n* The pairwise distance computation in greedy selection is potentially $O(n^{2})$, which may be expensive for very large corpora. A runtime analysis would be useful.\n\n[1] An effective negative sampling approach for contrastive learning of sentence embedding"}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ILeNs7kHLz", "forum": "SpJfgugXGt", "replyto": "SpJfgugXGt", "signatures": ["ICLR.cc/2026/Conference/Submission9736/Reviewer_6igN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9736/Reviewer_6igN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934598177, "cdate": 1761934598177, "tmdate": 1762921232391, "mdate": 1762921232391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new theoretical framework for analyzing convergence of neural networks based on data uniformity. To avoid the standard, but often impractical, Lipschitzness assumption in previous literature, the authors propose and use a new Poly-smoothness assumption which is weaker and compatible with empirical deep neural networks such as transformers and residual networks.The theoretical convergence analysis extends beyond the NTK regime. The authors also provide a new perspective on how residual connections help with neural network training from non-degeneracy of Jacobians.\n\nBased on the theoretical analysis, which shows that the uniformity of data (measured by minimum distance between data $h_{\\min}$) increases convergence speed of GD, the authors propose a new data selection metric encouraging uniformity. Empirical results show that this method achieves on-par or better performance compared with SOTA results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a new perspective on how data uniformity helps with training, justified with theoretical analysis. The effectiveness of the proposed approach is validated through empirical results.\n\n2. The proposed Poly-smoothness condition aligns better with neural networks used in practice, compared to standard Lipschitzness. This might be helpful for future analysis of deep neural networks."}, "weaknesses": {"value": "For the theoretical part:\n1. It is unclear why the minimum pairwise distance $h_{\\min}$ is a good characterization of data uniformity. Specifically, when the data distribution is fixed, $h_{\\min}$ will decrease as the sample size increases. This means that the convergence speed in Theorem 2 becomes slower with more samples and becomes $0$ when the sample size tends to infinity. Is this an intended behaviour? What if we consider infinitely many data points sampled from a continuous distribution (population loss)?\n\n2. Figure 2 is a good illustration of the proof sketch, but no other parts of the main text has explained how the proofs are constructed. Can you include more explanations on the theoretical ideas behind the proof, how does the proof connects convergence speed with data uniformity, and how does it go beyond the NTK regime?\n\nFor the empirical part:\n1. In figure 1(b), it is claimed that the 10k Uniform subset outperforms the 10k Random subset, but the random subset actually has higher accuracy on TruthfulQA MC (even higher than full training).\n\n2. In figure 4, why does the Z-core method take a longer training time than uniform selection, with about the same number of iterations and samples?\n\n3. The data selection method (Algorithm 1) has a computational cost depending quadratically on the dataset size. This can be burdensome when $N$ is large."}, "questions": {"value": "See the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VAWc1f664f", "forum": "SpJfgugXGt", "replyto": "SpJfgugXGt", "signatures": ["ICLR.cc/2026/Conference/Submission9736/Reviewer_Xh7L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9736/Reviewer_Xh7L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973484530, "cdate": 1761973484530, "tmdate": 1762921232086, "mdate": 1762921232086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes \"data uniformity\" (maximizing the minimum pairwise distance, $h_{min}$) as a principle for efficient LLM data selection. The authors present a theoretical framework, claiming to go \"beyond the NTK regime\", to argue that uniform data accelerates gradient descent training for a family of non-linear architectures. Empirically, they select a \"uniform\" subset of data using Word2Vec embeddings and show that they can fine-tune LLaMA models significantly faster (e.g., 2x) while achieving comparable accuracy to the full dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's key strength is its strong and practically relevant empirical result: that a small, uniformly-selected data subset can fine-tune LLMs significantly faster while matching the performance of the full dataset. Moreover, the paper is theoretically ambitious (I'm not sure if the results do actually imply what the authors claim, see weaknesses), tackling the important problem of data selection by attempting to build a convergence framework for non-linear architectures."}, "weaknesses": {"value": "**Presentation**: The paper is very densely written, making the theoretical arguments difficult to read and understand. The overall presentation could be significantly improved for clarity.\n\n**Beyond NTK Claim**: The 'beyond NTK' claim is not fully convincing. In standard NTK analysis, a PL-like inequality is proven where the constant is the minimum eigenvalue of the kernel at initialization. This paper seems to follow a similar structure, proving a PL-like inequality (Figure 2) where the PL-constant ($\\mu_{low,s,X}$) is now dynamic and path-dependent, and global smoothness is relaxed to a local smoothness. It remains unclear from Theorem 2 how this framework guarantees that feature learning (i.e., weights moving far from initialization) can actually occur, rather than just describing a different form of local convergence.\n\n**On Corollary 3**: Corollary 3 appears to contain a significant logical leap. It first establishes a general, interesting bound on the convergence rate parameter ($\\mu_{low,s,X}$) based on the density of local data clusters (the $\\sqrt{\\sum h_{ij}^2}$ term within a radius $H$). However, it then arbitrarily sets $H = h_{\\text{min}}$, which means the cluster $D_{i,H}$ is empty for almost every point $x_i$. The only time it is non-empty is when selecting the two points that are exactly $h_{\\text{min}}$ apart. The paper connects this to Theorem 1 (biased sampling $\\to$ small $h_{min}$) to claim that more data uniformity implies faster convergence. \n\nThis reduces the claim to the specific and well-known case that having a single pair of near-duplicates is bad for convergence. This specific case does not provide a sufficient theoretical justification for the main goal of the paper “data uniformity speeds up convergence”, making their actual result feel like a big over claim. \n\n**Confounded Definition of \"Uniformity\" in Experiments**: The practical definition of 'uniformity' in the experiments is confounded. As stated in Section 5.1, distances are measured in the embedding space of an external, pre-trained Word2Vec model. This means the selected \"uniform\" subset is an artifact of this specific and dated embedding choice, not a fundamental, intrinsic property of the data itself. Is this dependent on the embedding choice? What if you choose some other model? \n\n**Theory-Practice Disconnect**: The experiments in Section 5 do show a clear and valuable empirical finding: the uniformly-sampled subset converges significantly faster and achieves comparable performance (e.g., Figure 1, 5). This empirical contribution is good, but it is not convincingly explained by the provided theory—which seems to be the main point of this work.\n\nI am keeping a low score because of the above issues. I would be more than happy to engage with the authors during rebuttal and rethink my score."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9l03xA06fa", "forum": "SpJfgugXGt", "replyto": "SpJfgugXGt", "signatures": ["ICLR.cc/2026/Conference/Submission9736/Reviewer_8yGz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9736/Reviewer_8yGz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762836301868, "cdate": 1762836301868, "tmdate": 1762921231833, "mdate": 1762921231833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}