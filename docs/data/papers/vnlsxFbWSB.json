{"id": "vnlsxFbWSB", "number": 16362, "cdate": 1758263751204, "mdate": 1759897245732, "content": {"title": "Bench-CoE: A Framework for Collaboration of Experts from Benchmark", "abstract": "Large Language Models (LLMs) are key technologies that drive intelligent systems to handle multiple tasks. To meet the demands of various tasks, an increasing number of LLMs-driven experts with diverse capabilities have been developed, spreading from language to visual understanding and generalization, accompanied by corresponding benchmarks to evaluate their performance. This paper proposes the Bench-CoE framework, which enables Collaboration of Experts (CoE) by effectively leveraging benchmark evaluations to achieve optimal performance across various tasks. Bench-CoE consists of a set of specialized expert models, a router for assigning tasks to corresponding experts, and a benchmark dataset for training the router. Based on this framework, we first formulate Query-Level Bench-CoE that is an abstraction of existing CoE methods exploiting the benchmark dataset. We further propose Subject-Level Bench-CoE, a new method that effectively addresses the potential issues of Query-Level Bench-CoE in poor generalization and labeling costs during training the router. Experiments show that the Query-Level Bench-CoE excels in in-distribution tasks, while the Subject-Level Bench-CoE demonstrates stronger out-of-distribution generalization. Our proposed Bench-CoE achieves efficient expert collaboration with minimal training label costs, improving adaptability in multi-task and cross-domain scenarios.", "tldr": "We propose the Bench-CoE framework that enables Collaboration of Experts (CoE) by effectively leveraging benchmark evaluations to achieve optimal performance across various tasks.", "keywords": ["Bench-CoE", "Framework", "Collaboration of Experts", "Benchmark", "LLMs"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6511239f5f3769c1d5ff62d9986cc51ec0e06c44.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the Bench-CoE framework to enable effective collaboration among LLMs by leveraging benchmark evaluation results. The authors validate Bench-CoE through experiments on multimodal (MMMU, MMStar) and NLP (MMLU-Pro, BigBench-Hard) tasks, demonstrating that Subject-Level Bench-CoE outperforms individual experts and Query-Level methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The core insight of using benchmark evaluations as \"free labels\" for router training effectively solves two key pain points of existing CoE methods.\n- The subject-expert mapping mechanism allows dynamic updates (e.g., integrating new experts or updating leaderboard rankings) without retraining the router"}, "weaknesses": {"value": "- While the paper contrasts Bench-CoE with Mixture of Experts (MoE) and traditional CoE methods, it overlooks recent works that also leverage benchmarks for model selection or routing.\n- The paper mentions using BERT and TinyLLaVA as classifiers but provides no details on training details.\n- The experiments are weak, it only compare with single methods.\n- The citation format is not correct."}, "questions": {"value": "- What is the contribution of this work compared to recently proposed routing methods?\n- What if multiple experts perform equally well on a subject (e.g., two models with <1% accuracy difference on MMMU’s Math subject)? How does the framework handle ambiguity in mapping?\n- How to choose the subject effectively? What if I want to add new subject?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AuaYAetibj", "forum": "vnlsxFbWSB", "replyto": "vnlsxFbWSB", "signatures": ["ICLR.cc/2026/Conference/Submission16362/Reviewer_JVyp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16362/Reviewer_JVyp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720112220, "cdate": 1761720112220, "tmdate": 1762926490564, "mdate": 1762926490564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper builds a framework to route queries to different LLM-based experts to achieve high performance and generalize across queries from new tasks. It proposes to use subject level meta-data of benchmark evaluations to train a router. Assuming the subject-expert mapping (i.e., best expert for each subject) is available through benchmark evaluation, the router is trained to predict the subject of the query, to then route the query to the corresponding best expert."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem is highly relevant with respect to efficiency, reuse, and collaborative development. \n- The paper is easy to follow \n- It provides ablations showing that subject level routing generalizes better compared to existing approaches that route at query level without utilizing subject level meta-data"}, "weaknesses": {"value": "- It assumes that benchmarks have clearly separated subject level meta-data in them, which might not always be true. Categorizing a benchmark into a set of distinct subjects/expertise is a challenging problem on its own.\n- There are cases where even though the subject remains the same, there might be different difficulty associated with them which won’t be captured if routing is learnt at subject level. For example, GSM8k vs AIME benchmark fall under math subject, where as there might multiple experts associated with this subject and they have different performances across the datasets in a given subject. \n- Naive evaluation doesn’t make sense. You can’t have the same training and test dataset. \n- For other evaluations, please provide non-zero shot baselines like best expert in the pool or best expert per query when evaluated with every expert to get a sense of benefit of the approach. For these baselines, see (https://arxiv.org/pdf/2402.05859)"}, "questions": {"value": "Please see weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o4XWSTrjRd", "forum": "vnlsxFbWSB", "replyto": "vnlsxFbWSB", "signatures": ["ICLR.cc/2026/Conference/Submission16362/Reviewer_jzbB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16362/Reviewer_jzbB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765552597, "cdate": 1761765552597, "tmdate": 1762926489562, "mdate": 1762926489562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work follows the line of Collaboration of Experts and proposes 2 abstractions that route experts, Query-level and subject-level Bench-CoT and show interesting results that Query-level excels in in-distribution tasks and Subject-level works better in OOD tasks"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The methodology seems to be novel, simple, and effective, and potentially efficient.\n2. The related work section is well-written and helps with the understanding of the scope of this work"}, "weaknesses": {"value": "1. There is usually a suite of benchmarks people in this domain test to showcase that other benchmark performances do not drop; in this work, they do MMStar, and there are more OOD/Cross-Bench datasets like MME, etc., as well. It would be more convincing to show that the performances using this method are on par or do not drop much.\n2. Usually, it is great to show a model of different sizes for ablations to show the Subject-level and query-level different advantages. Doesn't need many, but good to have.\n3. L160, not sure where WGM, LGM come from\n4. The citation format can use some care."}, "questions": {"value": "1. Subject-level and query-level has different advantage, so what is the recommendation for practitioner"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "avn6wTBQAI", "forum": "vnlsxFbWSB", "replyto": "vnlsxFbWSB", "signatures": ["ICLR.cc/2026/Conference/Submission16362/Reviewer_yC4P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16362/Reviewer_yC4P"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939006484, "cdate": 1761939006484, "tmdate": 1762926488793, "mdate": 1762926488793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Bench-CoE, a new framework that trains a router to select expert models for a specific query. The paper proposes 2 methods: (1) query based router which directly routes the query to an expert model (2) subject based which first maps the query to a subject, then finds the expert that excels most at this subject to answer the question. The router is trained based on models' results on benchmark. The paper then conducts experiments and shows that this framework performs better than single expert models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes a new framework that trains a router to select an expert for a given query \n- Sees some performance gain over single experts.\n- The paper writing is generally clear and easy to follow"}, "weaknesses": {"value": "- Experiments are not solid enough and lack some essential baselines. The paper only compares their method of combining multiple experts with just a single model. It is not surprising that this would perform better than a single model. The paper should conduct more detailed analysis with simple baseline methods such as majority vote, or other router training methods / other expert selection methods. \n- The selected experts are outdated and not essentially the state-of-the-art models, it would be useful to incorporate the current state-of-the-art models / combine strong models with weak models and conduct how the performance would change.\n- In each experiment setting, how many expert models there should be and what the expert models should be are heuristically chosen. The paper lacks essential ablation studies on these design choices.\n- The design of the subject level router is not convincing enough. Especially in OOD cases because the performance is intrinsically bounded by the subject labels."}, "questions": {"value": "- For out of distribution cases (for example, when the subject labels from the training set and the test set are completely different), would the subject level router still help? It would be useful to discuss this.\n- How does the method perform compared with other baseline methods as mentioned in the weakness section?\n- How does the expert number / expert choices affect performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A3VdyZgcSa", "forum": "vnlsxFbWSB", "replyto": "vnlsxFbWSB", "signatures": ["ICLR.cc/2026/Conference/Submission16362/Reviewer_UVbT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16362/Reviewer_UVbT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989559262, "cdate": 1761989559262, "tmdate": 1762926486846, "mdate": 1762926486846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Bench-CoE, a framework for collaborating multiple expert LLMs by leveraging benchmark evaluation results for routing. The authors formalize 1) Query-Level Bench-CoE, which routes queries based on which expert performs best on each individual query, and 2) Subject-Level Bench-CoE, which classifies queries into subjects and routes to experts based on subject-level benchmark performance. Experiments on multimodal tasks and language tasks show that Query-Level excels on in-distribution data while Subject-Level achieves better cross-domain generalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The framework is well-defined and its difference from the existing methods are clearly stated.\n- The proposed methods are evaluated under both in-domain and out-of-domain scenarios which makes the evaluation comprehensive.\n- Experiments show performance improvement over the existing baselines."}, "weaknesses": {"value": "There are existing works that propose to route experts according to the queries or topics which are not included in the paper as baselines. The proposed methods, therefore, IMO, are not very novel and bear limited impact to the research community."}, "questions": {"value": "How do the proposed methods perform when compared to the \"LoRA Soups\" line of works, which seek to merge LoRA weights instead of routing inputs to them [1]?\n\n[1] Prabhakar, Akshara, et al. \"Lora soups: Merging loras for practical skill composition tasks.\" Proceedings of the 31st International Conference on Computational Linguistics: Industry Track. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yjwaDYCwvd", "forum": "vnlsxFbWSB", "replyto": "vnlsxFbWSB", "signatures": ["ICLR.cc/2026/Conference/Submission16362/Reviewer_FHpQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16362/Reviewer_FHpQ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16362/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762966169311, "cdate": 1762966169311, "tmdate": 1762966169311, "mdate": 1762966169311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their constructive feedback and for recognizing the importance of efficient expert collaboration and the potential of our Bench-CoE framework. We are encouraged that reviewers found our framework \"well-defined\" (FHpQ), the methodology \"simple and effective\" (yC4P), and the problem \"highly relevant\" (jzbB).We have addressed the common concerns raised by multiple reviewers below, followed by specific responses to each reviewer.1. Comparison with Additional Baselines (Reviewers UVbT, jzbB, JVyp):We acknowledge the suggestion to include more robust baselines. In the revised version, we will include:Oracle Baseline: The theoretical upper bound where the best expert is always selected for every query (effectively the \"ground truth\" for our Query-Level router).Majority Vote: A standard ensemble baseline where the most common answer among experts is selected.Random Selection: Randomly selecting an expert for each query.Preliminary results indicate that Subject-Level Bench-CoE significantly outperforms Random and approaches the Oracle performance in OOD settings more effectively than Query-Level routing due to better generalization.2. Clarification on \"Naive\" Evaluation (Reviewer jzbB):We apologize for the confusion regarding the \"Naive\" evaluation. This setting (Train on $B_{val}$, Test on $B_{val}$) was intended solely as a \"sanity check\" to verify that the router can learn the routing logic given perfect information, not to claim generalization. We will clarify this distinction in the revision and move strictly to In-Distribution (Test split) and Out-of-Distribution evaluations for performance claims."}}, "id": "Ehor4LrCn4", "forum": "vnlsxFbWSB", "replyto": "vnlsxFbWSB", "signatures": ["ICLR.cc/2026/Conference/Submission16362/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16362/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission16362/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763706509990, "cdate": 1763706509990, "tmdate": 1763706509990, "mdate": 1763706509990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}