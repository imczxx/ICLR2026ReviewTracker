{"id": "cgr5OAXe3q", "number": 14417, "cdate": 1758234848150, "mdate": 1759897371668, "content": {"title": "Towards Robust Referring Expression Segmentation for Complex Reasoning in the Wild", "abstract": "Despite the advances in Referring Expression Segmentation (RES) benchmarks, their evaluation protocols remain constrained, primarily focusing on either single targets with short queries (containing minimal attributes) or multiple targets from distinctly different queries on a single domain. This limitation significantly hinders the assessment of more complex reasoning capabilities in RES models.\n    We introduce  WildRES, a novel benchmark that incorporates long queries with diverse attributes and non-distinctive queries for multiple targets. This benchmark spans diverse application domains, thus enabling more rigorous evaluation of complex reasoning capabilities in real-world settings. \n    Our analysis reveals that existing RES models demonstrate substantial performance deterioration when evaluated on WIldRES. To address this challenge, we introduce SynRES, an automated pipeline generating densely paired compositional synthetic training data through three innovations: (1) a dense caption-driven synthesis for attribute-rich image-mask-expression triplets, (2) reliable semantic alignment mechanisms rectifying caption-pseudo mask inconsistencies via Image-Text Aligned Grouping, and (3) domain-aware augmentations incorporating mosaic composition and superclass replacement to emphasize generalization ability and distinguishing attributes over object categories.\n    Experimental results demonstrate that models trained with SynRES achieve consistent improvements on not only our complex WildRES benchmark but also classic RES benchmarks (e.g. RefCOCO/+/g).\nCode is available at https://anonymous.4open.science/r/SynRES-Review-4B1F.\nDataset will be available upon acceptance.", "tldr": "", "keywords": ["Referring Expression Segmentation", "Synthetic data", "Multimodal augmentation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ba281f586a4ad749dc642f25f94204bf778ed2c.pdf", "supplementary_material": "/attachment/c086636c0c2aeea809f4586753e9c5607fc0af65.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces WildRES, a new benchmark designed to evaluate Referring Expression Segmentation (RES) models under complex, real-world conditions that require advanced compositional reasoning. Unlike existing benchmarks, which mainly focus on simple, single-target expressions with limited attributes, WildRES includes long, attribute-rich expressions, multi-target cases with shared attributes, and domain-shifted images from scenes such as crowds, driving, and robotics. To address the lack of large-scale annotated data for such complex scenarios, the authors further propose SynRES, an automated synthetic data generation pipeline that creates densely paired image–mask–text triplets through dense caption-driven synthesis, semantic alignment grouping, and domain-aware augmentations. Experiments across several strong RES baselines (LISA, GSVA, and GLaMM) show that incorporating SynRES significantly improves performance and generalization on both WildRES and traditional datasets like RefCOCO, RefCOCO+, and RefCOCOg."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A novel benchmark for RES is proposed, which is meaningful and valuable. \n2. A new pipeline is proposed to generate and augment data that can be used for training RES models.\n3. Experimental results demonstrate the effectiveness of the proposed SynRES."}, "weaknesses": {"value": "1. The WildRES benchmark only has 724 images, which is relatively small.  \n2. In SynRES, why are the referring expressions synthesized and expanded from real images? Would it be possible to let an LLM directly generate a large number of referring expressions instead? This might further enhance the diversity of the dataset.\n3. In Section 4.3, the authors claim that the proposed method can help mitigate gender bias. However, no experiments are provided to support this claim. If the proposed synthetic dataset truly reduces bias, it would represent an interesting and valuable contribution.\n4. In Tables 1 and 2, all experiments are conducted using MLLM-based segmentation models. Can the proposed data synthesis approach also help enhance the performance of traditional RES models that do not rely on LLMs?\n5. As indicated in Lines 373-374, the experiments are conducted by fine-tuning models that have already been pretrained on segmentation data. It would be interesting to further verify whether the proposed synthetic data could also be used to train these models from scratch.\n6. As shown in Table H of the appendix, the performance improves only slightly when the amount of synthetic training data increases from 25% to 100%. Does this indicate that the proposed synthetic-data–based training strategy lacks strong scalability?"}, "questions": {"value": "1. During fine-tuning, apart from the proposed SynRES dataset, why do the authors also use other datasets such as gRefCOCO? Is the synthetic data alone insufficient for training? This raises another concern: is the observed performance improvement truly due to the synthetic data, or simply because tarining on existing datasets for more steps?\n2. In Table G of the appendix, there is a sharp performance fluctuation when $\\tau$ increases from 0.6 to 0.65. Could the authors provide an explanation for this behavior?\n3. Does WildRES include reasoning-required cases, similar to those in the ReasonSeg benchmark?\n\nI would greatly appreciate it and would be willing to reconsider my score if the authors address my concerns and questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5Wa9iURNYF", "forum": "cgr5OAXe3q", "replyto": "cgr5OAXe3q", "signatures": ["ICLR.cc/2026/Conference/Submission14417/Reviewer_2H4Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14417/Reviewer_2H4Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761470921433, "cdate": 1761470921433, "tmdate": 1762924827015, "mdate": 1762924827015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current Referring Expression Segmentation benchmarks mainly focus on either single targets with short queries or multiple targets from distinctly different queries on a single domain. To address this issue, this paper proposes WildRES, which incorporates long queries with diverse attributes and non-distinctive queries for multiple targets. Therefore, the proposal benchmark aims to deal with complex reasoning in a real-world setting. In light that existing models perform significantly worse on WildRES, this paper proposes SynRES, an automatic pipeline to generate paired synthetic data. SynRES shows promising performance gain for different models, not only on the WildRES dataset, but also on the classic RES benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The WildRES is well curated, which complements the current limitation in the RES benchmarks in 1) providing more attributes for the  single object referring, 2) images featuring multiple objects sharing the same class but distinct attributes \n\n\nIt is relatively hard to obtain high-quality paired data for the RES tasks, and the motivation to leverage the automatic synthetic data makes sense. The synthetic data generation pipeline is helpful in generating data in a more controlled setting: many attributes/ identical objects with different attributes. The author proposes a solid pipeline to conduct the data generation: 1) captioning to generate diverse attributes and T2I generation for the synthetic images; 2) pseudo-label generation and filtering via group similarity; 3) RES-specific augmentation enhancement. Comprehensive ablation studies are provided to justify the key design choices in this synthetic data generation workflow.\n\nExperimental results across both conventional RES benchmarks and the proposed WildRES dataset—under in-domain and cross-domain settings—demonstrate strong performance and validate the effectiveness of the proposed approach."}, "weaknesses": {"value": "The method part for the SynRES can be organized in a better way. In Section 4.2, it is good to have clear mathematical annotations, but it would be better to organize them in a more systematic manner. It is also suggested to have some high-level ideas before diving into the technical details. For example, what is the high-level idea and assumption for clustering according to expression pairs' similarity in a global manner (i.e., averaging over images)? Without it, the design choice would seem to be more random and not well-motivated. \n\nIn the Debiased Text Augment in Section 4.3, I understand that if you do the superclass replacement for the conventional RES dataset, there can be false negatives present. It also makes sense that we can avoid it with the synthetic data, but the way that it is achieved is not obvious enough. The author should make this part clearer.\n\nFrom the LISA baseline in Table one,  it seems that the improvement on the multiple objects with shared attributes is the most significant; this also applies to the LISA in Table 2 for the WildRES-DS setting. Adding the synthetic data training seems to hurt the many attribute performance a little bit in terms of gIoU in some settings. Can the Author comment on the possible reason?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3Snk6h1rDZ", "forum": "cgr5OAXe3q", "replyto": "cgr5OAXe3q", "signatures": ["ICLR.cc/2026/Conference/Submission14417/Reviewer_yvJL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14417/Reviewer_yvJL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674231204, "cdate": 1761674231204, "tmdate": 1762924826241, "mdate": 1762924826241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the Referring Expression Segmentation (RES) task. To address the issues of limited evaluation capability in existing benchmarks and suboptimal performance of models in real-world scenarios, this paper proposes the WildRES benchmark dataset and the SynRES synthetic data generation and augmentation pipeline, and validates the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. WildRES addresses the evaluation of complex real-world scenarios in the RES field and can serve as a new standard for measuring models' complex reasoning abilities.\n\n2. Specifically, SynRES addresses the shortage of complex RES data at a low cost by automatically generating high-quality, densely annotated paired data. This pipeline can enhance model performance in complex queries and cross-domain scenarios, while remaining compatible with classic benchmarks."}, "weaknesses": {"value": "1. The mentioned issues include the lack of evaluation in complex scenarios, poor generalization to complex scenarios, and overly brief descriptions. However, these issues have been discussed in many existing RES or RIS benchmarks, such as MMR[1], LLM-Seg40K[2], ReasonSeg[3], MUSE[4], etc. Moreover, these benchmarks are more complex than the one proposed by the authors. The authors need to clearly explain the differences and advantages between the WildRES benchmark dataset and these existing benchmarks.\n\n2. The experiment only selects three open-source models (LISA, GSVA, and GLaMM) and fails to cover other typical RES architectures (e.g., lightweight Transformer-based models or cross-modal fusion models), which fails to fully demonstrate the generalizability of SynRES across different architectures. Additionally, the authors do not compare its performance with that of other existing models (such as [5][6][7]) on the RefCOCO series datasets. Furthermore, WildRES-DS only covers three types of scenarios, and this benchmark fails to truly achieve generalization to out-of-domain data.\n\n3. The SynRES synthetic data pipeline proposed by the authors is a combination of two types of data augmentation. Essentially, it merges Mosaic augmentation with text rewriting augmentation, both of which are already widely used in the RES field. The authors need to clarify the novelty of this pipeline.\n\n4. The authors obtain better test results by using more training data, rendering this comparison unfair. It is necessary to compare the speed and efficiency of this method with other methods—specifically in terms of training time and FLOPs—to determine if there are any differences.\n\n[1] MMR: A Large-scale Benchmark Dataset for Multi-target and Multi-granularity Reasoning Segmentation[C]//The Thirteenth International Conference on Learning Representations.\n\n[2] Llm-seg: Bridging image segmentation and large language model reasoning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 1765-1774.\n\n[3] Lisa: Reasoning segmentation via large language model[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 9579-9589.\n\n[4] Pixellm: Pixel reasoning with large multimodal model[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 26374-26383.\n\n[5]  Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks[J]. Advances in Neural Information Processing Systems, 2024, 37: 69925-69975.\n\n[6] Vltp: Vision-language guided token pruning for task-oriented segmentation[C]//2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE, 2025: 9353-9363.\n\n[7]  F-lmm: Grounding frozen large multimodal models[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 24710-24721."}, "questions": {"value": "1. WildRES only covers three types of cross-domain scenarios: dense crowds (CrowdHuman), autonomous driving (Cityscapes), and robotics (ARMBench) (Section 3.2). What is the main rationale for the authors choosing these three types of scenarios? Does it verify whether the domain gaps in these three scenarios can represent the real cross-domain requirements of RES?\n\n2. The paper only focuses on segmentation accuracy (gIoU/cIoU) and does not test the training efficiency of the model enhanced by SynRES—whether adding synthetic data for training affects the training speed—and should conduct a more thorough performance comparison with existing RES models.\n\n3. SynRES's text augmentation uses hypernym replacement (e.g., woman→person, Section 4.3), with a default replacement probability of p = 0.7 (Section 5.1). However, the authors need to explain whether hypernym replacement might cause semantic ambiguity when the class noun in the expression is strongly associated with an attribute (e.g., pregnant woman → person, where the attribute \"pregnant\" is tightly bound to \"female\"). It is necessary to clarify the adaptive differences under different replacement probabilities, and if replacement leads to semantic ambiguity, whether it could introduce new segmentation errors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AQ6yFfnAqG", "forum": "cgr5OAXe3q", "replyto": "cgr5OAXe3q", "signatures": ["ICLR.cc/2026/Conference/Submission14417/Reviewer_qvxs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14417/Reviewer_qvxs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833540751, "cdate": 1761833540751, "tmdate": 1762924823855, "mdate": 1762924823855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}