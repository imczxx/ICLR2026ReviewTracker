{"id": "EW8DskWQ1K", "number": 21403, "cdate": 1758317188421, "mdate": 1759896923841, "content": {"title": "Occupancy Reward Shaping: Improving Credit Assignment for Offline Goal-Conditioned Reinforcement Learning", "abstract": "While offline goal-conditioned reinforcement learning (GCRL) provides a simple recipe to train generalist policies from large unlabeled datasets, Offline GCRL agents trained with sparse rewards typically struggle on long-horizon tasks. Manually designing task-specific reward functions undermines the simplicity, scalability and generality of this paradigm. Moreover, prior approaches to learn rewards for effective credit assignment fail to adequately capture goal-reaching information as tasks scale in complexity. To address this gap, we propose $\\textrm{\\textbf{Occupancy Reward Shaping(ORS)}}$, a novel reward-shaping approach that leverages a learned occupancy measure; a distribution that naturally captures complex long-horizon temporal dependencies between states; and distills goal-reaching information from the occupancy measure into a general-purpose reward function for effective credit assignment. We demonstrate that ORS achieves a $\\mathbf{2.3\\times}$ improvement in performance on average over its base RL algorithm across a diverse set of long-horizon locomotion and manipulation tasks and outperforms prior state-of-the-art methods.", "tldr": "We propose an novel and effective reward-shaping method for credit assignment based on generative modeling of the occupancy measure and optimal transport, demonstrating state-of-the-art performance in offline GCRL.", "keywords": ["Offline Goal-Conditioned Reinforcement Learning", "Reward Shaping"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c52bfef2034ca139af10613257410aaf54d168b6.pdf", "supplementary_material": "/attachment/6fd951b9d5a556aefb5583aa99e223d286c65409.zip"}, "replies": [{"content": {"summary": {"value": "Offline GCRL methods trained with sparse rewards struggles in especially long-horizon tasks, since learned value functions tend to be noisy. Task specific hand-crafted reward functions can address this problem, but the task-based reward design is challenging and not practical. It is stated that previous work mostly focuses on online GCRL and recent offline GCRL approaches employ graph based distance classifiers, which fail to scale to large datasets. In this paper, considering the importance of scalability and generalization, an occupancy measure based approach is proposed for offline GCRL. The main idea is that the occupancy measure over future states is learned via flow matching, and then a reward function is defined to guide the policy via occupancy measure matching. Overall the idea is interesting and experimental results show such an occupancy measure based algorithm improves the performance of offline GCRL in long-horizon settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Interesting Approach**: Learning the occupancy measure via flow matching and then using this similarity as the reward function is a novel and interesting approach.\n- **Improved Non-monotonicity**: It is nicely presented that the proposed approach has lower non-monotonicity when compared to sparse rewards. \n- **Analysis of the Proposed Approach**: The experimental analyses are well-presented, clearly showing the effectiveness of ORS over sparse rewards."}, "weaknesses": {"value": "- **Computational Overhead**: The computational overhead of the proposed approach is not discussed. ORS requires learning an occupancy measure before training the policy, making it appear computationally more complex than baselines. Therefore, the computational overhead should be discussed and compared with both graph-based and non-graph-based offline GCRL baselines.\n- **Novelty**: Occupancy measure matching has already been employed by recent works for GCRL. The contribution over these methods is not discussed, which makes the novelty questionable. The benefits of ORS over recent literature (a-b) must be elaborated.\n- **Baselines**: In the experiments, recent relevant works (a-c) are omitted from the comparisons.\n- **Experimental Setting**: The experiments only cover long-horizon tasks. The applicability of the proposed method to short- and medium-horizon tasks should be discussed.\n- **Experimental Results**: The environments used between Table 1 and Table 2 are not the same. It is not clear why some environments are included in Table 1 but omitted in Table 2. For a clear and fair evaluation of ORS, all environments in Table 1 must also be included in Table 2.\n- **Vague Explanation**: The main assumption of the paper (in long-horizon tasks, the value function exhibits a high level of non-monotonicity) is evaluated under section 3.2, however it is not clear how $\\hat{V}(s,g)$ is trained. It is not explained how the authors obtained $\\hat{V}(s,g)$ in 3.2, or which algorithm was used and how $\\hat{V}(s,g)$ was trained.\n\n[a]: Sikchi, Harshit, et al. \"Score models for offline goal-conditioned reinforcement learning.\" The Twelfth International Conference on Learning Representations. 2023.\n\n[b]: Ma, Jason Yecheng, et al. \"Offline goal-conditioned reinforcement learning via $ f $-advantage regression.\" Advances in neural information processing systems 35 (2022): 310-323.\n\n[c]: Zhou, John Luoyu, and Jonathan C. Kao. \"Flattening Hierarchies with Policy Bootstrapping.\" Workshop on Reinforcement Learning Beyond Rewards@ Reinforcement Learning Conference 2025."}, "questions": {"value": "- How computationally complex is the proposed method? Can you please provide computational efficiency comparisons with baselines and graph-based solutions?\n- Can you please elaborate on the benefits of the proposed approach over recent literature [a-b]?\n- Can you please compare the proposed method with [a-c], in addition to the current baselines?\n- Can you please elaborate on whether this approach would also be useful in medium or short-horizon tasks?\n- Can you please clarify how $\\hat{V}(s,g)$) was trained for the analysis in Section 3.2? What algorithm was used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hTKfFB5pUd", "forum": "EW8DskWQ1K", "replyto": "EW8DskWQ1K", "signatures": ["ICLR.cc/2026/Conference/Submission21403/Reviewer_xcHp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21403/Reviewer_xcHp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941286753, "cdate": 1761941286753, "tmdate": 1762941749189, "mdate": 1762941749189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of insufficient signals in offline goal-conditioned reinforcement learning (Offline Goal-Conditioned RL, GCRL) under sparse rewards and long-horizon tasks. It proposes Occupancy Reward Shaping (ORS), a learning-based reward shaping method grounded in the occupancy measure, which can capture temporal dependencies in long-horizon tasks. By integrating flow matching for fitting, ORS distills goal achievement information from the occupancy measure into a generalizable reward function."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written, logically structured and well organized. Technical details are thoroughly presented.\n2.The theoretical analysis is solid, providing proofs of convergence and an analysis of reward monotonicity. Starting from empirical evidence that sparse rewards lead to non-monotonic value functions, the paper proposes a reward shaping idea based on the occupancy measure, forming a complete logical chain.\n3.Experiments cover both locomotion and manipulation tasks. The experimental design is rigorous, and the results consistently demonstrate strong performance.\n4.The approach is compatible with existing offline goal-conditioned RL algorithms, and the analysis of the value function non-monotonicity is insightful. Experiments validate that ORS effectively alleviates this issue."}, "weaknesses": {"value": "1.While multiple tasks from OGBench are used, all of them are simulated environments. There is no mention of testing on real-world data or environments with different physical properties. It remains unclear whether the algorithm remains stable under real-world physics or diverse visual conditions.\n2.The theoretical assumptions are relatively strong. Discussions or brief experiments demonstrating robustness under stochastic dynamics are needed, or clarification on whether these assumptions still hold approximately in practice.\n3.Ablation studies are insufficient. Although the text mentions “conduct detailed analyses and ablations,” the contribution of different ORS components to the final performance is not shown."}, "questions": {"value": "1.The κ parameter in the ablation is highly sensitive. Could adaptive scheduling of κ improve stability?\n2.Does the optimality guarantee in Theorem 1 depend on data coverage quality? In datasets covering only part of the optimal path, can ORS still maintain optimality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Wiv6fz5h01", "forum": "EW8DskWQ1K", "replyto": "EW8DskWQ1K", "signatures": ["ICLR.cc/2026/Conference/Submission21403/Reviewer_bbdp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21403/Reviewer_bbdp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963612207, "cdate": 1761963612207, "tmdate": 1762941748761, "mdate": 1762941748761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to address the struggles of offline goal-conditioned RL on long horizon tasks via reward shaping. In particular, it propposes a novel reward shaping method, occupancy reward shaping, trained using flow matching to perform effective credit assignment as a reward function. Experiments show that it improves over prior Offline GCRL methods on long-horizon lcomotion and manipulation tasks in simulation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- clear motivation and extensive discussion on background\n- provides proofs for theoretical guiarantee"}, "weaknesses": {"value": "- It would be more convincing if results can also be demonstrated on real-world robotics tasks, where both data quantity and quality are lower\n- There should be more discussion on other ways of computing dense reward information\n- Results seem only to be marginally better, most of the gains over GO-FRESH are on 2 tasks\n- how did the authors select the tasks in the benchmark? why are tasks like ant soccer and humanoid maze not selected?"}, "questions": {"value": "- What is the quality and quantity of data that is required to train such a reward model? Will there be circumstances where the reward model is not accurate? If so how is the performance affected\n- In an offline RL setting, since there is no online interaction, why can't the reward be simply obtained using distance of current state to goal?\n- Can you discuss the comparison of your approach against works like GoFar (Ma et al), where shaped reward is not used, and goal reaching behavior is direclty learned by minimizing divergence between policy and expert's  goal conditioned state occupancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zQ771wfycj", "forum": "EW8DskWQ1K", "replyto": "EW8DskWQ1K", "signatures": ["ICLR.cc/2026/Conference/Submission21403/Reviewer_noM7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21403/Reviewer_noM7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762597532811, "cdate": 1762597532811, "tmdate": 1762941748398, "mdate": 1762941748398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}