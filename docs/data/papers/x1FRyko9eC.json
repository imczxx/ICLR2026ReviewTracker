{"id": "x1FRyko9eC", "number": 2652, "cdate": 1757182069302, "mdate": 1759898135393, "content": {"title": "One-Step Video Restoration via Diffusion Adversarial Post-Training", "abstract": "Recent advances in diffusion-based video restoration (VR) demonstrate significant improvement in visual quality, yet yield a prohibitive computational cost during inference.\nWhile several distillation-based approaches have exhibited the potential of one-step image restoration, extending existing approaches to VR remains challenging and underexplored, particularly when dealing with high-resolution video in real-world settings.\nIn this work, we propose a one-step diffusion-based VR model, termed as AnonymousVR, which performs adversarial VR training against real data.\nTo handle the challenging high-resolution VR within a single step, we introduce several enhancements to both model architecture and training procedures.\nSpecifically, an adaptive window attention mechanism is proposed, where the window size is dynamically adjusted to fit the output resolutions, avoiding window inconsistency observed under high-resolution VR using window attention with a predefined window size.\nTo stabilize and improve the adversarial post-training towards VR, we further verify the effectiveness of a series of losses, including a proposed feature matching loss without significantly sacrificing training efficiency.\nExtensive experiments show that AnonymousVR can achieve comparable or even better performance compared with existing VR approaches in a single step.", "tldr": "We propose a large-scale one-step diffusion transformer for video restoration, via diffusion adversarial post-training.", "keywords": ["One-step", "video restoration", "adversarial training"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/954dd4a0652e34d36167a977dd9467adfc4954c1.pdf", "supplementary_material": "/attachment/5d4592b97b0e62abbcab446b0c0ecbdfe0866f3c.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes AnonymousVR, a one-step diffusion-based video restoration (VR) model that aims to overcome the high computational cost of traditional diffusion-based VR methods. The paper introduce an adaptive window attention mechanism that dynamically adjusts window sizes according to output resolution to address inconsistency issues in high-resolution VR. They also design an adversarial post-training strategy incorporating a feature matching loss to enhance stability and quality without significantly increasing training cost. Experimental results suggest that AnonymousVR achieves competitive or superior performance compared with existing multi-step VR methods while requiring only a single inference step."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The experimental results are promising, showing a substantial improvement."}, "weaknesses": {"value": "1. The technical contributions of this paper appear rather weak, as the work seems to rely more on engineering efforts than on genuine technical innovation. The proposed window attention and adversarial diffusion training methods lack clear novelty and do not demonstrate substantial methodological advancement beyond existing approaches.\n\n2. Please clarify the core improvements of the proposed adaptive window attention compared with prior window attention mechanisms, such as those in SeedVR. Additionally, please elaborate on the key advancements of your adversarial diffusion training strategy relative to existing adversarial diffusion methods.\n\n3. The authors claim that their method is the first one-step video restoration approach; however, one-step super-resolution and restoration methods already exist in the literature (e.g., [1][2]). A more detailed discussion and comparison with these prior works are necessary to justify the claimed novelty.\n\n[1] DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution. NeurIPS 2025.\n\n[2] One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution. NeurIPS 2025."}, "questions": {"value": "see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KU89sgs3mV", "forum": "x1FRyko9eC", "replyto": "x1FRyko9eC", "signatures": ["ICLR.cc/2026/Conference/Submission2652/Reviewer_BpPA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2652/Reviewer_BpPA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881720451, "cdate": 1761881720451, "tmdate": 1762916318819, "mdate": 1762916318819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a one-step video restoration (VR) model that leverages a diffusion adversarial post-training (APT) framework to perform high-resolution video restoration. The key contributions include an adaptive window attention mechanism for high-resolution inputs, and a robust feature matching loss for improved training stability. The model is evaluated against state-of-the-art VR methods, demonstrating competitive performance while being four times faster than existing diffusion-based VR methods. The results suggest that it achieves comparable or even superior performance in real-world scenarios, especially with AI-generated content and high-resolution videos."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel one-step VR method by applying APT to diffusion-based models, reducing the computational burden significantly compared to traditional multi-step approaches.\n\n2. The adaptive window attention mechanism for handling high-resolution videos and the feature matching loss for training stability are key contributions that improve the model's performance and robustness across varying video resolutions.\n\n3. The method shows promising quantitative and qualitative results, outperforming existing VR approaches in real-world and synthetic benchmarks, demonstrating significant gains in speed and restoration quality."}, "weaknesses": {"value": "1. The paper lacks comparisons with the latest VSR methods presented at NeurIPS 2025 (such as DLoraL [1] and DOVE [2]). The authors should include comparisons with these methods to better demonstrate the competitiveness of the proposed approach.\n\n2. The paper does not provide results trained on public datasets (such as REDS). The reported improvements might stem from using a larger private dataset. Will the authors make the dataset publicly available?\n\n3. Despite achieving faster inference, the training requires 72 H100 GPUs and significant resources, which raises concerns about scalability and accessibility for broader research adoption.\n\n[1] One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution. NeurIPS2025\n\n[2] DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution. NeurIPS2025"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FUtUgEp3Gc", "forum": "x1FRyko9eC", "replyto": "x1FRyko9eC", "signatures": ["ICLR.cc/2026/Conference/Submission2652/Reviewer_Q1GV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2652/Reviewer_Q1GV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897047228, "cdate": 1761897047228, "tmdate": 1762916318632, "mdate": 1762916318632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AnonymousVR, a one-step diffusion-based video restoration method trained with adversarial post-training. It starts from a strong diffusion transformer, then applies progressive distillation and full adversarial tuning to remove the multi-step sampling cost. In addition, an adaptive window attention that adjusts window size to input resolution to avoid block boundaries at high resolutions, and a feature-matching loss taken from discriminator layers to replace expensive LPIPS during training are proposed. Experiments on synthetic, real, and AIGC videos demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The introduction of adaptive window attention effectively reduces boundary artifacts when processing high-resolution frames.\n\n- The training strategy which combines RpGAN, approximate R2 regularization, feature-matching losses, and progressive distillation to ensure stable convergence and high perceptual quality is comprehensive.\n\n- The experiments are extensive and include both synthetic and real-world data, multiple objective and perceptual metrics, as well as a well-organized user study."}, "weaknesses": {"value": "- My main concern is that the novelty of the method is somewhat limited, as it largely builds upon the existing Adversarial Post-Training (APT) framework, and the paper does not clearly explain the fundamental differences or new contributions beyond APT.\n\n- The training process is extremely resource-intensive, requiring 72 H100 GPUs, which significantly limits reproducibility and practical accessibility.\n\n- The method’s robustness under challenging conditions, such as heavy degradations, large motion, or complex real-world dynamics, appears limited."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wUUVtIQDfM", "forum": "x1FRyko9eC", "replyto": "x1FRyko9eC", "signatures": ["ICLR.cc/2026/Conference/Submission2652/Reviewer_rqaT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2652/Reviewer_rqaT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928333138, "cdate": 1761928333138, "tmdate": 1762916318501, "mdate": 1762916318501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "I think the paper tackles an important and timely problem: high-resolution one-step video restoration (VR). The method “AnonymousVR” initializes from a diffusion transformer (SeedVR) and then performs adversarial post-training (APT) to convert it into a single-step generator. The paper’s two main technical levers are:  \n\n(1) an adaptive window attention to avoid high-res window boundary artifacts.  \n\n(2) an adversarial post-training recipe with progressive distillation and a feature-matching loss (taken from the discriminator) to stabilize training while avoiding pixel-space LPIPS cost. Experiments suggest competitive or better perceptual quality vs multi-step diffusion VR at much lower latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I think the jump to truly one-step VR with a diffusion transformer (initialized from SeedVR) plus APT is a meaningful step beyond prior one-step image restoration; prior works are mostly teacher-distillation or rely on fixed diffusion priors that cap quality. This work claims distillation-free adversarial post-training against real data after a lightweight progressive distillation stage to bridge the gap, which is interesting for video. \n\n- The adaptive window attention to handle arbitrary resolutions with dynamic window size feels practical and addresses a real artifact at 2K/1080p; to my knowledge, such resolution-consistent windowing for VR in a one-step setting is new. \n\n- Using the discriminator’s multi-layer features as an LPIPS surrogate in latent / discriminator space for high-res VR is a reasonable, efficiency-motivated tweak (not conceptually new, but well-justified here).  \n\nOverall, I think the contribution is incremental-to-moderate in theory but practically impactful for high-res, fast VR."}, "weaknesses": {"value": "- I am concerned about the compute-heaviness. I think the approach relies heavily on significant compute (72×H100, 10M/5M pairs), which limits reproducibility in typical academic labs despite code release plans. Claims of “largest-ever VR GAN” underscore this.   \n\n- Scope of degradations. While synthetic degradations follow prior work, I think the paper could better characterize real-world degradation diversity and robustness (e.g., compression artifacts, rolling shutter, severe motion blur) beyond VideoLQ/AIGC28; the method’s failure cases are not deeply analyzed.  \n\n- Fairness of baselines. Diffusion baselines are run with 50 steps “to maintain stable performance”; I think it would be fair to include their fastest-setting curves (e.g., 10/25/50 steps trade-off plots) to contextualize speed-quality trade-offs.  \n\n- Temporal metrics and consistency are missing. The paper mostly emphasizes frame-wise perceptual metrics; I would expect temporal consistency metrics (e.g., t-LPIPS variants or VMAF-like temporal terms) or user study questions specific to flicker/temporal stability. Current user study aggregates “overall quality” but not explicitly “temporal coherence.”"}, "questions": {"value": "- Temporal stability: How does one-step AnonymousVR compare to SeedVR on temporal coherence (quantitative & qualitative)? Any metric beyond a user study that isolates flicker?   \n\n- Feature-matching loss: Why pick discriminator blocks 16/26/36 specifically? Did you try earlier/later layers or a learned weighting per layer? Impact on training speed?    \n\n- Progressive distillation details: What are the teacher/student schedules and hyper-params across strides (64→32→…→1)? How much of the final gain comes from progressive distillation vs APT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tzR9QML6rY", "forum": "x1FRyko9eC", "replyto": "x1FRyko9eC", "signatures": ["ICLR.cc/2026/Conference/Submission2652/Reviewer_WNGK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2652/Reviewer_WNGK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973330859, "cdate": 1761973330859, "tmdate": 1762916318379, "mdate": 1762916318379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}