{"id": "DOLF9TUBfa", "number": 13073, "cdate": 1758213300525, "mdate": 1759897467259, "content": {"title": "DarwinLM: Evolutionary Structured Pruning of Large Language Model", "abstract": "Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose DarwinLM, a method for training-aware structured pruning. DarwinLM builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage.\nWe validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, DarwinLM surpasses ShearedLlama while requiring $5\\times$ less training data during post-compression training. We also extend our method to MoE models like Qwen3-30B-A3B. To the best of our knowledge, this is the first work to explore structured pruning in MoE architectures. Our approach, DarwinLM, outperforms uniform pruning baselines and demonstrates the effectiveness of structured sparsity even in complex expert-based models. Code and weights are available.", "tldr": "", "keywords": ["Model Compression; Structured Pruning; Evolutionary Search"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/383fce175ac41fb765151a93843f307d9020076b.pdf", "supplementary_material": "/attachment/0402e0295a558ece980e86f14aaffe9d53c08145.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DarwinLM, a training-aware structured pruning framework that treats model compression as an evolutionary process, where candidate submodels are iteratively generated, fine-tuned on small datasets, and selected based on fitness metrics. By integrating lightweight fine-tuning into the evolutionary search, DarwinLM effectively predicts long-term recovery potential and optimizes sparsity allocation across model layers and modules, including both dense and mixture-of-experts architectures. Experiments on Llama and Qwen models demonstrate that DarwinLM achieves state-of-the-art pruning efficiency, recovering over 90% of accuracy with only one-fifth of the training data required by prior methods while nearly doubling inference speed."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper is innovative in reducing the retraining cost of pruned models through evolutionary search, and it demonstrates strong practical value for real-world applications.\n\n2. It is among the few works that conduct structured pruning on MoE models, which can inspire future research on lightweight optimization of MoE architectures.\n\n3. The experiments are solid and comprehensive, demonstrating the effectiveness of the proposed method across several recent large language models.\n\n4. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "The non-uniform sparsity allocation may affect the deployment efficiency of the pruned model. A more thorough discussion and analysis of this effect would strengthen the paper."}, "questions": {"value": "Does the non-uniform sparsity allocation affect the deployment efficiency of the pruned model？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZbHNJ1XbCo", "forum": "DOLF9TUBfa", "replyto": "DOLF9TUBfa", "signatures": ["ICLR.cc/2026/Conference/Submission13073/Reviewer_Va8F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13073/Reviewer_Va8F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13073/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928165211, "cdate": 1761928165211, "tmdate": 1762923798420, "mdate": 1762923798420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DarwinLM claims to improve structured pruning of LLMs using a hybrid of second-order saliency and evolutionary search over sparsity allocations. Each generation mutates layer-wise sparsity ratios, fine-tunes offspring, and keeps the fittest models. The authors report strong perplexity and downstream accuracy on LLaMA-2, LLaMA-3.1, and Qwen models, achieving up to 50–60% parameter reduction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Targets structured pruning, which is the only kind of sparsity actually exploitable by modern inference frameworks.\n\n2. Attempts to automate non-uniform sparsity allocation rather than relying on fixed heuristics.\n\n3. The proposed approach could be extend to MoE-based model pruning which is a nice bonus."}, "weaknesses": {"value": "1. The paper does not report latency, throughput, or kernel utilization results. I only found the authors roughly mentioned it in a small table (Table 4) Without demonstrating that the pruned models actually run faster on common inference engines such as TensorRT-LLM or vLLM, it remains unclear whether the proposed structured pruning translates into practical efficiency gains.\n\n2. The evolutionary search involves training many fine-tuned offspring models, which likely requires substantial compute. However, the paper does not provide GPU-hour or data-usage accounting, making it difficult to assess whether the approach offers a favorable cost-benefit trade-off compared to simpler pruning baselines.\n\n3. Combining second-order importance estimation with an evolutionary search procedure builds on several existing frameworks (e.g., AutoCompress, MetaPruning, ShearedLLaMA). The overall idea is coherent but does not introduce a clearly new optimization insight or theoretical advancement beyond prior art.\n\n4. (minor) The Darwinian terminology somewhat overemphasizes the novelty of the approach. In essence, the method performs parameter mutation and selection within a standard hyper-parameter search loop rather than a biologically inspired or algorithmically distinct evolutionary process."}, "questions": {"value": "1. Can you provide concrete latency numbers (e.g., ms/token) before and after pruning on e.g., A100/H100 GPUs under vLLM or TensorRT? Also, can you include more comparison methods instead of only the dense model as baselines in Table 4?\n\n2. What is the full compute cost (GPU hours) of the evolutionary search versus the achieved inference savings?\n\n3. Do your irregular sparsity patterns map cleanly to N:M kernels, or do they require custom CUDA kernels?\n\n4. How does this differ, concretely, from FlexPrune or ShearedLLaMA beyond empirical tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wsd2aIEOvW", "forum": "DOLF9TUBfa", "replyto": "DOLF9TUBfa", "signatures": ["ICLR.cc/2026/Conference/Submission13073/Reviewer_iyCb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13073/Reviewer_iyCb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13073/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949650408, "cdate": 1761949650408, "tmdate": 1762923798003, "mdate": 1762923798003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DarwinLM proposes training-aware, non-uniform structured pruning for LLMs. It first builds a per-layer “sparsity level” database via second-order one-shot pruning, then uses an evolutionary search with “level-switch” mutations under a target size/speed constraint. Fitness is measured via KL divergence to the dense model. Selection is multi-step and training-aware, followed by a final 10B-token post-compression finetune. The method reports one-shot and post-train results on Llama-2-7B, Llama-3.1-8B, Qwen-2.5-14B, and an MoE case (Qwen-3-30B-A3B)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Clear pipeline: second-order one-shot pruning, sparsity-level database, evolutionary search with speed/size constraints, short finetune. \n\n* Training-aware selection nicely predicts which offspring recover best after longer finetunes; ablation supports the idea."}, "weaknesses": {"value": "* In the main table (Table 1), the performance gaps shrink notably after finetuning. It’s unclear whether DarwinLM’s one-shot advantage persists under longer training or on performance after training converges. The pruned+finetuned models may still underperform public small dense models (e.g., pruned-llama 3.1 8B vs llama 3.2 3B model). While this is reasonable as only 10B tokens are used for recovering performance, this still raise the need for further fine-tuning the models to be actually used. It would be helpful to include results under longer training or distillation to see if the initialization gain is still there.\n \n* The search can yield highly non-uniform (unbalanced) per-layer sparsity. It would be beneficial to show how the final architecture looks and whether unbalanced shapes can harm optimization or stability during continued training. Comparing the final shape with Shearedllama can provide more insights.\n\n* The paper states that \"this is the first work to explore structured pruning in MoE architectures\" in the abstract, which is not accurate and overstated. Some works have worked on structured pruning of MoE [1,2].\n\n* It's not very accurate to claim \"Orthogonal to Minitron/Flextron”. Those pipelines couple structured pruning with KD and depth/width choices; DarwinLM’s search over non-uniform width allocations overlaps in spirit.\n\n* The non-uniform architecture can also affect the inference efficiency. Irregular shapes can reduce kernel efficiency and complicate inference optimization. Some discussion or experiments comparing uniform and non-uniform will be helpful.\n\n\n[1] SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation\n\n[2] Demystifying the Compression of Mixture-of-Experts Through a Unified Framework"}, "questions": {"value": "Included above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lA8OepVBgV", "forum": "DOLF9TUBfa", "replyto": "DOLF9TUBfa", "signatures": ["ICLR.cc/2026/Conference/Submission13073/Reviewer_6cfz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13073/Reviewer_6cfz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13073/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037734249, "cdate": 1762037734249, "tmdate": 1762923797661, "mdate": 1762923797661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DarwinLM, a method for structured pruning of large language models that uses evolutionary search to find optimal non-uniform compression patterns. The core idea is to generate multiple \"offspring\" models through mutations that shift sparsity between layers, then select the best candidates using a multi-step training-aware process. The method builds a database of pre-pruned layers at different sparsity levels using second-order information, then searches over combinations of these levels while incorporating lightweight fine-tuning to predict which models will perform best after full training. The authors test on several models (Llama-2-7B, Llama-3.1-8B, Qwen-2.5-14B) and show improvements over uniform pruning and competing methods like ShearedLlama and ZipLM. They claim to achieve comparable or better accuracy while using 5x less training data than ShearedLlama. The method is also extended to MoE architectures, which is claimed as a first for structured pruning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The multi-step selection strategy that progressively increases fine-tuning data (10K→50K→100K→200K tokens) is both intuitive and empirically validated. By showing in Figure 2 that small-scale training predicts larger-scale performance, the paper introduces a practical and well-motivated solution to a long-standing inefficiency in pruning and neural architecture search.\n\n2. The method is evaluated across diverse model families and sizes—up to 70B parameters—with generally fair baselines and detailed ablations (offspring count, sparsity levels, fitness metrics). The results consistently demonstrate superiority over baselines like ShearedLlama and ZipLM."}, "weaknesses": {"value": "See Questions."}, "questions": {"value": "1. Evolutionary search feels overcomplicated. The core is just trying different per-layer sparsity combinations. The mutation operator is trivial (swap sparsity between two layers).  Do simpler search methods like beam search also work? It would be better to have comparison with random search given the same budget.\n\n2. Search cost are unclear. 200 generations × 16 offspring = 3,200 evaluations, each requiring model stitching and training. How does total cost compare to ZipLM's dynamic programming or ShearedLlama's approach?\n\n3. How sensitive is DarwinLM to the choice of training-aware selection token budgets (10K–200K)? Could different scaling change the final selection outcome?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TrZekO3sD2", "forum": "DOLF9TUBfa", "replyto": "DOLF9TUBfa", "signatures": ["ICLR.cc/2026/Conference/Submission13073/Reviewer_1oF1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13073/Reviewer_1oF1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13073/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141883391, "cdate": 1762141883391, "tmdate": 1762923797361, "mdate": 1762923797361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}