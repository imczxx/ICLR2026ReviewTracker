{"id": "084SvT55yk", "number": 903, "cdate": 1756822893695, "mdate": 1763605619024, "content": {"title": "Native Adaptive Solution Expansion for Diffusion-based Combinatorial Optimization", "abstract": "One central challenge in Neural Combinatorial Optimization (NCO) is handling hard constraints efficiently. Beyond the two classic paradigms, i.e., Local Construction (LC), which sequentially builds feasible solutions but scales poorly, and Global Prediction (GP), which produces one-shot heatmaps yet struggles with constraint conflicts, the recently proposed Adaptive Expansion (AE) shares the advantages of both by progressively growing partial solutions with instance-wise global awareness.\nHowever, existing realizations bolt AE onto external GP predictors, so their solution quality is bounded by the backbone and their inference cost scales with repeated global calls.\nIn this paper, we fundamentally rethink adaptive expansion and make it native to a generative model, acting as its intrinsic decoding principle  rather than an external wrapper.\nWe propose NEXCO, a CO-specific masked diffusion framework that turns adaptive expansion into the model’s own iterative unmasking process.\nSpecifically, it involves a solution-expansion training procedure with a time-agnostic GNN denoiser, which learns diffusion trajectories between fully masked solutions and ground-truth solutions.\nWith the trained time-agnostic denoiser, we introduce a novel solution expansion scheme at the solving stage, enabling adaptive control over the intermediate solution states. \nIt is achieved by constructing candidate sets according to confidence scores and applying feasibility projection to expand the solution while respecting constraints. \nIn this way, ``adaptive\" is not an afterthought but the decoding itself: intermediate diffusion states are meaningful partial solutions and progress is instance-adaptive rather than schedule-bound.\nExtensive experiments on representative CO problems show that NEXCO achieves approximately 50\\% improvement in solution quality and up to $4\\times$ faster inference compared to prior state-of-the-art solvers.", "tldr": "", "keywords": ["mask diffusion model", "neural combinatorial optimization"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e33dca2e7b098f4c24ee6c98ae4784b6e1781d43.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Existing neural CO solvers either ensure local feasibility but lack global awareness (LC) or produce global predictions with constraint violations (GP). Current adaptive expansion is only an external wrapper with limited effectiveness.\nNEXCO makes adaptive expansion native through CO-specific masked diffusion where intermediate states are meaningful partial solutions, combined with time-agnostic training and confidence-based progressive unmasking with feasibility projection.\nThe framework achieves about 50% quality improvement and 2~4 times speedup over state-of-the-art, successfully embedding adaptive expansion as an intrinsic generative principle rather than external wrapper.\nNEXCO successfully realizes adaptive solution expansion as a native generative principle within masked diffusion, achieving superior performance across multiple CO problems. The framework opens new opportunities for integrating constructive expansion mechanisms into diffusion-based generative modeling, providing a step toward scalable and general-purpose neural solvers for combinatorial optimization."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Fundamental Reconceptualization\n\nThis is a strength because the shift from external wrapper to native integration addresses a fundamental architectural limitation. When adaptive expansion is just a wrapper (like COExpander), it must repeatedly call the underlying model, creating computational overhead and being limited by the backbone's capabilities. By making it native, NEXCO achieves the same adaptive behavior through the natural progression of the diffusion process itself, eliminating redundant computations and enabling tighter integration between the expansion logic and the generative model.\n\n2. Theoretically Motivated Design\n\nThe CO-specific corruption is theoretically sound because it respects the sparse nature of CO solutions. Most CO problems have sparse solutions, so treating 0s and 1s asymmetrically aligns with the problem structure. This prevents the \"conservative bias\" problem shown in Figure 2, where symmetric masking causes models to prefer safe 0 predictions. The theoretical motivation directly translates to empirical success.\n\n3. Computational Efficiency\n\nThe O(Ts) vs O(Ds Ts) complexity improvement is significant because it makes the method practical for real-world deployment."}, "weaknesses": {"value": "1. Scalability Questions\nThe jump from 10K to 100K+ nodes represents a 10x increase that often reveals new bottlenecks. Memory requirements for storing full adjacency matrices grow quadratically, and the GNN message passing might become prohibitively expensive. The paper's use of KNN sparsification helps but doesn't fully address whether the core approach scales to industrial-size problems like million-node routing networks.\n\n2. Training Data Requirements\nNEXCO requires high-quality reference solutions for training (from Concorde for TSP, KaMIS for MIS). For new problem types where optimal solvers don't exist or are too slow, obtaining training data becomes a bottleneck. While Table 3 shows robustness to suboptimal labels, the method still needs some reasonable baseline solutions, limiting applicability to well-studied problems where such data exists."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LaxIm3xFvT", "forum": "084SvT55yk", "replyto": "084SvT55yk", "signatures": ["ICLR.cc/2026/Conference/Submission903/Reviewer_VX9M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission903/Reviewer_VX9M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782199558, "cdate": 1761782199558, "tmdate": 1762915638803, "mdate": 1762915638803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces NEXCO, a masked-diffusion framework for neural CO that \n(i) replaces uniform bit-flip noise with a CO-specific, one-way masking that only turns selected 1’s to 0 (never adding false positives), \n(ii) trains a time-agnostic denoiser with time-agnostic optimization consistency (TOC), and \n(iii) decodes via Native Adaptive Expansion (NAE)—progressively unmasking variables while a problem-specific projector enforces feasibility. \nClaimed benefits: feasible partial states along the forward trajectory, schedule-free training, and constructive, efficient decoding; experiments are shown for TSP/MIS/CVRP with strong results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength and also the main novelty of this paper is that it proposes a masking diffusion process for combinatorial optimization, together with a constructive decoding process with feasibility projection, which addresses the “global heatmap decode” weakness in prior diffusion solvers. \nThe paper is well written and easy to follow."}, "weaknesses": {"value": "The major weakness lies in the effectiveness of the algorithm proposed. Even though the masking-then-expansion framework is novel compared to the existing literature, it seems that this paper missed a very important related work [1] which also addresses the “global heatmap decode” weakness much earlier diffusion solvers like T2T and DIFUSCO. Instead of corrupting the partial solution in this paper, [1] corrupts the solution to its sub-optimal neighbors, which can also reduce the steps needed in reconstruction, and thus improves the speed of diffusion solver. I highly recommend authors to compare their work with this paper on benchmarks and analyze the pros/cons compared to this baseline, in addition to earlier diffusion solvers that have already been included.\n\n[1] Generation as Search Operator for Test-Time Scaling of Diffusion-based Combinatorial Optimization (GenSCO)"}, "questions": {"value": "- Explicitly delineate what’s novel vs. \"Generation as Search Operator for Test-Time Scaling of Diffusion-based Combinatorial Optimization (GenSCO)\"\n\n- Provide analysis theoretically / intuitively or empirically on the comparison of the proposed approach vs GenSCO\n\n- Further ablation study to understand the source of gains and the importance of each component."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YMGLEnBoQz", "forum": "084SvT55yk", "replyto": "084SvT55yk", "signatures": ["ICLR.cc/2026/Conference/Submission903/Reviewer_Cjz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission903/Reviewer_Cjz9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838599034, "cdate": 1761838599034, "tmdate": 1762915638619, "mdate": 1762915638619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NEXCO, a diffusion-based framework for neural combinatorial optimization that makes adaptive expansion native to the generative model. The core idea is CO-specific masked diffusion that only drops active variables (1→0), a time‑agnostic GNN denoiser trained with optimization consistency across corruption levels, and an inference routine that expands solutions via confidence-ranked candidate sets with feasibility projection. Across TSP, MIS, and CVRP, NEXCO reports stronger solution quality and faster inference than prior LC/GP/AE baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "--Native AE design: Intermediate diffusion states are usable partial solutions, not just noisy heatmaps.\n--Clear efficiency story: Explicit O(Ts) complexity and practical speedups (e.g., TSP‑500 gap 0.39% → 0.26% with ~2× faster; MIS ER drop 9.31% → 4.20% with >2× speedup; CVRP‑100 4.19% → 1.40%).\n--Robustness and generalization: Cross‑scale transfer (Table 4) and training with suboptimal labels still yields strong solutions (Table 3).\n--Breadth: Applies to TSP, MIS, and CVRP, including cases where GP diffusion struggles (CVRP)."}, "weaknesses": {"value": "--Theory is light: Convergence and approximation are argued intuitively; formal guarantees are not provided.\n--Problem-specific components: Γ(·) requires tailored feasibility logic per task, which may limit plug‑and‑play generality.\n--Sensitivity: Performance depends on expansion steps and candidate threshold α (Figures 3–4); guidance is empirical rather than principled.\n--Scalability tactics: Large-scale TSP relies on KNN sparsification (§C.4, Table 8), which is effective but introduces an extra design choice."}, "questions": {"value": "--Can you provide any formal convergence or approximation guarantees for NAE, even under simplified assumptions?\n--How generalizable is Γ(·)? Could you outline patterns or templates to implement feasibility projection across new CO tasks?\n--What is the runtime breakdown between denoiser calls and Γ(·) projection across tasks/scales?\n--How robust is performance to α and Ds across distributions and instance scales? Any adaptive schemes that reduce tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6ycfxhIfSV", "forum": "084SvT55yk", "replyto": "084SvT55yk", "signatures": ["ICLR.cc/2026/Conference/Submission903/Reviewer_PqJb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission903/Reviewer_PqJb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154922424, "cdate": 1762154922424, "tmdate": 1762915638493, "mdate": 1762915638493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}