{"id": "dT399j1Azv", "number": 21691, "cdate": 1758320554271, "mdate": 1759896908407, "content": {"title": "Reward Models Inherit Value Biases from Pretraining", "abstract": "Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the \"Big Two\" psychological axes, we show a robust preference of Llama RMs for \"agency\" and a corresponding robust preference of Gemma RMs for \"communion.\" This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers' choice of base model is as much a consideration of values as of performance.", "tldr": "Reward models are not a blank slate - they inherit significant value biases from their base models that persist even through extensive preference training.", "keywords": ["reward models", "value alignment", "finetuning", "preference learning", "large language models", "RLHF", "AI safety", "bias", "pretraining"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3cf3cc2a7d24123178c3048fa5658910b53ee668.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates whether reward models (RMs) inherit value biases from their pretrained language model backbones. The authors conduct a systematic analysis across 10 open-weight RMs based on either Llama or Gemma architectures and show that these models consistently differ along established psycholinguistic dimensions of human value—particularly the \"Big Two\" axes of agency and communion. They demonstrate that Llama-based RMs exhibit stronger preferences for agency-related words (e.g., freedom, ability, success), whereas Gemma-based RMs prefer communion-related words (e.g., love, friendship, care). Importantly, the paper traces these differences back to the log-probability structure of the base models themselves and formulates an “implicit reward model” defined by log-probability deltas between two LLMs that captures such differences. Experiments with RM training further show that these biases persist even after training RMs on large preference datasets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Relevance and potential impact**: The work tackles an important and underexplored problem in the alignment literature—understanding how pretraining choices influence downstream reward models. Even though the result may not be entirely surprising, demonstrating it empirically and rigorously is valuable for the community.\n\n- **Methodological rigor**: The experiments are well-designed and carefully controlled, with sound statistical analyses (e.g., mixed-effects models, permutation tests, Bonferroni correction). The use of validated psycholinguistic corpora lends interpretability and psychological grounding to the findings.\n\n- **Comprehensive empirical validation**: The authors examine both real-world open-weight RMs and controlled in-house replications, providing converging evidence for the bias inheritance effect."}, "weaknesses": {"value": "- **Potential domain mismatch**: The preference datasets used for RM training (e.g., HelpSteer, UltraFeedback, HH-RLHF, Argilla-Math) are focused on instruction-following, helpfulness, honesty, and truthfulness, not the kinds of moral or social values represented in the psycholinguistic test sets. Thus, it is not clear whether the persistence of biases reflects insufficient training data volume or an out-of-distribution (OOD) evaluation setting. \n- **Formatting issue**: I believe the font used in the submission violates the ICLR template. Please revise this in the updated pdf."}, "questions": {"value": "1. Could the observed persistence of biases be primarily due to the psycholinguistic test corpora being OOD relative to the RM training data? For example, do preference gaps narrow more substantially on in-distribution prompts aligned with RM training datasets (e.g., helpfulness, truthfulness, safety)?\n\n---\n\nOverall, I find this an impactful and a well-executed study. I am willing to increase my score if the authors address my question regarding the OOD nature of the test sets relative to the RM training data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8KvgJDAqwk", "forum": "dT399j1Azv", "replyto": "dT399j1Azv", "signatures": ["ICLR.cc/2026/Conference/Submission21691/Reviewer_ffKc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21691/Reviewer_ffKc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760691668905, "cdate": 1760691668905, "tmdate": 1762941892343, "mdate": 1762941892343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how reward models (RMs) inherit value biases from the pretrained LLMs they are initialized from. Using validated psycholinguistic corpora (\"Big Two\" and MFD2), the paper finds Llama-based RMs consistently show a preference for \"agency\" values (e.g., freedom, success, ability), while Gemma-based RMs show a preference for \"communion\" values (e.g., love, family, friendship). This bias is traced directly back to the log probabilities of the instruction-tuned and even the original pre-trained models. The paper also shows that while the bias in RMs can be mitigated with sufficient preference data, it does not fully disappear."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "**S1.** The paper clearly traces moral-value biases (agency vs. communion) from the outputs of trained RMs back to the log-probabilities of the base pre-trained models, which provides a clear takeaway that the choice of base model for RM training is also a critical decision that will have downstream value implications.\n\n**S2.** The paper evaluates multiple open-weight RMs based on psycholinguistic validation through controlled ablations on data and base model selections."}, "weaknesses": {"value": "**W1.** The central claim that reward models (RMs) inherit biases from their base pretrained LLMs already feels intuitive and largely expected, given prior research demonstrating bias propagation across fine-tuning and alignment stages [1, 2]. Therefore, this makes the contribution of the paper primarily observational since it does not provide mechanistic interpretability, analysis of latent representations, or deeper causal insight into why such biases emerge. Furthermore, it doesn't offer any actionable mitigation strategy beyond suggesting careful base model selection.\n\n**W2.** The analysis is limited to a narrow experimental scope, focusing only on two base model families (Llama and Gemma; three if we consider Qwen in the Appendix) with two parameter scales (2B and 3B with LoRA), and primarily the agency/communion axis from the Big-Two framework despite referencing MFD2.\n\n## References\n\n[1] Fulay, S., Brannon, W., Mohanty, S., Overney, C., Poole-Dayan, E., Roy, D., & Kabbara, J. (2024). On the relationship between truth and political bias in language models. arXiv preprint arXiv:2409.05283.\n\n[2] Xiao, J., Li, Z., Xie, X., Getzen, E., Fang, C., Long, Q., & Su, W. J. (2025). On the algorithmic bias of aligning large language models with rlhf: Preference collapse and matching regularization. Journal of the American Statistical Association, (just-accepted), 1-21."}, "questions": {"value": "In terms of experimentation, it may be interesting to model scaling law behavior rather than just observe it, such as how increasing preference data or compute (model size) affects the persistence of inherited biases, along with proposing a concrete methodology for mitigating such biases at larger scales."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1WRmim0dtr", "forum": "dT399j1Azv", "replyto": "dT399j1Azv", "signatures": ["ICLR.cc/2026/Conference/Submission21691/Reviewer_8kSZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21691/Reviewer_8kSZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798733058, "cdate": 1761798733058, "tmdate": 1762941891943, "mdate": 1762941891943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents case studies that investigate how reward models (RMs) inherit systematic value biases from the base LLMs on which these RMs are instantiated. The authors examined the preferences on a token level along dimensions characterised by two psycholinguistic corpora (Big Two - Agency vs. Communion, and Moral Foundations Dictionary - 5 further aspects). By analysing token preference differences on 10 open-source RMs, the authors validate that Gemma- and Llama-based RMs have systematic differences. Further investigations on the Big Two corpora shows the differences can be traced back to the token probabilities of pre-trained Gemma and Llama models. Experiments tracking Big Two token preferences during RM training show that the inherited biases persist.\n\nThese insightful findings draw attention to the choice of base LLMs for RM training and thus the entire LLM pipelines, urging the research community to reflect on current standard practices."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- This paper offers some fundamental insights for the research community on choosing base LLMs for RM training, which is underexplored. A focus shift from pure performance metrics to more fine-grained details on value biases is much needed these days.\n- The investigations done in the paper make sense and are quite novel, providing solid evidence of the inheritance traces of value biases. Experiments also cover diverse aspects.\n- Clarity is excellent - clear motivation, adequate and in-depth discussions, good coverage of related work, and discussions on limitations."}, "weaknesses": {"value": "- The RMs used in Sections 3 and 4 are quite small (2B and 3B), somewhat limiting the significance of results and the validity of relevant claims.\n- Sections 3 and 4 focus on a binary value distinction between \"Agency\" and \"Communion\". This seems a bit arbitrary. It is also obvious that different types of LLMs (Llama vs. Gemma) would have systematic differences. I would assume that if I randomly choose two common value aspects to repeat the same investigations, I would observe different preferences anyway. Could the authors comment on this choice?\n- I have the impression that the findings this paper presents are an instantiation of a common phenomenon, model multiplicity, that we can obtain machine learning models that perform similarly but differ in their internals for the same task, in the realm of reward modelling and LLM training. How is your finding different from something like, \"for a tabular classification task predicting loan default, I find one neural network prefers feature AGE more, and another neural network prefers feature LOAN AMOUNT more\"?"}, "questions": {"value": "See weaknesses.\n- One additional comment: there are also RMs that are trained to explicitly predict scores along certain axes (helpfulness, verbosity, coherence, etc., see datasets like HelpSteer) in a multi-regression style. These prediction signals could potentially be a better playground to perform your investigations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4kvGiYRzsx", "forum": "dT399j1Azv", "replyto": "dT399j1Azv", "signatures": ["ICLR.cc/2026/Conference/Submission21691/Reviewer_XumC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21691/Reviewer_XumC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926212845, "cdate": 1761926212845, "tmdate": 1762941891728, "mdate": 1762941891728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors studied the problem of how base model induces inherent bias in the reward model that was fine tuned from them, using methods from psycholinguistics. Using two existing psycholinguistics datasets from domain experts, the authors tested RM from several based models and concluded they indeed induce inherent biases. They then traced the source of the biases and find log probability can explain these biases from the base model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors took an interdisciplinary approach in viewing the question --- we should indeed borrow existing domain knowledge from disciplines that studies human values. \n- The statistical tests are careful in e.g., FDR control\n- The framing of model difference as reward difference is an interesting view point"}, "weaknesses": {"value": "Exhaustive token search might itself have limitations, I am not sure how the prompting scheme have an influence in this process."}, "questions": {"value": "## Statistical analysis\n- The difference can be significant but not large, and a small p-value can be due to large sample size. I am not very sure what is the best way to interpret the differences the authors reported in Fig.1, they do not seem to be large in some cases. Especially since the authors showed error bar using standard errors rather than standard deviations. I have no doubt the differences are *statistically significant*, but are they really *meaningful*? E.g., median rank of 1000 and 1001 might not be that meaningful even if the difference is significant because the estimation error is small.\n\n-  Fig.2 provides a bit more insight since the density plot. \n\n## Implicit reward \n\n- Is it fair to say that the author would also view KL between two models to be reward that can make model A to B if trained with RL? \n\n## Prompting\n- Does prompting have an influence in vocab search?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rIFCdXwwY2", "forum": "dT399j1Azv", "replyto": "dT399j1Azv", "signatures": ["ICLR.cc/2026/Conference/Submission21691/Reviewer_gN69"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21691/Reviewer_gN69"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946264941, "cdate": 1761946264941, "tmdate": 1762941891522, "mdate": 1762941891522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}