{"id": "TND5hiXjeM", "number": 22574, "cdate": 1758333010764, "mdate": 1759896858646, "content": {"title": "VIBE: Vision transformer based experts network for SSVEP decoding", "abstract": "Steady-state visual evoked potential based brain–computer interfaces (SSVEP-BCIs) have attracted wide attention for their high information transfer rate (ITR) and non-invasiveness. However, existing deep learning methods for SSVEP-BCI decoding have reached a performance bottleneck, as they struggle to fully extract the complex neural signal features required for robust performance. Motivated by advances in vision and time series modeling, here we present a \\textbf{VI}sion Transformer \\textbf{B}ased \\textbf{E}xpert network (VIBE), a multistage deep learning framework for SSVEP classification. VIBE integrates a Vision Transformer (ViT) module to generate rich spatiotemporal representations with data and network enhancement modules in a decoder for frequency recognition. We evaluate VIBE on two large benchmark datasets, including the Benchmark and the BETA dataset spanning 105 subjects. Notably, with just 0.4 seconds of stimulation, our VIBE achieves an ITR of $263.8$ bits per minute (bpm) and $202.7$ bpm on the Benchmark and BETA datasets, respectively. Experimental results demonstrate that VIBE consistently outperforms state-of-the-art baselines in offline experiments, highlighting its effectiveness as a high-performance decoding strategy for SSVEP-BCIs.", "tldr": "VIBE leverages a ViT based model to extract EEG features, achieving state-of-the-art SSVEP classification performance on large datasets.", "keywords": ["EEG", "SSVEP", "ViT", "BCI", "MoE"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a2ed3e980584b2b695ba658e8d04618964bfccb.pdf", "supplementary_material": "/attachment/9cae5ed169f3d9164175575d19fd05a5bdc23449.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents VIBE, a new deep learning model designed to improve the decoding of SSVEPs for BCI. While the results reported are strong, the paper has several significant weaknesses that make it difficult to judge the true novelty and contribution of the work. The core issues revolve around motivation, technical justification, and experimental rigor."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Refer to Questions"}, "weaknesses": {"value": "Refer to Questions"}, "questions": {"value": "The introduction claims that current methods struggle to \"jointly exploit local inductive biases and global dependencies.\" This statement is too general and high-level. It doesn't clearly pinpoint the specific shortcomings of existing models. For example, what exact \"local inductive biases\" are current CNNs missing? How do existing Transformers fail to capture \"global dependencies\" in the specific context of EEG signals? A more concrete problem definition is needed.\n\nFurthermore, the literature review fails to capture the most recent advancements in the field. The most recent related work cited is from 2022, which is a considerable gap in the fast-moving area of deep learning for BCIs. For a paper submitted to ICLR 2026, it is expected to engage with work from 2023 and 2025. The absence of these references raises a critical question: Are the authors solving a problem that has already been addressed by more recent models? Without comparing against these newer methods, it's impossible to know if VIBE is truly state-of-the-art or simply reinventing the wheel.\n\nThe paper combines several existing components，a Vision Transformer (ViT), a Mixture of Experts (MoE) layer, and data augmentation, but the novelty of this combination is not well-justified. The idea of using a ViT to generate or extend a temporal sequence is interesting. However, the paper does not sufficiently explain why this is a better approach than other sequence generation or feature enhancement techniques. It feels more like an application of an existing architecture (\"let's try ViT on EEG\") rather than a principled innovation. The ablation study shows it helps, but the underlying \"why\" remains unclear. The decision to replace the last temporal convolution layer with an MoE layer is a major architectural choice, but it is made without strong justification. The authors provide an ablation table in appendix Table 2, showing the performance of MoE in different layers, but they do not explain why the second temporal layer is the optimal choice from a theoretical or neuroscience-informed perspective. Is it because this layer captures high-level, abstract temporal patterns that benefit from specialization? \n\nThe baselines chosen for comparison are not representative of the current cutting edge. Many other recent, high-performing models, e.g., other 2024/2025 Transformer variants, Mamba-based models like the one cited as \"SUMamba (2026)\",  are mentioned in the related work but are not included in the experimental comparisons.\n\nWhile the paper includes an ablation study, it is not thorough enough to fully validate the design, e.g., why this specific architecture, and why using the staged training scheme, which needs to be further justified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dSSRssjqQX", "forum": "TND5hiXjeM", "replyto": "TND5hiXjeM", "signatures": ["ICLR.cc/2026/Conference/Submission22574/Reviewer_ideL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22574/Reviewer_ideL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649678586, "cdate": 1761649678586, "tmdate": 1762942287288, "mdate": 1762942287288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework to address the SSVEP EEG classification task. It features a cleverly-designed ViT encoder-decoder structure, alongside a MoE, and several pre-training stages, that all-together result in good classification and efficiency performances. The paper is well-written but seems to lack some ablation studies (see below), some choices of hyperparameters are not specified, and the structure of the Method section is a bit hectic to go through sequentially. There are also some interesting introductions of data augmentation techniques."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- performances above current SotA on both datasets.\n\n- the introduction of an auxiliary load-balancing loss, which prevents the model from sticking to the same subset of experts in the MoE throughout training.\n\n- a series of t-SNE plots in the appendix to visualize the knowledge of the trained proposed model.\n\n- computational stats are provided in Tab. 7, such as training and test time. Indeed, the proposed approach seems very efficient in both phases.\n\n- code is provided to reproduce results."}, "weaknesses": {"value": "- there is no \"results table\", showing the numbers obtained by the proposed method compared to the rest of the related methods. What comes close are Fig. 2 and 3, but the individual numbers are not shown for each point in the plots.\n\n- the ablation study lacks some info, like the results of the baseline with/without all the data augmentation techniques to which to compare in Tab. 3. Moreover, no results about combination of techniques are provided (e.g. model using only stitching and channel shuffle, or model using only channel shuffle).\n\n- in the paragraph that starts at L656, it is not clear why such values for learning rate or batch size are used.\n\n- Fig. 1 is not so auto-explicative. This reviewer believe that there is too much text in it, without a clear indication about where to start looking at. Moreover, the caption only talks about points a) and e), but not b-d). I suggest redesigning the figure and its caption entirely."}, "questions": {"value": "- it seems interesting the authors are merging different chunks of EEGs in the cross-subject temporal stitching. But what is the rationale behind this choice? Creating new samples by concatenating two chunks pertaining to (eventually) different subjects or trials without accounting for inter-subject/trial variabilities may lead to meaningless new samples.\n\n- This reviewer gets the idea behind channel chunk shuffle, but intuitively it may completely alter the meaning of an EEG: think about how randomly swapping the RGB channels of a natural image completely alters its colors. Again, what is the rationale?\n\n- why did the authors choose values of 6 and 4 k-folds for the two datasets? If it is due to replicating the setting of another method, there should be a citation on L305."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9vu0ROYhHm", "forum": "TND5hiXjeM", "replyto": "TND5hiXjeM", "signatures": ["ICLR.cc/2026/Conference/Submission22574/Reviewer_S2CM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22574/Reviewer_S2CM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761689392872, "cdate": 1761689392872, "tmdate": 1762942287009, "mdate": 1762942287009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VIBE (Vision Transformer Based Experts Network), a multistage deep learning framework designed for steady-state visual evoked potential (SSVEP) classification in brain-computer interfaces. The work addresses the performance bottleneck of existing deep learning methods for SSVEP-BCI decoding, which struggle to fully extract complex neural signal features required for robust performance under short time windows. VIBE employs a dual-component architecture consisting of a Vision Transformer (ViT) module for temporal generation and a Mixture of Experts (MoE) decoder for frequency recognition. The ViT module expands short input EEG sequences (multi-band data represented as tensors with dimensions corresponding to sub-bands, channels, and time samples) into richer temporal representations by partitioning signals into non-overlapping patches, processing them through a transformer-based decoder, and concatenating the generated temporal segments with original inputs. The MoE decoder integrates subband, channel, and temporal dependencies through expert specialization, where a gating network routes inputs to top-k experts that process distinct temporal or spectral patterns, enhanced by an auxiliary load-balancing loss using KL divergence to encourage balanced expert utilization and prevent overfitting. The framework incorporates data augmentation strategies specifically designed for EEG, including cross-subject temporal stitching (creating synthetic trials by randomly sampling temporal segments across subjects to promote frequency-level invariance), channel chunk shuffling (simulating varied electrode placements by swapping channel chunks), random temporal cropping (addressing inter-subject latency variability), and channel decorrelation via covariance-based whitening to reduce subject-level variability. Training follows a four-stage procedure with staged transfer learning: ViT generative pretraining on all subjects, ViT subject-specific fine-tuning, MoE decoder pretraining using pooled data across subjects with augmentation, and MoE subject-specific fine-tuning for individual adaptation.\nThe experimental evaluation demonstrates substantial performance improvements over both traditional methods (CCA, TRCA, TDCA, eTRCA, eCCA, msTRCA) and deep learning baselines (DNN, SSVEPformer, TRCANet) on two large benchmark datasets spanning 105 subjects: the Benchmark dataset (35 subjects, controlled laboratory environment) and BETA dataset (70 subjects, naturalistic settings). At the shortest data length of 0.2 seconds, VIBE achieves the largest accuracy advantage (Benchmark: 65.5% vs. 58.8% for next-best; BETA: 53.7% vs. 46.2%), with maximum information transfer rates of 263.8±11.7 bpm at 0.4s for Benchmark and 202.7±8.9 bpm for BETA, exceeding DNN baseline by approximately 15 bpm and 12 bpm respectively. Statistical analysis via two-way repeated-measures ANOVA reveals significant interaction effects between method and data length (p<0.001 for both datasets), with paired t-tests confirming VIBE's superiority over DNN and TDCA across all evaluated time windows (all p<0.05), particularly pronounced at short durations (0.2s: p=1.0×10^-12 vs. DNN, p=2.2×10^-14 vs. TDCA for Benchmark). Comprehensive ablation studies quantify individual component contributions: removing ViT regeneration causes the largest performance drop to 61.8%, removing MoE reduces accuracy to 64.2%, and eliminating all data augmentation decreases performance to 62.1% from the full model's 65.5% on Benchmark at 0.2s. Further analysis shows that MoE placement on the second temporal layer is most effective, decorrelation has the strongest impact among augmentation strategies (63.3% without it), and optimal ViT generation time length is data-length dependent (longer generation benefits short trials, shorter generation for longer trials). t-SNE visualization reveals that the full model produces more compact and discriminative feature clusters compared to ablated versions, while subject-wise ITR radar plots demonstrate VIBE's consistent superiority across individual subjects, establishing its effectiveness as a high-performance decoding strategy for SSVEP-BCIs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents a well-motivated approach to SSVEP-BCI decoding with several notable strengths. The proposed VIBE framework demonstrates clear architectural innovation by combining ViT-based temporal generation with MoE-based decoding in a manner specifically tailored to EEG signal characteristics, moving beyond generic application of existing architectures. The experimental validation is comprehensive and rigorous, employing proper cross-validation protocols (k=6 for Benchmark, k=4 for BETA) across two large-scale datasets spanning 105 subjects in both controlled laboratory and naturalistic settings, with consistent performance gains across all evaluated conditions. The results are particularly compelling at short time windows (0.2s), where VIBE achieves substantial accuracy improvements (6.7% on Benchmark, 7.5% on BETA) that directly address the stated challenge of rapid decoding. The ablation studies are thorough and well-designed, systematically isolating the contribution of each component (ViT, MoE, augmentation strategies) and providing additional analyses on architectural choices (MoE layer placement, generation time length effects, individual augmentation impacts), which collectively validate the design decisions and demonstrate that improvements stem from the synergistic combination of components rather than any single element. The paper also provides thoughtful discussion of neural underpinnings for each proposed module, connecting design choices to physiological properties of SSVEP signals and EEG characteristics, while appropriately acknowledging limitations regarding inter-subject variability and the need for online validation."}, "weaknesses": {"value": "The paper's experimental design and evaluation methodology raise several concerns that limit the generalizability and interpretability of the findings. First, while the authors claim superior performance, the evaluation protocol employs trial-based cross-validation rather than subject-based cross-validation, which is problematic given that the model undergoes subject-specific fine-tuning in its fourth training stage. This means the model has already seen and adapted to data from the test subject during fine-tuning, potentially leading to optimistic performance estimates that may not reflect true generalization to completely unseen users. The claim of \"subject-independent evaluation protocol\" is therefore misleading, as genuine subject-independence would require testing on subjects whose data was never used during any training stage, including fine-tuning. Second, the comparison with baseline methods appears unbalanced: traditional methods like TDCA and TRCA receive no equivalent multi-stage training or subject-specific adaptation, while deep learning baselines like DNN are not provided with the same data augmentation strategies or temporal generation mechanisms that VIBE employs. A fairer comparison would either provide all baselines with similar data augmentation and multi-stage training opportunities, or evaluate VIBE without these advantages to isolate the true architectural contributions. Third, the paper lacks critical analyses for practical deployment, including computational cost comparisons during training and inference, memory requirements for storing subject-specific models, and cross-dataset generalization experiments where models trained on one dataset are directly tested on another without any fine-tuning. The brief mention of training times in the appendix (e.g., 4180 seconds for decoder pretraining on Benchmark) suggests substantial computational overhead, but no comparison with baseline methods is provided to contextualize whether this cost is justified.\n\nBeyond experimental concerns, the paper suffers from limited technical novelty and insufficient depth in several key aspects. The core components of VIBE are largely borrowed from existing architectures: ViT for temporal generation is a straightforward application of the standard Vision Transformer to EEG data with minimal adaptation beyond treating multi-band signals as image tensors, while MoE has been previously applied to EEG decoding tasks as acknowledged by the authors. The primary contribution appears to be the specific combination and staged training of these existing modules rather than fundamental architectural innovation, yet the paper does not adequately justify why this particular combination is theoretically well-suited for SSVEP decoding compared to alternative fusion strategies. The ablation study only compares against removing components entirely or using simple co-attention, without exploring other sophisticated fusion mechanisms from the multimodal learning literature. Furthermore, the data augmentation strategies, while diverse, are presented without sufficient justification: cross-subject temporal stitching may actually harm performance by mixing neural responses from different individuals with distinct physiological characteristics, yet no analysis is provided to verify whether this augmentation actually improves generalization or simply acts as noise injection. The physiological interpretations offered in the discussion are speculative and lack empirical validation through techniques such as attention weight visualization, frequency spectrum analysis of learned representations, or correlation with known neurophysiological markers of SSVEP responses. The paper also lacks insight into what patterns the MoE experts actually specialize in learning, which temporal or spectral features the ViT generation emphasizes, or whether the learned representations align with established neuroscience understanding of SSVEP generation mechanisms, making it difficult to assess whether the model's success stems from capturing meaningful neural dynamics or exploiting dataset-specific statistical regularities."}, "questions": {"value": "Please see your weaknesses and I will adjust the final score based on your answers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XUkQYsTg7w", "forum": "TND5hiXjeM", "replyto": "TND5hiXjeM", "signatures": ["ICLR.cc/2026/Conference/Submission22574/Reviewer_3S4g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22574/Reviewer_3S4g"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748502759, "cdate": 1761748502759, "tmdate": 1762942286784, "mdate": 1762942286784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VIBE (Vision Transformer-Based Experts Network), a deep learning framework designed for SSVEP-based BCI decoding. VIBE employs a Vision Transformer (ViT) for temporal signal extension, complemented by a Mixture-of-Experts (MoE) decoder to exploit subband, channel, and temporal dependencies. Novel EEG-specific augmentations (temporal stitching, channel chunk shuffle, decorrelation) enhance model generalization. Evaluations on Benchmark and BETA datasets (105 subjects total) demonstrate superior performance, consistently surpassing traditional (e.g., eCCA, TRCA, TDCA) and deep (DNN, SSVEPformer) baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The combination of ViT temporal generation and specialized MoE decoder is novel and thoughtfully justified. EEG-specific augmentations are creative and practically useful.\n- Comprehensive experimentation on large datasets and strong baselines clearly demonstrate performance improvements, supported by robust ablation analyses.\n- Well-described methodology, clear visualization (architecture, accuracy/ITR plots, t-SNE feature visualization).\n- Substantial accuracy and ITR gains for short observation periods demonstrate clear potential for practical high-speed BCIs."}, "weaknesses": {"value": "- Lack of cross-subject validation limits claims about broader applicability. Evaluations are confined to within-subject validation, limiting generalization conclusions.\n- The realism of channel chunk shuffle is not sufficiently evaluated. Alternative realistic augmentation methods are not explored.\n- Assumed gaze-shift times might differ in practical scenarios; online evaluation would validate practical effectiveness."}, "questions": {"value": "- Could you provide leave-one-subject-out (LOSO) evaluations to assess generalization robustness clearly?\n- Statistical Clarification: Were multiplicity corrections (Holm/FDR) applied to paired t-tests? Please clarify.\n- Electrode Sensitivity: How sensitive is performance to variations in electrode number or placement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uqDPdGoBtM", "forum": "TND5hiXjeM", "replyto": "TND5hiXjeM", "signatures": ["ICLR.cc/2026/Conference/Submission22574/Reviewer_bXb6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22574/Reviewer_bXb6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988742593, "cdate": 1761988742593, "tmdate": 1762942286517, "mdate": 1762942286517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}