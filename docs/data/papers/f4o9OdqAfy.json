{"id": "f4o9OdqAfy", "number": 738, "cdate": 1756779237061, "mdate": 1763088764697, "content": {"title": "TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation", "abstract": "Large Language Models (LLMs) are exhibiting emergent human-like abilities and are increasingly envisioned as the foundation for simulating a specific communication style, behavioral tendencies, and personality traits. \nHowever, current evaluations of LLM-based persona simulation remain limited: most rely on synthetic dialogues, lack systematic frameworks, and lack analysis of the capability requirement. \nTo address these limitations, we introduce TwinVoice, a comprehensive benchmark for assessing persona simulation across diverse real-world contexts. \nTwinVoice encompasses three dimensions: Social Persona (public social interactions), Interpersonal Persona (private dialogues), and Narrative Persona (role-based expression).\nThe ability of LLMs in persona simulation is further decomposed into six fundamental capabilities, including opinion consistency, memory recall, logical reasoning, lexical fidelity, persona tone, and syntactic style. \nExperimental results reveal that while advanced models achieve moderate accuracy, they remain insufficient in sustaining consistent persona simulation, especially lacking the capability of syntactic style and memory recall.\nOur data, code, and evaluation results are available at https://anonymous.4open.science/r/TwinVoice-B08E.", "tldr": "", "keywords": ["large langauge model", "digital twin", "persona simulation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/547c15bc0b39921e6906fde128a535a24b02be10.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TwinVoice, a benchmark for evaluating persona simulation using LLMs. TwinVoice includes three dimensions (settings or contexts for persona simulation) and six capabilities (evaluation criteria). TwinVoice sources its ground truth from real-world human writing across multiple languages and evaluates in both discriminative and generative answer formats. The empirical evaluations on various LLMs, including state-of-the-art LLMs, reveal that six capabilities for each LLM are correlated, while strengths and weaknesses across LLMs are stable."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses an important problem of evaluating LLMs at human simulation.\n\n2. The scaled-up evaluation protocol of TwinVoice provides more comprehensive information on the human-simulation performance of LLMs."}, "weaknesses": {"value": "1. The paper does not provide evidence of the value this new benchmark adds over the existing benchmarks. It is unclear whether the findings in this paper could have been obtained using existing benchmarks and, if not, which component of this benchmark enabled it.\n\n2. There is no clear rationale behind the choices of six capabilities. They seem to overlap, and not all of them will be relevant at every turn. For example, persona tone will correlate with both lexical and syntactic choices. And not all conversations involve logical reasoning.\n\n3. Discriminative evaluation is not well justified in its practicality and value. The real-world simulation will mostly take a generative form, so it would be helpful to see if discriminative evaluation agrees with generative evaluation instance-wise. Moreover, there are insufficient details about the choice of distractors for discriminative evaluation, which is essential for contextualizing it."}, "questions": {"value": "1. What are the findings that are not available in other benchmarks but TwinVoice enables? Would it require non-trivial effort to make similar findings on other benchmarks? Do you have supporting empirical evidence? What component of TwinVoice enables it?\n\n2. How did the authors decide on the six capabilities? Are all instances of the data appropriate for evaluating these six capabilities? If not, do you have an estimate on how much of the data is relevant to each capability? For each capability, are there any deterministic or cheaper alternative metrics that can replace LLM-as-judge?\n\n3. Is discriminative evaluation really needed in addition to generative evaluation? How were distractors selected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hRGnuOhJHH", "forum": "f4o9OdqAfy", "replyto": "f4o9OdqAfy", "signatures": ["ICLR.cc/2026/Conference/Submission738/Reviewer_8Myh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission738/Reviewer_8Myh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762805685, "cdate": 1761762805685, "tmdate": 1762915593664, "mdate": 1762915593664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* This paper introduces the TwinVoice benchmark, a novel and large-scale benchmark for realistic and fine-grained LLM persona simulation.\n* The work proposes a novel evaluation framework for persona fidelity, targeting analysis from 2 main aspects: mindset coherence and linguistic expression.\n* The paper conducts comprehensive evaluation experiments on persona simulation performance with multiple state-of-the-art LLMs, revealing current limitations in aspects such as syntactic style and memory recall."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed TwinVoice benchmark is more comprehensive than previous works:\n  * TwinVoice has over 4,500 personas for evaluation, exceeding the size of prior works. The persona dataset is dissected into 3 categories: social persona, interpersonal persona, and narrative persona. This improves the diversity and robustness of persona fidelity evaluation.\n  * TwinVoice consists of both real-world and synthetic data, resolving the issue with dominating synthetic data usage in existing works.\n* The proposed TwinVoice benchmark approaches persona simulation assessment from multiple aspects, offering new perspectives of evaluating personalized LLM systems. \n  * The work proposes a novel evaluation framework for persona fidelity, targeting analysis from 2 main aspects: mindset coherence and linguistic expression.\n    * The 2 aspects of persona simulation evaluation is further elaborated into the test of 6 fundamental capabilities, such as logical reasoning and opinion consistency for mindset coherence and lexical fidelity for linguistic expression.\n  * Evaluation methodology includes both discriminative multiple-choice assessment and open-ended LLM-as-a-Judge paradigm.\n    * The GPT5-as-a-judge evaluation framework is human-verified with decent agreement score.\n* The paper conducts comprehensive evaluation experiments on persona simulation performance with multiple state-of-the-art LLMs, revealing current limitations in aspects such as syntactic style and memory recall."}, "weaknesses": {"value": "* While I appreciate the authors for conducting a human study on the proposed LLM-as-a-judge evaluation framework, I still have minor concerns about the pipeline's robustness:\n  * First, for the human verification scale, 50 items per judging mode (100 total) for such a big evaluation benchmark might not be enough.\n  * Additionally, previous works [1] [2] have revealed robustness issues with LLM-as-a-judge frameworks, and similar biases could lead to robustness issues of the proposed evaluation framework. \n    * For example, [1] discusses how LLM judges prefer texts that are more familiar to them (self-preference bias), like their own generations or generations from LLMs of similar architecture / training data. **This might explain the good performance of GPT-5-Chat (strongest aggregate generative performance as stated in line 374), since you are using GPT5 as the judge.**\n    * Another example would be position bias [2]. **Permutating the choices in the multiple choice & ranking evaluation framework and take the aggregated result** will bring more robustness.\n* Lack of qualitative analysis on experiment results, especially on aspects where models fail (e.g. syntatctic style, memory recall). Looking at the numbers and charts, I have a hard time understanding what failure pattens we can observe in models' generation outputs.\n\n[1] Wataoka, Koki, Tsubasa Takahashi, and Ryokan Ri. \"Self-Preference Bias in LLM-as-a-Judge.\" Neurips Safe Generative AI Workshop 2024.\n[2] Thakur, Aman Singh, et al. \"Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges.\" arXiv preprint arXiv:2406.12624 (2024)."}, "questions": {"value": "* How robust is the proposed LLM-as-a-Judge pipeline against issues like position and self-preference biases?\n* Can you show some failure cases that show model failure modes in a more straightforward way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e89OaKl2A9", "forum": "f4o9OdqAfy", "replyto": "f4o9OdqAfy", "signatures": ["ICLR.cc/2026/Conference/Submission738/Reviewer_3bXF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission738/Reviewer_3bXF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809593668, "cdate": 1761809593668, "tmdate": 1762915593476, "mdate": 1762915593476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Please see the weakness section."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Please see the weakness section."}, "weaknesses": {"value": "**Significant ICLR formatting violation**\n\nThis submission appears to use 1.0 inch left / right margins, significantly below the regulation of 1.5 inch (“Formatting instructions for ICLR 2026 conference submissions,” Line 30, Line 50). This expands the text width from the mandated 5.5 inch to 5.5 + 0.5 * 2 = 6.5 inch, so 9 pages × (6.5 / 5.5) = 10.64 pages of effective content, exceeding the strict 9-page limit (“At the time of submission, the main text should be 9 pages or fewer… This limit will be strictly enforced. Papers with main text beyond the page limit will be desk-rejected.” of the ICLR 2026 author guide https://iclr.cc/Conferences/2026/AuthorGuide). The “Formatting instructions for ICLR 2026 conference submissions” also warns that “tweaking the style files may be grounds for rejection.”\n\nHistorically, there has been papers desk-rejected for the exactly same problem.\n\n- “Circuit Compositions: Exploring Modular Structures in Transformer-Based Language Models”, ICLR 2025 submission https://openreview.net/forum?id=u4XyECA6Zd)\n\n- “EVLM: An Efficient Vision-Language Model for Visual Understanding”, ICLR 2025 submission https://openreview.net/forum?id=S7M1iqFLVm\n\n- “Inductive Bias of Multi-Channel Linear Convolutional Networks with Bounded Weight Norm”, ICLR 2022 submission https://openreview.net/forum?id=NMSugaVzIT"}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hxC2hxunAH", "forum": "f4o9OdqAfy", "replyto": "f4o9OdqAfy", "signatures": ["ICLR.cc/2026/Conference/Submission738/Reviewer_z3pb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission738/Reviewer_z3pb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880340703, "cdate": 1761880340703, "tmdate": 1762915593325, "mdate": 1762915593325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper suggests a new benchmark that assesses LLM's ability of replicating human persona. The authors used three different datasets to model human persona in terms of three aspects: social, interpersonal, and narrative. To ensure the validity and generalizability of experiment, the authors measured three aspects with three different methods: discriminative, generative(scoring) and generative(ranking). Also, they further measured each aspect in terms of six capabilities. The result found that language models have some ability to replicate human persona, but there's still room for improvement, especially on capabilities of memory recall and persona-tone alignment. And, the authors also noted that the gap between discriminative and generative evaluation protocols indicates the difficulty of open-ended persona replication."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper could provide a framework for detailed analysis on digital twin (or human persona replication). \n- The paper provides some discussion about the strength and weakness of present LLMs.\n- The paper tested multiple models, which strengthens its generalizability."}, "weaknesses": {"value": "- The experimental method has issues. The selection of language models is not systematic (and there's no reason specified in the paper), and the judge model overlaps with the generation model. The agreement measurement is not proper. See Question A.\n- The paper's review on the previous benchmark or papers is somewhat shallow. Other researchers have been reported similar findings, especially on memory recall and persona consistency. Though the paper aims to be a unified framework, the paper should discuss the difference between it and the other previous work clearly to avoid confusion of reader. See Question B.\n- The paper's discussion is not deep enough. To provide some insight for the community, this paper could provide some reason or conjecture which can guide the future research. The current depth is insufficient regarding this aspect. See Questions A and B."}, "questions": {"value": "## Question A. Experimental method\n\nA1. Why did the authors selected those seven models? What is the main criteria of selection?\nA1-1. If there exists some criterion, could the authors link the findings with those criteria? Are there any factors affected the result?\n\nA2. The authors used GPT-5 for LLM-as-a-judge and GPT-5-Chat for answer generation. As the model stems from the same training data distribution, the judge might prefer its family which can show a similar generation behavior to them. Why did the authors selected same model for both judge and generation?\nA2-1. Does this selection affect the result? How?\n\nA3. The authors used Gutenberg project, which is a very well known corpus in the community. As the corpus was commonly used for training language models, it is highly likely that the models already learned the corpus. Doesn't this possibly affect the result for narrative aspect?\nA3-1. Can we check whether the model already learned the corpus, using the methods for checking data contamination?\n\nA4. This is a minor but could highly affect the result on human evaluation. The authors mentioned that they used three annotators. But they used Cohen's kappa, which is limited to two annotators because of its statistical assumption. In the case of three or more annotators, researchers usually adopt Fleiss' kappa, which is an extension of Cohen's kappa. So the question is that how did the authors provide the result of kappa? Were the authors averaged the result of all pairs?\nA4-1. For more statistically sound result, could the authors provide Fleiss' kappa instead of Cohen's?\n\nA5. Though the authors used temperature 0, it does not ensure deterministic behavior. Did the authors run the experiment multiple times? If so, could the authors provide the statistical errors or other statistical comparison results to support their discussion?\nA5-1. It could be better provide the result of statistical tests.\n\nA6. Lastly, how the authors provide history to generate chat in interpersonal experiment? Did the authors input all elements in the history?\nA6-1. Studies in persona-based dialogue generation usually use RAG or summarization systems to compact the history. If the authors used the entire history without summarization, can the difference affect the result? How?\n\n## Question B. Previous work\n\nB1. This question is in the same line with Question A6. Previous methods on persona-based dialogue generation use different methods. As these methods might affect the result, the authors should shortly and clearly state the difference between their method and the recent advances. For example, please refer to [1].\n\n[1] Saber Zerhoudi et al., PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents, https://arxiv.org/abs/2407.09394\n\nB2. Previously, researchers have discussed specific capabilities. What is the difference between this work and theirs, except for building a summative framework? For example, please refer to [1], [2], [3], and [4]. Note that these are few of related studies; there are lot of relevant studies.\nB2-1. Could the findings be connected with these studies?\n\n[2] Junhyuk Choi et al., Examining Identity Drift in Conversations of LLM Agents, https://arxiv.org/abs/2412.00804\n[3] J. Huang et al., On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs, ICLR 2024\n[4] R. Chen et al., Learning to Memorize Entailment and Discourse Relations for Persona-Consistent Dialogues, AAAI 2023\n\n## Question C. Other\n\nC1. In terms of presentation, Figure 3 is not appropriate if the authors want to compare between different capabilities of the same model. Redrawing the figure by grouping them with model, instead of capabilities might be more suitable; especially for lines 363-364.\n\nC2. I think the authors should warn the readers that their framework might be used for replicating a human behavior and can be used for fraud. This statement should be added in the ethics statement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3wSqHMLCAU", "forum": "f4o9OdqAfy", "replyto": "f4o9OdqAfy", "signatures": ["ICLR.cc/2026/Conference/Submission738/Reviewer_CJUf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission738/Reviewer_CJUf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission738/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014071075, "cdate": 1762014071075, "tmdate": 1762915593212, "mdate": 1762915593212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}