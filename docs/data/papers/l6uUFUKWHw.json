{"id": "l6uUFUKWHw", "number": 2647, "cdate": 1757180222489, "mdate": 1759898135685, "content": {"title": "iGRPO: Self‑Feedback–Driven LLM Reasoning", "abstract": "Large Language Models (LLMs) have demonstrated considerable promise in solving complex mathematical problems, yet they still fall short of producing fully accurate and consistent solutions. Reinforcement Learning (RL) has emerged as a powerful framework for aligning these models with task-specific rewards, thereby improving their overall quality and reliability. In particular, Group Relative Policy Optimization (GRPO) has emerged as an efficient, value-function-free alternative to Proximal Policy Optimization (PPO). In this paper, we introduce Iterative GRPO (iGRPO), a two-stage extension of GRPO that boosts performance by incorporating a self-feedback mechanism. Specifically, iGRPO first samples multiple candidate responses (self-feedback stage) and selects the highest-reward completion as feedback for the second stage (self-reflection stage). By conditioning the model on this top-performing draft, iGRPO effectively guides the policy to generate refined solutions surpassing the best first-stage candidates. Through systematic comparisons with GRPO across different base models (e.g., Nemotron-H-8B-Base-8K, DeepSeek-R1 Distilled), we extensively validate the effectiveness of iGRPO. In addition, further training the OpenReasoning-Nemotron-7B model with iGRPO algorithm results in new state-of-the-art benchmarks of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. These results underscores the potential of iterative, self-feedback–driven RL approaches for advancing the reasoning performance.", "tldr": "Self-feedback driven reasoning for LLMs.", "keywords": ["Reinforcement Learning", "Reasoning", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be2dfd10436d0c22e1f3d627c62c23368b43e81f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "iGRPO proposes the introduction of a self-feedback mechanism within the existing RL framework to enhance the performance of large language models (LLMs) in mathematical reasoning tasks.\nIn Stage 1 (self-feedback stage), the model generates multiple candidate responses for the same problem and selects the highest-scoring response via a reward model as the \"feedback text\";\nIn Stage 2 (self-reflection stage), the model concatenates this high-scoring response with the original prompt to regenerate a new answer, thereby achieving self-correction and optimization (see Figure 1, Sec. 3.2)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "In this paper, an Iterative GRPO (iGRPO) is investigated, where a two-stage extension of GRPO boosts performance by incorporating a self-feedback mechanism. The motivation is clear and this paper is easy to follow."}, "weaknesses": {"value": "1. Novelty: Several existing works utilize critics for self-improvement, such as Critique-GRPO [1]. The paper lacks methodological and performance comparisons with these works.\n2. Completeness: The authors only conducted experiments on five mathematical benchmarks, without validating generalization. The ablation study did not include case analyses of iGRPO's reasoning behavior or other effective analyses beyond performance metrics. Although GPU resource consumption was compared, the differences in resources and time required for full training compared to GRPO were not examined.\n3. Ablation Study: The ablation analysis is superficial, with some aspects being common to all RL methods, making it difficult to determine which specific components of the authors' design are necessary.\n\n[1] Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback."}, "questions": {"value": "1. In Figure 3, GRPO and iGRPO exhibit similar trends and dynamics. How does this demonstrate the advantages of iGRPO over GRPO?\n2. The ablation study in Figure 4(a) shows no significant differences. Moreover, the KL term is not specific to iGRPO, making this ablation experiment less meaningful.\n3. Why is the top draft \\hat{r} selected for reflection in Stage 1? Intuitively, reflecting on poorer samples would facilitate learning better responses. Given the same advantage function and rule-based evaluation as GRPO, samples in a batch that are either all correct or all incorrect have the same advantage. How are samples selected for Stage 2 prompts? A deeper methodological analysis and experimental explanation of this aspect are necessary.\n4. If another model (or the model itself) is used to generate high-quality data directly concatenated with the problem for Stage 2 (i.e., treating Stage 1 as an offline data generation process), how would the performance compare?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lgOuMYTfoI", "forum": "l6uUFUKWHw", "replyto": "l6uUFUKWHw", "signatures": ["ICLR.cc/2026/Conference/Submission2647/Reviewer_NC41"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2647/Reviewer_NC41"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552691881, "cdate": 1761552691881, "tmdate": 1762916318745, "mdate": 1762916318745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the need for improved accuracy in LLMs for complex mathematical reasoning tasks. The authors identify that existing RL alignment methods, such as GRPO, typically lack a mechanism for self-reflection. They propose Iterative GRPO (iGRPO), a two-stage extension of GRPO. In Stage 1 (Self-Feedback), the model samples $N$ candidate responses, and the highest-reward completion is selected. In Stage 2 (Self-Reflection), this top-performing draft is concatenated to the original prompt to form an \"augmented prompt\". A standard GRPO update is then performed using $G$ new completions sampled conditioned on this augmented prompt. Experimental results demonstrate that iGRPO consistently outperforms GRPO across various base models (7B, 8B, 14B) and achieves new state-of-the-art results on AIME24 (85.62%) and AIME25 (79.64%) when applied to a strong base model trained on a challenging dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The controlled studies in Table 1 rigorously demonstrate that iGRPO provides consistent improvements over a standard GRPO baseline across all tested models. This includes Nemotron-H-8B (+3.96% avg), DeepSeek-R1-7B (+1.58% avg), OpenMath-7B (+1.05% avg), DeepSeek-R1-14B (+1.73% avg), and OpenMath-14B (+1.27% avg).\n- The method is shown to scale effectively, delivering new state-of-the-art results (85.62% on AIME24, 79.64% on AIME25) when applied to the strong OpenReasoning-Nemotron-7B model on the AceReason-Math dataset.\n- The two-stage process adds \"practically negligible\" peak GPU memory overhead, with its peak usage being almost identical to that of the standard GRPO."}, "weaknesses": {"value": "- The mathematical formulation in Equation 3 5is a direct application of the GRPO objective from Equation 1 to this new prompt $q'$, rather than a modification of the optimization dynamics itself. The contribution is more accurately described as an in-context learning (ICL) or data augmentation strategy embedded within the RL training loop, rather than a novel RL algorithm.\n- It is simply being conditioned on its own best-performing output, which serves as a high-quality in-context exemplar. This is a passive conditioning step, not an active \"reflection\" process as the terminology implies.\n- The authors claim \"minimal extra overhead\". However, the experimental results in the appendix suggest otherwise. Table S.3 shows that iGRPO's training throughput is 0.34 samples/sec, compared to 0.41 samples/sec for GRPO. This constitutes a ~17% reduction in training speed. This is a significant cost that must be justified against the performance gains, which, while consistent, are often modest (e.g., a 1.05% average gain for OpenMath-Nemotron-7B or a 1.27% gain for OpenMath-Nemotron-14B ). The paper understates this trade-off.\n- Given that the method feeds the model's own best answer back to it as a prompt, the subsequent improvement in the second stage is expected. It is well-established that LLMs benefit from high-quality in-context exemplars. The paper does not adequately disentangle whether the gains come from this known ICL effect or from a more complex interaction with the RL policy update."}, "questions": {"value": "See *Weaknesses*."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "owk4CTi5FQ", "forum": "l6uUFUKWHw", "replyto": "l6uUFUKWHw", "signatures": ["ICLR.cc/2026/Conference/Submission2647/Reviewer_Ho8s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2647/Reviewer_Ho8s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572815790, "cdate": 1761572815790, "tmdate": 1762916318615, "mdate": 1762916318615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Iterative GRPO (iGRPO), a two-stage extension of GRPO that adds a self-feedback loop: the model first samples multiple completions, a reward model selects the top draft, and the second stage conditions on this exemplar for another GRPO update. The method is simple and shows moderate improvements on several math reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea is clear and straightforward to implement on top of existing GRPO pipelines.\n2. Consistent gains over GRPO across different models and datasets."}, "weaknesses": {"value": "1. The authors state that “this design preserves the efficiency of GRPO while introducing only minimal extra overhead” (line 65), , but the expensive part of GRPO is typically the rollout / long-cot sampling [1]. iGRPO performs two sequential sampling phases that cannot be parallelized, likely increasing per-step cost. The paper does not provide a direct comparison of per-step training time in the main body to substantiate this claim.\n2. The methods are not clearly described. The paper does not explain how the reward model $R_\\phi$ in stage-1 is trained or used. In cases where multiple completions share reward=1 or all receive reward=0, how is the top draft chosen? The paper also fails to explain what `\"<SelfFeedback>\"` actually is — a special token or a fixed prompt template? If it is a prompt template, the exact prompt should be provided.\n3. The experimental analysis section largely restates table values without deeper interpretation, and the manuscript contains at least one clear typo (end of paragraph at line 272).\n\n[1] RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training"}, "questions": {"value": "1. Will models trained in this method exhibit bias? The selection made in stage-1 appear to cause a collapse in the policy distribution (i.e., loss of diversity). The authors report reward and response length changes, but it is recommended to report entropy or other training dynamics metrics, too.\n2. During Stage 2 training the policy conditions on an augmented context (original query + top draft + feedback), but at downstream test time the model receives only the original query. Could this mismatch bias the output distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ePPSQ9C2xe", "forum": "l6uUFUKWHw", "replyto": "l6uUFUKWHw", "signatures": ["ICLR.cc/2026/Conference/Submission2647/Reviewer_xQA1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2647/Reviewer_xQA1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744320824, "cdate": 1761744320824, "tmdate": 1762916318432, "mdate": 1762916318432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Iterative GRPO (iGRPO), a two-stage reinforcement learning method that enhances large language models’ reasoning through self-feedback. The model first samples multiple responses, selects the best one, and uses it as feedback to refine its next generation. Experiments on mathematical reasoning benchmarks show that iGRPO consistently outperforms GRPO, achieving state-of-the-art results (85.62% on AIME24 and 79.64% on AIME25) with minimal extra cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a simple yet effective extension to GRPO called iGRPO, which adds a self-feedback stage to improve reasoning quality. The method is conceptually clear, easy to implement, and consistently boosts performance across benchmarks. Experiments are thorough and show strong, state-of-the-art results with minimal computational overhead."}, "weaknesses": {"value": "1. While integrating self-verification into an RL framework is interesting, similar ideas have been explored in prior work (e.g., *Incentivizing LLMs to Self-Verify Their Answers*). Including such methods as baselines could strengthen the empirical evaluation and clarify the contribution.\n\n2. The paper could provide more information about the reward model, such as its training setup and potential limitations. It would also be helpful to analyze how iGRPO’s performance depends on reward model quality or to discuss possible effects of using a generative reward model (e.g., GPT-5).\n\n3. The reported improvement over GRPO is relatively modest (around 1% in Table 1). Additional comparisons with strong baselines like DAPO or RLOO would help demonstrate how competitive iGRPO is under similar conditions.\n\n4. The paper would benefit from more discussion on how the trained model behaves during inference, especially regarding test-time scaling. Clarifying whether the improvements extend to realistic inference settings would make the results more convincing."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LGsX1dTfgK", "forum": "l6uUFUKWHw", "replyto": "l6uUFUKWHw", "signatures": ["ICLR.cc/2026/Conference/Submission2647/Reviewer_UaWP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2647/Reviewer_UaWP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987119813, "cdate": 1761987119813, "tmdate": 1762916318265, "mdate": 1762916318265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}