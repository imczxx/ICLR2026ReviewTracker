{"id": "omVhYvyTPJ", "number": 18164, "cdate": 1758284556143, "mdate": 1759897122345, "content": {"title": "LongRLVR: Long-Context Reinforcement Learning Requires Verifiable Context Rewards", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Large Language Models (LLMs) by optimizing them against factual outcomes. However, this paradigm falters in long-context scenarios, as its reliance on internal parametric knowledge is ill-suited for tasks requiring contextual grounding—the ability to find and reason over externally provided information. We identify a key reason for this failure: a reward based solely on the final answer is too sparse to effectively guide the model for identifying relevant evidence. We formally prove that the outcome-only reward leads to exponentially vanishing gradients for the context grounding process, rendering learning intractable. To overcome this bottleneck, we introduce LongRLVR to augment the sparse answer reward with a dense and verifiable context reward. This auxiliary signal directly incentivizes the model for selecting the correct grounding information, providing a robust learning gradient that solves the underlying optimization challenge. We validate our method on challenging long-context benchmarks using Qwen and LLaMA models. LongRLVR consistently and significantly outperforms the standard RLVR across all models and benchmarks, e.g., boosting a 14B model's scores on RULER-QA from 73.17 to 88.90 and on LongBench v2 from 39.8 to 46.5. Our work demonstrates that explicitly rewarding the grounding process is a critical and effective strategy for unlocking the full reasoning potential of LLMs in long-context applications.", "tldr": "RLVR for improving long-context capabilities.", "keywords": ["Large Language Models", "Long Context", "Reinforcement Learning with Verifiable Rewards"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b479ad8e566190e131eabd6fc5b873df9a1db35d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on improving long-context reasoning through Rule-based Rewards. They propose a new pipeline to generate data, thereby enabling the use of rule-based rewards to evaluate the evidence used in the LLM output.  Experiments show consistent gains over answer-only baselines, supporting the method’s effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem is significant, as RLVR may stimulate hallucinations and render the training process unstable, while its sparse rewards make effective exploration challenging in practice. \n\nThe paper addresses the issue of vanishing gradient in RLVR under sparse outcome-reward settings, examining its causes and implications.\n\nThe choice of the F1 score as a reward makes sense to me, since it balances precision and recall rather than encourages the model to cover the evidence as much as possible.\n\nThe experiments appear to support the authors’ claims and show consistent improvements."}, "weaknesses": {"value": "However, my concerns arose from the data generation pipeline and the usage of the verifier LLM. \n\n1. It seems that the method is only applicable for the Grounded QA, where evidence can be cleanly chunked. However, in such a case, performing rule-based rewards for the evidence suggestion should be straightforward. The usage of the F1-score is also straightforward to me, since recall encourages the policy to cover as many chunks as possible.\n2. A separate verifier LLM is used, which helps identify the evidence and check its alignments with the evidence library; however, it makes the comparison with RLVR unfair. Moreover, an additional LLM can do more (e.g., directly judge whether the final answer matches the reasoning path). Why not use semantic similarity or other similar metrics?\n3. The computational and human costs are nontrivial, both from data collection and the additional LLM verifier. Therefore, I am wondering: do the gains adequately justify the substantial supervision and computation?"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SfM6WaBT0E", "forum": "omVhYvyTPJ", "replyto": "omVhYvyTPJ", "signatures": ["ICLR.cc/2026/Conference/Submission18164/Reviewer_RAAP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18164/Reviewer_RAAP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919339196, "cdate": 1761919339196, "tmdate": 1762927916082, "mdate": 1762927916082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a fundamental limitation of RLVR when applied to long-context reasoning tasks. The authors identify that outcome-only rewards suffer from vanishing gradients for the contextual grounding process. They formally prove this vanishing gradient problem and propose LongRLVR, which augments sparse answer rewards with dense, verifiable context rewards that explicitly supervise evidence selection. The method is validated on RULER-QA, LongBench v2, and LongReason benchmarks, showing consistent improvement against vanilla GRPO and SFT. The approach requires ground-truth evidence annotations, and the authors also propose a data generation pipeline using clustering and rejection sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The formal analysis of why the outcome-only reward is insufficient for the long-context retrieval-based task provides some transferable insights.\n2. The modulated F-score reward, combining unconditional grounding reward and synergistic success reward, is thoughtfully designed.\n3. The paper provides extensive analysis on both synthetic and real-world long-context tasks, and the paper includes thorough ablations examining reward components, data quality, hyperparameters, and chunk number robustness, making the claims more sound."}, "weaknesses": {"value": "1. The comparison is a bit weak, which hinders the overall soundness of the work. Interleaving reasoning and retrieval is now becoming more popular. I would suggest comparing with some RAG baselines (which do not require RLVR but fit the same scenario), as well as some recent works like [1].\n2. Assumption 1 seems too strong for the analysis. In reality, the reward for retrieved evidence, if applied, should be more continuous than the 0 or 1 sparse reward. Also, the independence assumption for chunk selection might not be true since the evidence should be highly related in a multi-hop QA scenario like the ones in LongBench.\n3. A human evaluation of the validity of the generated data or some examples provided would be very beneficial.\n4. There is a potentially biased evaluation regarding the training data. The vanilla RLVR and SFT baselines are trained on the same generated data with evidence, but they don't have the corresponding training signal, which may introduce bias towards the proposed method.\n\n[1] Wang et al. 2025. Improving Context Fidelity via Native Retrieval-Augmented Reasoning. arXiv:2509.13683."}, "questions": {"value": "1. Would it be helpful if some existing QA datasets with ground-truth evidence (like HotpotQA) is used partially as the training data or as a seed dataset for the data generation process?\n2. Are the chunk identifiers ([CHUNK_N]) added as new special tokens?\n3. The useful chunks are generated after the thinking process. Does the model actually use or refer to the chunks during the training process?\n4. How does performance degrade with noisy evidence annotations?\n5. Can you evaluate on datasets with evidence annotations (e.g., HotpotQA) for the retrieval accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3StNCMbpSt", "forum": "omVhYvyTPJ", "replyto": "omVhYvyTPJ", "signatures": ["ICLR.cc/2026/Conference/Submission18164/Reviewer_fzK1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18164/Reviewer_fzK1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960869518, "cdate": 1761960869518, "tmdate": 1762927915567, "mdate": 1762927915567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LongRLVR, a reinforcement learning framework for long-context LLMs that introduces verifiable context rewards to overcome vanishing gradients in grounding. Instead of rewarding only final answers, LongRLVR adds dense rewards for correctly selecting evidence chunks, ensuring effective credit assignment across lengthy inputs. It decomposes the policy into grounding and answering heads, uses F-score–based context rewards, and achieves large gains on long-context QA benchmarks, outperforming outcome-only RL baselines and enabling smaller models to surpass larger ones."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Timely and impactful problem.**\nThe paper tackles a highly relevant and increasingly important issue — how to perform reinforcement learning effectively in long-context settings. As large-context reasoning becomes central to emerging LLM-based agents and search systems, addressing the credit-assignment and gradient-vanishing challenges identified here is both timely and of broad significance.\n2. **Strong motivation, clear formulation, and well-executed methodology.**\nThe study is well motivated and rigorously executed. It formally defines the reward-vanishing problem in long-context RL, provides theoretical analysis to explain why outcome-only rewards fail, and introduces a principled solution through verifiable context rewards. The inclusion of a synthetic yet well-controlled dataset allows precise testing, and the resulting performance gains over baselines are substantial and convincing.\n3. **Clear structure and presentation.**\nThe paper is clearly organized and well written, with intuitive explanations and consistent notation. The conceptual flow, from identifying the issue to formal analysis, method design, and empirical validation, is easy to follow, making the technical contributions accessible and well supported."}, "weaknesses": {"value": "1. **Strong theoretical assumptions.**\nThe analysis relies on several simplifying assumptions that may not fully hold in practice. In particular, it adopts an all-or-nothing reward assumption, where the answer reward increases only when the entire evidence set G is selected. In reality, LLMs often produce correct answers from partial or alternative evidence, making this assumption less realistic. Similarly, the independent Bernoulli selection assumption overlooks dependencies between evidence chunks—real policies typically select evidence in a correlated or sequential manner, which could alter the theoretical gradient behavior. It would strengthen the paper to discuss under what scenarios these assumptions are likely to hold (e.g., explicit fact-retrieval tasks) and where they may fail (e.g., open-domain reasoning). Such clarification would help readers understand the practical scope of the theoretical results.\n2. **Connection to broader long-context RL not explored.**\nThis is more like a suggestion. The paper could better relate its formulation and method to other long-context settings, such as agent RL, where the context includes environment state and action history (like adding a discussion section to appendix). Discussing which aspects of LongRLVR’s framework may transfer and which may not would improve generality and impact."}, "questions": {"value": "Some other gradient vanish work in LLM RL could be discussed in related work. e.g, \"Vanishing Gradients in Reinforcement Finetuning of Language Models\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "flLK8IwWkj", "forum": "omVhYvyTPJ", "replyto": "omVhYvyTPJ", "signatures": ["ICLR.cc/2026/Conference/Submission18164/Reviewer_kmU8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18164/Reviewer_kmU8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18164/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976567278, "cdate": 1761976567278, "tmdate": 1762927915056, "mdate": 1762927915056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}