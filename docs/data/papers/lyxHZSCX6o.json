{"id": "lyxHZSCX6o", "number": 25575, "cdate": 1758369228419, "mdate": 1759896714767, "content": {"title": "Curricular Adversarial Training for Robust Code Generation via Hierarchical Reinforcement Learning", "abstract": "In this paper, we propose a novel approach to boost the robustness of code generation models by curricular adversarial training driven by hierarchical reinforcement learning. Existing code generation systems are prone to breaks by adversarial perturbations, so we propose a two-tiered approach in which a high-level curriculum policy is used to adaptivelyChange complexity of adversarial challenges dynamically while a low-level perturbation policy will be used to generate specific input modifications. The high-level policy goes from simple to sophisticated perturbation based on model performance, which will ensure the gradient of adapting without overwhelming the generator too much.", "tldr": "", "keywords": ["Robust Code Generation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4efcab801bf6d8cd92d19c3e6927809778a1f342.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a hierarchical reinforcement learning (HRL) framework for curricular adversarial training in code generation. The method introduces two policies, i.e., a high-level policy that dynamically adjusts the complexity of adversarial perturbations, and a low-level policy that generates specific adversarial examples. The goal is to improve the robustness of large language models for code generation against adversarial perturbations. Experiments are conducted on HumanEval, APPS, and MBPP datasets using CodeGen-6B as the base model."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The general idea of combining curriculum learning with HRL for adversarial robustness is conceptually sound and has potential.\n\n2. The paper attempts to unify curriculum scheduling, hierarchical control, and adversarial training into a single framework."}, "weaknesses": {"value": "1. The manuscript suffers from many typos, grammatical mistakes, and formatting inconsistencies. The writing style lacks clarity and precision. Several sentences appear incomplete or poorly constructed (e.g., Section 1 and 4). The overall presentation resembles a coursework report rather than a polished conference paper.\n\n2. The paper does not clearly define the adversarial setting for code generation—what kinds of perturbations are used, how they are measured, and what “robustness” precisely means. The motivation for using hierarchical RL in this context is not well justified compared to simpler baselines.\n\n3. The experimental section is extremely limited. Only high-level summaries are given—no implementation details, hyperparameters, or reproducibility discussion. The results appear arbitrary and lack statistical analysis. There are no significance tests or comparisons beyond simple table metrics. Ablation studies and cross-dataset evaluations are superficial and not convincing."}, "questions": {"value": "This paper is not ready for publication. It reads more like a course project or exploratory report rather than a rigorously developed research paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "o6nVmnKeb9", "forum": "lyxHZSCX6o", "replyto": "lyxHZSCX6o", "signatures": ["ICLR.cc/2026/Conference/Submission25575/Reviewer_TNBK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25575/Reviewer_TNBK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914847181, "cdate": 1761914847181, "tmdate": 1762943481883, "mdate": 1762943481883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses curricular adversarial training and hierarchical reinforcement learning for code generation tasks. While the idea is sound, related works is good, and the overall structure for experiments seems solid, the paper seems a bit underdeveloped."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The underlying structure of the paper is sound. Code generation is an active area of research that is not yet fully solved. Adversarial curricular learning seems solid. Hierarchical RL also makes sense since most coding problem themselves are a form of hierarchical problem (class, function, etc). The algorithm was compared with some baselines as well."}, "weaknesses": {"value": "The paper up to introduction and related works seems OK, but the subsequent sections seems less developed. For example, it appears there's a typo in the Line 189 that looks like an LaTEX formatting issue. While bullet-pointing makes idea very clear, I think it would be better if there were more descriptions of the idea. For example, how exactly does the reward computation works here in section 4.5? How is robustness term calculated and how is it incorporated as? How are the gradient updates calculated and implemented? The figures are in curriculum stages, but how is it in terms of computational times or steps? There isn't enough details to more accurately assess the algorithm.\n\nThere are other stylistic issues as well. For example there's an typo at the abstract (adaptivelyChange). Text in figure 2 is too small to be legible. While Figure 2 surves as a good hierarchy, it might be nice to incorporate a pseudocode algorithm as well for a bit more clarity with nomenclature as well. Also, most LLM papers uses SFT as 'Supervised Fine-Tuning' so using SFT as 'Standard Fine-Tuning' may mislead some readers.Finally, In Figure 3, the porportion of perturbed types reaches higher than 1 at stage 1 throguh 3, below zero at stages 9 and 10, which seems like an visualization error."}, "questions": {"value": "This paper seems incomplete and should be rewritten."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q809G2hVvo", "forum": "lyxHZSCX6o", "replyto": "lyxHZSCX6o", "signatures": ["ICLR.cc/2026/Conference/Submission25575/Reviewer_8dGk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25575/Reviewer_8dGk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958487482, "cdate": 1761958487482, "tmdate": 1762943481575, "mdate": 1762943481575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The writing in this paper is poor and often difficult to follow, with numerous grammatical issues, incomplete sentences, missing definitions, and unclear explanations throughout. As a result, it is challenging to fully understand the proposed method, and the following summary reflects my interpretation to the best of my understanding. \n\nThis paper proposes a hierarchical reinforcement learning (HRL) framework for curricular adversarial training in code generation. A high-level policy controls the progression of adversarial perturbation complexity, while a low-level policy generates the specific perturbations."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- studies an important topic: improving the robustness of code generation models under adversarial perturbations."}, "weaknesses": {"value": "The paper is difficult to follow, with numerous grammatical errors, awkward phrasing, and undefined notations (e.g., Line 16, 50, 190, to name just a few). Several sections (e.g., Section 4) are nearly unreadable, containing undefined equations and terms, typos, and long, unstructured phrases that make the overall presentation very hard to read."}, "questions": {"value": "There are many things unclear in the current presentation. I think the paper will benefit from a complete rewriting to explain all aspects clearly. For example, how are the high-level and low-level policies trained jointly? How are adversarial perturbations for code defined and validated to maintain syntactic and semantic correctness? Can you provide concrete examples of adversarial perturbations at different curriculum stages? What is the rationale for using hierarchical RL, and how does it improve over simpler curriculum adversarial training baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BDqd64NMr3", "forum": "lyxHZSCX6o", "replyto": "lyxHZSCX6o", "signatures": ["ICLR.cc/2026/Conference/Submission25575/Reviewer_zv9A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25575/Reviewer_zv9A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111653511, "cdate": 1762111653511, "tmdate": 1762943481376, "mdate": 1762943481376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}