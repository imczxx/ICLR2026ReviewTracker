{"id": "0rlc34xAhz", "number": 17776, "cdate": 1758280429231, "mdate": 1759897154623, "content": {"title": "Efficient Multilabel Uncertainty Quantification with Conformal Ensembles", "abstract": "Multilabel classification (MLC) is challenging due to labels being often correlated and due to the highly complex decision boundaries. Moreover, uncertainty quantification, which helps address sparse label combinations, remains an area with significant room for further exploration. In many high-stakes domains, reliable predictions must not only be accurate but also quantify uncertainty to avoid missing critical cases. Conformal prediction (CP) offers distribution-free coverage guarantees, but when applied to individual models it can produce unstable or overly large prediction sets. Ensemble methods are a well-established approach to improve stability and efficiency, yet their potential in multilabel settings has not been fully explored. \nWe investigate ensemble conformal prediction for multilabel classification. Building on prior work on voting- and score-based ensembles, we adapt these strategies to the label-wise multilabel setting and conduct a systematic empirical study across multiple aggregation schemes: (i) majority voting, (ii) calibrated aggregation of nonconformity scores, and (iii) performance-weighted aggregation. The theoretical perspective frames independence assumptions and voting bounds in the multilabel ensemble setting, clarifying how coverage guarantees extend under majority voting.\nAcross standard MLC benchmarks (COCO, Yeast, Emotions), our ensembles consistently improve over single-model CP yielding more efficient prediction sets (smaller and more informative), while maintaining target coverage and achieving higher macro-F1 scores. We provide a systematic study of ensemble aggregation methods for conformal prediction in multilabel classification, combining theoretical perspective with a broad comparative evaluation.", "tldr": "", "keywords": ["Multilabel classification", "Conformal prediction", "Uncertainty estimation", "Ensembles", "Calibration", "Reliable AI"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f2a4f2d1693b79fb43f1b1f835ccdc684202b97.pdf", "supplementary_material": "/attachment/ffec26086a65013abadbde5bd18e12337db6194b.zip"}, "replies": [{"content": {"summary": {"value": "In many classification tasks, the target label is not just a single value from the output space but rather a collection of values. In such “multi-label” classification problems, there is no way yet to directly leverage ideas from conformal prediction to enable distribution-free uncertainty estimation. As a result, this paper proposes an approach to do multi-label conformal prediction, leveraging ideas from ensembling for improving predictive efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The general goal of tackling this coverage problem for multi-label problems does seem like a worthwhile contribution."}, "weaknesses": {"value": "The contributions of the paper as it currently stands does not seem to be significant enough to warrant publication. Solving the multi-label problem appears to have amounted to slightly modifying the standard score function of a single-label classification task to instead include *all* the positions where $y^{(j)} = 1$. The rest of the conformal pipeline seems as though it would “just work” from there. This then leads to the main question I have about this work: this proposed method appears as though it would work just the same for any choice of predictor (so long as this modified score is used). As a result, the ensembling idea seems completely independent of this whole problem: while I do not disagree that ensembling does seem like an idea that can likely improve the results, there appears to be nothing that had to be modified in the ensembling for this multi-label setting. I, therefore, do not see what the contributions of all the discussions around ensembling are, since they just appear to be the standard ensembling ideas applied to this newly proposed score."}, "questions": {"value": "1. Why was ensembling considered to be a core contribution of this paper? Was there any aspect of it that had to be tailored to the particular score or approach you were taking? \n\n2. It seems like Theorems 1 and 2 would just follow immediately from the previously established results of conformal ensemble voting: is there anything new that was required here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TsPq2Fns8Z", "forum": "0rlc34xAhz", "replyto": "0rlc34xAhz", "signatures": ["ICLR.cc/2026/Conference/Submission17776/Reviewer_EqKt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17776/Reviewer_EqKt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760919474752, "cdate": 1760919474752, "tmdate": 1762927617098, "mdate": 1762927617098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses uncertainty quantification in multilabel classification (MLC) using conformal prediction (CP) enhanced by ensemble methods. It extends prior ensemble-CP techniques (e.g., majority voting and score aggregation) from single-label to multilabel settings, providing label-wise theoretical coverage guarantees under independence assumptions. The authors propose a framework integrating homogeneous, heterogeneous, and stacked ensembles with CP, and evaluate it on different benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper bridges a gap in the literature combining a multilabel setting and ensemble methods.  \nExperimental results are extensive, demonstrating the performance on diverse datasets."}, "weaknesses": {"value": "The novelty of the proposed method appears limited, as it primarily adapts existing ensemble techniques to the multilabel classification setting without introducing significant new theoretical contributions. Moreover, the paper lacks formal proofs to support its claims, and the main results (Table 1)."}, "questions": {"value": "Please address the issues outlined above, as well as the additional questions listed below.\n\n1. How is Theorem 2 different from Theorem 1?\n2. Could you provide detailed proofs to Theorems?\n2. Are the proposed methods supposed to preserve overall coverage or per-label coverage? How is it reflected in the results in Table 1?\n3. Can you explain the reason for the missing entries in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CZMKiIFNPO", "forum": "0rlc34xAhz", "replyto": "0rlc34xAhz", "signatures": ["ICLR.cc/2026/Conference/Submission17776/Reviewer_YZog"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17776/Reviewer_YZog"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826477694, "cdate": 1761826477694, "tmdate": 1762927616366, "mdate": 1762927616366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies ensemble conformal prediction (ECP) for multilabel classification, aiming to improve uncertainty quantification efficiency and stability compared to single-model conformal predictors. The authors extend existing ensemble-CP formulations (majority voting, score averaging, performance-weighted fusion) to the multilabel setting and provide label-wise coverage guarantees under independence assumptions. Experiments demonstrate that ECP achieves smaller prediction sets and higher macro-F1 scores while maintaining valid coverage. The paper also includes ablation studies on ensemble size, model diversity, and miscoverage rate."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and clearly structured, providing a comprehensive empirical evaluation of ensemble-based CP in multilabel settings.\n2. The motivation, improving the stability and efficiency of conformal prediction, is practically meaningful.\n3. The inclusion of heterogeneous ensembles and stacking broadens the empirical relevance of the work."}, "weaknesses": {"value": "1. The theoretical results (Lemmas and Theorems) are direct restatements of Cherubin (2019) and Gasparin & Ramdas (2024), simply applied label-wise. There is no new theoretical insight into multilabel dependencies, efficiency, or calibration behavior.\n2. All guarantees assume model independence, which does not hold for ensembles trained on shared data or features (especially with CLIP embeddings). Thus, the theoretical coverage bounds do not meaningfully explain empirical results.\n3. The introduction emphasizes modeling label dependencies, yet the proposed method remains label-wise independent (binary relevance). The framework therefore does not actually address the claimed “inter-label correlation” problem.\n4. The paper omits discussion of key prior multilabel CP frameworks, most notably [1]. That work provides one of the foundational theoretical treatments of valid confidence sets in the multilabel setting, including a formal coverage–efficiency trade-off. Without acknowledging or contrasting it, the contribution and novelty of the present paper remain unclear.\n\n[1] Cauchois, M., Gupta, S., & Duchi, J. C. (2021). Knowing what you know: valid and validated confidence sets in multiclass and multilabel prediction. Journal of machine learning research, 22(81), 1-42."}, "questions": {"value": "1. How can the proposed framework genuinely capture or leverage inter-label dependencies, given that each label is still calibrated independently?\n2. Could you provide formal results or analysis on how ensemble diversity or correlation affects prediction set efficiency or coverage?\n3. How does your approach relate to prior multilabel conformal prediction theory, such as [1]? Does your ensemble formulation extend their coverage–efficiency framework, or does it rely on fundamentally different assumptions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M6DTEq65eD", "forum": "0rlc34xAhz", "replyto": "0rlc34xAhz", "signatures": ["ICLR.cc/2026/Conference/Submission17776/Reviewer_ALtB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17776/Reviewer_ALtB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937080811, "cdate": 1761937080811, "tmdate": 1762927615739, "mdate": 1762927615739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how to get reliable uncertainty for multilabel classification by using conformal prediction with ensembles. The core idea is simple. Train several multilabel models, calibrate each one label by label with conformal prediction, then combine them with rules like majority vote, averaging of scores, or weights based on validation F1. The paper gives label wise coverage bounds for these ensemble rules and then tests them on three benchmarks: COCO, Yeast, and Emotions. The main claim is that ensemble conformal prediction keeps target coverage while producing smaller and more useful prediction sets and better macro F1 than a single conformalized model or a post hoc conformalized ensemble."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Adapting several known ensemble ideas to the multilabel conformal setting and treating each label with its own calibration and aggregation is a clean and useful framing. The paper is not only about majority voting. It also brings in averaged nonconformity scores and performance weighted rules and sets them in one framework. \n\nThe method section is concrete. It states the label wise calibration rule, the way thresholds are computed, and how a label enters the prediction set. Theoretical parts give binomial style lower bounds for voting under independence and discuss dependence. The experiments cover three datasets with different label counts and show metrics that matter in practice such as empirical and marginal coverage, average set size, and macro F1. The tables and ablations make the trade offs visible and the gains look consistent. \n\n\nThe paper is easy to follow. It explains the multilabel setup, the ensemble types, and the conformal steps in plain terms. The voting bounds are stated with interpretation and the practical message is clear. The training setup is fully specified, which helps with reproducibility. \n\n\nReliable uncertainty for multilabel problems is important in many areas. A drop in set size at the same target coverage is valuable for human in the loop use. Showing that simple ensembles bring that benefit while keeping coverage can help more people adopt conformal prediction beyond single model baselines. The results on COCO where label space is larger make the case stronger."}, "weaknesses": {"value": "The theory section leans on independence across ensemble members and gives binomial tails. In real ensembles the members are often correlated because they share data and features. The paper acknowledges this but does not try to bound coverage under explicit dependence structures. A simple extension with a beta binomial style lower bound or an empirical calibration of the bound would make the claims tighter. \n\nThe experimental baselines include single model conformal prediction and post hoc conformalized ensembles. It would be good to add stronger multilabel baselines that capture label dependence without ensembles, for example classifier chains with conformal calibration or recent multilabel calibrated models, even if they do not have the same guarantee. This would help isolate how much of the gain comes from ensembling rather than just better modeling of label ties. \n\nEmpirical coverage is defined at the example level and marginal coverage is averaged over labels. These are reasonable. Still, multilabel users often care about recall of rare labels or coverage curves by label frequency. The paper reports macro F1 but does not dig into tail labels. A figure with per label coverage distribution or coverage as a function of prevalence would make the case stronger for imbalanced regimes. \n\nThe comparison shows that calibrating each member then aggregating usually beats aggregating then calibrating. It would help to explain when the reverse order might be better. For instance, when the meta learner in stacking is very strong, can post hoc conformalization close the gap. A small controlled study would clarify the design choice. \n\nThe paper lists one machine and gives a short runtime note in the appendix. A more detailed look at cost as a function of labels and ensemble size would help practitioners. In particular, label wise calibration across many labels can be heavy. Some sharing or batching tricks could be discussed. \n\nTable entries suggest some methods hit perfect coverage with very large sets. It would help to state the target miscoverage value used across all results and whether any methods drift above or below the target. A small calibration plot would make this transparent."}, "questions": {"value": "Could you add an empirical check that estimates effective correlation among ensemble members and overlays observed ensemble coverage against the binomial lower bound. Even a simple plot would help users judge safety margins. \n\nPlease add plots of marginal coverage by label frequency bucket and a cumulative view of per label coverage. This would make the value for rare labels more concrete and would complement macro F1. \n\nFor stacking, what happens if you train the meta learner first on probabilities and then apply conformal thresholds on the meta output versus conformalizing members first and then stacking. A head to head on one dataset would clarify which order to prefer. \n\nYou study majority voting, averaging, and F1 weighted rules. Could you add a small guide for when to use each rule in terms of data size, diversity, and calibration quality of members. A short decision chart would help adoption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p4XrAldtDs", "forum": "0rlc34xAhz", "replyto": "0rlc34xAhz", "signatures": ["ICLR.cc/2026/Conference/Submission17776/Reviewer_EoQK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17776/Reviewer_EoQK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762024877831, "cdate": 1762024877831, "tmdate": 1762927615198, "mdate": 1762927615198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}