{"id": "Ha1Z42wHqz", "number": 9527, "cdate": 1758126162606, "mdate": 1759897714237, "content": {"title": "Uncovering Activation Keys in the Dark: Revealing Learned Concepts in LoRA Text-To-Image Models", "abstract": "Low-Rank Adaptation (LoRA) has become a widely adopted technique for customizing large diffusion models, enabling users to inject new styles, characters, or identities into text-to-image generation with minimal computational cost. While this flexibility fuels creative expression, it also opens the door for injecting sensitive or potentially harmful content, such as political figures’ faces, copyrighted characters, or explicit imagery, into generative models. These LoRA adapters are often distributed without documentation, making it difficult to identify the concepts they encode or understand how they are triggered. This lack of transparency poses serious challenges for moderation, accountability, and large-scale content auditing in open-source model ecosystems. To address this risk, we adopt the role of a model investigator and introduce the LoRA ``activation key'' discovery problem: given a suspect LoRA and its base model, identify a text embedding that reliably activates behaviors unique to the LoRA. This activation key serves as a forensic probe to reveal hidden concepts introduced during fine-tuning. To achieve so, we propose a two-stage optimization framework. We first perform an evolutionary search in the token space to identify promising candidate prompts, followed by gradient-based refinement in the embedding space. Our objective encourages the LoRA model to generate concentrated outputs while maximizing divergence from the base model, resulting in an embedding that reveals distinct LoRA-specific behaviors. Experiments on six public LoRA adapters show that our method recovers ground-truth concepts in both white-box and black-box settings. Our work demonstrates the feasibility of LoRA forensics and highlights the need for auditing tools in open-source model ecosystems.", "tldr": "This work introduces a forensic method to uncover hidden “activation keys” in LoRA adapters by combining evolutionary search and gradient refinement, enabling reliable detection of LoRA-specific concepts for auditing and accountability.", "keywords": ["Text-to-image diffusion", "LoRA adapters", "Concept discovery", "Model forensics", "Prompt optimization", "Visual-semantic analysis", "AI accountability", "AI security", "Evolutionary search", "Gradient-based refinement"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b97aa2f91a8b95cb4ddaccaeb98947009aef708.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of uncovering hidden concepts in LoRA-fine-tuned diffusion models. It introduces a contrastive objective with three loss terms—intra-LoRA consistency, intra-base diversity, and inter-model dissimilarity—and a two-stage optimization framework to recover activation keys. Stage 1 conducts a black-box evolutionary search over discrete prompts, while Stage 2 refines the discovered embedding through gradient ascent. Experiments on several public LoRA adapters with Stable Diffusion 1.5 / SDXL show that the method performs effectively within the tested settings."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I genuinely like the paper’s methodological design — it is well-motivated, logically structured, and technically sound.\n\n1. The paper is clearly written and easy to follow, with well-organized sections and informative visualizations.\n\n2. The three-term objective (intra-LoRA consistency, intra-base diversity, and inter-model dissimilarity) is intuitive, elegant, and well grounded in the goal of contrasting LoRA and base behaviors.\n\n3. The two-stage design—evolutionary search followed by gradient-based refinement—is a smart and practical solution to the difficulty of optimizing discrete prompts."}, "weaknesses": {"value": "While the paper is well written and conceptually solid, the evaluation part feels somewhat limited.\n\n1. The experimental scale is relatively small—only six LoRA adapters and two base models (SD 1.5 and SDXL). Expanding to more diverse concepts or datasets, such as the DreamBooth dataset (might need to incorporate some automatic captioning process) or larger community LoRA collections, would better demonstrate robustness and generality.\n\n2. The current comparison baseline (a random or heuristic prompt) is weak. Incorporating stronger baselines such as prompt inversion or optimization methods  would make the empirical claims more convincing.\n\n3. The three loss components are central to the method, yet no ablation study or sensitivity analysis is provided. Evaluating the effect of removing or reweighting each term would clarify their relative importance.\n\n4. It would be valuable to test the approach on newer diffusion architectures (e.g., SD 3 or Flux) to assess whether the mechanism generalizes.\n\nOverall, the proposed method is insightful, and if its with stronger empirical validation, I would lean toward acceptance."}, "questions": {"value": "Apart from the weaknesses mentioned above, I also wonder:\n\n\nCould the proposed framework be extended to more general fine-tuning settings, such as full-parameter DreamBooth?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mfdqSKxMK6", "forum": "Ha1Z42wHqz", "replyto": "Ha1Z42wHqz", "signatures": ["ICLR.cc/2026/Conference/Submission9527/Reviewer_ffWZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9527/Reviewer_ffWZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760649947213, "cdate": 1760649947213, "tmdate": 1762921093407, "mdate": 1762921093407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the **LoRA activation key discovery problem**, a novel forensic approach for identifying hidden or undocumented concepts within T2I LoRA. Motivated by concerns over moderation and accountability in open-source generative ecosystems, the authors propose a **two-stage optimization framework** consisting of (1) **evolutionary search** in token space and (2) **gradient-based refinement** in embedding space. The objective maximizes behavioral divergence between the LoRA and base models using CLIP-based inter/intra-model similarity measures. Experiments on six public LoRA adapters show that the method effectively recovers ground-truth triggers and reveals distinct LoRA-specific behaviors, verified quantitatively (TrigSim, CapSim, CMMD) and semantically via VLM analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Novelty:** Defines the previously unexplored area of activation key discovery in LoRA models.\n\n- **Technical Depth:** Well-founded objective and two-stage optimization pipeline, combining discrete and continuous strategies.\n\n- **Empirical Breadth:** Evaluation on multiple LoRAs with robust metrics and semantic validation.\n\n- **Relevance:** Addresses growing safety, transparency, and forensic auditing challenges in T2I ecosystem.\n\n- **Presentation:** Strong writing, clear figures, and reproducible experiments."}, "weaknesses": {"value": "- **Scalability:** The procedure’s computational cost and time may hinder large-scale deployment.\n\n- **Limited Soundness:** The author present that \"Our approach is model-agnostic and applicable in both white-box and black-box settings\" while actually the proposed method is a two-stage method that relies on the initialization of a black-box token-level optimization for a white-box optimization, which is a white-box method. Results in Fig.5 and Fig.6 are not enough to demonstrate the proposed method is effective in white-box and black-box scenarios (Fig.6 should be organized as only stage1, only stage2 with random initialzed embedding and stage1+stage2 to demonstrate the improvement). \n\n- **Confusing Objectives:** The objective function of \"Consistency within the LoRA\", \"Diversity within the base model\" and \"Discrepancy across models\" is confusing without any rationale. First, some LoRAs are highly semantic-align with their corresponding trigger words which means that the LoRAs can actually replace by a prompt suffix, disaligning with the objective \"Diversity within the base model\". Second, why should there be \"Discrepancy across models\" if the LoRAs themselves do not introduce large semantic change to the base LoRA. There are all kinds of LoRAs, some of which might only be used for fine-grained adjustments. Third, \"Consistency within the LoRA\" is also not grounded, since for style-based LoRAs, CLIP might not capable to extract these features. I recommand the author clarify the audit scope (what types of LoRA) of their method.\n\n- **Ablation Study:** There are no ablation studies for the proposed three objectives and I wonder if they really contribute to the optimization for the above mentioned scenarios.\n\n- **Lack of Multi-task Discovering:** An adaptive attacker can easy design a multi-task LoRAs evasion attack by hidden a malicous task with complex trigger words into a benign task with simple trigger words. The benign task acts as a trapdoor to hijack you optimization."}, "questions": {"value": "Refer to **weakness**. The proposed scenario for LoRA auditing is promising and if the author can address some of my concerns above, I am willing to raise my rating anytime."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1l5qCtl7B6", "forum": "Ha1Z42wHqz", "replyto": "Ha1Z42wHqz", "signatures": ["ICLR.cc/2026/Conference/Submission9527/Reviewer_BCTv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9527/Reviewer_BCTv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761464946972, "cdate": 1761464946972, "tmdate": 1762921093026, "mdate": 1762921093026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses LoRA adapter auditing by discovering \"activation keys\", a concept defined for text embeddings that expose hidden concepts in fine-tuned diffusion models. A two-stage framework is proposed, combining evolutionary search over discrete tokens (stage 1) with gradient-based refinement in continuous embedding space (stage 2). The objective function balances intra-model dispersion (LoRA outputs should be consistent) against inter-model similarity (LoRA should differ from base model). Experiments on six publicly available LoRAs are solid, demonstrating successful concept recovery, though with notable computational costs and ~22% failure rate.\nThis work aligns well with ICLR's core themes. Model auditing and interpretability are increasingly critical as generative models proliferate across the fields. The paper combines optimization theory, computer vision, and ML security in ways that should interest the ICLR community. The focus on LoRA adapters is particularly timely given their widespread deployment as a powerful PEFT method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Important Problem: Auditing undocumented LoRA adapters matters. The community shares countless fine-tuned models, and yet many are without proper documentation. Having systematic ways to discover what they encode is genuinely useful.\n2. Clean Problem Formulation: The activation key concept is well-defined in a straightforward manner. The objective function combining intra-model dispersion and inter-model similarity is intuitive and principled.\n3. Practical Two-Stage Design: Starting with evolutionary search for coarse exploration, then refining with gradients makes sense. Stage 1 works in black-box settings; Stage 2 works with white-box access when available. This flexibility could be valuable.\n4. Solid Empirical Validation: Testing on both stylistic and identity-based LoRAs shows generality. The quantitative metrics (CMMD, CLIP similarity) combined with qualitative VLM analysis provide multiple perspectives."}, "weaknesses": {"value": "1. Objective Function Lacks Theoretical Grounding: Why should maximizing intra-LoRA dispersion while minimizing inter-model similarity necessarily recover the true concept? The paper doesn't provide a thorough theoretical justification. What guarantees exist that this objective aligns with finding semantically meaningful triggers rather than adversarial perturbations?\n2. Limited Baseline Comparisons: The random prompt baseline is not a very strong case to be based upon. Why not compare against existing LoRA auditing methods mentioned in related work (Yao 2024's weight leakage, membership inference approaches)? Without stronger baselines, it's hard to assess whether the complexity of the two-stage framework is justified.\n3. Computational Cost Is Prohibitive: Each experiment requires generating hundreds to thousands of images. On an A100, Stage 1 needs roughly 10×n×G images, Stage 2 adds about 1500 more. For practical auditing at scale, this cost seems problematic. The authors mention parallelization could help, but don't provide concrete timing comparisons or discuss computational efficiency as a design consideration.\n4. Failure Rate Concerns: Combined ~22% failure rate across stages is perhaps not an ignorable number. The explanation attributes this to \"initial random seeds in Stage 1,\" but this seems fixable. Why not run multiple Stage 1 initializations in parallel and select the best? The dependence on good initialization suggests the objective landscape could be better understood/designed.\n5. Limited Discussion of False Positives: Can this method be fooled? What if someone deliberately creates a LoRA that appears benign under this auditing approach? The adversarial robustness of the framework isn't explored.\n6. Evaluation Metrics Could Be Stronger: CMMD and CLIP similarity are reasonable but indirect. For identity-based LoRAs, why not consider adding face verification type of scores? For style LoRAs, perceptual metrics like LPIPS might be more appropriate. The VLM captioning is interesting, but perhaps can only serve as a qualitative (subjective) assistant judge."}, "questions": {"value": "1. Can you provide a theoretical analysis showing your objective function provably recovers ground-truth concepts under reasonable assumptions?\n2. What happens if you run multiple Stage 1 initializations? Does this reduce the failure rate proportionally, or are some LoRAs fundamentally harder to audit?\n3. Have you considered testing adversarial scenarios where someone actively tries to evade your auditing method?\n4. The objective function feels somewhat ad hoc. The authors don't justify the specific formulation of these terms beyond intuition. Why is Euclidean distance in CLIP space the right metric? Have you tried other divergence measures? Furthermore, the weighting parameters α and β are mentioned, but their values aren't specified. How sensitive are results to these choices? Ablation studies would strengthen this section. In addition, can you characterize when your objective succeeds? Even a toy model showing why dispersion+dissimilarity recovers concepts would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The authors do not explicitly discuss the ethical risks of the proposed method. Can an attacker utilize the method for an advanced attack or steal the functionality of the existing LoRA model?"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q51z4pEh9e", "forum": "Ha1Z42wHqz", "replyto": "Ha1Z42wHqz", "signatures": ["ICLR.cc/2026/Conference/Submission9527/Reviewer_qVw3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9527/Reviewer_qVw3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761520212496, "cdate": 1761520212496, "tmdate": 1762921092516, "mdate": 1762921092516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the auditing challenges of undocumented LoRA adapters in text-to-image diffusion models, which can be used to inject sensitive or harmful concepts without disclosure. The authors introduce Activation Key, which is generated through a two-stage search framework, to reveal the concept only embedded in the LoRA adapter. Experiments on six public LoRA adapters show that the method effectively recovers the hidden concepts. Besides, the authors visualize the optimization trajectory with t-SNE, demonstrating how their method progressively creates a clear separation between the LoRA and base diffusion model."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ Important research motivation.\n+ Reasonable design of the auditing method."}, "weaknesses": {"value": "- Poor writing and unclear methodology introduction.\n- White-box method design is inconsistent with contributions.\n- Insufficient experiment settings and evaluation."}, "questions": {"value": "1. According to “Related Work”, one of your contributions is that the method is applicable in both white-box and black-box settings, while the stage-2 needs white-box access as demonstrated in subsec 6.2.2.\n\n2. Many details of your method are difficult to understand and lack thorough introductions. What is the target to maintain the diversity within the base model in the training objective? How to achieve the token-level score $s_t$ of the prompt text based on $f(p)$, that is generated by image embedding?\n\n3. The writing employs some non-standard terms and contains several incomplete sentences. For example, you use “spread” to denote $S_M$ without a detailed explanation; The second sentence in subsect 5.2 ends suddenly and unnaturally.\n\n4. The experimental design lacks persuasiveness. You implement experiments only in six LoRA adapters without the large-scale datasets evaluation. Besides, the baselines are too limited and the effectiveness of “random prompt” is too weak as the baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BL6FABebzn", "forum": "Ha1Z42wHqz", "replyto": "Ha1Z42wHqz", "signatures": ["ICLR.cc/2026/Conference/Submission9527/Reviewer_i8Ai"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9527/Reviewer_i8Ai"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793142783, "cdate": 1761793142783, "tmdate": 1762921092155, "mdate": 1762921092155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the critical and underexplored problem of auditing undocumented Low-Rank Adaptation (LoRA) models. The authors formalize this as the \"LoRA activation key discovery\" task, where the goal is to find a text embedding that reliably triggers a LoRA's specific, fine-tuned behavior while remaining inert for the base model. To solve this, they propose a two-stage optimization framework: a black-box evolutionary search to find a promising initial prompt, followed by a white-box, gradient-based refinement of its embedding. The search is guided by a novel objective function designed to maximize the behavioral divergence between the LoRA and its base model. Experiments on six publicly available LoRA models show that the method can successfully recover the intended concepts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Important Problem Formulation: The paper identifies and formalizes a critical, real-world problem concerning the safety, accountability, and auditing of community-shared generative models. This is a significant contribution to the responsible AI ecosystem.\n\n2. Elegant Objective Function: The objective function, based on maximizing intra-LoRA consistency while minimizing inter-model similarity, is a principled and intelligent way to define the desired characteristics of an \"activation key.\"\n\n3. Principled Hybrid Search: The two-stage framework, combining evolutionary search for broad exploration and gradient-based methods for fine-tuning, is a strong and logical approach to the complex, hybrid search space."}, "weaknesses": {"value": "1. Crucial Mismatch in Experimental Validation: The most significant weakness is that the experiments do not validate the method's utility for its stated purpose. The paper claims to uncover concepts \"in the dark,\" but it is only tested on LoRAs with publicly known, non-adversarial triggers. This fails to demonstrate that the method can handle intentionally obfuscated, non-semantic, or compositional triggers that would be used in malicious LoRAs.\n\n2. Insufficient Experimental Scale: The evaluation is conducted on only six LoRA models. While diverse in type, this small sample size is insufficient to make strong claims about the method's generalizability across the vast and heterogeneous landscape of community-trained LoRAs.\n\n3. Practicality and Scalability Concerns: The proposed method, particularly the evolutionary search stage, is computationally expensive, requiring thousands of model inferences to audit a single LoRA. This raises serious questions about its feasibility for deployment at the scale required by model-sharing platforms."}, "questions": {"value": "1. The core claim of the paper is to uncover hidden concepts, but the experiments were performed on LoRAs with public, semantically meaningful triggers. Could the authors provide evidence of their method's performance on a LoRA trained with a deliberately obfuscated or non-semantic trigger (e.g., a random string) to better support the central thesis?\n\n2. Given the high computational cost, how do the authors envision this framework being practically deployed for large-scale auditing of thousands of models? Are there opportunities to significantly reduce the cost of the Stage 1 search?\n\n3. The optimal performance relies on Stage 2, which requires white-box access. In a more realistic black-box (API-only) auditing scenario, what is the performance of the Stage 1 evolutionary search alone, and is it sufficient for reliable concept discovery?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kM7jEMVyyC", "forum": "Ha1Z42wHqz", "replyto": "Ha1Z42wHqz", "signatures": ["ICLR.cc/2026/Conference/Submission9527/Reviewer_h4uh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9527/Reviewer_h4uh"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884336226, "cdate": 1761884336226, "tmdate": 1762921091839, "mdate": 1762921091839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}