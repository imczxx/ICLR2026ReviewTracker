{"id": "rbt3sFHuHm", "number": 3759, "cdate": 1757514950632, "mdate": 1763103127466, "content": {"title": "Native 3D Editing with Full Attention", "abstract": "Instruction-guided 3D editing is a rapidly emerging field with the potential to broaden access to 3D content creation. However, existing methods face critical limitations: optimization-based approaches are prohibitively slow, while feed-forward approaches relying on multi-view 2D editing often suffer from inconsistent geometry and degraded visual quality. To address these issues, we propose a novel native 3D editing framework that directly manipulates 3D representations in a single, efficient feed-forward pass. Specifically, we create a large-scale, multi-modal dataset for instruction-guided 3D editing, covering diverse addition, deletion, and modification tasks. This dataset is meticulously curated to ensure that edited objects faithfully adhere to the instructional changes while preserving the consistency of unedited regions with the source object. Building upon this dataset, we explore two distinct conditioning strategies for our model: a conventional cross-attention mechanism and a novel 3D token concatenation approach. Our results demonstrate that token concatenation is more parameter-efficient and achieves superior performance. Extensive evaluations show that our method outperforms existing 2D-lifting approaches, setting a new benchmark in generation quality, 3D consistency, and instruction fidelity.", "tldr": "instruction o native 3d editing that directly manipulates 3D latent in 3d space without any need for multi-view editing", "keywords": ["Native 3D edtiing", "3D editing", "3D generative model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d744f559e9a5827a5c2d4b7272242664c4ffd8e8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies 3D scene editing. The authors first curated a new dataset of 3D editing triplets to train the model. Then, the authors repurpose the TRELLIS model with module injection and fine-tuning to build their DiT model. The proposed method outperforms the very selective baselines."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The high-level ideas of the method are very easy to understand.\n- Fig.2 is visually nice.\n- A 3D editing dataset is proposed for training."}, "weaknesses": {"value": "- **Crucial.** There is no video or multi-view presentation of the results. **My reviewing score will be 0 without these representations.**\n    - Each result is only represented in a single-view image, which does not at all show clues about 3D consistency or global appearance.   \n    - 3D editing is a task highly dependent on qualitative results. All the quantitative metrics evaluate aspects that are only tangentially related, without considering the crucial aspects of 3D consistency and visual appearance. \n    - These presentations cannot convince me at all - it is even possible that all the results are actually in 2D instead of 3D. \n- **Crucial.** The baseline is very selective; all SDS-based baselines are missing. **My reviewing score will be <2 without these baselines.**\n    - The only compared baselines are \"Voxhammer, Instand3dit, Tailor3D, Hunyan3D-2.1, TRELLIS\". All other baselines discussed in related works, including all SDS-based ones like Instruct-NeRF2NeRF, Vox-E, Tip-Editor, DreamEditor, and FocalDreamer, are not compared without a valid reason.\n    - Being feed-forward does not waive the duty to compare SDS baselines, as the task is the general \"3D editing\", which is independent of the specific method. Similarly, a ViT-based image classification paper cannot waive the duty to compare against CNN-based classification models if those are potentially better. \n- The presentation of the method is very unprofessional.\n    - In Sec. 3.3, the cross-attn strategy is introduced before token-cat, but in Fig.2, token-cat is on the left, and cross-attn is on the right.\n    - The formulas (2)-(8) in Sec.. 3.3 are all trivial and uninformative definitions, e.g., meaninglessly repeating the mathematical definition of attention on different groups of QKVs. Removing them will make the paper shorter without any confusion.\n    - L248 \"Cross-Attention strategy\" is introduced as \"our first attempt\". This is not a suitable introduction in a formal paper, which should only appear as a \"proof of work\" in a course project.\n    - In fact, everything about \"Cross-Attention strategy\", including the related ablation study, can be removed as it is not a part of the final design.\n- The training dataset largely depends on an existing text-to-3D model \"Hunyuan3D 2.1\". \n    - This makes the model somehow distilled from, and therefore bounded by, the source model Hunyuan3D 2.1, which may limit its capability. \n    - Also, the generation of \"a pair of 3D objects\" is not mentioned using any controls to make sure the two 3D objects are similar and only differ in edited parts. Manual filtering is unable to ensure that the unedited parts are identical.\n- **Crucial.** Because of the previous point, the results have many expected artifacts in \"modify & add\":\n    - In Fig.3, \"add signboard on pole\" provides a completely different pole, which should be regarded as a failure.\n    - In Fig.4, \"put a vase on the table\"'s table has a very different shape, \"open lid\"'s frontal textures are unexpectedly changed, and \"add internal shelves\" is even difficult to understand which part should be edited.\n    - Even with video results, these artifacts are still not expected to be seen in the final output.\n- The idea to use full attention instead of additional cross-attention for conditioning is a very common idea, instead of a novelly proposed one. Most of the modern DiT-based image and video diffusion models even compute full attention among tokens from both videos/images and texts.\n- Other than TRELLIS repurposing, there are many missing necessary implementation details, including:\n    - The way of \"filtering for 3D assets with well-defined, hierarchical part structures\".\n    - The prompts to call MLLM to obtain the description of the object and the removed components.\n    - The detailed criterion of the curation protocol in L210.\n- There are many typos and grammatical mistakes. For example:\n   - TL;DR: \"instruction o **(of)** native 3d editing that directly manipulates 3D latent in 3d space without any need for multi-view editing.\"\n   - L111. \"For example, Instruct-NeRF2NeRF (Haque et al., 2023) iterative **(iteratively)** edits NeRF...\"\n   - Inconsistent letter cases: L248 \"Cross-**A**ttention **s**trategy\" v.s. L291 \"Token-**c**at **s**trategy\"."}, "questions": {"value": "- Could you please provide a video or multi-view (at least 4 views) results of all the edited scenes?\n- Could you please compare with some state-of-the-art SDS-based methods?\n- When generating triplets using Hunyuan3D 2.1, did you use any similarity control method like DiffEdit to systematically ensure that the two generated 3D scenes are identical in unedited parts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "67bVGrlhKX", "forum": "rbt3sFHuHm", "replyto": "rbt3sFHuHm", "signatures": ["ICLR.cc/2026/Conference/Submission3759/Reviewer_eqKL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3759/Reviewer_eqKL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718674897, "cdate": 1761718674897, "tmdate": 1762916970560, "mdate": 1762916970560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "MtRgWNyfWm", "forum": "rbt3sFHuHm", "replyto": "rbt3sFHuHm", "signatures": ["ICLR.cc/2026/Conference/Submission3759/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3759/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763103126665, "cdate": 1763103126665, "tmdate": 1763103126665, "mdate": 1763103126665, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for instruction-guided 3D object editing that operates natively in 3D space via a single feed-forward pass, avoiding the inconsistencies and inefficiencies of prior multi-view 2D editing pipelines. The authors create efficient data generation pipeline based on current large generative models. They explore two conditioning strategies—cross-attention and 3D token concatenation—demonstrating that the latter is more parameter-efficient and yields superior results. Some qualitative and quantitative results are provided."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Dataset Contribution**: The primary strength of this paper is the introduction of the large-scale dataset specifically for instruction-guided native 3D editing.\n- **Extensive evaluation results**: The qualitative results shown in Figures 3 and 4 illustrate the method's superiority in maintaining consistency and following instructions."}, "weaknesses": {"value": "- **Limited Methodological Novelty**: While the overall framework is effective, its methodological novelty is somewhat limited. The backbone architecture is an existing pre-trained model (TRELLIS). The core \"novel\" contribution, the 3D token concatenation strategy, is a well-established technique for conditioning in other generative domains (e.g., 2D inpainting and image editing). While the authors claim to be the first to apply this to the 3D domain, this is more of a successful adaptation than a fundamental architectural innovation.\n- **Reliance on Manual Curation**: The quality of the \"addition\" and \"modification\" portions of the dataset hinges on a \"rigorous manual curation process\". This introduces concerns about scalability, reproducibility, and potential human biases in the filtering criteria. The paper notes that low-quality or inconsistent pairs are discarded, but it does not quantify this, making it difficult to assess the true difficulty of the 2D-to-3D lifting task or the amount of human effort required."}, "questions": {"value": "- The ablation study in Section 4.4 and Figure 5(a) provides a clear qualitative comparison between the token concatenation and cross-attention strategies, showing the former is superior. The paper also claims token concatenation is \"more parameter-efficient\". Could you please quantify this claim? A small table comparing the number of additional trainable parameters required to adapt the TRELLIS backbone for the cross-attention strategy (i.e., the new attention layers ) versus the token-concatenation strategy would significantly strengthen this argument.\n- Regarding the manual curation for the ADD & MODIFY dataset: What was the rejection rate? Specifically, what percentage of the initial 3D pairs generated by Hunyuan3D 2.1 were discarded for failing to meet the three criteria (instruction fidelity, consistency, and quality)? This information is crucial for understanding the dataset's construction and the current limitations of 2D-to-3D lifting models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pqT9CDfUdT", "forum": "rbt3sFHuHm", "replyto": "rbt3sFHuHm", "signatures": ["ICLR.cc/2026/Conference/Submission3759/Reviewer_EdJn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3759/Reviewer_EdJn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981170151, "cdate": 1761981170151, "tmdate": 1762916970262, "mdate": 1762916970262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a native 3D editing framework that directly manipulates 3D mesh representations based on text instructions. The authors create a dataset by (1) procedurally removing parts from 3D objects for deletion tasks, and (2) lifting 2D edit pairs to 3D using image-to-3D models with manual curation for addition/modification tasks. They explore two conditioning strategies (1) cross-attention and  (2) token concatenation, finding that token concatenation performs better. The method generates edited 3D objects in 20 seconds using a two-stage transformer architecture built on TRELLIS. Experiments show improvements over baselines on FID, FVD, and CLIP metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Large-scale dataset construction. The paper creates over 110,000 training samples covering deletion, addition, and modification tasks through a systematic pipeline.\n\n2) Improvements on automatic metrics. Shows substantial gains over baselines (FID: 126.2 to 91.9) on standard benchmarks."}, "weaknesses": {"value": "1) The paper claims native 3D editing avoids 2D consistency problems, but evaluates using only 2D image metrics (FID/FVD/CLIP). These metrics cannot measure 3D geometric consistency, mesh quality, or whether edits are correctly localized in 3D space. \n\n2) No 3D geometric metrics. Missing essential measurements like Chamfer Distance on unedited regions, mesh quality checks (self-intersections, non-manifold edges), or 3D spatial accuracy of edits. Cannot verify the claimed advantage of native 3D editing.\n\n3) No human evaluation. Automatic metrics are insufficient to assess instruction following for 3D objects. Need human judges to evaluate whether edits match instructions when viewing objects from multiple angles, whether geometry is preserved, and overall quality.\n\n4) Incomplete ablation study. Token concatenation versus cross-attention comparison shows only 3 qualitative examples with no quantitative metrics. Cannot conclude which is actually better. Cross-attention implementation may be suboptimal.\n\n5) Unfair baseline comparisons. Different methods use different inputs (some need masks, some use 2D editing models). Cannot isolate whether improvements come from native 3D editing, larger dataset, architecture, or other factors.\n\n\n6) Missing critical details. No description of test set construction, no per-task performance breakdown (delete vs. add vs. modify), no analysis of generalization to unseen categories or instructions, no failure case discussion.\n\n\n7) Minor: Limited technical novelty. Token concatenation is not new; applying it to 3D editing with a specific architecture is incremental. Main contribution is the dataset and application."}, "questions": {"value": "1) Why no 3D geometric metrics?\n\n2) Can you provide a quantitative comparison of token concatenation vs. cross-attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x4zexu2qpI", "forum": "rbt3sFHuHm", "replyto": "rbt3sFHuHm", "signatures": ["ICLR.cc/2026/Conference/Submission3759/Reviewer_ejCW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3759/Reviewer_ejCW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762231748099, "cdate": 1762231748099, "tmdate": 1762916969957, "mdate": 1762916969957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the limitations that optimization-based approaches are prohibitively slow, while feedforward methods relying on multi-view 2D editing often suffer from inconsistent geometry and degraded visual quality, the authors propose a 3D editing framework that directly manipulates 3D representations in a single, efficient feed-forward pass. The work makes two main contributions:\n(1) a medium-scale multi-modal dataset for instruction-guided 3D editing, and\n(2) a 3D token concatenation approach as a conditional branch, whose principle closely resembles existing 2D works (e.g., CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n\n2. The experimental comparison is thorough, benchmarking against numerous established baselines."}, "weaknesses": {"value": "1. Limited visual quality. The method only supports very simple edits—essentially adding or replacing nearly solid-colored objects. For example, in Figure 3 (row 3), the added signboard is completely white with no texture or pattern. In contrast, HunYuan3D-2.1 produces significantly superior results. Most generated outputs also appear somewhat coarse and fall far short of the fidelity achieved by optimization-based methods (e.g., NANO3D).\n\n2. Lack of novelty. Both the cross-attention strategy and token concatenation strategy have been extensively explored in 2D editing tasks (e.g., Animate Anyone, CatVTON). The authors merely transfer these techniques to the 3D setting without introducing meaningful adaptations or domain-specific insights for 3D representation. Consequently, the technical contribution appears incremental.\n\n3. Missing efficiency analysis. The abstract claims that “optimization-based approaches are prohibitively slow,” yet the paper does not report inference speed or GPU memory consumption of the proposed method. Including such metrics would strengthen the evaluation.\n\n4. The ablation study evaluates different data-generation pipelines by training models on the resulting datasets and comparing downstream performance. This indirect proxy for data quality seems unnecessary and weak. Is there no more direct way to assess the quality of the generated training data itself (e.g., via geometric consistency, visual realism, or human evaluation)?"}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vygy2PHZyU", "forum": "rbt3sFHuHm", "replyto": "rbt3sFHuHm", "signatures": ["ICLR.cc/2026/Conference/Submission3759/Reviewer_QKjY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3759/Reviewer_QKjY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762852785940, "cdate": 1762852785940, "tmdate": 1762916969579, "mdate": 1762916969579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}