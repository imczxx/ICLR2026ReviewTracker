{"id": "GLPmZhhCAE", "number": 13349, "cdate": 1758216862464, "mdate": 1759897443402, "content": {"title": "Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs", "abstract": "Machine learning models trained on real-world data often inherit and amplify biases against certain social groups, raising urgent concerns about their deployment at scale. While numerous bias mitigation methods have been proposed, comparing the effectiveness of bias mitigation methods remains difficult due to heterogeneous datasets, inconsistent fairness metrics, isolated evaluation of vision versus multi-modal models, and insufficient hyperparameter tuning that undermines fair comparisons. We introduce NH-Fair, a unified benchmark for fairness without harm that spans both vision models and large vision–language models (LVLMs) under standardized data, metrics, and training protocols, covering supervised and zero-shot regimes. Our key contributions are: (1) a systematic ERM tuning study that identifies training choices with large influence on both utility and disparities, yielding empirically grounded guidelines to help practitioners reduce expensive hyperparameter tuning space in achieving strong fairness and accuracy; (2) evidence that many debiasing methods do not reliably outperform a well-tuned ERM baseline, whereas a composite data-augmentation method consistently delivers parity gains without sacrificing utility, emerging as a promising practical strategy. (3) an analysis showing that while LVLMs achieve higher average accuracy, they still exhibit subgroup disparities, and gains from scaling are typically smaller than those from architectural or training-protocol choices. NH-Fair provides a reproducible, tuning-aware pipeline for rigorous, harm-aware fairness evaluation.", "tldr": "", "keywords": ["AI Fairness", "AI & Society", "Utility-Fairness Trade-off", "Visual-language models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/77e02731bcfdcf241d798b2c163c42b9e880c46f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors created a big standardized benchmark called NH-fair. The main idea is to be able to compare debiasing methods against an ERM baseline. So they first tunned the base model (ERM) exhaustively using a DTO selection strategy, for over 10,000 A100 GPU hours, which proved that optimizer choice and pretrained wiehgts are crucial, fixing these choices leads to unfair comparisons. The results show that most specialized debiasing algorithms do not realiby outperform this ERM baseline. Instead, simple data augmentation (like RandAug) often achieves \"Fairness without harm\"(FWH), improving fairness without sacrificing utility. From their analysis, they revealed that huge LVLMs still exhibit significant subgroup disparities and simply scaling model size doesnt guarantee fairness."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I think the paper has a good originality as the NH-Fair unifies bias mitigation evaluation across tradicional supervised vision models and also LVLMs. It has a big significance as they introduce the FWH methodology. They did a big execution, solving the idea of insufficiently trained baseline problem as they spent over 10,000 A100 GPU hours with exhaustive hyperparameter optimization."}, "weaknesses": {"value": "The paper skips theoretical fairness concepts, as they only focus on group parity metrics, omitting individual fairness and counterfactual fairness.\nFuture work should try to incorporate these for a wider view of bias.\nMost part of data was about human faces, for future work needs to include more visual domains.\nThey did not fully use all sensitive attributes for evaluate intersectional groups (like gender vs race).\nLVLMs were evaluated only with zero shot prediction mode (classification) even the paper suggesting finetuning."}, "questions": {"value": "Data Augmentation is claimed to emerge as a promising practical strategy that \"most often achieves fairness without utility loss\" . Given that the FWH methodology allows for model selection in either the Optimal Zone or the Sub-Optimal Zone, could you quantify, across all seven datasets, the success rate of a method like RandAugment when classifying its best-found model into the Optimal Zone versus the Sub-Optimal Zone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YjzyvWyY65", "forum": "GLPmZhhCAE", "replyto": "GLPmZhhCAE", "signatures": ["ICLR.cc/2026/Conference/Submission13349/Reviewer_dLUc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13349/Reviewer_dLUc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781695310, "cdate": 1761781695310, "tmdate": 1762924001433, "mdate": 1762924001433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NH-Fair, a benchmark for \"Fairness Without Harm\" (FWH). Its core contribution is demonstrating that a rigorously tuned ERM baseline (using a novel DTO/FWH selection) often outperforms complex bias mitigation methods. The benchmark is extended to LVLMs, finding they still suffer from bias and that scaling does not automatically fix it."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Practical Problem: The \"Fairness Without Harm\" (FWH) principle is highly relevant for real-world deployment (e.g., healthcare).\n\n2.Strong Baseline: The paper's key finding—that a well-tuned ERM can beat specialized algorithms—is a crucial, critical finding for the field, backed by extensive HPO.\n\n3.Rigorous Protocol: The DTO/FWH model selection strategy provides a novel and fair method for comparing models.\n\n4.Timely LVLM Analysis: The paper correctly identifies that LVLMs are not a panacea for fairness, a critical and timely observation."}, "weaknesses": {"value": "1.Limited LVLM Scope: The LVLM evaluation is zero-shot only. This is a major gap, as models are typically fine-tuned, a process which could significantly alter fairness outcomes.\n\n2.No Intersectional Analysis: The benchmark only considers single sensitive attributes, ignoring intersectional biases (e.g., race and gender), which can be more severe.\n\n3.Lacks Deep Insight: The paper is excellent at observing (e.g., \"RandAug works well\" , \"Optimizers matter\" ), but provides little analysis as to why these phenomena occur."}, "questions": {"value": "1.LVLM Fine-tuning: How do you expect your zero-shot LVLM findings to generalize to the more common fine-tuning setting?\n\n2.HPO Cost: Your finding relies on 10k+ GPU-hours. What is the \"budget-friendly\" HPO recommendation for a practitioner who cannot afford this?\n\n3.Waterbirds: You note Waterbirds is a spurious correlation dataset, not a social bias one. Why include it in a social fairness benchmark if it's \"easier to resolve\" and may dilute the main message?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OXvx6Ins9s", "forum": "GLPmZhhCAE", "replyto": "GLPmZhhCAE", "signatures": ["ICLR.cc/2026/Conference/Submission13349/Reviewer_ZcM2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13349/Reviewer_ZcM2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801864882, "cdate": 1761801864882, "tmdate": 1762924001060, "mdate": 1762924001060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new benchmark for bias mitigation methods across 7 image classification tasks. It benchmarks traditional mitigation methods with a ResNet18 as well as vision-language models like CLIP and LLMs. It also includes analyses on the impact of hyper-parameters on model fairness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: The authors conduct a systematic hyper-parameter optimisation and model selection protocol, a step which is often overlooked in bias mitigation benchmarks.\n\nS2: The authors present some unique analyses which are not usually discussed in bias mitigation papers, for instance on the impact of hyper-parameter choice on fairness metrics (e.g., choice of optimiser appears more impactful than choice of weight decay), whether pre-training is done (I liked the plots in the Appendix Fig 5), and the size of LLMs. \n\nS3: They conduct extensive experiments and include a range of model types. \n\nS4: I appreciated this insightful point: \"Over-reliance on datasets like Waterbirds may therefore underestimate the difficulty of fairness challenges and overstate algorithmic effectiveness.\""}, "weaknesses": {"value": "**Major weaknesses**\n\nW1: My primary criticism to the paper is that I am not sure what the field needs is another benchmark suggesting that overall ERM performs better/more reliably than existing mitigation methods (I would say there is already a broad consensus on this). I would argue that actually, instead of having an aggregate benchmark where many methods/models are compared across different datasets and metrics, it would make more sense to do a tailored analysis looking at which mitigation methods work when. This is I believe in line with current research in the field [1,2,3,4] which suggest that it is important to understand what the cause of bias is, what type of bias it is, what fairness metric you want to optimise for, in order to determine if you should conduct mitigation and with what method. Similarly to this point, I think looking at so many fairness metrics at once is also counter-productive, as you will only really care about optimising one metric for a given setting (for instance there are very few settings where it would make sense to optimise for demographic parity) [5]. \n\nW2: I think the statement that \"the scaling law\" does not apply to AI fairness is over-stated given that the authors have only conducted a couple experiments on classification. \n\nW3: The authors say they \"suggest not confusing fairness datasets with domain generalization dtatasets (e.g., Waterbirds, Colored MNIST). While DG datasets probe robustness to distribution shifts, they do not contain socially meaningful sensitive attributes\". While I agree (as mentioned in S4) that it is important to not just evaluate mitigation methods on over-simplistic spurious correlation benchmarks like Waterbirds, I disagree that there is an inherent difference between domain generalisation and fairness datasets. I think what matters for bias mitigation methods is just to have some coherent grouping of data (and that can be based on demographic attributes or on other groupings, like where the image was taken, what device was used etc.). You can have super \"simple\" socially meaningful groups (e.g., skin tone in skin lesion) the same way you could have very complex “domains\" (e.g., geographic location of a sample) so I don't believe there is a hard difference for the mitigation algorithms. Furthermore, I don't think the authors can argue about using socially meaningful groups when they are using very un-meaningful classification targets for some of their experiments (e.g., whether someone has wavy hair in CelebA)!. \n\nW4: I generally think there are a lot of tables that are hard to interpret. Many also do not have boldings or standard deviations and inconsistent numbers of significant decimal places. This gives the manuscript an unfinished aspect. I would recommend that the authors try to make some more of the tables into plots (for instance Table 15 on LLM size). I would also recommend that they vary the interpretations, for example when you say that optimizer choice matters, instead of just listing all the different metric values for different datasets and optimiser it would be helpful to include some summary stats, e.g., overall accuracy varies by a max of this much, or accuracy gap can increase by 20%.\n\nW5: It would be interesting for the authors to check other LLM tasks like fairness of generated outputs. \n\nW6: In fig3 it would be helpful to show overall accuracy as well because we don’t know whether the models are harming overall performance or not. The authors could also include some kind of statistical testing to see if there are any significant differences relative to ERM. \n\nW7: One of your datasets fairface has no gap (0.86)! What’s the point of including it in the fairness analysis?\n\nW8: Small concern that gender and race classification tasks may be problematic.\n\nW9: There are lots of missing details on the way the mitigation methods work, even in the appendix. This is particularly true for bias mimicking, FIS, DFR (should specify that you are retraining the last layer on a balanced distribution! - right?), CLIP/BLIP models and for related post-training debiasing methods!\n\nW10: It would be helpful to show variance of results for each method across hyper-parameters.\n\n**Minor weaknesses**\n\nW11: Min max fairness definition shouldn't have the min as you are presenting the metrics not the way they're optimised (to keep it consistent with the other fairness definitions). \n\nW12: All the left quotes are wrong. Please use `` for left quotes and '' for right quotes in LaTeX!\n\nW13: Table 1 columns are a bit confusing. I would just put the proportion of the targets.\n\nW14: Colouring the cells in table 3 to give an indication of better worse metrics would be helpful for the reader.\n\nW15: The wording in the conclusion “the fairness issue remains unresolved.\" is overly simplified and referring to \"the fairness issue\" feels imprecise, as fairness is so multi-faceted.\n\nW16: What does this wording mean in section B1 “Without additional illustration”?\n\nW17: It would be helpful to add a note on which methods use demographic information.\n\nW18: Undefined reference line 1114.\n\nW19: Table 15: you can also get an SD for the average metrics.\n\n[1] Rethinking Fair Representation Learning for Performance-Sensitive Tasks, ICLR 2024.\n[2] Change is Hard: A Closer Look at Subpopulation Shift, ICML 2023.\n[3] Mind the Graph When Balancing Data for Fairness or Robustness, NeurIPS 2024.\n[4] Automatic dataset shift identification to support safe deployment of medical imaging AI, MICCAI 2025.\n[5] Critical Appraisal of Fairness Metrics in Clinical Predictive AI, arXiv 2025."}, "questions": {"value": "Q1: Why did the authors select the datasets they did? Why did they include two medical datasets of the same modality (skin images) instead of considering other medical modalities?\n\nQ2: Are you comparing two model selection methods, or just applying DTO to ERM and then doing fairness without harm for the rest? Later you say “best model selected under DTO or FWH criteria”.\n\nQ3: Why did the authors not include post-processing methods? these are often suggested to be the most useful in practice, as suggested here [1].\n\nQ4: Why do you use resnet18 as a backbone?\n\nQ5: Why do you only look at group x class grouping for resampling?\n\nQ6: Is there any literature on optimisers and fairness? Can you explain your results? Similarly do you have a hypothesis for the effect of batch size?\n\nQ7: Why do you think there is so much prompt sensitivity - this seems like a significant weakness? Also for that table 17, it would be helpful to summarise the variation in results as SD or mean difference in outputs. \n\nQ8: How do you obtain SDs? Average over random seeds?\n\n[1] Oxonfair, NeurIPS 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bb0hWBkuaE", "forum": "GLPmZhhCAE", "replyto": "GLPmZhhCAE", "signatures": ["ICLR.cc/2026/Conference/Submission13349/Reviewer_ByuY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13349/Reviewer_ByuY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921551005, "cdate": 1761921551005, "tmdate": 1762924000766, "mdate": 1762924000766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a comprehensive study of fairness-utility trade-off in modern vision and multimodal foundation models. The authors create a new benchmark consisting of various domains and tasks, a systematic model selection protocol, and a set of group fairness and utility metrics. This benchmark is used to evaluate and compare SoTA models, training strategies and debiasing methods to explore the optimal recipe for accurate and fair models. Experiments show that a well-optimized baseline with large-scale pretraining and data augmentation matches or beats sophisticated mitigation methods in fairness while maintaining performance on utility tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This work addresses several shortcomings in fairness evaluation in prior work, such as inconsistent hyperparameter selection, overlooked utility performance and not including pre-trained foundation models.\n- The evaluation framework is systematic, consisting of multiple datasets and domains, principled training and model selection, and numerous utility and bias metrics. This is crucial to ensure fair comparison between methods as well as reproducibility.\n- Extensive ablation experiments on training practices such as model size, batch size and weight decay, as well as evaluation of VLMs putting them in the same playing field with dedicated classification models."}, "weaknesses": {"value": "- The proposed benchmark is limited to classification task. Despite being reformulated for image-text matching (CLIP) and generative (LVLM) models, it does not truly address utility and fairness beyond closed-set predictions. I would have liked to see open-set tasks like free-form image-text retrieval and open-ended VQA, captioning or reasoning, as these are the tasks where vision(-language) foundation models truly overtake task-specific vision models. This would also enable the holistic evaluation of VLM debiasing methods, which also tend to overlook utility performance while optimizing for fairness.\n- Even in classification, I am slightly concerned that the performance is already saturating for most models and datasets, with group accuracy gap under 5% and DP, EqOdd over 95%. I am not sure how perceivable these disparities are in practice. It might also be helpful to construct a \"fairness-hard\" subset with more challenging datasets/subtasks (e.g., more attributes in CelebA correlated with either gender; only \"wavy hair\" is used in the current version).\n- In general, I feel that the conclusions from the experiments confirm anecdotal observations bias mitigation (e.g., that hyperparameter tuning and model selection make a big difference), but not groundbreaking in context of existing fairness benchmarks (e.g., FFB https://arxiv.org/abs/2306.09468).\n- Certain conclusions in hyperparameter choice are qualitative (e.g., optimizer choice and learning rate are more important than other hyperparameters) and could have benefitted from a quantified sensitivity metric."}, "questions": {"value": "See weakness section. In addition, I wonder if combining data curation and algorithmic mitigation can produce the best trade-off between utility and accuracy? Should future research explore the intersection between both, or prioritize data-centric methods as suggested in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9DyWs0swd2", "forum": "GLPmZhhCAE", "replyto": "GLPmZhhCAE", "signatures": ["ICLR.cc/2026/Conference/Submission13349/Reviewer_RqaZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13349/Reviewer_RqaZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13349/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762213782218, "cdate": 1762213782218, "tmdate": 1762924000199, "mdate": 1762924000199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}