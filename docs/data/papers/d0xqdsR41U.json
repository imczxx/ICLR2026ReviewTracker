{"id": "d0xqdsR41U", "number": 4726, "cdate": 1757753712894, "mdate": 1759898017720, "content": {"title": "WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks", "abstract": "Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: $\\textit{Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves?}$ In this paper, we introduce \\textbf{WebChoreArena}, a new fully reproducible benchmark comprising 532 carefully curated tasks over 300+ hours, designed to address more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) $\\textbf{Massive Memory}$ tasks requiring accurate retrieval of large amounts of information in the observations, (ii) $\\textbf{Calculation}$ tasks demanding precise mathematical reasoning, and (iii) $\\textbf{Long-Term Memory}$ tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, significant performance improvements are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with GPT-5, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.", "tldr": "We propose WebChoreArena, a benchmark of 532 complex and tedious web tasks. State-of-the-art LLM agents show notable performance drops, highlighting their limitations beyond general browsing.", "keywords": ["benchmark", "web browsing agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b12f43c723ea37725a6c88310156e9ccedfb3f4.pdf", "supplementary_material": "/attachment/c25efdbbc2cfb3326308763d30aea283bed2dbd7.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces **WebChoreArena**, a new benchmark designed to systematically evaluate LLM-powered web agents on tedious, memory-intensive, and reasoning-demanding web tasks. The benchmark consists of 532 manually constructed tasks across four simulated WebArena environments — *Shopping*, *Shopping Admin*, *Reddit*, and *GitLab* — totaling over 300 hours of human annotation. Tasks are categorized into Massive Memory, Calculation, Long-Term Memory, and Others, enabling comprehensive assessment of agents’ cognitive and operational capabilities. Fully compatible with WebArena for reproducibility and comparability, the authors evaluate AgentOccam and BrowserGym frameworks using several state-of-the-art LLMs, including GPT-4o, Claude Sonnet 3.7/4, Gemini 2.5 Pro, and GPT-5."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a well-motivated and timely benchmark that addresses an important gap in evaluating LLM-based web agents on complex, tedious, and memory-intensive tasks. It demonstrates strong methodological rigor, with 532 carefully curated tasks covering diverse domains and requiring multi-step reasoning, long-term memory, and precise calculation. The benchmark’s full compatibility with WebArena ensures reproducibility and enables fair cross-agent comparison. The authors conduct comprehensive experiments using multiple state-of-the-art frameworks (AgentOccam and BrowserGym) and several leading LLMs (GPT-4o, GPT-5, Claude, Gemini), providing valuable insights into model weaknesses and task-specific performance trends. In addition, the clarity of writing, systematic task taxonomy, and inclusion of human baselines make the paper highly accessible and informative. Overall, WebChoreArena is a significant and practical contribution that is likely to become an essential resource for future research on web agents and long-horizon reasoning."}, "weaknesses": {"value": "First, the study lacks algorithmic innovation, focusing solely on benchmark construction rather than proposing new agent mechanisms. Second, all experiments are confined to simulated environments, which limits the ecological validity and generalizability to real-world dynamic websites. Third, the model coverage is insufficient — only a few large proprietary models (e.g., GPT and Claude series) are tested, with no inclusion of diverse model sizes or more open-source models (such as LLaMA, Qwen, or Mistral families). Expanding the evaluation to a broader spectrum of model scales and architectures would make the benchmark more representative and strengthen its conclusions. Finally, the error analysis remains descriptive without suggesting concrete improvements, and the benchmark’s high manual construction cost may hinder scalability."}, "questions": {"value": "1. Model Diversity:\nThe paper evaluates several leading models (GPT-4o, GPT-5, Claude, Gemini), but the coverage remains limited. Could the authors expand the experiments to include a wider variety of both open-source and proprietary models, such as LLaMA, Qwen, Mistral, or DeepSeek, to better understand performance trends across different architectures and scales?\n\n2. Model Scaling Analysis:\nHave the authors examined how model size or reasoning depth affects performance on different task types (e.g., Massive Memory vs. Calculation)? Including a scaling curve analysis could provide valuable insights into whether task success correlates with model capacity or training methodology.\n\n3. Closed-Source Model Comparison:\nBeyond GPT and Claude, are there plans to evaluate other strong closed-source models, such as Gemini Ultra, Claude Opus, or proprietary enterprise agents? This would help position WebChoreArena as a more comprehensive benchmark for the broader LLM ecosystem.\n\n4. Open-Source Baseline Integration:\nWould the authors consider including smaller open-source baselines (e.g., 7B–14B models) to establish a clearer performance hierarchy and make the benchmark more accessible for the academic community?\n\n5. Generalization Beyond Simulation:\nSince the current setup relies on simulated WebArena environments, do the authors plan to extend the benchmark to real-world dynamic websites and test whether the same models maintain consistent performance under non-deterministic conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zroj4dt7l6", "forum": "d0xqdsR41U", "replyto": "d0xqdsR41U", "signatures": ["ICLR.cc/2026/Conference/Submission4726/Reviewer_HpZT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4726/Reviewer_HpZT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761404600982, "cdate": 1761404600982, "tmdate": 1762917538282, "mdate": 1762917538282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Given the complexity limitations of current agent benchmarks, this work proposes a new WebChoreArena benchmark, that features more challenging tasks stressing massive memory, calculation, and long-term memory handling. Experimental results with various agents reveal directions for future agent development."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Benchmarking Contribution to the Agent Community.**\n> Regarding recent agent progress. As existing mainstream benchmarks are beginning to be solved, this work introduces more complex tasks, particularly targeting massive memory, calculation, and long-term memory handling scenarios, to facilitate the development of more capable agents.\n\n2. **High-quality example curation process.**\n> The tasks are manually collected by agent researchers. Measures of task properties are also reported.\n\n3. **Decent benchmarking effort.**\n> This paper benchmarks multiple major open-source agent frameworks, with a series of LM backbones. Further, multiple analyses were conducted to offer more insights."}, "weaknesses": {"value": "1. **Lack of Fine-Grained Evaluation.**\n> As the tasks become more complex (i.e., involve more checkpoints that agents need to achieve), a single end-task evaluation tells less information. It would be more informative if more fine-grained intermediate evaluations, especially targeting each atomic requirement in the task instructions, were included in the benchmark.\n\n2. **Unclear in complexity?**\n> Although tasks are claimed to be more challenging than the WebArena benchmark, the best agent (with GPT-5) already achieves ~50% success rate, indicating that the benchmark may be largely solved by the frequently-updated models soon (?). Further on this point, it is very likely that the two agent frameworks experimented in this work do not bring out the best performance of GPT-5 (as opposed to ChatGPT or other popular agent frameworks), therefore it is possible that some existing agents already can solve 60-70% of this benchmark."}, "questions": {"value": "1. How realistic are the tasks in WebChoreArena, and what processes have been applied to ensure the realism of tasks? As opposed to overly driving up the task complexity, potentially to a point where humans wouldn’t even need to do such tasks. \nOn the other hand, is the purpose of this benchmark to: (i) reflect how agents perform in practice, (ii) act as stress tests to agents, or others?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HkIq74q2Ek", "forum": "d0xqdsR41U", "replyto": "d0xqdsR41U", "signatures": ["ICLR.cc/2026/Conference/Submission4726/Reviewer_5TKV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4726/Reviewer_5TKV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761506358983, "cdate": 1761506358983, "tmdate": 1762917537602, "mdate": 1762917537602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce WebChoreArena, a benchmark of 532 human curated web agent tasks designed to be more tedious and complex than WebArena tasks. These tasks feature three challenge types: massive memory, calculation, long term memory. The paper evaluated leading models including GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Pro with two agent frameworks of AgentOccam and BrowserGym."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper shows clear problem focus and task taxonomy: benchmark that targets tedious chores underrepresented in prior work.\n\nThe paper builds on top of existing, proven sandbox environments of WebArena, saving efforts for adaptation and potential environment pitfalls. \n\nAnnotators followed explicit guidelines to curate tasks, with cross checking to minimize labeling, evaluation errors."}, "weaknesses": {"value": "It would be helpful to see more analysis on agent’s use of calculation related tools in completing requirements, on top of the fact that models don’t always choose to use calculators. \n\nHas the author examined if models are given sufficient tools or agentic design that enables long term memory. An example could be a notebook page, or a function call that allows models to write/read/search from a notebook. It would be great to see some analysis on how well models utilize these functions to complete tasks that require longer term memory. \n\nIt would be great to provide more details on how different modality is used for the agent’s action."}, "questions": {"value": "Listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S8veuyp5Kj", "forum": "d0xqdsR41U", "replyto": "d0xqdsR41U", "signatures": ["ICLR.cc/2026/Conference/Submission4726/Reviewer_2m48"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4726/Reviewer_2m48"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986227873, "cdate": 1761986227873, "tmdate": 1762917537080, "mdate": 1762917537080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new benchmark called WebChoreArena, built on top of the WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. This new benchmark comprises of 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: 1) massive memory, 2) calculation, and 3) long-term memory. The experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Benchmark is well established on an existing infrastructure by WebArena, and following suit the best practices also makes this benchmark very reproducible and realistic, including good outcome-based reward to prevent reward hacking.\n\nCode repository and containers are well organized online and reproducible, documentation is good\n\nThe paper is clear and reasonable about why this benchmark is needed, on top of the existing WebArena benchmark. It's focus on chore tasks, with longer horizon but more repetitiveness is indeed valid concern and justifies the new benchmark to differentiate from existing. The large memory required is quite interesting indeed and can test the model's capability."}, "weaknesses": {"value": "As a benchmark paper, it would always be nice to present a bigger leaderboard and allow people to submit results. It mainly uses AgentOccam and BrowserGym as the agent framework, with several strong base models, but it would be really adding strength with more. \n\nFor Memory-intensive long horizon tasks, it would be nice to experiment with agent orchestration that has special focus on compression/condensation of observations, instead of more generic ones. One example could be OpenHands with has condenser feature.\nMore details about the data distribution and what is done to improve the data annotation matches what real world scenario presents is crucial. Given the environments are simulated, it's even more important to have realistic queries/tasks that closes the sim2real gap."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p48iNYwdhD", "forum": "d0xqdsR41U", "replyto": "d0xqdsR41U", "signatures": ["ICLR.cc/2026/Conference/Submission4726/Reviewer_cMr3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4726/Reviewer_cMr3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762339945719, "cdate": 1762339945719, "tmdate": 1762917535189, "mdate": 1762917535189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}