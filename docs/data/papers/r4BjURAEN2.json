{"id": "r4BjURAEN2", "number": 12859, "cdate": 1758211001974, "mdate": 1763377972154, "content": {"title": "Towards Causal Fine-Tuning under Latent-Confounded Shift", "abstract": "Adapting to latent-confounded shift remains a core challenge in modern AI. Such shift is driven by hidden variables that induce spurious, non-transportable correlations between inputs and outputs. A practical failure mode arises when fine-tuning pre-trained foundation models on confounded data (e.g., where certain text tokens or image backgrounds spuriously correlate with the label), leaving models vulnerable at deployment. We introduce *causal fine-tuning, which frame model adaptation as an identification problem* and pose an explicit causal model that decomposes inputs into low-level spurious features and high‐level causal representations. Under this family of models, we formalize the assumptions required for identification. Using pre-trained language models as a case study, we show how identifying and adjusting these components during causal fine-tuning enables automatic adaptation to such shift at test time. Experiments on real-world stress-test benchmarks demonstrate that our method outperforms black-box domain generalization baselines, highlighting the benefits of explicitly modeling causal structure.", "tldr": "", "keywords": ["causal inference", "confounded-shift", "causal generalisation"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/dcfe1f4ba0ef8424c87ba165b7b0f853012431dd.pdf", "supplementary_material": "/attachment/960fe38ed0ace4daa6be5b49697a75fa09b93732.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of \"latent-confounded shift,\" where foundation models, when fine-tuned, learn spurious correlations induced by unobserved confounders (e.g., data source) that do not generalize to out-of-distribution (OOD) settings. The authors propose \"Causal Fine-Tuning\" (CFT), a novel framework that recasts model adaptation as a causal identification problem. CFT is based on an explicit causal graph that decomposes the input into a high-level causal representation C, a high-level spurious representation S, and a low-level, environment-invariant representation Φ. The key technical contribution lies in a two-part identification strategy: 1) identifying the invariant causal feature C by contrasting representations from a frozen pre-trained model (R_0) and a fine-tuned model (R_1), and 2) using a front-door adjustment via the low-level features Φ to compute an interventional prediction p(y|do(x)). Experiments on sentiment analysis benchmarks with injected, controlled spurious correlations demonstrate that CFT significantly outperforms standard fine-tuning and other domain generalization baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a clear and compelling formulation of \"latent-confounded shift\" as a critical failure mode for fine-tuned foundation models. By moving beyond standard OOD settings and explicitly modeling the role of unobserved confounders, the work addresses a problem of significant practical and theoretical importance.\n\n2. The proposed CFT framework is a principled and elegant application of causal inference principles to model fine-tuning.\n\n3. The paper is well-organized and well-written."}, "weaknesses": {"value": "1. The entire framework is built upon a detailed and complex set of causal assumptions (Assumptions 4.1-4.4 and the graph in Fig. 2). While these assumptions enable the identification strategy, their validity in real-world scenarios is difficult. For example, the assumption that low-level features Φ (e.g., the embedding layer) are strictly environment-invariant (no arrow from σ) is a strong claim. Environmental shifts could subtly influence token distributions and their embeddings. Also, the front-door path assumption (Assumption 4.4) is also very strong, requiring that Φ contains all necessary information to identify C and that C fully mediates the effect of Φ on Y. The paper could be strengthened by discussing the sensitivity of the method to violations of these assumptions.\n\n2. The experimental validation is limited. It is confined to text classification with artificially injected spurious correlations (e.g., modified stop words), which may not reflect more natural and subtle forms of confounding found in real-world. Furthermore, the lack of experiments on other modalities, such as vision, leaves the method's generalizability across different tasks an open question.\n\n3. The authors argue against using GenAI tools like ChatGPT for creating benchmarks due to cost and reproducibility. This is a valid point. However, could GenAI be used to generate more diverse and subtle confounders than the simple suffix-based injection used in the paper, providing a more challenging testbed?\n\n4. The graph includes confounders U_S and U_Φ. The role of U_S (confounding R_1 and Φ) and U_Φ (confounding Φ and Y) is crucial for justifying the front-door adjustment. However, the intuition for what these confounders represent in a real-world NLP task is not fully developed. Providing concrete examples would greatly improve clarity.\n\n5. Loss Function L_C (Eq. 3): The first term enforces invariance by minimizing the L2 distance between two distributions. Was this choice of metric (L2) crucial? Have other divergence measures (e.g., KL, Wasserstein) been considered?\n\n6. The font size in Figures 4 and 5 is too small, making them difficult to read. I would strongly recommend increasing the font size to improve readability."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XTwjDueP7X", "forum": "r4BjURAEN2", "replyto": "r4BjURAEN2", "signatures": ["ICLR.cc/2026/Conference/Submission12859/Reviewer_Hstx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12859/Reviewer_Hstx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794789631, "cdate": 1761794789631, "tmdate": 1762923652019, "mdate": 1762923652019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Authors’ Comment (No Rebuttal Planned)"}, "comment": {"value": "We thank all reviewers for their time and thoughtful feedback. While we recognize that the topic of causal fine-tuning sits at the intersection of causal inference and representation learning, and may therefore be unfamiliar to some reviewers. We sincerely appreciate the effort everyone made to engage with the work but in light of the clash of expectations between what we expected from reviewers and where the reviewers might come from suggests to us that a discussion is unlikely to be productive and everyone's time is scarce. \n\nReviewer C5VF’s comments, for instance, reflect gaps in understanding which we do not believe are justified. For what's worth the statement *\"Non-standard (and confusing) use of the regime variable. Writing makes sigma both a random variable and an intervention command\"* is not what we would expect from a conference such as ICLR. Not only we explicitly introduce sigma as a regime variable, not random, but we used the standard semi-colon \";\" separator to distinguish conditioning from indexing. **We also invite the reviewer to get acquainted with the proof of the back-door criterion (a graphical criterion for conditional ignorability).** A good start is Eq. 3.8 in Pearl's Causality book (Cambridge University Press, 2009), but perhaps a reading of the whole chapter would be helpful. For a more in-depth reading, the referred Dawid (2021) goes in detail. We won't comment on the other shortcomings of the review which we disagree with.\n\nIn general, reviewers provided valuable pointers on clarifying assumptions, representation and notation. We believe this exchange has helped us frame the causal fine-tuning problem more precisely, and we are grateful for the reviewers’ insights even where opinions differed."}}, "id": "Gi4oead6M6", "forum": "r4BjURAEN2", "replyto": "r4BjURAEN2", "signatures": ["ICLR.cc/2026/Conference/Submission12859/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12859/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12859/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763377829827, "cdate": 1763377829827, "tmdate": 1763377829827, "mdate": 1763377829827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "lpBznMOkRm", "forum": "r4BjURAEN2", "replyto": "r4BjURAEN2", "signatures": ["ICLR.cc/2026/Conference/Submission12859/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12859/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763377971443, "cdate": 1763377971443, "tmdate": 1763377971443, "mdate": 1763377971443, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes causal fine-tuning for robust generalization under confounded shifts, validated via experiments on sentiment analysis using language models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Propose a front-door adjustment fine-tuning method, offering a principled approach to handling latent confounding.\n- Provides a theoretical analysis with clear problem formulation, ensuring the method's derivation is well-motivated and grounded."}, "weaknesses": {"value": "- There are some unclear assumptions and designs:\n  - Why does the causal variable $C$ remain the same across pre-training and fine-tuning?\n  - How to select $k$ for low-level features in different applications, and there are no ablation studies.\n\n- Experiments are confined to BERT on sentiment analysis, leaving broader NLP tasks and models unverified.\n\n- Writing issues, for example:\n   - Line 170, missing \"an\" before \"assumption\"\n   - Line 250, ambiguous phrasing \"the pre-trained and training fine-tuning data\". It may be \"the pre-trained model\"\n   - Line 354, likely typo \"WSA\" should be \"SWA\""}, "questions": {"value": "- How do the computational cost and training time of causal fine-tuning compare to baseline methods?\n- Given that SWA outperforms CFT in some scenarios (Table 2), how can we determine whether to select CFT over other methods in real-world applications, particularly when the level of spurious correlations is unknown?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2JufID4L7u", "forum": "r4BjURAEN2", "replyto": "r4BjURAEN2", "signatures": ["ICLR.cc/2026/Conference/Submission12859/Reviewer_m11s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12859/Reviewer_m11s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834398617, "cdate": 1761834398617, "tmdate": 1762923651763, "mdate": 1762923651763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a causally inspired framework for improving model robustness during fine-tuning. It introduces latent variables representing causal and spurious factors and assumes that spurious correlations in pretraining and fine-tuning arise from distinct sources. Under several structural assumptions, the authors derive a formulation for estimating p(y∣do(x)) and propose an algorithm that approximates this intervention. Experiments on synthetic and text classification benchmarks suggest improved out-of-distribution generalization compared to standard fine-tuning baselines."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper introduces an interesting and ambitious causal framing for fine-tuning;\n2. The confounded shift scenario in Figure 1b is under-explored;\n3. The proposed algorithm yields consistent OOD improvements on sentiment and synthetic datasets."}, "weaknesses": {"value": "1. Non-standard (and confusing) use of the regime variable $\\sigma$. Writing $\\sigma=do(x)$ makes $\\sigma$ both a random variable and an intervention command.\n2. (R_0,R_1,\\Phi)=f(X)$ are introduced non-constructively, then used in proofs as if measurable variables. The paper later says it will \"explicitly\" construct them, but the construction is only sketched and heavily heuristic. It is unclear how the chosen construction satisfies the causal graph in Figure 2.\n3. Assumption 4.2 requires $\\{S_0,S_1,C\\}$ to be mutually independent, with $S_0 \\to R_0$ only, $S_1 \\to R_1$ only, and $\\sigma$ can only affect $S_1$. It also postulates that any dependence between $S_0$ and $S_1$ is solely via $C$. These restrictions drive the identification story, but seem implausible in many real settings. It is also not validated empirically if these assumptions hold in real data.\n4. Assumption 4.3 states $\\sigma$ affects the system only via S, but Assumption 4.2 already restricts $\\sigma$’s effect to $S_1$ (not $S_0$).\n5. Front-door–style Assumption 4.4 is very strong and untested. It requires no direct edge $\\Phi\\to Y$, while allowing confounding between $\\Phi$ and $Y$. This is a demanding structural claim, and the paper gives no empirical test for it; yet it is pivotal for identification.\n6. The estimation for P(Y|do(X)) is computed over all possible $\\Phi$ and $x$, but in practice is only estimated within a mini-batch. This surrogate for interventional averaging lacks a causal justification and may bias the estimate.\n7. Experiments are run on synthetic datasets where spurious signals are injected with controlled strengths. This is useful for controlled tests, but it doesn’t demonstrate that the assumptions (4.2–4.4) hold in naturally occurring OOD shifts.\n8. Core choices (e.g., choice of $\\Phi$, dimensionality of C, shuffle scheme) are not thoroughly ablationed.\n9. The language is very informal (e.g. \"we may be thrown away too far...and the safer bet...\") and the paper appears to be unpolished."}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MVYeNS9VzL", "forum": "r4BjURAEN2", "replyto": "r4BjURAEN2", "signatures": ["ICLR.cc/2026/Conference/Submission12859/Reviewer_C5VF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12859/Reviewer_C5VF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841801898, "cdate": 1761841801898, "tmdate": 1762923651493, "mdate": 1762923651493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes \"Causal Fine tuning\" as a new fine-tuning method that is robust against spurious correlations. This involves first fine-tuning the model in a standard way. Next, they learn the hidden causal invariant feature using the method from Kugelgen et al. Then, using C together with low-level features, they estimate P(y|do(x)). Experiments show that CFT is more robust to spurious correlations or change in the (hidden) confounder compared to SFT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper solves an important problem of distribution shift in fine-tuning\n2. The paper clearly states its assumptions in the main paper"}, "weaknesses": {"value": "**Theory**\n1. The main theoretical result (identifiability of C) is from Kugelgen et al. (2021) and the paper is cited. However, I could not find any discussion on how 1)this paper's notation corresponds to the Kugelgen paper. For example, what is the C, R0 and R1 in the Kugelgen paper? 2)whether the assumptions of this paper satisfy the assumptions required for Kugelgen's theorem 4.4. Omitting this places a burden on the reader to go through and understand the complete Kugelgen paper (including their notation, assumptions etc.). I would also expect a proof sketch or even a complete proof in the appendix even if it largely follows Kugelgen's proof. \n\n2. Several assumptions are not clearly mentioned. For example Line 248 mentions \"Let the mapping between {S0, S1, C} and {R0, R1, Φ} obey the invertibility conditions of Von Kugelgen et al. (2021)\". However, the paper should explicitly mention what these are in this context. As far as I understand, it is required that the function from C,S0,S1 to R0,R1 to be invertible. Why is this a reasonable assumption?\n\n3. Component 2: Line 315 says the loss function is constructed from Theorem 4.5. It is not clear to me, however, how it is constructed from Theorem 4.5. Please add discussion and proof in the appendix.\n\n4. The new variables introduced in Section 4 should be explained with an example. It is hard to understand why these variables were introduced and what they intuitively mean. For example, in the Kugelgen paper, C is the content and S is the style. However, I am not sure what they mean in this paper's setting.\n\n**Experiments**\n\n5. The causal graph in Section 4 is quite complex and I would have liked the experimental section to show that the framing of the problem as this causal graph is really necessary. For example, is it necessary to learn C? One good experiment might be to replace C by R1 in Step 8 of Algorithm 1. That is, you still do the shuffling of $\\phi$ but use R1 for the final prediction.\n\n6. While CFT outperforms SFT, the performance of CFT too decreases considerably as the degree of shift increases. Could the authors explain why it is so? Is it because real data may not satisfy all the assumptions? If there is indeed a setting where all the assumptions are satisfied, would we still expect the performance to decrease?\n\n**Minor**\n\n7. Some missing references on hidden confounder shift papers: [1] [2] \n\n8. I would like to see results on fine-tuning bigger models like Qwen-2.5-3b, Llama-3-8b, Olmo-2-7b etc I am interested in seeing if they too degrade with the shifts in the current experimental section. I understand the authors may face compute constraints and the review period may not be enough time. Therefore, this is just a minor weakness for me. But experiments on one of these big models would be great.\n\n\n[1]Tsai, Katherine, et al. \"Proxy methods for domain adaptation.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2024.\n[2] Prashant, Parjanya Prajakta, et al. \"Scalable Out-of-Distribution Robustness in the Presence of Unobserved Confounders.\" International Conference on Artificial Intelligence and Statistics, PMLR, 2025\n\n**Note:** While my current rating is quite low, I am open to raising it based on the rebuttal. In particular, I would like the author to add discussion that clearly shows that they satisfy all the conditions required for Kugelgen's theorems. Also, I would like to see solid evidence that the current framing of the problem in terms of the causal graph in Figure 2 is required and something simpler will not work."}, "questions": {"value": "Some questions are mentioned in the weaknesses.\n\n1. The authors mention that each model was finetuned upto 10 epochs. I am curious what happens if you finetune only for 1 or 3 epoch? What about if you use LoRA? Does this sort of early stopping or LoRA training stop SFT from fitting so much to the spurious features?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IPj1VGqkrq", "forum": "r4BjURAEN2", "replyto": "r4BjURAEN2", "signatures": ["ICLR.cc/2026/Conference/Submission12859/Reviewer_XPZa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12859/Reviewer_XPZa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965227329, "cdate": 1761965227329, "tmdate": 1762923651258, "mdate": 1762923651258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}