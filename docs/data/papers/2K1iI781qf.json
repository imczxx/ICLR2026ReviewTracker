{"id": "2K1iI781qf", "number": 11135, "cdate": 1758190257428, "mdate": 1759897605855, "content": {"title": "Dual-Combiner Network with Multi-Attention for Composed Image Retrieval", "abstract": "Composed Image Retrieval (CIR) is a challenging task that aims to retrieve target images based on multimodal queries consisting of a reference image and modifying text. Due to the semantic and modal gaps between images and text, existing CIR methods struggle to accurately compose reference images and modifying text. Although some of these methods can establish fine-grained correspondences between local text tokens and visual regions, they often focus on text-specified content in the reference image, which overlooks the consistency of unmentioned regions with the target image. To address the limitation, we propose a novel Dual-Combiner with Multi-attention (DCMA) network that integrates self-attention, cross-attention, and channel-attention mechanisms to well capture the query intent of users. Specifically, the Global Combiner leverages the multi-attention framework to capture global context information of the query intent of users. In parallel, the Local Combiner is designed to maintain the fine-grained information of reference images in the fused representations for preserving the consistency between the reference and target images. Therefore, the proposed DCMA can precisely encode multi-granularity multi-modal query information into the fused representations by using the multi-attention framework. Extensive experiments demonstrate that DCMA achieves new state-of-the-art results across multiple benchmark datasets, validating its effectiveness in capturing complex multi-modal interactions for composed image retrieval. The source code for this work will be available later.", "tldr": "We propose a Dual-Combiner with Multi-attention network that significantly improves composed image retrieval.", "keywords": ["Composed Image Retrieval;Multi-modal Fusion;Image Retrieval"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d482b6569d91d10a1cf1882e355e454b5adbf225.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Dual-Combiner with Multi-Attention (DCMA), a network for Composed Image Retrieval (CIR), which aims to retrieve images based on a reference image and a textual modification. The proposed method integrates CLIP and SigLIP for local/global feature representations, combining them through self/cross/cannnel attention modules in two parallel combiners."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The authors describe the method architecture in detail.\n- Strong empirical results on two CIR benchmarks."}, "weaknesses": {"value": "## Limited Contribution ##\n- The paper’s main contribution is narrow and insufficiently distinguished from prior work. It overlooks [1], which already introduced cross- and channel-attention modules specifically for CIR. The concluding statement of the Related Work section - \"Moreover, the capability of existing approaches to fuse multi-granular semantic information remains suboptimal\" does not acknowledge this prior work. The authors should clarify how their method conceptually differs from [1], as both aim to enhance bimodal fusion via cross/channel attention, even if the implementations differ.\n\n- The contribution list at the end of the Introduction section is misleadingly structured: the two middle points describe method characteristics rather than genuine contributions, and should be merged with the first item.\n\n- The final claimed contribution “Extensive experiments on standard CIR benchmarks” is inaccurate. 'Shoes' is not a standard CIR benchmark in current literature; more relevant ones are CIRCO, CIRR, and FashionIQ (the latter being included in this paper). I assume the authors know these benchmarks from the papers they cite (in Table 1 for example). This discrepancy is notable given that the proposed method uses general-domain backbones (SigLIP, CLIP) and is not restricted to fashion-specific datasets.\n\nConsidering these issues, the overall novelty and contribution of the paper appear limited.\n\n## Related Work ##\nLine 135 states: \"Chen et al. (2020) proposed the VAL framework, which embeds composite transformers within convolutional neural networks to selectively retain and modify visual content based on textual feedback\". This description reads like a generic LLM-generated summary and does not demonstrate understanding of the referenced work. The \"based on textual feedback\" imply for one of the fundamental use-cases for CIR, so it like saying \"this paper address CIR\". The authors’ explanation adds no meaningful context.\n\n## Paper Writing ##\n- Lines 160–161 contain a contradiction: the authors claim that their approach does not involve training, while Section 3 explicitly describes a training process.\n- Line 048: \"Despite these advances, CIR remains highly challenging due to the inherent semantic and modal gaps between images and text, which hinder accurate modeling of joint retrieval intent\" - this claim lacks supporting evidence or citation, the authors should justify or reference it. Note that Image2Image Retrieval is also challenging, but we can safely say that there is no modality gap between image and image.\n\n\n## Evaluation ##\n- The paper omits a baseline evaluation of the chosen backbones. It is unclear whether performance gains stem from the proposed architecture or simply from using a stronger pre-trained model (SigLIP). Table 1 lists multiple CIR methods based on large vision-language models (e.g., CLIP4CIR, BLIP4CIR) that could potentially surpass the proposed approach if similarly updated to SigLIP (e.g., SigLIP4CIR).\n- Relatedly, the method combines two separate backbones (CLIP and SigLIP) but compares against works using only one. This raises a fairness concern - improvements might primarily arise from increased model capacity rather than the dual-combiner design. The paper would be improved with, for example, a control experiment using a single backbone twice for a fair comparison.\n\n### Minor comments ###\n- Fix spacing before/after citations/periods (e.g., lines 038, 053, 131).\n- The paper inconsistently refers to the same objective as both Batch-Based Classification Loss and Contrastive Loss. The standard term in literature is Contrastive Loss and should be used consistently.\n\n[1] Levy, M., Ben-Ari, R., Darshan, N., & Lischinski, D. (2024). Data Roaming and Quality Assessment for Composed Image Retrieval. Proceedings of the AAAI Conference on Artificial Intelligence, 38(4), 2991-2999."}, "questions": {"value": "- Line 139: “Specifically, the reference image and the modifying text are often not directly related but rather exhibit subtle latent correlations” - not clear. What does it mean? Based on what? Any reference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "afmxpvvlJj", "forum": "2K1iI781qf", "replyto": "2K1iI781qf", "signatures": ["ICLR.cc/2026/Conference/Submission11135/Reviewer_GwVx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11135/Reviewer_GwVx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761399861719, "cdate": 1761399861719, "tmdate": 1762922307402, "mdate": 1762922307402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed DCMA network, with its dual-combiner architecture and multi-attention fusion mechanism, effectively addresses key challenges in the CIR task. The global combiner captures overall contextual semantics, while the local combiner preserves fine-grained visual consistency. The method achieves promising results across multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Leverages the strengths of CLIP (excelling at local details) and SigLIP (excelling at global discrimination) instead of relying on a single model, ensuring the capture of multi-granularity information at the feature extraction level;\n\n2. Designs a multi-attention combiner that progressively fuses self-attention, cross-attention, and channel attention;\n\n3. Introduces no additional trainable parameters (only utilizing pre-trained VLPMs) and requires no fine-tuning of the VLPMs;"}, "weaknesses": {"value": "1. The so-called local-global learning has been extensively studied in multi- and cross-modal learning, such as word-patch or sentence-image alignment. This work does not provide new insights in this regard;\n\n2. Before computing the final loss, the authors design an adaptive fusion mechanism. However, it is unclear why dynamic weighting of global and local similarities is necessary, rather than simply summing or concatenating them. In retrieval tasks, a fixed fusion weight or a static weight learned through a network might already suffice;\n\n3. It is unclear whether the performance improvement of DCMA relies on specific attributes of fashion items, such as regular variations in color, texture, or shape. Would the method be equally effective in more complex and semantically rich scenarios, such as natural images, interior design, or abstract concept retrieval?\n\n4. The paper does not provide any key efficiency metrics for the model, such as inference speed (FPS), parameter count, or FLOPs. In real-world retrieval systems, efficiency is as important as accuracy. It remains unclear whether the performance gains of DCMA justify its additional computational overhead compared to simpler approaches using a single VLPM backbone, such as directly using CLIP or SigLIP with simple fusion."}, "questions": {"value": "See the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iv8pwbhC43", "forum": "2K1iI781qf", "replyto": "2K1iI781qf", "signatures": ["ICLR.cc/2026/Conference/Submission11135/Reviewer_NepZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11135/Reviewer_NepZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560507184, "cdate": 1761560507184, "tmdate": 1762922306747, "mdate": 1762922306747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors focused on composed image retrieval. Considering the existing studies overlooked the consistency of unmentioned regions with the target image, the authors proposed a dual-combiner with a multi-attention network for composed image retrieval. Specifically, the designed method comprises a global combiner and a local combiner, where the former captures global contextual information and the latter extracts fine-grained local details. Experimental results prove the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method integrates multiple fusion strategies to achieve more comprehensive multimodal fusion.\n2. The integration of CLIP and SigLIP enhances the feature extraction capability for both query and target."}, "weaknesses": {"value": "1. The proposed method lacks significant innovation. The integration of local and global modules has been widely adopted in existing literature, as exemplified by CLVC-Net [1] and IUDC [2]. Given this, the method is somewhat incremental.\n[1] Comprehensive Linguistic-Visual Composition Network for Image Retrieval. SIGIR 2021\n[2] LLM-Enhanced Composed Image Retrieval: An Intent Uncertainty-Aware Linguistic-Visual Dual Channel Matching Model. ACM TOIS 2025\n2. All datasets selected for evaluation are confined to the fashion domain, meaning the model’s generalization capability in open-domain scenarios (such as the CIRR dataset [3]) remains unverified.\n[3] Image retrieval on real-life images with pre-trained vision-and-language models. ICCV 2021"}, "questions": {"value": "As listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xnBzztfY64", "forum": "2K1iI781qf", "replyto": "2K1iI781qf", "signatures": ["ICLR.cc/2026/Conference/Submission11135/Reviewer_3VFf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11135/Reviewer_3VFf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918902260, "cdate": 1761918902260, "tmdate": 1762922304838, "mdate": 1762922304838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses Composed Image Retrieval (CIR) by proposing a Dual-Combiner with Multi-Attention (DCMA) architecture. The core idea is to fuse multi-granularity features from two pre-trained vision-language models (intermediate CLIP features as local cues and SigLIP global representations) via two parallel combiners: a global combiner (self-, cross-, and channel-attention to model global context) and a local combiner (fine-grained region–token interaction). Experiments on FashionIQ and Shoes show consistent improvements over a set of prior methods, and ablations claim each module contributes to the gain."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is well-structured, with clear explanations of the methodology, equations, and figures. The problem definition and framework overview are particularly accessible.\n\n2.The integration of CLIP and SigLIP for complementary feature extraction, combined with a dual-combiner architecture and multi-attention fusion. The progressive fusion strategy effectively bridges modal gaps and captures both local and global semantics."}, "weaknesses": {"value": "1. The proposed architecture shows limited methodological novelty. Both the Local Combiner and the Global Combiner share almost identical structures, differing mainly in their input features—one derived from CLIP and the other from SigLIP. Conceptually, the design can be interpreted as performing two rounds of inference using different backbone encoders, followed by a weighted similarity aggregation. As such, the method appears to be an architectural reuse rather than a fundamentally new compositional learning framework.\n\n2. The experiments are restricted to fashion-domain datasets (FashionIQ and Shoes) and do not include evaluations on more general-purpose or open-domain benchmarks such as CIRR [1], which is a standard dataset for assessing compositional image retrieval performance under real-world and diverse scenarios. The absence of such experiments raises concerns about the proposed method’s generalization ability to open-domain or real-world retrieval tasks.\n\n3. The use of dual-model backbones (CLIP and SigLIP) combined with multiple attention modules likely introduces substantial computational overhead. However, the paper does not provide any analysis of inference speed, memory footprint, or computational complexity. Since deployment efficiency is crucial for practical retrieval systems, the lack of this analysis undermines the paper’s claims of applicability in real-world scenarios.\n\n4. The paper lacks sufficient visual analysis to support the quantitative results. For instance, retrieval visualizations are not provided, and Figure 3 omits explicit marking of the ground-truth target image, which weakens the interpretability and persuasiveness of the results.\n\n5. The internal design of the proposed combiners is not empirically analyzed. There is no ablation experiment to examine the contribution of each attention component or fusion module, leaving the rationale for the current structural choices unexplained.\n\n6. In Section 4.3, the authors mention replacing the “multi-attention combiner with a simple feature combination strategy” for comparative analysis, but the paper does not clarify what the “simple feature combination” specifically entails. The lack of detailed description prevents a fair and reproducible comparison.\n\n7. In Table 3, given that both combiners share an identical structure, the difference between “Bi w/o local combiner” and “Bi w/o global combiner” is essentially the use of different backbone encoders. The observed performance gains thus seem more attributable to the backbone change rather than the architectural design itself, suggesting that the reported improvement might stem from stronger encoders instead of the proposed technique.\n\n8. What would be the result if the Local Combiner used the final output features of the CLIP model instead of its intermediate layer features?\n\n[1] Liu, Z., Rodriguez-Opazo, C., Teney, D., Gould, S., 2021b. Image retrieval on real-life images with pre-trained vision-and-language\nmodels, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2125–2134."}, "questions": {"value": "Please the part of \"Weakness\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Fzc6QMBF3P", "forum": "2K1iI781qf", "replyto": "2K1iI781qf", "signatures": ["ICLR.cc/2026/Conference/Submission11135/Reviewer_6FSh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11135/Reviewer_6FSh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983070454, "cdate": 1761983070454, "tmdate": 1762922304340, "mdate": 1762922304340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}