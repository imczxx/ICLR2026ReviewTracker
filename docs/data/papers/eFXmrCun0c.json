{"id": "eFXmrCun0c", "number": 11087, "cdate": 1758188893445, "mdate": 1763733634058, "content": {"title": "Group-Normalized Implicit Value Optimization for Language Models", "abstract": "Fine-tuning Large Language Models (LLMs) with reinforcement learning (RL) has become a key technique for enhancing performance on a wide range of tasks, from user alignment to complex reasoning. However, this approach is often hindered by the difficulty of fine-grained credit assignment, as it typically relies on sparse rewards given only at the end of a completely generated sequence. Conventional solutions often require training an auxiliary value network known as critic, which introduces significant computational overhead and training instability. We present Group-Normalized Implicit Value Optimization (GN-IVO), a novel, critic-free algorithm that directly addresses this challenge. GN-IVO learns step-level values implicitly from the policy through a group-normalized distributional matching objective. This approach elegantly circumvents the need for an explicit critic and avoids the computation of the intractable partition function by normalizing values across a group of sampled model responses. Theoretically, we prove that our objective recovers the true value function up to a constant, guaranteeing that the optimal policy is preserved. We demonstrate the practical effectiveness of GN-IVO on a diverse set of text generation and reasoning tasks, showing that it consistently outperforms strong RL baselines for LLMs.", "tldr": "", "keywords": ["LLM post-training"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6b153c30685b18b2f985e5209aef615c0b48e11.pdf", "supplementary_material": "/attachment/374ed1c94156a926d9bc661d8b3a1e4533a79078.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces Group-Normalized Implicit Value Optimization (GN-IVO), a critic-free policy-gradients loss to optimize the KL-regularized RL objective common in LLM post-training at the token level. They achieve this by first defining a value function as an expectation of the reward over possible continuations of a sequence, and 1) using a list-ranking loss to approximate the relative ordering of the values in a group of responses to get rid of the intractable partition function, and 2) using the DPO trick of using the new and old policy log-likelihood ratios as an estimator of the value function.  The main advantage of the technique is that it provides an explicit-credit assignment distribution of the reward over specific tokens. The approach is validated in mathematical reason and text generation benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- S1) The paper tackles an important problem when seeking to improve sampling efficiency by explicitly computing the credit-assignment distribution over the individual actions.\n- S2) The method is solidly motivated\n- S3) The method is evaluated on 2 different kinds of tasks showing consistent improvements."}, "weaknesses": {"value": "- W1) While the paper considers a wide range of baselines, I find that the unbiased version of GRPO --Dr. GRPO-- is prominently missing.\n- W2) I'm not convinced on the performance metrics of section 4.2. The authors use avg. reward as a metric in different LLM post-training task, but this is not necessarily the same as good performance. For example, once can have a collapsed model that outputs high-reward samples according to the reward function, but where the LM could have degenerated in various ways. The alpha parameter is controlling for this trade-off which is particularly important for the experiments used in Section 4.2. Better metrics could be an LLM judge and/or KL divergence to the target goal probability (Eq. 2).\n\nI'm happy to revise my score upwards if these concerns and other concerns described in the questions below are addressed."}, "questions": {"value": "- Q1) I'm not sure I understand the need to start from Eq. 7. Couldn't have you derived your objective from $CE(V, V_\\psi)$ where the cross-entropy is computed across a batch of K samples?\n- Q2) Theorem 3.2: \"equivalently...\", I think there is a typo in $yt$, and should be $y<t$, right?\n- Q3) In the loss of GN-IVO, you write that t is sampled from $U\\{1...T\\}$. Why not directly a sum? And how many t's you sample in practice?\n- Q4) What is the effect of not normalizing $\\exp(R/\\alpha)$? I wonder if the normalization induces some bias in the algorithm?\n- Q5) Sorry if I missed this, but what value of $\\alpha$ are you using in your experiments?\n- Q6) GRPO has been observed to generate less-diverse models (e.g., https://arxiv.org/abs/2506.02355v1). Is it the case for models trained using your proposed loss? To see this, you could report, say, Pass@128. Less-diverse models struggle at leveraging the additional sampling budget.\n- Q7) I'm surprised by what you call \"rejection sampling\". For me, rejection sampling would be generating a bunch of candidates, retaining only the high reward ones, and do SFT on those (e.g. https://arxiv.org/abs/2203.14465)\n- Q8) L419: What does a single-step objective mean in your case? You say $T=1$, but that reads as if there would be just one token to me.\n- Q9) I'm not sure I agree with the discussion on the temperature coefficient in section 4.3. \"We observe that the model trains effectively with lower temperature values (α = 0.1, 0.2). In contrast, for higher values such as α = 0.5 and 1.0, learning is significantly slower, and the models ultimately achieve lower performance. This is because a more uniform target distribution provides a weaker and less discriminative training signal, making it difficult for the policy to distinguish superior from mediocre responses.\" => As noted in Eq. 2, different values of alpha correspond to different target distributions and so, higher values of alpha correspond to lower expected reward. Couldn't the effect of $\\alpha$ be explained by different target distributions instead of different learning dynamics?\n- Q9) Proof of Theorem 3.1: why the integrals? These are discrete values...\n- Q10) How does this distribution matching approach differ from others such as https://arxiv.org/abs/2302.08215, https://arxiv.org/abs/2012.11635 or https://arxiv.org/abs/2205.14219 ?\n- Q11) I think your internal value-ranking loss is equivalent to ListNet: from \"Learning to Rank: From Pairwise Approach to Listwise Approach\". (https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf) Do you agree?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yrOZvi8tLj", "forum": "eFXmrCun0c", "replyto": "eFXmrCun0c", "signatures": ["ICLR.cc/2026/Conference/Submission11087/Reviewer_7Q1n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11087/Reviewer_7Q1n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761221827081, "cdate": 1761221827081, "tmdate": 1762922267069, "mdate": 1762922267069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Group‑Normalized Implicit Value Optimization (GN‑IVO), a critic‑free RL fine‑tuning algorithm for LLMs. Starting from a KL‑regularized objective, the authors prove an explicit link between the prefix policy and a soft value. Then train values via a group‑normalized distributional matching objective, and finally eliminate the explicit value network."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper proposes a new loss that induces token/step‑level credit assignment without a critic by normalizing over groups of sampled completions and working with prefix policy ratios rather than explicit value estimates or partition functions.\n\nTheorems 3.1–3.2 give a principled derivation: (i) an explicit policy–prefix‑value relationship; (ii) a consistency result that the group‑normalized objective recovers $V$ up to a constant; and (iii) policy invariance to additive shift.\n\nThe experiments are extensive and span both reasoning and open‑ended generation with multiple backbones."}, "weaknesses": {"value": "The related‑work and experiments omit PRIME, which also leverages the policy to construct dense objectives. Although PRIME lacks the theoretical guarantees you present, it is methodologically close in spirit (dense reward objective constructed based on the implicit representation of the policy) and would be an informative reference and empirical comparison.\n\n\nThe objective presented use a uniform distribution sampling mechanism to compute the token level objective. Typically, the loss functions in RL average the loss including all tokens. It is not clear why this common approach is not used. Does this decision affect training? Can the author provide experiments results around this?"}, "questions": {"value": "Is there some kind of gradient detatch in the new loss function?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9iWfeWEoj4", "forum": "eFXmrCun0c", "replyto": "eFXmrCun0c", "signatures": ["ICLR.cc/2026/Conference/Submission11087/Reviewer_SjqK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11087/Reviewer_SjqK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909370997, "cdate": 1761909370997, "tmdate": 1762922266704, "mdate": 1762922266704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a distribution matching approach to train a policy where the actions are tokens/steps in a natural language response. To achieve this, the authors generate multiple responses given a query and compute the reward  of each of the responses. A softmax over the rewards is computed to define the target distribution. A network is trained to predict the value at each token/step for each of the generated outputs. A softmax is applied over the outputs of the value network and cross-entropy loss between the softmaxed values and softmaxed rewards is minimized.\n\nThe authors show that the value function at any time-step can be parameterised using the ratio of \\pi_\\theta and \\pi_old of the pastial sequence till that step. This allows training of the policy whose actions are tokens or steps without actually learning a value function or a stepwise reward model.\n\nThe results show reasonable performance improvement on Llama as well as Qwen models for Math as well as text benchmarks"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The primary novelty of the paper lies in using the relationship between the soft value function and prefix probability ratios for directly training the policy without the intermediate value function. \n- The writing is crisp and clear. \n- The results show reasonable gains on Math as well as general text generation benchmarks."}, "weaknesses": {"value": "- The paper motivates fine-grained credit assignment, but Equation (9) assigns the same scalar return to every prefix of a sampled completion. \n   - For example, if output y(1) earns higher terminal reward than y(2), the loss in (9) gives every prefix y(1)_{<t} higher weight than the same-length prefix y(2)_{<t}, regardless of which step actually caused the outcome difference. This is a Monte Carlo, every-visit return broadcast uniformly along the trajectory, not step-level credit assignment. \n   - Accordingly, the claim that the method “solves credit assignment without a critic” is overstated. This does not imply the derivation around Eq. (7) is incorrect. Using a single sampled continuation to form a one-sample estimate of the soft value V(x, y_{<t}) is standard and, in expectation, corresponds to a weighted cross-entropy / distributional matching objective between the true and learned values. The issue arises when that one-sample value proxy is used directly to train the policy in Eq. (9): without a time-varying baseline/critic, the update collapses to sequence-level weighting replicated at all timesteps, providing no within-trajectory credit differentiation.\n\n- Many of the ideas in the paper have been explored in some form or the other in other papers:\n   - Distribution matching of self-normalized probability ratios with softmaxed rewards has already been explored in [1] (check equation 15 and equation 21)\n   - The relationship between the soft value function and the probability of prefix of a trajectory has been explored in Eqs. (23)–(25) and (30)–(32) of [2].\n  - The novelty of this current paper is to use the above relationship for directly training the policy without the intermediate value function.\n- There are other works such as VinePPO[3] and SPO[4] that do Monte-Carlo simulations to estimate value functions without training a critic. These works are worth mentioning.\n\n[1] BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback\n[2] Revisiting Maximum Entropy Inverse Reinforcement Learning: New Perspectives and Algorithms\n[3] VinePPO \n[4] SPO: Segment Policy Optimization"}, "questions": {"value": "- From Eq. (7) to Eq. (9): When you use the one-sample value estimate directly to weight policy updates at every timestep, what prevents the objective from collapsing to sequence-level weighting replicated across timesteps? Is there any mechanism that yields differential credit across steps of the same trajectory?\n- The authors introduce a softmax after equation (9) claiming that it adds stability. Why wasn't it introduced in equation (7) itself in their intuitive objective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LDD9X4rDio", "forum": "eFXmrCun0c", "replyto": "eFXmrCun0c", "signatures": ["ICLR.cc/2026/Conference/Submission11087/Reviewer_qVzy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11087/Reviewer_qVzy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010416124, "cdate": 1762010416124, "tmdate": 1762922266310, "mdate": 1762922266310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Reinforcement Learning has become central to LLM tuning and therefore RL research in this area has experimented a lot of interesting developments. To include some, the application of PPO to LLM RLHF, DPO and recently GRPO application to RLVR. This last one avoids the need for training a value network of PPO, leveraging instead empirical estimates using rollouts. Importantly, GRPO directly backpropagates the signal provided by the reward at the end of generation (thinking and answer) and has therefore no explicit credit attribution i.e. does not attempt to determine which steps (token generation, thinking step generation) have more relevance for the final reward observed.\n\nThe method proposed here also provides a well justified method to avoid having to train a separate value function while still obtaining step-level credit attribution. It also relies on empirical estimates as in GRPO.\n\nThe main idea is based on a learning objective that attempts to learn the value function from rollouts. \n\n1. It proposes for this a distribution matching approach that can be seen as a weighted cross entropy loss between two energy models (Eq 7). One is constructed from the true value function and another is our parametrized value function that we want to learn. \n\n2. In a similar vein as DPO, the optimal policy is here leveraged to express the parametrized value function in terms of the current and old policy, both eliminating the need for a separate network and providing a connection with the LLM that we want to train. This requires the formulation of the optimal policy at step level"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The method seems, to the best of my understanding, correctly derived from well known principles and yields a very interesting formulation that achieves credit attribution while avoiding a separate value network and incorporating empirical estimates as GRPO. \n\n2. Experimental setup is convincing and the results are strong across multiple categories including math and text generation. They also provide sensible baselines that also achieve step-level credit attribution through other formulations."}, "weaknesses": {"value": "Only minor: some more results would further strengthen the paper i.e. having results for step-level vs non step-level for math would be interesting. For math, authors defined a step is as a reasoning step so having these results would be relevant. \n\nAlso since the samples are here used to describe distributions it is interesting to see if this makes it more dependent on a high K than e.g. GRPO. This could be added to the results in Figure 2."}, "questions": {"value": "1. What are the one-step results for the math datasets in Table 1? why not include these since they are included in the rest of the experiments (maybe I missed something). Also what is the effect of K on alternative methods like GRPO?.\n\n2. Please also explain how the findings and stated theorems relate to the findings of [2]. In particular how Theorem 3.1 relates to the result from (Ziebart, 2010) used in Eq 5.\n\n[2] From r to Q∗: Your Language Model is Secretly a Q-Function"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ECj9rVM3nY", "forum": "eFXmrCun0c", "replyto": "eFXmrCun0c", "signatures": ["ICLR.cc/2026/Conference/Submission11087/Reviewer_mHf2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11087/Reviewer_mHf2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762100514130, "cdate": 1762100514130, "tmdate": 1762922265874, "mdate": 1762922265874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}