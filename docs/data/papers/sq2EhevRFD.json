{"id": "sq2EhevRFD", "number": 15329, "cdate": 1758250324046, "mdate": 1759897313292, "content": {"title": "Eliminating Steady-State Oscillations in Distributed Optimization and Learning via Adaptive Stepsize", "abstract": "Distributed stochastic optimization and learning is gaining increasing traction due to its ability to enable large-scale data processing and model training across multiple agents without the need for centralized coordination. However, existing distributed stochastic optimization and learning approaches, such as distributed SGD and their variants, generally face a dilemma in stepsize selection: a small stepsize leads to low convergence speed, whereas a large stepsize often incurs pronounced steady-state oscillations, which prevents the algorithm from achieving stable convergence accuracy. In this paper, we propose an adaptive stepsize approach for distributed stochastic optimization and learning that can eliminate steady-state oscillations and ensure fast convergence. Such guarantees are unattained by existing adaptive stepsize approaches, even in centralized optimization and learning. We prove that our proposed algorithm achieves linear convergence with respect to the iteration number, and that the convergence error decays sublinearly with the batch size of sampled data points. In the specific case in terms of deterministic distributed optimization with exact gradients accessible to agents, we prove that our proposed algorithm linearly converges to an exact optimal solution. Moreover, we quantify that the computational complexity of the proposed algorithm is on the order of $\\mathcal{O}(\\log(\\epsilon^{-1}))$, which matches the existing results on adaptive stepsize approaches for centralized optimization and learning. Experimental results on machine learning benchmarks confirm the effectiveness of our proposed approach.", "tldr": "We propose an adaptive stepsize approach that  eliminates steady-state oscillations and ensures fast convergence in distributed  learning and optimization.", "keywords": ["Distributed learning", "linear convergence", "adaptive stepsize"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ca5c58c093729f0c8c7394ad51ab494b11eb29a9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies adaptive algorithms in federated learning where there is no central server involved in the stepsize scheduling. They call this a fully decentralized setting. Then, they provided an adaptive stepsize algorithm which is claimed to eliminate steady-state oscillations. Numerical experiments are performed to validate the results."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The fully decentralized stepsize scheduling seems to be a new setting. In this setting, the authors provided a method and showed linear convergence under some circumstances."}, "weaknesses": {"value": "First of all, I find the motivation of this setting quite weak. In the stepsize scheduling, the number of bits to communicate is usually a small constant (compared to communicating a full gradient). Therefore, even in a decentralized network, this usually takes much less time than the gradient gossipings. I cannot see any practical reasons why the (global) stepsize coordination takes too much of communication time. \n\nMoreover, the theorem statements in the paper did not provide good convergence guarantees. For instance, in eq. (2) in Theorem 1, this is not even a convergence result with a non-vanishing term of $O (\\frac {\\sigma^2} {|\\mathcal B|})$. \n\nI feel very suspicious about the correctness of the main technical results. For instance, in Corollary 1, the authors claimed to get an $\\epsilon$-solution in $O ((|\\mathcal B| + M) \\log \\frac 1 \\epsilon)$ gradients. This claim also does not follow from Theorem 1."}, "questions": {"value": "See the “weakness” paragraphs above. I believe that the setting of fully decentralized stepsize scheduling is not well motivated and the main technical claims in the paper are wrong."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9wFsbpTeqR", "forum": "sq2EhevRFD", "replyto": "sq2EhevRFD", "signatures": ["ICLR.cc/2026/Conference/Submission15329/Reviewer_WeYr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15329/Reviewer_WeYr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760642213992, "cdate": 1760642213992, "tmdate": 1762925624818, "mdate": 1762925624818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work solves the problem of stepsize selection in distribution optimization. For this purpose, the authors propose Algorithm which automatically sets the value of stepsize and prove the convergence guarantee for this algorithm. Algorithm 1 achieves comparable convergence rate with the centralized optimization. Experimental results confirm the effectiveness of Algorithm 1."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work studies an important problem of selecting stepsize to avoid both slow convergence and steady-state oscillations.\n\n2. The authors provide stong theoretical guarantee for the proposed algorithm.\n\n3. The experiment results (such as Figure 1) demonstrate the claims of this work."}, "weaknesses": {"value": "1. Hyperparameter tuning: While automatically setting the stepsize, Algorithm 1 introduces additional parameters including $\\beta$ $r$ and $M$, requiring futher tuning. The experiments in Figure 3 cannot solve this problem, as training with a batch size of 128 on the MNIST dataset is a very stable process\n\n2. Lack of large-scale experiments: The experiments use five nodes and a specific data distribution, which may be not practical. It is hard to see if the results can generalize to other setups.\n\n3. If I understand correctly, Theorem 1 requires $M > M_0$  inner-consensus-loop iterations for convergence. It is hard to estimate the value of $M_0$. If this value is very large, the theoretical gurantee does not hold in practice."}, "questions": {"value": "1. How to select the values of $\\beta$ $r$ and $M$ in applications?\n\n2. How do Algorithm 1 compare with baselines in a large-scale setup and different data distributions?\n\n3. How to select coupling weight W? Is the choice in Section 6 manually tuned or randonly set?\n\n4. In Figure 1(f), why does the Stepsize decrease suddenly? Is this phenomenon met in every running of CIFAR10?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "moYwIciNQx", "forum": "sq2EhevRFD", "replyto": "sq2EhevRFD", "signatures": ["ICLR.cc/2026/Conference/Submission15329/Reviewer_WeSj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15329/Reviewer_WeSj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652566842, "cdate": 1761652566842, "tmdate": 1762925623969, "mdate": 1762925623969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an adaptive stepsize approach for distributed stochastic optimization and learning, which can eliminate steady-state oscillations and ensure fast convergence. For the deterministic gradients, the authors prove convergence to an exact optimal solution, whereas for stochastic gradients, they establish linear convergence with respect to the iteration number and prove that the convergence error decreases sublinearly with the batch size of sampled data points. The convergence results are reasonable to me.\n\nIn my opinion, the most interesting property of the proposed adaptive stepsize approach is that it allows the stepsize to be large in the early stages of the algorithm to accelerate convergence, while rapidly decreasing to a small value near the optimal solution to ensure stable convergence performance. This property appears to provide a \"stop signal\" for distributed optimization algorithms. If so, it would be useful in real-world distributed optimization applications, as most existing algorithms lack a clear criterion for determining when to stop.\n\nNevertheless, I have some concerns about this paper, which are summarized below:\n\n(i) Lack of discussion on nonconvex settings (see Weakness 1 for details); \n\n(ii) Limited variety of experimental tasks (see Weakness 2 for details);"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and easy to follow. The convergence proofs are complete and rigorous. In addition, the proposed adaptive stepsize approch provides an easily identifiable stop signal for distributed algorithms (for example, when the stepsize falls below a given threshold), which is of practical significance."}, "weaknesses": {"value": "1. Lack of discussion on nonconvex settings: Although the experimental results demonstrate the effectiveness of the proposed approach in nonconvex settings, it would be helpful if the authors could elaborate on how the current theoretical results might be extended to nonconvex objective functions.\n\n2. Limited variety of experimental tasks: Aside from logistic regression (in Appendix C), the experiments mainly focus on image classification tasks. To more comprehensively evaluate the effectiveness of the proposed approach, it would be beneficial to test it on other types of machine learning tasks, e.g., distributed training of recurrent neural networks (RNNs) for next-character prediction tasks.\n\nIf the authors could address Weaknesses 1 and 2, I would consider raising the score."}, "questions": {"value": "See the weaknesses above. In addition, I have the following questions:\n\n1. In Algorithm 1, the parameter $\\beta$ is set as $\\beta \\in (0, 1.36)$. However, if $\\beta < 1$, the stepsize in Algorithm~1 Line 16 would decay exponentially, which may prevent the algorithm from achieving convergence. I also observed that Eq. (43) in Appendix uses $\\beta \\in (1, 1.36)$. Please clarify the range of $\\beta$ to ensure consistency.\n\n2. In the CIFAR-10 experiment, Algorithm 1 does not appear to achieve stable convergence. Therefore, the authors should increase the number of iterations to more clearly show the convergence trend.\n\n3. The use of the iterates $y_{i,2}^{t}$ requires further explanation. It seems to be an additional variable compared with conventional gradient-tracking approaches. \n\n4. There are some typos, e.g.,  (i) under Eq. (7) in Appendix, $||\\frac{1}{m}\\sum_{i=1}^m a_i||^2 \\leq \\frac{1}{m}\\sum_{i=1}^m ||a_i||^2$ should hold for any vector $a_i \\in \\mathbb{R}^n$ rather than $a_i \\in \\mathbb{R}$; and (ii) in Eq.(75), the closing bracket \"]'' should be replaced by \"\\}''."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "W0aHzhJN5X", "forum": "sq2EhevRFD", "replyto": "sq2EhevRFD", "signatures": ["ICLR.cc/2026/Conference/Submission15329/Reviewer_snAG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15329/Reviewer_snAG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702677378, "cdate": 1761702677378, "tmdate": 1762925623554, "mdate": 1762925623554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new algorithm with an adaptive step size for decentralized optimization problems. The authors provide convergence theorems and experimental results for this algorithm.\n\nThis paper should be rejected for the following reasons: (1) It contains false theorems and (2) It uses mathematical statements without proof."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors research the interesting scientific problem - the adaptive step size for decentralized optimization."}, "weaknesses": {"value": "To prove the rejection, I provide the following arguments.\n\n**Main arguments**\n\n1. Theorem 2 contains a false statement about the convergence of the proposed Algorithm ($ \\mathbb{E}||x_i^T - x^*||^2 \\leq \\mathcal{O}(\\gamma^T)$, where $\\gamma = \\max (1 - \\frac{\\mu^2}{4L}, \\frac{91}{92})$), because if this statement were true, we would get any convergence by rescaling. Moreover, this result contradicts standard lower bounds for $L$-smooth and $\\mu$-strongly convex functions with $n=1$ and $\\sigma = 0$.\n\n2.  P. 16 line 858. The authors used the fact that $L_i^t \\leq L$, where $L_i^t$ is defined in Algorithm 1 and $L$ is defined in Assumption 1. Can you provide proof for this fact?\n  \n3. P. 20 line 1034. I can't understand this transition. Can you clarify it?\n\n4. p. 20 last inequality. It is not obvious why we can always choose such parameters for all $i$.\n\n5. Also, the relationship (27)  is not obvious.\n\n**Minor comments**\n\n1. How do we define $L_{i}^{t+1}$ if $x_{i}^{t+1} = x_{i}^t$?\n\n2. P. 17 eq. (14). It seems like this equation contradicts Assumption 1."}, "questions": {"value": "I provided questions in the weakness section.\n\n**Things to improve the paper that did not impact the score:**\n\nProvide theoretical guarantees for each statement in this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "auaJyhdQJm", "forum": "sq2EhevRFD", "replyto": "sq2EhevRFD", "signatures": ["ICLR.cc/2026/Conference/Submission15329/Reviewer_6Yxs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15329/Reviewer_6Yxs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762349317585, "cdate": 1762349317585, "tmdate": 1762925622872, "mdate": 1762925622872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}