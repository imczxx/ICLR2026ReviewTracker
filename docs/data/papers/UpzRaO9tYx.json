{"id": "UpzRaO9tYx", "number": 17667, "cdate": 1758278995292, "mdate": 1759897161777, "content": {"title": "Learning fair latent representation with Multi-Task Deep Learning", "abstract": "The problem of group-level fairness in machine learning has received increasing attention due to its critical role in ensuring the reliability and trustworthiness of models deployed in sensitive domains. Mainstream approaches typically incorporate fairness by enforcing constraints directly within the training objective. However, treating fairness solely as a regularisation term can lead to suboptimal tradeoffs with loss of accuracy or insufficient fairness guarantees. In this work, we propose a novel approach that formulates fairness as an auxiliary task in a Multi-Task Learning (MTL) paradigm. In contrast to embedding fairness constraints into a single-task objective, explicitly modelling the problem as multi-objective optimisation (MOO) allows to decouple the learning of a fair internal representation from the optimisation of the predictive task: these two conflicting objectives are optimised concurrently. We introduce two novel fairness loss functions that are better tailored to an MTL approach. We provide a theoretical analysis of the generalisation properties of the proposed approach. The experimental analysis on benchmark datasets shows that in spite of not embedding a fairness loss function directly on the predictive task the MTL formulation consistently improves group-level fairness metrics compared to both standard regularisation-based methods and other MTL architectures, while maintaining competitive predictive performance", "tldr": "", "keywords": ["Multi-task Deep Learning", "Auxiliary Task", "Fairness", "Multi-Objective Optimisation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e67bac23681c97b55485a0527344220f168d8cb6.pdf", "supplementary_material": "/attachment/70a2674e84176f079aabda8f72ef9b2d871ee495.pdf"}, "replies": [{"content": {"summary": {"value": "This work aims to learn fair latent representation through multi-objective optimisation in a multi-task learning (MTL) paradigm. As such, two novel fairness loss functions are proposed for model training under the MTL approach. A theoretical analysis of the generalisation properties and empirical results supports the proposed method. \n\nOverall, the MTL paradigm shows existing work. However, it is unclear if the proposed loss term is effective (compared to BGL). Most improvement comes from the MTL paradigm. The authors are encouraged to clarify this concern."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work considers fair representation as a multi-task learning problem, where group fairness is regarded as an auxiliary task to augment the main prediction task. Under this framework, two new fairness-oriented loss functions are proposed to guide model optimisation. The latest loss function aims to minimise the loss between different sensitive groups and promote equitable treatment across sensitive groups.\n- This work provided a theoretical analysis to show how multi-task objective provably reduces the actual group fairness gap via explicit generalisation bounds. It also establishes rigorous optimisation-to-fairness guarantees in a multi-task learning paradigm. I did not find obvious errors in the theoretical proof. \n- Code is provided (anonymously) for review and will (probably) be made available in the future."}, "weaknesses": {"value": "According to the empirical results, the adaptation of MTL for fair latent representation learning is effective. However, it is not entirely clear how the two loss terms, GLD and GLF, contribute significantly to MTL. The difference in performance is significantly smaller than the SD. The author is encouraged to conduct a statistical analysis to justify the proposed loss term."}, "questions": {"value": "- Please discuss why in COMPAS, the performance of all three compared fairness losses are identical.\n- In the experiment, models from the best-performing epoch are selected for results reporting. Can the author confirm if the training stabilises at certain epochs, or experiences high fluctuation? Also, do you know if the selected epoch is consistent across the 10-fold validation? It would be problematic if the most optimal performance (in terms of epoch) is chosen on each hold-out set. \n- Missing related work: (1) Fair Representation: Guaranteeing Approximate Multiple Group Fairness for Unknown Tasks, TPAMI 2023, (2) FARE: Provably Fair Representation Learning with Practical Certificates, ICML 2023.\n\nMinor comments:\n- In Fig. 5, the colour label between the female and male groups is very close; it is hard to interpret the region in the plot.\n- Table 1 is overly small for print.\n- In Fig. 3 (supplementary material), the label is unreadable. Authors should always consider readability in print.\n- The citation is not appropriately referenced in the main text. \n- 10-fold repeated hold-out protocol -> 10-fold cross-validation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KpjngMki9n", "forum": "UpzRaO9tYx", "replyto": "UpzRaO9tYx", "signatures": ["ICLR.cc/2026/Conference/Submission17667/Reviewer_6fnZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17667/Reviewer_6fnZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761209441787, "cdate": 1761209441787, "tmdate": 1762927518735, "mdate": 1762927518735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents fairness training with Multi-task learning (MTL). Compared to single-task learning, the author claims that using fairness loss in MTL can achieve a better trade-off performance. The author proposed two fairness losses as different tasks to train the backbone model, such that the group performance can be more even. The methods are being tested on the benchmark fairness-related research widely accepted datasets, and the author claims that their method outperforms the baselines."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. MTL has recently been a hot research topic.\n\n2. This paper has made a great effort to find a theoretical analysis of the generalization properties of MTL.\n\n3. Experiments result are trustworthy."}, "weaknesses": {"value": "I have read this paper at least three times.\n\nUnfortunately, this paper has a fundamental flaw in modeling fairness. To put it briefly, this paper underestimated the approach to achieving fairness. **Making the losses from different demographic groups equal or similar is not an appropriate way to ensure fairness.**\n\nNo matter from Equation (5), Equation (6), or line 274, we can see that the authors are trying to make different sensitive groups' losses similar, e.g., the performance of males and females to be the same. However, one of the major causes that forms fairness issues is spurious correlation. A spurious correlation occurs when the task label correlates with the sensitive attribute but has no causal effect. A very famous example is Waterbirds. The task is to identify the type of bird (waterbird or landbird), and the sensitive attribute is the background (water background and land background). The spurious relationship is that the waterbird with a water background has a much higher chance than the waterbird with a land background, and vice versa. Because there are data imbalance issues within a sensitive attribute, if we use the loss function presented in equation (5), which asks for the performance of the water background and the land background show the similiar loss, the advantage groups (waterbird with water background and landbird with land background) will dominate and achieve similiar good perfornace, and two disadvantage groups (waterbird with land background and landbird with water background) show similar bad performance. But it also meets the constraints that two sensitive groups have similar losses. If you compute Equal Opportunity Difference, there would be a large value. \n\nThat explains why the results from Table 1 show very limited or no improvement compared with the very weak baseline Single-task Classification (STL)\n\nI would highly recommend that the author read this section and read papers like GroupDRO to refine their methods. A straightforward way to modify your methods is to define the group as the Cartesian product between the sensitive attribute and the target attribute. \n\nBeyond the fundamental flaw, I would also like to point out an issue in the theorem section. Since your claim is that fairness in MTL is better than in STL with a regularizer, I would like to see a theoretical aspect that proves this point.\n\nIn addition, this method shows no advantage over existing approaches in terms of annotation requirements. For example, it still requires both target labels and sensitive labels, and it also needs to be trained from scratch. Therefore, the motivation for accepting this paper is reduced, especially considering that there is already an abundance of research on fairness."}, "questions": {"value": "I would like to apologize for not being able to vote to accept this paper, but I hope my comments can help improve your submission in the future. I don't have more questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n9HaUZe06r", "forum": "UpzRaO9tYx", "replyto": "UpzRaO9tYx", "signatures": ["ICLR.cc/2026/Conference/Submission17667/Reviewer_7nuJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17667/Reviewer_7nuJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786997282, "cdate": 1761786997282, "tmdate": 1762927518280, "mdate": 1762927518280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates fairness in machine learning by re-casting group-level fairness mitigation as an auxiliary task within a multi-task learning (MTL) framework. Rather than embedding fairness constraints solely as regularizers in the loss of a single task, the authors propose to decouple fair representation learning and prediction through concurrent, parallel optimization. The paper contributes two new fairness loss functions tailored for MTL auxiliary heads, provides generalization and fairness gap bounds connecting empirical and population objectives, and compares the proposed approach theoretically and empirically to regularization-based and adversarial methods across standard tabular and image benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The central idea—framing fairness as an explicit auxiliary task in an MTL paradigm instead of as a loss regularizer—offers a clear perspective shift that addresses known trade-offs and optimization pitfalls in fairness-regularized learning. \n\n2. Two new fairness-oriented loss functions (Group Loss Fairness [GLF] and Group Loss Divergence [GLD]) are constructed to operate independently of prediction heads; both are thoroughly defined (Equations 5–7, Pages 5–6) and well-motivated for MTL settings.\n\n3. The experimental evaluation covers multiple standard tabular datasets (Adult Income, COMPAS, Bank Marketing) and one image dataset (MTF), using both binary and multi-group sensitive attributes. Results demonstrate the scalability and comparable or improved fairness outcomes of the proposed method versus established regularization and adversarial baselines."}, "weaknesses": {"value": "1. **Missing Discussion of Directly Related MTL–Fairness Trade-off Papers**:\nThe literature review lacks a direct discussion and comparison with several closely related studies, particularly [1], which also addresses fairness through an MTL framework and appears to incorporate an additional fairness loss component (see Figure 4 in [1]). The authors should provide a more thorough discussion of how their method differs from or improves upon such prior work, ideally accompanied by empirical comparisons to clarify the relative advantages and limitations.\n\n2. **Limited Explanation of Results Beyond Pareto and Aggregate Metrics**:\nAlthough Figure 2 presents a favorable Pareto front, the accompanying discussion provides limited insight into the practical differences between the two proposed fairness losses—GLF and GLD. It remains unclear whether one loss consistently outperforms the other, or under what data conditions specific trade-offs emerge. Neither Table 1 nor the results narrative clarifies which component contributes most to the reported fairness gains. More detailed ablation studies or per-dataset analyses would strengthen the empirical evidence and help contextualize the observed improvements.\n\n3. **Lack of Computational Cost Analysis**:\nThe paper omits a quantitative analysis of training efficiency. The added MTL heads and parallel objectives likely increase time, memory, and convergence cost, but these trade-offs are not reported.\n\n4. **Unclear Presentation of Key Results**:\nSome key results are difficult to interpret. For instance, the text and labels in Figure 2 and Table 1 are too small, making it hard to clearly compare different methods.\n\n5. **Missing References to Advanced MTL Methods**:\nThe paper aims to address fairness using an MTL framework but lacks discussion of recent advanced MTL methods [1,2,3]. Including and positioning the proposed approach relative to these works would improve clarity and strengthen the paper’s contribution.\n\n[1]. Understanding and Improving Fairness-Accuracy Trade-offs in Multi-Task Learning. KDD 2021\n\n[2]. Fair resource allocation in multi-task learning. ICML 2024\n\n[3]. Revisiting Fairness in Multitask Learning: A Performance-Driven Approach for Variance Reduction. CVPR 2025"}, "questions": {"value": "Refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fAuZ8cbkA4", "forum": "UpzRaO9tYx", "replyto": "UpzRaO9tYx", "signatures": ["ICLR.cc/2026/Conference/Submission17667/Reviewer_YzTD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17667/Reviewer_YzTD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902395214, "cdate": 1761902395214, "tmdate": 1762927517801, "mdate": 1762927517801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses group-level fairness in machine learning by reframing the problem as a multi-task learning (MTL) setting rather than embedding fairness directly as a regularizer in the predictive task. The authors propose modelling an auxiliary fairness task alongside the main predictive objective so that the network learns a latent representation that supports both accuracy and fairness. They introduce two novel fairness loss functions tailored for this MTL setup, provide a theoretical analysis of the generalization properties of their formulation, and empirically demonstrate on benchmark datasets that their MTL approach yields improved group-fairness metrics compared to both standard regularization-based fairness methods and other MTL variants, while maintaining competitive predictive performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The method is model-agnostic and can be integrated with existing architectures or loss functions without major redesign.\n\nThe paper offers generalization guarantees for the proposed MTL fairness formulation."}, "weaknesses": {"value": "The paper’s core idea—disentangling task-related and fairness-related information—closely mirrors prior work [1], yet the draft does not cite or differentiate from it. It is highly recommended to add [1] to related work and make the distinction explicit (e.g., what is new in your objective, architecture, optimization, or theory), include ablations isolating your innovations, and provide a controlled head-to-head comparison under the same protocol and tuning budget to demonstrate non-trivial gains over [1].\n\nThe proposed method is only compared with very old benchmarks. The authors should incorporate more recent state-of-the-art baselines (contemporary MTL fairness methods, adversarial debiasing, group-DRO/thresholding, post-hoc calibration) and report fairness–utility frontiers to substantiate superiority.\n\nResults are largely on toy datasets, leaving scalability and deployment unclear.\n\nMulti-task heads add parameters and training time; no cost–benefit analysis or runtime/memory profile compared to simpler baselines is provided.\n\n[1] Jang, Taeuk, and Xiaoqian Wang. \"Fades: Fair disentanglement with sensitive relevance.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I don't recognize significant ethical concerns."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IRM9sUNEvI", "forum": "UpzRaO9tYx", "replyto": "UpzRaO9tYx", "signatures": ["ICLR.cc/2026/Conference/Submission17667/Reviewer_veg4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17667/Reviewer_veg4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147611634, "cdate": 1762147611634, "tmdate": 1762927517140, "mdate": 1762927517140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}