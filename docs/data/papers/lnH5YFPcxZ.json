{"id": "lnH5YFPcxZ", "number": 14493, "cdate": 1758237185418, "mdate": 1759897367111, "content": {"title": "PEML: Parameter-efficient Multi-Task Learning with Optimized Continuous Prompts", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is critical for adapting Large Language Models (LLMs) for various tasks. \nRecently, there has been an increasing demands for fine-tuning LLMs for multiple tasks because it requires overall less data for fine-tuning thanks to the common features shared among tasks. More importantly, LLMs are resource demanding and deploying a single model for multiple tasks facilitates resource consolidation and consumes significantly less resources compared to deploying individual large model for each task. Existing PEFT methods like LoRA and Prefix Tuning are designed to adapt to a specific task. LoRA and its variation focus on aligning the model itself for tasks, overlooking the importance of prompt tuning in multi-task learning while Prefix Tuning only adopts a simple architecture to optimize prompts, which limits the adaption capabilities for multi-task. To enable efficient fine-tuning for multi-task learning, it is important to co-optimize prompt optimization and model adaptation. In this work, we propose a Parameter-Efficient Multi-task Learning (PEML), which employs a neural architecture engineering method for optimizing the continuous prompts while also performing low-rank adaption for model weights. We prototype PEML by creating an automated framework for optimizing the continuous prompts and adapting model weights. We compare against state-of-the-arts MTL-LoRA, MultiLoRa, C-Poly, and MoE, and results on the GLUE, SuperGLUE, Massive Multitask Language Understanding and commonsense reasoning benchmarks. The evaluation results presents an average accuracy improvement of up to 6.67%, with individual tasks showing peak gains of up to 10.75%.", "tldr": "We prototype PEML by creating an automated framework for optimizing the continuous prompts and adapting model weights.", "keywords": ["Multi-task Learning", "Parameter-Efficient Fine-Tuning", "Neural Architecture Engineering", "Continuous Prompts Optimization", "Low-Rank Adaptation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f1d155cc75b9699ca4771984eaaf76493f7c4d11.pdf", "supplementary_material": "/attachment/1a8f57722867c51a536042ca1cd576b505778270.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel framework named PEML (Parameter-Efficient Multi-task Learning)\nto address the limitations of existing Parameter-Efficient Fine-Tuning (PEFT) methods in multi-task\nlearning (MTL) scenarios. PEML integrates low-rank adaptation (LoRA) with a Neural Architecture\nSearch (NAS) method called PrefixNAS to collaboratively enhance both model weights and the\nstructure of continuous prompts. The authors conduct extensive experiments across multiple\nbenchmarks, which demonstrates that PEML outperforms current state-of-the-art multi-task PEFT\nmethods in both performance and computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This study presents an innovative application of Neural Architecture Search (NAS) to optimize\ncontinuous prompt structures for multi-task learning. The authors show consistent (though\nsometimes marginal) improvements over several LoRA-based MTL baselines, demonstrating that\nthe idea is practically plausible. The paper also includes valuable efficiency analyses comparing\nVRAM, throughput, and inference latency, which are crucial for PEFT methods."}, "weaknesses": {"value": "1. The paper lacks a rigorous explanation that directly links the optimality of the discovered\narchitecture to the performance gain. The authors compare PEML (LoRA + PrefixNAS) only\nagainst other LoRA-only baselines. The most critical ablation is missing: LoRA + standard\nPrefix-Tuning. Without this baseline, it is impossible to determine if the performance gains\nstem from the sophisticated PrefixNAS search or merely from the addition of any prompttuning\nmodule. This is a significant omission that needs to be addressed.\n\n2. The paper positions PEML as a \"parameter-efficient\" and resource-conscious method. While\nit is efficient in terms of trainable parameters and VRAM usage during training, this narrative\nconveniently ignores the massive upfront computational cost of the NAS search. A fair\ncomparison must account for this search cost, which is entirely absent from the baseline\ncomparisons."}, "questions": {"value": "1. Is there a way to gain insights that using PrefixNAS is a better choice than simply combining\nLoRA with a standard, off-the-shelf Prefix-Tuning module?\n\n2. Can the authors provide a rigorous comparison against a baseline of LoRA + standard Prefix-\nTuning? This seems essential to justify the complexity and cost of the entire PrefixNAS\nframework.\n\n3. Are there attributes of PEML other than the \"optimized\" architecture that may contribute to\nits performance? How can the authors justify the NAS cost-benefit ratio when the gains are\nmarginal?\n\n4. Could you provide a quantitative comparison of the total computational cost (Search Time +\nTraining Time) for PEML versus the (Training Time)-only cost of baselines like MTL-LORA?\n\n5. I noticed in Table 6, PEML significantly underperforms MTL-LORA and DORA on HellaSwag.\nDo you have a hypothesis for this failure case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9WbPYmdDdd", "forum": "lnH5YFPcxZ", "replyto": "lnH5YFPcxZ", "signatures": ["ICLR.cc/2026/Conference/Submission14493/Reviewer_oAEt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14493/Reviewer_oAEt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964605443, "cdate": 1761964605443, "tmdate": 1762924891237, "mdate": 1762924891237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PEML, a method that combines LoRA with PrefixNAS to enhance multi-task learning in LLMs. Unlike existing PEFT methods that focus solely on model weight adaptation, PEML jointly optimizes both prompt alignment through PrefixNAS and model adaptation through LoRA. The authors construct paired training data from FLAN and evaluate on GLUE, SuperGLUE, MMLU, and commonsense reasoning benchmarks using T5-Large, FLAN-T5-Large, LLaMA-7B, and LLaMA2-7B. Results show average accuracy improvements up to 6.67% over baselines including MTL-LoRA, MultiLoRA, C-Poly, and MoE."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Well-motivated problem: The paper clearly identifies limitations of existing multi-task PEFT methods—adapter switching overhead, lack of prompt optimization, and VRAM inefficiency in methods like MultiLoRA.\n\nComprehensive evaluation: Testing across four major benchmarks (GLUE, SuperGLUE, MMLU, commonsense reasoning) with multiple model families (T5, LLaMA) demonstrates breadth.\n\nThorough ablation studies: Section 5 and Appendix 7.4 provide good analysis of design choices including layer count, optimization order (parallel vs. sequential), and search space operations.\n\nPractical considerations: The paper addresses real deployment concerns like VRAM usage (Section 7.5) and inference latency (Section 7.7), showing PEML avoids the linear VRAM growth of MultiLoRA."}, "weaknesses": {"value": "1. Computational cost not properly accounted:\n\t○ NAS search requires 2 hours on 8×A100 GPUs (16 GPU-hours) per benchmark, representing significant upfront cost.\n\n\t○ This one-time cost is dismissed too lightly—for new task combinations, the search must be repeated.\n\n\t○ Fair comparison should include baseline hyperparameter tuning time or report total wall-clock time including search.\n\n\t○ The claim of \"efficiency\" is misleading when ignoring NAS computational budget.\n\n2. Incomplete analysis and missing experiments:\n\n\t○ No analysis of which tasks benefit most from prompt optimization vs. weight adaptation.\n\n\t○ Missing comparison to simpler alternatives: What if we just use LoRA with larger rank? What about manually designed prefix architectures?\n\t○ Generalization not tested: Does a PrefixNAS architecture found on GLUE transfer to SuperGLUE? This would test if the search truly finds universal structures.\n\n3. Theoretical analysis limitations:\n\n\t○ Section 7.1 convergence analysis assumes convex optimization properties (β-smoothness, bounded gradients) that may not hold for neural architecture search.\n\n\t○ The analysis does not account for the discrete architecture selection after continuous relaxation.\n\n\t○ No analysis of how architecture search affects the joint optimization landscape.\n\n\t○ Gap between theory (assuming smooth optimization) and practice (discrete architecture decisions).\n\n4. Limited scope of \"multi-task\":\n\n\t○ All tasks are still within NLU—no evaluation on truly diverse tasks like generation, translation, code, reasoning.\n\n\t○ \"Multi-task\" means multiple NLU benchmarks, not fundamentally different task types.\n\n\t○ Unclear if approach would work for more heterogeneous task mixtures."}, "questions": {"value": "1. Cost-benefit analysis: Can you provide a comprehensive comparison including the NAS search time? \n\n2. Architecture transferability: If you find an optimal prefix architecture on GLUE, does it transfer to SuperGLUE or MMLU without re-searching? This would validate whether PrefixNAS discovers general structures vs. overfitting to each benchmark.\n\n3. Simpler alternatives: Have you compared against LoRA with rank=96 (matching your total parameter budget)? Table 5 shows LoRA_r=96 achieves 75.2% while PEML gets 80.3%—but how much of this is from NAS search vs. just the combination?\n\n4. Per-task analysis: Which types of tasks benefit most from prompt optimization? Are there tasks where LoRA alone is sufficient? This would provide insights into when PrefixNAS is worth the cost.\n\n5. Failure analysis: Table 6 shows HellaSwag performance drops significantly (77.4% vs. 93.1%). Can you explain why PEML underperforms on this task? What characteristics cause failures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T79WsNjBeZ", "forum": "lnH5YFPcxZ", "replyto": "lnH5YFPcxZ", "signatures": ["ICLR.cc/2026/Conference/Submission14493/Reviewer_Ey24"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14493/Reviewer_Ey24"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045251514, "cdate": 1762045251514, "tmdate": 1762924890638, "mdate": 1762924890638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a parameter-efficient multi-task framework that jointly optimizes LoRA weight updates with a differentiable PrefixNAS module. During training, LoRA and PrefixNAS are optimized in parallel; after training, LoRA is merged into the base model and only the learned prefix architecture is kept for inference, avoiding adapter switching. Evaluations on GLUE, SuperGLUE, MMLU, and commonsense benchmarks report average gains up to 6.67% (peaks up to 10.75%) over strong PEFT baselines (LoRA, AdaLoRA, MultiLoRA, C-Poly, MoE)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Clear unified design: concurrent LoRA + PrefixNAS with a concrete training algorithm (Alg. 1). \n* Deployment efficiency: LoRA is merged; inference uses one prefix, reducing switching/VRAM overhead. \n* Broad empirical coverage with consistent improvements across multiple benchmarks."}, "weaknesses": {"value": "* Differentiable–discrete gap: architecture is relaxed via soft weights but finalized with argmax selection (Eq. 7), lacking analysis of search-time gradient bias or stability after discretization. \n\n* Search cost & fairness: PrefixNAS + TPE requires non-trivial compute (e.g., 8×A100, hours per benchmark), raising risks of validation overfitting and unequal hyperparameter budgets vs. baselines. \n\n* Task sensitivity: results note variability (e.g., WSC), suggesting remaining brittleness in cross-task generalization despite average gains."}, "questions": {"value": "* The PrefixNAS module relies on a continuous relaxation during search but finalizes the architecture via a discrete argmax operation (Eq. 7).How stable is the gradient-based optimization when transitioning from continuous to discrete architectures, and could a smoother relaxation (e.g., Gumbel-Softmax or straight-through estimators) yield more consistent convergence and better generalization?\n\n* PEML’s joint LoRA + PrefixNAS optimization requires multi-GPU resources (up to 8 × A100 for each benchmark).How can the framework ensure fair comparison with lightweight PEFT baselines like LoRA or AdaLoRA, and could a two-stage or surrogate-based NAS reduce cost without sacrificing accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SyYEJmeTXZ", "forum": "lnH5YFPcxZ", "replyto": "lnH5YFPcxZ", "signatures": ["ICLR.cc/2026/Conference/Submission14493/Reviewer_1Pgd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14493/Reviewer_1Pgd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762133860536, "cdate": 1762133860536, "tmdate": 1762924890220, "mdate": 1762924890220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}