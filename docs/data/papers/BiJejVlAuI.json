{"id": "BiJejVlAuI", "number": 18068, "cdate": 1758283468814, "mdate": 1763748611653, "content": {"title": "Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization", "abstract": "Supervised Fine-Tuning (SFT), which lays an important foundation of effective reasoning in LLMs, typically uses only correct Chain-of-Thought (CoT) data whose final answers match the ground truth, suffering from poor generalization due to overfitting and wasted data from discarding incorrect samples.  \n\nConsidering that incorrect samples contain implicit valid reasoning processes and diverse erroneous patterns, we investigate whether incorrect reasoning trajectories can serve as valuable supervision and surprisingly find that they substantially improve out-of-domain (OOD) generalization over correct-only training.  \n\nTo explain this, we performed an in-depth analysis through data, training, and inference, revealing 22 different patterns in incorrect chains, which yield two benefits:  \n1. *For training*, they produce a slower loss descent, indicating a broader optimization landscape that mitigates overfitting.  \n2. *For inference*, they raise model's policy entropy in the reasoning process by 35.67% over correct-only training (under on-policy strategy) and encourage exploration of alternative reasoning paths to improve generalization.  \n\nInspired by this, we propose **Gain-based LOss Weighting** (`GLOW`), an adaptive, sample-aware method that prompts models to identify underexplored patterns by rescaling sample loss weights based on inter-epoch progress. Theoretically, it converges to more generalizable solutions. Empirically, it outperforms full-data training across different model sizes and significantly improves the OOD performance of Qwen2.5-7B trained on math reasoning by 15.81% over positive-only training.  \n\nCode is available at [Github](https://anonymous.4open.science/r/GLOW-6F7C).", "tldr": "This work shows that fine-tuning on incorrect reasoning boosts out-of-domain generalization, with an adaptive loss that exploits this signal automatically.", "keywords": ["LLM reasoning", "generalization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a315a11d19458412f161133fd528cfb82c080bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the practice of Supervised Fine-Tuning (SFT) of using only \"positive\" Chain-of-Thought (CoT) samples during distillation. The authors argue this approach leads to overfitting, poor out-of-domain (OOD) generalization, and inefficient data use by discarding \"negative\" (incorrect) samples. While the authors findings that OOD benchmarks perform better with negative samples is interesting, the paper requires major revisions. The communication is very challenging to follow; with some statements being incorrect. The problem setting’s impact is unclear, and experimental choices need clarification. Some analysis interpretation’s validity are also questionable."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The empirical finding is interesting. \n2. The empirical analysis is interesting."}, "weaknesses": {"value": "As the paper mentions, the pre-training + post-training (SFT, RL) is the common paradigm. There are clear evidence that shows that RL generalizes and SFT memorizes,e.g.\n“SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training”\nModern post-training of reasoning models never stop at SFT, but always include the RL stage as it dramatically improves performance. Given that it is possible that RL improves generalization, for this work to be practically impactful it would need to show that the generalization gains at SFT lead to improved generalization or in-distribution performance post-RL.\n\nFirst line of the abstract is confusing. The phrasing makes it sound like SFT always uses CoT, which is incorrect. SFT with CoT is only for reasoning post-training. \n\nWhy is there + in the data annotation in Fig. 1? This seems unnecessary as it is not increasing; it is just the performance annotation.\n\nTypos should be fixed. Also, there should be a space before parenthesis, E.g. \n> Chain-of-Thougnt(CoT)\n\n> tasks(see \n\n> benchmarks..\n\nThe communication is confusing. What does dominant SFT mean? Why is the idea of distillation suddenly introduced here?\n\n> (2) Data Inefficiency: Dominant SFT relies on distilled reasoning paths and uses rejection sampling to select those leading to correct answers and formats\n\nThe text size in figures and diagrams should be increased in size. It is non-legible when printed out in A4 paper.\n\nThis phrase is confusing. The communication needs revision.\n> While relying solely on negative samples is a possible strategy, it remains a form of rejection sampling with low data efficiency\n\nIt is not correct to call MMLU and MMLU-pro a common sense benchmark\n> We use Qwen3-8B to distill responses from OpenMathReasoning (Moshkov et al., 2025) and the MMLU (Hendrycks et al., 2021b) training set as training data for mathematical and common sense tasks. \n\n\nMMLU includes mathematics so it is not wholly OOD?\n\nWhy is some of the color coding and bolding not available for some cells in Tab. 1, 2? If it is to indicate that the cell’s result is conforming to the hypothesis, these details should be available in the Table’s caption\n\nIt is unclear where the empirical evidence for this is? Tab. 3 only shows the results for negative samples. If it is “richer” than positive samples we need to compare and contrast\n> Negative samples exhibit a richer variety of reasoning patterns, whereas positive data tend to follow more consistent trajectories.\n\nThis can not be the right interpretation. If you Maximum likelihood estimate (MLE) on any given data, the train loss will fall. \n> The negative curve remains monotonic and closely tracks the positive curve, indicating that negatives provide meaningful learning signals rather than noise\n\nWhy are sec. 5’s experimental results missing the Qwen2.5-32B and Llama3.1-8B present in previous section’s experiments?\n\nThere should be a limitation section, even if it is in the appendix due to space constraints."}, "questions": {"value": "In what scenario is incorrect samples readily available other than distillation?\n> us to inquire whether an SFT approach can not only improve data efficiency through utilizing all available samples but also benefit from this expanded exploration space to enhance generalization. \n\nSince you claim that it explores more at inference time, it will be more convincing if you could try to do some simple test-time scaling like majority voting with N samples and see if your method helps.\n\nCan the authors explain how\n>• We provide the first systematic study demonstrating that negative reasoning samples constitute valuable supervision\n\nIs true when you have an extended related work section on “Learning from Negative Data “\n\nCan the authors explain in what real-world scenario we would distill Qwen3-8B → Qwen-2.5 14B, 32B ? \n\nAre there any cost overheads for GLOW over the naive approach?\n\nHas this work been compared with \"Large Reasoning Models Learn Better Alignment\nfrom Flawed Thinking\" ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4gOcUpsggp", "forum": "BiJejVlAuI", "replyto": "BiJejVlAuI", "signatures": ["ICLR.cc/2026/Conference/Submission18068/Reviewer_SmfD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18068/Reviewer_SmfD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761232596579, "cdate": 1761232596579, "tmdate": 1762927853207, "mdate": 1762927853207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the practice of Supervised Fine-Tuning (SFT) of using only \"positive\" Chain-of-Thought (CoT) samples during distillation. The authors argue this approach leads to overfitting, poor out-of-domain (OOD) generalization, and inefficient data use by discarding \"negative\" (incorrect) samples. While the authors findings that OOD benchmarks perform better with negative samples is interesting, the paper requires major revisions. The communication is very challenging to follow; with some statements being incorrect. The problem setting’s impact is unclear, and experimental choices need clarification. Some analysis interpretation’s validity are also questionable."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The empirical finding is interesting. \n2. The empirical analysis is interesting."}, "weaknesses": {"value": "As the paper mentions, the pre-training + post-training (SFT, RL) is the common paradigm. There are clear evidence that shows that RL generalizes and SFT memorizes,e.g.\n“SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training”\nModern post-training of reasoning models never stop at SFT, but always include the RL stage as it dramatically improves performance. Given that it is possible that RL improves generalization, for this work to be practically impactful it would need to show that the generalization gains at SFT lead to improved generalization or in-distribution performance post-RL.\n\nFirst line of the abstract is confusing. The phrasing makes it sound like SFT always uses CoT, which is incorrect. SFT with CoT is only for reasoning post-training. \n\nWhy is there + in the data annotation in Fig. 1? This seems unnecessary as it is not increasing; it is just the performance annotation.\n\nTypos should be fixed. Also, there should be a space before parenthesis, E.g. \n> Chain-of-Thougnt(CoT)\n\n> tasks(see \n\n> benchmarks..\n\nThe communication is confusing. What does dominant SFT mean? Why is the idea of distillation suddenly introduced here?\n\n> (2) Data Inefficiency: Dominant SFT relies on distilled reasoning paths and uses rejection sampling to select those leading to correct answers and formats\n\nThe text size in figures and diagrams should be increased in size. It is non-legible when printed out in A4 paper.\n\nThis phrase is confusing. The communication needs revision.\n> While relying solely on negative samples is a possible strategy, it remains a form of rejection sampling with low data efficiency\n\nIt is not correct to call MMLU and MMLU-pro a common sense benchmark\n> We use Qwen3-8B to distill responses from OpenMathReasoning (Moshkov et al., 2025) and the MMLU (Hendrycks et al., 2021b) training set as training data for mathematical and common sense tasks. \n\n\nMMLU includes mathematics so it is not wholly OOD?\n\nWhy is some of the color coding and bolding not available for some cells in Tab. 1, 2? If it is to indicate that the cell’s result is conforming to the hypothesis, these details should be available in the Table’s caption\n\nIt is unclear where the empirical evidence for this is? Tab. 3 only shows the results for negative samples. If it is “richer” than positive samples we need to compare and contrast\n> Negative samples exhibit a richer variety of reasoning patterns, whereas positive data tend to follow more consistent trajectories.\n\nThis can not be the right interpretation. If you Maximum likelihood estimate (MLE) on any given data, the train loss will fall. \n> The negative curve remains monotonic and closely tracks the positive curve, indicating that negatives provide meaningful learning signals rather than noise\n\nWhy are sec. 5’s experimental results missing the Qwen2.5-32B and Llama3.1-8B present in previous section’s experiments?\n\nThere should be a limitation section, even if it is in the appendix due to space constraints."}, "questions": {"value": "In what scenario is incorrect samples readily available other than distillation?\n> us to inquire whether an SFT approach can not only improve data efficiency through utilizing all available samples but also benefit from this expanded exploration space to enhance generalization. \n\nSince you claim that it explores more at inference time, it will be more convincing if you could try to do some simple test-time scaling like majority voting with N samples and see if your method helps.\n\nCan the authors explain how\n>• We provide the first systematic study demonstrating that negative reasoning samples constitute valuable supervision\n\nIs true when you have an extended related work section on “Learning from Negative Data “\n\nmodified 11.17: What about comparison or discussion with \"The Surprising Effectiveness of\nNegative Reinforcement in LLM Reasoning\"?\nCan the authors explain in what real-world scenario we would distill Qwen3-8B → Qwen-2.5 14B, 32B ? \n\nAre there any cost overheads for GLOW over the naive approach?\n\nHas this work been compared with \"Large Reasoning Models Learn Better Alignment\nfrom Flawed Thinking\" ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4gOcUpsggp", "forum": "BiJejVlAuI", "replyto": "BiJejVlAuI", "signatures": ["ICLR.cc/2026/Conference/Submission18068/Reviewer_SmfD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18068/Reviewer_SmfD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761232596579, "cdate": 1761232596579, "tmdate": 1763338941109, "mdate": 1763338941109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether incorrect reasoning samples, usually discarded in supervised fine-tuning (SFT) of LLMs, can enhance generalization. The authors show that negative samples introduce diverse reasoning patterns, slow overfitting, and increase exploration during inference, leading to significant out-of-domain (OOD) gains. They further propose GLOW, which adaptively emphasizes underexplored samples, achieving consistent improvements across tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) This paper presents a novel perspective by treating incorrect reasoning trajectories as useful supervision.\n\n(2) The findings that incorrect reasoning samples are useful supervision are insightful, providing a potential direction for improving OOD generation in LLMs\n\n(3) The writing is well-structured and easy to follow. \n\n(4) Both empirical evidence and theory analysis are provided."}, "weaknesses": {"value": "[1] The claim that this is “the first systematic study demonstrating that negative reasoning samples constitute valuable supervision” may be overstated, as similar claims have been made in prior work (e.g., [1]).\n\n[2] The experimental evaluation is limited in scope, as most results are based on Qwen models. It remains unclear whether GLOW would provide consistent improvements across other architectures such as GPT-OSS-20B or LLaMA.\n\n\n[1] Xinyu Zhu et al. The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning. Nuerips 2025."}, "questions": {"value": "(1)Could the authors clarify whether different types of reasoning errors contribute differently to supervision? In particular, how are supervision signals distributed among the various error categories, and do some error types provide more benefit than others?\n\n(2) Can the authors comment on whether GLOW achieves consistent improvements on other architectures, such as GPT-OSS-20B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P0sCzjhwTr", "forum": "BiJejVlAuI", "replyto": "BiJejVlAuI", "signatures": ["ICLR.cc/2026/Conference/Submission18068/Reviewer_ZzKP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18068/Reviewer_ZzKP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665564423, "cdate": 1761665564423, "tmdate": 1762927852751, "mdate": 1762927852751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper starts by observing and analyzing that negative reasoning traces are important. Then, based on these observations, the authors propose a GLOW mechanism that adaptively reweights the loss. Its efficacy is shown for various tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Well-written\n- Interesting observations regarding negative reasoning traces, and a relatively simple proposed fix via loss-based reweighting\n- Promising preliminary empirical results"}, "weaknesses": {"value": "- Theoretical discussions and statements in the main text and Appendix A.5 should be cleaned up:\n   - In Eqn. (7), the index $t$ is missing rom LHS and $\\ell_i^{(t)}$ should be just $\\ell_i$? I guess that this is the training objective per epoch? So then $t$ is the epoch, not the individual (SGD) step?\n   - What is $\\delta$ in line 404?\n   - Please utilize appropriate amsthm environments (lemma, theorem, etc). Also, theorem statements should not contain parts of the proof.\n   - (A4) is not really an assumption.\n   - What is the justification of the energy condition for Lemma 3?\n   - In Lemma 3, why suddenly introduce a low-dimensional subspace?\n   - As the main text promised rigorous statements, they should be included! For instance, Lemma 5 simply refers to Bousquet & Elisseeff (2002); Hardt et al. (2016), which makes the paper not so much self-contained..\n   - The authors claim that the proposed reweighting scheme makes the loss more well-conditioned, per epoch. However, considering that the \"true\" objective is $\\sum_i \\ell_i$, the correct phrase is that the gradient direction is the gradient of a well-conditioned pseudo-loss per iteration. Of course, as the weights change, so does the minimum. Thus, as in [1, Theorem 2], a meaningful theoretical contribution may be to provide some convergence guarantee under this dynamic reweighting scheme.\n- The proposed approach is, naturally, quite closely related to prior loss-based reweighting schemes ([2,3] and references therein). Yet, it is not clear what the advantages of the proposed reweighting are from this paper over the prior works and why this is particularly well-suited for LLM reasoning task beyond other ML tasks where reweighting has seen much success.\n\n\n[1] https://openreview.net/forum?id=gU4ZgQNsOC\n\n[2] https://arxiv.org/abs/2408.09849\n\n[3] https://openreview.net/forum?id=GLUIuli3Sm"}, "questions": {"value": "1. I'm a bit confused on the training objective. So, per (ques, ans) pair, Qwen3-8B is used to sample a (reasoning, sampled-ans) conditioned on ques? Then the authors train on $\\log \\pi(reasoning, sampled-ans | ques)$, *regardless* of the correctness of the sampled-ans? Intuitively (e.g., as in [4]), at least I thought that this would bias the model to generate incorrect responses for the training set...? Here, $\\pi$ is the model used for SFT, such as Qwen2.5 or Llama-3.1.\n\n2. Recent work [4] considers the effect of negative reinforcement in LLM for RL (PPO and GRPO). Of course one clear difference is that they consider RL while this work considers SFT. Still, \n\n3. I didn't quite understand why the diversity of incorrect reasonings is related/understood via IRM. What is the \"environment\" here? When doing next-token prediction, what is the shared representation $\\Phi$ and predictor $w$ in our scenario? As the authors claim part of their contribution to be \"deep analysis of how negatives improve generalization\", I feel that this should be clarified and elaborated much more.\n\n4. In Figure 1(b), is the plotted loss the same total training loss? What I mean is, is the blue and red curves both tracking $\\sum_i \\ell_i(w_t)$, where $w_t$ either tracks the AdamW of positive loss or negative loss?\n\n5. Beyond SFT, would a similar approach work for self-improvement-style algorithms like STaR? According to this paper, am I correct in understanding that filtering is not necessary? This seems somewhat contradictory to many REINFORCE-style algorithms, where in verifiable reward setting (e.g., $0$ or $1$), the gradient is weighted by the indicator reward. I guess here, some discussions related to [4] would be appreciated as well.\n\n6. Personally, I've never seen the stepwise loss decay as shown in all the plots with only cosine scheduling (as written in Appendix A.1). This seems like a stepwise schedule...? Is it usual?\n\n7. (minor) Is it natural that the full fine-tuning is the only baseline?\n\n8. Is it safe to claim that \"training solely on negative samples outperform those trained on positive samples\" based on the paper's experiments?\n\n9. (Minor) Very recently, there have been some issues regarding floating-point operations (https://x.com/QPHutu/status/1984258808332550245). Can the authors comment on whether this issue(?) applies to their experiments?\n\n[4] https://openreview.net/forum?id=ftVlLG9cks (arXiv ver1 came out on June, and so I don't think that it is unreasonable to bring this up)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FdeM7yt9PU", "forum": "BiJejVlAuI", "replyto": "BiJejVlAuI", "signatures": ["ICLR.cc/2026/Conference/Submission18068/Reviewer_4qLV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18068/Reviewer_4qLV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918680535, "cdate": 1761918680535, "tmdate": 1762927852269, "mdate": 1762927852269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Comment by Authors"}, "comment": {"value": "We thank all reviewers for their careful reading of our paper and for the constructive feedback. We have updated the paper in this revised version and provided point-by-point responses that address each reviewer’s concerns, weaknesses, and questions in detail. We hope that these responses and new results help resolve the issues raised. If any points remain unclear, we would be very happy to continue the discussion and we look forward to an active and constructive exchange."}}, "id": "pNCmcVVe96", "forum": "BiJejVlAuI", "replyto": "BiJejVlAuI", "signatures": ["ICLR.cc/2026/Conference/Submission18068/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18068/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission18068/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763752991255, "cdate": 1763752991255, "tmdate": 1763752991255, "mdate": 1763752991255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}