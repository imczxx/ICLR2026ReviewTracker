{"id": "7zztxcmlyZ", "number": 9685, "cdate": 1758134456467, "mdate": 1759897704317, "content": {"title": "Understanding and Improving Continuous LLM Adversarial Training via In-context Learning Theory", "abstract": "Adversarial training (AT) is an effective defense for large language models (LLMs) against jailbreak attacks, but performing AT on LLMs is costly. To improve the efficiency of AT for LLMs, recent studies propose continuous AT (CAT) that searches for adversarial inputs within the continuous embedding space of LLMs during AT. While CAT has achieved empirical success, its underlying mechanism, i.e., why adversarial perturbations in the embedding space can help LLMs defend against jailbreak prompts synthesized in the input token space, remains unknown. This paper presents the first theoretical analysis of CAT on LLMs based on in-context learning (ICL) theory. For linear transformers trained with adversarial examples from the embedding space on in-context linear regression tasks, we prove a robust generalization bound that has a negative correlation with the perturbation radius in the embedding space. This clearly explains why CAT can defend against jailbreak prompts from the LLM's token space. Further, the robust bound shows that the robustness of an adversarially trained LLM is closely related to the singular values of its embedding matrix. Based on this, we propose to improve LLM CAT by introducing an additional regularization term, which depends on singular values of the LLM's embedding matrix, into the objective function of CAT. Experiments on real-world LLMs demonstrate that our method can help LLMs achieve better jailbreak robustness-utility tradeoff.", "tldr": "We theoretically explain why continuous adversarial training helps LLMs defend against jailbreak prompts from the token space via the in-context learning theory.", "keywords": ["LLM adversarial training", "Jailbreak attacks", "In-context learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0cfa27e2b02d5bf05a67497b1aafb319d93bd541.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors investigate the ability of continuous adversarial training (CAT) to improve the robustness of LLMs using in-context learning (ICL) theory. The authors formalize a simplified surrogate problem based on linear transformers trained on in-context linear regression tasks. They prove that the robust generalization bound of models trained with CAT negatively correlates with the perturbation radius in embedding space and depends on the singular values of the embedding matrix. Based on their theoretical observations, they argue that the singular values should be constrained in the embedding matrix and provide a new version of CAT that achieves better accuracy robustness tradeoffs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The beginning of the paper is easy to follow and motivates the proposed analysis intuitively with prior work. The relevant context is clearly explained and provides a better formal description than the original works in some cases\n- Provides the first theoretical analysis of CAT, explaining why embedding-space perturbations can improve robustness to input-space jailbreaks, a contribution missing from the large number of adversarial training works in the LLM space.\n- Based on their theory, the authors are able to provide a practical algorithm (even if empirical results are somewhat inconclusive in my opinion)"}, "weaknesses": {"value": "- I found the description of the ICL theory hard to follow and would have appreciated additional text motivating / explaining the provided notation. Section 4.1 is hard to read.\n- The results provided regarding the performance of ER-CAT are somewhat inconclusive. For LLAMA-2-7B there is no clear advantage visible for ER-CAR. Moreover, it remains unclear if the benefits stem from the regularization approach or just from insufficient hyperparameter tuning of both methods. The large number of hyperparameters in LLM adversarial training appear to make it difficult to compare two appraoches (specifically if results are somewhat close)\n- Instead of averaging the ASR for the @5 approach, I would argue it's more sensible to compute the \"max\" harmfulness over all trials, as we are generally interested in lower bounds on the robustness.\n- Could the authors provide an \"ensemble ASR\" value that describes the lower bound robustness against all attacks (e.g., if one attack out of all succeeds for a prompt it can be considered broken). This would enable easier comparisons between CAT and ER-CAT regarding utility robustness trade-offs \n- Assumptions made in the theoretical part could be more clearly highlighted / summarized\n- Relevant hyperparameters that are important in the theoretical part are not analyzed (e.g., the perturbation magnitude constraint)"}, "questions": {"value": "- The theory provided in the paper provides some connections between the operator norm of the embedding function and the robustness of LLMs. Do the authors think that there is a general connection concerning the Lipschitzness of the function (LLM) and its robustness in the context of adversarial robustness in LLMs? (e.g., similar to [1])\n- Limited empirical analysis regarding model properties. How robust are models to continuous attacks? Do the regularized embedding matrices successfully reduce the effect of perturbations? Are the singular values of the final matrices considerably lower? Is there a connection between the magnitude of the singular values and the robustness? Do more robust existing models generally have lower singular values?\n- The surrogate setting appears somewhat simplified and not able to capture the complexity of the actual problem. Could the authors motivate why this might not be the case?\n\n[1] Roth, Kevin, Yannic Kilcher, and Thomas Hofmann. \"Adversarial training is a form of data-dependent operator norm regularization.\" Advances in Neural Information Processing Systems 33 (2020): 14973-14985."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XhTuEnbSTx", "forum": "7zztxcmlyZ", "replyto": "7zztxcmlyZ", "signatures": ["ICLR.cc/2026/Conference/Submission9685/Reviewer_3RMS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9685/Reviewer_3RMS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381912084, "cdate": 1761381912084, "tmdate": 1762921200781, "mdate": 1762921200781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of Continuous Adversarial Training (CAT) for LLMs, an efficient defense against jailbreak attacks that operates in the continuous embedding space. While CAT is empirically effective, the theoretical reason why perturbations in the embedding space can confer robustness against attacks in the discrete token space has remained unknown.\n\nThe authors provide the first theoretical analysis to bridge this gap. They leverage In-Context Learning (ICL) theory, modeling the problem using a Linear Self-Attention with Embedding (LSA-E) model trained on linear regression tasks. Their key theoretical contribution is a robust generalization bound for this model. This bound demonstrates that the model's robust risk (against input-space perturbations) is negatively correlated with the perturbation radius (epsilon) used during embedding-space AT. This result provides a formal explanation for CAT's effectiveness.\n\nFurthermore, the bound reveals that model robustness is closely tied to the singular values of the embedding matrix. Based on this insight, the authors propose a novel method, Embedding-Regularized Continuous AT (ER-CAT), which adds a regularization term to the CAT objective to minimize the variance of the embedding matrix's singular values. Empirical results on six real-world LLMs and six jailbreak attacks show that ER-CAT generally achieves a better robustness-utility tradeoff than standard CAT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**: The paper's primary contribution is highly original. It tackles a novel and important theoretical question (i.e., \"Why does CAT work?\") that sits at the intersection of LLM robustness and efficiency. The choice of using ICL as the analytical framework is creative and non-trivial, establishing a new and insightful link between these fields. The development of a new defense (ER-CAT) directly from the theoretical findings is a good example of theory-driven research.\n\n**Significance**: The work is of high significance to the ICLR community. As LLMs become more pervasive, efficient and principled defense mechanisms are critical. CAT is a pragmatically important method, and this paper provides the first solid theoretical grounding for it. This understanding can lead to more principled improvements beyond simple heuristics, strengthening the foundation for future work on robust LLMs.\n\n**Clarity**: The paper is well-written. The abstract and introduction clearly articulate the problem, the gap in existing knowledge, and the paper's contributions. The logical flow from the core research question to the theoretical setup, the key findings (Theorem 2), the implications (Sec 4.3), and the proposed method (ER-CAT, Eq. 15) is clear and easy to follow."}, "weaknesses": {"value": "**The Theory-Practice Gap**: The primary weakness of this paper is the gap between the theoretical model and the practical application. The analysis relies on a Linear Self-Attention (LSA-E) model trained on a linear regression task. This is a massive simplification of a multi-billion parameter, highly non-linear Transformer LLM performing the complex task of \"refusal.\" While the authors provide some justification (Sec 4.1), the leap of faith required is substantial. The paper would be strengthened by a more explicit discussion of these limitations and the potential risks of generalizing these linear insights to the non-linear deep learning regime.\n\n\n**Missing Ablation for New Hyperparameter**: The proposed ER-CAT method introduces a new hyperparameter, beta (the coefficient for the singular value regularization), which is set to 0.2. The paper provides no ablation study or sensitivity analysis for this hyperparameter. How was this value chosen? Is the method's performance robust to changes in beta? This is a crucial missing piece of the empirical validation."}, "questions": {"value": "1. How was the regularization coefficient beta = 0.2 for ER-CAT selected? Could you provide a sensitivity analysis or ablation study on at least one model to show how performance (both ASR and LC-WinRate) changes with different values of beta?\n\n2. What is the practical computational overhead (e.g., increase in training time per step/epoch) of calculating and backpropagating through the singular value variance regularization in ER-CAT compared to the standard CAT baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E5IlqWzZ1U", "forum": "7zztxcmlyZ", "replyto": "7zztxcmlyZ", "signatures": ["ICLR.cc/2026/Conference/Submission9685/Reviewer_iaCj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9685/Reviewer_iaCj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839273871, "cdate": 1761839273871, "tmdate": 1762921199762, "mdate": 1762921199762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper performs a theoretical analysis on the Continuous Adversarial Training (CAT) method for defending LLMs against jailbreak attacks. Building upon ICL theory, the authors leverage a systematic derivation using a linear Transformer framework and propose a deference approach, termed Embedding-Regularized Continuous Adversarial Training (ER-CAT)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Improving Adversarial Training (AT) efficiency through embedding-space perturbations offers a promising and theoretically grounded direction for enhancing model robustness.\n2. The authors provide a comprehensive theoretical proof under the in-context learning (ICL) framework, establishing clear connections between embedding-space adversarial perturbations and robustness in input space. Based on this analysis, they ultimately propose a new CAT method.\n3. ER-CAT extends CAT by introducing an additional embedding regularization term, which constrains the variance of singular values in the embedding matrix. It can be seamlessly implemented within existing CAT training pipelines."}, "weaknesses": {"value": "1. The theoretical analysis is derived using Linear Self-Attention (LSA) models. While this is a common in ICL, the gap between the linearized setting and real large-scale LLMs is neither fully explored nor quantitatively characterized. A discussion or empirical validation of how the derived theory translates to practical LLM is preferred.\n2. The paper does not report ablation studies on key hyperparameters. For instance, it remains unclear how sensitive the results are to the regularization strength (β) and perturbation radius (ε).\n3. All evaluations are conducted using known jailbreak attack benchmarks. It is unclear for other types of attacks.\n4. One of CAT’s main motivations is to improve training efficiency over standard adversarial training. However, ER-CAT’s computational overhead relative to CAT is not analyzed.\n5. Since the proposed regularization explicitly constrains the singular value distribution of the embedding matrix, it would be insightful to show how these singular values evolve during training. A visualization could empirically support the theoretical claim."}, "questions": {"value": "See the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mOnaCN34KO", "forum": "7zztxcmlyZ", "replyto": "7zztxcmlyZ", "signatures": ["ICLR.cc/2026/Conference/Submission9685/Reviewer_hwXR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9685/Reviewer_hwXR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871031531, "cdate": 1761871031531, "tmdate": 1762921199404, "mdate": 1762921199404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper takes a step toward understanding why adversarial training (AT) in the embedding space provides robustness to attacks in the token space. To enable analysis, they focus on linear self-attention with embedding module (LSA-E) trained on linear regression ICL tasks and prove that embedding-space AT on LSA-E yields an upper bound on the robust generalization risk of LSA-E models under ICL suffix adversarial attacks, an attack in the token space. \n\nThey show that this upper bound (1) has a negative correlation with the adversarial perturbation during AT; therefore, a larger perturbation radius provides better input-space robustness, and (2) links robustness to singular values of the embedding matrix. Motivated by this, they propose Embedding-Regularized CAT (ER-CAT), which adds a penalty on the variance of the singular values of the embedding matrix to push them away from being too large or too small. They compare ER-CAT with CAT on several open-source LLMs and jailbreak attacks to show that ER-CAT achieves a better robustness-utility tradeoff."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The theoretical analysis on LSA-E models and the robust generalization upper bound is an interesting contribution that sheds light on how robustness of embedding-space adversarial training is connected to the singular values of the embedding matrix and the adversarial perturbation radius.\n\n2. Motivated by the theoretical findings, the paper proposes ER-CAT, a singular-value-based regularization for CAT, and presents experiments where ER-CAT provides some improvements in model robustness (lower ASR) while maintaining approximately similar utility. Although the evaluation setup could be improved (see below), this shows some promise in using embedding-based regularization within CAT."}, "weaknesses": {"value": "1. The connection between ICL embedding AT for LSA-E models and continuous AT in LLMs is underexplained. The theoretical analysis relies on a linear self-attention surrogate and does not capture non-linearities in modern LLMs. Although the paper claims ICL embedding AT is a good approximation for real-world LLM CAT, this is insufficiently supported, and while the empirical results are encouraging, it's unclear when and why robustness gains proven for LSA-E transfer to CAT on large non-linear multilayer transformers.\n\n2. The empirical results present robustness (Table 1) and utility (Table 2) separately, making it difficult to evaluate whether ER-CAT truly provides a better robustness-utility tradeoff than CAT. When ER-CAT achieves lower ASR (better robustness), it often comes at the cost of reduced utility. The paper would benefit from visualizations such as ROC-style curves that plot utility against robustness, allowing direct comparison of the methods at equivalent operating points. \n\n3. Writing/notation issues:\n- The loss defined for continuous AT in equation (3) has a mistake. Given that in the text, $y$ is the target harmful resposne and $\\tilde{y}$ is the safe response, the current version of the loss in equation (3) is encouraging the harmful response and decreasing the likelihood of the safe response. $y$ and $\\tilde{y}$ should be swapped in the Adversarial Loss. This holds similarly for equation (4).\n\n- (minor point) In several of the equations (such as equations (7) and (9), dot (\".\") is used for matrix multiplication, which to me is a non-standard and confusing notation. I suggest using dots only for dot products and not matrix multiplication."}, "questions": {"value": "1. In the ICL linear-regression surrogate, what is the intended correspondence between ($x_{\\tau, i}$, $y_{\\tau,i}$) and standard LLM training data? Specifically, should $x_{\\tau, i}$ be thought of as a single-token feature vector or a compressed/pooled representation of a multi-token prompt example, and if the latter, how is it modeled, and at which stage relative to $W_E$? Also, in this case, how realistic is the modeling choice that $x_{\\tau, i}, x_{\\tau,q} \\sim N(0, \\Lambda)$? A clarification on this helps in understanding the modeling better.\n\n2. What is the reason behind choosing ICL suffix embedding attack for the robust generalization risk analysis? What other types of token-space adversarial attacks do you expect your analysis to extend to?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hFvYafRBWI", "forum": "7zztxcmlyZ", "replyto": "7zztxcmlyZ", "signatures": ["ICLR.cc/2026/Conference/Submission9685/Reviewer_8RQd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9685/Reviewer_8RQd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981238105, "cdate": 1761981238105, "tmdate": 1762921199034, "mdate": 1762921199034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}