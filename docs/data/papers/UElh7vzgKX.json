{"id": "UElh7vzgKX", "number": 21173, "cdate": 1758314501523, "mdate": 1763741758822, "content": {"title": "Scaling Goal-conditioned Reinforcement Learning with Multistep Quasimetric Distances", "abstract": "The problem of learning how to reach goals in an environment has been a long-\nstanding challenge in for AI researchers. Effective goal-conditioned reinforcement\nlearning (GCRL) methods promise to enable reaching distant goals without task-\nspecific rewards by stitching together past experiences of different complexity.\nMathematically, there is a duality between the notion of optimal goal-reaching\nvalue functions (the likelihood of success at reaching a goal) and temporal dis-\ntances (transit times states). Recent works have exploited this property by learning\nquasimetric distance representations that stitch long-horizon behaviors using the in-\nductive bias of their architecture. These methods have shown promise in simulated\nbenchmarks, reducing value learning to a shortest-path problem. But quasimet-\nric, and more generally, goal-conditioned RL methods still struggle in complex\nenvironments with stochasticity and high-dimensional (visual) observations. There\nis a fundamental tension between the local dynamic programming (TD backups,\ntemporal distances) that enables optimal shortest-path reasoning in theory and the\nstatistical global MC updates (multistep returns, suboptimal in theory). We show\nhow these approaches can be integrated into a practical GCRL method that fits a\nquasimetric distance using a multistep Monte-Carlo return. We show our method\noutperforms existing GCRL methods on long-horizon simulated tasks with up to\n4000 steps, even with visual observations. We also demonstrate that our method\ncan enable stitching in the real-world robotic manipulation domain (Bridge setup).\nOur approach is the first end-to-end GCRL method that enables multistep stitching\nin this real-world manipulation domain from an unlabeled offline dataset of visual\nobservations.", "tldr": "Applying multistep value backup in a quasimetric distance architecture can yield surprisingly strong results.", "keywords": ["Goal-conditioned reinforcement learning", "quasimetrics", "robotics"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b57865292f71b395d69e0157ade766ba64698e9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on offline goal-reaching reinforcement learning, and pinpoints two families of methods: on-policy algorithms based on Monte-Carlo returns (and in several cases contrastive learning), and off-policy algorithms which enforce the known invariances of goal-conditioned value function through quasi-metric network architectures. This works extends TMD (Myers 2025), which previously united this two frameworks, by considering multi-step returns. Interestingly, this induces significant gains in performance across standard goal-conditioned tasks from the OGBench suite. The authors further perform an evaluation on the BRIDGE hardware setup, which confirms the strong performance of the algorithm. These results are accompanied by relevant ablations on the multi-step loss."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Empirical results are very convincing, both in breadth (involving a large number of OGBench tasks as well as hardware experiments) and in outcomes (displaying large improvements over strong baselines)."}, "weaknesses": {"value": "- The proposed algorithm is strongly aligned with TMD, but in my opinion does not fully acknowledge this. To the best of my knowledge, each component in Section 4 (except for (11), which is a direct multi-step extension of the TD loss (12)) was already introduced in TMD. If I understand correctly, the final algorithm is simply TMD with a multi-step loss. If this is the case, it should be acknowledged in full.\n- This works claims to combine off-policy and on-policy learning, but does not comment on whether the final value estimates are recovering the optimal value, or the value of the behavioral policy. Can the authors provide a formal discussion of what the proposed objectives recover, and why it is motivated?\n- Related works are overall short (e.g. there is a single reference in the paragraph on GCRL), poorly formatted (several undefined references) and poorly written (the structure of the sentences in the last two paragraph is incorrect)."}, "questions": {"value": "### Minor issues and questions\n- Line 12: remove \"in\"\n- There are several broken or wrongly formatted references through the paper\n- Line 122: this sentence is hard to follow: \"X have demonstrated that Y instead of Z\"\n- Equation 7: the notation is unclear, as $s$ and $a$ appear under the expectation, as well as on the left side. formulating the expectation over $s'$ alone would be cleaner in my opinion.\n- Equation 10: this choice seems to follow from TMD, which should be referred to in this case\n- Figure 1: the caption refers to a 10x larger horizon, is this an overstatement? What is the length of optimal trajectories in giant and colossal mazes?\n- What is the reasoning behind the selection of environments in Table 1? e.g. why is antmaze-large-explore evaluated instead of antmaze-large-navigate? The current selection appears somewhat arbitrary.\n- Two of the ablation studies (matching the first two questions in 5.3) were relegated to the Appendix. This should be noted in the text.\n- Table 3: how is the regularization parameter $\\alpha$ tuned in each environment/algorithm combination?\n\n### Conclusion\nDespite the impressive empirical results, I currently lean towards rejection. To the best of my understanding, this method is an n-step extension of TMD, which is a minor contribution but is not problematic per se. My main concern is that this strong connection between the algorithms is not directly highlighted in the paper, which as a result seems to over claim its contribution. In am happy to further discuss whether my understanding is correct during the rebuttal phase. Moreover, unlike TMD, this works lacks an analysis of the objective and its solutions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o7yNEB8bPV", "forum": "UElh7vzgKX", "replyto": "UElh7vzgKX", "signatures": ["ICLR.cc/2026/Conference/Submission21173/Reviewer_jD1e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21173/Reviewer_jD1e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761475259901, "cdate": 1761475259901, "tmdate": 1762941570989, "mdate": 1762941570989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method for offline GCRL. The authors introduce Multistep Quasimetric Estimation (MQE), which applies a multi-step Monte-Carlo return to a quasimetric distance-based method. They evaluate on OGBench and a real robot, where MQE can outperform all baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical part is clear and easy to understand. The method is simple, but it still brings a notable improvement in performance. This makes the idea accessible for other researchers and shows that even a straightforward change can lead to meaningful progress.\n\n2. The experiments provide strong comparison results. The authors choose many baselines, including some recent works. This helps confirm that the gains are not due to weak baselines and shows that the method remains strong under fair comparisons.\n\n3. The paper includes real-world testing. The method achieves a higher success rate in practice, and it is able to combine multiple motion segments into complete trajectories. This supports the claim that the approach can work outside simulation.\n\n4. There is enough hyperparameter study. The authors include an analysis that explains how different settings affect performance. This makes the method easier to apply and helps readers understand which parts matter most.\n\n5. The code had been released, showing its reproducibility"}, "weaknesses": {"value": "1. While MQE can indeed improve performance, the choice of hyperparameters and the selection of the best waypoint introduce heavy computational cost. This limits the method’s practical use, especially when scaling to more complex tasks or real-time applications.\n\n2. There are many citation errors in the paper, for example, at line 96, 91, 399, and 431. In addition, there are indexing mistakes and incorrect claims. For instance, in Algorithm 1, the loss at line 5 should refer to Eq. 11. Another example is at line 435, where the text claims that TRA-g reaches a positive success rate in the quadruple PnP task, but the table shows 0/15. These issues show that the writing and validation of statements are not strict enough.\n\n3. The value of $\\alpha$ used during policy extraction is a hyperparameter that must be tuned separately for different environments. This shows that the method needs heavy hyperparameter tuning to reach the reported performance, which is not practical. I suspect that much of the performance gain shown in the paper comes from tuning these hyperparameters instead of the strength of the method itself."}, "questions": {"value": "1. What happens if the hyperparameters are not tuned, or only tuned with a small number of trials? How much does the performance drop in that case?\n\n2. Can the authors provide theoretical support for using a geometric distribution to select the waypoint? Why is this distribution a reasonable choice compared with other alternatives?\n\n3. In line 386, the paper states that the task \"has never been completed without the use of hierarchical policies or high-level planners\". Then why is there no comparison against hierarchical policies? Also, can MQE be integrated into a hierarchical framework? If so, why not include such a comparison to better understand its advantages and limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YgFKeGxbkJ", "forum": "UElh7vzgKX", "replyto": "UElh7vzgKX", "signatures": ["ICLR.cc/2026/Conference/Submission21173/Reviewer_m7gs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21173/Reviewer_m7gs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761497099076, "cdate": 1761497099076, "tmdate": 1762941570222, "mdate": 1762941570222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Multistep Quasimetric Estimation (MQE), a novel goal-conditioned reinforcement learning (GCRL) method that unifies temporal-difference (TD) and Monte Carlo (MC) learning through a quasimetric distance representation. MQE leverages multistep returns under a quasimetric architecture to propagate value information efficiently across long horizons while maintaining theoretical consistency with optimal value functions. It further enforces action invariance and one-step consistency constraints to stabilize learning. Empirically, MQE achieves state-of-the-art performance on long-horizon offline GCRL benchmarks (up to 4000 steps) and demonstrates strong compositional generalization in real-world robotic manipulation tasks. Its key contribution is showing that multistep temporal consistency and quasimetric structures can be combined to enable scalable, end-to-end goal-reaching from raw, unlabeled offline data."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper makes an original contribution by unifying multistep value learning and quasimetric representations into a single, scalable framework for goal-conditioned reinforcement learning. The idea of integrating multistep temporal consistency with quasimetric architectures is both novel and technically elegant, addressing long-standing limitations in horizon generalization and stability. The paper is of high quality, with strong theoretical grounding, clear algorithmic exposition, and comprehensive experiments across both simulated and real-world robotic domains. Its clarity allows readers to follow complex ideas with precision, and its significance lies in demonstrating a practical path toward scalable, compositional goal-reaching in offline RL—bridging a crucial gap between theory and real-world applicability."}, "weaknesses": {"value": "While the paper is strong overall, several aspects could be improved to strengthen its impact. The theoretical analysis, while elegant, could benefit from clearer discussion of its assumptions and limitations—particularly regarding stability and convergence in high-dimensional or stochastic environments. Finally, the presentation could be improved by providing more intuition and visualization of the learned quasimetric distances to help readers better grasp the geometric and representational properties driving the observed performance gains."}, "questions": {"value": "1.Could the authors clarify the conditions under which the proposed multistep quasimetric estimation guarantees convergence or consistency? Specifically, how sensitive are these guarantees to function approximation errors or off-policy data distributions?\n\n2.The method integrates multistep temporal consistency into the quasimetric framework, but it remains unclear how much of the empirical gain arises from longer-horizon updates versus the quasimetric structure itself. Could the authors provide quantitative or qualitative evidence disentangling these effects (e.g., via controlled ablations or visualizations)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FIm28XTbpj", "forum": "UElh7vzgKX", "replyto": "UElh7vzgKX", "signatures": ["ICLR.cc/2026/Conference/Submission21173/Reviewer_4Ky7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21173/Reviewer_4Ky7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639442758, "cdate": 1761639442758, "tmdate": 1762941568934, "mdate": 1762941568934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of scaling RL to real-world, long-horizon robotics-tasks via offline GCRL. The main contribution of the paper is a novel algorithm, Multistep Quasimetric Estimation (MQE). The main technical innovation is how to learn multistep returns under quasimetric architectures while preserving the ability to learn the optimal value function. The authors show that MQE obtains state-of-the-art results in challenging long-horizon GCRL tasks in OGBench and a real-world robotics task."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The paper addresses an fundamental challenge in RL - namely, how can we scale to real-world tasks? Personally, I think the approach of trying to induce compositional learning via offline GCRL, succ, and goal-stitching is very promising. The topic will definitely be interesting to the ICLR community.\n    \n- The proposed approach is empirically highly effective compared to baselines in state-based datasets and the real-world robotics task. Overall, the evaluation is thorough. There is a large-scale evaluation of many methods and tasks."}, "weaknesses": {"value": "- **Writing:** the writing is the biggest weakness of this paper. Unfortunately, the authors focus on explaining technical details of the method to an audience of experts in offline RL / GCRL, while failing to communicate high level, key messages to a general audience (see below). I’m concerned that this will limit its interest/impact to the general ICLR audience.\n    \n    - The motivation and the problem being solved is not very coherent at the beginning of the paper. I found myself wondering for a while whether the authors were addressing GCRL, or offline RL. It took until I almost finished skim-reading the paper to confirm that the authors address offline GCRL. Looking back at the paper, I think this occurred because the abstract only addresses GCRL rather than offline GCRL, and the term “Offline GCRL” only appears in conjunction in 2 places in the manuscript.\n        \n    - What is the key insight of the paper? Why do multistep returns matter so much torwards improving performance on the long-horizon offline tasks?\n        \n    - Some minor typos in the citations:\n        \n        - Please make sure to use citep where appropriate.\n            \n        - See Lines 91, 96, 122, 399\n            \n    - Lack of clarity:\n        \n        - What is task prowess metric in Fig 3 and how does it differ from success rate?\n            \n- **Soundness of Method**: The proposed method relies on trick of sampling next states w/ higher prob according to a Bernoulli distribution, thus indicating the auxiliary loss in 4.2 is not sufficient to fix bias issues. Can the authors comment on this point? What is the relative contribution of this trick to the final performance of MQE vs the auxiliary loss?"}, "questions": {"value": "- The waypoints are randomly sampled according to the geometric distribution. Are there any theoretical benefits of geom distr. over other distributions, esp those considered in App. E?\n    \n- What is the benefit of multi-step return in this setting? From another angle, what is the problem with single step returns?\n    \n- Intuitively, why does Lp violate optimality while Lt doesn't? Can the authors provide an illustrative example or proof?\n    \n- Expts:\n    \n    - Why does MQE do particularly poorly on cube_triple_noisy?\n        \n    - Why does MQE underperform baselines on visual tasks, especially compared to HIQL?\n        \n    - Why not compare against HIRL in Bridge Data?\n        \n- Is this method applicable to online settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ecPystv5F5", "forum": "UElh7vzgKX", "replyto": "UElh7vzgKX", "signatures": ["ICLR.cc/2026/Conference/Submission21173/Reviewer_pzjw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21173/Reviewer_pzjw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709391968, "cdate": 1761709391968, "tmdate": 1762941567424, "mdate": 1762941567424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Multistep Quasimetric Estimation (MQE), an offline GCRL method that integrates multistep returns within a quasimetric network architecture.\nThe paper aims to reconcile the tension between the theoretical optimality of local TD updates and the superior horizon scaling of global Monte-Carlo methods.\nMQE updates values by regressing distances using geometrically sampled \"waypoints\" and enforcing action invariance.\nMQE is evaluated on complex stitching and noisy tasks from OGBench where it outperforms common baselines, including often outperforming the hierarchical HIQL.\nMoreover, the paper presents impressive compositional generalization in real-world robotic manipulation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper's main strength is the integration of multistep backups into the quasimetric learning framework.\nThis enables superior horizon generalization compared to prior methods, which the paper demonstrates extensively on challenging environments from OGBench requiring extremely long-horizon planning.\nThe success in the real-world BridgeData experiments is compelling.\nTo my knowledge, complex multi-stage compositionality (e.g., Quadruple Pick and Place) using a flat (non-hierarchical) policy architecture represents significant progress in scalable GCRL."}, "weaknesses": {"value": "* The multistep backup (Eq. 9) is inherently biased towards the behavior policy​ when the step k′>1. While the authors mitigate this with 1-step consistency weighting and action invariance, the justification for how this combination robustly overcomes the strong bias of the multistep return is primarily empirical.\n* In my view, the paper is missing a comparison to recent work [1] that addresses the identical challenge of achieving long-horizon GCRL performance with a flat policy.\n\nThe presentation has several issues:\n* The related work section has several missing or ill-formatted citations.\n* The paper has several incomplete sentences, e.g., line 91.\n* Page 5 includes a lot of extra spacing.\n* Overall, I find the methods section (Section 4) hard to follow. It would be beneficial to add structure to the section and provide additional guidance to the reader."}, "questions": {"value": "* Is the quasimetric architecture essential for the success of the multistep backup, or could this backup strategy also improve standard value-based methods (e.g., IQL) without the explicit distance structure?\n\nNote: The paper title of the PDF does not match the title on OpenReview."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wliiuaN3FT", "forum": "UElh7vzgKX", "replyto": "UElh7vzgKX", "signatures": ["ICLR.cc/2026/Conference/Submission21173/Reviewer_sYkc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21173/Reviewer_sYkc"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909784307, "cdate": 1761909784307, "tmdate": 1762941565532, "mdate": 1762941565532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}