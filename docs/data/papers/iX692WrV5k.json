{"id": "iX692WrV5k", "number": 13388, "cdate": 1758217297289, "mdate": 1759897440914, "content": {"title": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains", "abstract": "The rapid advancement of large language models (LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research.", "tldr": "", "keywords": ["Large Language Models (LLMs)", "Benchmarking", "Domain-specific Evaluation", "Multilingual Evaluation", "Indic Languages", "Low-resource Domains", "Knowledge Systems", "Agriculture", "Legal", "Finance", "Ayurveda", "Subdomain-level Analysis", "Open-source Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/85da8e5b53cff19c3da414ac144670ad2f6cae98.pdf", "supplementary_material": "/attachment/13a9aa023c0fc45b4bf703e6c04e7be5d3853ac9.zip"}, "replies": [{"content": {"summary": {"value": "this paper propose a bilingual (English+Hindi) benchmark for evaluating LLMs in Indian domains (like agriculture, legal, finance, and ayurveda). It includes 74000+ QA pairs from authentic exams. Testing 29+ models reveals strong performance in legal and finance but clear weaknesses in ayurveda and hindi tasks. The work fills a key gap in domain and culture specific evaluation for India."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. the benchmark dataset is very rich in terms of (a) number of sample points (QA), (b) in language (i.e., non-english) and (c) domain/subdomains focused on.\n\n2. the number of models being evaluated is huge and ranges from small param to larger -> so the conclusions/insights are extensive since the work covers many models"}, "weaknesses": {"value": "1. I am excited to see the extensive number of llms being evaluated however it would be helpful to have some sort of qualitative error analysis to find the patterns where them model(s) fail and perhaps that will indicate \"why\" they fail. the current results list aggregate scores across domains/subdomains, but there’s little insight into failure modes (common errors). Having some sort of error taxonomy with manual audits per domain and difficulty might be a useful information to readers\n\n2. the results section shows a lot of numbers (as expected since multiple models), but it would help to have some way to differentiate between numbers (perhaps via bold, using colors of varying degree) to quickly grab reader's attention on high scores vs least scores (for example)\n\n3. The paper shows motivation for the need of this proposed benchmark. however, it  does not map the proposed benchmark against related Indic datasets at the task and domain level. How does this benchmark overlaps/gaps against existing datasets? A comparison table might help to highlight the true uniqueness of this benchmark."}, "questions": {"value": "1. There are a few grammatical errors in the paper like eg: \n(a) line 048 (\"alone, Over 40 million\")\n(b) line 192 (maybe you want to put the footnote towards the end of sentence?\n\n2. Would it be possible to provide more details about the computational setup and API usage (such as hardware resources or evaluation environment) used to run the model evaluations? I do see some info in appendix but more infor might be needed for reproducibility for someone reading the paper and planning to build on top of this work.\n\n3. the dataset creation relies heavily on OCR extraction from diverse exam PDFs. so how did you ensure consistency between eng and hindi and handle cases where OCR quality was poor or ambiguous?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PDWlnS99Ey", "forum": "iX692WrV5k", "replyto": "iX692WrV5k", "signatures": ["ICLR.cc/2026/Conference/Submission13388/Reviewer_4EtE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13388/Reviewer_4EtE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857229824, "cdate": 1761857229824, "tmdate": 1762924026798, "mdate": 1762924026798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BhashaBenchV1, a multi-task bilingual benchmark in Hindi and English. The benchmark spans four major domains: Agriculture, Legal, Finance and Ayurveda. It includes diverse task formats, questions specific to India’s cultural and religious context. It includes questions in diverse formats and questions are categorized into three difficulty levels. The evaluation shows that models perform consistently better in English compared to Hindi. Models tend to struggle in areas such as Agriculture and Ayurveda."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a new benchmark that includes questions related to less-represented domains such as Agriculture, Ayurveda, Legal and Finance in the Indian cultural context. \n2. The authors use a sound multi-step data preparation pipeline which focuses on ensuring quality by using formatting pipelines and manual validation by experts. \n3. The benchmark is composed of diverse question types and represents 90+ subdomains. \n4. The authors evaluate open and closed source models of various sizes across these domains and present the analysis on domains/sub-domains where models struggle to do well. The results clearly show the importance of the benchmark and the need to measure performance in diverse linguistic and cultural contexts."}, "weaknesses": {"value": "1. The benchmark has a bias towards exam-style questions since it is sourced from professional and government exams. Similarly, it covers a limited set of domains with 70% of the data being in English."}, "questions": {"value": "1. How scalable is the data preparation pipeline to extend it to other topics/domains? The paper mentions manual annotation of the dataset. What proportion of questions required modification during manual review?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AoGf8vKN4R", "forum": "iX692WrV5k", "replyto": "iX692WrV5k", "signatures": ["ICLR.cc/2026/Conference/Submission13388/Reviewer_Ug1c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13388/Reviewer_Ug1c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888845394, "cdate": 1761888845394, "tmdate": 1762924026429, "mdate": 1762924026429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce BhashaBench, a domain-specific, multi-task bilingual benchmark aimed at evaluating language models in Indic contexts. The dataset is sourced from publicly available government and domain-specific exams. The benchmark is evaluated across 29+ LLMs, revealing performance gaps across domains between English and Hindi."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The dataset is a valuable contribution, as the authors curate a benchmark that addresses the underrepresentation of benchmarks that evaluate language models in Indic-centric contexts.\n- The benchmark contains questions across diverse topics and subdomains and enables fine-grained evaluations across all. \n- Their experiments cut across both frontier and open-source models, revealing disparities in model performance. \n- The bilingual focus of their model is very relevant for evaluating models ' equity and utility in English and Hindi."}, "weaknesses": {"value": "- The evaluation relies heavily on multiple-choice questions. MCQs are easy to evaluate and have been employed across several benchmarks. There are still questions surrounding whether they truly capture model understanding or reasoning abilities.  Are the distractors sufficiently challenging, particularly on domains where performance is above  90%? What happens if you scale the options? Previous work has shown that sometimes scaling distractors beyond 4 options increases the difficulty of guessing from the models. Also, is it possible to randomize the option ordering during evaluation and repeater evaluations across multiple permutations, just so results are more significant? \n\n- The paper does a lot of evaluations and provides a lot of numbers across domains, languages, and models. However, their presentation lacks a thorough qualitative analysis of model failures across domains. The paper reads like a laundry list of numbers and is a bit overwhelming to read through the numbers. There are little to no insights that link model performance to data characteristics or domain difficulty\n\n- The dataset is built from “publicly available exam question papers online” and similar sources. While a lot of models do not release their data publicly, are there thoughts on potential data contamination and leakage?  Also, since the majority of questions in the benchmark are in English, is the potential for data leakage even stronger ? Like, even if it's in English in an Indian context, is it possible that it would have been seen during training? \n\n- The absence of confidence intervals or significance testing makes it hard to assess how meaningful the reported accuracy differences are. Can you report them ? \n\n-  Efforts like this to bridge the gap in model utility between high-resourced and low-resourced languages are commendable. However, the current scope of the work is largely Hindi-centric, rather than broadly multilingual. This makes the contribution feel somewhat limited in scope for a venue like ICLR, which tends to expect more depth in experiments and analysis for a benchmark paper. Given its focus and depth in one language, the work might be better suited for a *CL where language-specific benchmarks and analyses might be more strongly valued."}, "questions": {"value": "- Can you report significance tests or confidence intervals for the results?\n- Is there a reason why Gemini models were not included in the evaluation? They are known to have strong multilingual capabilities and could provide a more complete picture of state-of-the-art results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EDSeZt2OqD", "forum": "iX692WrV5k", "replyto": "iX692WrV5k", "signatures": ["ICLR.cc/2026/Conference/Submission13388/Reviewer_fpeJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13388/Reviewer_fpeJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047231175, "cdate": 1762047231175, "tmdate": 1762924025994, "mdate": 1762924025994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}