{"id": "CrXcfMLR9q", "number": 507, "cdate": 1756743179944, "mdate": 1759898256599, "content": {"title": "Triangle Multiplication is All You Need for Biomolecular Structure Representations", "abstract": "AlphaFold has transformed protein structure prediction, but emerging applications such as virtual ligand screening, proteome-wide folding, and de novo binder design demand predictions at a massive scale, where runtime and memory costs become prohibitive.\nA major bottleneck lies in the Pairformer backbone of AlphaFold3-style models, which relies on computationally expensive triangular primitives—especially triangle attention—for pairwise reasoning.\nWe introduce Pairmixer, a streamlined alternative that eliminates triangle attention while preserving higher-order geometric reasoning capabilities that are critical for structure prediction.\nPairmixer substantially improves computational efficiency, matching state-of-the-art structure predictors across folding and docking benchmarks, delivering up to 4x faster inference on long sequences while reducing training cost by 34%.\nIts efficiency alleviates the computational burden of downstream applications such as modeling large protein complexes, high-throughput ligand and binder screening, and hallucination-based design.\nWithin BoltzDesign, for example, Pairmixer delivers over 2x faster sampling and scales to sequences 30% longer than the memory limits of Pairformer.", "tldr": "Simple biomolecular structure prediction architecture", "keywords": ["structure prediction", "cofolding", "triangle multiplication"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/539f2897f4b1848ae927fec84e80f87f1629319e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work applies triangle multiplication in the large scale of current biomolecular structure prediction. Such application scenario has not been investigated with models using triangle multiplication. The authors show that, for the Pairformer backbone, when maintaining triangle multiplication, and omitting triangle attention, the training and inference efficiency can be improved."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, clearly depicting the methodology;\n2. The training cost and inference speed are both improved;\n3. The large-scale biomolecular structure prediction scenario is of great practical importance;\n4. The experiments are sufficient."}, "weaknesses": {"value": "1. This work lacks of methodological contribution. The authors omit other modules and maintaining only the triangle multiplication. This modification is considered very trivial and not that novel. The importance of triangle multiplication has already been investigated by several proceeders, as the authors themselves claimed in the paper. The author is the first one deleting triangle attention for PairFormer.  The main contribution is testing if we can achieve better performance-efficiency trade-off using only triangle multiplication. \n2. The efficiency improvement is not that satisfying. To be fair, it is adequate when the methodological contribution is enough. But since the contribution is minor, I would expect giant efficiency leap to complement the limited contribution."}, "questions": {"value": "Whether there is any non-trivial part for deleting triangle attention and other modules? This could be a potential methodological contribution if there is any.\n\nSuggestion: If my evaluation remains consistent after I read the authors' response, I would suggest that the author considers submitting this work to an application-oriented venue, e.g., Nat. Comm., Nat. Comp. Sci., Sci. Adv. The value of this work is mostly about the application results but without meaningful methodological insights. After giving stronger application evaluations, packing as a tool box or executable empirical platform, this work is more suitable to those top application-oriented venues. The methodological novelty, in my opinion, is enough for them. \n\nI am also open to change my mind if the author can prove the methodological value that I failed to see."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tpvXc0GfSk", "forum": "CrXcfMLR9q", "replyto": "CrXcfMLR9q", "signatures": ["ICLR.cc/2026/Conference/Submission507/Reviewer_J8sg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission507/Reviewer_J8sg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760604934003, "cdate": 1760604934003, "tmdate": 1762915534237, "mdate": 1762915534237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the computational efficiency issue in AlphaFold and argues that the triangle attention operation contributes little to the final predictive performance while introducing significant computational overhead. Based on this observation, the authors propose the PairMixer block, a simplified variant of the Pairformer block used in AlphaFold3, in which operations with minimal performance benefits are removed. Experimental results on the RCSB test set demonstrate that PairMixer achieves comparable performance to the original Pairformer while offering up to a 4x improvement in inference speed."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper focuses on an important research problem, i.e., to accelerate AlphaFold and make it more lightweight, which is critital for down-stream applications like virtual screening;\n2. This paper is well-written and clearly-structured. The figures effectively support the understanding of the proposed method, and the authors provide sufficient background and preliminaries to contextualize their work."}, "weaknesses": {"value": "1. The contribution of this paper appears limited. The proposed method can be viewed primarily as an engineering optimization of the original Pairformer, without introducing substantial new insights. Without deeper analysis or justification of the design choices, the current contribution may not meet the novelty threshold typically expected for a venue such as ICLR. Furthermore, the finding that the triangle attention module contributes minimally to performance is not particularly surprising; this has been informally noted by several researchers through ablation studies, even though such observations have not been formally published.\n\n2. The experimental evaluation of PairMixer is insufficient. The results are reported only on the RCSB dataset (533 structures), which limits the generalizability of the conclusions. It is recommended that the authors adopt a broader evaluation protocol, such as the one used in Boltz-2, to strengthen the empirical validation of their method.\n\n3. The use of the phrase “all you need” in the title, while common in machine learning literature, is not appropriate in this context. The PairMixer/Pairformer serves only as the model trunk within a structure prediction framework, whereas the diffusion module plays an equally important role. Therefore, the current title may overstate the scope and completeness of the proposed contribution."}, "questions": {"value": "No further questions. Please see the “Weaknesses” section for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1IvCyt32JN", "forum": "CrXcfMLR9q", "replyto": "CrXcfMLR9q", "signatures": ["ICLR.cc/2026/Conference/Submission507/Reviewer_GgDp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission507/Reviewer_GgDp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761200740509, "cdate": 1761200740509, "tmdate": 1762915534107, "mdate": 1762915534107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Pairmixer, a modified AlphaFold3-style backbone that (a) deletes triangle attention and sequence updates from the Pairformer backbone, (b) keeps only triangle multiplication + pairwise FFNs to update the pair representation, and (c) leaves the single-sequence representation unchanged and feeds it directly to the downstream diffusion module. The experiments are shown with maintaining folding / docking accuracy while significantly improving speed (up to 4× faster inference on 2048-token sequences and 34% less training compute), and unlock downstream design workflows that previously ran out of memory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Reviewer appreciates the following contributions:\n\n- **impactful and practical**: the paper addresses a real bottleneck in AlphaFold-style models by removing triangle attention for faster, more scalable inference and training. Furthermore, it enables long-sequence and large-complex modeling that was previously infeasible due to memory or computational limits.\n\n- **Empirical validation**: Demonstrates near-identical accuracy to AlphaFold3-class baselines across folding, docking, and binder design tasks, with up to 4× speedup and lower memory use. Experiments include comparisons on Boltz-1, Transformer, and ablations, FLOPs analysis, and realistic large-scale design benchmarks.\n\n- **Simple but be efficient**: the paper shows that triangle multiplication alone suffices for capturing higher-order geometric consistency, providing a clearer understanding of what inductive biases matter. In particular, the final architecture will be the form of *Pairmixer = triangle multiplication + FFN over z, recycle it N times*. \n\n- **presentation**: the paper is well-written and easy to follow."}, "weaknesses": {"value": "- **Method Novelty**: Novelty somewhat incremental: Prior works (e.g., Genie2, MiniFold) already suggested triangle multiplication is key; this paper mainly extends the idea to AF3 scale rather than introducing it conceptually. This requires further analysis to highlight key differences between Pairmixer versus prior works, for e.g., what should we do to adapt for the protein structure design task?\n\n- **Limited generalization tests:**\nWhile the paper benchmarks extensively on protein–protein and protein–ligand systems, all evaluations remain within domains similar to the training distribution of Boltz-1 (PDB-scale protein complexes). The work does not assess generalization to more diverse biomolecular systems, such as RNA–protein assemblies, RNA-only structures, metalloproteins, or highly flexible/transient complexes. These categories often require different geometric reasoning and long-range constraints, where triangle attention might still provide advantages. Without such tests, it’s quite unclear whether the proposed architecture truly generalizes beyond well-structured protein complexes, or if its performance degrades on systems with **unconventional topologies or more dynamic conformational behavior**."}, "questions": {"value": "**Missing sequence update ablation:**\nThe paper removes sequence updates but doesn’t isolate their effect. It’s unclear how much of the performance change comes from dropping triangle attention versus removing sequence updates.\n\n**Lack of discussion on limitations vs AlphaFold3:**\nCan authors discuss where the simplified model may fail compared to full AlphaFold3 — for example, in modeling highly flexible regions, RNA–protein complexes, or subtle side-chain rearrangements requiring long-range attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZjLt1O7zIZ", "forum": "CrXcfMLR9q", "replyto": "CrXcfMLR9q", "signatures": ["ICLR.cc/2026/Conference/Submission507/Reviewer_r3YY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission507/Reviewer_r3YY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833819644, "cdate": 1761833819644, "tmdate": 1762915533878, "mdate": 1762915533878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new operator, Triangle Multiplication, as a core mechanism for relational reasoning and geometric representation learning. The method is presented as a lightweight yet expressive alternative to traditional self-attention, aiming to capture higher-order interactions among triplets of entities efficiently. The authors apply this operation to tasks, showing that it can achieve competitive or improved performance compared to transformer-style baselines.\n\nThe conceptual idea is creative and well-motivated; however, the presentation lacks sufficient technical clarity, the experimental scope is somewhat limited, and the empirical analysis does not fully demonstrate the operator’s claimed generality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tInteresting conceptual direction: The idea of moving from pairwise attention to triangle-based relational modeling is novel and aligns with emerging research on geometric and higher-order attention.\n2.\tSimplicity of the operator:  The formulation is elegant and could potentially be a computationally efficient substitute for attention in specific contexts.\n3.\tPotential for extension: The proposed mechanism could inspire further work in 3D molecular or graph-structured domains, where triplet relations are natural.\n4.\tReadable overall motivation:  The high-level rationale and related work are generally well-written."}, "weaknesses": {"value": "- Limited Comparative Breadth\n\nThe experiments benchmark against a few baselines but omit several directly relevant contemporary models, including:\n\t1. Higher-order attention variants (e.g., Tensor Attention, Relational Transformer)\n\t2. Geometric and 3D reasoning frameworks (e.g., SE(3)-Transformer, EGNN)\n\t3. Diffusion-based relational models and equivariant graph networks.\n\nWithout these comparisons, it is difficult to judge whether Triangle Multiplication provides a fundamentally better abstraction or merely a reparameterization of higher-order attention.\n\n- Lack of Theoretical Clarity\n\nThe mathematical definition of “Triangle Multiplication” is presented at a high level but lacks rigorous derivation or clear connection to known tensor operations:\n\t1. The operator’s expressive power (what functions it can approximate) is not discussed.\n\t2. No complexity analysis is provided—readers cannot tell if it scales better than self-attention for large N.\n\n- Weak Empirical Validation\n\nWhile the experiments show some improvement, they remain qualitative and dataset-limited:\n\t1. The selected tasks are small-scale and do not reflect real-world complexity.\n\t2. No ablation studies are provided to isolate the contribution of the triangle operator vs. other components."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QSbjp62ACj", "forum": "CrXcfMLR9q", "replyto": "CrXcfMLR9q", "signatures": ["ICLR.cc/2026/Conference/Submission507/Reviewer_qqDT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission507/Reviewer_qqDT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009412750, "cdate": 1762009412750, "tmdate": 1762915533709, "mdate": 1762915533709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}