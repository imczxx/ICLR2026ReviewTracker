{"id": "diVf17SNek", "number": 7300, "cdate": 1758014831136, "mdate": 1763145568190, "content": {"title": "Precise and Interpretable Editing of Code Knowledge in Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated outstanding capabilities in various code-related tasks, including code completion, translation, or summarization. However, these pretrained models are static, posing a challenge to incorporate new knowledge into an LLM to correct erroneous behavior. Approaches such as retraining or fine-tuning demand extensive labeled datasets and might be computationally expensive, while prompt engineering fails to change models permanently. Knowledge Editing (KE) techniques offer a more efficient alternative, enabling model updates with minimal data, even just a single example. Nevertheless, existing KE methods often manipulate parameters within the Transformer's multi-layer perceptrons (MLPs), where neuronal polysemanticity hinders both the precision and interpretability of the edits. To address these limitations, we exploit TransCoder, an MLP-like model component with a wide and sparsely activated hidden feature vector. Specifically, we introduce **TransCoder-based Precise Editing** (**TCPE**), a novel method that leverages the sparsity and monosemanticity of the TransCoder’s neurons for highly localized knowledge editing. TCPE exhibits neuron-level mechanistic interpretability characteristics, revealing the correspondence between the edited neurons and the specific code-related knowledge. Furthermore, we present KECode, a new evaluation benchmark for code-to-code translation based on functional equivalence. Using KECode, we conduct a systematic evaluation of representative KE methods in the context of code-to-code translation. Our experimental results demonstrate that TCPE outperforms existing KE methods, achieving a substantial improvement of translation accuracy of CodeLlama-7b-Instruct from 57.5% to 64.0% in a low-resource scenario of Java-to-D translation.", "tldr": "", "keywords": ["Programming Languages", "Code-to-code Translation", "Knowledge Editing", "code LLMs", "Software Engineering"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be2170ddf4abf5f67dcf8bf554146fa875397dd4.pdf", "supplementary_material": "/attachment/1e8f5d18422c6429813fe08f05b217e50d0c9ba3.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the TCPE model editing method based on TransCoder. This method replaces the original MLP layers with an MLP-like model component that has a wide and sparsely activated hidden feature vector, avoiding the imprecision and uninterpretable edits caused by the neuronal polysemanticity of the original MLP. The paper also introduces KECode, a new code-to-code model editing benchmark. Experiments on KECode and existing model editing benchmarks demonstrate the effectiveness of TCPE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Replacing the original MLP with a wider and sparse MLP using TransCoder is novel and interesting.\n* KECode, a code-to-code model editing benchmark, is proposed, providing a valuable resource for the community.\n* The effectiveness of TCPE is demonstrated on the KECode, ZsRE, and CounterFact datasets.\n* An in-depth analysis of TransCoder neurons is conducted, offering valuable insights."}, "weaknesses": {"value": "* TCPE lacks principled innovation, as both TransCoder and ROME-like editing are existing works.\n* The evaluation of the model editing task seems limited to single-case edits, lacking assessments closer to real-world scenarios such as sequential or batch edits.\n* There is no discussion of TCPE’s scalability; its performance when editing large batches or performing sequential edits remains unknown.\n* TransCoders require additional training, and the training environment appears to be more demanding compared to the baselines. Moreover, it is unclear whether replacing the MLP with TransCoders affects the model’s original capabilities."}, "questions": {"value": "Would the introduction of TransCoders increase inference latency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BRy8QL6PCH", "forum": "diVf17SNek", "replyto": "diVf17SNek", "signatures": ["ICLR.cc/2026/Conference/Submission7300/Reviewer_FKNG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7300/Reviewer_FKNG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761492165767, "cdate": 1761492165767, "tmdate": 1762919418555, "mdate": 1762919418555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper makes two key contributions to improve the precision and interpretability of knowledge editing in code LLMs on code translation domain.\n\nFirst, it proposes TransCoder-based Precise Editing, which is a method that replaces the Transformer’s MLP layer with a sparse TransCoder module, enabling edits that target specific highly activated neurons associated with a particular piece of code knowledge. This design allows localized and interpretable edits, minimizing side effects on unrelated behavior while providing clear neuron-level insight into how knowledge is inserted.\n\nSecond, the authors introduce KECode, a new benchmark for evaluating knowledge editing in code translation. KECode consists of 600 Java-to-D translation examples paired with unit tests for functional correctness verification. Using KECode, the paper shows that TCPE significantly outperforms existing methods such as ROME, MEMIT, and PMET on correcting Java-to-D translation errors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* For originality, TCPE presents a novel mechanism that leverages the sparsity and monosemanticity of TransCoder neurons to perform more precise, localized edits. \n* For clarity, the authors place strong emphasis on interpretability. Because TCPE operates at the neuron level, they can explicitly identify which neurons are edited and link them to corresponding knowledge changes. The empirical finding that “highly active neurons carry more essential information during knowledge injection” is well-supported and insightful, strengthening the paper’s interpretability claims.\n* For quality, the evaluation is methodologically solid. The paper reports multiple complementary metrics, including efficacy, specificity, and reliability, as well as detailed ablation studies and granular analysis tailored to the knowledge editing context."}, "weaknesses": {"value": "* For scope and generalization, this paper focuses solely on Java-to-D code translation. It is unclear whether the approach generalizes to other software engineering tasks such as code completion, bug fixing, or program repair. The KECode benchmark evaluates functional error correction, which is one specific type of knowledge editing. It would also be interesting to discuss or demonstrate applicability to, for example, inserting new API knowledge or modifying non-functional aspects of code, which surface broader SWE contexts.\n* For practicality and Integration, TCPE requires modifying the model architecture by replacing MLP layers with TransCoder modules. The paper does not clarify whether this change requires retraining the new layers or fine-tuning the entire model. Additional discussion on how TCPE integrates with other architectures (e.g., MoE) would help assess its practical adoption potential.\n* For baseline adaptation, while TCPE outperforms existing NLP-based editing methods, like ROME, MEMIT, etc, these baselines were originally designed for factual knowledge editing in natural language models, not code. Because code has strict syntax and execution semantics, these baselines may be disadvantaged. The paper would benefit from either a stronger code-specific baseline, or a clearer discussion of how baseline implementations were adapted to ensure fairness."}, "questions": {"value": "Two questions:\n\n* How were the specific layers {10, 19, 23} selected for replacing MLP layers with TransCoder modules? Were these layers empirically identified or based on prior interpretability insights about CodeLlama?\n* When initializing a TransCoder-modified model from pretrained weights, how are the parameters loaded or transferred to the new architecture? Is there a compatibility or adaptation step between the original MLP weights and the TransCoder modules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PlzFBjFVMV", "forum": "diVf17SNek", "replyto": "diVf17SNek", "signatures": ["ICLR.cc/2026/Conference/Submission7300/Reviewer_sF6r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7300/Reviewer_sF6r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761784767579, "cdate": 1761784767579, "tmdate": 1762919418153, "mdate": 1762919418153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of precise and interpretable knowledge editing (KE) in Large Language Models (LLMs) for code-related tasks. The authors argue that existing KE methods, which often target standard MLP layers, are hindered by neuronal polysemanticity, leading to imprecise edits and poor interpretability. To solve this, the authors propose TransCoder-based Precise Editing (TCPE). This method involves two key stages:\n1. Architectural Modification: The standard MLP layer in a target Transformer (CodeLlama-7b-Instruct) is replaced by a \"TransCoder\" module—a sparse, wide, MLP-like component from prior work (Dunefsky et al., 2024) that is trained to have more monosemantic neurons.\n2. Editing Mechanism: A ROME-like update is applied, but it is restricted only to the small set of \"active neurons\" in the TransCoder module that are relevant to the knowledge being corrected.  \n\nFor evaluation, the authors introduce KECode, a new benchmark for code-to-code translation (Java-to-D) that uses functional equivalence (i.e., unit test pass/fail) as the success metric. Their experiments show that TCPE on the modified architecture (e.g., \"LTC4\") outperforms baseline KE methods (ROME, MEMIT, etc.)"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Knowledge editing is an important research direction to save computational resources by avoiding retraining."}, "weaknesses": {"value": "I'm not familiar with this field, so I will give my confidence score to 1. Please lower my score weight for this paper."}, "questions": {"value": "Does the method apply to more applications and datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "h2J8f7shsl", "forum": "diVf17SNek", "replyto": "diVf17SNek", "signatures": ["ICLR.cc/2026/Conference/Submission7300/Reviewer_VVHx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7300/Reviewer_VVHx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912206500, "cdate": 1761912206500, "tmdate": 1762919417786, "mdate": 1762919417786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a TransCoder-based Precise Editing (TCPE) method, to edit code knowledge in large language models. It also presents a new benchmark, KECode, for code-to-code translation based on functional equivalence. Experimental results\ndemonstrate that TCPE outperforms existing KE methods, achieving a substantial improvement of translation accuracy of CodeLlama-7b-Instruct from 57.5% to 64.0% in a low-resource scenario of Java-to-D translation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a new knowledge editing method for code LLMs. There is also a new benchmark for evaluating the performance of LLMs on code-to-code translation.\n2. Experimental results show that, TCPE outperforms existing knowledge editing methods with significant margins.\n3. A neuron-level interpretability mechanism is introduced to effectively indicates the connection between the edited neurons and the inserted knowledge."}, "weaknesses": {"value": "1. Neither codebase nor dataset is provided to confirm the reproducibility. \n2. This paper lacks discussions of limitations and broader impact.\n3. The presentation should be improved. For example, fonts in tables and figures can be larger for better reading experience."}, "questions": {"value": "Would you like to enlarge the fonts in Figure 1, Figure 2, Table 3 and Table 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pLnB6Z7XsH", "forum": "diVf17SNek", "replyto": "diVf17SNek", "signatures": ["ICLR.cc/2026/Conference/Submission7300/Reviewer_76ke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7300/Reviewer_76ke"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932758204, "cdate": 1761932758204, "tmdate": 1762919417436, "mdate": 1762919417436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}