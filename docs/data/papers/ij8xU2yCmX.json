{"id": "ij8xU2yCmX", "number": 23916, "cdate": 1758350316958, "mdate": 1759896790642, "content": {"title": "Less is More: Resource-Efficient Low-Rank Adaptation", "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for large language models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While recent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, ReLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. ReLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices update to dynamically trade-off the system resource budget and model performance. ReLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demonstrating improved efficiency and robustness. Anonymous codes are submitted with the paper and will be publicly available.", "tldr": "E-LoRA reduces training overhead and task interference by sharing low-rank adapters across layers and selectively dropping parameters, enabling efficient and robust fine-tuning.", "keywords": ["Large Language Models; Efficient Fine-Tuning; Low-rank Adaptation;"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/827a777686f64e44fbfdc3dc6da422dc5f5e6ebd.pdf", "supplementary_material": "/attachment/f9c172007e402ff84acb55b0758e02f72a847b14.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ReLORA, a lightweight framework designed to overcome the parameter redundancy and task interference limitations inherent in standard LoRA. ReLORA's architecture is built on two core innovations: a unified asymmetric architecture that utilizes a single, globally shared A matrix across all transformer layers to encode common knowledge, which is complemented by specialized, layer-specific B matrices to capture fine-grained adaptations. Additionally, the framework features a dynamic Reducer mechanism that, during training, intelligently freezes less important B matrices based on calculated importance scores, allowing for a flexible trade-off between computational resources and model performance. The paper validates this approach across diverse modalities—including language, vision-language, and diffusion models—demonstrating that ReLORA consistently outperforms standard LoRA and other strong baselines on tasks like commonsense reasoning, visual instruction tuning, and image generation, all while achieving superior parameter efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Superior Parameter Efficiency via Asymmetric Architecture: The paper introduces a unified asymmetric architecture that effectively tackles parameter redundancy. By employing a single, globally shared A matrix across all layers, it captures common, generalizable knowledge , while using specialized, layer-specific B matrices to learn fine-grained task details. This design drastically reduces the total number of trainable parameters compared to standard LoRA and its variants, achieving SOTA results with significantly fewer parameters.\n\n- Dynamic Resource-Aware Training: ReLoRA introduces a Reducer mechanism, which is a dynamic training strategy, not a post-training pruning method. This component intelligently calculates the importance of different B matrices during training and selectively freezes the least contributive ones . This allows users to flexibly trade off the computational budget and model performance, enabling efficient adaptation even under stringent resource constraints.\n\n- Effective Mitigation of Task Interference: The framework is explicitly designed to handle complex, heterogeneous datasets where standard LoRA suffers from task conflicts . By decoupling general knowledge (in the shared A matrix) from specialized knowledge (in the B-heads), ReLoRA effectively reduces conflicting objectives. This is strongly validated in multimodal experiments, where ReLoRA avoids the significant performance collapse that vanilla LoRA experiences when trained on diverse data mixtures.\n\n- Broad Generalizability Across Modalities: The paper demonstrates that ReLoRA's benefits are not limited to language models. The method is successfully applied and shown to consistently outperform strong baselines across three distinct and diverse modalities: commonsense reasoning in LLMs, visual instruction tuning in multimodal models, and high-fidelity image generation with diffusion models."}, "weaknesses": {"value": "- `Limited Innovation`: The ReLoRA method is overly simplistic, and its core motivation ($i.e.$, low-resource LoRA research [1-3]) has already been extensively explored.\n- `Limited Presentation`: Figure 4 fails to adequately explain the proposed ReLoRA. It is highly confusing whether the number of B matrices is singular, multiple per layer, or determined by the single-task vs. multi-task experimental setup. Furthermore, the mechanism of the importance score vector is unclear; how importance is judged and the utility of the resulting sampling distribution are not elaborated upon.\n- `Limited Comparison`: ReLoRA's approach of decoupling the LoRA A and B matrices across different layers restricts the applicability of other, more efficient LoRA initialization methods [4-6].\n- `Limited Code Availability`: The code in the supplementary material is disorganized. The provided scripts are non-functional due to numerous missing dependencies and files, many of which seem to be included merely as placeholders. This severely impedes further review and reproducibility.\n\n[1] LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters\n\n[2] VeRA: Vector-based Random Matrix Adaptation\n\n[3] LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning\n\n[4] PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models\n\n[5] CoLA: Collaborative Low-Rank Adaptation\n\n[6] LoRA-GA: Low-Rank Adaptation with Gradient Approximation"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DwYqsEtaSJ", "forum": "ij8xU2yCmX", "replyto": "ij8xU2yCmX", "signatures": ["ICLR.cc/2026/Conference/Submission23916/Reviewer_Fhov"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23916/Reviewer_Fhov"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482376256, "cdate": 1761482376256, "tmdate": 1762942854253, "mdate": 1762942854253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes E-LoRA, a variation of LoRA for PEFT.\n\nIts general idea is to sharing one matrix across layers and pruning only a subset of adapters per step with a small router. \n\nThe proposed method shows clear improvement on the modalities and tasks across language, VLM, and diffusion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The general idea to *general idea is to sharing one matrix across layers and pruning only a subset of adapters per step with a small router.* is pratical and straight-foward. \n\n+ Clear improvement over the baselines.\n\n+ Overall this paper is easy to follow.\n\n+ This paper covers the improvement across multi-modalities."}, "weaknesses": {"value": "- The technique novelties of the proposed method remains limited. Specifically, prior work (e.g., AdaLoRA, DyLoRA) has already shared or frozen matrix to remove inter-matrix redundancy and shown dynamic or adaptive budgets across layers.\n\n- The compared state-of-the-art PEFT methods are significantly missing. Some more recent and much stronger PEFT methods are mssing for comparison, for example:\n\n[1] DoRA: Weight-Decomposed Low-Rank Adaptation. ICML 2024.\n\n[2] VeRA: Vector-based Random Matrix Adaptation. ICLR 2024.\n\n[3] Foura: Fourier low-rank adaptation. NeurIPS 2024.\n\n[4] SSH: Sparse Spectrum Adaptation via Discrete Hartley Transformation. NAACL 2024.\n\n- The Router in the proposed method is described as “lightweight”, but its gating granularity (token, head, or sequence), regularization and throughout are not quantified.\n\n- No activation memory or KV-cache impact is reported. Please note, for sequence models this dominates inference costs.\n\n- A more breakdown analysis on the effciency should be reported, for example, on the FLOPs of the adapter, multi-head and etc. \n\n- The ablation study is not extensive. Ideally, the impact of each component should be considered as a setting and report the outcomes.\n\nMeanwhile, there is also some minor issue to be fixed:\n\n- The impact of the temperature parameter is not analyzed. \n\n- Some figures mix parameter count.\n\n- This paper only discusses training-time only. The inference-time and other inference behaviour should be considered."}, "questions": {"value": "Please refer to the weakness section and address these point-by-point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U4c2Zoleb5", "forum": "ij8xU2yCmX", "replyto": "ij8xU2yCmX", "signatures": ["ICLR.cc/2026/Conference/Submission23916/Reviewer_t7Ep"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23916/Reviewer_t7Ep"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750614343, "cdate": 1761750614343, "tmdate": 1762942853849, "mdate": 1762942853849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReLoRA (Resource-Efficient Low-Rank Adaptation), a novel parameter-efficient fine-tuning framework that revisits LoRA through the lens of cross-layer and inter-matrix redundancy. The authors identify that conventional LoRA suffers from duplicated low-rank subspaces and interference across heterogeneous domains. ReLoRA addresses these issues via two key designs:\n* A Unified Asymmetric Architecture, where all Transformer layers share a single global down-projection matrix $A$, while layer-specific up-projection matrices $B_i$ are dynamically selected by a router network;\n* A Reducer module, which dynamically freezes less important $B_i$ matrices based on layer contribution scores, thereby balancing performance and resource usage during training.\nExperiments across language reasoning (LLaMA3-8B), multimodal instruction tuning (LLaVA-7B), and diffusion image generation demonstrate that ReLoRA outperforms LoRA and strong variants like HydraLoRA and GraphMoE, achieving comparable or better accuracy with 40–60% fewer trainable parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* *Clear motivation and solid empirical grounding*: The paper convincingly identifies inter-matrix and intra-layer redundancies in LoRA through well-designed empirical observations.\n* *Simple yet generalizable design*: The unified $A$+dynamic $B$ mechanism is conceptually elegant and applicable to various architectures (LLMs, MLLMs, diffusion models) without invasive changes.\nStrong empirical validation – Results are consistent across diverse modalities with detailed ablations (e.g., drop-ratio, number of $B$ heads, importance-based vs. random pruning).\n* *Improved efficiency–performance trade-off*: Demonstrates that ReLoRA attains HydraLoRA-level or better performance with about half the tunable parameters, confirming its efficiency claim.\n* *Readable and well-structured*: The paper’s presentation is coherent; figures and tables are clear, and the motivation-to-method transition is smooth."}, "weaknesses": {"value": "* *Incomplete resource-efficiency evaluation*: The “resource-efficient” claim mainly relies on parameter count and training time; other dimensions such as GPU memory footprint, FLOPs under varying K, or energy consumption are not analyzed. What's more, only three model is adopted.\n* *Generalization explanation*: The paper attributes performance gains to shared-A capturing “global knowledge,” but lacks deeper analysis (e.g., probing studies or representation overlap) to validate this hypothesis."}, "questions": {"value": "* How sensitive is the performance to the choice of $K$ (the number of active layers) and the frequency of Reducer updates? Is there any adaptive rule to select $K$ automatically?\n* Have the authors measured runtime or memory savings during inference (not just training)?\n* Does sharing a global $A$ harm specialization in deeper layers for tasks requiring hierarchical representations (e.g., syntax vs. semantics)?\n* Could ReLoRA be combined with rank-adaptive or quantized LoRA variants?\n* For multimodal tuning, does the same shared $A$ span text and visual projections, or are separate $A$ matrices used per modality?\n* How will the performance be on other models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N6sXbLx04W", "forum": "ij8xU2yCmX", "replyto": "ij8xU2yCmX", "signatures": ["ICLR.cc/2026/Conference/Submission23916/Reviewer_R4AV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23916/Reviewer_R4AV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848239052, "cdate": 1761848239052, "tmdate": 1762942853458, "mdate": 1762942853458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a parameter-efficient finetuning method for transformers. It uses a single downprojection matrix A across layers and then prunes B matrices for use in different layers to balance expressivity and parameter count. Experiments are thorough and results are strong with some details missing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* good comparison against existing works\n* compared using current architectures (like llama-3)\n* proposed method is very parameter-efficient.\n* experiments cover several domains from LLMs to VLMs to diffusion models\n* includes training complexity/cost analysis which is favorable"}, "weaknesses": {"value": "* the methodname \"ReLora\" is already taken from a fairly well-cited work of 2023. published at iclr\n* Limited novelty. sharing weights across layers is not novel (e.g. HydraLoRA), using a MoE like router has also been used in LoRAMoE and other works. What remains as novelty seems to be the \"reducer\" part of the method which only aids training and does not have any effect on downstream use. \n* unclear hyperparameter selection. LoRA and all the PEFT methods are extremely dependend on correct and fair hyperparameter choice. The paper does not mention what hparam grid was used across the methods and how the final selection took place (some methods benefit from higher learning rate etc).\n* Family of LoRA methods missing: the works that use random matrices are extremely parameter-efficient and not compared against nor mentioned in related works. these include BOFT/NOLA, VeRA and LoReTTA, for example"}, "questions": {"value": "* hyperaparameter-grid selection and details on final model reporting (see above)\n* performance against random-matrix based approaches\n* details of difference compared to existing works such as MoELoRA should go into related works. \n* For the VLM experiment: a full-finetune baseline is missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ROQM4X8k1u", "forum": "ij8xU2yCmX", "replyto": "ij8xU2yCmX", "signatures": ["ICLR.cc/2026/Conference/Submission23916/Reviewer_N1Ed"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23916/Reviewer_N1Ed"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762246427404, "cdate": 1762246427404, "tmdate": 1762942852748, "mdate": 1762942852748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}