{"id": "RQm2KQTM5r", "number": 21539, "cdate": 1758318714644, "mdate": 1759896916904, "content": {"title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning", "abstract": "Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language often provides a much richer learning medium for LLMs, compared to policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error.\n    Given any AI system containing one or more LLM prompts, GEPA samples trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain.\n    Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% (e.g., +10% accuracy on AIME-2025).", "tldr": "GEPA uses natural language reflection to optimize prompts, outperforming GRPO and MIPROv2 while needing far fewer rollouts.", "keywords": ["prompt optimization", "natural language", "reflection", "large language models", "agent design", "agent discovery", "code optimization", "compound AI systems", "genetic", "language based learning", "evolutionary algorithms"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/15307fd8c26d6315f95fd8ec1252dfeebe7acd62.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes GEPA, a novel prompt optimization algorithm based on genetic algorithms. The key components of GEPA are (1) reflective prompt mutation based on execution traces generated by LLMs and evaluation traces from environments, and (2) pareto-based candidate selection that effectively balances exploration and exploitation. The paper shows that GEPA outperforms MIPROv2, a recent prompt optimization algorithm based on Bayesian optimization, and GRPO on various tasks encompassing mathematical reasoning, multi-hop reasoning, and instruction-following."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Overall, the paper is well-written and easy to understand. Although the paper is 94 pages long, including the appendix, I had no significant difficulty reading all of it.\n- The experiments are very well-designed. The benchmarks were appropriately selected across a diverse range of tasks, and the choice of models, one reasoning model (Qwen3) and one non-reasoning model (GPT-4.1), is also considered appropriate.\n- The inference-time search experiment in the appendix looks very promising.\n- The proposed system is production-grade, which makes it seem highly practical."}, "weaknesses": {"value": "- The paper's comparison is insufficient, benchmarking GEPA against only one baseline method, MIPROv2. The authors should expand the comparison to include other relevant prompt optimization techniques, such as ORPO, TextGrad, and Trace [1, 2, 3]\n- Reflective prompt optimization seems not novel, as it has been widely used in TextGrad and Trace.\n- Pareto-based candidate selection seems to play a crucial role in performance improvement, yet the analysis on this component appears insufficient. The paper only presents a simple comparison against greedy selection.\n\n[1] Yang et al., Large Language Models as Optimizers, ICLR 2024 \\\n[2] Yuksekgonul et al., TextGrad: Automatic \"Differentiation\" via Text, arXiv 2024 \\\n[3] Cheng et al., Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs, NeurIPS 2024"}, "questions": {"value": "I will raise my rating to 8 or 10 if my major questions are well addressed.\n\n- (Major) Could you explain the rationale for not comparing GEPA with TextGrad or Trace? If feasible, I would strongly encourage the authors to run these additional experiments.\n- (Major) Why are the math benchmark results for Qwen3 missing? Could you explain the reason for excluding these results? Is it because the performance improvement was not significant compared to MIPROv2? Or, were the math benchmarks added closed to the deadline, which only allowed time to conduct experiments for GPT-4.1?\n- (Major) Could you please compare GEPA with MIPROv2 without few-shot examples? If few-shot examples are not effective for Qwen3 or GPT-4.1, it seems better to remove them and allocate more budget to the response.\n- (Major) Could you please elaborate on lines 219-221? You mentioned that feedback can be module-specific and augmented with human-written text. Could you please explain this more concretely using HotpotQA as an example?\n- (Minor) What was the monetary cost of running the experiments on GPT-4.1? Also, what would be the estimated cost if you were to use GPT-5 instead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1exoJw5odB", "forum": "RQm2KQTM5r", "replyto": "RQm2KQTM5r", "signatures": ["ICLR.cc/2026/Conference/Submission21539/Reviewer_VhzU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21539/Reviewer_VhzU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760684408524, "cdate": 1760684408524, "tmdate": 1762941827048, "mdate": 1762941827048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that the interpretable nature of natural language can serve as a richer learning signal for large language models (LLMs) compared to conventional reinforcement learning (RL) fine-tuning. To this end, the authors propose GEPA (Genetic-Pareto), a natural-language-based prompt optimization framework that integrates reflective reasoning and evolutionary search. GEPA iteratively samples trajectories, reflects on them in natural language to diagnose errors, proposes and tests improved prompts, and aggregates complementary insights from the Pareto frontier of its own optimization history. This approach reportedly enables more efficient improvement of LLM performance using significantly fewer rollouts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe motivation to reduce rollout costs during fine-tuning while maintaining or improving performance is both practically valuable and timely, given the rising computational demands of LLM alignment and adaptation.\n2.\tThe proposed method is intuitively appealing and conceptually straightforward, making it easy to follow."}, "weaknesses": {"value": "1.\tAlthough Equation (2) defines two sets of parameters for potential optimization, the proposed framework appears to update only the prompt parameters, leaving the model weights fixed. In contrast, baseline methods such as GRPO involve updating the network parameters directly in their original paper. This discrepancy raises concerns about fairness in the comparison, as the two approaches optimize fundamentally different objectives and operate under different levels of model adaptability.\n2.\tWhile the problem is framed as an optimization task in Equation 2, GEPA does not employ gradient-based backpropagation ensuring convergence to an optimum. The paper should clarify how the method guarantees optimization stability or convergence, either through theoretical justification or empirical analysis. Without such assurance, it remains unclear whether GEPA reliably approaches a Pareto-optimal solution or simply performs heuristic search without convergence guarantees."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iFPrRKWHdx", "forum": "RQm2KQTM5r", "replyto": "RQm2KQTM5r", "signatures": ["ICLR.cc/2026/Conference/Submission21539/Reviewer_J1yS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21539/Reviewer_J1yS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761285415427, "cdate": 1761285415427, "tmdate": 1762941826808, "mdate": 1762941826808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GEPA, a reflective prompt optimizer for compound AI systems that combines textual reflection with multi-objective evolutionary search. The method iteratively mutate prompts by leveraging natural language feedback from rollouts. Evaluations across six benchmarks demonstrate its effectiveness and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method leverages explicit reflection and Pareto-based selection for prompt optimization, and shows its superior sample efficiency and performance against baselines."}, "weaknesses": {"value": "The novelty of the paper is limited. Reflection has been used in prior works (e.g. Reflexion and Self-Refine) for iterative improvement, and genetic algorithms have also already been applied to prompt optimization (e.g., EvoPrompt, Promptbreeder, and EvoAgent).\n\nThe paper is hard to follow, and needs more clarification. For example, what is the detail of the Reflective Prompt Mutation, especially for the feedback. In addition, System aware merge should be described clearly in the paper. How to select these two strategies to create new candidates in each iteration, and how many prompts are selected to mutate? According to the problem statement, the system comprises multiple modules, yet the paper does not elaborate on these modules in sufficient detail.\nMoreover, while GEPA is presented as optimizing only the prompts (i.e., only optimizing $\\Pi$), the optimization problem defined in Equations (2) and (3) also includes the module weights $\\Theta$. This apparent inconsistency should be addressed and clarified. \n\nIn GRPO, it is common practice to generate 16 rollouts per query. However, the paper mentions 24,000 rollouts in GRPO, which requires clarification. Additionally, the computational overhead of GEPA itself (e.g., the number of LLM calls required for reflection) is not discussed.\n\nThe experiments are insufficient in several aspects. First, the paper should compare with other prompt optimization methods such as EvoPrompt, PromptBreeder, and EvoAgent. Second, how do the performance of these methods vary under different computational budgets? Third, why are not all benchmarks included in Table 1. Specifically, why are the results for AIME-2025 and LiveBench-Math missing in Table 1?"}, "questions": {"value": "See the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lxUfvtHMEs", "forum": "RQm2KQTM5r", "replyto": "RQm2KQTM5r", "signatures": ["ICLR.cc/2026/Conference/Submission21539/Reviewer_QSDo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21539/Reviewer_QSDo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814939078, "cdate": 1761814939078, "tmdate": 1762941826594, "mdate": 1762941826594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GEPA (Genetic-Pareto), a novel and highly sample-efficient prompt optimizer for LLMs. The authors argue that conventional adaptation methods, particularly RL techniques like GRPO, are inefficient as they require tens of thousands of trials and rely on sparse scalar rewards, failing to utilize the rich information in an LLM's own operational traces. In contrast, GEPA leverages the LLM's inherent linguistic capabilities through a process of \"reflective evolution.\" It analyzes the detailed, natural language traces of its own attempts—including reasoning steps and tool usage—to diagnose failures, propose targeted improvements to its prompts, and learn high-level strategies. This reflective process is combined with a Pareto-based evolutionary algorithm that maintains a diverse set of high-performing prompt candidates to avoid local optima. Across multiple benchmarks, GEPA is shown to significantly outperform GRPO, achieving superior results with up to 35 times fewer rollouts, and also surpasses the state-of-the-art prompt optimizer MIPROv2. The work demonstrates that leveraging language-based reflection is a more powerful and efficient method for optimizing complex, real-world AI systems compared to traditional RL approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1、It proposes a prompt optimization algorithm based on the Pareto frontier, which avoids the problem of local optima found in previous prompt optimization methods.\n\n2、Combining prompt optimization method with the multiple-rollout approach of GRPO, it demonstrates a significant improvement in the effectiveness and sample efficiency of prompt optimization. I think the main improvement comes from contrasting multiple rollouts and produce a better prompt.\n\n3、The performance of the proposed framework is demonstrated across multiple tasks."}, "weaknesses": {"value": "1、Like many prompt optimization methods, this approach is highly dependent on the model's own reasoning and summarization capabilities. It requires the model to analyze successful and failed rollouts and distill effective textual experience into the prompt. Consequently, this method may not be suitable for less capable LLMs, an aspect the paper fails to analyze.\n\n2、The paper does not analyze the types of tasks for which this prompt optimization method is effective and those where it might not be. For instance, on tasks like mathematical reasoning, the effectiveness of this approach is not guaranteed.\n\n3、Does the prompt exclusively contain summarized experiences, or does it also incorporate the trajectories of failed rollouts?\n\n4、I am concerned that the context within the prompt could become increasingly long. Theoretically, as the number of learning samples grows, the volume of summarized experience in the prompt should also increase.\n\n5、The paper lacks some necessary citations. [1] is very similar to the process in this paper; both automatically optimize prompts through LLM. \n\nMissing citations：\n\n[1] Zhang, Wenqi, et al. \"Agent-pro: Learning to evolve via policy-level reflection and optimization.\" ACL 2024."}, "questions": {"value": "Stated in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BV14X4aNQE", "forum": "RQm2KQTM5r", "replyto": "RQm2KQTM5r", "signatures": ["ICLR.cc/2026/Conference/Submission21539/Reviewer_pax9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21539/Reviewer_pax9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927119347, "cdate": 1761927119347, "tmdate": 1762941826365, "mdate": 1762941826365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of post-training large language models (LLMs). Existing RL–based methods, such as GRPO, demand thousands of rollouts, leading to substantial computational costs. To overcome this, the authors introduce GEPA, an evolutionary, LLM-driven approach to prompt optimization. Unlike RL, which relies solely on scalar rewards, GEPA leverages execution and evaluation traces to provide richer, denser learning signals. It also maintains a Pareto set during optimization, promoting solution diversity. Experiments across multiple benchmarks demonstrate that GEPA is up to 35× more sample-efficient than GRPO and achieves superior performance compared to both GRPO and other prompt optimization frameworks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper addresses an important and timely challenge—improving the sample efficiency of LLM post-training—and presents a well-motivated approach to tackle it.\n\n2. The proposed method overcomes key challenges of existing RL-based approaches, such as credit assignment from single scalar rewards and low sample efficiency.\n\n3. The results are compelling and challenge the existing paradigm of RL-based post-training in the field of LLM post-training."}, "weaknesses": {"value": "1. (Line 220) “Human-written explanations” – does the human have to provide textual feedback based on the scalar reward?\n\n2. What is r in Algorithm 4?\n\n3. Other LLM-based evolutionary methods have been explored in the context of adversarial prompt generation and red-teaming, and these works should be appropriately cited, for example [1].\n\n[1] Samvelyan, M. et al. (2024). Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts. In NeurIPS 2024."}, "questions": {"value": "1. In Observation 4, could the weaker performance from adding few-shot demonstrations be attributed to the evolutionary framework of MIPROv2? In other words, how might GEPA perform if it were similarly constrained to include few-shot demonstrations alongside the instructions?\n\n2. In the experiments without a validation set (Observation 1), does this hurt the generalization performance?\n\n3. An additional advantage of GEPA is that the optimized prompts can potentially generalize across models. Have the authors tried this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QjgOcoMHan", "forum": "RQm2KQTM5r", "replyto": "RQm2KQTM5r", "signatures": ["ICLR.cc/2026/Conference/Submission21539/Reviewer_dW6h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21539/Reviewer_dW6h"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993027254, "cdate": 1761993027254, "tmdate": 1762941826115, "mdate": 1762941826115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}