{"id": "8yWECy22Zi", "number": 16308, "cdate": 1758262995797, "mdate": 1759897248541, "content": {"title": "MFCL: A Multi-modal Function Calling Evaluation for Large Language Models", "abstract": "Large language models are evolving into multi-modal agents that call tools directly from raw speech or images. Yet we still lack a principled metric for how well they convert perception into accurate function calls. We introduce \\textbf{MFCL}, the first large-scale benchmark for \\emph{Multi-modal Function Calling}, comprising \\textbf{8.2K} expert-verified tasks across three suites—\\textbf{True Audio}, \\textbf{Text Audio}, and \\textbf{Vision}. Each example pairs a multi-modal user query with a ground-truth tool-call trace. To examine different capabilities of the LLM's perception-to-action pipeline, we introduce controlled perturbations: for audio, accents, contractions, simplified forms, casual pronouns, slang, disfluencies (fillers, hesitations, repetitions), and background noise; for images, crops and resizes, occlusions, grayscale and other color shifts, and related transformations. Image crops and resizes, occlusions, black-and-white and other color filters, etc for images. Our automatic grader computes exact-match scores for both function names and their arguments, removing dependence on brittle LLM judges and isolating errors in perception, reasoning, and formatting. We evaluate  leading models and present a taxonomy of failure models: named-entity ASR errors, conversational drift, and tool avoidance. By releasing MFCL's dataset, taxonomy, and diagnostics, we hope to accelerate research on multi-modal agents that can effectively invoke tools.", "tldr": "", "keywords": ["Function Calling Evaluation", "Tool use", "Large Language Models", "Multimodal", "Audio", "Vision"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/364e66e0d3b04e6b1b5aebe2c1f45f933697e221.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "1. This paper introduces MFCL, the first unified benchmark to evaluate structured function calling from multi-modal inputs (speech and vision).\n\n2. It systematically injects realistic perception perturbations and uses exact-match automated scoring to diagnose failures\n\n3. Results reveal significant degradation under noise and visual distortions, exposing critical weaknesses such as tool avoidance, keyword selection errors, and conversational drift in modern models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Realistic perturbation design that simulates real-world failure conditions in multimodal function calling and thoroughly analyzes their impact.\n\n2. Extensive evaluation across multiple leading commercial models, demonstrating substantial experimental effort and providing meaningful comparative evidence for the community."}, "weaknesses": {"value": "1. Despite arguing for “real-world audio robustness,” the dataset relies on synthetic TTS rather than human-recorded speech. \n2. The benchmark’s core metric (exact-match JSON output) misaligns with real agent objectives, overlooking task-level success, semantic equivalence, and cost-aware behaviors.\n3. The turn and clarification rules constrain reasonable uncertainty-handling strategies, potentially biasing models toward brittle “just emit JSON” behaviors instead of safe, real-world interaction patterns."}, "questions": {"value": "While the benchmark is valuable, I feel there are several questions:\n1. How does strict exact-match scoring avoid misaligning the benchmark with real-world multi-turn agent behavior (uncertainty handling, clarification, self-correction)?\n2. The turn semantics and clarification rules only allow spelling/value confirmations, while ignoring broader ambiguity resolution. How do these constraints avoid discouraging realistic uncertainty-handling strategies that agents must perform in practical deployments?\n2. Given the recent emergence of audio-based function-calling benchmarks, is it appropriate for MFCL to claim to be the “first” in this space, and could the authors clarify the concrete differences that distinguish MFCL from prior speech-focused evaluations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q5kQ4WJ0Fc", "forum": "8yWECy22Zi", "replyto": "8yWECy22Zi", "signatures": ["ICLR.cc/2026/Conference/Submission16308/Reviewer_wS9C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16308/Reviewer_wS9C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761527969243, "cdate": 1761527969243, "tmdate": 1762926448987, "mdate": 1762926448987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses two limitations of current multimodal benchmarks: 1) reliance solely on text-based tools, and 2) lack of feedback across different modalities and fine-grained details. To tackle these issues, the authors propose MFCL (Multi-modal Function Calling Evaluation), which consists of three components: True Audio, Text Audio, and Vision. The authors establish specific guidelines for generating each type of data. They evaluate several mainstream models on the proposed benchmark and analyze the results from both audio and visual perspectives. From the audio perspective, they find that current multimodal large models are highly sensitive to speech noise and often fail to confirm critical entities, leading to task failure. From the visual perspective, they observe that these models still lack sufficient attention to details, along with limited tool-calling capabilities and self-correction abilities."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "There is currently a scarcity of in-depth evaluation benchmarks for multimodal large models, and this paper contributes meaningfully to this field."}, "weaknesses": {"value": "1. The introduction is somewhat disorganized. The authors mention two research gaps, but starting from the third paragraph, they delve into the construction of the benchmark without directly linking it to how these gaps are addressed. I suspect the authors intended to highlight the lack of a benchmark combining API and multimodal evaluation, but the current version is hard to follow, making the motivation unclear. Additionally, Figure 1 is not referenced in the text.\n\n2. There is a lack of data validation, particularly human evaluation, making it difficult to assess the benchmark’s quality and potential biases. Furthermore, certain details remain unclear. For instance, the authors state that vision data requires \"one clear visual clue,\" but there is no in-depth analysis of how \"clear visual clue\" is defined or identified.\n\n3. The benchmark does not effectively integrate multiple modalities. Although the authors claim that the three components are mutually supportive, the paper does not demonstrate how these components interact.\n\n4. Experimental settings and evaluation metrics are crucial, yet placing them entirely in the appendix makes the paper hard to follow."}, "questions": {"value": "The analysis section summarizes numerous issues. Among these, which problem is the most critical and has the greatest impact on the performance of current multimodal LLMs? Could resolving this issue potentially lead to the resolution of other problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rBPjTBN7hl", "forum": "8yWECy22Zi", "replyto": "8yWECy22Zi", "signatures": ["ICLR.cc/2026/Conference/Submission16308/Reviewer_ReJp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16308/Reviewer_ReJp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898371883, "cdate": 1761898371883, "tmdate": 1762926448654, "mdate": 1762926448654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MFCL, a new benchmark for function calling in multimodal scenarios. The paper examines several cutting-edged models and reveals common failure patterns of these models, providing insights into developing multimodal agents."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The topic (function-calling and multimodality) is timely.\n- Comprehensive dataset design\n- Clear description for the dataset construction"}, "weaknesses": {"value": "- Limited insight beyond enumeration of “failure modes.” Most FM categories merely restate known LLM limitations (ASR errors, over-reliance on text, conversational drift).\n- Missing implementation details, such as the decoding hyperparameters. This may reduce the reproducibility of the paper.\n- Statistical shallowness. Reported numbers are raw accuracies with no confidence intervals or significance testing."}, "questions": {"value": "- How did you verify that the TTS-generated and noise-augmented audio realistically represents spontaneous human speech or real-world acoustic conditions? Was any human evaluation conducted to confirm perceptual naturalness?\n- What procedures ensured the correctness and consistency of the expert-verified tasks? How many annotators were involved? What inter-annotator agreement was achieved?\n- Given that the Vision set contains only 250 examples, why do you consider its coverage sufficient for robust evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AtUC4Z44Ww", "forum": "8yWECy22Zi", "replyto": "8yWECy22Zi", "signatures": ["ICLR.cc/2026/Conference/Submission16308/Reviewer_ESaR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16308/Reviewer_ESaR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918731904, "cdate": 1761918731904, "tmdate": 1762926448126, "mdate": 1762926448126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MFCL, the first large-scale benchmark for evaluating multi-modal function calling (tool use) in large language models (LLMs). MFCL comprises 8.2K expert-verified tasks across three suites: True Audio, Text Audio, and Vision. Each example pairs a multi-modal user query (text, speech, or image) with a ground-truth toolcall trace, and includes controlled perturbations (accents, noise, occlusions, etc.) to stress the perception-to-action pipeline.\n\nMFCL provides an automatic grader for exact-match scores on function names and arguments, enabling robust, reproducible evaluation without reliance on LLM judges. The authors benchmark leading models (e.g., GPT-4o, Gemini, Claude, GLM, xLAM, etc.), analyze failure modes (named-entity ASR errors, conversational drift, tool avoidance), and present a taxonomy to guide future research. The dataset, taxonomy, and diagnostics are released to accelerate progress on reliable multi-modal agents."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of the paper are:\n\n1. Originality: \n    - First benchmark to systematically evaluate multi-modal function calling under real-world perturbations.\n    - Introduces controlled perturbations and a taxonomy of failure modes.\n    - Unifies text, audio, and vision evaluation in a single framework.\n\n2. Quality:\n    - Expert-verified tasks, realistic data augmentation, and comprehensive error analysis.\n    - Automatic grading at function and argument level, enabling reproducible and robust evaluation.\n    - Strong experimental design, with ablations and comparisons across many models.\n\n3. Clarity:\n    - Clear motivation, methodology, and results presentation.\n    - Figures and tables directly support claims; taxonomy is actionable.\n\n4. Significance:\n    - MFCL will become a standard for evaluating multi-modal tool-augmented agents.\n    - The insights into failure modes and robustness are valuable for both research and deployment."}, "weaknesses": {"value": "No major weaknesses. The study of a multi-modal functional calling benchmark is very useful for developing agentic LLM in real-world scenarios."}, "questions": {"value": "Minor questions:\n\n1. The failure mode analysis is very interesting. Did authors have quantitative results in addition to the qualitative examples?\n2. Any plan for the release of the benchmark?\n3. Have you/ do you have plans to evaluate smaller models on the benchmark? like Qwen-omni, and other multi-modal LLMs with similar size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6UAY2UvupT", "forum": "8yWECy22Zi", "replyto": "8yWECy22Zi", "signatures": ["ICLR.cc/2026/Conference/Submission16308/Reviewer_FoDx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16308/Reviewer_FoDx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762029041218, "cdate": 1762029041218, "tmdate": 1762926447736, "mdate": 1762926447736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}