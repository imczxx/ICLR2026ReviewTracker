{"id": "sh1hWO9RHo", "number": 21307, "cdate": 1758316113644, "mdate": 1759896929604, "content": {"title": "WHAT IS YOUR AGENT’S GPA? A FRAMEWORK FOR EVALUATING AGENT GOAL-PLAN-ACTION ALIGNMENT", "abstract": "We introduce the Agent GPA (Goal-Plan-Action) framework: an evaluation\nparadigm based on an agent’s operational loop of setting goals, devising plans,\nand executing actions. The framework includes five evaluation metrics: Goal Fulfillment, Logical Consistency, Execution Efficiency, Plan Quality, and Plan Adherence. Logical Consistency checks that an agent’s actions are consistent with\nits prior actions. Execution Efficiency checks whether the agent executes in the\nmost efficient way to achieve its goal. Plan Quality checks whether an agent’s\nplans are aligned with its goals; Plan Adherence checks if an agent’s actions are\naligned with its plan; and Goal Fulfillment checks that agent’s final outcomes\nmatch the stated goals. Our experimental results on two benchmark datasets – the\npublic GAIA dataset and an internal dataset for a production-grade data agent –\nshow that this framework (a) provides a systematic way to cover a broad range\nof agent failures, including all agent errors on the GAIA benchmark dataset; (b)\nexhibits strong agreement between human and LLM judges, ranging from 80% to\nover 95%; and (c) localizes errors with 86% agreement with human annotations\nto enable targeted improvement of agent performance.", "tldr": "The Agent GPA evaluation framework mirrors an agent's operational loop with goals, plans, and actions, and captures a broad range of agent failures, with LLM judges agreeing with human on error detection and localization from 80% to 95% of the time.", "keywords": ["LLM Agent", "LLM Judge", "Evaluation", "Eval"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e9219035c07495825fb595d50668f3d9d5141fc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents the Goal-Plan-Action (GPA) framework for evaluating agents. It aims to evaluate agents along five dimensions, namely, Goal Fulfillment, Logical Consistency, Execution Efficiency, Plan Quality, and Plan Adherence. They also design corresponding LLM judges  to evaluate agents along those criteria. Based on experiments on the TRAIL dataset and a small internal dataset, the judges are able to detect and localize a broad range of agent failures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Intuitive and general GPA framework that aligns well with the lifecycle of agents\n- Experimental results clearly show the strong performance of the judges compared to TRAIL baseline judge (95% vs. 54% errors detected)\n- Experimental results show high consistency among multiple runs of the LLM judges, demonstrating their reliability"}, "weaknesses": {"value": "- Only one external dataset is used and internal data is quite small. It would be good to evaluate GPA on other datasets and compare performance against other baselines [1, 2, 3]\n- Although at a high level GPA is intuitive as it aligns with agent lifecycle, it is unclear how the specific criteria were selected. For example, why is only \"Tool Calling\" under Plan adherence? The taxonomy/framework lacks theoretical rigor. \n- Based on Table 17, only Claude-4-Sonnet achieves good performance on the internal dataset. Other models are not evaluated on the external dataset. Thus, it is unclear if the GPA framework generalizes to other models. \n\n[1] Zhang, Shaokun, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li et al. \"Which agent causes task failures and when? on automated failure attribution of llm multi-agent systems.\" arXiv preprint arXiv:2505.00212 (2025).\n\n[2] Cemri, Mert, Melissa Z. Pan, Shuyi Yang, Lakshya A. Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer et al. \"Why do multi-agent llm systems fail?.\" arXiv preprint arXiv:2503.13657 (2025). \n\n[3] Zhang, Guibin, Junhao Wang, Junjie Chen, Wangchunshu Zhou, Kun Wang, and Shuicheng Yan. \"AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?.\" arXiv preprint arXiv:2509.03312 (2025)."}, "questions": {"value": "- Line 452 mentions \"custom instructions\" were required to adapt the judges to the internal dataset? How much customization is required for a new domain? This would impact the generalizability of the judges.\n- Why were only two judges used for the internal dataset?\n- Are seven judges really necessary? There seems to be some overlap among the evaluation criteria. So, why not just use a single judge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rjyGuEqqV7", "forum": "sh1hWO9RHo", "replyto": "sh1hWO9RHo", "signatures": ["ICLR.cc/2026/Conference/Submission21307/Reviewer_Vw1K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21307/Reviewer_Vw1K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761110850931, "cdate": 1761110850931, "tmdate": 1762941684381, "mdate": 1762941684381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Agent GPA (Goal-Plan-Action) framework, a new paradigm for evaluating LLM-based agents. The core idea is to decompose the evaluation process to mirror an agent's operational loop: setting goals, devising plans, and executing actions. The authors propose five primary metrics—Goal Fulfillment, Plan Quality, Plan Adherence, Logical Consistency, and Execution Efficiency, with auxiliary tool-related metrics. To automate this evaluation, the authors employ a suite of specialized \"LLM-as-a-Judge\" evaluators, with each judge tailored to a specific metric. The framework is empirically validated on the TRAIL/GAIA benchmark and a smaller, internal dataset. The results suggest that this decomposed approach achieves high agreement with human evaluators in both error detection (95% coverage on the GAIA test set ) and localization (86% coverage ), outperforming baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a critical and timely challenge in the field of AI agents. As the authors correctly point out, many existing evaluation methods focus heavily on final outcomes, providing little actionable insight for debugging and iterative improvement.\\\\\n\n2. The proposed GPA framework is logically sound and easy to understand. Decomposing the complex behavior of an agent into the core components of Goal, Plan, and Action provides a systematic way to categorize and analyze potential failure modes. This structured approach is a clear strength over monolithic, black-box evaluation methods.\\\\\n\n3. I like the thoughtful design choice of using a suite of specialized LLM judges instead of a single, all-purpose evaluator. As the paper suggests, asking a single LLM to simultaneously identify, localize, and classify errors in long traces is fragile. By breaking the job down into distinct roles (like having one judge just for Plan Quality and another for Tool Calling), the framework can get much more reliable and focused feedback on each part of the agent's behavior."}, "weaknesses": {"value": "1. Critical Reliability Issues in Core Components: The entire framework's validity rests on the reliability of its individual judges, and several key components appear to be unreliable. The Plan Quality (PQ) and Plan Adherence (PA) judges, which are central to the \"Plan\" aspect of the GPA framework, exhibit very poor performance.\n\n2. The prompts for each judge were \"iteratively refined\" and include 1-2 few-shot examples drawn directly from the development dataset. This creates a high risk of the judges overfitting to the specific error patterns and linguistic styles present in the GAIA traces.\n\n3. The scope of the experiments is too narrow to fully support the broad claims made by the authors. The study exclusively uses the 117 traces from the TRAIL/GAIA subset, while explicitly excluding the SWE-bench dataset. The justification is to focus on \"internal agent errors,\" but this decision conveniently sidesteps more complex agent-environment interactions found in software engineering tasks, thus limiting the demonstrated applicability of the framework. The validation on the internal \"ANON-Data-Agent\" is based on only 17 traces, which is insufficient to make a strong case for the framework's effectiveness in a production environment.\n\n4. While the packaging of the \"GPA\" framework is neat, the underlying concepts of evaluating goals, plans, and actions are not entirely new in AI research. The main novelty appears to be the application of a suite of LLM judges to these facets. Given the aforementioned reliability and generalization issues with this novel component, the overall technical contribution feels incremental at this stage."}, "questions": {"value": "1. The Plan Quality (PQ) and Plan Adherence (PA) judges are presented as core components of the GPA framework, yet they demonstrate very low precision and F1-scores on the test set (Table 3), indicating a high rate of false positives. How do you envision developers practically using these specific judges in a debugging workflow when their feedback is so unreliable? Does this not suggest a fundamental limitation of current LLMs in evaluating more abstract concepts like plan quality, thereby undermining a key pillar of the proposed framework?\n\n2. The LLM judges were developed using few-shot examples drawn from the TRAIL/GAIA development set, which features a specific manager-search agent architecture. How confident are you that the high performance reported is not overfitted to this particular agent's structure and output style? Have you performed any validation to assess the framework's \"out-of-the-box\" performance on traces generated by agents with fundamentally different reasoning patterns, such as a pure ReAct-style agent?\n\n3. The study exclusively uses the GAIA subset of the TRAIL benchmark, while omitting the SWE-bench subset because it involves factors outside the agent's direct control. However, real-world agentic systems must constantly interact with and handle failures from external tools and environments. Doesn't this exclusion significantly limit the demonstrated applicability of the GPA framework to the very complex, real-world scenarios it claims to be designed for? How would your framework distinguish between an agent's failure to handle an API error versus the API error itself?\n\n4. Your paper argues that the judges are consistent (most dimensions show Krippendorff’s α > 0.7; Table 7). However, the Plan Quality (PQ) judge records α = 0.628 and the largest run-to-run variance, whereas EE and TS reach about 0.934 and 0.907, respectively. Given that planning quality is central to agent behavior, how do you justify the framework’s reliability when the component assessing plan quality is the least consistent? What concrete ablations or rubric/prompt interventions can raise PQ’s α into the >0.7 range without sacrificing precision (e.g., more granular rubrics, calibration, disagreement-aware aggregation, or judge ensembles)?\n\n5. On Generalization to Different Agent Architectures: The GPA framework was developed and tuned using traces from the specific manager-search agent architecture in TRAIL/GAIA. To truly validate the framework's claimed utility and generalization, its performance on fundamentally different agent architectures must be assessed. Could the authors apply the unmodified GPA framework to evaluate traces from a benchmark featuring a different agent architecture, such as a ReAct-style agent from AgentBench  or a web navigation agent from WebArena? This experiment is crucial to demonstrate that the specialized judges are not overfitted to one particular interaction pattern."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VJqVS2khLI", "forum": "sh1hWO9RHo", "replyto": "sh1hWO9RHo", "signatures": ["ICLR.cc/2026/Conference/Submission21307/Reviewer_vK34"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21307/Reviewer_vK34"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761266995021, "cdate": 1761266995021, "tmdate": 1762941683777, "mdate": 1762941683777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Agent GPA - a framework for evaluating AI agents across different metrics such as Plan Quality, Plan Adherence, Goal Fulfillment, Logical Consistency, Execution Efficiency etc.  The authors validate their approach on two datasets (TRAIL/GAIA and an internal production agent dataset), showing better error localization and higher alignment with human annotators compared to the baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The author tackle a very important and relevant problem in today's world of AI agents -  automatic evaluation of agentic trajectories, which is a crucial component in developing and debugging agentic systems.\n2. Each of the metrics is well-defined and clearly motivated.\n3. The authors show that their framework is capable of more than error detection, being able to also localize errors with decent accuracy which can help in better agent debugging.\n4. The authors measure the robustness of the LLM judges by measuring consistency across multiple runs."}, "weaknesses": {"value": "1. Limited Generalizability - The authors evaluate their approach on 2 datasets one of which is internal and only contains 17 traces. For the other one - TRAIL, which consists of traces from 2 benchmarks namely GAIA and SWE-Bench, they only evaluate on GAIA which is a general AI assistant benchmark and skip SWE-Bench raising question about the applicability of the framework for agentic systems in different domains such as Software Engineering, IT Automation etc. The authors also mention small sample size for PA and PQ errors in the dataset which raises questions of statistical significance of the results.\n2. The authors compare their approach against TRAIL's LLM as a judge method. However TRAIL's main objective is fault categorisation and localization which is just one aspect of agentic trajectory evaluation. Hence it's a weak baseline to compare with.\n3. The authors mention that '$\\textit{Overall, our judges exhibit strong agreement with human annotators across the board.}$'. However this claim is not validated by the results present in table 4 which shows low human agreement for LC, PQ and TC. There is also no error analysis of why the agreement is low. Moreover, even though the authors acknowledge the low precision of metrics like LC and PA, there is no explanation or analysis provided on why is it so."}, "questions": {"value": "1. Since each metric captures a distinct aspect of agentic trajectory evaluation, how can the metric scores be aggregated - beyond a simple weighted scheme -  to provide a single aggregate correctness score for the trajectory which can be used for downstream tasks (such as reward for RL training)\n2. How much do the different metrics agree / differ in their evaluation? A detailed analysis of cross metric agreement / disagreement would provide better insights and utility of each metric in the grand scheme of evaluation.\n3. Plan Quality and Plan Adherence assume explicit planning / reasoning in agentic trajectories. However there could be agents whose trajectories only contain action-output pairs without an explicit plan? How can these metrics be computed for such agentic traces."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GVl76u38FV", "forum": "sh1hWO9RHo", "replyto": "sh1hWO9RHo", "signatures": ["ICLR.cc/2026/Conference/Submission21307/Reviewer_x8YC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21307/Reviewer_x8YC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937917592, "cdate": 1761937917592, "tmdate": 1762941683550, "mdate": 1762941683550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides an automated evaluation framework for LLM-based agents, centered on an \"LLM-as-judge\" methodology. The paper proposes a taxonomy of 5 key evaluation metrics: Goal Fulfillment, Logical Consistency, Execution Efficiency, Plan Quality (PQ), and Plan Adherence (PA).\n\nThe framework presents specialized LLM-judges for each of these criteria. With these, the paper reports achieving 95% agreement with human annotations, as compared to 55% achieved by the LLM-judge presented in TRAIL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a taxonomy of agent evaluation among 5 dimensions.\n- The presented LLM-as-judge achieves high alignment on datasets like TRAIL."}, "weaknesses": {"value": "- While the interannotator agreement is reported between stochastic annotations created by repeated calls to LLM-judge, the paper does not report human-LLM annotation agreement metrics.\n- The paper presents an agent evaluation rubric, along with LLM-judges that evaluate an agentic trace on these rubric dimensions. However, this idea has been explored in prior work on agentic and multi-agent systems, for example, MAST, which provided a failure taxonomy along with LLM-as-judge to apply the taxonomy.\n- The paper does not highlight how the evaluation rubrics or LLM-judges can be used to improve the reliability of agentic systems. The paper would benefit by including a study on applying the proposed evaluation framework, to improving agentic systems (either via offline or online intervention). The usefulness of an agent evaluation framework would come from its ability to augment agentic systems."}, "questions": {"value": "- Regarding the Plan Adherence metric: Is an agent penalized for deviating from a low-quality plan? How does the framework differentiate between a \"good\" deviation (i.e., adapting to a bad plan) and a \"bad\" deviation (i.e., failing to follow a good plan)?\n- Why do the authors' not evaluate against the TRAIL/GAIA traces, with its native annotations, instead of using human annotators to map the native error categorisation to GPA framework?\n- Can the authors discuss the choice and use of manual iterative prompt refinement over automated prompt optimization techniques like PromptWizard, MIPRO, GEPA, TextGrad, etc. which could avoid the need for human involvement and potentially improve the performance?\n- Beyond evaluation, did the authors explore the applicability of these judges for improving agentic systems, for example, by providing real-time feedback during inference or by being used as a reward signal for fine-tuning?\n- The inter-annotator agreements are reported between different LLM-as-judge calls. However, the inter-annotator agreement metrics should address the question of applicability of the taxonomy to the domain. Can the authors' provide the inter-annotator agreement metrics for Human-Human annotations, and Human-LLM annotations?\n- Could the authors' discuss how the provided evaluation framework differ from prior work like MAST, which proposed an annotation driven error taxonomy, along with LLM-as-judge based evaluations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sIO4Og5eWh", "forum": "sh1hWO9RHo", "replyto": "sh1hWO9RHo", "signatures": ["ICLR.cc/2026/Conference/Submission21307/Reviewer_qsHQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21307/Reviewer_qsHQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136423118, "cdate": 1762136423118, "tmdate": 1762941683267, "mdate": 1762941683267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}