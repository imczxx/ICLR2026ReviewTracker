{"id": "i7QNKZioN6", "number": 19977, "cdate": 1758301174998, "mdate": 1759897008864, "content": {"title": "Measuring LLM Novelty As The Frontier Of Original And High-Quality Output", "abstract": "As large language models (LLMs) are increasingly used for ideation and scientific discovery, it is important to evaluate their ability to generate novel output. Prior work evaluates novelty as originality with respect to model training data, but original outputs can be of low quality. In contrast, non-expert judges more reliably score quality but may favor memorized outputs, limiting the reliability of human preference as a metric. We introduce a new novelty metric for LLM generations that balances originality and quality---the harmonic mean of the fraction of \\ngrams unseen during training and a task-specific quality score. Using this framework, we identify trends that affect the novelty of generations from three families of open-data models (OLMo, OLMo-2, and Pythia) on three creative tasks---story completion, poetry writing, and creative tool use. We find that model-generated text from some base LLMs is less novel than human-written text from the internet. However, increasing model scale (OLMo 1B to 7B to 32B) and post-training reliably improves novelty due to improvements in output quality. We also find that improving the base model at the same scale (\\eg OLMo 7B to OLMo-2 7B) leads to higher novelty due to higher originality. Finally, we observe that inference-time methods, such as prompting and providing novel in-context examples, have a much smaller effect on novelty, often increasing originality at the expense of quality. This highlights the need for further research into more effective elicitation strategies as we use models for creative applications.", "tldr": "We quantify and evaluate the novelty in LLM generations as the harmonic mean of the n-gram originality and output quality, and analyse how model scale, training methods and inference methods impact it.", "keywords": ["generation", "evaluation", "memorization", "novelty", "benchmark", "creativity"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c417c2784c3f622e2b297b5e1f21abbe9db9f85b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new metric to measure the novelty and originality of content (across different tasks) generated by LLMs.\nExisting n-gram-based methods for originality may assign high scores to low-quality text, as they only consider whether the generated text appears in the training data or not.\nTo address this limitation, the paper introduces a harmonic mean between the n-gram-based originality metric and an LLM-based quality metric.\nThe authors evaluate their proposed metric on various models and show that increasing model size leads to improved performance in novelty."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is easy to read.\nThe problem being addressed is interesting, as it is necessary to measure the quality of the content generated by the models"}, "weaknesses": {"value": "I did not find much clarity and usefulness in the proposed metric, and it is also unclear why the authors chose to use the harmonic mean.\n\nMoreover, using an n-gram model to capture originality may not be an appropriate choice, since a model could generate a sentence that conveys the same meaning or context as one found in the training data, but with a different ordering of words."}, "questions": {"value": "The authors use an LLM to quantify the quality of the generated text. However, is it possible for an LLM to assess both novelty and quality simultaneously?\n\nI believe that an n-gram model may not effectively capture the context of a statement. Since a coherent and meaningful sentence can be expressed in many different ways, an n-gram–based approach might classify a statement as novel even when its underlying context is similar to one already present in the training data.\n\nCould the authors also clarify why they chose to use the harmonic mean to combine the two metrics?\n\nHere, the quality of the generated text is evaluated by another LLM model? How do you make sure, the assessment by the LLM model is accurate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C0qEGCy7us", "forum": "i7QNKZioN6", "replyto": "i7QNKZioN6", "signatures": ["ICLR.cc/2026/Conference/Submission19977/Reviewer_KYVM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19977/Reviewer_KYVM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461525386, "cdate": 1761461525386, "tmdate": 1762932879251, "mdate": 1762932879251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new metric for evaluating the novelty of LLM generations by combining originality and quality. Originality is defined as the fraction of n-grams in the generated text that are not present in the model's training data (harmonic mean). Quality is measured via a task-specific, LLM-as-a-judge score. The proposed novelty metric is the harmonic mean of these two components. Using this framework, the authors conduct a thorough empirical study on open-data models (OLMo, OLMo-2, Pythia) across three creative tasks. Their findings indicate that model scale, improved base models, and post-training all contribute to higher novelty, primarily by improving output quality. Conversely, inference-time techniques like adjusting temperature or using novel in-context examples offer limited gains, often trading originality for quality."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper operationalizes the vague concept of \"novelty\" into a measurable quantity, moving beyond just \"memorization\" to include output quality. This is a critical step for evaluating LLMs on creative and scientific tasks. I know it is a limited version of novelty and many might complain about such limited definitions but this one is intuitive. \n\nThe study is thorough, investigating multiple factors including model scaling, architecture improvements (OLMo vs OLMo-2), post-training, and various inference-time methods. This provides a holistic view of what truly drives novelty.\n\nBy focusing on open-data models with indexed corpora, the study ensures its originality claims are grounded and verifiable, which is a major strength over analyses that use a generic web corpus as a proxy.\n\nthat fundamental model improvements (scale, training) push the novelty frontier while inference tweaks mostly move along it - are both well-supported and intuitive."}, "weaknesses": {"value": "As with all work in this area, the quality scores are a proxy. A ~0.5 Spearman correlation is moderate at best and could influence the results. The harmonic mean is sensitive to low scores, so noisy quality judgments could be impactful.\n\nThe analysis is restricted to open models which, while necessary for the methodology, are not at the state-of-the-art. It remains an open question whether these trends hold for top-tier proprietary models, or even for more modest openly downloadable but not open source models\n\nThe work relies on n-gram overlap, which is a good starting point but doesn't capture semantic or stylistic novelty. The authors acknowledge this as a possible extension.\n\nThe section on sampling temperature feels... naive. The authors show the temp/novelty curve, which is fine, but they completely ignore the vast literature on modern sampling techniques. Where is the discussion of recent methods like Min-P, Top N-sigma, Top-H, or Mirostat, etc sampling? These are becoming standard practice and have complex interactions with temperature (namely they allow very high temperatures in some cases up to infinity). Treating temperature in a vacuum is a glaring omission in an otherwise thorough paper.\n\nWorse, the analysis misses the implementation details that actually matter in production. The order in which you apply samplers (e.g., top-p, then temp) varies across frameworks like vLLM, sglang, and the default huggingface generate method. This order has a massive impact on the final distribution. Treating temperature as a pure thermodynamic concept without acknowledging the grubby reality of its implementation is an oversimplification. For a paper about generation, this lack of systems-level awareness is a miss. I will give them a pass here ONLY because literally the entire field also makes this same mistake and it's unfair to penalize them hard for it here - but it could invalidate the entire sections results... \n\n:et's be real, the models they evaluate kind of suck. OLMo-2 and Pythia are important for the open community, and I get that you have to use them to get access to the training data for a true originality check. But it's still sad. Seeing these trends on a model that can actually perform complex creative tasks would be far more compelling. That said, the authors correctly identify this as a necessary evil, and their framework is the obvious path forward for when we have better tools to analyze SOTA models."}, "questions": {"value": "Why was the harmonic mean chosen over other aggregations, like the geometric mean, or simply analyzing the Pareto frontier of quality vs. originality? The harmonic mean heavily penalizes low scores in either dimension right? Was this behavior explicitly desired?\n\nYour temperature experiments show a clear trade-off. How do you think other modern sampling methods alongside higher temperature (e.g., Min-p sampling, Mirostat, etc) would affect the novelty frontier? Would they offer a better trade-off than temperature alone?\n\n If you were given oracle access to the training data of a frontier model like GPT-4 or Claude 3.5, which of your findings do you believe would change the most, and why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VIgW4ua5Kk", "forum": "i7QNKZioN6", "replyto": "i7QNKZioN6", "signatures": ["ICLR.cc/2026/Conference/Submission19977/Reviewer_ePsk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19977/Reviewer_ePsk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627924371, "cdate": 1761627924371, "tmdate": 1762932878479, "mdate": 1762932878479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This articles proposed to use a harmonic mean as a measurement of novelty of LLM and analyzed widely used methods incuding scaling, posting training etc. to observe improvement in orignnality of the generated contexts"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Creativity of LLM is relatively less explored area in LLMs. Even though creativity = a balance of quality and novelty, the authors provide a more detailed quantiative framework for measurrement. And provide evaluation using a series of different experiments , including Story completion., Poetry writing. and Creative tool use. on  OLMo, OLMo-2, and Pythia at different scale. The authors calculate originality as the fraction of n-grams that do not appear in model training data and se LLM-as-a-judge to approximate the measure of output quality from human annotators. \n\nImportantly, the author did a good job to invetigate what factors affect the novelty of LLMs, includeing sizes, quality of base model, post-training , temperature, in-context learning and prompt engineering."}, "weaknesses": {"value": "3 weakest point:\n\n1) definition of originality is too limited. Should at least try 1-2 other defnition and check results. the novelty score, by LLM, alone is not enough other method is needed \n\n2) I understand why the authors start by using 3 open-data LLM. However, we also need to check other SOTA model at least the open-source ones such as LLama, deepseek, Qwen etc. \n\n3) The contribution is only at intermediate level because there has been many work talking about creativity = balance of novelty and quality"}, "questions": {"value": "The U-shape effect of temperature is interesting , any theoretical explanation ？\n\nnovelty of reasoning model vs. non-reasoning model? \n\nscalability of creativity by test-time computing"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "tTZpbQKkEy", "forum": "i7QNKZioN6", "replyto": "i7QNKZioN6", "signatures": ["ICLR.cc/2026/Conference/Submission19977/Reviewer_bdHB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19977/Reviewer_bdHB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739681225, "cdate": 1761739681225, "tmdate": 1762932877763, "mdate": 1762932877763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper demonstrates an n-gram based metric for novelty of LLM generations in a domain, where the metric is computed using n-gram statistics on the training set. The metric is not semantically aware but does combine quality and diversity in a reasonable and novel way. Evaluations are on open-source model suites with open training data (mostly Olmo 1 & 2). Interesting findings are that post-training helps novelty, prompt engineering (as far as tested) does not noticably affect it, and model scale improves things."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Good quality experiments validating the core metric, and the paper is persuasive that post-training helps novelty significantly, which I might not have expected a priori.\n- The conclusion about prompt engineering techniques yielding limited diversity is valuable, although it could be made more robust by investigating a greater variety of techniques (for example, Bradley et al., (2023) looked at poetry generation and used an evolutionary algorithm-based elicitation technique for increased diversity)."}, "weaknesses": {"value": "- The core metric the paper develops is not semantically aware (as the authors note). As a result, I am uncertain how I would usefully apply this in a practical context (e.g., scientific idea generation conditioned on the literature). Perhaps I would use it as part of a basket of other diversity metrics? The authors could clarify.\n- I would have liked to see more of a focus on post-trained models, since a common debate regarding the diversity of LLM generations is whether RL (DPO, PPO, RLVR) will tend to collapse the diversity of outputs. The paper's finding that post-training helps on Olmo is good, but to make it more robust it would have been great to evaluate e.g. DeepSeek models, or another similar provider? Some of these models are significantly larger than 32B so would give much more robust conclusions—as it is, we cannot justifiably have much confidence in these conclusions generalizing well to the frontier, because there is more than an OOM of parameters between the biggest model here and the frontier.\n- Comparing to training data is appropriate given this paper's goals, but for maximum usefulness it would have been good to spend more time in the main paper operationalizing how one would use this metric without reference to training data (perhaps instead with reference to a precomputed corpus/presampled corpus). After all, one of the main uses of a diversity metric in general would be to evaluate e.g. GPT-5 in a domain of choice to see how diverse its outputs are relative to either some human baseline (where that makes sense) or to other models."}, "questions": {"value": "- Please cite Shypula et al. 2025 (https://arxiv.org/abs/2504.12522) and other similar prior work proposing diversity/quality metrics. The paper could do with better contextualizing the novelty of its contributions. For example: it is one of few works which draw on the training data statistics; it is one of few works which unify quality and diversity directly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GttvBpImCq", "forum": "i7QNKZioN6", "replyto": "i7QNKZioN6", "signatures": ["ICLR.cc/2026/Conference/Submission19977/Reviewer_XcZG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19977/Reviewer_XcZG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930286156, "cdate": 1761930286156, "tmdate": 1762932876819, "mdate": 1762932876819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}