{"id": "ZRDYvWF1ZJ", "number": 12392, "cdate": 1758207522117, "mdate": 1763559838821, "content": {"title": "GIM: Improved Interpretability for Large Language Models", "abstract": "Ensuring faithful interpretability in large language models is imperative for trustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where networks compensate for reduced signal in one component by amplifying others, masking the true importance of the ablated component. While prior work attributes self-repair to layer normalization and back-up components that compensate for ablated components, we identify a novel form occurring within the attention mechanism, where softmax redistribution conceals the influence of important attention scores. This leads traditional ablation and gradient-based methods to underestimate the significance of all components contributing to these attention scores. We introduce Gradient Interaction Modifications (GIM), a technique that accounts for self-repair during backpropagation. Extensive experiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B, Qwen 1.5B/3B) and diverse tasks demonstrate that GIM frequently achieves state-of-the-art results on circuit identification and feature attribution. Our work is a significant step toward better understanding the inner mechanisms of LLMs, which is crucial for improving them and ensuring their safety. Our code is available at https://anonymous.4open.science/r/explainable_transformer-D693.", "tldr": "We identify the attention self-repair effect and propose a new state-of-the-art gradient-based explanation method circumventing it.", "keywords": ["Interpretability", "Explainability", "Faithfulness", "Mechanistic Interpretability", "Feature attributions", "Saliency", "Circuit Identification", "Self repair", "Explainable LLM"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c532bbf2776d81650d8d364b680f4cabeb7b9481.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies attention self-repair, where softmax redistribution within the attention mechanism masks the importance of individual scores, leading gradients to underestimate their true contribution.\nTo address this, the authors propose Gradient Interaction Modification (GIM), which adjusts the softmax gradient via temperature scaling to mitigate this effect.\nThe paper claims improved faithfulness over prior gradient-based feature attribution methods across several datasets and models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel diagnostic observation:** The paper identifies and analyzes the “attention self-repair” phenomenon—an overlooked cause of gradient underestimation in attention-based attribution methods. This diagnostic contribution is conceptually interesting and relevant to the interpretability community.\n\n2. **Clear motivation and simple, insightful remedy:** The motivation around self-repair is well-grounded, and the proposed gradient modification (TSG) provides a simple yet insightful approach that could inspire follow-up research on faithful attention attribution.\n\n3. \u001c**Lightweight and easily reproducible implementation:** Despite its conceptual simplicity, GIM can be applied as a small backward-pass modification without retraining, making it readily testable in other interpretability pipelines and future model analyses."}, "weaknesses": {"value": "1. **Local-only correction:** GIM modifies gradients within each layer but ignores inter-layer coupling through residual and normalization paths. The method remains a local fix rather than a globally consistent gradient correction.\n\n2. **Inter-head redundancy omitted:** Self-repair and TSG are defined per head; cross-head compensation and shared-output interactions are unaddressed.\n\n3. **Top-2 simplification:** Figure 2, which forms a central motivation of the paper, measures self-repair using only the two largest attention scores, missing higher-order redundancy among 3–4 correlated tokens.\n\n4. **Input-length sensitivity ignored:** No experiments are provided to analyze the effect of input token length on TSG behavior.\n\n5. **Limited task scope:** Experiments are restricted to binary classification; the generality of the phenomenon to more complex tasks is unclear."}, "questions": {"value": "**Questions for the Authors**\n\n1. Have you analyzed self-repair effects that arise across multiple layers or between different attention heads? If such evidence exists, including those results would significantly strengthen the paper.\n \n2. Why were only the two largest attention scores used in measuring self-repair (Figure 2)? Have you examined whether including top-3 or top-4 attention weights changes the measured frequency or strength of self-repair? \n\n3. What is the rationale for selecting “significant attention weights” as > 0.01 (lines 305–306)? Was this empirically tuned, and how sensitive are your results to this threshold? If available, please include supporting results.\n\n4. Table 6 shows minimal sensitivity to $T$. What explains this stability? \n\n5. Since softmax normalization can be influenced by sequence length, did you analyze how TSG behaves on longer versus shorter inputs? Would an adaptive or length-dependent temperature yield more stable results?\n    \n6. Have you tested GIM on non-binary or more complex tasks (e.g., multi-class or open-ended settings)?\n\n**Additional Suggestions**\n\n1. Table 1 appears to present mean results only. Including standard deviations or confidence intervals would improve statistical transparency.\n    \n2. Clarify experimental setup: The main text (Eq. 8 and Eq. 9) does not specify layer-wise evaluation, yet Figure 3 reports layer-wise metrics. It would be helpful if the authors could explicitly clarify how layer-wise results were derived, perhaps in the appendix if space is limited.\n\n3. Minor presentation issue: Table 3 appears truncated—please correct this formatting problem."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mEdIKqPQGO", "forum": "ZRDYvWF1ZJ", "replyto": "ZRDYvWF1ZJ", "signatures": ["ICLR.cc/2026/Conference/Submission12392/Reviewer_NupE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12392/Reviewer_NupE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619319947, "cdate": 1761619319947, "tmdate": 1762923293444, "mdate": 1762923293444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an attribution method (GIM) for LLMs. Attribution methods are a key tool for interpretability research as they allow us to quickly identify causally important components of models or inputs. GIM is a combination of existing methods plus the newly-developed temperature-adjusted softmax gradient (TSG) method.\n\nThe paper formalizes the attention self-repair problem where if two positions provide redundant information, and both have high attention scores, ablating one of the two causes little change in the output (the attention weights focus on the other position and obtain the same information, leading to the same output).\n\nTSG aims to address the attention self-repair issue by increasing the softmax temperature during the backward pass to yield gradients that provide better attributions.\n\nGIM is compared to alternative attribution methods, and shown to perform the best when rated on comprehensiveness and sufficiency. Ablations to identify the impact of TSG and other individual ingredients of GIM are performed, showing that each ingredient adds to the performance of GIM.\n\nMandatory disclosure of LLM usage by the reviewer: LLMs were used to format the review text into prose and lists."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies self-repair due to attention as a clear failure-mode of conventional gradient attributions and illustrates it with a toy example (Figure 1).\n\n2. It shows that self-repair occurs in a variety of LLMs and tasks (Figure 2a and appendices).\n\n3. The paper combines several existing tools (layernorm freeze, gradient normalization) and the novel TSG into a single method (GIM).\n\n4. The metrics (sufficiency and comprehensiveness) seem well-chosen for this problem, directly benchmarking attribution scores by how well they predict cumulative ablations.\n\n5. The paper thoroughly compares GIM to methods from the literature (GradientXInput, Integrated Gradients, DeepLIFT, TransformerLRP, AttnLRP), across multiple models and datasets.\n\n6. The paper performs extensive ablations (Figure 4, Table 3) to test which improvements contribute to better attribution."}, "weaknesses": {"value": "1. The paper describes TSG as an empirically-developed method that provides better attributions for OR-gate-like components. This seems well-supported by the mathematical and empirical arguments. However, I wonder whether this method is addressing a symptom or fixing the underlying issue of the attribution: Does TSG still work on non-OR-gate-like settings? Does it make attributions worse in some settings? This is somewhat discussed in the discussion section but left for future work; I would expect to see this investigation in the current paper as proposing TSG as a modification to attribution methods is the central claim of the paper.\n\n2. The statistical significance of the improvement adding TSG is not convincing: Figure 4 compares the attributions with and without TSG. The boxplots are quite close & overlapping. It would be useful to quantify how significant the difference is; potentially also to analyse whether this improvement is on data points where attention self-repair was previously identified. Table 3 only lists point estimates without uncertainties."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "itjVJuMLUG", "forum": "ZRDYvWF1ZJ", "replyto": "ZRDYvWF1ZJ", "signatures": ["ICLR.cc/2026/Conference/Submission12392/Reviewer_zyGM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12392/Reviewer_zyGM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859656877, "cdate": 1761859656877, "tmdate": 1762923293135, "mdate": 1762923293135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a novel self-repair phenomenon within the attention mechanism of transformers. When values at positions with large attention weights contribute similarly toward the output, the effect of ablating an individual attention weights is obfuscated by softmax normalization, mirroring OR-gate like behavior. The authors demonstrate the phenomenon of attention self-repair on a variety of datasets and models by showing that the effect of jointly ablating two attention scores is much stronger than the sum of the effect of ablating the individual attention scores. To mitigate this in the context of attribution, the authors introduce gradient interaction modifications (GIM), a collection of three individual components (temperature-adjusted softmax gradients, layernorm freeze, and gradient normalization). Using GIM, they often achieve higher comprehensiveness and lower sufficiency when ablating model components deemed crucial to the output as compared to other attribution methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The mathematical explanation of the self repair mechanism for attention weights is clear and convincing. \n2. The proposed TSG solution is well motivated theoretically.\n3. The breadth of models and datasets is quite comprehensive.\n4. GIM as a whole often outperforms prior attribution methods in comprehensiveness and sufficiency across a variety of models and datasets."}, "weaknesses": {"value": "1. The depth of analysis for the attention self-repair effect is somewhat weak. While the same analysis is present across multiple datasets and models, it seems limited to comparing the joint and sum of individual ablation effects. Are there any other analyses that the authors investigated to better understand this novel phenomenon, such as the average group size of similarly contributing values with substantial attention weights, or how many groups of such values there tend to be?\n2. Looking at Table 3 in the appendix (which is slightly cut off), it seems that the attribution performance improvement of TSG alone upon Gradient X Input is relatively minor and weaker than either of the two previously established techniques it is combined with, LN freeze and grad norm. Furthermore, while the addition of TSG does somewhat improve performance over just LN freeze and grad norm, such gains are relatively small and inconsistent across different conditions. This seems to be the case both when comparing TSG + LN freeze to LN freeze, TSG + grad norm to grad norm, and TSG + LN freeze + grad norm to LN freeze + grad norm.\n3. As noted in the weaknesses, there is no causal link between TSG better approximating the effects of joint attention weight ablation and the increase in attribution faithfulness of GIM.\n\nAs it currently stands, despite the novelty of the self-repair mechanism for attention and the comprehensive benchmarks run across different models, my score is a 4. This is because the analysis into the novel discovery of the self-repair mechanism seems underexplored, the causal link between attention self-repair and the improvements in faithfulness via TSG are not established, and said improvements are relatively incremental upon prior methods."}, "questions": {"value": "1. See Weaknesses.\n2. For faithfulness per layer, do you have any data for the larger models such as Llama 3.1 8B or Gemma 2 9B? Even without the Integrated Gradients approach, which might take too long?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xdQlns4d8t", "forum": "ZRDYvWF1ZJ", "replyto": "ZRDYvWF1ZJ", "signatures": ["ICLR.cc/2026/Conference/Submission12392/Reviewer_hYi1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12392/Reviewer_hYi1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878361619, "cdate": 1761878361619, "tmdate": 1762923292342, "mdate": 1762923292342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that standard gradient-based explanations can miss important attention behaviours in transformers due to “attention self-repair,” where multiple high-weight tokens with similar value contributions can swap probability mass without changing the output, driving per-score gradients toward zero. To address this, the authors propose a set of tweaks, raising the softmax temperature only during backpropagation to better capture joint effects among interacting scores, along with stabilising adjustments around LayerNorm and gradient scaling. Empirically, they report improved faithfulness metrics across several models and tasks, suggesting these modifications yield explanations that align better with the model’s true reliance on attention"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The approach addresses redundancy-driven cancellation inside the attention mechanism, and integration seems lightweight e.g no model retraining and only small code edits.\n\nThe paper spans multiple model families and datasets with cumulative analyses. The Improvements in comprehensiveness/sufficiency suggest better alignment with model behaviour."}, "weaknesses": {"value": "Most evaluations are short-context benchmarks; it’s unclear how well the approach holds up in longer contexts.\n\nSection 5.2 introduces a 0.1 threshold to decide when to treat effects as joint, which helps but feels coarse, as it is a global heuristic with no principled grounding. Over and underaggregation might occur.\n\nThe approach should be tested to determine whether attributions stay stable under light, meaning-preserving edits (paraphrases, synonym swaps, punctuation/tokenisation noise) and simple adversarial tweaks. Otherwise, the reported gains may be brittle in realistic settings."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3DwnwdZogN", "forum": "ZRDYvWF1ZJ", "replyto": "ZRDYvWF1ZJ", "signatures": ["ICLR.cc/2026/Conference/Submission12392/Reviewer_8bWm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12392/Reviewer_8bWm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950905308, "cdate": 1761950905308, "tmdate": 1762923291979, "mdate": 1762923291979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}