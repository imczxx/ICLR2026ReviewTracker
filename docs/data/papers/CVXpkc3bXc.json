{"id": "CVXpkc3bXc", "number": 21512, "cdate": 1758318386315, "mdate": 1759896918380, "content": {"title": "Kronecker Factorization Improves Efficiency and Interpretability of Sparse Autoencoders", "abstract": "Sparse Autoencoders (SAEs) have demonstrated significant promise in interpreting the hidden states of language models by decomposing them into interpretable latent directions. However, training and interpreting SAEs at scale remains challenging, especially when large dictionary sizes are used. While decoders can leverage sparse-aware kernels for efficiency, encoders still require computationally intensive linear operations with large output dimensions. To address this, we propose **KronSAE** – a novel architecture that factorizes the latent representation via Kronecker product decomposition, drastically reducing memory and computational overhead. Furthermore, we introduce mAND, a differentiable activation function approximating the binary AND operation, which improves interpretability and performance in our factorized framework.", "tldr": "", "keywords": ["Sparse Autoencoders", "Mechanistic Interpretability", "Kronecker Product"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da5f88b8797fc8b6a91312e55636b03d8f4b3284.pdf", "supplementary_material": "/attachment/f09177e24d2065c7fefed7c2a6b738473835b08f.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce a new variant of dictionary learning for language model interpretability. Motivated by increasing computational efficiency, the Kroneker encoder trains a 2-level hierarchy, composing multiple encoders that each operate on distinct subspaces of the embedding space."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The architecture is substantially more parameter efficient that existing SAE architectures.\n- The toy model of correlation section provides evidence of improved learning of correlations compared to TopK SAEs."}, "weaknesses": {"value": "1a. In my view, the main novelty of this work is imposing a prior on which structures to learn: a 2-level hierarchy, for subspaces of the residual stream space. While I agree this architecture improves over compute efficiency, the paper needs a better motivation for the structural prior. Why do we expect language models to learn this specific hierarchical structure? What kind of features is a TopK SAE not able to learn, while the Kron SAE is?\n\n1b. The analysis section provides a qualitative discussion of examples of learned feature hierarchies. Table 2 exemplifies a polysemantic base component that extends to seemingly unrelated concepts: comparative words and directional words and spiritual words. Quantifying the extent to which ground truth concept hierarches in natural language are identified but the KronSAE. Overall, the hierarchical nature of learned features remains underexplored. The Feature Absorbtion evaluation and SAEBench results provide a useful signal to compare performance of existing saes, but does not directly evaluate the recovery of feature hierarchies. \n\n2. The hierarchical prior of KronSAEs is related to the prior of Matryoshka SAEs. I'd like to see a baseline of Matryoshka SAE scores on all evaluations."}, "questions": {"value": "Why are heads only operating on distinct subspaces of the residual streams. What happens to LM features that are an element of the union over input spaces of multiple heads?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ujjfAatFsD", "forum": "CVXpkc3bXc", "replyto": "CVXpkc3bXc", "signatures": ["ICLR.cc/2026/Conference/Submission21512/Reviewer_GyXY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21512/Reviewer_GyXY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924956158, "cdate": 1761924956158, "tmdate": 1762941812274, "mdate": 1762941812274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new sparse autoencoder (SAE) architecture, KronSAE, where the SAE's encoder consists of multiple Kronecker-factored blocks. They present iso-FLOP comparisons between KronSAEs and TopK SAEs in terms of reconstruction, feature absorption, and interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The KronSAE architecture is clearly described.\n2. In some sections, the authors sweep over certain key hyperparameters (F and m) to understand their effect.\n3. I found the discussion of the relationship between pre- and post-latent interpretations interesting."}, "weaknesses": {"value": "Overall I would like to see more systematic reporting of results:\n1. Multiple models. KronSAEs are trained on three models, but most results are only reported for one model. This makes it difficult to tell if results are cherry-picked.\n2. Consistent values of m and F. The reconstruction performance results are shown for multiple values of m, with m=1 being the best. But later, only m=4 is shown for interpretability results. Since, in general, we should expect there to be a trade-off between reconstruction and sparsity (which is typically correlated with interpretability), I worry that there is a tradeoff to KronSAEs which is not being shown here.\n3. Multiple sparsities. Figures 3 and 4 sweep over multiple sparsities; ideally all of the plots would do the same.\nOverall, I would find it much easier to understand the results if the plots in this paper were replaced with line plots where the x-axis was sparsity, and there were multiple lines corresponding to different values of m. There should be one such plot for each model (though some of them can be reported in the appendix).\n\nOther notes:\n1. KronSAEs are only compared to TopK SAEs. As the authors note, Matryoshka SAEs are an idea in a somewhat similar vein. So it would be better if KronSAEs were also compared at least against Matryoshka SAEs as well.\n2. Reconstruction results are best when m=1, but this is also the case when KronSAEs are most similar to the standard architecture; the reconstruction hit is more substantial for m=4,8.\n\nIf these concerned are addressed, I could see myself raising the score to as high as 6."}, "questions": {"value": "1. Is there a reason we would want to localize correlated features to the same head? The paper writes about this as if it's a desirable property, but it's not clear to me why it matters. \n2. This paper makes the choice to study KronSAEs in a resource-constrained setting, i.e. where SAEs are trained for <=1B tokens. I'm curious if you have any sense of what the results look like when training for longer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AqVxIoY42F", "forum": "CVXpkc3bXc", "replyto": "CVXpkc3bXc", "signatures": ["ICLR.cc/2026/Conference/Submission21512/Reviewer_dNqs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21512/Reviewer_dNqs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993579011, "cdate": 1761993579011, "tmdate": 1762941812023, "mdate": 1762941812023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new SAE variant with a structured encoder, called KronSAE. Instead of using a since encoder matrix, this work uses a sum of smaller Kronecker factored matrices. While it doesn't improve training speed or final reconstruction, it improves over the baseline in two domains:\n- Lower parameter count and all the subsequent advantages.\n- A clustering-based (or AND-based) prior for feature extraction, reducing feature splitting.\n\nThis leads to extracted features that are more useful for interpretation over the baseline."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Introducing structured priors into SAEs is useful. This offers both regularization for the training process but also helps interpretability of latents. I think this paper hence makes meaningful progress in an important domain.\n\nDespite my later comments nit-picking sections, the overall presentation is sound. The text is clearly structured and the story makes sense.\n\nThe presented experiments are thorough and most my questions were answered immediately."}, "weaknesses": {"value": "--- **Weaknesses** ---\n\nThe correlation experiment seems heavily favoured towards your approach since this is precisely what the Kronecker structure relies on for extraction. It's nice to see the KronSAE succeeds but I'm fairly certain there's an equally contrived experiment where TopK will find ground truth structure much better than KronSAEs. Actually, looking at Figure 9, it seems that this correlation plot is basically the same regardless of the original patterns, which further undermines this qualitative experiment.\n\nNot sure why mAND is featured so prominently, if the simpler setup of simply doing $u*v$ is only 1% worse, why not just use that? If I'm not mistaken, this is extremely akin to other efficient matrix factorizations like Butterfly/Monarch matrices. The use of these efficient parameterizations is not new in deep learning [1] and a more thorough literature discussion mentioning this would be useful.\n\n[1]: https://arxiv.org/abs/2204.00595\n\n--- **Improvements** ---\n\n\nThe notation was also a bit confusing to me because bold characters are often used to indicate something being a vector of the same matrix (especially if indexed), e.g. $p^k$ is row $k$ of $P$. While it's the text makes it clear, I recommend changing notation a bit. On a related note, why does table 1 use $u$ and $v$ rather than $p$ and $q$, they're the same thing right?\n\nWhile satisfactory, the explanation in Section 3 could be easily improved by adding a diagram. I saw there is one in appendix F but it uses strange notation (and doesn't include the top-k). The diagram should be (in einops notation), einsum(x, x, Q, P, \"... xtop, ... xbot, xtop q h, xbot p h -> (p q h)\"). The Kronecker product is implicit.\n\nFigure is quite hard to read. perhaps there's a nicer way to present this? Also, since you introduced a new metric it seem useful to walk the reader through what they're seeing here. It took some time to figure that out myself.\n\nIn general, many of the figures are too cluttered to read easily. I suggest extracting the important bits and moving the full figure to the appendix or something."}, "questions": {"value": "*\"We had not observed any notable differences in feature geometry between TopK and our SAEs\"*\\\nHow did you measure or observe this?\n\nIf mAND just the RELU variant but with a square root? What's the reason this was preferred outside the slightly higher reconstruction scores?\n\nYou mention the KronSAE is unstable w.r.t n, m and h. What happens qualitatively to the features? Any idea why it fails for specific setups? Answering these questions would provide some insight into the representation structure.\n\nDo you intend to share the code?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5JSa5WAyrm", "forum": "CVXpkc3bXc", "replyto": "CVXpkc3bXc", "signatures": ["ICLR.cc/2026/Conference/Submission21512/Reviewer_eE5v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21512/Reviewer_eE5v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762513976831, "cdate": 1762513976831, "tmdate": 1762941811772, "mdate": 1762941811772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the KronSAE Sparse AutoEncoder (SAE) architecture, which decomposes latent spaces using Kronecker-factorization across multiple heads. Theoretically, KronSAE improves computational efficiency relative to prior SAE approaches; and empirically, authors show across a wide range of experiments and ablations that KronSAE substantially reduces feature absorption, better captures compositional structure in latent spaces, and improves interpretability of learned features."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The work introduces an innovative SAE architecture, KronSAE, that makes substantive improvements over prior SAEs in several dimensions (efficiency, feature absorption, compositionality, and interpretability), and authors clearly demonstrates these improvements empirically across comprehensive experiments. Each of these improvements presents real value to the community in their own right; and taken together, KronSAE represents a clear and significant contribution to the SAE literature."}, "weaknesses": {"value": "I do not see any particularly substantive weaknesses in terms of the technical contributions and empirical work presented in the paper.\n\nMy one concern is that the paper fails to cite any works from the very closely-related research area of tensor product representation (TPR). TPR, first introduced in 1990 [1], has long studied how to encode compositional representations in dense embedding vectors via tensor products (a generalization of the Kronecker product), with more recent works leveraging TPR to interpret compositional representations in LLMs [2-5].\n- Note that I do not believe this significantly erodes the novelty of the paper -- to my knowledge, there is no work applying TPR directly to SAEs, which I understand as being the primary contribution in this work -- but it is important to acknowledge this large body of highly relevant work and compare it with KronSAE. For instance, [3-4] also leverage dictionary learning with TPR to interpret neural representations; but KronSAE requires less advance knowledge of specific role/filler features to look for, and is more general in terms of where it can be applied to interpret model representations (e.g., [4] requires approximating the mapping from input token embeddings (fillers) all the way to the layer whose activations are being reconstructed; whereas SAEs, including KronSAE, can be easily applied to any layer without having to approximate the full model up to that layer).\n\n[1] Smolensky, P. (1990). Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2), 159-216.            \n[2] Smolensky, P., McCoy, R., Fernandez, R., Goldrick, M., & Gao, J. (2022). Neurocompositional computing: From the central paradox of cognition to a new generation of ai systems. AI Magazine, 43(3), 308-322.                 \n[3] McCoy, R. T., Linzen, T., Dunbar, E., & Smolensky, P. (2019). RNNs implicitly implement tensor-product representations. In International Conference on Learning Representations.                                \n[4] Soulos, P., McCoy, R. T., Linzen, T., & Smolensky, P. (2020, November). Discovering the compositional structure of vector representations with role learning networks. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP (pp. 238-254).                                \n[5] Smolensky, P., Fernandez, R., Zhou, Z. H., Opper, M., & Gao, J. (2024). Mechanisms of symbol processing for in-context learning in transformer networks. arXiv preprint arXiv:2410.17498."}, "questions": {"value": "The authors explain KronSAE's improvements on reducing feature absorption in sec 4.2 as follows:\n- Per \"Smooth mAND activation\", you state that \"we introduce a differentiable AND gate [(mAND)] that prevents a broadly polysemantic primitive from entirely subsuming a more specific one.\" *It is not clear to me why this would be the case. Can you explain this rationale in greater detail?* (That is: I understand that, empirically, KronSAE demonstrably reduces absorption per the SAEBench tests -- which is impressive and significant -- and the second explanation of \"Head-wise Cartesian decomposition\" also seems intuitive; I just find the \"Smooth mAND activation\" explanation to be quite opaque. I think the second explanation is already sufficient to make your point, but if you can better clarify the first point as well, that would be helpful.)\n\nOne additional note for the authors is: I personally find the improvements in feature absorption (per sec 4.2) and interpretability/specificity/compositionality (per sec 5.3) to be much more significant contributions than what seem like comparatively modest efficiency gains. As such, I feel that motivating this work primarily from the perspective of efficiency (per the abstract and introduction) \"undersells\" the contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UgED4m0mLF", "forum": "CVXpkc3bXc", "replyto": "CVXpkc3bXc", "signatures": ["ICLR.cc/2026/Conference/Submission21512/Reviewer_Hh9i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21512/Reviewer_Hh9i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762543722309, "cdate": 1762543722309, "tmdate": 1762941811371, "mdate": 1762941811371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces what the authors call KronSAE, an SAE architecture with an idea to address the scalability and interpretability challenges associated with large dictionary sizes in traditional SAEs. The primary optimisation involves factorising the latent representation using Kronecker product decomposition, which aims to reduce the computational and memory overhead of the dense encoder projection. This was identified as one of the key bottlenecks. The architecture also uses mAND, a differentiable activation that approximates the logical AND operation, encouraging compositional structure and improving latent quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Applying Kronecker product decomposition to SAE seems new.  The tensor factorisation in model compression have been considered before (e.g. Edalati et al. 2021) but the specific application to SAE latent spaces with head-wise decomposition as far as the reviewer can see is novel.\n\n- Aiming to address both computational efficiency (encoder bottleneck) as well as interpretability (compositional structure) is  a more ambitious task than usual optimisation-first approaches.\n\n- The authors use a reasonably comprehensive exprimental setup involving 3 LLMs (Qwen, Pythia, Gemma), 3 dictionary sizes  (32k-131k), multiple token budgets.\n\n- The paper is overall written with clarity and nicely presented figures, although some of the labels are too small (most figures)."}, "weaknesses": {"value": "- The paper mentions that Kronecker factorisation induces compositional/hierarchical features. But the mechanism seems to be underspecified and not rigorously justified/described\n\n- The mAND activation (Eq. 3) is one of the important elements of the method but the paper seems to lack principled justification beyond empirical performance. Why square root? How closely does mAND approximate true binary AND? (Fig. 10)  How much the smoothness introduces unwanted activations (e.g. false positives in logical sense)?\n\n- As considered by Bussmann at al. 2025, “Matryoshka SAEs” impose feature hierarchy by nested training. Is there a direct comparison in expriments? How similar/different is KronSAE’s hierarchy from Matryoshka?"}, "questions": {"value": "- Can you provide a more formal theoretical justification as for why Kronecker factorisation induces hierarchical/compositional features? What mathematical properties of the Kronecker product lead to semantic compositionality?\n\n- Could you provide a more direct comparison to Matryoshka SAEs (Busmann et al, 2025), Switch SAEs (2025) and Gated SAEs (Rajamonoharan et al. 2024)? Since you briefly discuss these methods improve efficiency, but how does KronSAE compare quantitatively? Can you provide a more direct comparison under perhaps equal FLOP/paraemter budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rI9KlYQ8hz", "forum": "CVXpkc3bXc", "replyto": "CVXpkc3bXc", "signatures": ["ICLR.cc/2026/Conference/Submission21512/Reviewer_cP5m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21512/Reviewer_cP5m"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762781475123, "cdate": 1762781475123, "tmdate": 1762941811060, "mdate": 1762941811060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}