{"id": "vQLUAkl5SG", "number": 13769, "cdate": 1758222278349, "mdate": 1759897414050, "content": {"title": "DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning", "abstract": "Unlearning in Large Language Models (LLMs) is crucial for protecting private data and removing harmful knowledge. Most existing approaches rely on fine-tuning to balance unlearning efficiency with general language capabilities. However, these methods typically require training or access to retain data, which is often unavailable in real world scenarios. Although these methods can perform well when both forget and retain data are available, few works have demonstrated equivalent capability in more practical, data-limited scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented GeneratiON (DRAGON),  a systematic, reasoning-based framework that utilizes in-context chain-of-thought (CoT) instructions to guard deployed LLMs before inference. Instead of modifying the base model, DRAGON leverages the inherent instruction-following ability of LLMs and introduces a lightweight detection module to identify forget-worthy prompts without any retain data. These are then routed through a dedicated CoT guard model to enforce safe and accurate in-context intervention. To robustly evaluate unlearning performance, we introduce novel metrics for unlearning performance and the continual unlearning setting. Extensive experiments across three representative unlearning tasks validate the effectiveness of DRAGON, demonstrating its strong unlearning capability, scalability, and applicability in practical scenarios.", "tldr": "We propose DRAGON, a lightweight black-box unlearning framework that leverages detection and chain-of-thought reasoning to enforce safe, in-context interventions without modifying the underlying LLM.", "keywords": ["LLM Unlearning", "In-context Learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d14c7999641439dfe053f3940de4402f691f60b2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces DRAGON (Detect–Reasoning Augmented GeneratiON), a framework for black-box unlearning in large language models (LLMs).\nUnlike prior training-based unlearning methods (e.g., TOFU, RMU, NPO-RT, FLAT) that require access to both forget and retain data and involve fine-tuning, DRAGON proposes a training-free, in-context alternative that guards deployed LLMs before inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Addresses a real deployment pain point: unlearning without retraining for black-box LLMs.\n* Effective and scalable: once trained, the guard model generalizes across models and tasks.\n* Comprehensive evaluation: nine LLMs, multiple domains, and both synthetic and realistic datasets.\n* Good ablations: CoT necessity, detection variants, continual unlearning, robustness under attack."}, "weaknesses": {"value": "* Over-reliance on refusal behavior. - DRAGON’s “forgetting” may correspond to censorship, not true knowledge deletion. The paper occasionally conflates the two.\n* Detection dependency. - The unlearn store and paraphrase-based matching may not scale or generalize to high-entropy data (e.g., private identifiers unseen during training).\n* Latency and cost. - Despite claims of low overhead, Table 18 shows inference latency > 600 ms for guarded prompts—non-trivial for interactive systems."}, "questions": {"value": "* How is “forgetting” conceptually distinct from “refusal to answer”? Could DRAGON actually preserve memorized data internally while just refusing to reveal it?\n* How sensitive is performance to the choice of CoT guard model (e.g., 7 B vs 13 B)?\n* Are DDS/DUS correlated with human judgments of stability, or are they purely mathematical heuristics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NyH1C3AZjd", "forum": "vQLUAkl5SG", "replyto": "vQLUAkl5SG", "signatures": ["ICLR.cc/2026/Conference/Submission13769/Reviewer_aVh4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13769/Reviewer_aVh4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761338362659, "cdate": 1761338362659, "tmdate": 1762924301808, "mdate": 1762924301808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DRAGON, a framework for unlearning in LLMs that operates without fine-tuning the base model or requiring access to retain data. It focuses on practical scenarios like privacy protection, removal of harmful knowledge, and compliance with regulations such as GDPR. DRAGON uses a lightweight detection module to identify \"forget-worthy\" prompts based on paraphrased negative data stored in an unlearn store, combining similarity metrics with a trained scoring model for harmful content. Upon detection, it routes the prompt through a fine-tuned CoT guard model to generate reasoning instructions, which are prepended to the input for in-context intervention, enabling refusals or redirections. The framework supports both sample unlearning and concept unlearning. Experiments on datasets like WMDP and TOFU show DRAGON achieving near-random guessing accuracy on forget sets with high RQ scores, while preserving MMLU performance, outperforming baselines across models from Zephyr-7B to Mixtral-47B."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The experimental evaluation is thorough and well-designed, with results across nine diverse LLMs on three tasks, including ablation details in Appendix C and comparisons to strong baselines like RMU and ICUL+ using both standard metrics and the novel RQ/DDS/DUS, demonstrating rigorous validation of scalability and generalizability.\n- DRAGON's design supports continual unlearning without cumulative utility degradation, as evidenced by low DDS and high DUS in sequential TOFU unlearning steps, making it suitable for dynamic environments where unlearning requests arrive over time.\n- It scales efficiently to black-box LLMs without parameter modifications, incurring no additional fine-tuning costs for larger models, and performs well on copyrighted content unlearning, showing versatility across privacy, harm, and IP tasks."}, "weaknesses": {"value": "- Fine-tuning the CoT guard model on a fixed dataset of 1,000 question-CoT pairs limits generalization to out-of-distribution unlearning tasks.\n- The threshold-based triggering is adaptive but requires manual tuning of hyperparameters, which could introduce instability in real-world deployments without automated calibration, and the paper lacks sensitivity analysis on these values across different LLM sizes."}, "questions": {"value": "- The paper mentions using GPT-4o for CoT dataset curation in harmful knowledge tasks; is there a risk of privacy leakage when relying on external APIs for sensitive unlearning?\n- How does DRAGON's in-context intervention compare to ALKN's dynamic masking on task vectors for mitigating accumulative errors in continual unlearning [1]? In what ways does DRAGON's training-free base model design outperform or underperform GRU's gradient projection method in balancing unlearning and retention [2]?\n\n[1] Wuerkaixi, Abudukelimu, et al. \"Adaptive localization of knowledge negation for continual llm unlearning.\" Forty-second International Conference on Machine Learning. 2025.\n\n[2] Wang, Yue, et al. \"Gru: Mitigating the trade-off between unlearning and retention for large language models.\" arXiv e-prints (2025): arXiv-2503."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lP3hb9bYPw", "forum": "vQLUAkl5SG", "replyto": "vQLUAkl5SG", "signatures": ["ICLR.cc/2026/Conference/Submission13769/Reviewer_ZPsu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13769/Reviewer_ZPsu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745658486, "cdate": 1761745658486, "tmdate": 1762924301447, "mdate": 1762924301447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DRAGON, a in-context unlearning method for LLMs. The method operates in a black-box setting, detecting and intervening sensitive or harmful requests during inference without retraining. The paper also introduces three evaluation metrics, RQ for assessing quality of refusals, and DDS and DUS for measuring stability in continual unlearning. \n\nThe paper offers comprehensive experiments on privacy (TOFU), hazardous knowledge (WMDP), and copyright content (MUSE) and show that DRAGON outperforms selected unlearning baselines. The paper further includes analyses such as continual learning and robustness evaluation, adding valuable insights into its practical performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Practical setting**: the method works at inference time in a black-box setting, making it more practical than training-based unlearning methods, which often suffer from unpredictable side effects. Experimental results in the paper show that DRAGON outperforms both training-based and training-free baselines, demonstrating its effectiveness. \n\n2. **Strong empirical results**: the paper presents consistent performance across multiple datasets and models. The ablation studies in section 5.2 & 5.3 also gives storng justification for the detection method & CoT guard components. \n\n3. **Continual unlearning formulation**: The inclusion of metrics and experiments for continual unlearning is, as far as I know, novel and valuable."}, "weaknesses": {"value": "1. **Scalability and Latency**: The paper does not report latency or cost overhead of real-time detection and CoT generation, which could limit practical deployment. I am especially concerning about using GPT-4o for CoT generation. \n   - Questions: \n      1. Can you provide a simple runtime or cost overhead estimation of DRAGON?\n      2. Could ligher models replace GPT-4o for CoT generation? ( idle thought, no need to respond to this question) \n\n2. **False positive rate**: A analysis on false positive rate is provided in table 17, the reported FPR are high (e.g. 11% for WMDP - Simpe QA, 5% for WMDP - Alpaca-400). As far as I know, these datasets differ substantially from WMDP; evaluation on out-of-distribution datasets more similar to the target query distribution (e.g. subsets of MMLU) would strengthen the claim of robustness. \n   - Questions: \n      1. Can you provide more evidence for the robustness of the detection method, especially about false positive rates? \n\n3. **Robust to adversarial attacks**: The paper does not examine the method’s resilience to adversarial or jailbreaking prompts, which is crucial for assessing its real-world reliability.\n   - Questions: \n      1. Have you considered evaluating DRAGON under adversarial conditions or simple jailbreak attempts to test its stability and refusal consistenty?"}, "questions": {"value": "Please see the questions in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ExIMobmzWq", "forum": "vQLUAkl5SG", "replyto": "vQLUAkl5SG", "signatures": ["ICLR.cc/2026/Conference/Submission13769/Reviewer_zvir"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13769/Reviewer_zvir"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761784537825, "cdate": 1761784537825, "tmdate": 1762924300811, "mdate": 1762924300811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DRAGON, a training-free framework for LLM unlearning—removing harmful or private knowledge from deployed models without retraining or accessing retain data. Instead of modifying model weights, DRAGON performs in-context unlearning through a two-step process:\n1. Detection Module: Identifies “forget-worthy” prompts using a hybrid approach combining a trained scoring model and similarity-based metrics.\n2. CoT Guard Model: Generates chain-of-thought (CoT) instructions that guide the base LLM toward safe refusals or redirections in real time.\n\nExperiments on WMDP, TOFU, and copyrighted content unlearning demonstrate DRAGON’s superior trade-off between forgetting efficiency and general utility across multiple LLM architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "In-context unlearning paradigm: DRAGON departs from prior fine-tuning-based unlearning methods by leveraging reasoning and CoT prompting as an adaptive safety mechanis.\n\nNew evaluation metrics: The proposed dynamic metrics expand evaluation beyond one-shot unlearning, emphasizing temporal stability and utility preservation.\n\nExtensive experimentation: Evaluation spans 9 diverse models and multiple benchmarks (WMDP, TOFU, MMLU). The use of continual unlearning setups and ablations (CoT necessity, detection module) strengthens empirical validity.\n\nThe paper is clearly written, with well-structured motivation and illustrations."}, "weaknesses": {"value": "Dependency on synthetic data quality: The unlearn store and CoT dataset rely on paraphrased or GPT-4o–generated samples. The paper doesn’t quantify how paraphrase quality or distribution shift affects detection robustness or unlearning accuracy.\n\nLimited discussion of failure modes: While DRAGON performs well on WMDP/TOFU, potential adversarial rephrasing attacks (e.g., subtle prompt obfuscations) are only briefly mentioned. More robustness evaluation would strengthen the claims.\n\nNo rigorous theoretical justification: The connection between CoT reasoning and improved unlearning efficacy is largely empirical. A formal analysis would enhance interpretability.\n\nEvaluations: \n\nExperiments focus mostly on English text and a narrow set of unlearning domains.\n\nReal-world deployment latency, cost, and inference overhead of the detection + guard model pipeline are not quantified."}, "questions": {"value": "Ablation of paraphrase generation: How sensitive is detection performance to the choice or number of synthetic paraphrases in the unlearn store?\n\nInference latency and scalability: Since DRAGON adds a detection and guard model in the loop, how does it affect real-time deployment costs?\n\nRobustness under adversarial prompts: Have the authors tested prompt injection or obfuscation attacks?\n\nMetric design: Could the authors clarify how the three subcomponents of RQ are weighted and normalized? Would human evaluation correlate with RQ improvements?\n\nGeneralization beyond safety/privacy unlearning: Could DRAGON handle fine-grained knowledge editing tasksor multimodal unlearning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WIseTSuh2r", "forum": "vQLUAkl5SG", "replyto": "vQLUAkl5SG", "signatures": ["ICLR.cc/2026/Conference/Submission13769/Reviewer_GuoJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13769/Reviewer_GuoJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919495343, "cdate": 1761919495343, "tmdate": 1762924300317, "mdate": 1762924300317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}