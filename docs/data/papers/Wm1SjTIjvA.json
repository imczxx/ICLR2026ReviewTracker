{"id": "Wm1SjTIjvA", "number": 8661, "cdate": 1758093892882, "mdate": 1759897771305, "content": {"title": "Stability Matters: Combating Parameter Shifts in Low-Rank Adaptation for Continual Learning", "abstract": "Continual Learning (CL) has increasingly embraced Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), to balance task adaptability with parameter efficiency.  Existing LoRA-based approaches resort to low-rank matrices to inherently capture task-specific parameter shifts, whereas meantime mitigate interference between tasks through architectural design (e.g., Mixture-of-Experts) or optimization constraint (e.g., orthogonality). However, they largely overlook how these shifts evolve across tasks, i.e., the internal dynamics of parameter space, which is a crucial yet underexplored factor in model forgetting.  In this work, our analysis reveals a key insight that abrupt performance drops often coincide with drastic changes in the distribution of learned parameter shifts. Motivated by this, we propose a simple yet effective Parameter Stability Loss that regularizes both the sign and magnitude of parameter updates to mitigate forgetting. Beyond training-time regularization, we also introduce a post-training model merging step that bridges earlier directions with the current one and further combats the inevitable drift toward new tasks. Our method Parameter Stable LoRA (PS-LoRA) achieves state-of-the-art results on multiple continual learning benchmarks, with performance improvements of up to 3%, and can be integrated with existing approaches.", "tldr": "", "keywords": ["continual learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba62a8e8439f180128c40d5757b99fba527d754e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates continual learning (CL) using Low Rank Adaptation (LoRA) parameter-efficient fine-tuning. The authors find that significant drops in performance during CL were consistently accompanied by abrupt changes to the LoRA parameters. In accordance with their findings, they propose a regularization term to mitigate large-magnitude and opposite-sign updates during training. Subsequently, they integrate their method with existing “model-merging” techniques for post-training merging. Finally, they provide a comprehensive evaluation on various vision and language benchmarks, demonstrating the effectiveness of their method, as well as performing ablation on the different components of their methodology."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The findings regarding sudden parameter distribution shifts, while not unexpected, are insightful into the mechanisms of catastrophic forgetting\n- The paper explores a relatively novel aspect of regularization during LoRA, having to do with the individual signs of the parameter updates\n- The benchmark experiments are extensive, using both vision and language tasks and various architectures"}, "weaknesses": {"value": "- The main finding of the paper, the sudden parameter shifts, is not explored/highlighted sufficiently. Furthermore, the results in the appendix do not seem particularly consistent with the claims. Perhaps I am missing something. Refer to the questions section for more discussion regarding this.\n- Very little/weak theoretical justification, in fact, section 3.4 seems unnecessary as the argument is very simple, not particularly novel or specific to the method/findings of the paper. For example, the same argument would apply to a simple L2 penalty. Perhaps it is better placed in the appendix"}, "questions": {"value": "- 1. Major concern: As mentioned in the weaknesses, the figures in the appendix (fig 8 & 9 & 10) do not seem to follow the same pattern of abrupt parameter changes, and do not include the model performance in tandem (like that of fig. 1). In fact, in fig 8 and fig 10 it seems that the initial tasks have the largest parameter shifts. I remain unconvinced that the visible abrupt change in parameters, accompanied by a large drop in performance, is a widely occurring phenomenon in LoRA CL or just an artifact of the specific experiment used in fig. 1 & 2. If the authors could provide convincing evidence or if there is a problem with the figures that could be fixed, that would have an impact on my suggested decision.\n- 2. Minor comment on fig 1 2: I believe showing the $\\Delta W$ for each task separately (like fig 2) is much more illustrative, and renders the aggregated version (fig 1 b) obsolete. Is there any benefit to showing the aggregate $\\Delta W$?\n- 3. As mentioned above, the theoretical result does not seem particularly relevant to the proposed method. I don't think a strong theoretical result is a requirement for acceptance (although a stronger result would certainly increase the contribution of the work in my view). If the authors cannot provide additional insight into why this result is important or extend their result further, I believe it's best to move section 3.4 to the appendix. \n- 4. Line 202 Mentions bottom k% of parameters. Bottom with respect to what? Magnitude?\n\n- 5. Eq 4: The loss equation seems confusing, as the $(1 - tanh \\cdot tanh)$ term is the same dimension as the weight update, but the l2 norm term is a scalar. (the overall loss term needs to be a scalar) Could the authors clarify how the loss is computed? \n\n- 6. If I understand correctly Eq. 5 reduces to just taking the weight update with the largest magnitude? If so, is there a need for the recursive notation?\n\n- 7. Algorithm 1: in the text there is no mention of merging the loras during training (it is explicitly called a post-training step), but Algorithm 1 shows the usage of merged previous loras during the forward pass? Could the authors elaborate?\n\n- Finally, small comment about notation of LoRA, sometimes it is written as $\\Delta W$ but other times as product $A B$. As there seems to be no relevance to the usage, I would encourage consistency of notation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oukfXqdemz", "forum": "Wm1SjTIjvA", "replyto": "Wm1SjTIjvA", "signatures": ["ICLR.cc/2026/Conference/Submission8661/Reviewer_w91z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8661/Reviewer_w91z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761082630299, "cdate": 1761082630299, "tmdate": 1762920479124, "mdate": 1762920479124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript studied in the problem of continual learning with pre-trained models. Unlike the existing methods that mitigated the task interference through Mixture-of-Expert (MoE) or orthogonal optimization constraint, this manuscript took a closer look about how the parameter space shifts happened and how it excerbated the forgetting. Based on the empirical anslysis, the authors indicated that the abrupt performance drops during training often happened with drastic changes in the distribution of learned parameter shifts. Based on this observation, the authors proposed the Parameter Stability Loss to regularize both the sign and magnitude of parameter updates. Meanwhile, a post-training model merging step was also proposed to bridge earlier directions with the current direction. The proposed frame PS-LoRA showed good performances on multiple CV and NLP benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Starting with the empirical analysis in the observation, the authors proposed a detailed investigation on the phenomenon of parameter space shift. The motivation from these analyses is clear to the readers.\n2. The proposed Parameter Stability Loss was easy to implement, and the computation seems light according to Eq. (4).\n3. The proposed framework also introduced the model merging strategy, which combined all task-specific LoRA branches into a single weight form, reducing the storage overhead for the inference. \n4. The empirical verifications were conducted on both CV and NLP tasks, covering the representative model."}, "weaknesses": {"value": "1. Some details were not clear to the reviewer. \n2. More baselines method can be involved to discuss and compare."}, "questions": {"value": "1. Although we can apply model merging described in Eq. (5), I am still wondering if we need to store every task-specific LoRA branches for $\\forall i=1,\\dots,t-1$ to apply the regularization loss in Eq.(4). In Eq. (4), it indicated that we need to store the accumulated term $\\sum_{i=1}^{t-1} \\mathbf{A}_{i} \\mathbf{B}_{i}$, \n\nwhich was also illustrated in Fig. 5. In Line 297, the last line of Algorithm 1 also indicated that the merging was finished after training all tasks, instead of training each task. Thus, I wonder if we need to store all $\\mathbf{A}_{i}, \\mathbf{B}_{i}$, as well as $\\Delta \\mathbf{W}_{[1: t-1]}$, at the same time. \n\nIf so, it seems that more parameter states need to be maintained during training. I wonder if it is still efficient compared to other baselines.\n\n2. The meaning of Fig 1(b) is confusing. Is it $\\Delta \\mathbf{W}$ that you are visualizing? Please give an accurate description regarding \"parameter shift\".\n\n3. It seems that some recent LoRA-based continual learning methods were not discussed or compared in the experiments, e.g., InfLoRA [1] and BiLoRA [2].\n\n4. The authors discussed the constraints from two perspectives, (1) sign constraint; (2) magnitude constraints. I wonder if the authors could provide ablation studies regarding these two aspects. I only noticed the ablation studies regarding the two components in Table \n5.\n\nReferences:\n\n[1] InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning. CVPR 2024.\n\n[2] BiLoRA: almost-orthogonal parameter spaces for continual learning. CVPR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7ajlyjAtjT", "forum": "Wm1SjTIjvA", "replyto": "Wm1SjTIjvA", "signatures": ["ICLR.cc/2026/Conference/Submission8661/Reviewer_MTuw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8661/Reviewer_MTuw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761538214731, "cdate": 1761538214731, "tmdate": 1762920478672, "mdate": 1762920478672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the relationship between parameter shifts and catastrophic forgetting in LoRA-based Continual Learning. Through empirical analysis (Figure 1), the authors observe that abrupt performance drops on previous tasks often coincide with drastic changes in the distribution of learned LoRA parameter shifts ($\\Delta W_t$ or $\\sum \\Delta W_i$), particularly involving large updates with signs opposite to the accumulated direction. Motivated by this, they propose PS-LoRA, which incorporates two main components: 1) A Parameter Stability Loss applied during training. This loss regularizes LoRA updates ($A_t B_t$) by penalizing both large magnitudes and sign inconsistencies relative to the accumulated previous updates ($\\sum_{i=1}^{t-1} A_i B_i$), using a $tanh$-based alignment term. 2) A post-training model merging step that combines the LoRA updates from all tasks ($\\Delta W_1, ..., \\Delta W_N$) into a single update $\\Delta W_{[1:N]}$ using a magnitude-based strategy (element-wise selection of the value with larger absolute magnitude). This merged update is then added to the base model $W_0$ for inference. The paper claims that PS-LoRA achieves state-of-the-art results on NLP and CV continual learning benchmarks by stabilizing parameter updates and effectively consolidating knowledge."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Relevant Perspective: Focuses on the dynamics of parameter shifts during LoRA-based CL, an important but relatively under-explored aspect compared to architectural modifications (MoE) or strict optimization constraints (orthogonality). The empirical observation linking drastic shifts (especially sign flips) to forgetting (Figure 1, Figure 3) provides a compelling motivation.\n\n* Good Empirical Performance: Achieves strong results on various CL benchmarks, outperforming several existing LoRA-based CL methods (Table 1, Table 4), demonstrating the practical effectiveness of the proposed stability constraints and merging strategy."}, "weaknesses": {"value": "* Limited Novelty: The core components, while combined effectively, possess limited individual novelty. Regularization to prevent forgetting is a cornerstone of CL (e.g., EWC, SI, LwF). While $\\mathcal{L}_s$ specifically targets LoRA parameter signs and magnitudes, it falls under the broad category of regularization. Similarly, model merging techniques are increasingly studied, and magnitude-based merging criteria have precedents (e.g., related ideas in MagMax). The paper's main contribution lies in observing the specific issue in LoRA CL and applying these existing concepts.\n\n\n* Basic Theoretical Analysis: The theoretical justification provided (Section 3.4) uses a second-order Taylor expansion of the loss to argue that forgetting depends on the magnitude of parameter updates ($\\|\\theta - \\theta_A^*\\|^2$) and loss curvature ($H_A$). While correct, this is a fairly standard result and doesn't offer deep insights specific to the sign alignment component of $L_s$ or the merging strategy.\n\n\n* Merging Strategy Limitations: The simple magnitude-based merging M(⋅,⋅) might discard parameters that, despite having smaller magnitudes, are crucial for specific older tasks. While effective on average, it could lead to suboptimal performance on certain tasks compared to methods that retain task-specific parameters. The paper lacks analysis on task-specific performance post-merging. Ablation in Table 19 shows it's better than other merging methods in this context, but the fundamental limitation remains."}, "questions": {"value": "* Regarding the motivation: Could the observed large parameter shifts and performance drops both be consequences of large domain shifts between tasks, rather than the shifts directly causing the forgetting? How does PS-LoRA ensure sufficient plasticity when encountering a very dissimilar task that might require large parameter adjustments?\n\n* The Parameter Stability Loss includes magnitude and sign terms. Can the authors provide a more fine-grained ablation study showing the separate contribution of just the magnitude constraint versus just the sign alignment term within $\\mathcal{L}_s$? (Table 12 ablates $\\mathcal{L}_s$ vs merging, but not the components within $\\mathcal{L}_s$).\n\n* How sensitive is the performance to the hyperparameter $\\lambda$ (weighting $\\mathcal{L}_s$) and $\\alpha$ (temperature in $tanh$)? Could the authors provide sensitivity analysis? (Table 13 shows sensitivity for $\\lambda$ but not $\\alpha$).\n\n\n* How does the magnitude-based merging strategy compare to more sophisticated merging techniques (e.g., averaging, task arithmetic, TIES-merging) when applied after training with the Parameter Stability Loss? (Table 19 compares merging strategies but seemingly without the stability loss during training for baselines). Does $\\mathcal{L}_s$ make the parameters more amenable specifically to magnitude-based merging?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RnnYNnKSe2", "forum": "Wm1SjTIjvA", "replyto": "Wm1SjTIjvA", "signatures": ["ICLR.cc/2026/Conference/Submission8661/Reviewer_D8Ns"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8661/Reviewer_D8Ns"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761564227517, "cdate": 1761564227517, "tmdate": 1762920478198, "mdate": 1762920478198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a real problem in continual learning with parameter efficient tuning. When LoRA modules are trained on a sequence of tasks, their low rank parameters can change direction and magnitude quite abruptly. The authors refer to this as parameter shift. They observe that such shifts often correlate with sharp drops on earlier tasks. To address this, the paper proposes a stability regularizer that penalizes large deviations of LoRA parameters from their previous state, and a simple model fusion step at inference time that blends information from several task adapters. The intent is to slow down destructive updates and to preserve older knowledge while keeping the method lightweight."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper targets a concrete problem in continual learning with LoRA style PEFT. The observation that low rank parameters can shift sharply across tasks and that such shifts correlate with drops on earlier tasks is reasonable and worth documenting. The proposed stability penalty is simple, can be added to existing LoRA trainers without changing the backbone, and therefore has immediate practical value. The paper also tries to keep the solution lightweight and proposes an inference time fusion step to reuse past task adapters, which is attractive for practitioners."}, "weaknesses": {"value": "The contribution is incremental. A penalty on parameter change is a very direct idea and is close in spirit to classic continual learning tools such as EWC, L2-SP and adapter freezing. The paper does not clearly justify why this particular form of stability regularization is better than those alternatives. The experimental evidence is not strong enough for ICLR. Most task sequences are short, there are no results with multiple random seeds, and the gains over LoRA baselines are often small. The comparison set does not include stronger recent PEFT-for-CL methods that use multiple adapters or routing, so it is hard to tell where this method stands relative to the current state of the art. The proposed inference time fusion step seems to contribute a non-trivial part of the final improvement, but the paper does not separate the effect of fusion from the effect of the stability penalty, which makes the core contribution less clear. The method also introduces a stability weight that is not analyzed; without a sensitivity study it is unclear whether the method is robust across tasks and backbones."}, "questions": {"value": "1. Can you run a longer continual sequence, for example 8–10 tasks, and report average accuracy, forgetting and backward transfer with at least 3 seeds?\n2. Can you give a sensitivity plot for the stability weight on two different backbones?\n3. Can you compare to at least one recent multi-adapter or routing based PEFT-CL method under the same compute and memory?\n4. Can you provide a table that reports: LoRA baseline, LoRA + stability penalty, LoRA + fusion only, and your full method?\n5. If fusion is essential, can you clarify the deployment setting where multiple task adapters are kept at test time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DqcuMzzwx8", "forum": "Wm1SjTIjvA", "replyto": "Wm1SjTIjvA", "signatures": ["ICLR.cc/2026/Conference/Submission8661/Reviewer_jCA9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8661/Reviewer_jCA9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804811725, "cdate": 1761804811725, "tmdate": 1762920477607, "mdate": 1762920477607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}