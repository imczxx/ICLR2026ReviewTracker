{"id": "0ZRne2Nt8t", "number": 25589, "cdate": 1758369329328, "mdate": 1759896714044, "content": {"title": "MAIG: Multi-agent system for Academic Illustration Generation based on deep search and reflection", "abstract": "While text-to-image models have revolutionized creative content generation, they fall short in the domain of academic illustration, which demands stringent scientific accuracy and informational completeness, creating a significant bottleneck in automated scientific communication. Existing models often produce illustrations that are factually incorrect, omit critical information, and are limited to simple structured diagrams, failing to render the complex, unstructured conceptual visuals common in science. To address these challenges, we introduce \\textbf{MAIG}, a novel multi-agent framework that mimics an expert's workflow. MAIG first employs a deep research agent to ground the generation process in a factual knowledge base, ensuring all necessary background information is available. Subsequently, reflection and editing agents iteratively verify the visual output against this knowledge, identifying and correcting scientific errors. In the meantime, evaluating scientific figures is a parallel challenge plagued by subjective and unscalable methods, we also propose a novel Question-Answering (QA) based Evaluator. This method leverages the strong reasoning capabilities of modern Multimodal Large Language Models (MLLMs) to quantitatively measure both informational completeness and factual correctness, providing an objective and scalable assessment of an illustration's quality. Extensive experiments across various scientific disciplines demonstrate the effectiveness of MAIG, which achieves minimal factual errors and the most complete knowledge coverage, significantly outperforming state-of-the-art models.Our results validate that the proposed research-reflect-edit loop is crucial for generating high-fidelity scientific illustrations and that our QA-based evaluator offers a reliable assessment methodology, together forming a comprehensive solution for advancing automated scientific visualization.", "tldr": "", "keywords": ["Image Generation", "Multi-Agent", "Academic Illustration"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/12568a349c67261d96ca8ec782fbbeb5ad7e7868.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "MAIG integrates multiple agents to generate academic illustrations from textual descriptions. The authors identify key shortcomings in existing text-to-image models when applied to scientific domains, namely a lack of factual accuracy, informational completeness, and the inability to generate complex, unstructured conceptual visuals. The proposed MAIG system leverages a team of specialized AI agents, including a manager, a search agent, an illustration generation agent, and a reflection agent, to address these challenges. The workflow involves the search agent gathering relevant information, the generation agent creating an initial illustration using existing tools, and the reflection agent iteratively refining the output for accuracy and completeness. The authors use a QA-based evaluation metric to assess the quality of generated illustrations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The system is well-designed, e.g., the integration of external tools (Search, ECharts, Stable Diffusion) within the agentic workflow is a practical and effective method. \n\n- The paper is easy to follow."}, "weaknesses": {"value": "- It seems like an overclaim that \"we also propose a novel Question-Answering (QA) based Evaluator\". QA based evaluation is not a new thing. \n\n- It is more like an engineering work rather than research. The overall concept of **using a multi-agent system for a complex task** has been explored extensively. The authors should, for instance, discuss how their framework compares to general-purpose multi-agent frameworks like AutoGen or CrewAI, and why a bespoke system was necessary.\n\n- Insufficient comparison to recent agentic frameworks. The paper positions itself as a novel multi-agent system but fails to adequately discuss or compare its approach with a growing body of work on agentic workflows and multi-agent collaboration.\n\n- The paper presents a very positive view of the MAIG system. A more critical discussion of its limitations and failure modes would be valuable. When does the system fail? Are there certain types of diagrams or concepts that it struggles with? How robust is the system to noisy or ambiguous inputs?\n\n- typos, e.g., ”question (line150)"}, "questions": {"value": "- Do not want to mention this in the weakness that this paper is an okay application paper, stacking models and tools, but not sure if it is really related to \"learning representations\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5qabVxwH1l", "forum": "0ZRne2Nt8t", "replyto": "0ZRne2Nt8t", "signatures": ["ICLR.cc/2026/Conference/Submission25589/Reviewer_xRDB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25589/Reviewer_xRDB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760545356869, "cdate": 1760545356869, "tmdate": 1762943487908, "mdate": 1762943487908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce an inference pipeline combining LLMs and image generation/editing models to automatically generate academic illustrations as raster graphics. The authors demonstrate that their approach outperforms other end-to-end methods through an automatic evaluation based on Q&A with VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The authors tackle a challenging problem and achieve state-of-the-art results.\n* The paper is well-written and easy to follow."}, "weaknesses": {"value": "* The authors promote their Q&A-based evaluation framework as one of the core contributions, but later acknowledge that they have adopted this approach from prior work (l.248), which limits the novelty of their contribution.\n* The other core contribution is the inference pipeline itself. While it performs well, it is essentially a wrapper around proprietary APIs, and the finding that investing more compute leads to better performance is hardly surprising."}, "questions": {"value": "Text rendering appears to be a crucial component, do the authors evaluate this aspect? If not, how well did it perform in their experience?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NFwO5igLE1", "forum": "0ZRne2Nt8t", "replyto": "0ZRne2Nt8t", "signatures": ["ICLR.cc/2026/Conference/Submission25589/Reviewer_zpkA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25589/Reviewer_zpkA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797976917, "cdate": 1761797976917, "tmdate": 1762943487637, "mdate": 1762943487637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Multi-agent pipeline (research → generate → reflect/edit) for scientific illustration; introduces an MLLM QA-based metric (ACC/NBR) for factuality/completeness; reports gains over GPT-4o/Qwen on small bespoke tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem framing (accuracy + completeness in academic figures).  \n- End-to-end system with module ablations.  \n- Tries to go beyond style metrics via automatic evaluation."}, "weaknesses": {"value": "- **Thin novelty**: routing + research–reflect–edit mirrors prior multi-agent T2I/diagram pipelines, as well as QA-evaluator = MLLM-as-grader.  \n- **Questionable evaluation**: self-referential grading, ad-hoc ACC/NBR, small author-curated datasets.  \n- **Baselines dubious**: Qwen near-zero, unclear prompts/settings; no significance tests/error bars.  \n- **Reproducibility**: heavy reliance on closed models (GPT-4o/“GPT-5”).  \n- **Leakage/overfitting risk**: authors design questions and then evaluate with same paradigm."}, "questions": {"value": "No questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bhvJ7LcH3E", "forum": "0ZRne2Nt8t", "replyto": "0ZRne2Nt8t", "signatures": ["ICLR.cc/2026/Conference/Submission25589/Reviewer_26nX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25589/Reviewer_26nX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824899190, "cdate": 1761824899190, "tmdate": 1762943487435, "mdate": 1762943487435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes two multi-stage generation pipelines for scientific Illustrations. If a context is available (as usual for scientific papers captions and texts), they (1) extract a description from the caption, (2) enrich it with details from the paper text, (3) generate an initial image, (4) reflect on the content and (5) edit the image, where every step is conducted by GPT. When context is scarce (like for generating a graphic for teaching scenarios), they first acquire context via web search. For analysis, they source 100 basic drawing questions for 5 scientific fields from PhD students and 220 caption-image pairs from SridBench. The 100 questions are proposed together with 5 multiple choice questions that can be answered from correct images. For the 220 images, the questions and answer options are generated by GPT5. Then, GPT5 is asked to answer the multiple choice questions based on the generated images. The ratio of correctly answered questions is reported as accuracy. Another metric is the ratio of images, where at least on question could be answered. They find that their pipeline provides improvements over plain prompting and open-source models."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This is a very interesting and useful research topic for our community, as the successful image generation could save researchers and authors a lot of time.\n\n- The paper provides many details on how their reported metrics will change if components of their pipeline will drop or are exchanged."}, "weaknesses": {"value": "- I see the main weakness of the paper in using a rather unverified metric: asking GPT5 if questions (in >60% generated by GPT5) can be answered correctly based on the image. There is no human analysis whether GPT5 is able to perform this task correctly. Further, it is unclear, how the questions are distributed into correct and incorrect answers and whether the questions could also be answered without the image. The nature of the questions is also unclear, because the paper does not provide any examples. If the questions instead are of the nature \"does this image show X?\", the problem lies in the ability of GPT5 to check fine-grained image details. For example, the MAIG images that the paper presents in Figure 3 contain much more text, which might coax GPT5 into saying that content is actually represented. For example, for Meiosis, while the phases mostly follow the reference image, the things that are happening to the chromosomes inside the cells are largely without sense. The tree in Figure 5 has a lot of stray text like \"water absorption through roots\" at the top of the stem. DNA Ligase in Figure 6 is shown outside of the strand. In other words, while the images perhaps seem more correct in terms of visual similarity, important small details are missed, which could cause issues if the images were to be used in teaching, etc.. I suggest to (1) perform a human evaluation of some images, to see if the human accuracy matches that given by GPT5 and (2) to explore how biased the model is towards text shown in the images.\n\n- While there is no human evaluation, the QA-based metric is also not compared to other potential metrics, like VQAScore with GPT backend. \n\n- On another note, ratings might be biased if images are generated and rated by GPT models. \n\n- Generally, it seems a bit unsurprising that a pipeline that is using much more resources on checking itself performs better. It would be interesting to contrast this with the costs of running the pipeline.\n\n- I don't see the use-case for generating textbook illustrations without any changes. Given the power of current search engines, it would be much easier, to just find a matching image (perhaps also with an LLM). Also, it is unclear, to which degree the LLM will just reproduce memorized graphics\n\n- As you describe, there are many works that consider Scientific Figure Generation. Perhaps, it would be interesting to compare how your pipeline will perform on these.\n \n**Small things:**\n- In line 456, you suddenly write GPT-o3\n- I would suggest to use a different format for the variable names, as the text appears quite spaced out"}, "questions": {"value": "- Tying in with the weaknesses, my question would be the same: Can you provide some examples of image - question pairs that were graded with GPT-5 as example?\n\n- Also, do these questions have correct and incorrect options? If the latter is the case you should probably report the F1 score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mFxsg6g2O5", "forum": "0ZRne2Nt8t", "replyto": "0ZRne2Nt8t", "signatures": ["ICLR.cc/2026/Conference/Submission25589/Reviewer_vR7j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25589/Reviewer_vR7j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825372429, "cdate": 1761825372429, "tmdate": 1762943487054, "mdate": 1762943487054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}