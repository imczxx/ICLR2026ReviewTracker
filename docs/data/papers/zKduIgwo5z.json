{"id": "zKduIgwo5z", "number": 19628, "cdate": 1758297782282, "mdate": 1759897029529, "content": {"title": "Stable Basis Deep Neural Policy Training", "abstract": "Following the initial success of deep reinforcement learning in learning policies just by interacting with complex, high-dimensional representations, and a decade of significant research, deep neural policies have been applied to a striking variety of fields ranging from pharmaceuticals to foundation models. Yet one of the strongest assumptions in reinforcement learning is that a reward signal will be available in the MDP. While this assumption comes in handy in certain fields, such as automated financial markets, it does not naturally fit in many others where the computational complexity of providing such a signal for the task at hand is greater than the complexity of learning one. In this paper we focus on learning policies in MDPs without this assumption, and study sequential decision making without having access to information on rewards provided by the MDP. We introduce a training method in high-dimensional MDPs and provide a theoretically well-founded algorithm that significantly improves the sample complexity of deep neural policies. The theoretical and empirical analysis reported in our paper demonstrates that our method achieves substantial improvements in sample efficient training while constructing more stable and resilient policies that can generalize to uncertain environments.", "tldr": "", "keywords": ["stable basis training", "deep neural policies"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97f7e943f4dbf7fb7aef7cb997e59562fe6687bf.pdf", "supplementary_material": "/attachment/114670021c3c80f9ccebb984381bab69026cefd0.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a new reinforcement learning framework called Harmonic Learning, which enables sample-efficient and robust policy training in high-dimensional Markov Decision Processes (MDPs) . Training occurs without access to explicit reward signals. The key contribution is in defining a stable harmonic basis of the state representation, and introducing a noise-injection procedure that improves both learning efficiency and policy robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Interesting spectral perspective on policy learning: The paper introduces a novel spectral viewpoint for policy training by leveraging harmonic or Fourier basis perturbations. While the theoretical underpinnings are partly heuristic, the idea of connecting uncertainty quantification in reinforcement learning to harmonic analysis is conceptually appealing and provides a fresh way to study robustness in high-dimensional MDPs.\n\n2. Empirical consistency and sample-efficiency improvement: The experiments, conducted across several Atari environments demonstrate that the proposed method can achieve comparable or superior performance with significantly fewer interactions (reported as up to 20× sample-efficiency gains in Tables 1–2 and §4 of the main paper."}, "weaknesses": {"value": "1. Presentation is lacking. Currently when I read the paper, in first reading it is not clear to me what exactly is being addressed in this work: is it training policies in absence of reward signals? Is it learning robust policies? or is it a reduced sample complexity? Several details are lacking, I have listed these in Questions section.\n\n2. No discussion, or intutition is provided as to why the algorithm is supposed to work. Also, no comparison is provided with previous existing works. I was expecting some \"sample complexity type result\" which would quantify the improvements, but this is lacking.\n\n3. Theoretical framework lacks formal convergence guarantees: While the paper connects harmonic learning to randomized least-squares value iteration (RLSVI), this connection is conceptual rather than rigorous. No formal convergence theorem or regret bound is provided for the proposed algorithm beyond analogy to RLSVI. Even these derivations are relatively straightforward, and not surprising.\n\n4. Lack of formal theoretical link between harmonic perturbations and robustness (Main paper 3.1–4.1, lines 270–323 & 370–420; Supplement 6, lines 260–310): While the paper claims that removing Fourier components during training induces robustness through the uncertainty principle, this connection remains conceptual and empirically motivated rather than theoretically substantiated. In the main paper (Sections 3.1 and 4.1), robustness is justified by analogy to harmonic analysis. Loose argument is made that a policy insensitive to frequency-specific perturbations is inherently robust. However, no formal analysis is given connecting the removal of spectral components to quantifiable robustness metrics such as Lipschitz smoothness, stability under distributional shift, or bounded change in Q-values. Evidence relies entirely on empirical visualizations (Figures 2, 3) and heuristic reasoning. The supplementary material ( Section 6, \"Theoretical Basis and Empirical Analysis\", lines 260-310) reiterates the same argument; that \"removal of one element of the Fourier basis is spread out in the function without any semantic changes to the natural image\". It then infers that this property implies robustness. But the supplement still provides no formal robustness guarantee, bound, or theorem linking harmonic perturbations to policy resilience. The discussion equates empirical stability of scores under spectral deletions (SBRA) with theoretical robustness, without proving this correspondence."}, "questions": {"value": "1. line 92: should it be \\theta_t in r.h.s.?\n\n2. \"Despite recent progress, a fundamental assumption persists in reinforcement learning: that the agent has\ndirect access to the reward function of the MDP.\"--> in RL assumption typically is knowledge of reward obtained i.e. r(s_t,a_t), and not the knowledge of the reward function itself r: S \\times A \\mapsto R.\n\n3. Please explain this \"Orthogonal to these advances while the instabilities of deep neural networks under non-robust directions have been\na subject of discussion (Goodfellow et al., 2015), recent work demonstrated that these instabilities are currently also present in deep neural policies (Huang et al., 2017).\" What is meant by \"instability\" here? Also this \"Furthermore, more recent studies demonstrated that these non-robust directions can be semantically meaningful changes to the environment (Korkmaz, 2024).\"--> Its hard to understand for someone not working in this exact same field.\n\n4. \"robust decisions in unstable and non-robust environments?\" whats an unstable environment?\n\n5.  \"Harmonic learning achieves substantial sample-efficiency resulting in requiring up to 20× fewer samples while achieving better performance\"--> is this 20x improvement theoretically shown or empirical?\n\n6. \"spectrally analyze the robustness of deep neural policies\"--> more details can be provided (which theorem proves this? or what does this statement mean).\n\n7. \"to as an imitation and inverse reinforcement learning algorithm interchangeably\" --> why so, how is imitation related to inverse RL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UzmBFZF2SZ", "forum": "zKduIgwo5z", "replyto": "zKduIgwo5z", "signatures": ["ICLR.cc/2026/Conference/Submission19628/Reviewer_fqjL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19628/Reviewer_fqjL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896061859, "cdate": 1761896061859, "tmdate": 1762931478591, "mdate": 1762931478591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Observations in Reinforcement Learning (RL) environments such as the Arcade Learning Environment (ALE, Bellemare et al. 2013) are often image-based. This paper argues that a robust policy should not depend strongly on non-perceptual features of the observation. In particular, that the policy should be robust to the removal of any particular frequency component in Fourier space.\n\nIn the linear function approximation setting, they connect the removal of random basis vectors to Randomised Least Squares Value Iteration [(RLSVI; Osband et al. 2016)](https://arxiv.org/abs/1402.0635), and generalise this to the general function approximation setting by randomly masking certain Fourier components of each observation, before learning the value function using a standard RL algorithm. The resulting algorithm is dubbed Harmonic Learning.\n\nIn experiments on the ALE, Harmonic Learning improves on the baseline IQ-Learn [(Garg et al. 2021)](https://arxiv.org/abs/2106.12142) for inverse RL, resulting in improved sample efficiency (up to 50K env steps).\n\nFinally, they propose Stable Basis Robustness Analysis (SBRA), an analysis of the policy performance robustness when perturbing the Fourier components of the observation, as in Harmonic Learning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The viewpoint of this paper is rather original. The impact of image-based perturbations of the observations in Deep RL is a relatively understudied area, and this paper connects the random removal of basis vectors (in the linear function approximation setting) to Randomised Least Squares Value Iteration (RLSVI). The connection to RLSVI is interesting (however please see the related question)."}, "weaknesses": {"value": "**1) Positioning within existing literature**\n\nThe motivation and setting in the introduction — the utility of RL methods which do not have direct access to the reward –– appears to be mostly unrelated to the main work apart from the fact that the empirical results are built on top of an inverse RL algorithm.\n\nThe work would be better positioned directly in the context of robustness to perturbations of the image observations. In this context, there are several related works which consider the impact of perturbations of image inputs in the Fourier space on RL, in particular [(Huang et al. 2022)](https://proceedings.neurips.cc/paper_files/paper/2022/file/802a4350ca4fced76b13b8b320af1543-Paper-Conference.pdf), which introduces \"Spectrum Random Masking\", which also modifies the observations by masking out components in the Fourier domain. Also see [(Lee et al. 2025)](https://ieeexplore.ieee.org/document/10833629).\n\n\n**2) Connection to RLSVI**\n\nPlease see the question in the later section about the connection to RLSVI in the general function approximation setting.\n\n**3) Empirical evaluation**\n\nThe comparison with Inverse Q-Learning are only reported at 50K environment interactions, while previous benchmarks report performance up to 1M. With the exception of Pong, the performance of Harmonic Learning is significantly lower than the final performance achieved by IQ-Learn (at 1M timesteps).\n\nTherefore, it is unclear if Harmonic Learning continues to learn and match (or exceed) that of e.g. IQ-Learn. While it is not necessary to outperform the state-of-the-art in every metric, understanding the limitations of the proposed method would be informative.\n\n**4) Stable Basis Robustness Analysis experiments**\n\nThe experiments conducted in Sec 4.1-4.2 propose the use of SBRA as a general diagnosis tool for overfitting and robustness in RL. However, the experiments compare a single baseline algorithm and its Harmonic Learning counterpart. To establish a correlation that generalises beyond this specific pair of algorithms, it would be necessary to compare several inverse RL algorithms and their Harmonic Learning counterpart.\n\nIn addition, for the state-action values experiment (Fig 4), it is difficult to draw conclusions by comparing the Q-value estimates, without access to the true Q-values at those states. The true Q-values can be estimated via Monte Carlo rollouts (see e.g. [(Chen et al. 2021)](https://arxiv.org/abs/2101.05982) and references within)."}, "questions": {"value": "1. Is seems that it is possible to apply Harmonic Learning to more general standard RL setup, beyond Inverse RL. Did the authors also try this, or is there some reason that Harmonic Learning cannot be applied to the typical RL setup?\n\n2. RLSVI [(Osband et al. 2016)](https://arxiv.org/abs/1402.0635) samples over a posterior distribution over value functions. In contrast, Harmonic Learning samples from a distribution of states. Proposition 3.3 makes the connection with RLSVI explicit for the linear function approximation setting but there does not appear to be an analogous proposition for the general function approximation setting. It would be appreciated if the authors could clarify this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DEu24gmieU", "forum": "zKduIgwo5z", "replyto": "zKduIgwo5z", "signatures": ["ICLR.cc/2026/Conference/Submission19628/Reviewer_GMNU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19628/Reviewer_GMNU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949975673, "cdate": 1761949975673, "tmdate": 1762931477852, "mdate": 1762931477852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces *harmonic learning* in the context of learning policies in reinforcement learning problems without access to a reward signal, analogous to the inverse RL or imitation learning literature. Using spectral analysis, the authors propose a theoretically sound method for robust Q-learning that can be readily used on top of inverse Q-learning, a current state-of-the-art method for inverse RL/imitation learning, to provide a more sample efficient and robust algorithm for learning RL policies without reward signals."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The authors propose a novel add-on applied to inverse Q-learning that stabilizes training, and greatly improves the sample efficiency of existing methods.\n- Their method is well founded theoretically, and the authors provide empirical results that both validate their method based on performance, and analyze how and why harmonic learning improves performance."}, "weaknesses": {"value": "- The experiments are limited to the a handful of games in the Arcade learning environment. It could be nice to have additional experiments in robotics physics based simulators such as Mujoco to better understand whether the improvements of harmonic learning also extend to low-dimensional observation spaces as well. \n-  The use of figures and tables in the main text is sometimes not very efficient and unclear as to how it relates to the main text. For example, it is unclear what conclusion is supposed to be drawn from Figures 1 and 3. Table 1 is not referenced in the main text. The final performances achieved from their harmonic learning method is repeated in Table 1, Table 2 and Table 3."}, "questions": {"value": "- Since harmonic learning is modular, can it be used for any Q-learning algorithm? Why is this work limited to the inverse RL problem? If an advantage of the method is that it is modular, could you also apply the method on top of other methods such as BC, GAIL, SQIL, and vDice and see similar improvements?\n- One of the main claims of the work is that harmonic learning is much more sample efficient. Claims about sample efficiency can be visualized through learning curves, it would be nice for the authors to include some to further illustrate this point\n- The behavior cloning results in Table 1 show no standard errors, except for the breakout environment, where the result is 0. +/- 0. Did something happen with the experiments for BC? Did it fail completely? If so, why?\n- Do the results in Figure 4 report a single seed? If so,  it is not clear from a single seed whether claims about value overestimation can be made from this plot alone. If this plot shows an aggregation over multiple seeds, it would be beneficial to add some statistics about the variance as a shaded area."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OPM3lvQgE9", "forum": "zKduIgwo5z", "replyto": "zKduIgwo5z", "signatures": ["ICLR.cc/2026/Conference/Submission19628/Reviewer_ybKU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19628/Reviewer_ybKU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977582154, "cdate": 1761977582154, "tmdate": 1762931477474, "mdate": 1762931477474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}