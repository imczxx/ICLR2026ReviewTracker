{"id": "CwownEMv9z", "number": 9947, "cdate": 1758151766463, "mdate": 1763757856065, "content": {"title": "Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs", "abstract": "The classical policy gradient method is the theoretical and conceptual foundation of modern policy-based reinforcement learning (RL) algorithms. Most rigorous analyses of such methods, particularly those establishing convergence guarantees, assume a discount factor $\\gamma < 1$. In contrast, however, a recent line of work on policy-based RL for large language models uses the undiscounted total-reward setting with $\\gamma = 1$, rendering much of the existing theory inapplicable. In this paper, we provide analyses of the policy gradient method for undiscounted expected total-reward infinite-horizon MDPs based on two key insights: (i) the classification of the MDP states into recurrent and transient states is invariant over the set of policies that assign strictly positive probability to every action (as is typical in deep RL models employing a softmax output layer) and (ii) the classical state visitation measure (which may be ill-defined when $\\gamma = 1$) can be replaced with a new object that we call the transient visitation measure.", "tldr": "", "keywords": ["undiscoutned expected total-reward infinite horizon MDPs", "visitation measure", "recurrent and transient states", "policy gradient theorem", "natural policy gradient", "projected policy gradient", "convergence analysis"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a56541dd50dbcd1279a9bc81831d35f7107e1e0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper considers the infinite-horizon and undiscounted Markov Decision Processes and studies the convergence of policy gradient theorem. It starts by first constructing a Pathological MDP and points out two discrepancies: the discontinuity of the value functions and the non-optimality of policies. The paper then introduces the definition of transient and recurrent states and divide the state space into two corresponding parts. Based on this, the paper introduces the transient visitation measure and the associated Transient Policy Gradient, followed by the convergence analysis and empirical results."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally understandable and well written. The proposed problem of convergence analysis on policy gradient theorem is interesting when the policy gradients are applied to train large language models. The analysis is complete as it covers both theoretical results and empirical evaluation."}, "weaknesses": {"value": "The paper in its current version has several major weaknesses that affects my assessment: 1. There are quite some vague (appears contradictory or even wrong) analysis and unstated assumptions that significantly downplays the potential significance of the results; 2. The empirical results appear not to reflect or corroborate the theoretical results and analysis.   \n\n### Confusing (and contradictory) analysis\n1. In pathology 1, the discussion of discontinuity is confusing. “the optimal action at state $s_1$ is to remain at $s_1$”: (it can be true if the optimal policy for state $s_1$ is restricted to the set of deterministic policies $\\Pi/\\Pi_+$) then “any policy assigning a non-zero probability to the other action” (now the set of stochastic policies $\\Pi_+$ is considered). It’s not clear if the discontinuity discussion is valid since two different sets are considered here. Importantly, if the discussions are narrowed to the set of stochastic policies, any stochastic policy at state $s_1$ would be optimal then. \n\n2. In pathology 1, “a policy gradient method cannot be expected to succeed in the presence of such discontinuities”: the policy gradient method was proposed specifically to circumvent the discontinuities between deterministic policies and value functions, and hence to optimize stochastic policies only, see [Policy Gradient Methods for\nReinforcement Learning with Function Approximation]( https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)\n\n3. In pathology 2, “the optimal action at $s_2$ is to transition to $s_4$”: this might be wrong. Particularly, according to the analysis in Pathology 1, “any policy assigning a non-zero probability to the other action” would yield the same value and thus any stochastic policies $\\Pi_+$ are optimal. But the follow-up discussion then restricts the policy set to be deterministic: $\\pi(s)\\in\\arg\\max Q^*(s, a)$. Evidently, a deterministic policy can’t be an optimal policy if the optimal policy belongs to the stochastic set. One example the Rock-Paper-Scissors game: any deterministic policy wouldn’t be optimal if the opponent is playing uniformly randomly. \n\n4. Can authors discuss whether the transient visitation measure itself can be well-defined and under what types of Markov chains the Transient Policy Gradient should be applied upon? As the number of visits for these transient states is finite, compared to recurrent ones, the probability in Equation on line 261 can be all zeros. Consider the example in Figure 1, the transient visitation measure for state $s_2$ may not be defined: if the policy at state $s_2$ is to stay at $s_2$, then $s_2$ is recurrent; if the policy is to transition to $s_4$, then what’s the Probability of being at $s_2$? Furthermore, Transient Policy Gradient (Theorem 1) differs from the policy gradient theorem in that the state visitation measure has been replaced by the transient visitation measure. The original policy gradient theorem assumes the induced Markov chain is ergodic such that the state visitation measure is stationary (thus well-defined). But it’s unclear what Markov chain the Transient Policy Gradient is based upon. From reviewer’s perspective, it seems such transient policy gradient itself may be ill-defined.    \n\n5. The analysis of Natural Policy Gradient appears to repeat the existing results reported in Agarwal et al. (2021); Xiao (2022); Bhandari & Russo (2024); Mei et al. (2020), especially on the linear convergence with adaptive size. The only difference is the distribution mismatch coefficient. The closed form for the update rule does not provide any new insights. Further, the definition of modified Fisher information matrix can be problematic as $\\delta_\\mu^{\\pi}$ may not be a well-defined probability distribution.  \n\n### Empirical evaluation\n\nIt’s unclear if the two toy examples, Frozenlake and CliffWalk, were used as undiscounted total-reward infinite-horizon MDPs (how to implement this as infinite-horizon MDPs? Is there any time-out used in the training?) Moreover, these two examples may not even have the transient states as defined by the paper (or the paper should highlight some of the states are transient?) The experiments on Pathological MDP in the appendix seem not to truly reach the conclusion that “it is not subject to this issue” (Line 172). The Transient Policy Gradient still suffers from the same issue?"}, "questions": {"value": "1. Line 034, “arbitrarily long horizons”, both references are actually considering trajectories with finite horizons (or truncated long trajectories into segments)\n\n2. Line 043, “may ill-defined” --> “may be ill-defined”\n\n3. Line 107, Can authors elaborate on how “the monotone convergence theorem guarantees that each limit exists (possibly infinite)”? \n\n4. Line 132, the equation $Q^\\pi = PV^\\pi + r$ is not defined. \n\n5. Line 136, can authors put the Theorem 7.19 for reference? “an optimal policy always exist in the undiscounted total-reward setup with finite state and action spaces”. The reviewer couldn’t find the edition of Puterman 2014. The online available edition is Puterman is 1994, 2005 by John Wiley & Sons. \n\n6. Line 142, “$P^\\pi(s)$ is defined as…” there is no such $P^\\pi(s)$ defined (assume this is a typo);  Further, $P^\\pi(s\\rightarrow s’)$ is defined on a single sampled action $a\\sim\\pi$, not the expectation form? Also, $Pr$ is not defined, at least different from the transition matrix $P$. \n\n7. Line 143, can authors elaborate the formula $V^\\pi = \\sum_{n=0}^{\\infty} (P^\\pi)^n r^\\pi$? How to get this formula? \n\n8. Line 185, “$\\sum_{k=0}^{\\infty} (P^\\pi)^k(s, s) = \\infty$: $(s, s)$ is not defined. Not sure if it refers to $P^\\pi(s\\rightarrow s)$. \n\n9. Line 191, both $\\bar{S}^\\pi$ and $\\bar{S}^\\pi_n$ are not defined. \n\n10. Line 208, the “reward-to-go” is not defined (or introduced beforehand). \n\n11. Line 213, “$P^\\pi(s_1, s_2)$” is not defined (or it refers to $P^\\pi(s_1 \\rightarrow s_2)$?)\n\n12. Line 255, “the state visitation measure is defined as $d_{s_0}^\\pi(s) = (1-\\gamma)\\sum_{i=0}^{\\infty} (\\gamma P^\\pi)^I (s_0, s_i)$. The definition appears incorrect: 1. There is no $s$ on the right hand side; 2. There is no term of the initial state distribution $\\mu$\n\n13. Line 257, “In the undiscounted setting with $\\gamma=1$, this object becomes undefined”. Even for undiscounted case, the state visitation measure may still be defined: check Eq. (8) in [Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation]( https://papers.nips.cc/paper_files/paper/2018/hash/dda04f9d634145a9c68d5dfe53b21272-Abstract.html)\n\n14. Line 303, $\\eta_k$ is not defined. \n\n15. Line 370, $Q^{\\pi_n}$ is not defined (or a typo?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r8zpWr66WS", "forum": "CwownEMv9z", "replyto": "CwownEMv9z", "signatures": ["ICLR.cc/2026/Conference/Submission9947/Reviewer_LNyM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9947/Reviewer_LNyM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744201414, "cdate": 1761744201414, "tmdate": 1762921394226, "mdate": 1762921394226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the convergence of policy gradient methods in the undiscounted total-reward infinite-horizon setting. Recent applications including RLHF and RLVR for LLMs often use undiscounted total reward settings, motivating theoretical foundations in this regime. The paper introduces a transient visitation measure to replace the standard discounted visitation measure and uses a recurrent-transient decomposition of the MDP to derive convergence results for projected and natural policy gradient algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper develops a rigorous convergence analysis for policy gradient methods in an underexplored setting, providing a new transient policy gradient theorem\n- The recurrent–transient decomposition is carefully applied, leading to clean lemmas (Lemma 2, Lemma 5) and new insight into continuity properties of the value function\n- Clear presentation in terms of the gradual development from Section 3 to Section 5."}, "weaknesses": {"value": "- Assumption 1 (finite total reward criterion) is quite strong and misleading in its current form of presentation. Assumption 1 enforces finiteness of value functions for all, which implies all recurrent states must have zero reward (Lemma 1). This effectively removes any interesting behavior in recurrent states and turns the problem into one over transient MDPs. This should not be buried as an assumption as it fundamentally alters the nature of the problem. The title and abstract should make this restriction explicit.\n- I believe the paper has limited technical novelty because once the transient-state formulation and restrictions are in place, the resulting analysis mainly requires continuity and contraction properties and the rest of the proof mirrors existing proof techniques. The “transient visitation measure” is essentially an adaptation of the standard visitation measure to a subspace.\n- If transient states are visited only finitely often, then it is unclear how one can meaningfully learn about them from sample-based estimates. The current theoretical setup assumes full knowledge of transitions and gradients, but practical implications for sample-based or online algorithms remain entirely open."}, "questions": {"value": "Suggestions for Clarity:\n- The paper motivates its setting using language modeling and RLHF, claiming these are undiscounted total-reward infinite-horizon problems. However, in practice, both are finite-horizon due to maximum token limits. The authors should clarify why it makes sense to model such tasks as infinite-horizon.\n- The paper contrasts its setting with discounted MDPs but does not sufficiently discuss its relationship to average-reward (multi-chain) MDPs and the literature therein (Putterman, 1994), which are the more natural analogues of undiscounted problems. The distinctions between total and average reward formulations should be made explicit."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1Y7zD46JZn", "forum": "CwownEMv9z", "replyto": "CwownEMv9z", "signatures": ["ICLR.cc/2026/Conference/Submission9947/Reviewer_aaNs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9947/Reviewer_aaNs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787532853, "cdate": 1761787532853, "tmdate": 1762921393980, "mdate": 1762921393980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies policy gradient methods in the undiscounted infinite-horizon setting. However, the authors overlook the extensive prior literature on average-cost MDPs, where this problem has been thoroughly analyzed since the 1990s. Conceptually, the formulation is fundamentally flawed: the paper considers an undiscounted total-reward criterion over an infinite horizon without any normalization or averaging, which leads to divergence unless the rewards are zero. The ad hoc assumption that both the positive and negative parts of the value function remain finite does not resolve this inconsistency. Beyond these foundational issues, the paper offers little novelty or insight, and the technical assumptions undermine its validity. Overall, the contribution is not meaningful in its current form, and I recommend rejection."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "None."}, "weaknesses": {"value": "- The paper overlooks the extensive literature on average-cost MDPs, where the undiscounted infinite-horizon problem has already been analyzed in depth from multiple theoretical perspectives.\n\n- The assumption that the value function remains finite without discounting or averaging is unjustified and mathematically inconsistent under standard reward structures. The analysis ignores the fundamental role of discounting or averaging in ensuring convergence of value functions, leaving the proposed framework theoretically unsupported."}, "questions": {"value": "Have you considered that Assumption 1 can only hold if the reward function is identically zero within the recurrent class?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WE5IxW2aNt", "forum": "CwownEMv9z", "replyto": "CwownEMv9z", "signatures": ["ICLR.cc/2026/Conference/Submission9947/Reviewer_F2er"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9947/Reviewer_F2er"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937803550, "cdate": 1761937803550, "tmdate": 1762921393667, "mdate": 1762921393667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies policy gradient methods for undiscounted total reward infinite-horizon MDPs. After introducing the setting, a few differences with the discounted setting are discussed in section 3. Then, section 4 established a policy gradient theorem using some insights from the theory of Markov chains, introducing in particular a transient visitation measure.  It is also shown that the recurrent-transient classification of states is independent of the choice of the policy (in the set of policies with full support). Section 5 provides a sublinear ($1/k$ order) convergence rate for projected policy gradient descent when the projection is over a set of full support policies (controlled by a constant $\\alpha$). Proofs rely on showing smoothness of the value function combined with a gradient domination property similarly to the undiscounted setting. Finally, section 6 analyzes natural policy gradient with a constant or an adaptive step size showing sublinear and linear rates respectively. A simple toy experiment concludes the paper by illustrating the convergence behavior of the different proposed methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is nicely written and to the point. The presentation is clear and the paper is well organized, making the paper easy to read (at least for someone familiar with the theory of policy gradient methods). I enjoyed reading it. \n2. The contributions are solid and novelty is clear, building on a few insights from the fundamental theory of (finite state) Markov chains (recurrence and transience). The simplification of Lemma 1, 2 which also leads to the policy gradient theorem is a nice insight (for undiscounted infinite-horizon MDPs). The results are sound, I skimmed through the proofs which look correct to the best of my knowledge (not line by line though for some of them), they mostly follow similar lines as prior work. \n3. The literature on the theory of policy gradient methods for discounted infinite-horizon MDPs has witnessed a lot of developments in the last few years. However, less is known for undiscounted infinite horizon settings. This paper provides a nice and solid addition to the literature, establishing similar results as the discounted setting with some nuances and caveats.\n4. The relevance of the paper is further supported by the fact that in practice, discounting is not always used (probably even seldom used) as mentioned in the paper. I believe this is important and will open the door to further developments. This also complements a recent trend in the development of guarantees beyond the discounted settings, e.g. last year at the same conference with the study of policy gradient methods for the average reward setting."}, "weaknesses": {"value": "1. **Technical novelty**: the development of the results mainly follows existent techniques (Xiao 2022 in particular) with a few adaptations using the theory of Markov chains (for finite states) which are fairly straightforward (as the proofs p. 13 show for instance). Overall, I think that technical novelty is limited but this is not a major weakness in my opinion as there are some interesting distinctions with the discounted setting and the analysis is well executed.  \n\n2. **Corollary 1**: $\\alpha$ depends on $\\epsilon$. It is not clear how the constant $C_{\\alpha}$ behaves as a function of $\\epsilon$. Hence it is unclear if the ‘convergence rate is sublinear’ as stated in l. 348 in this setting.  \n\n3. **Policy gradient theorem:** It is not clear if the policy gradient theorem is operationalizable: can we actually compute stochastic policy gradients using the provided formula in a different way than usual? Is the transient kernel $T$ actually accessible? Is there a way to write the same theorem using trajectories like in the discounted setting? Or can one just also write the same policy gradient as usual for sampling and the theorem mainly serves as a tool for analysis. This somehow relates to the stochastic setting, but still I think it would be useful to comment on that since (a) the value of the policy gradient theorem is usually (partly) in the stochastic policy gradient computation it can unlock and (b) the paper does talk about policy gradient algorithms, so a word about their implementation would be welcome. \n\n4. **Theorem 4**: I think it would be useful to comment on cases where the convergence rate is truly linear, i.e. cases where the distribution mismatch between the initial distribution and the visitation measure (at an optimal policy) is strictly larger than 1 (it might almost always be given the definition of the visitation measure). Giving more intuition on what makes convergence faster  would be useful since the theorem somehow provides an instance dependent rate (different from the other results in the paper).\n\n**Typos:** \n- l. 54: ‘stochastic’ capital letter. \n- l. 97: $\\pi(a|s)$ instead of $\\pi(s|a)$. \n- l. 131: $P^{\\pi}$ instead of $P$? \n- l. 191: the matrix $S$ is not defined. \n- l. 286: $\\pi_{\\theta}$ instead of $\\pi$ under the expectation. \n- l. 309: stp, setup \n- l. 357: $\\theta_{s,a’}$ in the denominator. \n- l. 361: ‘, The’, lower case. \n- l. 370: $\\pi_i$ for the Q-function term. \n- Proof lemma 5, some undesirable commas remaining. \n- l. 731: 'exsit', exist. \n- l. 742, 743: $\\pi_{\\Pi}$?, 'diffretiable'."}, "questions": {"value": "1. l. 157-161: can you highlight better the contrast with the discounted setting in terms of pathology? \n2. l. 335: Can you justify the limit when $\\alpha \\to 0$? Does the constant $C_{\\alpha}$ remain controlled as a function of $\\alpha$ when $\\alpha \\to 0$?\n3. l. 397: $V_{+,\\mu}^{\\star}$ is defined as a supremum. Is it attained at some policy? How do you obtain this limit statement? Do you achieve that the value is achieved at some policy $\\pi^{\\star}$ and plug in that policy in l. 395 using $\\pi=\\pi^{\\star}$? \n4. It is known in the discounted setting that NPG approximates Policy Iteration (PI) which is known to converge linearly using $\\gamma$-contraction (as discussed e.g. in Xiao 2022). Is there any such meaningful link to draw here in the undiscounted setting with PI (without $\\gamma$ contraction), for instance to interpret/discuss Theorem 4?   \n5. Corollary 2 (minor): Is there a missing norm of $V^{\\pi_0}$ term in the bound since this is a corollary of Theorem 3?\n6. Proof of Lemma 3, l. 671 (minor): Why does fixed transient class (invariance to policy) imply continuity of $T^{\\pi}$? I guess it is useful to stress that the form of the matrix will stay the same but for continuity not sure it is enough, can't we just say that $P^{\\pi}$ is continuous in $\\pi$ (as it is linear in $\\pi$) and then $\\bar{T}^{\\pi}$ is a submatrix of $P^{\\pi}$ hence also continuous (as a fixed projection by the invariance shown)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8mwPuDKokX", "forum": "CwownEMv9z", "replyto": "CwownEMv9z", "signatures": ["ICLR.cc/2026/Conference/Submission9947/Reviewer_2pSR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9947/Reviewer_2pSR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986721616, "cdate": 1761986721616, "tmdate": 1762921393232, "mdate": 1762921393232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response"}, "comment": {"value": "First of all, we thank the reviewers for their detailed and thoughtful feedback. The reviewers generally agree on the importance of the problem and the novelty of our core insight, while noting that the analyses in Sections 5 and 6 closely mirror proof strategies from prior work in the discounted-rewards setting.\n\nIndeed, we show that by decomposing the states into recurrent and transient states and introducing the transient visitation measure, techniques developed for the discounted setting can be carried over to the undiscounted total-reward setup. We believe this is a valuable insight.\n\nIn our view, the fact that our novel reformulation allows proof techniques from a different setup to transfer cleanly into this one **is a strength, not a weakness**. Such reformulations make connections between subareas, and these connections are, we argue, more valuable than entirely separate analyses based on new and disjoint techniques.\n\nOur intent was to present this main point in clear, to-the-point writing. We respectfully ask the reviewers to assess our work based on the value this core insight provides."}}, "id": "XuNJxs5dmN", "forum": "CwownEMv9z", "replyto": "CwownEMv9z", "signatures": ["ICLR.cc/2026/Conference/Submission9947/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9947/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9947/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728180310, "cdate": 1763728180310, "tmdate": 1763728180310, "mdate": 1763728180310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers developing a policy gradient algorithm for total reward MDPs. The entire formulation and study is in the context of planning (ie no learning). Previous contexts of total reward MDPs study bounded non-negative, or negative value functions, or the stochastic shortest path problem where as the current paper considers all MDPs with a bounded value function (corresponding to both negative and non negative rewards). \n\nThe core intuition lies in identifying the role of transition states. Since the assumption of bounded value functions necessarily imply all recurrent states to have a reward of 0, the analysis boils down to studying the behavior of transient states. The authors heavily leverage the finiteness of states and actions in their analysis and the intuition that under policies that have nonzero measure on all actions have the same characterization of transient and recurrent states. \n\nThe authors then proceed to analyze global convergence of policy gradient (and provide sublinear rates), natural policy gradient (NPG) (provide a sublinear rate) and NPG with adaptive step size (provide a linear rate). They further conduct simulations to verify the theory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem studied is of importance but has not received much attention due to some pathological conditions (such as lack of continuity of value function wrt policy and existence of solutions to the bellman equation). The authors provide a principled approach to this and analyze standard policy gradient algorithms in this setting. \n\nThe paper is over well written and the core intuition behind their analysis is easy to follow."}, "weaknesses": {"value": "The technical novelty is not entirely clear. The analysis is pretty much exactly same as prior works except for considering the transient state probabilities instead of the entire probability transition matrix. \n\nThe policy gradient analysis seems moot since the NPG bounds seem to perform much better and have better convergence properties (no dependence on state and action spaces).\n\nThe results would be a lot more compelling if the state and actions spaces are countable (In practice many systems are incredibly large so analyzing those systems without relying on finiteness of state action spaces would make for a very interesting problem)."}, "questions": {"value": "Some typos: line 54- Stochastic, line 97: $\\pi(a|s), line 309: step,  line 334 Theorem 1, \n\n1. Does zero reward for recurrent states imply finiteness of value function?\n\n2. In Lemma 5, isnt the $V^\\pi(s)=0$ for all recurrent states true for all policies under assumption 1?\n\n3. In corollary 1, what is the intuition behind that choice of $\\alpha$? \n\n4. Is the infinity norm in the finite rates in theorem only with respect to the transient states?(since $\\delta$ is defined only for transient states) \n\n5. How are equations in lines 363-366 related to the equation in line 370? \n\n6. Why do we need $\\mu$ to have a full support (over transient(?) states)? Shouldn't it suffice to start at some fixed transient state and the conditions for existence of solutions etc would still be met?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g9ldsReqZZ", "forum": "CwownEMv9z", "replyto": "CwownEMv9z", "signatures": ["ICLR.cc/2026/Conference/Submission9947/Reviewer_mXRp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9947/Reviewer_mXRp"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762733407938, "cdate": 1762733407938, "tmdate": 1762921392981, "mdate": 1762921392981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}