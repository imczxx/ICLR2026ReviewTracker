{"id": "mvLhN0veUd", "number": 14011, "cdate": 1758226847798, "mdate": 1763726599034, "content": {"title": "Breaking Barriers: Do Reinforcement Fine-tuning Gains Transfer To Unseen Domains?", "abstract": "Reinforcement post training (RPT) has recently shown promise in improving the reasoning abilities of large language models (LLMs).\nHowever, it remains unclear how well these improvements generalize to new domains, as prior work evaluates RPT models on data from the same domains used for fine-tuning. \nTo understand the generalizability of RPT, we conduct \ntwo studies.\n(1) Observational: we compare a wide range of open-weight RPT models against their corresponding base models across multiple domains, including both seen and unseen domains in their fine-tuning data. \n(2) Interventional: we fine-tune LLMs with RPT on \nsingle domains and evaluate their performance across multiple domains. Both studies \nconverge on the same conclusion that, although RPT brings substantial gains on \ntasks similar to the fine-tuning data, the gains generalize inconsistently \nand can vanish on domains with different reasoning patterns.", "tldr": "In this paper, we systematically investigate the generalizability of reinforcement fine-tuning.", "keywords": ["large language models", "reinforcement learning", "supervised fine-tuning", "generalizability"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/148837057d0357ca75c734ecb3a108f35a9ecee6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper looks at how reinforcement learning post-training generalizes to out-of-domain evaluations. The authors curate various models on huggingface trained with RL, and train their own models on specific domains. They find that RL post-training does not seem to generalize to arbitrary unseen domains, but does show generalization between math and code domains, which the authors hypothesise is due to similar reasoning templates being applicable in the two domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A wide range of models and evaluations are studied, making the findings seem fairly robust (especially with the intervention experiments).\n- The evaluation itself is well-performed, using appropriate metrics and statistical testing.\n- Paper is clear and reasonably structured, and the findings appear useful (especially that knowledge-focused tasks do not seem to transfer well to/from math and code settings)."}, "weaknesses": {"value": "- It appears that the open source models do not uniformly do better IID than OOD, for example, model 4 (Eurus-Prime) does 15 points better OOD than IID! Do you have explanations for this beyond ‘differences in implementation details’? It would be useful to have some idea of why there is this variance between models - the ID-OOD gap has a standard deviation of ~18 in table 2! I believe the trends described hold, but it would be good to have some idea of why there is this variance across models.\n- It seems that knowledge-RPT drops performance on the knowledge tasks - could this be more due to a domain mismatch between the knowledge-RPT data and evaluation tasks? The evaluations are very domain specific (medical QA, legal QA), while the training data is from multi-subject RLVR, which covers a much broader set of domains.\n- It would be useful to quantify/test the hypothesis in section 4.3 more thoroughly: if the reasoning templates are similar between code and math, could you examine some samples or measure overlap between reasoning chains to test this hypothesis? Looking only at downstream numbers does not fully explain what is happening. For example, it may be that only smaller subsets of the code data are similar to the math data, or that there is some cross-domain contamination between the two sets (e.g., code questions that require doing math, or math problems that require writing code).\n- For the intervention experiments, I’d be curious to see if the base model is a potential confounder. The deepseek distil model used has been extensively trained on math data, so it may be that this makes it less easy to adapt to knowledge tasks, or better primed to improve math performance when trained on code data.\n\nOverall, I think this is a solid paper, although its scope is somewhat limited. It would be useful to get more justifications around the knowledge-RPT setting and some discussion around the variance in the observational results."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ukzy12osuf", "forum": "mvLhN0veUd", "replyto": "mvLhN0veUd", "signatures": ["ICLR.cc/2026/Conference/Submission14011/Reviewer_FGBZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14011/Reviewer_FGBZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667083907, "cdate": 1761667083907, "tmdate": 1762924506594, "mdate": 1762924506594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper looks at how reinforcement learning post-training generalizes to out-of-domain evaluations. The authors curate various models on huggingface trained with RL, and train their own models on specific domains. They find that RL post-training does not seem to generalize to arbitrary unseen domains, but does show generalization between math and code domains, which the authors hypothesise is due to similar reasoning templates being applicable in the two domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A wide range of models and evaluations are studied, making the findings seem fairly robust (especially with the intervention experiments).\n- The evaluation itself is well-performed, using appropriate metrics and statistical testing.\n- Paper is clear and reasonably structured, and the findings appear useful (especially that knowledge-focused tasks do not seem to transfer well to/from math and code settings)."}, "weaknesses": {"value": "- It appears that the open source models do not uniformly do better IID than OOD, for example, model 4 (Eurus-Prime) does 15 points better OOD than IID! Do you have explanations for this beyond ‘differences in implementation details’? It would be useful to have some idea of why there is this variance between models - the ID-OOD gap has a standard deviation of ~18 in table 2! I believe the trends described hold, but it would be good to have some idea of why there is this variance across models.\n- It seems that knowledge-RPT drops performance on the knowledge tasks - could this be more due to a domain mismatch between the knowledge-RPT data and evaluation tasks? The evaluations are very domain specific (medical QA, legal QA), while the training data is from multi-subject RLVR, which covers a much broader set of domains.\n- It would be useful to quantify/test the hypothesis in section 4.3 more thoroughly: if the reasoning templates are similar between code and math, could you examine some samples or measure overlap between reasoning chains to test this hypothesis? Looking only at downstream numbers does not fully explain what is happening. For example, it may be that only smaller subsets of the code data are similar to the math data, or that there is some cross-domain contamination between the two sets (e.g., code questions that require doing math, or math problems that require writing code).\n- For the intervention experiments, I’d be curious to see if the base model is a potential confounder. The deepseek distil model used has been extensively trained on math data, so it may be that this makes it less easy to adapt to knowledge tasks, or better primed to improve math performance when trained on code data.\n\nOverall, I think this is a solid paper, although its scope is somewhat limited. It would be useful to get more justifications around the knowledge-RPT setting and some discussion around the variance in the observational results.\n\nEdit: the authors addressed my concerns above with fairly detailed additional new results and I am accordingly raising my score."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ukzy12osuf", "forum": "mvLhN0veUd", "replyto": "mvLhN0veUd", "signatures": ["ICLR.cc/2026/Conference/Submission14011/Reviewer_FGBZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14011/Reviewer_FGBZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667083907, "cdate": 1761667083907, "tmdate": 1763767259454, "mdate": 1763767259454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts an extensive study on publicly released models to understand the cross-domain skill transfer during reinforcement learning. They explore mathematical, code, and knowledge-intensive reasoning, evaluating how much performance improves from the base model when models are trained on data from different domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The results, while not incredibly surprising for those with substantial experience performing RL finetuning on language models, are quite valuable to see. The study is quite broad, only models with publicly available training data are included, and the experimental design is sound."}, "weaknesses": {"value": "It would be helpful to list the models that you tested, both for reproducibility and for clarity. One question I have is how diverse the *base* model pool was; e.g. were most models based on Qwen (which is quite strong on math and code already), or was there a diverse set of model families included in your study?\n\nIf possible, it would be very enlightening if there could be a further study on the *kinds* of reasoning each model uses, to see if there are explicit strategies common amongst them (so we can better understand what \"tools\" models need for e.g. math or code), but this is mostly out of scope of this paper."}, "questions": {"value": "When selecting domains, did you consider any others? If time allows, I think instruction following is a good verifiable domain to explore as well, and previous work has shown that models struggle to generalize to constraints beyond those they were trained on: https://arxiv.org/abs/2507.02833\n\nAlso, I'd recommend tweaking the citations in the first paragraph, right now there's essentially just a run on sentence of citations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MCpcLPCUWk", "forum": "mvLhN0veUd", "replyto": "mvLhN0veUd", "signatures": ["ICLR.cc/2026/Conference/Submission14011/Reviewer_8H9o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14011/Reviewer_8H9o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885235586, "cdate": 1761885235586, "tmdate": 1762924505758, "mdate": 1762924505758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines whether reasoning gains from RPT generalize beyond the training domains. Through both observational and controlled interventional studies across math, code, and knowledge-intensive tasks, the authors find that RPT improvements are domain-specific, effective within similar structured domains, eg. math and code, but failing to transfer to unstructured ones, e.g., legal, medical. The work highlights the limited cross-domain generalizability of current RPT approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important and timely question about whether reasoning improvements from reinforcement post-training can truly generalize beyond the training domain.\n- The study design is comprehensive and convincing, combining large-scale observational analysis of public RPT models with controlled interventional experiments under unified settings.\n- The experiments are extensive and well-documented, covering 16 diverse benchmarks across mathematics, code, and knowledge reasoning with appropriate statistical validation."}, "weaknesses": {"value": "- The experiments are conducted on relatively small models (up to 8B) with limited-scale RPT training, leaving it unclear whether the same generalization patterns would persist under larger LLMs.\n- The paper stops short of analyzing how different aspects of RPT training, such as reward signal quality or optimization dynamics, might contribute to the observed lack of cross-domain transfer, leaving the underlying cause somewhat underexplored.\n- The paper does not include any longitudinal or ablation analysis during training, which could reveal how generalization patterns evolve over time or collapse across domains.\n- The interventional experiments are all based on a single backbone DeepSeek-R1-Distill-Qwen-1.5B), so the conclusions are lacking in generality as the observed trends may depend on that model’s pre-training distribution."}, "questions": {"value": "- In the interventional experiments, were the three single-domain RPT models trained with identical reward functions or domain-specific ones? Clarifying such details could help interpret whether the observed generalization gaps stem from reward differences or reasoning differences.\n- Could the authors comment on whether a mixed-domain RPT training setting, e.g., combining math, code, and knowledge reasoning, might mitigate the observed specialization? This would help verify whether domain isolation itself causes the loss of generalization.\n- Have the authors considered analyzing intermediate checkpoints during RPT training to see if cross-domain performance degrades gradually or abruptly? Such temporal analysis might shed light on when specialization emerges."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IA2utca9hl", "forum": "mvLhN0veUd", "replyto": "mvLhN0veUd", "signatures": ["ICLR.cc/2026/Conference/Submission14011/Reviewer_TWrh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14011/Reviewer_TWrh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958735763, "cdate": 1761958735763, "tmdate": 1762924504461, "mdate": 1762924504461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an empirical study that assess to which degree reinforcement learning post-training enables generalizable improvement of reasoning across domains. It is structured into two parts, an observational and an interventional study."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The topic addresses a currently open important question for reasoning LLMs.\n\nCertainly a strength of the paper is its systematic and transparent setup (in particular for the selection of tested models) and statistical evaluation."}, "weaknesses": {"value": "A key weakness of the study is the focus on small models. While I understand the computational limitations. However, it seems reasonable that a certain model complexity might be required to actually generalize across domains. Therefore, it is not clear how the findings actually generalize to larger models that might in any case better suited for complex reasoning tasks.\n\nSimilarly, only one particular Reinforcement Learning process is tested for fine tuning with a single snapshot after one epoch. Here, it would be key to also see the development over multiple snapshots. With the current setup, one could hypothesize that generalization just sets in later. An evaluation would be interesting.\n\nThe paper overall is very sparse with the exact evaluation results for the different tests and models. I would expect that the paper reports on the detailed per task per model accuracies.\n\nThe used evaluation measure is fine. However, I think there is a large difference between an increase of accuracy from 60% to 61% or an improvement from 95% to 96%. In other words, the relative improvement is also important and should be addressed in a second measure."}, "questions": {"value": "The issues to be discussed in my opinion can be derived straightforward from the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x7J7rHR1oU", "forum": "mvLhN0veUd", "replyto": "mvLhN0veUd", "signatures": ["ICLR.cc/2026/Conference/Submission14011/Reviewer_172Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14011/Reviewer_172Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990237000, "cdate": 1761990237000, "tmdate": 1762924503931, "mdate": 1762924503931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}