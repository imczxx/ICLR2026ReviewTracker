{"id": "BRoAjhYWoQ", "number": 1360, "cdate": 1756875543239, "mdate": 1759898212766, "content": {"title": "Anime-Ready: Controllable 3D Anime Character Generation with Body-Aligned Component-Wise Garment Modeling", "abstract": "3D anime character generation has become increasingly important in digital entertainment, including animation production, virtual reality, gaming, and virtual influencers. Unlike realistic human modeling, anime-style characters require exaggerated proportions, stylized surface details, and artistically consistent garments, posing unique challenges for automated 3D generation. Previous approaches for 3D anime character generation often suffer from low mesh quality and blurry textures, and they typically do not provide corresponding skeletons, limiting their usability in animation. In this work, we present a novel framework for high-quality 3D anime character generation that overcomes these limitations by combining the expressive power of the Skinned Multi-Person Linear (SMPL) model with precise garment generation. Our approach extends the Anime-SMPL model to better capture the distinct features of anime characters, enabling unified skeleton generation and blendshape-based facial expression control. This results in fully animation-ready 3D characters with expressive faces, bodies, and garments. To complement the body model, we introduce a body-aligned component-wise garments generation pipeline (including hairstyles, upper garments, lower garments, and accessories), which models garments as structured components aligned with body geometry. Furthermore, our method produces high-quality skin and facial textures, as well as detailed garment textures, enhancing the visual fidelity of the generated characters. Experimental results demonstrate that our framework significantly outperforms baseline methods in terms of mesh quality, texture clarity, and garment-body alignment, making it suitable for a wide range of applications in anime content creation and interactive media.", "tldr": "", "keywords": ["3D anime character generation", "stylized body modeling", "component-wise garment generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f489db638804da10059a4773a4823a800068e6a.pdf", "supplementary_material": "/attachment/60cbf01edfea9119068a1958178812007f48248c.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a high-quality 3D anime character generation method. Overall, the proposed method is well-motivated, and the experimental results seems good."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tThe paper is well-written with a logical structure that makes the technical contributions easy to follow.\n•\tThe proposed  framework is reasonable and well-justified. The experimental results demonstrate the effectiveness of the approach.\n•\t The demo videos are excellent supplementary materials."}, "weaknesses": {"value": "•\tRecent works have explored video generation model-based avatar animation  capabilities. A more thorough comparison and discussion of the relationship between ANIME-READY and these methods would strengthen the paper. For example:\n•\tAnimate anyone performs avatar animation via cross-attention and  video generation module. How between ANIME-READY compare to this strategy?\n•\tWhat are the trade-offs between the SDS-loss approach HumanNorm[2], video generation based approach Animate Anyone and the proposed method? Could you clarify when your method is preferable over other menetioned existing techniques?\n•\tCould you please provide more details about the private dataset?\n\n\n1] Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation\n[2] HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation"}, "questions": {"value": "I wonder can we use the Rodin to perform 3D avatar generation and then perform auto-rigging like Mixamo. What is the comparison with the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R4vF3gcspM", "forum": "BRoAjhYWoQ", "replyto": "BRoAjhYWoQ", "signatures": ["ICLR.cc/2026/Conference/Submission1360/Reviewer_SGSx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1360/Reviewer_SGSx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749450414, "cdate": 1761749450414, "tmdate": 1762915747739, "mdate": 1762915747739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Anime-Ready, a framework for generating high-quality, controllable 3D anime characters from text or a single image. The key innovations include: (a) Anime-SMPL, a stylized extension of the SMPL body model that captures anime-specific proportions and provides unified UV maps for texture synthesis; (b) a Multi-Shape DiT with an MoE design for modular garment generation; (c) a body-aligned garment modeling scheme that encodes sampled body points to enforce spatial consistency and reduce interpenetration; (d) a component-wise high-resolution texture generation pipeline using multi-view diffusion and self-attention for disentangled texture synthesis.\nExperiments show improvements in mesh and texture quality over baselines, also supported by a user study."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The system design is very comprehensive, which integrates parametric body modeling, 3D diffusion, and texture synthesis, addressing multiple practical issues (alignment, rigging, texture bleeding).\n- The novel parametric model is interesting and meaningful. It combines realistic rigging consistency with stylized geometry, enabling animation readiness.\n- Applications such as retargeting and motion control demonstrate that the results are not merely visual but animation-ready."}, "weaknesses": {"value": "- Quantitative evaluation is limited. The reliance on user studies instead of geometric or perceptual metrics (e.g., FID-3D, CLIP-score, interpenetration rate) makes comparisons less reproducible.\n- The paper uses a private dataset of 20k characters, which may limit replicability. No evidence of generalization to unseen datasets is given.\n- Though not considered as a reason of not accepting the paper, the training requires multiple A100s for ~10 days—resource cost is high for an ICLR contribution. Efficiency or inference speed is not discussed. Also, other components (e.g., MoE expert routing, diffusion conditioning choices) lack separate ablations."}, "questions": {"value": "- How large is the parameter difference between Anime-SMPL and the original SMPL? Are blendshape parameters manually designed or learned?\n- Does the body-aligned garment generation guarantee no interpenetration, or are post-processing steps still required?\n- Could the authors release the Anime-SMPL model separately, even without the full dataset, to enable replication?\n- The supplementary video basically shows frontal results. How robust is the canonical-pose generation to unusual camera angles or occluded limbs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "L9un6s32nG", "forum": "BRoAjhYWoQ", "replyto": "BRoAjhYWoQ", "signatures": ["ICLR.cc/2026/Conference/Submission1360/Reviewer_M983"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1360/Reviewer_M983"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801577636, "cdate": 1761801577636, "tmdate": 1762915747557, "mdate": 1762915747557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified framework for generating high-quality, animation-ready 3D anime characters. The authors introduce a parametric anime body model (Anime-SMPL) and design a component-wise garment generation pipeline. The resulting models are partially rigged, skinned, and fully textured. The method outperforms prior work in terms of mesh quality, texture clarity, and garment-body alignment.\n\n**Contributions** \n- A novel parametric anime-style body model, Anime-SMPL, with full skeleton and blendshape rigging for animation.\n- A component-wise garment modeling pipeline that separately generates hairstyles, upper garments, lower garments, and accessories, each aligned to the underlying body.\n- A multi-view texture generation approach that operates per garment component to improve texture fidelity and modularity.\n- Experimental results showing improvements over baselines in mesh quality, garment-body alignment, and texture realism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper introduces a parametric anime body model, Anime-SMPL, learned from 20,000 anime-style 3D models. This contribution is valuable for future research on animatable anime character modeling. \n- A novel pipeline is proposed for generating separate garments and the underlying body, making the outputs easy for editing and good for application. \n- The texture generation approach is also innovative: instead of generating multi-view images of the entire body at once, the model generates multi-view textures for individual garments separately."}, "weaknesses": {"value": "- From my perspective, the output body is not fully skinned. Only the inner body is rigged and skinned, while body-hugging garments and accessories are bound to the inner body using nearest-neighbor (NN) skinning. The skirt is animated via physical simulation rather than skinning. \n- For tight-fitting garments, inheriting skinning weights via nearest-neighbor sampling introduces hard assignments, which can result in unnatural deformations—particularly in regions with complex articulation or discontinuous topology.\n- The garments are decodered by the VAE decoder, which are independent to the inner body parametric model. This might lead to garment-body intersections or poor fit between the garments and the underlying body. \n- No ablation studies are provided to evaluate the effectiveness of the MoE (Mixture-of-Experts) structure.\n- The paper provides very limited details on how the template Anime-SMPL model is generated, as well as its rigging parameters—including the blendshapes (expression, pose, and shape), joint regressor, and skinning weights."}, "questions": {"value": "**Rigging and Animation**\n- What is the dimensionality of the blendshape matrix used in SMPL-Anime? Does the model incorporate separate shape, expression, and pose blendshapes as in SMPL-X, or does it rely solely on shape blendshapes?\n- The paper would benefit from more details on how the Anime-SMPL template is constructed. Specifically, what is the process to obtain rigging components (e.g., blendshapes and skinning weights) obtained? In SMPL, an important factor for learning accurate rigging parameters is that the scans are captured from minimally clothed or tight-clothing subjects. Do you remove garments or otherwise preprocess the 3D anime models to isolate the underlying body before estimating rigging parameters?\n- How is the ground-truth $\\boldsymbol{\\beta}$ (shape) parameter estimated for each 3D anime model?  \n\n**MoE-structured DiT Block**\n\nIt is reasonable to model the four components—hairstyles, upper garments, lower garments, and accessories—separately using a MoE design. However, it would be better to provide ablation results comparing models w/ and w/o the MoE structure. Intuitively, MoE is effective when different experts are activated for distinct inputs or tasks. For example, using upper garment latent tokens would trigger the corresponding expert for upper garments. In this paper, however, both the input image and the noisy garment tokens represent the full body and all garments. As a result, the experts (E1, E2, E3, E4) may process the input holistically rather than specializing, making it unclear how much benefit the MoE structure actually provides."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hwZucjFl6y", "forum": "BRoAjhYWoQ", "replyto": "BRoAjhYWoQ", "signatures": ["ICLR.cc/2026/Conference/Submission1360/Reviewer_f3Tk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1360/Reviewer_f3Tk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833807623, "cdate": 1761833807623, "tmdate": 1762915747369, "mdate": 1762915747369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Anime-Ready, a novel framework for generating high-quality, controllable, and animatable 3D anime characters from text or a single image. The core contributions are: 1) Anime-SMPL, a new parametric body model adapted from SMPL to better represent the unique and exaggerated proportions of anime characters. 2) A body-aligned, component-wise garment generation pipeline. This pipeline decomposes the character into a body, hairstyle, upper garment, lower garment, and accessories. It uses a novel MoE-based Multi-Shape DiT to generate each garment component's mesh. Experimental results show the proposed method produces siginificant better results than the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **High-Quality Results:** The method demonstrates a significant improvement over existing baselines (CharacterGen, StdGEN, Hunyuan3D 2.0) in user studies. The qualitative results, especially for fine-grained details, are visually impressive.\n2. **Practical Applications:** The framework is designed for practical use. By generating a rigged (Anime-SMPL) body and separate garment components, it directly enables downstream tasks like garment retargeting."}, "weaknesses": {"value": "1. **Anime-SMPL Template Details:** \n    - How were the ground-truth shape parameters for the 20,000 characters obtained? Line 235 mentions training a shape prediction network using MSE against \"ground truth,\" but the origin of this ground truth is unclear.\n    - The paper argues that Anime-SMPL is different from SMPL, but provides few details. A visualization of the unified body template's joint structure is needed to understand its topology.  (e.g. number of joints)\n    - How does the unified Anime-SMPL model handle non-humanoid \"accessories\" like wings or tails? Are these part of the body model's joints? If so, how is the model \"unified\" when some characters have these features and others do not (or have multiples)?\n    - Furthermore, a supplemental figure demonstrating the effect of varying the shape (beta) components would be helpful.\n2. **Missing Ablation Studies:**\n    - **MoE-structured DiT:** A central claim is that the MoE-structured DiT (L250) achieves \"precise, component-aware generation\" with \"minimal parameter overhead.\" This claim requires ablation studies to be substantiated. I’m wondering how does this model compare against: (a) training separate, independent DiT models for each of the four components (hair, upper, lower, accessories); (b) a single DiT that does not use MoE, but is instead conditioned on a label token to differentiate the component to be generated.\n    - **MVAdapter \"Color Bleeding\" (L308-310):** The paper mentions that an initial attempt using MVAdapter directly resulted in \"color bleeding.\" A visual comparison showing this failure case should be provided.\n3. **Reproducibility:** The entire method, and especially the core Anime-SMPL model, relies on a large, private dataset of 20,000 characters. The authors do not state whether this dataset or the pre-trained Anime-SMPL model will be released. Without access to either, the results are not reproducible, which is a significant drawback for the research community."}, "questions": {"value": "1. How does the MoE approach ensure that the generated components (e.g., an upper and lower garment) are geometrically seamless? \n2. Hybrid Motion Control (L452): The hybrid approach for garment animation requires more detail. For simulation-driven garments: What physics simulation engine is used? How are the garment's physical parameters (e.g., mass, stiffness, friction) estimated from the image or text input? Are they simply hard-coded, and if so, how does this generalize?\n3. The paper motivates Anime-SMPL by stating that anime skeletons differ from real human ones. However, the \"Image-to-Image Synthesis\" (pose canonicalization) stage uses OpenPose (L200) to estimate a skeleton for conditioning. Since OpenPose is trained on human poses, doesn't this introduce a contradiction?\n4. In Fig 2, the input image for the character with a tail shows the tail is almost completely occluded. How is the model able to reconstruct it so accurately? Does this suggest potential overfitting to characters present in the training set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LlmtsTxOY4", "forum": "BRoAjhYWoQ", "replyto": "BRoAjhYWoQ", "signatures": ["ICLR.cc/2026/Conference/Submission1360/Reviewer_JKU9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1360/Reviewer_JKU9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912427917, "cdate": 1761912427917, "tmdate": 1762915747206, "mdate": 1762915747206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}