{"id": "uO2WfDYfeK", "number": 1258, "cdate": 1756867994012, "mdate": 1759898218607, "content": {"title": "ChartMaster: Advancing Chart-to-Code Generation with Real-World Charts and Chart Similarity Reinforcement Learning", "abstract": "The chart-to-code generation task requires MLLMs to convert chart images into executable code. This task faces two main challenges: limited data diversity and the difficulty of maintaining visual consistency between generated charts and the original ones. Existing datasets mainly rely on synthetic seed data to prompt GPT models for code generation, resulting in homogeneous samples that limit model generalization to real-world chart styles. To address this, we propose **ReChartPrompt**, leveraging real-world, human-designed charts extracted from arXiv papers as prompts. By harnessing the rich content and diverse visual styles of arXiv charts, we construct ReChartPrompt-240K, a large-scale and highly diverse dataset that better reflects realistic chart variations.\nFor the second challenge, although SFT improves code understanding by optimizing next-token prediction, it does not provide direct supervision on visual features. As a result, it often fails to guarantee that the generated charts visually match the original ones. To address this, we propose **ChartSimRL**, a GRPO-based reinforcement learning algorithm guided by a novel chart similarity reward. This reward consists of two components: *attribute similarity*, which measures the overlap of chart attributes like layout and color between the generated and original charts, and *visual similarity*, which evaluates overall visual features, including texture, using convolutional neural networks. Unlike traditional text-based rewards, our reward accounts for the multimodal nature of the chart-to-code generation task, significantly enhancing the model's ability to accurately reproduce charts.\nIntegrating ReChartPrompt and ChartSimRL, we develop the **ChartMaster** model, achieving SOTA results among 7B-parameter models and rivaling GPT-4o on various chart-to-code benchmarks.\nWe will release all code, datasets, and models to facilitate further research.", "tldr": "", "keywords": ["Chart-to-Code Generation; Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63d7a5139ec7acd729e596ca8d1cd5e5807bfd23.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Introduced “ReChartPrompt-240K”, a new real-world chart-to-code dataset with higher variability.\nIntroduced ChartSimRL, a GRPO-based RL algorithm to ensure the code-generated images align with the visual and the semantic attributes of the input image."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The first model to focus on addressing the Chart-To-Image challenges.\nComparison to many of the known base language models.\nComprehensive and fine-grained ablation studies justifying almost every decision.\nInclusion of RL training tailored for visual structure and semantic similarities.\nIntroduces a new dataset with higher real-world varieties.\nWell presented.\nSOTA results compared to other similar and larger models."}, "weaknesses": {"value": "Qwen2.5-VL-72B was used to filter the non-chart images, but this approach was not validated. What if some images are categorized as charts when they are not? Yes, the dataset achieved better results at the end, which indicates that it worked, but this may need further validations; maybe another tailored way of classification would enhance the introduced dataset.\n\nNo explanation of how the introduced semantic attribute extraction tool works, it is mentioned that it extracts the colors, numbers, texts, and the layout, but it’s not mentioned how exactly this is done from the chart image, and if the extraction is validated somehow."}, "questions": {"value": "Have you done any prompt-based analysis? You used 20 prompts for the chart generation. Do some prompts persistently produce better results?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "They used the arXiv API to download the papers and used the images from 30,071 different papers, but they didn’t mention whether they had validated under which license those papers were published.\n\nAs far as I researched, the arXiv content, like images, is not necessarily published under a license that allows reusing the images in this manner. However, most of the papers might be published under a relaxed license that allows doing so, but this might need to be reviewed to make sure the used papers are published under the correct license, especially since they are planning to publish their constructed dataset (From the downloaded papers' images) as an open source dataset.."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o0zUz5JVZg", "forum": "uO2WfDYfeK", "replyto": "uO2WfDYfeK", "signatures": ["ICLR.cc/2026/Conference/Submission1258/Reviewer_ER2g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1258/Reviewer_ER2g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895666128, "cdate": 1761895666128, "tmdate": 1762915719610, "mdate": 1762915719610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose ChartMaster, a chart to code system with two main parts: ReChartPrompt data pipeline that crawls charts (from arxiv) and uses an VLM to generate plotting code and ChartSimRL, a GRPO style RL stage that optimizes code with a combined reward: attribute similarity and visual similarity, plus execution signals. Their model is trained on Qwen2.5-VL 7B and reported to outperform open-source baselines and approach large closed models on ChartMimic, Plot2Code and ChartX."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear algorithmic core is heart of the paper, where reward is explicitly defined and implementable (Jaccard on attributes + ResNet-18 cosine on multi-level features under a GRPO objective). This is a reasonable and task-aligned signal for chart reproduction.\n- Competitive 7B results are posted. authors claims best open-source 7B performance across several chart-to-code benchmarks.. the table lists both closed and open models for context.\n- Leakage awareness is studied to an exten. Authors state they excluded papers used as benchmarks during arXiv crawling to reduce leakage risk..good / recommended practice for this area"}, "weaknesses": {"value": "- my one concern is that their data construction lacks quantitative auditing. While the paper says it crawls real-world tables/charts and filters non-executable code, it doesn’t quantify data quality: e.g., % of images incorrectly chart-typed, code-render success rate after filtering, or inter-annotator checks. Without a data card–style audit, it’s hard to judge noise, coverage, and bias in ReChartPrompt.\n- [minor] does reward overfit to style surrogates? their visual reward uses ResNet-18 features and cosine similarity averaged across blocks. this is simple and efficient, but may overweight low-level textures/layout while underweighting higher-order chart semantics (e.g., axis scaling quirks, legend mapping, tick formatting). some abalation evidence (even if qualitative) of failure cases or sensitivity analyses can make things stronger.\n- a reproducibility appendix with exact prompts and decoding configs for every baseline you reran would help..since benchmarks ChartMimic, Plot2Code, ChartX are \"filled by our own experiments\"\n-  intro/claims suggest \"near GPT-4o performance with only 7B.\"  comparison table indeed shows strong 7B results. Please consider either (a) add paired, identical decoding for GPT-4o across all tasks/metrics you report, or perhaps (b) soften wording to \"competitive among open-source 7B.\" to be fair.\n- Excluding benchmark papers is good, but not sufficient: figures often recirculate across venues, tech blogs, and textbooks. if possible, kindly show a near-duplicate search (image hashing / CLIP retrieval) between train vs. eval images (and code) with collision rates to substantiate the no-leakage claim.\n- Also, data collection retains only 12 predefined chart types. That keeps scope crisp, but may bias models to common/simple forms and limit transfer to bespoke charts (multi-panel, maps, complex faceting). A stress test on “unseen” or composite chart types would strengthen the story.\n- [Minor] Missing error analysis and qualitative diagnostics.\n\nmissing references of some relevant papers on visual reasoning and visual RL: \n[1] Masry et al. BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning, https://arxiv.org/abs/2508.09804 \n[2] Rodriguez et al, BigDocs: An Open Dataset for Training Multimodal Models on Document and Code Tasks. https://arxiv.org/abs/2412.04626 \n[3] Xia et al, StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding.\n[4] Awal et al. WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation https://arxiv.org/abs/2508.16763."}, "questions": {"value": "- What is the code execution pass-rate after filtering? What % of crawled images end up with valid, style-consistent reproductions? Any manual spot-checks or annotator agreement?\n- wondering how sensitive are results to the ResNet-18 backbone? What happens if you switch to a chart-aware visual encoder or add OCR-aware features to the reward?\n- What is a crucial difference and possible advantage between this work and BigCharts-R1? To me it's nearly the same.\n\n\nidea (attribute and visual rewards under GRPO)  and the 7B results look strong, but the paper may needs deeper data QA, leakage audits, and ablations to match top-tier standards. Addressing the points above would significantly strengthen the case. Happy to increase the scores during the response period."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ABFywNFlBh", "forum": "uO2WfDYfeK", "replyto": "uO2WfDYfeK", "signatures": ["ICLR.cc/2026/Conference/Submission1258/Reviewer_5icT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1258/Reviewer_5icT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932447931, "cdate": 1761932447931, "tmdate": 1762915719460, "mdate": 1762915719460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ChartMaster, a state-of-the-art chart-to-code model trained using a two-stage pipeline: supervised finetuning (SFT) and GRPO. To achieve this, the authors create the ReChart Prompt dataset which is a visually diverse chart-to-code dataset that generates synthetic chart-code pairs by “replotting” real-world images sourced from arXiv. For GRPO, the authors propose two novel reward metrics: visual attribute similarity (overlap in the layout/color attributes between the generated and ground truth chart/code) and visual similarity (measured using the cosine similarity between the geneated and reference chart using Resnet18 for feature extraction). The authors evaluate their model, ChartMaster, on three downstream tasks such as ChartMimic, ChartX, and Plot2Code, and the model achieves SOTA performance on most of them. Finally, the authors conducted a set of ablation studies to justify their design choices and done some qualitative analysis to showcase the superiority of their approach and model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The ReChart Prompt pipeline is quite novel and helps increase the visual diversity of the resulting dataset. It proposes a “chart replotting” technique that conditions the generation of synthetic chart-code pairs on real-world charts. \n\n\n* The authors designed two novel reward functions for the GRPO algorithm with detailed ablations to justify and support their design choices (Tables  3, 4, and 5). Also, the resulting model ChartMaster achieves SOTA results on a variety of chart-to-code tasks. \n\n\n* I believe the ReChart Prompt dataset could be valuable to the research community and could be extended to other domains such as QA and Fact Checking."}, "weaknesses": {"value": "* The chart images are sourced from one source, arXiv, which may limit the visual and topics diversity in the dataset. \n\n* The authors claim that their approach of replotting real-world chart images increases the diversity of the dataset compared to existing approaches that just prompts LLM to generate chart-code pairs. While I believe this is likely true, there’s no analysis to support this claim. \n\n* The paper is only limited to chart-to-code which is a very niche task and doesn’t explore the potential of the proposed approach on more diverse chart understanding tasks such as QA and Fact checking."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OmV52elyFd", "forum": "uO2WfDYfeK", "replyto": "uO2WfDYfeK", "signatures": ["ICLR.cc/2026/Conference/Submission1258/Reviewer_DZAz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1258/Reviewer_DZAz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959160398, "cdate": 1761959160398, "tmdate": 1762915719275, "mdate": 1762915719275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the task of chart-to-code generation, which aims to translate chart images into executable plotting code. It propose a complete data synthesis and distillation pipeline that constructs a large-scale, high-quality dataset (ReChartPrompt) from real-world scientific figures. Building upon this, the paper designs a reinforcement learning framework based on GRPO, incorporating a dual reward signal that jointly measures visual similarity and attribute-level semantic consistency between generated and reference charts. Finally, it trains a 7B-parameter model named ChartMaster, which achieves performance comparable to GPT-4o."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality:\nWhile the overall training paradigm (data distillation → SFT → GRPO) follows established LLM fine-tuning practices, the paper demonstrates an original and well-motivated application of this pipeline to the underexplored domain of chart-to-code generation. The proposed ChartSimRL introduces a novel dual-reward design that jointly leverages visual and attribute similarity signals—an inventive adaptation of multimodal reward shaping to code generation tasks. This represents a meaningful step toward aligning visual and semantic fidelity in multimodal reasoning.\nQuality:\nThe paper is technically solid and empirically thorough. The proposed components are well implemented and supported by extensive experiments across multiple benchmarks (ChartMimic, Plot2Code, ChartX). Ablation studies clearly isolate the effects of the dataset and each reward component, and results are reproducible with open-source models and frameworks. The training and evaluation pipeline is carefully described, suggesting high implementation quality.\nClarity:\nThe paper is well structured and clearly written. The motivation, methodology, and experimental design are logically presented with sufficient mathematical and procedural detail. Figures and tables are effectively used to illustrate qualitative and quantitative results, making the narrative easy to follow for both machine learning and vision-language audiences.\nSignificance:\nThe work makes a strong empirical contribution by achieving performance comparable to GPT-4o with an open-source 7B model, thus narrowing the gap between proprietary and community models in chart-to-code generation. The newly constructed ReChartPrompt dataset also provides lasting value to the research community as a more realistic and diverse resource for training multimodal reasoning systems. Together, these contributions make the paper a valuable and impactful addition to the field."}, "weaknesses": {"value": "1. Insufficient analysis of the SFT–RL interplay.\nThe paper does not clearly isolate the contribution of the SFT and GRPO stages in the final model’s performance. Specifically, it remains unclear whether the improvement of ChartMaster over the SFT baseline arises from the GRPO phase itself or from the preceding supervised fine-tuning on ReChartPrompt. The authors did not conduct an experiment where Qwen2.5-VL-7B is directly fine-tuned with GRPO without SFT, which would have provided stronger evidence for the necessity of the full training pipeline. Without this, it is difficult to assess whether GRPO alone suffices for the chart-to-code domain or if SFT is an essential prerequisite to preserve general multimodal reasoning ability.\n2. Lack of fine-grained quantitative analysis.\nAlthough qualitative examples are informative, the evaluation does not fully leverage the fine-grained metrics available in ChartMimic, such as text accuracy, layout fidelity, chart type classification, and color consistency. A more detailed quantitative breakdown would help clarify which aspects of visual-semantic consistency are most improved by ChartSimRL. The current presentation may leave readers uncertain about where the model’s strengths and weaknesses lie within specific visual attributes.\n3. Minor issues in limited methodological originality.\nAlthough the paper proposes a well-executed adaptation of GRPO to the chart-to-code domain, the overall training pipeline closely follows the now-standard Distillation → SFT → GRPO paradigm used in recent works such as DeepSeek-R1 and Vision-R1. The novelty lies mainly in the domain-specific reward formulation (visual + attribute similarity), which is a practical but incremental design rather than a conceptual advance in RL or multimodal alignment. The authors could strengthen this aspect by providing deeper analysis or theoretical motivation for their reward shaping strategy, or by showing that their formulation generalizes to other multimodal generation tasks beyond chart-to-code."}, "questions": {"value": "1. On the necessity of SFT before GRPO.\nCould the authors clarify whether the supervised SFT stage is essential for achieving the reported performance of ChartMaster? Such an experiment would help disentangle how much of the performance gain originates from the reinforcement learning phase versus from the prior data distillation and SFT stages. Understanding this would also shed light on whether GRPO alone suffices for adapting pretrained multimodal models to the chart-to-code domain, or if SFT provides necessary grounding of fundamental reasoning capabilities.\n2. On the relationship between reward components and fine-grained metrics.\nThe proposed dual reward combines attribute-level similarity (R_attr) and visual similarity (R_vis). Could the authors provide more evidence or analysis on how these two components correlate with fine-grained evaluation metrics—such as text accuracy, layout alignment, color consistency, or chart type fidelity—available in benchmarks like ChartMimic? Clarifying which reward component most influences each aspect of chart reconstruction quality would help readers better understand the strengths and limitations of the ChartSimRL design.\n3. On the dependence of model performance on the teacher used in data distillation.\nGiven that the ReChartPrompt-240K dataset was generated using Qwen2.5-VL-72B as the teacher model, how sensitive is the final performance of ChartMaster to the teacher’s quality? In other words, if the same pipeline were applied with a stronger teacher model (e.g., GPT-4o) for data distillation, would the resulting student potentially exceed GPT-4o’s performance on chart-to-code benchmarks, or does the distillation process inherently cap the achievable quality? A discussion or small-scale experiment could clarify this dependency and help position the contribution relative to future stronger teachers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jsdnFcKlE6", "forum": "uO2WfDYfeK", "replyto": "uO2WfDYfeK", "signatures": ["ICLR.cc/2026/Conference/Submission1258/Reviewer_Gapn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1258/Reviewer_Gapn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986649916, "cdate": 1761986649916, "tmdate": 1762915718975, "mdate": 1762915718975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}