{"id": "GiNy68FgKR", "number": 20220, "cdate": 1758303812691, "mdate": 1759896989974, "content": {"title": "Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling", "abstract": "Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows incorporating equivariance to arbitrary pseudo-Euclidean groups, including isometries of Euclidean space and Minkowski spacetime. In this work, we demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the model expressivity. To address this issue, we propose Conditional Clifford-Steerable Kernels, which augment the kernels with equivariant representations computed from the input feature field. We derive the equivariance constraint for these input-dependent kernels and show how it can be solved efficiently via implicit parameterization. We empirically demonstrate an improved expressivity of the resulting framework on multiple PDE forecasting tasks, including fluid dynamics and relativistic electrodynamics, where our method consistently outperforms baseline methods.", "tldr": "", "keywords": ["Clifford-Steerable CNNs", "geometric deep learning", "PDE modeling", "Poincaré-equivariance"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/580738abc99caa49090d783f34d1a5b17381b4dd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Conditional Clifford-Steerable CNNs (C-CSCNNs), an extension of Clifford-Steerable CNNs (CSCNNs). The core idea is to address the incomplete kernel basis of the original CSCNN framework by augmenting the steerable kernels with auxiliary variables derived from the input feature field. The authors claim this \"conditioning\" mechanism completes the kernel basis, thereby enhancing the model's expressivity. They derive the equivariance constraint for these input-dependent kernels and demonstrate their method's effectiveness on several PDE forecasting tasks, where it reportedly outperforms baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "*   **Significance of the Problem:** The paper correctly identifies a significant limitation in the CSCNN framework (Zhdanov et al., 2024)—the incompleteness of its kernel basis. Addressing this limitation is a worthwhile endeavor, as a complete basis would theoretically lead to a more expressive and powerful equivariant model, which is crucial for accurately modeling physical systems with complex symmetries.\n*   **Clarity of Motivation:** The introduction clearly articulates the problem. The authors provide a good intuition in Section 3.3 and Example 3.2 on why the original CSCNNs fail to generate certain kernel components (e.g., frequency-2 components for vector-vector interactions in $O(2,0)$), which effectively motivates the need for a solution."}, "weaknesses": {"value": "This paper, while addressing an important problem, suffers from significant weaknesses in its algorithmic contribution and, most critically, in its experimental validation. These issues make it impossible to fairly assess the true effectiveness of the proposed method.\n\n1.  **Limited Algorithmic Contribution:** The paper is heavily laden with mathematical formalism, much of which is directly inherited from the prior work on CSCNNs and general steerable CNN theory. The actual novel contribution is the \"conditioning\" mechanism, which is implemented in a very straightforward manner: using global mean pooling to generate a single conditioning multivector. While the derivation of the corresponding equivariance constraint (Lemma 4.1) is technically sound, the core algorithmic idea itself is an incremental extension rather than a substantial leap forward. The heavy reliance on existing mathematical frameworks makes the paper's own contribution appear smaller than presented.\n\n2.  **Insufficient and Inconsistent Experimental Setup:** This is the most critical flaw of the paper. The experimental setup is confusing, inconsistent, and incomplete, which severely hinders the reader's ability to verify the authors' claims.\n    *   **Inconsistent Baselines and Presentation:** The paper claims to compare against 14 baselines, but this is not reflected consistently in the results. Figure 2 shows only 5 baselines. Table 1 includes 8 baselines, with the data admittedly being taken from another paper (Wang et al., 2025), which is not a direct, controlled comparison. Figure 3 shows only 6 baselines. Furthermore, the set of baselines changes from one experiment to another. This inconsistency makes it impossible to draw a clear, overarching conclusion about the method's performance relative to the state-of-the-art.\n    *   **Lack of Standardized Benchmarking:** To ensure a fair and reproducible comparison, the authors should evaluate their model on a standardized benchmark like PDEBench. I strongly recommend that the authors re-run their experiments and present the main results in a comprehensive table. This table should include, for all datasets, a consistent set of strong baselines, such as **Transolver, the original CSCNN, Swin-Transformer, CViT-L, and DPOT-H**. The table format used in the main experiments of the DPOT paper could serve as a good template.\n\n3.  **Confusing and Unconvincing Scaling Analysis:**\n    *   The results in Table 1 are perplexing. It appears to show that by simply increasing the number of parameters, other methods (e.g., U-F2Net, CViT-L) can outperform the proposed C-CSCNN (Large). This raises a critical question: **What is the true significance of equivariance if it can be surpassed by simply scaling up a non-equivariant or less-equivariant model?** The authors do not address this point. To make a convincing case, the authors must demonstrate how their model performs at larger scales (e.g., 100M, 200M, 400M parameters). Can a scaled-up C-CSCNN still maintain a performance advantage over other SOTA models with a similar parameter count? Without this analysis, the benefits of the proposed method remain questionable.\n\n4.  **Uninformative Visualizations:**\n    *   Figures 4 and 5, which show signed residuals and rollout errors, are presented without any quantitative context or comparison to the key baselines mentioned above. As they stand, these figures are purely illustrative and provide very little information. Are the residuals for C-CSCNN significantly smaller than for Transolver or CViT-L? How does the rollout error accumulate compared to other methods? These results should be presented in a tabular format with clear metrics (e.g., Mean Squared Error, L2 error at different rollout steps) and compared against the same strong set of baselines used in the main experiments."}, "questions": {"value": "1.  The central claim of the paper is that C-CSCNNs have a \"complete kernel basis,\" yet this is presented as a conjecture (Conjecture 4.1) without a formal proof. While I understand the proof might be non-trivial, could you provide a more rigorous argument beyond the $O(2,0)$ example? For instance, can you show that for any irreducible representations (irreps) of $O(p,q)$, the tensor product of the input and conditioning multivector representations contains all the necessary irreps for a complete kernel basis? Without stronger theoretical backing, the main claim feels unsubstantiated.\n\n2.  Regarding the experimental setup: Why was there such a high degree of inconsistency in the choice of baselines across different experiments? A consistent comparison against a fixed set of state-of-the-art models (e.g., Transolver, CViT-L, DPOT-H) across all tasks would make the results much more convincing. Can you clarify the rationale behind the current experimental design?\n\n3.  Following up on Table 1: The performance of your 55M parameter model is comparable to or worse than several larger models. Could you provide results for a C-CSCNN model scaled to a much larger size (e.g., ~100M or ~200M)? This is crucial to understand if the benefits of equivariance persist at scale or if they are washed out by the sheer capacity of larger, less-structured models.\n\n4.  Could you provide the quantitative results corresponding to Figures 4 and 5 in a table, comparing them directly against the key baselines (Transolver, CSCNN, Swin-Transformer, CViT-L, DPOT-H)? This would allow for a much clearer assessment of the performance improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b1FdBHmbRK", "forum": "GiNy68FgKR", "replyto": "GiNy68FgKR", "signatures": ["ICLR.cc/2026/Conference/Submission20220/Reviewer_3dR3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20220/Reviewer_3dR3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833516518, "cdate": 1761833516518, "tmdate": 1762933717433, "mdate": 1762933717433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses an incompleteness on the kernel representation defined over the Clifford algebra. The incompleteness is reported at least for the Clifford algebra with characteristics (2, 0), and it has a deteriorating impact on the performance on Clifford neural networks defined with Clifford steerable kernels. The authors introduced (Clifford) group-steerable kernels augmented with auxiliary variables derived from input feature fields to complement the lack of freedom. The proof for the equivariance of the kernels is also given. The experiments are conducted in a range of representative scenarios where the Clifford algebra is known to model well. \n\nOverall, the paper is not ready to be published in the current form, because the significance and indication of the work are unclear. The mathematical notions included in this paper are also very vague and many of the accounts are loose. I expect the paper would need a couple of profound updates and revisions."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Simplicity of the idea. The authors figured out a simple way to make the change to the group steerable kernel. This helps to avoid overly complicating the kernel to be equivariant to the action and also has a knock-on effect for the conciseness of the proof.\n-  A conjecture for the completeness is also given in an attempt to promote the research in this field.\n- Experiments are conducted in representative scenarios and show an evidence that filling the void of the incompleteness with the conditional kernel could reduce the prediction error."}, "weaknesses": {"value": "**Significance of the work:** While the paper attempts to fill the gap between the theoretical results on the steerable kernel known in the existing literature and its practical implementation, the significance of the theoretical contribution and the empirical results is unclear. I believe this vagueness is stemmed from the lack of satisfactory discussion on applications or scenarios in which this incompleteness causes a crucial problem in downstream tasks. While in Introduction an issue associated with the existing kernel bases is mentioned, they are simply accounted for the \"weak expressivity,\" and it is still unclear when and how this \"weak expressivity\" is crucial. This also leaves me uncertain that the present work solely focused on $O(2, 0)$ is significant enough, and I also had a difficulty in assessing the significance of the performance improvement in the experiments due to the lack of the discussion. All those aspects leave the overall significance of the work hard to assess. \n\n**Straightforward extension:** The theoretical results do not involve any non-trivial proofs and the majority of them are essentially relying on the same proof as in [1]. Given the unclear motivation mentioned above, the extent of difficulty in giving proof also matters and the theoretical result is relatively weak.\n\n**Mathematical notations:** Mathematical notations are loosely introduced and the paper is very hard to follow. The following is the list of (non-exhaustive) vague points I believe need more detailed accounts:\n- Line 113: Mention on the definition of $W$ is missing. I presume, it should be a vector space.\n- Equation (2) does not make sense for me. How is the action of $G$ onto the pseudo-Euclidean space formalized here?\n- Line 135: Eq. 3.1 (typo)\n- Example 3.1 does not follow if we follow the equation (2)\n- Line 144: … integral 3.1 … (typo)\n- What is det$(g)$? Do the authors assume $G$ is a subgroup in the matrix group?\n\n\n[1] Zhdanov et.al., Clifford-Steerable Convolutional Neural Networks, ICML 2024."}, "questions": {"value": "Please translate the unclear or uncertain points mentioned in the Weakness column to my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BExgQbjEKn", "forum": "GiNy68FgKR", "replyto": "GiNy68FgKR", "signatures": ["ICLR.cc/2026/Conference/Submission20220/Reviewer_f8xF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20220/Reviewer_f8xF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900791814, "cdate": 1761900791814, "tmdate": 1762933717063, "mdate": 1762933717063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Conditional Clifford-Steerable CNNs (C-CSCNNs), an extension to Clifford-Steerable CNNs that aims to address the limited expressivity of the original model. The core idea is to condition the steerable convolutional kernels on auxiliary variables derived from the input feature field, thereby making the kernels input-dependent. The authors provide a theoretical derivation of the equivariance constraint for such conditional kernels and propose an efficient implementation using implicit parameterization and global mean pooling. The method is evaluated on several PDE forecasting tasks, demonstrating improved performance over baseline models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper attempts to ground the proposed method in the established theory of steerable CNNs and Clifford algebra.\n- The proposed model demonstrates certain empirical performance."}, "weaknesses": {"value": "- **Lack of Novelty:** The central mechanism of conditioning convolutional kernels on input features is a well-established technique in the broader deep learning literature (e.g., in dynamic filtering and attention). The paper does not demonstrate a fundamental innovation beyond a relatively straightforward application of this idea within the Clifford algebra framework. The conceptual leap from standard conditional convolution to the proposed C-CSCNN is insufficiently articulated.\n- **Weak Theoretical Foundation:** The paper's most significant theoretical claim—the completeness of the conditional kernel basis—is presented merely as a conjecture (Conjecture 4.1) without a rigorous proof. For a method that is heavily theorized and whose primary motivation is to solve a theoretical limitation, this is a major shortcoming. The validity of this central claim remains unverified.\n- **Inadequate Experimental Analysis and Baselines:**\n  - **Lack of Rigorous Ablation:** A critical flaw is the absence of a thorough ablation study. It remains entirely unclear whether the performance gains stem from the sophisticated conditional kernel design or simply from the model having access to a global context vector via mean pooling. Without experiments that ablate the conditioning mechanism itself and compare it to simpler ways of incorporating global information, the claimed 'improved expressivity' remains unsupported and could be an artifact of increased model input information.\n  - **Unfair and Unclear Baseline Comparisons:** The paper fails to provide detailed configurations for the baseline models. For instance, it highlights that C-CSCNNs outperform large models like FNO (270M) and UNO (440M), but does not justify **why these baselines require so many parameters**. It is plausible that for the considered tasks, these models were over-parameterized or not optimally configured (e.g., using an excessive number of Fourier modes), which would unfairly inflate the perceived advantage of the proposed method. A fair comparison requires transparent and task-appropriate baseline setups.\n- **Major Issues in Presentation and Narrative:**\n  - The paper is severely unbalanced. The `Related Work` and `Theoretical Background` sections span over two and a half pages, predominantly reviewing existing knowledge without effectively foregrounding the paper's own contribution. These sections should be significantly condensed and reorganized.\n  - Conversely, the `Method` section is underdeveloped. It focuses on mathematical derivations and an idealized case (O(2,0)) but lacks a clear, practical description of the concrete implementation. The narrative fails to guide the reader from the theory to a tangible algorithm, making the work difficult to understand, verify, and replicate. In its current form, the writing quality is unacceptable for publication."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7Z75LJB0Pg", "forum": "GiNy68FgKR", "replyto": "GiNy68FgKR", "signatures": ["ICLR.cc/2026/Conference/Submission20220/Reviewer_BPnS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20220/Reviewer_BPnS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070155831, "cdate": 1762070155831, "tmdate": 1762933716516, "mdate": 1762933716516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes new theoretical analysis and layer design for Clifford-streerable CNNs. The motivation is that the original CSCNN is not complete in terms of equivariant information. The theoretical analysis eventually leads to a simple design where the layer is additionally conditioned on the average-pooled features. The design conforms with the theoretical insights although the completeness of the simple design is not proved and is formulated as a conjuncture. The model is tested on serval PDE learning tasks including NS, SWE and Maxwell."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper includes in-depth theoretical analysis which leads to a simple and pratical design.\n- The presented results show improvement against CSCNN baseline."}, "weaknesses": {"value": "- The presentation may not be easily accessible for readers not familiar with CSCNN.\n- The performance of the proposed model does not seem to be very competitive on SWE-5."}, "questions": {"value": "- What is the equivariance of the PDE tasks?\n- What is the major difference between CSCNN and steerable CNNs? Is the goal of CSCNN also achieving rotation equivariance?\n- [1] shows that techniques such as circular padding is important for tasks with periodic boundaries (e.g., SWE). Can C-CSCNN also be adapted for periodic boundary?\n- If applicable, you may consider including [1] in the discussion or results since they also use the SWE dataset (also NS).\n- In Figure 5, why does the C-CSCNN prediction have a different scale (colorbar) compared to the ground truth?\n\n[1] Sinenet: Learning temporal dynamics in time-dependent partial differential equations. https://arxiv.org/abs/2403.19507"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XyAjM3aCNn", "forum": "GiNy68FgKR", "replyto": "GiNy68FgKR", "signatures": ["ICLR.cc/2026/Conference/Submission20220/Reviewer_jhkk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20220/Reviewer_jhkk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762187288526, "cdate": 1762187288526, "tmdate": 1762933715648, "mdate": 1762933715648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}