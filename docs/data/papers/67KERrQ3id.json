{"id": "67KERrQ3id", "number": 3429, "cdate": 1757424563424, "mdate": 1759898090134, "content": {"title": "OSCAR: Orthogonalized Sequential Component Analysis for Tensor-on-Tensor Regression", "abstract": "Tensor-on-tensor (TOT) regression is a critical task in many fields. However, its application is severely hindered by the curse of dimensionality arising from the exponential growth of parameters in the coefficient tensor. Existing methods primarily fall into two categories: low-rank approximations, which often have limited predictive accuracy and interpretability, and sequential component extraction methods that rely on data-space deflation. This deflation mechanism suffers from greedy sub-optimal solutions, error propagation, and a lack of component orthogonality, hindering feature disentanglement. To address these limitations, we propose $\\textbf{O}$rthogonalized $\\textbf{S}$equential $\\textbf{C}$omponent $\\textbf{A}$nalysis for Tensor-on-Tensor $\\textbf{R}$egression ($\\textbf{OSCAR}$). First, we design an Input-Mode Orthogonal Block Term ($\\textbf{IMOBT}$) low-rank structure for the coefficient tensor, which inherently enables the supervised extraction of orthogonal components. Building on this, we develop a Sequential Riemannian Optimization ($\\textbf{SRO}$) framework that replaces classical data-space deflation with explicit geometric constraints in the parameter space. This is achieved through a Subspace Constraint Riemannian Gradient Descent algorithm on a Stiefel manifold to rigorously enforce orthogonality. Furthermore, to alleviate the greedy bias of sequential learning, we introduce a novel collaborative refinement mechanism that re-optimizes the synergy among all components whenever a new one is added, enabling an iterative look-back for a superior global solution. Extensive experiments on synthetic and real-world datasets demonstrate that our proposed OSCAR framework not only achieves competitive predictive performance but also shows significant advantages in supervised component extraction and feature disentanglement.", "tldr": "", "keywords": ["Tensor-on-Tensor Regression", "Tensor Decomposition", "Component Analysis", "Riemannian Gradient Descent(RGD)"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3bab1e1c86fce50b224bda8d79c22e0eddc25c19.pdf", "supplementary_material": "/attachment/c2f565263ca7af85a0d4aadbcbd32ba986a3203c.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the tensor-on-tensor regression problem by introducing OSCAR, an algorithm that tries to avoid the pitfalls of low-rank approximations and deflation-based sequential methods. The key idea is an Input-Mode Orthogonal Block Term (IMOBT) structure that yields supervised, mutually orthogonal components in the coefficient tensor. Optimization is performed via a Sequential Riemannian Optimization. There is an additional 'collaborative refinement step' that re-optimizes all previously learned components whenever a new one is added. Experiments on synthetic and real datasets are presented."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The challenges the paper aims to address are clearly stated in the Introduction. The representation of the tensor regression coefficient is novel."}, "weaknesses": {"value": "Some acronyms in the Introduction are not expanded on first appearance (e.g., N-PLS, HOPLS).\n\nThe notation is inconsistent and occasionally confusing. For example, ``Mode-$n$ unfolding'' defined on p. 3 is never used later. The loss is denoted both by $L$ and $\\mathcal{L}$.\n\nSeveral symbols appear without definition or with unclear roles, e.g., $b_i$ (p. 5, first paragraph), $r_i$ (p. 5, second paragraph), and $\\Theta$ in Eq. (11). In addition, the ``population risk'' on p. 13 includes a regularizer on $W$ without justification for why $W$ should be penalized at that stage.\n\nThe paper discusses ``low-rank'' and ``sparsity'' but does not formally define the low-rankness imposed (e.g., Tucker rank, CP rank, block-term ranks) or motivate the sparsity pattern (which factors/entries are encouraged to be sparse and why).\n\nThe algorithm is not fully presented. There is no information about how A and W are optimized in section 3. There is only vague description of the order of optimization of $B_{i,p}$, however, the model has parameters W, A, B jointly generating the regression coefficient.  \n\nKey settings are missing. For several experiments, the number of components $K$ is not reported; only Fig.~4 specifies $K=4$. Please report $K$, rank-related hyperparameters, regularization strengths, and optimization details (learning rate, epochs/iterations, tolerances) for all experiments.\n\nIn Fig. 4, the first principal component outperforms baselines, yet adding the 2nd--4th components yields worse overall performance than existing methods. This trend raises concerns about the results in the previous experiments. Please (i) analyze why later components hurt performance (e.g., overfitting, non-orthogonality in practice, suboptimal refinement), (ii) report per-component contributions with confidence intervals, and (iii) clarify whether this indicates the method is overall inferior to baselines when multiple components are used."}, "questions": {"value": "See \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UTf6rDSu3S", "forum": "67KERrQ3id", "replyto": "67KERrQ3id", "signatures": ["ICLR.cc/2026/Conference/Submission3429/Reviewer_Xa7y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3429/Reviewer_Xa7y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789880272, "cdate": 1761789880272, "tmdate": 1762916720638, "mdate": 1762916720638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors develop an OSCAR framework for tensor-on-tensor (TOT) regression. The IMOBT structure is introduced for supervised extraction of orthogonal components. The Riemannian optimization is utilized to enforce orthogonality in parameter space, and a refinement mechanism is utilized to get the global solutions. Theoretical analysis of the proposed algorithm is provided. Experiments on synthetic and real-world datasets show the desired performance of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. A new framework for ToT is developed.\n2. The IMOBT and SRO are proposed to enable orthogonal component extraction and enforce orthogonality.\n3. Experimental results show the improvement of the proposed method compared to existing algorithms."}, "weaknesses": {"value": "1. The paper is not well organized. The main algorithm and its theoretical analysis are not shown in the main paper.\n2. The time complexity of the proposed method is not given.\n3. Experiments are limited to 3-order tensors."}, "questions": {"value": "1. The organization of the paper should be improved. The pseudo-code should be provided. The main algorithm should appear in the main text, not in the appendix. More explanation for Figure 2 should be added.\n2. The appendices are hard to read. For example, it is unclear what Appendix A and Theorem 1 derive for (I cannot find any relation in the paper to Appendix A or Theorem 1). Also, it is unclear which algorithm the convergence guarantee in Appendix B refers to (perhaps the algorithm in Appendix C).\n3. What is the theoretical advantage compared to TTReg (Qin & Zhu (2025))?\n4. What is the time complexity of the proposed method, and how does its running time compare to other methods?\n5. How does the method perform on higher-order tensors (order 4 or higher)?\n6. Some typos are not well described. What does the y label (R^2) in Figure 3 mean? What does “population model” mean in the paper (it appears in line 339/547)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pYSqzZkOMF", "forum": "67KERrQ3id", "replyto": "67KERrQ3id", "signatures": ["ICLR.cc/2026/Conference/Submission3429/Reviewer_Z8Sz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3429/Reviewer_Z8Sz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963130961, "cdate": 1761963130961, "tmdate": 1762916720423, "mdate": 1762916720423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To remedy the issue of  “curse of dimensionality” arising in tensor-on-tensor regression, this work proposes a novel framework that unifies low-rank modeling with supervised orthogonal component extraction. This framework could also enable the supervised extraction of orthogonal components, resulting in a more accurate and interpretable regression framework. Extensive experiments on both synthetic and real-world datasets are carried out to show that the proposed framework not only surpasses existing state-of-the-art methods in predictive performance but also has unique advantages in model interpretability and feature disentanglement."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. A soundness and efficient framework is provided to address the issue of  “curse of dimensionality” for the tensor-on-tensor problem.\n2. A new stage-wise RGD-based optimization scheme is developed to solving the resulting problem.\n3. Extensive experiments are carried out to demonstrate the merits of the proposed framework."}, "weaknesses": {"value": "1. The procedure for generating synthetic data is not clearly described.\n2. The proof sketch of the main Theorem 1 is hard to follow.\n3. The reasonableness of the Assumptions 1-5 was not discussed."}, "questions": {"value": "1. How to choose the starting point for the developed RGD algorithm?\n2. How does the performance of the proposed framework extend beyond the setting of Gaussian noise?\n3. How can the tensor rank be effectively tuned for the proposed framework in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qEc9fEacNh", "forum": "67KERrQ3id", "replyto": "67KERrQ3id", "signatures": ["ICLR.cc/2026/Conference/Submission3429/Reviewer_5EqZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3429/Reviewer_5EqZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145283591, "cdate": 1762145283591, "tmdate": 1762916720229, "mdate": 1762916720229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}