{"id": "duQJWXvUu8", "number": 9707, "cdate": 1758135902961, "mdate": 1759897703131, "content": {"title": "Uncertainty-Aware Scheduling: State-Dependent Training-Free Diffusion Alignment", "abstract": "Training-free guidance has emerged as a promising approach to aligning diffusion models with downstream objectives by steering the denoising process with reward-based signals. Typically, reward functions are trained on clean images then applied to noisy intermediate predictions ($\\hat{x}_{0|t}$), suffering from a domain gap that compromises effective guidance. Existing methods improve guidance through handcrafted schedules, such as fixed or time-dependent tempering. However, such state-agnostic scheduling is prone to suboptimal alignment as we discovered that denoising progress varies widely across samples. We propose State-Dependent Adaptive Guidance (SDAG) to schedule guidance through an uncertainty-aware confidence-calibrated assessment. SDAG introduces a lightweight quality predictor that estimates denoising progress from intermediate states, i.e. the closeness between their approximated and the final clean targets. Through a last-layer Laplace approximator, this predictor provides uncertainty estimates, which are used together with the closeness scores to scale guidance reliably. Our SDAG applies to both standard denoising and population-based sampling, such as Sequential Monte Carlo, where coordination by effective sample size ensures robust collective guidance. Experiments demonstrate that SDAG achieves superior alignment while maintaining computational efficiency, establishing a promising paradigm for adaptive guidance in training-free diffusion alignment.", "tldr": "", "keywords": ["diffusion models", "alignment", "generative models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f35095d698006bd85e1aebe8a0f2307d85919151.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The core idea is a quality predictor estimating the closeness between the approximate and final denoised image. This is used to provide a semblance of uncertainty and thus adjusting the importance of intermediate samples towards a rewards tilted posterior."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed idea is interesting and the results seem to be promising.\n- The paper is well-written and structured and has a coherent narrative. \n- A set of both qualitative and quantitative results are provided."}, "weaknesses": {"value": "- I'm not convinced if one can consider this approach training free, and thus compare against those in that bucket. Section 4.1 elaborates on the fact that quality predictor $Q(.)$ has to be trained during the training process of the diffusion model, limiting plug-and-play inference-time application of SDAG. Am I missing something? \n\n- The results are not notably better than the likes of DAS. Please see more on this in my questions (next section). \n\n- The paper will definitely benefit from a thorough round of proof-read. Several typos and undefined notations (LLL?) can be found throughout the text and in image captions (\"floow\")."}, "questions": {"value": "- Where does Eq (6) comes form? Good to provide reference as it is not the core proposition of the paper. \n\n- How should one read the oscillation in $\\lambda_t$ in Fig. 3. DAS is pretty linear, but in practice provides results almost as good as SDAG (according to Table 1).\n\n- How would SDAG compare against DAS in terms of computational complexity? Why would one opt for SDAG if the results are so close according to Table 1? Qualitative results can be cherry-picked, as everyone does, so I would not strongly rely on those. Please elaborate on this, in detail, in the paper.\n\n- Are you are interested in sampling based approaches prior to DAS to enrich your literature survey? Consider (i) CoDE https://arxiv.org/pdf/2502.00968 (more on image-and-text-to-image settings) (ii) SVDD: https://arxiv.org/pdf/2408.08252v3 (also on generic non-image data)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "g8JvcQuQFb", "forum": "duQJWXvUu8", "replyto": "duQJWXvUu8", "signatures": ["ICLR.cc/2026/Conference/Submission9707/Reviewer_BN3G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9707/Reviewer_BN3G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761318980989, "cdate": 1761318980989, "tmdate": 1762921213256, "mdate": 1762921213256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies the challenge that existing training-free guidance methods use fixed or time-dependent scheduling that ignores heterogeneous denoising progress across samples at the same timestep. To solve this, this paper proposes State-Dependent Adaptive Guidance (SDAG), a method that dynamically schedules guidance strength for each sample based on its estimated denoising progress and the associated uncertainty. SDAG introduces a lightweight quality predictor equipped with Last-Layer Laplace approximation to estimate denoising progress and predictive uncertainty. This estimate is used within a confidence-aware line search to determine a safe guidance step size. The method is extended to Sequential Monte Carlo (SMC) sampling, where population-level coordination ensures stable collective guidance. Experiments demonstrate that SDAG achieves superior alignment across various reward functions while maintaining computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel problem insight and motivation: This paper provides clear empirical evidence showing that denoising progress varies significantly across samples at the same timestep, directly introducing the foundation limitations of current fixed and state-agnostic scheduling methods.\n\n2. Technical framework with theoretical grounding: In Sec. 4, the authors provide clear theoretical proof and guarantees for the SDAG method. The method provides a solid theoretical foundation through optimization-based line search (Definition 4.1), and convergence guarantees for population-level coordination (Lemma 4.2) through dual-constraint optimization balancing ESS and quantile constraints (Definition 4.2).\n\n3. Convincing experimental evaluation: This paper compares against three strong baselines across four diverse reward functions, providing a thorough performance assessment. The analysis of experimental results is sound."}, "weaknesses": {"value": "1. Limited model generalizability: All experiments in this paper are conduct on stable-diffusion-v1.5, which is a quite out-of-date model. Can you provide  experiments on other diffusion architecture and discuss the model generalizability for SDAG?\n2. Lack quality predictor validation: This paper doesn't  thoroughly analyze the prediction accuracy of the quality predictor and how prediction errors affect final performance. Although Sec. D.3 and Fig.6 provide qualitative analysis, can you provide more quantitative analysis?\n3. The conclusion of the number of particles: The authors mention \"The gains are most pronounced between 1-4 particles, with diminishing returns beyond 8 particles.\" However, we can observe that there is still a significant gain between 8-16 particles. Can you provide more experiments of other metrics or a more detailed analysis of your choice?\n4. More challenging tasks: In Fig. 3, the provided prompt is quite simple. Could you provide experimental results under more challenging prompts to further compare SDAG and the baseline method?"}, "questions": {"value": "In weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OgzmzLoXYn", "forum": "duQJWXvUu8", "replyto": "duQJWXvUu8", "signatures": ["ICLR.cc/2026/Conference/Submission9707/Reviewer_QvVH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9707/Reviewer_QvVH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964531839, "cdate": 1761964531839, "tmdate": 1762921212920, "mdate": 1762921212920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes State-Dependent Adaptive Guidance (SDAG), a training-free alignment method for diffusion models that adaptively schedules reward-based guidance per state and per particle. A lightweight “quality predictor” estimates denoising progress by predicting the cosine similarity between intermediate clean estimates and final outputs using a vision encoder. A last-layer Laplace approximation supplies epistemic uncertainty. The initial step scale is tied to the magnitude of classifier-free guidance for sensible normalization. Experiments across multiple reward functions show improved target alignment while maintaining diversity and quality, while a temporal-window application reduces runtime with improved alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(i) The paper features broad evaluation with multiple reward functions, yielding consistent gains on targets without large drops on complementary metrics and preserving diversity\n\n(ii) The temporal-window guidance analysis is useful and actionable, yielding speedups with quality gains\n\n(iii) The paper proposes a lightweight predictor and straightforward integration"}, "weaknesses": {"value": "(i) Claims w.r.t. “training-free” is somewhat stretched. A new quality predictor is trained and calibrated and generalization across prompts, rewards, and domains may be brittle\n\n(ii) The uncertainty quality w.r.t. LCB calibration does not appear to be not rigorously validated\n\n(iii) Computational details and selection procedure of parameters are not fully specified\n\n(iv) Presentation of parmetarization could be improved to ensure the reproducibility of results"}, "questions": {"value": "In addition to the weaknesses outlined in points (i-iv), I present the following questions for the authors to address:\n\n(1) Some of the parameterization is unclear to me. E.g., what values were used for $\\delta$ (ESS threshold) and $\\rho$ (quantile constraint proportion) across experiments, and how sensitive are results to these choices?\n\n(2) How is $\\lambda_{ESS}$ computed efficiently at each step (grid-search or solved analytically)? What is the added computational overhead?\n\n(3) Could the method be extended to support non-differentiable rewards? If so, would $\\hat{v}$ be obtained or approximated?\n\n(4) How does SDAG interact with different CFG scales and sampler settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WHjewvZiIe", "forum": "duQJWXvUu8", "replyto": "duQJWXvUu8", "signatures": ["ICLR.cc/2026/Conference/Submission9707/Reviewer_Rhkv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9707/Reviewer_Rhkv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986759182, "cdate": 1761986759182, "tmdate": 1762921212597, "mdate": 1762921212597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on training-free guidance for aligning diffusion models. The proposed method uses an adaptive guidance scale for approximate guidance methods to incorporate completion of generation, i.e., how close the approximation of a clean image via one-step denoising is to the final clean image. Experiments are conducted with image generation with various rewards to compare with previous guidance methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper identifies heterogeneity during denoising that the completion of the predicted image via one-step denoising may differ across different samples.\n2. The method considers both per-particle guidance and guidance with particle-based sampling."}, "weaknesses": {"value": "1. Motivation and justification, nor detailed implementation of per-particle confidence-aware line search, which is the key of the proposed method, aren't presented thoroughly.\n2. Quantitative results presented in Section 5.1 show only incremental improvement beyond previous methods, where the performance difference is mostly inside the margin of error. It's questionable whether heterogeneity during denoising is a critical problem, and adaptive guidance is necessary.\n3. The method relies on a pre-trained encoder for the quality predictor, and the experiments are done only in the image domain, so it's unclear whether the method will generalize to other domains.\n4. The method isn't totally training-free since it needs to train a quality predictor, though it's relatively easy to train."}, "questions": {"value": "1. The adaptive guidance scale seems to be very noisy, according to Figure 3. Since the diffusion model continuously refines the prediction, shouldn't the guidance scale almost monotonically increase over time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SclJbD74Fz", "forum": "duQJWXvUu8", "replyto": "duQJWXvUu8", "signatures": ["ICLR.cc/2026/Conference/Submission9707/Reviewer_QyK4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9707/Reviewer_QyK4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999417837, "cdate": 1761999417837, "tmdate": 1762921212193, "mdate": 1762921212193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}