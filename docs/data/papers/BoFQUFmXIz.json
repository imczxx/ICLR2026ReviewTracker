{"id": "BoFQUFmXIz", "number": 13093, "cdate": 1758213504867, "mdate": 1759897465904, "content": {"title": "On Kernel RL and Optimistic Closure", "abstract": "We study episodic reinforcement learning with a kernel (RKHS) structure on state-action pairs. Previous optimistic analyses in this case either pay a data-dependent covering-number penalty that can grow with time and undermine no-regret guarantees, or it assumes a strong “optimistic closure” condition requiring all optimistic proxies to lie in a fixed state-RKHS ball. We take a different approach that removes the covering-number dependence without invoking optimistic closure. Our analysis builds a uniform confidence bound, derived via conditional mean embeddings, that holds simultaneously for all proxy value functions within a bounded state-RKHS class. We introduce \\textbf{KOVI-Proj}, an optimistic value-iteration scheme that explicitly projects the optimistic proxy back into the state-RKHS ball at every step, ensuring that the uniform bound applies throughout the learning process. Under a restricted Bellman-embedding assumption (bounded conditional mean embeddings), KOVI-Proj enjoys a high-probability no-regret guarantee whose rate is governed by the task horizon and the kernel’s information gain. When the optimal value function lies in the chosen state-RKHS ball (realizability), the regret is sublinear; in the agnostic case, an explicit approximation term reflects the best RKHS approximation error. Overall, this work provides a new pathway to no-regret kernel RL that is strictly weaker than optimistic closure and avoids covering-number penalties.", "tldr": "kernals, RL, closure", "keywords": ["RL", "kernel", "theory"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de6e2e45a77f0d44656062d42ae0a89a82ab0105.pdf", "supplementary_material": "/attachment/b8b81ba3a2935f284f6c1e5f020ef8fa17637fb0.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a general algorithm for kernel-based episodic reinforcement learning. The method uses the framework of conditional kernel mean embeddings to construct approximations for value functions to guide an RL agent. These approximations use an upper confidence bound (UCB) over the state-action value function Q derived from confidence regions in a reproducing kernel Hilbert space (RKHS). The UCB approximation of Q is then projected onto a state-RKHS ball of radius B to construct the state-value function approximation for the next round, and the algorithm proceeds iteratively over episodes. Theoretical guarantees on the value function approximations and the algorithm's regret with respect to the optimal policy are presented. Experimental evaluations demonstrate the method and assess its regret compared to other kernel-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is mostly well written and addresses a relevant problem in the literature of RKHS-based RL methods and their analysis. As the paper briefly mentions in its introduction, the link between kernel methods and neural networks in their infinite-width limit, given by the neural tangent kernel and its associated literature, makes the study of kernel-based methodologies useful beyond traditional kernel methods, with its theoretical analysis providing insights onto modern deep learning frameworks as well.\n\nAn extensive theoretical analysis is presented covering the main aspects of the methodology and its performance, with extensions to address potential limitations. In addition, experimental results illustrate how the method performs on toy problems corroborating the theoretical results."}, "weaknesses": {"value": "Some aspects of the presentation could be improved, especially considering readers less familiar with the kernelized RL literature. However, my main concern with the paper is regarding its theoretical results, as there seems to be a subtle, though critical, mistake in the proof of Lemma 3.2, which might make this lemma and the subsequent main results invalid, as they heavily rely on it.\n\nLemma 3.2 establishes a concentration bound for the pointwise approximation to the true conditional mean embedding $\\mu(z)$ given by the algorithm's estimator $\\hat\\mu_n(z)$. As $\\mu: Z \\to H_\\ell$ takes values in the possibly infinite-dimensional RKHS $H_\\ell$, typical pointwise approximation bounds for scalar-valued functions are not directly applicable. Hence, one would need to take careful additional steps to deal with the infinite-dimensional nature of the codomain. However, in the textual argument following Eq. 22 in Appendix E (proof of Lemma 3.2), the paper extends a concentration bound valid for fixed unit vectors $u \\in \\mathbb{S}$ to a bound over the supremum by claiming that as the upper bound does not depend on $u$, one can take the supremum over all $u \\in \\mathbb{S}$. This claim, in general, **does not hold**.\n\nThe bound in Eq. 22 can be seen as $$\\forall u \\in \\mathbb{S}, \\quad P[ |\\langle w_n, u \\rangle| \\leq b_n] \\geq 1 - \\delta,$$ where $w_n$ is a random vector taking values in the RKHS, and $b_n$ is a non-negative random variable that does not depend on $u$. One cannot directly extend this bound to $$P[\\forall u \\in \\mathbb{S}, \\quad |\\langle w_n, u \\rangle| \\leq b_n] = P\\left[ \\sup_{u \\in \\mathbb{S}} |\\langle w_n, u \\rangle| \\leq b_n\\right] = P[\\lVert w_n \\rVert \\leq b_n] \\geq 1 - \\delta,$$ as the paper seems to claim. As a simple counter-example, if $w$ is a random unit vector (i.e., $\\lVert w \\rVert = 1$ almost surely), one can usually establish a non-zero bound on $P[|\\langle w, u \\rangle| \\leq b]$ for any $b \\in (0, 1)$, but it is clear that $P[\\Vert w \\rVert \\leq b] = 0$ for any $b < 1$. Another way to state the issue is that the $u$ that achieves the supremum in Eq. 22 is a random variable, not fixed. Therefore, the claim extending the $u$-dependent bound in Eq. 22 in the proof of Lemma 3.2 to a uniform bound is invalid. It is possible, however, that the main result remains valid at least in some similar form (e.g., Thm. 1 in Chowdhury & Oliveira, 2023), but that would require a potentially very different proof or perhaps a different set of assumptions.\n\nAs a lesser issue, it is not very clear from the start that the paper is also not making an \"optimistic closure\" assumption, since the proposed method still uses functions $V \\in H_\\ell$ with bounded norm $\\lVert V \\rVert \\leq B$, which reads like the optimistic closure assumption in Chowdhury and Oliveira (2023, Assumption 1). The distinction is only clarified later when the method is introduced and it is shown that the UCB-based approximations are projected onto the ball of radius B, whereas Chowdhury and Oliveira assumed that the UCB-based value function approximations themselves were always within the ball. I believe that distinction is crucial and should be explained earlier in the paper, perhaps by providing further background on what is meant by optimistic closure to the (possibly unfamiliar) reader. The paper should also have a more detailed discussion on why that assumption is possibly problematic, so that the paper's significance and potential impact can be strengthened.\n\nMinor issues:\n* In point 2 in the contributions list, it is stated that the bound holds for \"all V in the ball\", but that's redundant, as it's already implied by the supremum.\n* \"Assumption 3.2\", which does not exist, is mentioned in the statement of Lemma 3.2, but I believe that was referring to be Assumption 2.1.\n* The first name of the first author in Chowdhury and Gopalan (2017) and both authors in Chowdhury and Oliveira (2023) are incorrect."}, "questions": {"value": "Please, refer to the issues pointed in the weaknesses section above. My main concern is with the theoretical result in Lemma 3.2. Have the authors considered alternative proofs, which might not suffer from the same issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rZZ2f3SKO2", "forum": "BoFQUFmXIz", "replyto": "BoFQUFmXIz", "signatures": ["ICLR.cc/2026/Conference/Submission13093/Reviewer_oSzx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13093/Reviewer_oSzx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760528274076, "cdate": 1760528274076, "tmdate": 1762923818549, "mdate": 1762923818549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a variant of optimism in face of uncertainty for kernel-based RL. Leveraging a combination of conditional mean embedding (CME) perspective and projection into a bounded space, the authors achieve sublinear regret that depends on standard information gain factors as well as the bounds on CME and function spaces."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The authors improves over the existing literature for kernel-based RL: 1) they achieve sublinear regret instead of possibly vacuous bounds obtained through covering arguments; 2) they avoid optimistic closure assumptions by adding a projection step in the algorithm."}, "weaknesses": {"value": "* The projection step comes at the cost of solving a QP optimization. While the authors claim that this additional complexity can be properly managed, it could reduce the practicality of the proposed method (see questions below).\n* The paper does not seem to have major technical novelty wrt to existing work both in kernel-based RL or linear MDP literature (see questions below)."}, "questions": {"value": "1. At the best of my understanding, Proposition 3.1, Lemma 3.2 and Theorem 3.3 can be largely extracted from (Chowdhury & Olivera, 2020/2023). On the other hand, the key contribution of this paper is the projection in step3 of the algorithm, which allows replacing the optimistic closure assumption with an enforced one. If the above is correct, it’d be useful to understand what are the critical steps in the overall proof that are affected by changing from the assumption to the projection. Does it require introducing any specific treatment? Just to be clear, I’m asking about the technical challenges here, I totally appreciate the conceptual advantage of removing the assumption.\n2. Re the algorithmic approach, it’d be helpful if the authors would review the algorithms used in the parametric/linear case, where assuming bounded features and enforcing bounded weights by L2 projection is a fairly common approach.\n3. I’d like the authors to further clarify the assumptions/requirements for the projection step to work \"appropriately”. In the paper, there is a generic reference to \"anchor states”, but I suspect that a too small set or not “good” samples could compromise the accuracy of the project step and the boundedness property needed for the theory to work.\n4. I’m curious to have the authors’ opinion on the following: Let’s consider the algorithm without projection and let’s just add the assumption that the optimal value function is actually bounded. In practice, I guess the algorithm would still be consistent and after a sort of burn-in time, as estimates converge towards the optimal value, the boundedness that is ensured by the projection in the proposed algorithm would naturally occur by the dynamics of the algorithm. As a result, I would expect an algorithm without projection to incur a larger initial regret but “eventually” align with the same regret trend of the proposed algorithm. Is this correct? This does not seem to be the case in the experiments, where KOVI0 keeps a larger regret trend. Nonetheless, the amount of episodes considered in the experiments may be too small to see that effect.\n\nMinor\n* L043: introduce the definition of z\n* L065: introduce the definition of n\n* Fig.1 KOVI0 is barely visible.\n* L357 It seems like there is issue with the formatting"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PCGfMFioJr", "forum": "BoFQUFmXIz", "replyto": "BoFQUFmXIz", "signatures": ["ICLR.cc/2026/Conference/Submission13093/Reviewer_xxAq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13093/Reviewer_xxAq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761139158486, "cdate": 1761139158486, "tmdate": 1762923818140, "mdate": 1762923818140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses RL with kernel-based value function approximation. This paper works on a set of proxy value functions whose Bellman average admits a conditional mean embedding in the RKHS space (Assumption 2.1). This assumption generalizes the optimistic closure assumption in the previous work. This paper provides regret analysis of the proposed algorithm as well as numerical results."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "(+) The proposed framework generalizes the optimistic closure assumption in the previous work."}, "weaknesses": {"value": "(-) The writing and the organization of this paper make it hard to read. I think that presenting the algorithm in pseudo-code can improve the readability of Section 4. Besides, I do not think Sections 7, Broader Impact, Reproducibility, and Ethics Statement should be included in the main text. Also, Section 8 seems to be redundant.\n\n(-) The numerical experiment section lacks baseline methods, e.g., the CME-RL algorithm in the work of [1]"}, "questions": {"value": "Q1. What is Assumption 3.2 in Lemma 3.2?\n\nQ2. Maybe I am missing some details, but is Step 3 (Line 216) the major difference between the proposed KOVI-Proj algorithm and the CME-RL algorithm [1]? Under the optimistic closure assumption, CME-RL can perform closed-form estimation of the value functions. Instead, the KOVI-Proj algorithm utilizes projection to achieve that.\n\n[1] Chowdhury and Oliveira. Value function approximations via kernel embeddings for no-regret reinforcement learning. ACML 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F20tG0suty", "forum": "BoFQUFmXIz", "replyto": "BoFQUFmXIz", "signatures": ["ICLR.cc/2026/Conference/Submission13093/Reviewer_kzb7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13093/Reviewer_kzb7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761784579915, "cdate": 1761784579915, "tmdate": 1762923817606, "mdate": 1762923817606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses an online reinforcement learning problem where functions in an RKHS are given as a function class. Unlike the previous analyses, this paper only assumes that the transition distribution can be expressed by the function class."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The analysis is conducted under the weakest assumption for kernel-based RL methods."}, "weaknesses": {"value": "1. There are multiple parts in the proof that seem incorrect:\n\n- The definition of $\\varepsilon_ B$ is defined as $\\inf \\lVert V - V_ h^* \\rVert_ \\infty$ over $\\lVert V \\rVert_ {\\mathcal{H}_ {\\ell}} \\le B$ in Theorem 5.1, which is interpreted as the approximation error. However in Appendix D, it is replaced with $\\sup$ instead of $\\inf$, and the fact that it is defined as $\\sup$ is important in Lemma D.2. If $\\sup$ is used, it simply means that the given function class only contains functions that are close to $V^* $, so using any function would guarantee $HT \\varepsilon_ B$ regret.\n\n- The equation in between Eq (38) and Eq. (39) (line 1431) in the proof of Lemma G.8 doesn't seem right. The first term is a scalar but the last term is an element in $\\mathcal{H}_ {\\ell}$. I am also not sure why $\\sum_ {i=1}^n \\alpha_ i(z) (\\mu(z) - g^* (z))$ is zero. Partially due to this inconsistency, Lemma G.8 is nearly impossible to verify.\n\n- I don't understand why the inequality in Lines 1108-1111 is true. It doesn't seem true for the 1-d linear case (k(a, b) = ab) with $n = 1$, reducing to $\\frac{(z_ 1 z)^2}{\\rho + z_ 1^2} \\le \\frac{z^2}{\\rho + z_ 1^2}$, but there is no constraint that implies $z_ 1^2 \\le 1$. The same issue for line 1266.\n\n- In Lemma E.2, I do not see why $\\mathcal{M}$ is a supermartingale. The form looks different from what is studied in Chowdhury & Gopalan (2017).\n\n- In the equation block in Line 1258, $\\lVert K(\\cdot, z) \\rVert_ {\\mathcal{H}_ {k, I}}$ is not defined since $K(\\cdot, z)$ is not in $\\mathcal{H}_ {k, I}$. The following logic assumes that it is bounded by 1, but I don't see why.\n\n- There seems to be a slight mismatch between Eq. in line 1223 and line 1227. The regularization should be imposed on $\\mathbf{C}$ for line 1227 to be true as in Lemma G.6. I do not see why Remark G.7 is true when $\\lVert \\mathbf{C} \\rVert_ {\\mathcal{H}_ l^ n}$ and $\\lVert h \\rVert_ {\\mathcal{H}_ {k, I}}$ could be different, which also affects Lemma G.3.\n\n- In Remark D.3/D.4, while the argument is correct when the expectation is taken, but the difference between the expectation and the actual sample is not addressed (difference between $\\mathbb{E}[ \\tilde{Q}_ {h, t}(z_ {h, t}) ]$ and $\\tilde{Q}_ {h, t}(z_ {h, t})$).\n\n2. The paper is highly unorganized.\n\nFormatting issues:\n\n- Broader impact statement is in between Sections 6 and 7, and Section 7 is LLM Usage statement. Section 8 is empty, then the main text continues to Section 9.\n\n- Last three sections of the appendix are all titled \"Additional Results\".\n\n- The same paper (Muandet et al. 2017a, 2017b) is cited twice in the references.\n\n- Remark 3.4 and Remark (v) in Section 4 overlap.\n\nIn addition, the proof is presented in a way that makes it unnecessarily hard to follow.\nMany times, some steps (either simple or important) are located far away from the main proof, for example in remarks. Sometimes the same fact is proved multiple times here and there.\n\n- Remarks D.3 and D.4 explain the same thing twice.\n\n- Remarks D.6-D.9 are simple steps (e.g., bounding $n_ {h, t} \\le HT$, applying Cauchy-Schwarz inequality) that could have been explained within the main proof, but they are separated as remarks, making it hard to follow.\n\n- Definition G.1, Lemma G.3, Lemma G.6, and Remark G.7 are essentially about the same thing: obtaining the closed form of the projection, which could have been compressed into one lemma.\n\n- In Appendix H, the whole section is devoted to taking the union bound over $HT$."}, "questions": {"value": "Typos:  \nLine 165, should be Assumption 2.1 instead of 3.2.  \nThe title of Appendix C is Proof of Theorem 1, but it should be Proposition 3.1 instead."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q47AICfI3l", "forum": "BoFQUFmXIz", "replyto": "BoFQUFmXIz", "signatures": ["ICLR.cc/2026/Conference/Submission13093/Reviewer_cj7K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13093/Reviewer_cj7K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848001632, "cdate": 1761848001632, "tmdate": 1762923817222, "mdate": 1762923817222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}