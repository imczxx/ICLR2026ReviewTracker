{"id": "DinXMuw6ED", "number": 17568, "cdate": 1758277662857, "mdate": 1759897167199, "content": {"title": "Aya Vision: Advancing the Frontier of Multilingual Multimodality", "abstract": "Building multimodal language models is fundamentally challenging: requiring alignment of vision and language modalities, curating high-quality instruction data, and preserving existing text-only capabilities once vision is introduced. These difficulties are further magnified in multilingual settings, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address these issues, we propose: (1) a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data across many languages; (2) a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Together, these contributions yield \\textbf{Aya Vision}, a family of open-weights multilingual multimodal models (8B and 32B) that achieve \\textbf{leading performance across both multimodal and text-only tasks}, outperforming significantly larger models. Our work provides guidance and reusable components for scalable multilingual data curation, robust multimodal training, and advancing meaningful evaluation in multilingual multimodal AI.", "tldr": "We release the recipe to build state of the art multilingual multimodal models", "keywords": ["deep learning", "multimodal learning", "vision-language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a00d145aef7d083db5f4214a81f55e09ed3c256a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Aya Vision, a family of open-weight 8B and 32B multilingual multimodal models (MLLMs) designed to address key challenges in non-English vision-language understanding. The authors identify two primary obstacles: (1) the scarcity of high-quality, diverse multilingual data, as simple machine translation often introduces errors and \"translationese\", and (2) the \"catastrophic forgetting\" of text-only capabilities that occurs when vision is introduced. To solve these problems, the paper proposes two main contributions: A Multilingual Multimodal Synthetic Annotation Framework and a Cross-Modal Model Merging. The paper also introduces new benchmarks, including Aya VisionBench, for evaluating open-ended multilingual generation"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper’s most significant contribution is the cross-modal model merging technique. It provides an elegant, simple, and training-free solution to the well-documented problem of catastrophic forgetting.\n2. The authors provide a thorough evaluation by not only testing multimodal performance but also rigorously assessing text-only capabilities, which is critical given the paper's motivation."}, "weaknesses": {"value": "1. The paper criticizes machine translation for causing cultural bias and misalignment but still relies on a translation-based pipeline, i.e., machine translation + LLM post-editing. While this improves fluency, it fails to address the core cultural issue—a translated American-centric caption remains American-centric. Although Section P mentions limited image re-rendering, the method itself does not bridge the cultural-context gap it highlights.\n2. The data-balance ablation (Figure 8) suggests issues with the quality of the synthesized multilingual data. Performance peaks at 35% multilingual data and drops at 67%, which the paper attributes to upsampling and the need for diverse English data. This implies the synthesized multilingual data is inherently less diverse or useful than English, undermining the claim of a “high-quality, diverse multilingual multimodal instruction data” framework.\n3. One of my main concerns is that the paper claims to “set a new standard for multilingual performance,” yet its results do not consistently support this. In Table 1, Aya-Vision-8B scores 46.16 versus Qwen-2.5-VL-7B’s 50.00, and the gap widens at larger scales (Aya-Vision-32B: 52.81 vs. Qwen-2.5-VL-72B: 59.72). Despite claims of being “optimized for open-ended generation,” the model underperforms on structured tasks like VQA (CVQA and MTVQA) and reasoning."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GKBcGx6Q2R", "forum": "DinXMuw6ED", "replyto": "DinXMuw6ED", "signatures": ["ICLR.cc/2026/Conference/Submission17568/Reviewer_kmAj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17568/Reviewer_kmAj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841432801, "cdate": 1761841432801, "tmdate": 1762927429222, "mdate": 1762927429222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AyaVision, multilingual multimodal LLMs (8B/32B) that handle up to 23 languages. The motivation is to address three challenges: (i) data scarcity and translationese to train multilingual MLLMs, addressed via a synthetic data engine that enriches the annotations with additional context about he images, (ii) text-only capability loss after adding vision, which is addressed via a training-free cross-modal weight-merging method, (iii) lack of open-ended multilingual evaluation, addressed via new benchmarks (AyaVisionBench, m-WildVision and xChatBench). The paper reports strong preference win rates on the proposed open-ended AyaVisionBench."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper makes a meaningful data contribution by using a pipeline that combines distillation, filtering, and context-aware rephrasing to reduce translation artefacts. The data engine also increases lexical diversity and sequence length of instruction pairs, which is quantified by gains and results in an automatic quality improvement on COMET.\n\n- The paper also makes a meaningful benchmark contribution, it proposes three benchmarks: AyaVisionBench,  m-WildVision and xChatBench offer open-ended, multimodal, multilingual evaluation. This extends evaluation beyond multiple-choice or english-only setups and should help standardize evaluation for future multilingual multimodal work.\n\n- The training-free cross-modal merging method is simple and effective, as shown in the ablations. The reported deltas show large recovery on text-only win rates and non-trivial gains on multimodal win rates."}, "weaknesses": {"value": "- The model shows large gains on the two proposed benchmarks, AyaVisionBench and m-WildVision, measured as open-ended pairwise win rates averaged across 23 languages. However, on standard accuracy-scored benchmarks, its performance is clearly below strong baselines. The paper does not explain why a model that improves under open-ended evaluations falls behind by 6 to 17 points on benchmarks like xMMMU, CVQA, and MTVQA. If this gap comes from answer-format constraints (for example, multiple choice or one-word responses) or a mismatch between free-form outputs and the expected scoring format, the paper should include a simple normalization step that maps free-form text to the required label, for example, an LLM-based post-processing that converts a sentence to option B or to the exact short answer. Without such a comparison, it is hard to tell whether the wins on AyaVisionBench reflect alignment with the response style used in training and evaluation, rather than improvements that transfer to widely used benchmarks.\n\n- The paper mixes several motivations, but the evidence for each one is narrow. The paper claims translationese in training data is a major problem in data annotation methods of existing multilingual MLLM and proposes translation plus LLM rephrasing. Yet it does not show that prior datasets have this issue (quantitatively or qualitatively), nor that the proposed method fixes it beyond a COMET jump. There is no human verification/alignment and no comparison to a simple off-the-shelf LLM translation, so it is unclear whether any benefit comes from the proposed method or from the tools used. The paper also argues that text-only performance worsens when additional languages are added in multilingual multimodal performance and uses linear weight merging, but this relation is not explained, and there is no evidence that higher text-only scores lead to higher multimodal accuracy. Altogether, the motivations feel mixed. \n\n- The citation format does not match the official ICLR citation requirement. \n\nOverall, the main technical contribution is not supported by clear evidence, and its novelty is limited. The method is a simple linear weight mix between the text-only LLM and the multimodal model while keeping vision modules fixed. The paper does not examine basic design alternatives or stronger baselines that would test whether this is the right choice, nor does it provide clear guidance on when and why the method helps. At the same time, the performance improvements are only seen in the pairwise LLM-judge win rates on proposed open-ended benchmarks, while scores on widely used benchmarks are lower than the baselines."}, "questions": {"value": "Please explain why the scores on CVQA, MTVQA, and xMMMU are lower. Is the main cause answer-format mismatch, short-answer matching, domain differences, or lower model accuracy? If it is mostly formatting, please say whether a small LLM post-processing step that maps free-form outputs to the exact expected label would raise the scores, and by roughly how much."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tuZB9hZ8Jv", "forum": "DinXMuw6ED", "replyto": "DinXMuw6ED", "signatures": ["ICLR.cc/2026/Conference/Submission17568/Reviewer_55FY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17568/Reviewer_55FY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936865887, "cdate": 1761936865887, "tmdate": 1762927428879, "mdate": 1762927428879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Aya Vision, a family of multilingual multimodal large language models (8B and 32B parameters) designed to handle both image and text inputs across 23 languages. The authors propose two main ideas: **(1)** a synthetic multilingual data generation and translation pipeline, and **(2)** a cross-modal model merging technique to preserve text-only performance after multimodal training. The paper also presents AyaVisionBench, a new benchmark for evaluating multilingual multimodal models. Experimental results show Aya Vision performing reasonably against several existing open-weight models and stronger on its own proposed benchmark."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The following are the strengths of the paper:\n\n1. The paper tackles an important problem of extending multimodal models beyond English, making progress toward inclusive and global AI systems.\n\n2. The effort to maintain text-only capability while introducing visual understanding is valuable, as it tries to address the catastrophic forgetting issue common in multimodal training. The cross-model merging approach is interesting and if works in general, could be very useful for MLLM post training."}, "weaknesses": {"value": "The following are the weaknesses of the paper:\n\n1. Many important implementation details are missing or unclear. For example, how dataset categories were selected and sampled to reach 2.29M samples (lines 126-128) is not explained. The process of “regularization” and sampling remains undefined.\n\n2. The keyword-based filtering step is unclear. The “curated list of keywords” is never provided, making it unclear how such filtering can reliably detect errors in generated descriptions (refer to lines 156-160).\n\n3. In Stage 2: LLM-based semantic filtering, the LLM-based semantic filtering is a weak proxy since the LLM cannot see images. As stated, complex samples are often discarded, which limits the model’s ability to learn complex reasoning tasks.\n\n4. The hybrid translation pipeline lacks justification. Simply chaining NLLB and command-r-plus does not guarantee better quality, and the paper does not explain why this choice was made over stronger multilingual multimodal translation models such as LLaMA-4 or Qwen3. The subset of data chosen for translation (lines 182–183) is also not described.\n\n5. There seems to be data size inconsistencies.  Synthetic re-annotation (3.5M of 2.29M original) and downsampled original data (3.7M from 6M) do not match the claimed final training set size of 2.75M samples. How is this 2.75M number exactly calculated?\n\n6. The cross-model merging method is interesting but conceptually unclear. Theoretically, the text-only and multimodal models occupy different weight spaces, and the paper does not justify how linear interpolation of weights can work reliably.\n\n7. AyaVisionBench cannot be considered a major contribution. All its details are buried in the appendix, and it is small, only 135 samples per language (around 3K total). The comparison with other multilingual multimodal benchmarks is missing. In fact, stronger public benchmarks like “All Languages Matter (ALM-Bench) [1]” already exist, covering 100 languages and 22K samples.\n\n8. Aya Vision performs poorly on academic and structured benchmarks (Table 1), with models such as Qwen2.5-VL outperforming it consistently. Its strong results appear only on its own benchmark, which weakens the claim of “best-in-class performance.”\n\n9. Many components depend on proprietary LLMs (command-r-plus, GPT-based judges) and claim “human-preferred” outputs without any real human evaluation. For example, as show in Appendix K, simply asking LLM/MLLM to generate \"human-preferred\" does not mean that the resulting text is \"human-preferred\".\n\nOverall, the paper lacks clarity, key methodological details, and fair validation. The improvements shown are narrow and not convincingly demonstrated beyond the proposed benchmark.\n\n---\n\n[1] Vayani, A., Dissanayake, D., Watawana, H., Ahsan, N., Sasikumar, N., Thawakar, O., Ademtew, H. B., Hmaiti, Y., Kumar, A., Kuckreja, K., et al. (2024). All languages matter: Evaluating LMMs on culturally diverse 100 languages. CVPR 2025."}, "questions": {"value": "Please refer to the Weaknesses section for detailed points requiring clarification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xEpslKnnPT", "forum": "DinXMuw6ED", "replyto": "DinXMuw6ED", "signatures": ["ICLR.cc/2026/Conference/Submission17568/Reviewer_6eZf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17568/Reviewer_6eZf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936898520, "cdate": 1761936898520, "tmdate": 1762927428377, "mdate": 1762927428377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose Aya Vision, a model family of multilingual multimodal open models. The authors also propose a new framework for synthetic annotation of multilingual multimodal instruction tuning data. The authors also introduce a new benchmark AyaVisionBench, a multilingual multimodal benchmark."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The authors introduce a strong set of models Aya Vision, achieving state of the art performance on multilingual multimodal benchmarks. \n- The authors performed comprehensive experiments, comparing various different models against Aya Vision, demonstrating the effectiveness of their models.\n- The proposed new framework of generating synthetic annotation could help scale up multilingual multimodal training by increasing the amount of data that could be gathered for large scale training.\n- The authors constructed a new benchmark AyaVisionBench, which has a broad coverage of language and topics that could comprehensively evaluate multilingual multimodal models on generation quality.\n\nAll these resources are very helpful to the multilingual community."}, "weaknesses": {"value": "- The authors employed LLM as a judge approach for evaluation of win rate, which could be very unreliable. It would be good if they could have alternative evaluation format or perform a small set of manual evaluation to verify the results.\n- The authors claim that they proposed a novel multimodal merging technique, yet multimodal merging is commonly discussed in literature. It would be good if the authors could compare their work against existing work that discuss multimodal merging.\n- The synthetic data relies on expensive proprietary models. It would be great if the authors could compare with existing open models and demonstrate on par performance."}, "questions": {"value": "- What are the computational resources required to train the Aya Vision model family?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3xDwkot3HI", "forum": "DinXMuw6ED", "replyto": "DinXMuw6ED", "signatures": ["ICLR.cc/2026/Conference/Submission17568/Reviewer_ibxe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17568/Reviewer_ibxe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975627732, "cdate": 1761975627732, "tmdate": 1762927428032, "mdate": 1762927428032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the important challenge of building multilingual multimodal language models, focusing on data scarcity, translation distortion, and catastrophic forgetting when integrating visual understanding into multilingual LLMs. To tackle these issues, the authors introduce (1) a synthetic annotation framework for high-quality multilingual multimodal data and (2) a cross-modal model merging technique to preserve text-only capabilities while enhancing multimodal generation. The resulting Aya Vision models (8B and 32B) demonstrate strong performance on both text-only and multimodal benchmarks, surpassing larger counterparts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a highly relevant and timely problem. Building multilingual multimodal systems remains a core challenge for the next generation of foundation models, and the proposed dataset construction framework provides valuable insights for the community.\n\n2. The writing is clear, well-structured, and easy to follow. The motivation and contributions are concisely articulated, making the work accessible to a broad audience."}, "weaknesses": {"value": "Major Concerns\n\n1. Clarity on Key Concepts (Lines 62–63): The terms context-aware and human-preferred are mentioned as central ideas but are not elaborated in Section 2. The paper should explicitly explain how these notions are implemented or reflected in the proposed framework, particularly the human-preferred aspect.\n\n2. Justification for Translation Choice (Lines 171–187): It remains unclear why the NLLB-3.3B model is preferred over GPT-based translation. The paper lacks empirical or qualitative evidence supporting this decision. A brief comparative evaluation or rationale would strengthen the argument.\n\n3. Missing Experimental Analyses:\n\n- The paper notes that Aya-Vision is built upon multilingual post-trained base models. The authors should discuss why this multilingual prior is essential and whether similar results can be achieved by directly fine-tuning strong visual-language models such as Qwen-VL.\n\n- Figure 9’s ablation fails to demonstrate the effectiveness of the proposed context-aware rephrasing step. The authors should include a comparison with models trained on unprocessed data to quantify its benefit.\n\n- There is little discussion on alternative model-merging strategies. Incorporating or at least discussing comparisons with TIES-Merging [1] and DARE [2] would provide deeper insight into the design choices.\n\nMinor Issues\n\n1. The Related Works section should not be confined entirely to the Appendix; key discussions should appear in the main text to improve context and readability.\n\n2. Appendix C merely lists supported languages without justifying their selection. The authors should briefly explain the inclusion criteria or regional balance.\n\n3. Appendix J introduces SafeGuards, but the main text does not reference or evaluate safety performance. A clear link to this appendix and a short discussion in the results section would help readers understand its relevance.\n\n[1] TIES-Merging: Resolving interference when merging models. NeurIPS 2023.\n\n[2] Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch. ICML 2024."}, "questions": {"value": "Please see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ul1LFwW0S2", "forum": "DinXMuw6ED", "replyto": "DinXMuw6ED", "signatures": ["ICLR.cc/2026/Conference/Submission17568/Reviewer_KQV9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17568/Reviewer_KQV9"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988151315, "cdate": 1761988151315, "tmdate": 1762927427659, "mdate": 1762927427659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response by Authors"}, "comment": {"value": "We would like to express our sincere gratitude to all the reviewers for their thoughtful and constructive feedback on our work. We are greatly encouraged by the positive reception and are pleased that the reviewers recognized our work as a step toward \"inclusive and global AI systems\" [6eZf], acknowledging our \"excellent\" [ibxe] and \"meaningful\" [55FY] contributions to the community.\n\nWe are particularly heartened that reviewers recognized the core strengths of our work, including:\n- The effectiveness of our cross-modal model merging technique as a \"simple and effective\" [55FY], \"elegant\" [kmAj], and \"interesting\" [6eZf] solution to catastrophic forgetting.\n- Our synthetic annotation framework for generating high-quality data, which provides \"valuable insights\" [KQV9] and represents a \"meaningful data contribution\" [55FY].\n- The \"strong results\" [KQV9] and \"state of the art performance\" [ibxe] of our models, often \"surpassing larger counterparts\" [KQV9].\n- The contribution of our new resources, including AyaVisionBench, which are \"very helpful to the multilingual community\" [ibxe] and help \"standardize evaluation\" [55FY].\n- The overall quality of the manuscript, described as \"clear, well-structured, and easy to follow\" [KQV9].\n\nBelow, we address the major themes raised across reviews regarding benchmark discrepancies, data pipeline design, and the merging technique.\n\n### Academic Benchmarks vs. Human Preference & New LMSys Results\n\nReviewers noted a gap between Aya Vision’s strong performance on open-ended evaluations (AyaVisionBench, m-WildVision) and its lower scores on strict-format academic benchmarks (xMMMU, CVQA) compared to larger models. We clarify that this difference reflects Aya Vision’s optimization objective: the model is trained to produce fluent, helpful, instruction-following responses rather than the short, exact-match answers emphasized in traditional VQA benchmarks. As discussed in our individual responses, such benchmarks often penalize valid, natural-language answers that do not exactly match a reference string, even when they are semantically equivalent.\n\nTo further validate that this design choice leads to better real-world utility, we submitted Aya Vision to the LMSys Chatbot Arena for blind human evaluation. The results strongly support our approach:\\\n**Aya Vision 32B (ELO: 1097): Outperforms Molmo 72B (1076) and Llama-3.2-90B (1069).**\\\n**Aya Vision 8B (ELO: 1069): Matches Llama-3.2-90B, despite being ~10x smaller.**\n\nThese results demonstrate that Aya Vision sets a new standard for efficiency and user preference within the open-weight model category. Moreover, comparisons to large-scale baselines such as Qwen-2.5-VL-72B should be interpreted in light of compute efficiency. Qwen models are trained on approximately 4 trillion tokens, whereas Aya Vision uses fewer than 40 billion, about 100 times less data, making its performance highly competitive relative to its compute scale. \n\nAs detailed in Appendix I, training Aya Vision 32B requires a total of 8192 H100 GPU hours. On 128 GPUs, this corresponds to 64 hours (2.66 days). Assuming identical throughput and scaling to 1024 GPUs, training Qwen-2.5-VL-72B would take approximately $64\\times100/(8\\times24)$ = ~33.3 days.\n\n### Rationale for the Hybrid Data Pipeline\n\nReviewers requested clarification on our decision to use a hybrid translation pipeline (NLLB + LLM rephrasing) instead of relying solely on either machine translation (MT) or large language model (LLM) translation. We adopt the hybrid approach to balance faithfulness and fluency: pure LLM translation often introduces hallucinations or grammatical inconsistencies in low-resource languages, while pure MT systems tend to produce rigid, unnatural “translationese”. By first generating faithful translations with NLLB and then applying LLM-based rephrasing, we combine the semantic accuracy of MT with the naturalness of LLM outputs.\n\nEmpirically, this hybrid design yields clear benefits.\\\n**Data Level**: We observe consistent COMET score improvements across all 23 languages (0.75 $\\to$ 0.83 avg) and a massive increase in lexical diversity (MTLD 11.0 $\\to$ 61.2).\\\n**Model Level**: Figure 9 demonstrates that training on this processed data yields a 17.2% increase in multilingual vision win rates compared to unprocessed data.\n\n### Novelty and Positioning of Cross-Modal Merging\nReviewers asked for comparisons to general merging methods (e.g., TIES) and clarification on novelty.\nWe clarify that our contribution is not a generic merging algorithm, but a specific cross-modal strategy tailored to mitigate catastrophic forgetting. We demonstrate that simple interpolation between a multilingual text backbone and its multimodal continuation restores text capabilities and improves multimodal generation without additional training. We view this as orthogonal to complex weight-merging schemes like TIES, offering a lightweight solution specifically for the modality gap."}}, "id": "FanWAKQari", "forum": "DinXMuw6ED", "replyto": "DinXMuw6ED", "signatures": ["ICLR.cc/2026/Conference/Submission17568/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17568/Authors"], "number": 26, "invitations": ["ICLR.cc/2026/Conference/Submission17568/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763657371693, "cdate": 1763657371693, "tmdate": 1763657371693, "mdate": 1763657371693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}