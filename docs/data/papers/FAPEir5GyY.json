{"id": "FAPEir5GyY", "number": 10298, "cdate": 1758166405340, "mdate": 1759897660078, "content": {"title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo", "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-Dojo, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a benchmark for executable-agent learning, CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems.", "tldr": "", "keywords": ["capture the flag", "llms", "security"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2b0bd01b03374946591d7dc0c15760d051f6eeb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents CTF-DOJO, a large-scale executable environment for Capture-The-Flag (CTF) tasks, and CTF-FORGE, an automated pipeline that converts public CTF materials into reproducible Dockerized challenges. Using DOJO, the authors collect a small but high-quality set of execution-validated trajectories (≈486) and fine-tune open-source base models (primarily Qwen3 8B/14B/32B) with rejection-sampling SFT. The resulting agents achieve competitive Pass@1 on three benchmarks (InterCode-CTF, NYU CTF Bench, Cybench), with the 32B variant around the low-30% Pass@1 range, while showing strong data efficiency. The paper also studies factors such as environment randomization, leveraging writeups as non-citation hints, and teacher-model diversity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) Valuable infrastructure: A reproducible, containerized CTF environment at scale and an automatic “forge” pipeline are strong contributions likely to benefit the community.\n2) Executable supervision: Training from execution-validated traces is a principled way to reduce hallucinations and reward specification issues common in security tasks.\n3) Data efficiency: Competitive numbers are achieved with a surprisingly small number of high-quality trajectories (~486), highlighting the utility of executable signals.\n4)The collection setup (agent loop, rollout budget), training recipe (RSFT), and evaluation benchmarks are described in a way that practitioners can reproduce."}, "weaknesses": {"value": "1) Methods relying on more abundant synthetic trajectories  report higher Pass@1 in similar settings. The paper emphasizes efficiency, but direct apples-to-apples comparisons at fixed data budgets would clarify the trade-offs.\n2) Benchmark comparability details: Small differences in patched tasks, scaffolding, or decoding hyperparameters can materially affect Pass@1. A fully standardized evaluation harness (same temperatures, retries, timeouts, and tool stacks) would aid comparability to prior work.\n3) Shaping and sensitivity depth: The paper discusses helpful training signals (execution outcomes, writeups), but provides limited sensitivity on shaping choices (e.g., negative/partial rewards, step-wise curriculum) and alternative supervision (e.g., learned difficulty signals, self-reflection)."}, "questions": {"value": "1) Can you provide matched-data comparisons against larger synthetic-trajectory approaches (e.g., train both with 500, 1k, 2k examples) to quantify sample efficiency directly?\n2) What steps ensure no answer leakage from writeups and trajectories to evaluation (e.g., time splits, near-duplicate filtering)?\n3) How sensitive are results to decoding settings (temperature, top-p), step budgets, and tool availability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NIK7xM8uKB", "forum": "FAPEir5GyY", "replyto": "FAPEir5GyY", "signatures": ["ICLR.cc/2026/Conference/Submission10298/Reviewer_js9Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10298/Reviewer_js9Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601703784, "cdate": 1761601703784, "tmdate": 1762921647187, "mdate": 1762921647187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed CTF-Dojo, which automatically transforms CTF challenges into Docker environments. By training agents on trajectories from these environments, the authors manage to achieve new open-weight state-of-the-art performance using a 32B model on InterCode-CTF, NYU CTF Bench, and Cybench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. An automatable pipeline to generate an agent environment for cybersecurity agents.\n2. Demonstrate better performance on representative benchmarks with better data efficiency compared to Cyber-Zero."}, "weaknesses": {"value": "1. Automatically creating the execution environment is not novel. For example, in SWE-smith, the authors use SWE-agent to automatically create a Docker given any GitHub repo.\n2. The scalability seems to be a bigger issue than discussed. As the author noted, each CTF challenge is uniquely designed, and the current pipeline can only leverage existing challenges instead of generating synthetic training instances from scratch. For comparison, SWE-smith includes the procedure of automatically generating issues to solve, which fundamentally removes the scalability issues. While I agree there is a trade-off between scalability and quality/diversity, the current framework seems extremely constrained by scalability.\n3. What is the impact of 4.1 and 4.2 on training? Like, what is the performance of trained agents using the baselines in 4.1 and 4.2?"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gU1defppbG", "forum": "FAPEir5GyY", "replyto": "FAPEir5GyY", "signatures": ["ICLR.cc/2026/Conference/Submission10298/Reviewer_TJVw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10298/Reviewer_TJVw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866141778, "cdate": 1761866141778, "tmdate": 1762921646841, "mdate": 1762921646841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper collects CTF cybersecurity competitions and forms them into a benchmark and training environment for LLM code generators. A software pipeline, including LLMs, for setting up the testing environment is constructed and shown to often work. Two models are tested in this environment and traces were collected to serve as a training dataset for other models. The authors show that training on this set as well as training on human generated advice/plans for the problems can increase model performance on this and other offensive cybersecurity benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- a sizeable collection of CTF competitions is collected for use in model testing\n- reasonable tests show that training on successful outputs from this benchmark generalize to other CTF competitions\n- The authors claim that setting up a CTF testing environment is difficult and time-consuming and they can automate the process with 98% accuracy\n- The authors show traces from their constructed environment are useful for training for other CTF problems.\n- The ethical implication of improving offensive cybersecurity capabilities are discussed."}, "weaknesses": {"value": "- It is unclear to me what environments the automated setup was tested on and whether this will hold true for many users and, especially given the use of LLMs, will continue to be reliable in the future.\n- It is unclear what the copyright status of the original CTF competitions is and whether the authors are ok with it's collection and use in bench-marking and training LLMs."}, "questions": {"value": "I have no additional questions or comments for the authors."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "- \"CTF-DOJO is the first environment designed to synthesize verified agent trajectories for training\nLLMs on offensive cybersecurity tasks involving vulnerability detection and exploitation\" Improving offensive cybersecurity capabilities is a potential security risk. The authors address this in the paper and I believe the work is ethical; however, I wanted to note it given my lack of experience in the area.\n\n- It is unclear what the copyright status of the original CTF competitions is and whether the authors are ok with it's collection and use in bench-marking and training LLMs."}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZD4ZLsDRHC", "forum": "FAPEir5GyY", "replyto": "FAPEir5GyY", "signatures": ["ICLR.cc/2026/Conference/Submission10298/Reviewer_YuRb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10298/Reviewer_YuRb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977186944, "cdate": 1761977186944, "tmdate": 1762921646390, "mdate": 1762921646390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on the lack of high-quality training data for LLM agents in Capture-The-Flag (CTF) tasks. Leveraging standardized, publicly available data, the authors developed a prompt-based pipeline to convert these artifacts into an executable and verifiable Docker environment named CTF-DOJO. Using this environment, they collected 486 successful trajectories for training. The experiments demonstrate the effectiveness and high data efficiency of training models on these execution-verified trajectories."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. High-quality training data is a critical factor influencing model performance, and the authors' insight on this point is correct.\n\n2. There has been no prior work on building such an environment specifically for training agents on CTF tasks. This paper makes a valuable contribution in this direction."}, "weaknesses": {"value": "1. The paper's core claim that training data must be \"execution-verified\" and \"correct\" is questionable. This methodology is contradicted by the paper's own results, which show that a model trained on a much larger, unverified (and presumably noisy) dataset outperforms the authors' model. This suggests that data quantity may be more important than the strict verification the authors insist upon. The justification of \"safety-critical domains\" for this training choice is also unconvincing.\n2. The technical contribution seems limited. The authors chose to build their system on a single, pre-standardized data archive (pwn.college) to avoid integration challenges. The core CTF-FORGE pipeline is a straightforward \"prompt-based\" workflow. A more significant contribution would have involved integrating and normalizing multi-source, heterogeneous data, rather than selecting the most convenient, pre-cleaned source.\n3. The paper's structure is poor. Section 2 is bloated with excessive, non-novel details about data sourcing and processing. These engineering details belong in the appendix. This padding makes the main body of the paper feel hollow and detracts from the space needed for core methodological arguments."}, "questions": {"value": "1.The constructed environment (CTF-DOJO) with its 650+ verified, executable challenges seems perfectly suited as a high-value evaluation benchmark. Why do the authors insist on framing this contribution as a training data solution? I understand the \"official\" reason that no such training datasets currently exist, but I am interested in deeper insights. Does framing this as a \"training problem\" unlock a specific scientific contribution that framing it as a \"benchmark\" would not?\n2.Regarding the alignment of CTF challenges and writeups, why was \"fuzzy matching\" necessary? One would expect a natural, direct mapping to exist between a specific challenge and its corresponding writeup.\n3.I encourage the authors to provide a rebuttal to the points raised in the \"Weaknesses\" section. I am willing to reconsider my score based on a compelling response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tjM7JsobvP", "forum": "FAPEir5GyY", "replyto": "FAPEir5GyY", "signatures": ["ICLR.cc/2026/Conference/Submission10298/Reviewer_9TRz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10298/Reviewer_9TRz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762109640857, "cdate": 1762109640857, "tmdate": 1762921645989, "mdate": 1762921645989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}