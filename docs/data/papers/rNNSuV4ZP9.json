{"id": "rNNSuV4ZP9", "number": 21859, "cdate": 1758322727392, "mdate": 1759896899590, "content": {"title": "CLIQ: Contrastive Learning with XAI-guided Interpretation and Model Quantization for EEG-based Emotion Recognition", "abstract": "Electroencephalogram (EEG) may be a promising way to recognize human emotions in contrast to outward expressions, which may be hidden or artificially simulated. This paper applies self-supervised learning (SSL) to process complex EEG signals with low amount of labeled data for solving emotion recognition task. Proposed approach is based on a convolutional encoder with a novel contrastive loss and batching function. It has been evaluated on SEED and DEAP datasets. We also compared different preprocessing techniques in temporal, frequency and temporal-frequency domains. We achieved fairly high accuracy even on small amount of labeled data with the best accuracy of 88.7% and 87,3% on SEED, and 95.3% and 63.1% accuracy on DEAP for subject-dependent and subject-independent evaluations, respectively. Additionaly, we performed feature analysis and found that the greatest inter-emotional difference was shown in the T7 and T8 channels. We validated these findings with an iterative application of DeepLIFT. Combined with model quantization, these insights enabled us to reduce data and model size without significant decrease of accuracy. The proposed approach achieved separable vector representations of EEG and performance compatible with SOTA, enabled insightful data analysis, model interpretation with reasonable data reduction, and efficient model quantization.", "tldr": "We introduce CLIQ, a contrastive learning approach with pairing and batching for EEG emotion recognition. We perform quantization, feature analysis and interpretation to identify 2 channels of high emotional difference and reduce data and model size.", "keywords": ["Emotion recognition", "EEG", "Self-supervised learning", "Contrastive learning", "XAI", "Quantization", "CNN"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab87e01f0619febf2be13f888274cc7ae76a56f3.pdf", "supplementary_material": "/attachment/e5b87b0466f212140cb4b4093dec8136eb14d44e.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a self-supervised EEG emotion recognition framework called CLIQ, designed to enhance classification accuracy and model efficiency through an improved contrastive learning strategy. Based on a convolutional encoder, the method introduces a modified negative sample construction and batching approach to better utilize limited labeled data. Experiments on SEED and DEAP datasets under both subject-dependent and subject-independent settings show that CLIQ achieves comparable or better performance than existing methods with fewer parameters and faster inference. The authors also employ feature visualization and post-training quantization to identify key EEG channels (e.g., T7, T8) and compress the model.\nOverall, the paper aims to build an end-to-end and interpretable EEG emotion recognition system, but its methodological novelty and experimental depth remain limited, representing mainly an incremental adaptation of existing contrastive learning techniques."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper applies self-supervised learning to EEG-based emotion recognition, a topic of practical relevance. The authors emphasize model efficiency and deployability by exploring channel selection and post-training quantization, while introducing a soft–hard distinction for negative pairs in contrastive learning. Overall, the strengths lie in the practical significance of the research direction and the completeness of the engineering implementation."}, "weaknesses": {"value": "1) The paper lacks clear structural and visual explanations of the proposed method. Key figures (e.g., Figure 2) are low-resolution raster images rather than vector graphics, making them blurry when enlarged. Moreover, the architecture diagram is overly simplified and does not adequately illustrate the relationships and data flow between core modules, which makes the overall framework difficult to follow.\n2) The experiments are conducted only on two relatively small public datasets, SEED and DEAP, which is insufficient to support the paper’s general claims. The lack of validation on larger or more diverse datasets limits the generality and robustness of the conclusions. In addition, some parts of the experimental setup appear to reuse configurations or feature settings from prior work without ensuring strict consistency in preprocessing or evaluation, raising concerns about the fairness of the comparisons.\n3) Based on the experimental description and partial code inspection, it seems that in the subject-independent (SI) setting, the same subject was fixed as the test set instead of performing random or cross-validated splits, which could introduce bias.\nFurthermore, for the SEED dataset, if data splitting is done at the sample level rather than by trial, there is a high risk of data leakage that could inflate performance. Although the authors mention using 10-fold and Leave-One-Subject-Out validation, the exact splitting criteria and randomization strategy are not clearly stated, leaving the experimental reliability in question.\n4) The proposed distinction between hard and soft negative pairs, with different weighting in the contrastive loss, is highly similar to existing approaches that perform sample re-weighting or hard-negative mining based on similarity or difficulty. The presented loss function essentially corresponds to a temperature-scaled weighted BCE formulation, without any new theoretical derivation or property analysis. Compared to the standard InfoNCE loss, the proposed modification appears to be an engineering tweak rather than a principled methodological innovation, and the reported performance gain is not substantial."}, "questions": {"value": "1) Please clarify the train/test partitioning strategy used in the subject-independent (SI) experiments. Was the same subject fixed as the test set across all runs, or was a random or cross-validated split applied? In addition, for the SEED dataset, was the data split performed at the trial level or at the sample level? If it was the latter, there is a potential risk of data leakage between training and testing samples, which may affect the reliability of the reported results.\n2) The paper only reports results on two datasets, SEED and DEAP, which limits the generalizability of the findings. Could the authors explain why additional public EEG emotion datasets such as SEED-IV, DREAMER, or AMIGOS were not included for further validation of the proposed model?\n3) The proposed distinction between hard and soft negative pairs appears similar to existing approaches in contrastive learning that use sample re-weighting or hard-negative mining strategies. Could the authors elaborate on how their formulation differs fundamentally from prior work and provide empirical evidence that it offers measurable improvements over the standard InfoNCE loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WNDW0UYzph", "forum": "rNNSuV4ZP9", "replyto": "rNNSuV4ZP9", "signatures": ["ICLR.cc/2026/Conference/Submission21859/Reviewer_DjcT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21859/Reviewer_DjcT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838153544, "cdate": 1761838153544, "tmdate": 1762941958789, "mdate": 1762941958789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CLIQ, a convolutional encoder trained with a novel contrastive framework for EEG-based emotion recognition. The work combines (1) multiple preprocessing pipelines (temporal, frequency, time frequency), (2) a modified contrastive loss that weights positive / soft-negative / hard-negative pairs, (3) a batch construction algorithm intended to guarantee representative pair composition, (4) feature analysis plus iterative DeepLIFT-based channel pruning, and (5) post-training symmetric quantization to shrink the model for edge deployment. Experiments on SEED and DEAP show competitive accuracy (e.g., SD/ SI numbers reported in Tables 1–2) together with large reductions in model size and training/inference time after pruning/quantization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a relevant and practical problem: improving EEG-based emotion recognition while keeping models lightweight enough for edge devices. The combination of self-supervised learning, explainable pruning, and post-training quantization is clearly motivated by real deployment needs.\n2. The experiments are comprehensive. The authors evaluate on two widely used datasets (SEED and DEAP).\n3. Ablation studies are solid. The comparison between the proposed setup and a simpler baseline provides clear evidence that the new components contribute to performance gains."}, "weaknesses": {"value": "1. α=β=0.4, γ=0.2, τ=0.5 lack sensitivity analysis. Add tests on parameter ranges (e.g., τ=0.1-1.0) and compare with equal weights.\n2. Expand Eq. (1) with explicit indices, define $f_{ij}$​ precisely (e.g., cosine similarity scaled by $\\tau$), and state how $\\delta$ is assigned for positive/soft-/hard-negative pairs; brief pseudocode for pair/view formation would help reproducibility."}, "questions": {"value": "1. How does CLIQ perform under cross-dataset transfer without fine-tuning?\n2. How do the author ensure no subject/session leakage in contrastive pair construction and preprocessing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F5gTFeKFUr", "forum": "rNNSuV4ZP9", "replyto": "rNNSuV4ZP9", "signatures": ["ICLR.cc/2026/Conference/Submission21859/Reviewer_KQv6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21859/Reviewer_KQv6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931786187, "cdate": 1761931786187, "tmdate": 1762941958387, "mdate": 1762941958387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper applies self-supervised learning (SSL) to process complex\nEEG signals with low amount of labeled data for solving emotion recognition\ntask. Proposed approach is based on a convolutional encoder with a novel contrastive\nloss and batching function. It has been evaluated on SEED and DEAP\ndatasets. The paper also compared different preprocessing techniques in temporal, frequency\nand temporal-frequency domains. We achieved fairly high accuracy even\non small amount of labeled data"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a self-supervised sentiment recognition method and achieves favorable results in both subject-dependent and subject-independent paradigms, demonstrating a certain degree of contribution."}, "weaknesses": {"value": "1. The innovation of this paper is relatively weak, as it merely combines convolutional neural networks (CNNs) with contrastive loss in a straightforward manner.\n\n2. The implementation of this paper lacks detailed ablation experiments, only including the baseline model and the final model. The performance of its baseline model is superior to that of most comparative models, while the improvement of the final model is limited.\n\n3. The experiments on each dataset lack specific sentiment classification breakdowns."}, "questions": {"value": "Please refer to the issues mentioned in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CNRShhVFD5", "forum": "rNNSuV4ZP9", "replyto": "rNNSuV4ZP9", "signatures": ["ICLR.cc/2026/Conference/Submission21859/Reviewer_XheM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21859/Reviewer_XheM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988254677, "cdate": 1761988254677, "tmdate": 1762941958196, "mdate": 1762941958196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a contrastive learning approach that integrates explainable artificial intelligence guided interpretation mechanisms with model quantization techniques for EEG-based emotion recognition. The study addresses the scarce annotation issue, high model complexity, and insufficient interpretability. The method was evaluated on two widely used datasets. Key EEG channels (T7 and T8) were identified through feature analysis and DeepLIFT, achieving competitive accuracy while reducing model size via quantization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The ablation study confirms component efficacy. Cross-dataset transfer and embedding visualizations (t-SNE/PCA) test its generalizability. \n2. T7/T8 identification matches literature on temporal lobe emotional processing, avoiding \"black-box\" limitations.\n3. This paper achieves 4x memory reduction and faster inference, directly supporting edge-device deployment."}, "weaknesses": {"value": "1. The batching algorithm sets parameter k=2 to \"handle dataset variability\", but provides no justification.\n2. In cross-dataset transfer, the subject-independent accuracy when transferring from SEED to DEAP was only 63.4%, yet the authors did not conduct in-depth analysis to clarify the fundamental reasons behind this suboptimal result.\n3. In Line 337, which mentions the biological relevance of the T7/T8 channel, lacks supporting citations, thereby weakening the claim of interpretability.\n4. The CLIQ algorithm was not tested on signals containing simulated artifacts (such as muscle noise of varying intensities and eye movements) to assess whether its performance degrades."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7IiI0NI4II", "forum": "rNNSuV4ZP9", "replyto": "rNNSuV4ZP9", "signatures": ["ICLR.cc/2026/Conference/Submission21859/Reviewer_XYDD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21859/Reviewer_XYDD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762352938645, "cdate": 1762352938645, "tmdate": 1762941957998, "mdate": 1762941957998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}