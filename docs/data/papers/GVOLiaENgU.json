{"id": "GVOLiaENgU", "number": 23704, "cdate": 1758347388018, "mdate": 1759896800773, "content": {"title": "A Bayesian Nonparametric Framework For Learning Disentangled Representations", "abstract": "The field of unsupervised disentangled representation learning concerns itself with discovering and organizing patterns in a dataset without the use of an oracle capable of giving us the ground-truth labels. However, current approaches face fundamental challenges in determining both the appropriate latent space complexity needed to capture underlying generative factors and the optimal strength of regularization constraints required for effective disentangled representations. We address these challenges by proposing a Bayesian nonparametric prior over the embedding space of latent-quantization autoencoders, enabling the model to adaptively adjust representation capacity and regularization in accordance with the intrinsic data complexity. To support inference under this nonparametric prior, we introduce an infinite-capacity variational family which preserves the hierarchical prior structure and latent quantization biases while maintaining sufficient expressiveness to capture complex dependencies essential for effective representation learning. Our proposed approach consistently achieves superior or competitive performance relative to strong baseline methods on both 3DShapes and MPI3D datasets characterized by diverse source variation distributions. We empirically demonstrate that the proposed probabilistic framework achieves disentangled representation learning through intrinsic structural biases and a unified objective function, obviating the need for auxiliary regularization constraints or careful hyperparameter tuning.", "tldr": "A nonparametric variational inference framework for the unsupervised learning of disentangled representations", "keywords": ["representation learning", "disentangled representations", "unsupervised learning", "nonparametric methods"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11048bd54a129d68648542747ceeb1f3e66ff20a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a Bayesian nonparametric prior over the embedding space of latent-quantization autoencoders to determine the appropriate latent space complexity and the optimal strength of regularization constraints, and shows the effectiveness with experiments on two benchmark datasets. The proposed method produces competitive performance relative to some baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- A sound theoretical derivation of the new framework for unsupervised learning of disentangled representation.\n\n- Experiments support the proposed framework with relative superiority to the baselines."}, "weaknesses": {"value": "- Relatively weak novelty on the methods for disentangled representation learning. More recent methods should be compared and discussed.\n\n- The comparing methods should be updated with recent methods."}, "questions": {"value": "- What is the main difference of the proposed method compared with the recent methods for disentangled representation learning? Most of the methods in related works are out of dated.\n\n- Is the performance gain enough to argue as an achievement? Depending on the metrics, some of the baselines produce better performance. More in-depth discussion is required."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "72HD7pNmc0", "forum": "GVOLiaENgU", "replyto": "GVOLiaENgU", "signatures": ["ICLR.cc/2026/Conference/Submission23704/Reviewer_Avba"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23704/Reviewer_Avba"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761617058739, "cdate": 1761617058739, "tmdate": 1762942774782, "mdate": 1762942774782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Bayes-QLAE, an innovative framework for learning disentangled representations through a Bayesian non-parametric prior. Its core idea is to use a Dirichlet Process to allow the codebook for each latent dimension to adaptively adjust its capacity based on data complexity. Experiments show that the method achieves disentanglement performance comparable to strong baselines on standard benchmarks without the need for additional regularization."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Transforming the codebook size from a fixed hyperparameter into a quantity learned from data via a  Dirichlet Process is an elegant and significant advancement that directly addresses a key limitation of existing quantization methods.\n2. The paper provides solid and rigorous theoretical proofs, while also achieving competitive results on standard datasets. Furthermore, ablation studies analyze the contribution of the model's various components."}, "weaknesses": {"value": "1. While making the codebook size adaptive is intuitively appealing, it contradicts findings from prior work (e.g., Tripod et al.), which suggests that maintaining a smaller codebook is beneficial for disentanglement. How does the proposed method balance the tendency of the codebook to grow with the need to maintain a compact, disentangled representation? A clear discussion or analysis of this trade-off is necessary.\n2. The current experimental results are limited to standard disentanglement metrics like InfoMCE and DCI. A more comprehensive evaluation is needed. For instance, a comparison of reconstruction accuracy against baseline methods would provide crucial insight into whether the gains in disentanglement come at the cost of representation fidelity.\n3. The paper hypothesizes that allowing adaptive codebook sizes helps align individual factors with single dimensions. However, the experiments fail to substantiate this claim. The final learned sizes of the codebook dimensions should be reported to demonstrate this adaptivity. Furthermore, visualizing the effect of a single latent variable is essential to qualitatively assess the smoothness and interpretability of the learned representations, which is a standard practice for evaluating disentanglement."}, "questions": {"value": "Please refer to Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fb7e2kTRhf", "forum": "GVOLiaENgU", "replyto": "GVOLiaENgU", "signatures": ["ICLR.cc/2026/Conference/Submission23704/Reviewer_EAMz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23704/Reviewer_EAMz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713687651, "cdate": 1761713687651, "tmdate": 1762942774041, "mdate": 1762942774041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work replaces the gaussian prior of latent variables with discrete codebook with a nonparametric Dirichlet Process (DP). This process can adjust representation capacity according to the data complexity. The authors propose a hierarchical prior structure to capture complex dependences of latent variables.  The proposed method is verified on 3DShapes and MPI3D to show superior disentanglement scores."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work aiming at removing assumption of the capacity of each factor could be an important step to practical applications of disentanglement learning.\n\nThe proposed solution, Dirichlet Process for a Bayesian nonparametric prior on latent variables seems technically sounds.\n\nThe proposed method, Bayes-QLAE, achieves good disentanglement metrics on both 3DShapes and MPI3D."}, "weaknesses": {"value": "The proposed method still lacks practical proof in real situations where some combinations of generative factors are missing.\nExperiments did not verify how the DP adjusts the capacity of each factor, which is an important claim of this work.\nNo visualization to demonstrate the reconstruction quality.\nThe experimental results show that the proposed Bayes-QLAE did not surpass Tripod on 3Dshapes and MPI3D."}, "questions": {"value": "What are the advantages of Bayes-QLAE compared to Tripod?\nWhy do we need the hierarchical Bayesian nonparametric approach? Are there any special benefits beyond disentanglement?\nCan the experiments be added to demonstrate the advantage of the work for practical problems? \nAlso, adding training details would be beneficial. \nI case of deep encoders with a lot of non-linearities consisting of a high-dimensional latent space, would the encoder network not be able to learn to project the data into a space where Gaussian prior assumption would be enough? \nCan the principles laid in this work for learning disentangled representation be shown on some recent architectures (e.g., disentanglement in the latent space of diffusion models)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MSh0iAOHie", "forum": "GVOLiaENgU", "replyto": "GVOLiaENgU", "signatures": ["ICLR.cc/2026/Conference/Submission23704/Reviewer_5ePo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23704/Reviewer_5ePo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947712609, "cdate": 1761947712609, "tmdate": 1762942773671, "mdate": 1762942773671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}