{"id": "TZBof1S00t", "number": 15921, "cdate": 1758257131290, "mdate": 1759897272735, "content": {"title": "Provably Communication-Efficient Federated Graph Neural Network", "abstract": "Graph neural networks (GNNs) are powerful tools for relational data, but their application is often limited by data silos and privacy concerns, as real-world graphs are frequently distributed across multiple clients. While federated learning (FL) offers a privacy-preserving training paradigm, existing federated GNN approaches suffer from a critical flaw: they either ignore the crucial links between clients, sacrificing accuracy, or require impractically high communication overhead.  \nWe introduce CE-FedGNN, a communication-efficient federated GNN framework for such coupled graphs. Instead of sharing raw data or per-iteration embeddings, CE-FedGNN infrequently transmits only aggregated, high-level embeddings, preserving critical structural context while minimizing privacy leakage and communication costs.\nDespite the challenges of optimization under multi-layer composition and coupled data, we establish a convergence rate of $O(1/\\sqrt{T})$ to a stationary point while the communication complexity is $O(T^{3/4})$. We further derive bounds for injecting Gaussian noise that provide formal differential privacy. \nOur experiments on a synthetic interbank anti-money laundering task show that the effectiveness of CE-FedGNN, which can be preserved even with injected Gaussian noise for differential privacy.", "tldr": "", "keywords": ["Federated Learning", "Graph Neural Network", "Fraud Detection"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f47756dd23a7f05423fe343783b33eee10dcd95.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CE-FedGNN, a federated graph neural network (GNN) framework that integrates cross-client edges while minimizing communication overhead. By incorporating differential privacy, CE-FedGNN provides formal privacy guarantees. Experimental evaluation on a synthetic interbank anti-money laundering task demonstrates that CE-FedGNN consistently outperforms selected baselines."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and timely problem, federated learning on graph-structured data, which has broad potential applications in domains such as finance, healthcare, and social networks."}, "weaknesses": {"value": "1. The notation is unclear and lacks rigorous definitions, making the algorithmic details difficult to follow.\n2. The related work discussion is incomplete. Several key studies on cross-client edge modeling in federated GNNs are missing, raising novelty concerns.\n3. The experimental section is weak. Only one dataset is used, and there are no communication overhead comparisons with relevant baselines."}, "questions": {"value": "Based on the review above and some other issues, the reviewers have the following questions or comments.\n\n1. The current notations are inconsistent and not well-defined. This makes it difficult to fully understand the workflow and theoretical design of the proposed algorithm. Please revise and clarify all variables, symbols, and equations.\n2. The paper should include discussions on representative studies that explicitly handle cross-client edge modeling, such as FedSage [1] and FED-PUB [2]. These methods should also be included as experimental baselines to ensure a fair comparison.\n3. The experiments rely on only one synthetic dataset. Additional experiments on datasets from other domains (e.g., citation networks, social graphs) would help demonstrate the generalizability of CE-FedGNN.\n4. Since one of CE-FedGNN’s main claims is reduced communication cost, quantitative comparisons of communication overhead with other federated GNN methods are needed to substantiate this claim.\n5. The paper mentions convergence rate and communication complexity but provides no formal proof or empirical validation. The authors should either include theoretical justification or remove these claims.\n6. The paper mentions a “biased gradients” challenge on Page 4, but does not clearly explain its origin or impact. Please clarify whether this is a federated learning-specific problem or a general deep learning issue.\n\n[1] Ke Zhang, Carl Yang, Xiaoxiao Li, Lichao Sun, Siu-Ming Yiu: Subgraph Federated Learning with Missing Neighbor Generation. NeurIPS 2021: 6671-6682\n\n[2] Jinheon Baek, Wonyong Jeong, Jiongdao Jin, Jaehong Yoon, Sung Ju Hwang: Personalized Subgraph Federated Learning. ICML 2023: 1396-1415"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xEl0VREdwn", "forum": "TZBof1S00t", "replyto": "TZBof1S00t", "signatures": ["ICLR.cc/2026/Conference/Submission15921/Reviewer_e5Xn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15921/Reviewer_e5Xn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900793253, "cdate": 1761900793253, "tmdate": 1762926138924, "mdate": 1762926138924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the communication-accuracy-privacy trade-off in federated graph neural networks and proposes CE-FedGNN. The framework uses moving-average estimation for node embeddings/gradients and infrequently transmits high-level embeddings to reduce overhead and privacy leakage. It provides theoretical guarantees to a stationary point, communication complexity and formal differential privacy via Gaussian noise injection. Experiments on synthetic cross-bank anti-money laundering (AML) data show CE-FedGNN outperforms baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Focuses on a critical, unresolved issue in FedGNNs, existing methods either ignore cross-client edges or have high communication costs, aligning with real needs.\n2. Provides formal analysis for convergence, communication complexity, and DP, which is rare in FedGNN works and strengthens the method’s credibility.\n3. Simulates realistic federated settings (edge attributes shared, node attributes private) and tests on AML tasks (a privacy-sensitive domain), making the work relevant to real-world applications."}, "weaknesses": {"value": "1. Relies Exclusively on Synthetic, Non-Public Data: All experiments use synthetic AML data generated by a simulator. There is no validation on public FedGNN benchmarks. This means CE-FedGNN’s performance on non-AML tasks or real-world graphs is unproven. Additionally, key details for data generation are not shared, harming reproducibility.\n2. Theoretical Assumptions Conflict with Practical GNN Use: The theory requires activation functions to be \"smooth\", but mainstream GNN activations (e.g., ReLU) are not smooth. This creates an unavoidable issue: either CE-FedGNN cannot be used with most practical GNNs, or the entire convergence analysis is invalid.\n3. Core Designs Lack GNN-Specific Innovation: The two key ideas are adaptations of existing federated learning concepts. For example, moving averages are used in FL methods like FedProx to control variance. The paper does not add unique designs tailored to GNNs.\n4. Differential Privacy Analysis Is Incomplete: The paper only analyzes noise effects in a single communication round, but FedGNN training requires multiple rounds and noise accumulation across rounds (which impacts both DP budget and accuracy) is not studied. It also fails to provide a clear quantitative relationship between noise level, DP budget (ϵ), and model accuracy, making the DP feature hard to use in practice.\n5. Baseline Comparisons Are Inadequate: The paper omits recent FedGNN baselines focused on privacy. It also lacks a \"Local GNN\" baseline that ignores cross-client edges entirely, so the actual benefit of CE-FedGNN’s cross-client edge handling cannot be quantified."}, "questions": {"value": "1. Can CE-FedGNN be adjusted to work with non-smooth activations while keeping its theoretical guarantees?\n2. Would CE-FedGNN maintain its performance advantage when tested on public FedGNN datasets (not just synthetic AML data)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GSz470NeI8", "forum": "TZBof1S00t", "replyto": "TZBof1S00t", "signatures": ["ICLR.cc/2026/Conference/Submission15921/Reviewer_VT7c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15921/Reviewer_VT7c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980427955, "cdate": 1761980427955, "tmdate": 1762926138272, "mdate": 1762926138272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CE-FedGNN achieves a balance between accuracy, communication efficiency, and privacy for federated GNNs. It uses moving-average embeddings/gradients and low-frequency high-level embedding transmission. Gaussian noise injection ensures differential privacy, and experiments on anti-money laundering data show it outperforms baselines even with privacy guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "• Communication efficiency: Avoids per-iteration embedding sharing, cutting complexity;\n• Captures cross-client edges via 1-hop shared embeddings, outperforming methods that ignore global structure.\n• Privacy guarantee: Formal DP via Gaussian noise, with >95% accuracy retained under moderate noise."}, "weaknesses": {"value": "• Relies on cross-client edge attribute sharing (e.g., inter-bank transaction details), which may not hold in all scenarios.\n• Requires manual tuning of DP noise intensity without automation.\n• While tested with up to 32 clients, scalability to hundreds or thousands of clients typical in large-scale FL systems is not evaluated."}, "questions": {"value": "AML is an essential task in financial risk control, simulation dataset may not be sufficient to show the effectiveness in reality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rugr6Mul1S", "forum": "TZBof1S00t", "replyto": "TZBof1S00t", "signatures": ["ICLR.cc/2026/Conference/Submission15921/Reviewer_LLUi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15921/Reviewer_LLUi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992549293, "cdate": 1761992549293, "tmdate": 1762926137867, "mdate": 1762926137867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CE-FedGNN, a communication-efficient federated learning framework for Graph Neural Networks (GNNs) operating on graphs distributed across multiple clients with inter-client edges. The core challenge is that accurate GNN training requires information from a node's neighbors, which may reside on other clients, but frequent exchange of embeddings incurs prohibitive communication costs and privacy risks. CE-FedGNN addresses this by using moving-average estimators for both node embeddings and gradients. Clients infrequently share only these aggregated, high-level embeddings instead of raw data or per-iteration updates. The authors provide a convergence analysis for their non-convex optimization problem, proving a rate of \\(O(1/\\sqrt{T})\\) to a stationary point with a communication complexity of \\(O(T^{3/4})\\). They also extend the framework with Gaussian noise injection for differential privacy and analyze its convergence. Experiments on a synthetic anti-money laundering task demonstrate the method's effectiveness, communication efficiency, and utility under DP constraints."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  Well-Motivated and Important Problem: The paper tackles a critical and realistic problem at the intersection of federated learning and graph learning. The scenario of a graph distributed across clients with connecting edges (e.g., cross-bank transactions) is common in practice but poorly handled by standard FL methods. The motivation is clear and compelling.\n2.  Novel Algorithmic Design: The use of moving-average estimators for both embeddings (H) and gradients (G) is a clever and novel core contribution. This design elegantly decouples the need for frequent communication from the training process, allowing clients to use slightly stale but low-variance estimates of their neighbors' states. The insight to apply this specifically to nodes (not edges) is practical, as nodes are typically fewer than edges.\n3.  Significant Theoretical Contributions: Providing convergence guarantees for a federated GNN algorithm is non-trivial due to the multi-layer compositional structure and cross-client dependencies. The paper makes a substantial theoretical contribution by proving convergence rates and communication complexity for their non-convex problem. Extending this analysis to the differentially private version is another key strength.\n4.  Comprehensive Experimental Evaluation: The experiments are well-designed, using a realistic synthetic financial dataset (anti-money laundering) that mirrors the paper's motivating application. The comparison against relevant baselines (Single Client, FedAvg, Swift-FedGNN, FedGCN) across different dataset scales and illicit transaction ratios (High/Low) provides strong empirical support. The ablation studies on communication intervals (K) and DP noise levels are particularly informative."}, "weaknesses": {"value": "1.  Dependence on Synthetic Data: While justified by the unavailability of real financial data, the exclusive reliance on a single synthetic dataset (from Altman et al., 2023) is a limitation. The generalizability of the results to other graph types (e.g., social networks, citation networks) and real-world distributions remains unproven. Performance on standard graph benchmark tasks would strengthen the claims.\n2.  Simplified Privacy Analysis and Threat Model: The DP analysis focuses on adding Gaussian noise to shared embeddings. However, the overall privacy guarantee is somewhat nuanced. The privacy accounting (in Appendix C) considers the embedding sharing in isolation. A more comprehensive analysis would consider the end-to-end privacy loss accounting for all shared information (models, gradients, embeddings) together. Furthermore, the threat model primarily considers attribute inference attacks (AIA); a discussion of other potential attacks like membership inference or graph reconstruction attacks would provide a more complete security picture.\n3.  Assumptions in Theoretical Analysis: The convergence proofs rely on several standard but strong assumptions (e.g., Lipschitz continuity and smoothness of functions, bounded gradients, existence of a minimum). The practical impact of these assumptions, especially for complex GNN architectures and real-world graph data which may not perfectly satisfy them, is not deeply discussed.\n4.  Limited Discussion of System Overheads: While communication rounds are reduced, the local computation and memory overhead of maintaining moving-average estimators (H and G) for all relevant nodes is not thoroughly analyzed. For very large graphs, this storage cost per client could be non-negligible and might affect the practical scalability."}, "questions": {"value": "1.  How does the memory and computational overhead of maintaining the moving-average estimators H and G scale with graph size and number of clients? Could this become a bottleneck for massive graphs?\n2.  Beyond the synthetic financial transaction graph, have you tested CE-FedGNN on other types of graph data (e.g., social networks, molecular graphs) to demonstrate its general applicability? What adaptations, if any, are needed?\n3.  The privacy analysis focuses on noise added to embeddings. How would you perform a comprehensive privacy accounting that composes the privacy loss from sharing model parameters, gradients, and embeddings throughout the training process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HFpg1eHi6k", "forum": "TZBof1S00t", "replyto": "TZBof1S00t", "signatures": ["ICLR.cc/2026/Conference/Submission15921/Reviewer_xVhn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15921/Reviewer_xVhn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995642619, "cdate": 1761995642619, "tmdate": 1762926137539, "mdate": 1762926137539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}