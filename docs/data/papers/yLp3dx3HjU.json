{"id": "yLp3dx3HjU", "number": 20570, "cdate": 1758307597992, "mdate": 1759896970713, "content": {"title": "Ax-Prover: Agentic LEAN Proving with LLMs and MCP-based Verifiers", "abstract": "We present Ax-Prover, a domain-agnostic multi-agent system for theorem proving. \nThe task of generating formal proofs requires both creativity and precise formalization. \nAx-Prover addresses this challenge by combining large language models (LLMs), which contribute knowledge and reasoning ability, with MCP-based verification and validation tools, which enforce rigor and syntactic correctness. We benchmark our approach on the large-scale NuminaMath-LEAN dataset and introduce two new datasets: one in Abstract Algebra and one in Quantum Theory. Our experiments show that Ax-Prover consistently outperforms state-of-the-art (SOTA) provers across domains. Notably, we find a large performance gap in the newly introduced domains, suggesting that while Ax-Prover adapts readily to novel areas, existing SOTA systems remain highly specialized to their training domains and struggle to generalize.", "tldr": "", "keywords": ["Agentic AI", "LLMs", "Formal Verification", "MCPs", "Physics"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a7b1c8e1382c6678aee1e53ba626814d2bbf85b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Ax-prover, a multi-agent system for automated theorem proving. Ax-prover combines Large Language Models (LLMs) with MCP-based verification and validation tools. The experiments show that its performance outperforms current state-of-the-art (SOTA) provers across various domains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work combines LLM provers with tool-use capabilities, allowing the model to utilize functions like Lean search and Lean verification. This method is novel and holds significant value for the automated theorem proving (ATP) field.\n\nFurthermore, their evaluation results show the system excels not only on standard math competition problems but also performs well in domains like abstract algebra and quantum theory. This breadth demonstrates the strong capabilities of their system."}, "weaknesses": {"value": "The main weakness of this paper is its lack of technical detail. Only very high-level ideas are presented (e.g., the existence of three sub-agents).\n\nCrucial information is missing, including:\n\n1. How the models are trained.\n\n2. The amount of data used for training.\n\n3. Even the specific details of how their prover functions."}, "questions": {"value": "Could you extend the experimental comparison to include more recent SOTA provers, such as Goedel-Prover-V2?\n\nCould you release more specific details regarding your model training process and the overall system design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1ekfklFSNP", "forum": "yLp3dx3HjU", "replyto": "yLp3dx3HjU", "signatures": ["ICLR.cc/2026/Conference/Submission20570/Reviewer_RQ46"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20570/Reviewer_RQ46"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577035466, "cdate": 1761577035466, "tmdate": 1762933983632, "mdate": 1762933983632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Ax-Prover, a multi-agent system for automated theorem proving in Lean. The system employs an agentic approach that combines the broad reasoning capabilities of general-purpose Large Language Models (LLMs) with the formal rigor guaranteed by MCP toolsets integrated with the Lean environment.\nThe evaluation was conducted on the existing NuminaMath-LEAN dataset, as well as two new benchmarks newly constructed for this study: AbstractAlgebra (AA) and QuantumTheorems (QT). The experimental results show that Ax-Prover outperforms existing specialized provers (DS-Prover, Kimina) across all domains, demonstrating high generalization performance, especially on the new datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Construction of Novel Datasets**: \n  A clear contribution of this research is the creation of two new Lean datasets (AA and QT) in new domains (abstract algebra and quantum theory). These complement existing benchmarks that are primarily focused on mathematics competitions. These datasets can serve as valuable resources for future automated theorem proving research to measure the generalization capabilities of models.\n2. **Verification of Generalization Performance**: \n  It is commendable that the paper uses these new datasets to demonstrate that while existing specialized provers lose performance outside their training domains, the proposed method shows domain adaptability."}, "weaknesses": {"value": "1.  **Insufficient and Inappropriate Baseline Comparison**:\n    * There are significant concerns about the validity of the paper's experimental evaluation. The Kimina Prover, used as a baseline, is arguably not a state-of-the-art (SOTA) model.\n    * Regarding open-source models, comparisons against known stronger models, such as Goedel Prover v2, are missing.\n    * The evaluation of closed models is also insufficient. Notably, the paper lacks a performance comparison against true SOTA models like Google's Gemini or OpenAI's GPT-5, which have reported extremely high performance in recent math competitions (e.g., IMO Grand Challenge). It is difficult to assert the effectiveness of the proposed method without demonstrating superiority over these models.\n\n2.  **Constraints from Closed-Model Dependency**:\n    * The reliance of the \"Prover\" agent on a specific closed API, Claude Sonnet 4, is a major limitation.\n    * Importantly, this architecture fundamentally precludes research advancements that would be possible with open models, such as using reinforcement learning (RL) to further optimize the agent's proof strategies.\n\n3.  **Lack of Novelty in Agent Architecture**:\n    * The multi-agent configuration (Orchestrator, Prover, Verifier) itself does not present significant technical novelty compared to existing agent frameworks designed for proof assistant interaction.\n    * The core idea of enabling a general-purpose LLM to solve tasks using external tools (in this case, Lean's MCP tools) is already well-established, making the paper's unique, fundamental contribution unclear."}, "questions": {"value": "1.  **Regarding Baselines**: Why were more powerful, recent SOTA models (e.g., Goedel Prover v2, the latest Gemini, the latest GPT) not adopted as baselines? How would Ax-Prover's performance (and inference cost) be expected to compare against these models?\n2.  **Regarding Closed-Model Dependency**: If a high-performance open-source model were used as the Prover agent instead of Claude Sonnet 4, how much of the performance would be maintained? Are there plans to reduce this dependency on closed models, or are there results from reproducibility experiments using open models?\n3.  **Regarding Architectural Novelty**: Compared to existing LLM agent frameworks, what specific aspects of the proposed agent design are superior for the task of \"theorem proving\"? Please clarify the technical novelty more distinctly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OUrXcWpJin", "forum": "yLp3dx3HjU", "replyto": "yLp3dx3HjU", "signatures": ["ICLR.cc/2026/Conference/Submission20570/Reviewer_gWSZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20570/Reviewer_gWSZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965707315, "cdate": 1761965707315, "tmdate": 1762933983108, "mdate": 1762933983108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Ax-Prover, a multi-agent system for automated theorem proving in Lean that bridges the gap between specialized theorem provers (narrow but Lean-integrated) and general-purpose LLMs (broad but lacking formal tools). The system consists of three agents: an Orchestrator (task management), a Prover (proof construction), and a Verifier (correctness checking). Ax-Prover leverages the lean-lsp-mcp protocol to give LLMs (specifically Claude Sonnet 4) direct access to Lean tools for inspecting goals, searching theorems, and verifying code.\n\nThe authors evaluate Ax-Prover on three datasets: NuminaMath-LEAN (competition mathematics), and two newly introduced benchmarks, AbstractAlgebra (graduate-level algebra) and QuantumTheorems (quantum physics). Results show Ax-Prover achieves 51%, 64%, and 96% accuracy respectively, outperforming both standalone Claude Sonnet 4 and specialized provers (DeepSeek-Prover-V2 and Kimina-Prover). The paper includes a case study verifying a cryptography result, demonstrating practical human-AI collaboration capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors successfully connect general-purpose LLMs to Lean via MCP, avoiding the need for specialized fine-tuning while maintaining formal rigor. \n\nThe multi-agent architecture is well-designed, with modular separation of concerns (orchestration, proving, verification) that enables independent optimization and extension.\n\nTesting across competition mathematics, abstract algebra, and quantum physics demonstrates domain flexibility.\n\nNew benchmarks address a gap. AbstractAlgebra and QuantumTheorems target research-level mathematics and physics, expanding beyond competition-style problems.\n\nThe cryptography verification example (Appendix F) demonstrates practical utility and identifies an error in the original proof.\n\nTable 3 and Section 5.3 show Ax-Prover uses 10 tactics not employed by specialized provers, suggesting enhanced exploration.\n\nDeployment accessibility addresses a real point as specialized provers require expensive GPUs and engineering expertise while Ax-Prover only needs API access.\n\nTool usage analysis demonstrates average of 100.76 tool calls per run, providing evidence for the importance of Lean integration."}, "weaknesses": {"value": "Major:\n\n1. Critically flawed baseline comparison:\n- Kimina results use pass@68 from training (acknowledged in footnote) vs. Ax-Prover's pass@1\n- Table 2 notes \"obtained during its RL training phase with, on average, pass@68\"\n- This makes the comparison meaningless, need fair pass@1 comparison\n- Claims like \"Ax-Prover largely outperformed all the baselines\" are not supported\n\n\n2. Severely limited experimental validation:\n- Only 300/104,000 samples from NuminaMath-LEAN (0.3%)\n- New datasets are small: 100 (AA) and 134 (QT) problems\n- No evaluation on full miniF2F or Putnambench despite these being standard benchmarks\n- Justification that \"they contains similar problems types\" is insufficient\n- Only pass@1 evaluation (pass@k is standard in the field)\n\n\n3. Poor generalization contradicts main claims:\n- Appendix D (Figure 7) shows Ax-Prover and GRPO-hybrid both fail on miniF2F\n- Paper claims Ax-Prover \"adapts readily to novel areas\" and existing systems \"struggle to generalize\"\n- But miniF2F is the standard OOD benchmark, failure here is highly concerning\n- The strong performance on new benchmarks might simply mean they're easier or closer to Claude's training distribution\n\n\n4. Misleading cost analysis (Section 5.4):\n- Compares 4000 dollars (Ax-Prover pass@1) vs. 300 dollars / 2000 dollars (DS / Kimina pass@1)\n- Then argues specialized models \"would have far exceeded ours\" with pass@32+\n- But this is unfair, comparing different evaluation protocols\n- The paper uses pass@1 everywhere due to \"budget restrictions\" but then criticizes specialized models for needing pass@k\n- Actual cost comparison should use same pass@k\n\n\n5. Missing critical ablations:\n- How much improvement comes from Claude Sonnet 4's base capability vs. the agentic workflow?\n- What if you just give Claude Sonnet 4 access to diagnostic messages without full workflow?\n- What percentage of proofs actually benefit from tool usage?\n- How does performance scale with number of tool calls?\n\n\n6. Dataset quality concerns:\n- AbstractAlgebra: Semi-automatically generated, only 100 problems, no community validation\n- QuantumTheorems: Only 134 problems, \"human-in-the-loop process\" but quality metrics unclear\n- No difficulty analysis or comparison with existing benchmarks\n- Risk that these datasets may be unintentionally biased toward Ax-Prover's strengths\n\nMinor:\n\n7. Case study limitations (Appendix F):\n- Single example\n- Required \"expert human oversight on 2 steps\"\n- \"whole process lasted two working days\" with \"over 2,000 lines of lean code\"\n- Not clear this is significantly better than manual formalization\n\n\n8. Overclaiming deployment advantages:\n- Valid point that specialized models need GPUs\n- But Anthropic API calls also have rate limits, costs, and dependencies\n- \"hundreds of problems per dataset to be evaluated without any engineering burden\" overstates ease\n- Running 1000 problems at $4000 is not trivial for most researchers\n\n\n9. Limited algorithmic novelty:\n- Multi-agent systems are well-established\n- The main contribution is engineering integration with MCP\n- Workflow (identify -> sketch -> formalize -> solve -> verify) is standard human approach\n\n\n10. Missing failure analysis:\n- What are common failure modes?\n- Which types of problems does Ax-Prover struggle with?\n- When do tool calls help vs. hurt?\n- What causes the Orchestrator to give up?"}, "questions": {"value": "Critical Questions:\n\n1. Can you provide a fair pass@1 comparison with Kimina and DeepSeek-Prover on the same 300-sample NuminaMath subset? The current comparison with Kimina's pass@68 training results is not valid.\n\n2. Why only 300 samples from NuminaMath-LEAN? With 104k problems available, this 0.3% sample seems insufficient. Can you evaluate on at least the full test set?\n\n3. Can you explain the miniF2F failure? This directly contradicts claims about generalization. What specific challenges does miniF2F pose that your new benchmarks do not?\n\n4. What is Ax-Prover's pass@k performance? Even pass@8 or pass@16 would be informative for understanding the method's reliability.\n\n5. Can you run on full miniF2F and Putnambench? These are standard benchmarks and necessary for proper comparison with SOTA. The claim they are \"too expensive\" is undermined by spending $4000 on smaller datasets.\n\nImportant Questions:\n\n6. What is the ablation of Claude Sonnet 4 base capability vs. agentic workflow? How does Sonnet 4 with just diagnostic messages compare to the full Ax-Prover workflow?\n\n7. What percentage of successful proofs actually use Lean tools meaningfully? The average 100.76 tool calls includes many that might not contribute to success.\n\n8. How does performance correlate with proof length/complexity? Are there systematic patterns in what Ax-Prover can vs. cannot prove?\n\n9. What does \"expert human oversight on 2 steps\" mean in the case study? How much human intervention is generally needed? This is critical for assessing practical utility.\n\n10. Can you provide inter-annotator agreement or expert validation for the new datasets? How do we know AbstractAlgebra and QuantumTheorems are well-formed and of appropriate difficulty?\n\nMinor Questions:\n\n11. How sensitive is performance to the 25-minute timeout? What about varying the number of Orchestrator-Prover-Verifier loops?\n\n12. What is the distribution of tool usage across the different tool categories (Table 1)?\n\n13. Can you provide a detailed comparison of proof strategies between Ax-Prover and specialized models beyond just tactic usage?\n\n14. How does Ax-Prover perform when the base model is switched (e.g., GPT-4 instead of Claude Sonnet 4)?\n\n15. What is the failure rate due to timeout vs. incorrect proofs vs. inability to find proof sketch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p97k8G9mKU", "forum": "yLp3dx3HjU", "replyto": "yLp3dx3HjU", "signatures": ["ICLR.cc/2026/Conference/Submission20570/Reviewer_49ve"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20570/Reviewer_49ve"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762209027256, "cdate": 1762209027256, "tmdate": 1762933982663, "mdate": 1762933982663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced Ax-prover that consists of the prover, verifier and MCP tool usage. Their prover search for unfinished proofs, generate informal (natural language style) proof sketches, perform stepwise formalization, solve and also verify them. Their verifier verifies the generated proofs from the prover based on formal reasoning of Lean. Although their prover looks doing complex things, they indeed decompose the task and stepwisely generate formal code (lean) with the help of informal reasoning. They also introduced two new datasets: AbstractAlgebra(AT) and QuantumTheorems(QT) that cover from abstract algebra to quantum mechanics. Instead of widely-used formal reasoning datasets (miniF2F and PatnumBench), they did their experiments within these datasets, sacrificing the fair comparisons with existing strong formal reasoning models.\n\nThe critical problem of this paper is that  this paper ignores the existing approaches that utilize the prover and verifier in formal and informal math reasoning(Please closely see Baba et al. 2025 and DeltaProver). Indeed, this paper mentions lines of studies in agentic ai research out of formal math reasoning such as Gottweis et al. 2025 and Yamada et al. 2025 in Line 101 regardless of existing more relevant studies in formal math reasoning. This paper also fails to mention DeltaProver. In experiments, they compare their model with quite limited existing studies. Therefore the author failed to adequately position their study in lines of the formal math reasoning studies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- S1: Proposal of new benchmark sets of AbstractAlgebra(AT) and QuantumTheorems(QT) that cover from abstract algebra to quantum mechanics."}, "weaknesses": {"value": "- W1: The use of both prover and verifier is not novel and at least not the first idea in this paper. Please see Baba et al. (2025) and Zhou et al. (2025).\n- W2: Lack of the comparison within the major evaluation benchmark (miniF2F and PatnumBench). They compare their model with three existing models mostly in their own configuration. This makes a fair comparison of Ax-Prover with many existing strong prover models (such as SeedProver and GoedelProver) almost impossible.\n- W3: Limited effectiveness of the proposed benchmark sets: since the compared baselines are quite limited, the effectiveness and difficulty of the proposed datasets are not well presented."}, "questions": {"value": "- Q1: Can you present the result if you apply your model to miniF2F and PatnumBench?\n- Q2: Can you present more baseline model results with AbstractAlgebra(AT) and QuantumTheorems(QT)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EnM2p9sr5i", "forum": "yLp3dx3HjU", "replyto": "yLp3dx3HjU", "signatures": ["ICLR.cc/2026/Conference/Submission20570/Reviewer_LY2h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20570/Reviewer_LY2h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20570/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762359617792, "cdate": 1762359617792, "tmdate": 1762933982160, "mdate": 1762933982160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}