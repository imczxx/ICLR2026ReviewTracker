{"id": "cTqGsLYkRl", "number": 6731, "cdate": 1757993794473, "mdate": 1759897898246, "content": {"title": "VideoAgent: All-in-One Agentic Framework for Video Understanding and Editing", "abstract": "Video editing has become essential in digital media creation, yet existing automated systems are restricted to short segment processing and domain-specific tasks. They face two critical limitations: i) inability to handle diverse video comprehension and editing operations, and ii) lack of long-video understanding for coherent narrative creation. We propose VideoAgent, an all-in-one agentic framework addressing these challenges through two key innovations. First, we develop automated video shot creation with shot planning agents for coherent narratives and cross-modal retrieval for aligned visual content. Second, we design a multi-agent orchestration framework integrating over thirty specialized editing agents. Intent parsing filters relevant tools while self-reflective graph orchestration assembles complex editing pipelines. Extensive experiments on our newly-proposed VideoEdit benchmark and public datasets demonstrate VideoAgent's superiority over existing multimodal LLMs and agentic systems. VideoAgent achieves 87-98% orchestration success rates while reducing API costs by 60%. Human evaluation across six video categories shows VideoAgent produces professional-quality content approaching human-level performance, with ratings only 4% below human-created videos.", "tldr": "", "keywords": ["Multi-Agent Systems; Multimodal Content Editing; Agentic AI"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c58c924baed5381eebdfeb4b7c1515f9c4c5b316.pdf", "supplementary_material": "/attachment/026381dc502ed23824895d4016100d97ae3b3431.zip"}, "replies": [{"content": {"summary": {"value": "This paper identifies the two main problems in automated video editing (lack of coherence in long-form narratives and inability to handle diverse tasks), and proposes VideoAgent, an all-in-one agentic framework with these main contributions:\n\n* a method for video shot creation with cross-model and context on the full narrative of a target output video\n* an orchestration framework that can coordinate a large set of agents to generate a final video edit out of the created video shots\n* a new video edit benchmark (VideoEdit)\n\nComplete evaluation follows using both VideoEdit and Shot2Story, which display high performance against the proposed baselines (87-98% success rate), reduced API costs (60% lower costs), and comparable results to those produced by human editors (just 4% below human edits).\n\nThe paper also provides detailed prompts and pseudocode for each of the agents used by VideoAgent, and includes source code for this task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strengths of this work are:\n\n* Sound engineering, combining a large amount of agents with a solid orchestration method\n* Great quantitative results, generally superior to those of the baselines and, especially, very close to human-created videos (with the caveats discussed in the weaknesses section)\n* Novel approach that combines multiple-agents with a long-narrative aware video shot process, and an orchestration framework with self-aware elements\n* Clear descriptions of the methods used\n* Exhaustive details on prompts and pseudocode used in the agents, and open source code, both of which allow for high reproducibility of the work"}, "weaknesses": {"value": "The main weaknesses of this work are:\n\n* Lack of details about the human baselines. Just 4% below human performance is a very impressive result, however, which is greatly diminished by lack on details about how this human performance is measured. For example, who are the humans, what is their expertise, which tools have they used, for how long, etc.\n  * this point is really critical because the non-human baselines are based on systems that aren't designed to handle multi-modal video editing. So it's difficult to understand what the quality of this system is without a valid human comparison.\n  * (minor suggestion): Besides an ad-hoc human baseline of videos edited just for the purpose of this evaluation, one can also wonder how the system would compare against video edits seen on the wild, for different categories. What is the success rate against, for example, against fan edits of existing works.\n\n* Lack of actual examples in video format. The paper displays many examples as frame sequences, but given the nature of this work, the addition of examples in video format would be beneficial to this work. Being able to actually watch and listen to the videos produced by the system (and to compare them to the raw input materials) would provide a better understanding of the system quality.\n\n* While this work details the creation of a significant engineering system, with many agents and a solid orchestration method, the research contributions appear more incremental. To make the research contributions clearer, the paper could describe in more detail how novel aspects in the introduced orchestration system differ from those in other multi agent systems based on LLMs that also deal with graphs. The Related Work section at this time merely describes the application to multimodal video editing workflows which I don't think is enough novelty. \n\n* (minor) Lack of details regarding system latency (though API costs are provided), especially when compared against other baselines. Ideally the paper would include a plot with an axis for latency and another for each key metric, such that the tradeoffs between quality and performance can be better understood. (A similar plot for API costs would be interesting too, and given the reduced API costs of this system, good evidence in favor of this work)\n\n* (minor): Lack of mentions of key downsides for this approach, and potential future work. What do the failure modes look like? What is insightful about them?\n\n* (minor): The appendix may be excessive. The paper could improve by just listing a short summary of each agent behavior, and pointing to the supplementary materials for details."}, "questions": {"value": "* Have you considered using video generation models too, as an agent, for adding shots not included in the input materials?\n\n* Could a much simpler version of this work approach the same quality? For example, could the graph be non-dynamic but fixed, with each node gated on a selector for whether the node agent needs to be applied or not? (this has been explored to some extent in Table 3 which removes Intent Parsing and Agent Graph elements, but I wonder specifically about an existing but fixed graph)\n\n* Have you considered ablating the specific list of agents, to measure how they rank compared to each other? this could inform which other agents are possible future additions for the system"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "* The paper displays copyrighted work (e.g. SpongeBob in page 16)\n\n* The voice synthesis and voice cloning features could be used for impersonation\n\n* Possible impact on employment is not discussed; could this work be done in such a way that it allows human video editors to collaborate with the system?"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hriGuqzCrR", "forum": "cTqGsLYkRl", "replyto": "cTqGsLYkRl", "signatures": ["ICLR.cc/2026/Conference/Submission6731/Reviewer_pQcp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6731/Reviewer_pQcp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761239291987, "cdate": 1761239291987, "tmdate": 1762919018825, "mdate": 1762919018825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VideoAgent, an multi-agent framework for automated video understanding and editing, aiming to enable general-purpose video creation with coherent narratives and long-video reasoning. The framework consists of two major components including automated video shot creation and multi-agent orchestration. Besides, a new VideoEdit benchmark is introduced for evaluation. Experiments on video understanding, video retrieval, and workflow orchestration show that VideoAgent outperforms existing multimodal LLMs and agentic systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe presentation is clear and easy to follow.\n\n2.\tThe experiments and visualizations are reasonable and well done.\n\n3.\tOver 30 tool agents support a wide range of operations (audio, visual, translation, meme creation, etc.), suggesting high practical applicability."}, "weaknesses": {"value": "1.\tWhile the system integration is impressive, most modules (retrieval, trimming, intent parsing) adapt existing methods rather than proposing new algorithms.\n\n2.\tThe reliance on proprietary or external APIs (e.g., GPT-4o, Claude-Sonnet, Gemini-2.5) may limit true reproducibility and comparability.\n\n3.\tThe new VideoEdit benchmark seems self-curated and may not fully represent real-world creative diversity.\n\n4.\tThe paper lacks discussion on computational efficiency or latency of the full multi-agent pipeline — an important factor for large-scale or real-time production."}, "questions": {"value": "1.\tThe paper mentions multi-agent orchestration – integrating more than 30 specialized editing agents for diverse operations (e.g., rhythm detection, voice cloning, translation, and trimming). Can the entire system operate end-to-end automatically, or does it require manual intervention between stages? If so, how efficient is the end-to-end pipeline in real use cases? What is the average generation time and computational cost for producing a multi-scene video?\n\n2.\tGiven so many external or API-based agents, how reproducible are the results if other researchers attempt to re-run the same pipeline?\n\n3.\tHow well does the system handle long-form or multi-hour videos? Are there memory or latency constraints when orchestrating dozens of agents?\n\n4.\tHow does the self-reflective orchestration prevent error propagation between dependent agents, and can failed subgraphs be re-executed automatically?\n\n5.\tWhat is the maximum number of characters/scenes that VideoAgent can process simultaneously while maintaining quality and coherence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P53MFRUfZ9", "forum": "cTqGsLYkRl", "replyto": "cTqGsLYkRl", "signatures": ["ICLR.cc/2026/Conference/Submission6731/Reviewer_CE6n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6731/Reviewer_CE6n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894856275, "cdate": 1761894856275, "tmdate": 1762919018179, "mdate": 1762919018179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VideoAgent, an agent-based framework for automated video editing. By introducing a global-aware video shot creation mechanism and a self-reflective agent graph orchestration strategy, VideoAgent demonstrates promising results. Nevertheless, the paper still has several aspects that could be further improved."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on the task of video editing and content creation, which holds significant practical value in real-world applications.\n\n2. The paper is well-written and easy to follow, with a comprehensive appendix that provides detailed explanations of the technical aspects of the proposed work."}, "weaknesses": {"value": "1. The definition and research scope of the task are not clearly articulated. Video editing is a highly broad concept, and the authors should explicitly specify which sub-tasks are covered by this work.\n\n2. In Section 2.3.1, the authors mention functionalities such as face swapping and lip synchronization, yet there appears to be no corresponding agent described in Appendix A.5.\n\n3. The paper lacks methodological novelty and sufficient contribution; the proposed system is largely built upon existing techniques and relies heavily on prompt engineering rather than introducing new algorithmic insights.\n\n4. The overall framework appears redundant and overly complicated. Constructing a dedicated dataset to train a more compact and unified model would likely be more effective.\n\n5. The paper makes extensive use of LLMs, but does not include a dedicated section “Usage of LLMs”."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mQisVrLNbk", "forum": "cTqGsLYkRl", "replyto": "cTqGsLYkRl", "signatures": ["ICLR.cc/2026/Conference/Submission6731/Reviewer_oTRV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6731/Reviewer_oTRV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966546881, "cdate": 1761966546881, "tmdate": 1762919017546, "mdate": 1762919017546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents VideoAgent, an all-in-one agentic framework that integrates video understanding, editing, and workflow orchestration within a unified system. It introduces a shot planning agent for coherent long-form video generation and a self-reflective agent graph orchestration module that dynamically assembles workflows using specialized agents. Evaluations on the new VideoEdit benchmark show significant improvements over baselines such as VideoRAG and VideoMind. Human evaluations rate its outputs close to professional-level quality, demonstrating strong potential for scalable, automated video creation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Comprehensive End-to-End Pipeline:\nThe manuscript describes a full pipeline for generating video content from multi-modal inputs. By integrating narrative planning with an execution engine, it effectively links high-level creative intent with concrete video editing and synthesis tasks, offering a coherent workflow from intention to output.\n\n2. Flexible Multi-Agent Orchestration:\nThe multi-agent orchestration framework constitutes a strong systems contribution. Its graph-based, self-reflective architecture that dynamically assembles workflows from over thirty specialized agents demonstrates scalability and adaptability. This design is well suited to the non-linear and modular nature of video editing tasks.\n\n3. Benchmark and Evaluation Framework:\nThe introduction of the VideoEdit benchmark is a valuable contribution to the community, offering a standardized resource for comparison in future work. The empirical validation includes ablation studies and performance analyses that provide credible evidence of the system’s efficiency and effectiveness."}, "weaknesses": {"value": "1. Novelty and Positioning:\nThe shot-planning module and graph-based orchestration are conceptually related to existing systems such as TeaserGen (Xu et al., 2025) and GPTSwarm (Zhuge et al., 2024). The paper would benefit from clearer articulation of domain-specific innovations tailored to video editing.\n\n2. Evaluation Methodology:\nThe human evaluation lacks detail on criteria, sample selection, and scoring consistency. The use of a single quality metric limits interpretability, and missing information about excluded baselines and cost calculations weakens transparency.\n\n3. Benchmarking Scope:\nThe evaluation compares only against general-purpose frameworks. Including domain-specific systems such as ReelDeal or VideoRepurpose would better contextualize performance claims."}, "questions": {"value": "1. How does the proposed system fundamentally differ from existing narration-driven or graph-based orchestration frameworks like TeaserGen or GPTSwarm?\n\n2. Could the authors provide more detail on the human evaluation protocol, including rating criteria and inter-rater agreement?\n\n3. Does the reported cost-efficiency include the entire pipeline or only selected phases?\n\n4. Are there plans to extend evaluation to additional video categories or domain-specific baselines for a fairer comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fjZod9l3Y7", "forum": "cTqGsLYkRl", "replyto": "cTqGsLYkRl", "signatures": ["ICLR.cc/2026/Conference/Submission6731/Reviewer_bw9K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6731/Reviewer_bw9K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6731/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987612934, "cdate": 1761987612934, "tmdate": 1762919017140, "mdate": 1762919017140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}