{"id": "xYFMinJcJ2", "number": 1248, "cdate": 1756867477659, "mdate": 1759898219192, "content": {"title": "SynopticMind: An Instruction Tuning MLLM For Weather  Forecasting Report Generation", "abstract": "Accurate weather forecast reporting enables individuals and communities to better plan daily activities, agricultural operations, and transportation. However, the current reporting process primarily relies on manual analysis of multi-source data, which often leads to information overload and reduced efficiency. With the rapid advancement of multimodal large language models (MLLMs), leveraging data-driven models to analyze and generate reports in the weather forecasting domain remains largely underexplored. In this work, we propose the Weather Forecasting Report (WFR) task and construct the first instruction-tuning dataset for this task, named WFInstruct. Based on this corpus, we develop the first model, SynopticMind, specialized in generating weather forecast reports. Experiments on our dataset show that SynopticMind surpasses leading GPT-5. In addition, we analyze the generalization ability of the model, examine the influence of different visual inputs, and evaluate the contribution of individual categories of meteorological variables. SynopticMind offers valuable insight for developing MLLMs specialized in weather report generation.", "tldr": "", "keywords": ["MLLM", "Weather Report"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/219082ea3946b6093e7d4182926761eed98f9b28.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a multimodal task and model for generating human-style weather forecast reports from ERA5 heatmaps. The pipeline trains a Qwen-VL variant via SFT to “RFT” (mining multiple generations) to DPO, and evaluates with an LLM-as-judge on two rubric dimensions. Results show improvements over several baselines on the authors’ dataset."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors tackle the important problem of generating weather reports from ERA5 data. \n2. The paper is well-presented and relatively easy to follow along.\n3. The authors include code, the complete dataset, and other artifacts are included as part of the submission, aiding reproducibility and comprising of a substantial contribution."}, "weaknesses": {"value": "1. The same class of LLMs is used to (a) score intermediate generations during RFT/DPO and (b) score final outputs at evaluation time. This design creates a closed loop where the model can learn judge-specific artifacts rather than meteorological correctness, heightening the risk of reward hacking.\n\n2. The 0–5 “factual accuracy” prompt lacks a rubric defining what each score means and provides no protocol for repeated sampling or inter-rater checks. Prior work [1, 2] shows LLM evaluators can be inconsistent and biased, which undermines conclusions drawn from a single judge with a vague scale. A stronger setup would (i) define clear rubrics that define the value for each ordinal of the produced score (ii) diversify (or swap) the judge model at evaluation, (iii) include human forecaster side-by-side, or at the very least, check the alignment between human expert scores and LLM scores.\n\n3. RFT amplifies samples produced by the same model family and selected by the same judge, encouraging alignment to judge-preferred phrasing rather than ground-truth meteorology. A more principled alternative is to paraphrase the gold reports with a text LLM (with factuality checks) to increase lexical diversity while preserving facts. If self-augmentation is retained, the paper should quantify factual drift against ERA5 signals and compare against gold-paraphrase augmentation.\n\n\n### References\n\n[1] Lee, Noah, Jiwoo Hong, and James Thorne. \"Evaluating the consistency of llm evaluators.\" arXiv preprint arXiv:2412.00543 (2024). \n[2] Stureborg, Rickard, Dimitris Alikaniotis, and Yoshi Suhara. \"Large language models are inconsistent and biased evaluators.\" arXiv preprint arXiv:2405.01724 (2024)."}, "questions": {"value": "### Questions (in addition to points raised in the Weaknesses section)\n\n1. You set a threshold of 5 for HK but 4 for other cities when selecting RFT samples. Why is HK strictly filtered while others are looser?\n2. Which fields from ERA5 were given to the model? How was the context provided, i.e. as a single image or multiple images? This should be stated clearly in the main paper.\n\n\n\n### Suggestions\n\n1. Improve the grading rubric and compare against human expert evaluators (if possible). Report more diverse metrics that focus on factuality.\n2. Line 85: typo ('extensove'). Line 128: Direct Preferred Optimization should be Direct Preference Optimization.\n3. Consider writing a more descriptive caption for Figure 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vJtS1ng4PH", "forum": "xYFMinJcJ2", "replyto": "xYFMinJcJ2", "signatures": ["ICLR.cc/2026/Conference/Submission1248/Reviewer_YHTv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1248/Reviewer_YHTv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761258859999, "cdate": 1761258859999, "tmdate": 1762915716822, "mdate": 1762915716822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SynopticMind, a multimodal large language model (MLLM) designed for weather forecasting report generation. The authors propose a new task, named Weather Forecasting Report (WFR) and construct the first instruction-tuning dataset, WFInstruct, which pairs ERA5-based meteorological heatmaps with expert-written weather reports. Built on Qwen2.5-VL-7B, SynopticMind is trained through a three-stage process combining supervised fine-tuning, rejection sampling, and preference optimization to enhance factual accuracy, lexical diversity, and alignment with human reporting styles. The paper’s main contributions are:\n\n1. Introduction of the WFR task and WFInstruct dataset, enabling multimodal instruction-tuned weather report generation.\n\n2. Development of SynopticMind, the first open-source MLLM specialized for generating human-readable and meteorologically accurate forecasts.\n\n3. Comprehensive evaluation demonstrating that SynopticMind-DPO surpasses GPT-5 and other leading MLLMs in accuracy, detail, and cross-city generalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and easy to follow.  \n\nThe authors propose a novel multimodal task and dataset for weather report generation, extending instruction tuning to a new and practical domain.  \n\nThe three-stage training pipeline (SFT→RFT→DPO) is methodologically sound and effectively combines existing alignment techniques in a new application setting.  \n\nThe work demonstrates clear practical significance by outperforming strong baselines and highlighting a promising direction for domain-specialized MLLMs."}, "weaknesses": {"value": "1. While the paper introduces a new dataset and task, the novelty is primarily in dataset construction and application rather than in model architecture or learning principles; the method relies heavily on established techniques (SFT, RFT, DPO) without clear algorithmic innovation.  \n2. All evaluation depends on LLM-as-a-Judge scoring, which is known to be prone to bias [1]. Complementary human or meteorological expert assessments would improve credibility.  \n3. My biggest concern is the evaluation design. The LLM-as-a-judge provides only an overall score for entire reports, which may mix correct and incorrect statements without transparency.  A more rigorous and explainable evaluation would be decomposing reports by temporal segments or forecast points, enabling fine-grained comparison with human reports and clearer correctness signals.\n4.  \"Direct Preferred Optimization\" on line 128 should be \"Direct Preference Optimization\", \n\n\n[1] Gu, Jiawei, et al. \"A survey on llm-as-a-judge.\" arXiv preprint arXiv:2411.15594 (2024)."}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J7wcni2isn", "forum": "xYFMinJcJ2", "replyto": "xYFMinJcJ2", "signatures": ["ICLR.cc/2026/Conference/Submission1248/Reviewer_HBEr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1248/Reviewer_HBEr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794784352, "cdate": 1761794784352, "tmdate": 1762915716599, "mdate": 1762915716599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the task of weather forecast report generation using multimodal large language models (MLLMs). They construct a instruction-tuning dataset for this task (WFInstruct), and then develop the SynopticMind model, which specializes in weather report forecasts. The authors claim that their model surpasses GPT-5 and other SOTA models in this task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) They address an important task with a clear gap in the literature where current models are not strong.\n\n(2) Their three stage training pipeline appears effective, with each stage providing additional gains.\n\n(3) They include extensive ablations, such as examining the effects of different variables (Figure 6), which offer useful insights."}, "weaknesses": {"value": "(1) While asking an LLM to assign a score from 0 to 5 is useful, it is too abstract on its own. A complementary approach would be to extract claims from both the ground truth and the generated reports and compute precision, recall, and F1 over those claims.\n\n(2) In line 234, the threshold is set to 5 for HK and 4 for other cities. The rationale for this difference is unclear and appears arbitrary.\n\n(3) Given the abstract nature of the metric, I am concerned about how reliable the claim is that the model outperforms GPT-5 and similar systems.\n\n(4) Lines 351 to 365 state that training on some cities yields good results on geographically close locations but poor results on distant ones. This warrants deeper analysis, as it may indicate that evaluation on nearby locations is too similar to the training distribution and suggests overfitting rather than true generalization.\n\n(5) The paper’s writing needs significant improvement. Table captions are not self explanatory. For example, Table 1 should state that the column headers are city acronyms, and the word performance should be explicitly defined in Table 2. Many explanations appear too late, and readers must search across sections to understand earlier references. The ordering of information is off, with concepts introduced long before they are clearly explained."}, "questions": {"value": "(1) The paper mentions four distance based strategies to measure diversity, but it is unclear how these strategies are combined or selected in practice.\n\nPlease also refer to the \"Weaknesses\" for additional questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5tFsC6KhJC", "forum": "xYFMinJcJ2", "replyto": "xYFMinJcJ2", "signatures": ["ICLR.cc/2026/Conference/Submission1248/Reviewer_DYHt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1248/Reviewer_DYHt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974469336, "cdate": 1761974469336, "tmdate": 1762915716422, "mdate": 1762915716422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SynopticMind, a multimodal instruction-tuned LLM for automatic weather report generation. We introduce the Weather Forecasting Report (WFR) task and release WFInstruct, a city-level paired dataset of weather imagery and human reports. Built on Qwen2.5-VL-7B, SynopticMind uses a three-stage training pipeline to improve report quality and is evaluated against leading closed-source LLMs with extensive ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly motivated by the information overload and subjectivity issues in current weather report workflows \n2. The introduction and public release of WFInstruct offer a much-needed benchmark for aligning multimodal models with real weather forecast report writing, addressing a major gap in the field."}, "weaknesses": {"value": "1. Limited Breadth of Data/Domains:dataset and results are limited to ten cities, the results may not fully represent global performance or non-English settings, and generalization to low-data regions is not directly evaluated.\n2. Evaluation Standardization and LLM-as-Judge Reliability: Main evaluation relies on LLM-as-judge, There is no human expert evaluation reported, and it remains unclear how robust or calibrated LLM-judgment is to nuanced meteorological reporting, which may undermine the conclusiveness of the main results.\n3. Recent vision-language models for meteorological (Specialized model) are missing."}, "questions": {"value": "1. Is there any experiment/analysis regarding the robustness of predictions to rare but high-impact severe events (e.g., typhoons, severe storms), versus routine, fine-grained forecasts?\n2. Can the authors clarify to what extent the reported improvements in Table 2 would persist if using human (domain expert) evaluators.\n3. How difficult would it be to extend WFInstruct to additional languages or locations? Are there any preliminary results or barriers for, say, European or African cities, or non-English reports?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hsL2ITcgeN", "forum": "xYFMinJcJ2", "replyto": "xYFMinJcJ2", "signatures": ["ICLR.cc/2026/Conference/Submission1248/Reviewer_Emta"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1248/Reviewer_Emta"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991844407, "cdate": 1761991844407, "tmdate": 1762915716261, "mdate": 1762915716261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}