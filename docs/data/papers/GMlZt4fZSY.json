{"id": "GMlZt4fZSY", "number": 6924, "cdate": 1758002166143, "mdate": 1763688707621, "content": {"title": "Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes", "abstract": "The paradigm shift in large language models (LLMs) from instinctive responses to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1) reasoning capabilities only emerge in sufficiently large models, and (2) such capabilities require training on massive datasets. While the first assumption has already been challenged by recent sub-billion-parameter reasoning models such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely unquestioned. In this work, we revisit the necessity of scaling to extremely large corpora (>10T tokens) for reasoning emergence. By carefully curating and resampling open-source datasets that we identify as beneficial under our designed metrics, we demonstrate that strong reasoning abilities can emerge with far less data. Specifically, we show that only ~2T tokens of high-quality data are sufficient, and pre-training with 4.2T tokens on the dataset resampled from these ~2T tokens, followed by a established post-training procedure, enables the development of X-LLM-R1, a series of sub-billion-parameter reasoning models that substantially outperform prior models trained on fully open-sourced data. For example, X-LLM-R1-950M achieves an AIME score of 15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B. Remarkably, despite being trained on only 11.7% of the tokens compared to Qwen3’s proprietary 36T-token corpus for pretraining, X-LLM-R1-950M matches or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate further research in this direction, we release the complete training recipe, data sources, data mixing ratio, and model checkpoints, together with the key insights obtained throughout this study.", "tldr": "", "keywords": ["on-device model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8b803030a63894616149867dc118e35851f30aa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper challenges the prevailing assumption that achieving strong reasoning capabilities necessitates massive training datasets (e.g., >10T tokens). The authors propose a data-centric and highly curated training pipeline, introducing a series of sub-billion parameter language models called X-LLM-R1 (140M, 360M, 950M).\n\nThe core contribution of this work is a fully open-sourced \"recipe\" for efficiently training small reasoning models, which features two main stages of innovation:\n1.  **Pre-training Stage:** A \"Datamixing via Cross-Capability Self-Influence\" strategy is proposed. This benchmark-free method automatically optimizes the mixing ratios of various open-source datasets by calculating the influence scores of data samples on \"capability-probing datasets\" (covering code, math, and knowledge).\n2.  **Mid-training Stage:** A \"data-model co-evolution\" strategy for knowledge compression is introduced. This strategy dynamically computes sample influence scores during training and iteratively removes (filters out) samples with zero or negative influence, enabling the model to absorb knowledge more efficiently.\n\nExperimental results show that the X-LLM-R1-950M model, trained on only ~2T tokens of curated open-source data (4.2T tokens total pre-training), matches or even surpasses the performance of Qwen3-0.6B (trained on 36T tokens) on several reasoning benchmarks, particularly AIME and HumanEval. The authors commit to releasing all data sources, mixing ratios, models, and code."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Important Research Problem:** The paper tackles a significant research question: how to train small, efficient, and deployable reasoning models. Its argument for \"quality over quantity\" in data provides a valuable path for researchers with limited resources.\n2.  **Novel Methodology:** The core contributions (influence-score-based pre-training mixture and mid-training knowledge compression) are novel and principled. The \"benchmark-free\" nature of these methods is a key strength, avoiding overfitting to downstream benchmarks.\n3.  **Strong Empirical Results:** The results are highly competitive. X-LLM-R1 significantly outperforms other fully open-source models (like OLMo, SmolLM) at all parameter scales. The fact that X-LLM-R1-950M (4.2T tokens) achieves performance comparable to or better than Qwen3-0.6B (36T tokens) on key reasoning benchmarks is a strong demonstration of the recipe's effectiveness.\n4.  **Commendable Transparency and Reproducibility:** The paper excels in its commitment to openness. The authors promise to provide full details of their training recipe, including all data sources, mixing ratios, architecture, and hyperparameters. This transparency is of great value to the community.\n5.  **Insightful Ablation Studies:** The paper provides valuable insights through detailed ablations, such as the LOO analysis of data sources, the effect of learning rates, and the SFT vs. RL discussion."}, "weaknesses": {"value": "1.  **Implicit Computational Cost of Curation:** The paper emphasizes its \"token efficiency\" but does not explicitly discuss the computational cost of the data curation process itself. For instance, (1) the LOO analysis requires training multiple models; (2) calculating influence scores, while scalable, also requires significant compute (e.g., training domain-specific checkpoints). A comparison of the *total compute* (curation compute + training compute) versus a \"brute force\" approach (like Qwen3's 36T) would make the \"efficiency\" argument more complete.\n2.  **Sensitivity to \"Capability-Probing Datasets\":** The pre-training data mix relies heavily on the constructed \"capability-probing datasets.\" Although this process is \"benchmark-free,\" the construction itself (e.g., using Ask-LLM with specific prompts for hierarchical rejection sampling) introduces designer priors. The paper lacks a sensitivity analysis on how robust the final data recipe is to changes in the design of these probing datasets.\n3.  **Trade-off between Reasoning and Knowledge:** The results in Table 8 show that while X-LLM-R1-950M-base excels on GSM8K and HumanEval, it lags behind Qwen3-0.6B-Base and SmolLM2-1.7B-base on MMLU. This suggests that the curation strategy, which is highly optimized for code and math reasoning, might come at the cost of retaining broad factual knowledge."}, "questions": {"value": "1.  **Regarding Curation Compute Cost:** Could the authors provide an estimate of the computational overhead required for the full data curation process (LOO analysis, influence score calculation, etc.)? How does the *total compute budget* (curation + training) of this method compare to the budget needed to simply train a baseline (like Qwen3-0.6B) on 36T tokens?\n2.  **Regarding Sensitivity to Probing Datasets:** The pre-training mix depends on the \"capability-probing datasets.\" How much would the final data recipe and model performance change if the construction of these datasets were altered (e.g., different Ask-LLM prompts, or a different sampling threshold than 10%)?\n3.  **Regarding the Knowledge vs. Reasoning Trade-off:** The MMLU performance in Table 8 suggests a potential trade-off where optimizing for reasoning (especially in pre-training) may reduce general knowledge. Was this an intentional design choice, or do the authors view this as an unavoidable trade-off for small models with limited capacity?\n4.  **Regarding Mid-training Convergence:** In Section 3, the model undergoes two \"knowledge compression\" stages. Why were two stages chosen? As shown in Figure 5, the influence scores are indeed compressed in Stage 2, but have they fully converged (i.e., reached the \"zero or negative influence\" state)? Could the authors provide the percentage of data that was filtered out in each stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vvLy3VufR3", "forum": "GMlZt4fZSY", "replyto": "GMlZt4fZSY", "signatures": ["ICLR.cc/2026/Conference/Submission6924/Reviewer_9Hby"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6924/Reviewer_9Hby"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916027073, "cdate": 1761916027073, "tmdate": 1762919160400, "mdate": 1762919160400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper the authors explore whether it’s possible to obtain strong reasoning capacity in sub-billion parameter models, without using proprietary data or an enormous compute budget. They introduce the X-LLM-R1 family of models ranging from 140M to 950M parameters, trained only on ~ 4.2T tokens. (NB: compare to Qwen-3, this is ~ 12% of the data). The achieve state of art reasoning results among open models, and demonstrate this ability comes more from data quality than pure scale, and show how to co-evolve the model and data using an influence score to focus the training procedure on data that will still contribute to the learning.\n\nThe authors also will (/ have but can’t link due to double blind review) release the data, training recipe, architecture implementation, making this an extremely transparent piece of work.\n\nOverall it was a pleasure to read this paper, and I hope my comments provide a useful perspective."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- This paper presents an extremely clear example of the separation of pre-, mid-, post-training and fine-tuning - and gives strong evidence of the importance of each. This makes for an extremely valuable resource for others to build upon.\n    \n    - As does the open release of the model.\n        \n- The performance on key benchmarks is extremely good - especially for an open source, sub-billion parameter model.\n    \n- The LLO analysis offers valuable insight into the data mixtures required during an efficient pre-training phase, and which of these are the most influential.\n    \n- The key novelty presented in the paper is the use of an influence sampling / weighting method that computes the impact of training data on a approximate sample wise basis on the “probing datasets” - this allows the training corpus to be resampled between each phase during mid training to focus on high utility datasets, making the training process extremely efficient.\n    \n- The removal of low / negatively impacting data samples offers a clear and seemingly extremely efficient way to train models using significantly fewer tokens than naively would be expected."}, "weaknesses": {"value": "- The focus of the paper is on the reasoning ability of the model - and makes only very passing comment (lines 362/379) on the impact of the knowledge retention / general usability of the model. Could further comment be provided on how the language understanding, factual retention, and general instruction alignment is affected with such a strong emphasis on reasoning during training?\n    \n- The method as presented is powerful - however the authors only show the influence sampling at two points during training (I believe) - between the two pre-training phases and between the two mid training phases. Could comment be provided on whether the authors think additional weighting phases would be more impactful? Or how frequently one could plausibly use such a method? To only compute the influence and weight twice might not be so different to hand curated weightings of data mixtures based on intuition. And that the real power of such a method might be in the ability to compute this frequently?\n    \n- There is obviously a compute cost associated with this sampling / probing approach - could the authors comment on this and put it in the context of the relatively small token budget required? One assumes the cost of this influence approach is not excessive, but it is noted the influence is computed only on a subset of the data.\n    \n- Additionally the weighting is applied on a dataset by dataset basis - what scope is there for differentiating between samples in a dataset which are well understood by the model vs individual samples which are poorly described?"}, "questions": {"value": "1. Would it be possible to add a set of evaluations on the models general competence compared to Qwen (ideally others, but specifically as the closet similar model) to show if there is a trade off for general ability vs reasoning specific ability? (e.g. HeelaSwag, PIQA, NaturalQuestions… etc)\n    \n2. The leave one out analysis is really nice, but I was curious about the impact on downstream benchmarks rather than just the NLL? Would it be possible to see the impact of leaving out specific datasets on the reasoning tasks rather than just the model NLL? Alternatively could the authors comment on the effect / importance of the LLO beyond just the increase in pre-training loss and what this says about pre-training data mixtures?\n    \n3. You reweight the data at two boundaries. Do you expect more frequent reweighting to help? And where would the diminishing returns point be?\n    \n4. What is the total compute overhead of training a model in this way? (In GPU days) Relative to one epoch of normal training? i.e. can you show the cost of running the evaluations for the influence as part of the analysis?\n    \n5. I would also really appreciate to see how much of the dataset is required to be sampled when computing the influence to compute the weights?\n    \n6. Can you comment on / provide the impact of weighting on each of the datasets? ie. how much do different datasets get weighted up / down at different times, and how predictable is this? Do you need to compute the influence for new models, or is this relative proportion more or less static? In which case would it be possible to take the proportions and train a similar model without the influence step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T23ZnAs4Xb", "forum": "GMlZt4fZSY", "replyto": "GMlZt4fZSY", "signatures": ["ICLR.cc/2026/Conference/Submission6924/Reviewer_dhHj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6924/Reviewer_dhHj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946179201, "cdate": 1761946179201, "tmdate": 1762919159589, "mdate": 1762919159589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to enhance the reasoning capability of sub-billion-parameter language models through data-centric training strategies.\nTechnically, the paper presents two key contributions:\nInfluence-based DataMix for pretraining, which adaptively re-weights training datasets based on their estimated contribution to different reasoning domains.\n\nMid-Training Knowledge Compression, a dynamic filtering mechanism that removes samples with non-positive influence during mid-training, enabling co-evolution between model and data to reduce redundancy and stabilize learning.\nEmpirically, the proposed 950M-parameter model outperforms OLMo-2-1.48B and SmolLM-2-1.7B, while matching or exceeding Qwen3-0.6B, using significantly less data and compute. All training recipes, datasets, and checkpoints are reported to be fully open-sourced."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* Clear motivation and practical significance: The paper challenges the common “scaling is all you need” assumption and demonstrates that strong reasoning capability can emerge in small models through carefully optimized data recipes.\n* Technical contribution is sound: Extends AutoMixer from dataset-level weighting to multi-capability, sample-level influence estimation. The proposed method leverages internal capability-probing datasets to guide data weighting without relying on external benchmarks.\n* Experimental validation is concrete: Provides impressive empirical results and demonstrates strong efficiency, achieving competitive or superior performance to larger models with substantially less data and compute."}, "weaknesses": {"value": "* Approximation reliability: The Hessian–vector product (HVP) approximation for influence estimation may introduce noise. Validation is indirect, without small-scale ground-truth comparisons or formal variance analysis.\n* Capability definition: Evaluations are limited to three domains—Code, Math, and Knowledge—oversimplifying the diversity of reasoning skills such as planning, commonsense, and multilingual reasoning.\n* Some missing baselines: The study lacks comparisons with strong alternatives, including AutoMixer, In-Run Data Shapley.\n\nChang, Ernie, et al. \"Automixer: Checkpoint artifacts as automatic data mixers.\" arXiv preprint arXiv:2506.21910 (2025).\n\nWang, Jiachen T., et al. \"Data shapley in one training run.\" arXiv preprint arXiv:2406.11011 (2024)."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mkjNOZ3dLd", "forum": "GMlZt4fZSY", "replyto": "GMlZt4fZSY", "signatures": ["ICLR.cc/2026/Conference/Submission6924/Reviewer_3Hy6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6924/Reviewer_3Hy6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967417924, "cdate": 1761967417924, "tmdate": 1762919159225, "mdate": 1762919159225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a details of data-centric framework for creating high-performing sub-billion-parameter language models (LLMs) with strong reasoning capabilities, named X-LLM-R1. The central finding challenges the assumption that reasoning requires massive training corpora (>10T tokens) by demonstrating that models trained on only 4.2T curated tokens can match or exceed models trained on 36T proprietary tokens, like Qwen3-0.6B. Key methodological contributions include a benchmark-free, self-evolving data optimization approach using cross-domain influence scores to dynamically manage the training data mixture, along with a phased pre-training and mid-training curriculum focused on token efficiency. The authors highlight the critical role of data quality and structured post-training (SFT) over sheer scale, ultimately achieving state-of-the-art results among small, fully open-sourced reasoning models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel data curation and training strategy\n\n2. Challenging the conventional belief of training with large corpora of tokens for reasoning capability emergence\n\n3. SoTA reasoning on SLMs, and making models open weight. The performance gain over other open source SLMs are convincingly better."}, "weaknesses": {"value": "1. Inherent Limitations of Small Model Capacity still remains a questionable issue along with the constraint of performance on long context data.\n\n2. Ineffectiveness of RL on SLMs is a concern here.\n\n3. The concept of reasoning remains vague in the paper."}, "questions": {"value": "1. Can you please test in details the long context inference performance with these models?\n\n2. Please compare with SoTA reasoning models for the reasoning tasks (not only the instruction tuned models)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pLUD2002up", "forum": "GMlZt4fZSY", "replyto": "GMlZt4fZSY", "signatures": ["ICLR.cc/2026/Conference/Submission6924/Reviewer_GtCo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6924/Reviewer_GtCo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973336865, "cdate": 1761973336865, "tmdate": 1762919158632, "mdate": 1762919158632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank reviewers for their positive feedback and insightful comments. We are glad that reviewers recognized the following strengths:\n\n- **Important research direction**: challenges the common “scaling is all you need” assumption and demonstrates that strong reasoning capability can emerge in small models through carefully optimized data recipes. [Reviewers **GtCo**, **3Hy6**, **dhHj**, ​​**9Hby**]\n\n- **Novel and principled methodology**: utilizes novel “benchmark-free, self-evolving” optimization that avoid overfitting and ensure generalizability. [Reviewers **GtCo**, **9Hby**]    \n\n- **Strong experimental results**: significantly outperforms other fully open-source models. [Reviewers **GtCo**, **3Hy6**, **dhHj**, ​​**9Hby**]\n\n- **Commendable transparency**: All training recipes, datasets, and checkpoints are fully open-sourced [Reviewers **3Hy6**]. This makes for an extremely valuable resource for others to build upon. [Reviewers **dhHj**, **9Hby**]. \n\nThe discussion raised a lot of insightful and valuable questions that have helped us strengthen the comprehensiveness of the paper. In response to the reviewers’ comments, we have revised the manuscript accordingly, with all modifications highlighted in blue. The major modifications are listed below:\n\n- Added a discussion section of the reasoning concept, and outlined directions for studying small language models on broader reasoning tasks in the future work section. [Reviewers **GtCo**, **3Hy6**]\n\n- Included variance analysis of the sensitivity to capability-probing datasets. [Reviewers **3Hy6**, ​​**9Hby**]\n\n- Compared with more datamix baselines. [Reviewers **3Hy6**]\n\n- Added analyses comparing how strong reasoning-centric training impacts language understanding, factual retention, and general instruction alignment.  [Reviewers **dhHj**]\n\n- Added ablations on more frequent reweighting and on whether it is necessary to compute the influence for new models. [Reviewers **dhHj**]\n\n- Included the computational cost of data curation. [Reviewers **dhHj**, ​​**9Hby**]\n\n- Listed the percentage of data that was filtered out in each stage. [Reviewers **9Hby**]\n\nOverall, we appreciate the reviewers’ recognition of the paper’s new insights, as well as its openness and reproducibility. We hope these contributions can help drive continued progress toward fully open language models in the broader community."}}, "id": "A5dT8paIwH", "forum": "GMlZt4fZSY", "replyto": "GMlZt4fZSY", "signatures": ["ICLR.cc/2026/Conference/Submission6924/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6924/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission6924/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763689318470, "cdate": 1763689318470, "tmdate": 1763689574209, "mdate": 1763689574209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}