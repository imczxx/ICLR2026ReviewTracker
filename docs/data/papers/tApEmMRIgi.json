{"id": "tApEmMRIgi", "number": 7198, "cdate": 1758011277142, "mdate": 1759897867167, "content": {"title": "Inductive Representation Learning of Temporal Pattern Subgraphs in Temporal Networks", "abstract": "Subgraph structure learning on graph neural networks (GNN) has attracted considerable attention recently because of its capacity to encode high-level graph structural features. Temporal network representation learning has been used in many real-world dynamic systems that usually evolve according to some temporal patterns, such as the triadic closure laws in social networks. Inductive representation learning of temporal networks should be able to capture these temporal patterns and further apply them to nodes which were not discovered during the training. Previous work neglected to extract these patterns, or could not be applied to downstream tasks because of the high time complexity of matching. In this paper, we design the strategy for capturing two types of prevalent temporal patterns. We propose the TPSN framework for inductive temproal pattern learning, in which we perform adjacent edge reconstruction on the extracted subgraphs, thereby improving the learning efficiency of temporal triadic closure laws and reducing the possibility of oversmoothing. Furthermore, we use multi-subgraph contrastive learning to achieve higher accuracy with fewer negative samples. Our proposed method outperforms baselines in all three downstream tasks and maintains acceptable time complexity. Ablation experiments also validate the effectiveness of our proposed model and module.", "tldr": "", "keywords": ["graph representation learning; time series analysis"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2af7f5698239accf3bcbc4067adf62992f2e9330.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TPSN, a novel framework for inductive representation learning in temporal graphs, designed to model dynamic relationships where test-time nodes may not have been observed during training. The method integrates information from two temporal pattern subgraphs, TFS-subgraph (capturing recent interactions) and FFS-subgraph (capturing frequent interactions), to derive node embeddings that effectively encode the laws of node evolution, such as the triadic closure principle.\n\nTo unify representations across multiple temporal subgraphs, TPSN employs a multi-subgraph contrastive learning strategy that aligns embeddings of nodes likely to interact in the future while maintaining separation between unrelated ones.\n\nEmpirical results across three downstream tasks, including transductive link prediction, inductive link prediction, and dynamic node classification, on two benchmark datasets (Reddit and Wikipedia) show that TPSN consistently outperforms baseline methods. Complementary ablation studies further validate the contribution of each component, highlighting the effectiveness of combining TFS and FFS subgraphs with contrastive learning for temporal pattern modelling."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a novel inductive representation learning framework that effectively generalizes to unseen nodes in dynamic graphs by combining TFS-Subgraph (capturing recency-based temporal patterns) and FFS-Subgraph (capturing frequency-based temporal patterns) to model key laws of node evolution, such as triadic closure.\n\n- The papers presents a new contrastive optimization approach across multiple temporal subgraph types, enabling TPSN to reconcile and unify embeddings from heterogeneous temporal patterns through multi-subgraph contrastive learning.\n\n- On two benchmark datasets, TPSN consistently outperforms several state-of-the-art baselines across diverse evaluation settings, including transductive and inductive link prediction as well as dynamic node classification.\n\n- Comprehensive ablation and attention analyses demonstrate the effectiveness and interpretability of the proposed components, showing that integrating multiple temporal subgraphs and the contrastive learning module significantly contributes to performance gains."}, "weaknesses": {"value": "**W1: Wring and Readability.** The current manual script has several writing and formatting issues that hinder readability and comprehension.\n- Please insert spaces between model names and author names, and use \\citep to enclose author names in parentheses, clearly separating method names from citations.\n\n-  Line 093: avoid repeating author names; use \\cite instead of repeating to maintain clarity.\n\n- The paper lacks a clear problem statement section. It is recommended to explicitly define the problem, the tasks addressed, and relevant definitions, especially for readers new to temporal graph learning, immediately before Section 3. This should include the definition of temporal graphs (currently Lines 139-143) and formal problem formulations for link prediction and node classification.\n\n- Several variables and notations are used without prior definition, resulting in confusion. For example, in Algorithm 1, the variable “C” is used before being defined (Lines 2 and 3). Similarly, the index “i” appears undefined in Equations 1-3.\n\n-  Notation throughout the paper is inconsistent. For instance, Line 215 uses “i” to denote a node, whereas Lines 195-198 use “u” and “v.” In Algorithm 1, “j” is an index, but in Algorithm 2, “j” refers to nodes. This inconsistency should be resolved for clarity.\n\n- In Algorithm 1, the variable name “Neighbor(N)” is confusing, as it syntactically resembles a function call on N rather than a variable or data structure\n\n**W2: Out-of-date Baselines.** The work compares the performance of TGT against out-of-date baselines. A naive but effective baseline, such as EdgeBank[1], and recent TGNN models[2,3,4]\n\n**W3: Reproducibility.** Neither the source code for implementation nor the details about the hyperparameters (e.g learning rate, number of epochs, $\\alpha$ used in Equation 5, etc.) used in experiments. These questions about the reproducibility of the work.\n\n**W4: Dataset Diversity.** TPSN is evaluated only on two relatively limited datasets. For broader validation, consider including additional, larger-scale, and diverse benchmark datasets from the Temporal Graph Benchmark (TGB) [5] , covering more comprehensive link prediction and node property prediction scenarios.\n\n**W5: Appendix Structure.** The appendix section would benefit from improved structure and should include detailed information about the baselines used, including their configurations and any implementation details relevant to fair comparison.\n\n\n**Minor**\n- The paper mainly reports AUC and AP for link prediction. Including Mean Reciprocal Rank (MRR) under the Temporal Graph Benchmark (TGB) evaluation settings would provide a more comprehensive view of TPSN 's ranking performance and enhance comparability with other temporal graph models evaluated on TGB[5].\n---\n\n[1] Poursafaei, Farimah, et al. \"Towards better evaluation for dynamic link prediction.\" *Advances in Neural Information Processing Systems* 35 (2022): 32928-32941.\n\n[2] Yu, Le, et al. \"Towards better dynamic graph learning: New architecture and unified library.\" *Advances in Neural Information Processing Systems* 36 (2023): 67686-67700.\n\n[3] Lu, Xiaodong, et al. \"Improving temporal link prediction via temporal walk matrix projection.\" *Advances in Neural Information Processing Systems* 37 (2024): 141153-141182.\n\n[4] Ding, Zifeng, et al. \"Dygmamba: Efficiently modelling long-term temporal dependency on continuous-time dynamic graphs with state space models.\" *arXiv preprint arXiv:2408.04713* (2024).\n\n[5] Huang, Shenyang, et al. \"Temporal graph benchmark for machine learning on temporal graphs.\" Advances in Neural Information Processing Systems 36 (2023): 2056-2073."}, "questions": {"value": "- It is unclear whether Algorithm 1 assumes that $C$ is chronologically sorted. If the correctness or stability of the algorithm depends on the temporal ordering of $C$, this requirement should be explicitly stated at the beginning.\n\n- In Algorithm 2, please clarify the criterion for selecting the top‑k elements; is it based on time order, interaction frequency, or another factor?\n\n- Lines 185–188: The explanation of how the law of triadic closure is captured through two different paths in the proposed graph matching algorithm is unclear. Can the authors elaborate on the mechanism, specifically, how these paths operationally encode or approximate triadic closure within the temporal subgraph structure?\n\n- Line 194: If multiple paths exist between two nodes, how is the aggregated path distance computed, by the shortest, average, or total path length?\n\n- Lines 195–198: It is unclear why node $v_i$  is replaced by node $A$ to create a new virtual edge. How does this help to reduce the possibility of oversmoothing?\n\n- Figure 3: The meaning of “layer” is unclear. Does it refer to neighbor hops or network layers? Please clarify the terminology and ensure it is used consistently throughout the paper.\n\n- Line 266: The term “negative sample” is unclear. Does it refer to a negative edge or another sampling strategy?\n\n- Equation 6: The model’s main training task is unclear. Is it trained with positive and negative edges? Are nodes $i$\n and $j$ positive or negative pairs? Also, $h_n$ is undefined; can the author clarify this?\n\n\n- Section 4.1.4: The values for the number of 1-hop neighbors $N$ nd subgraph size $M$ are not specified. Please clarify their values and discuss how varying$N$ and $M$ affects model performance and computational cost.\n\n- Could the authors provide a detailed runtime analysis of TPSN compared to baseline methods? Specifically, how do the individual components, including TFS-subgraph generation, FFS-subgraph generation, and adjacent edge reconstruction, scale with increasing graph size, especially on large-scale datasets like those in TGB[5]?\n- It is not clear how the model is evaluation, is it evaluated with positive and negative egdes?\n\n- Tables 4 and 5: Are the results from a single run with one random seed? Please provide standard deviations to understand the variability and robustness of the results.\n\n- In Table 5, why is the result for$N=1$ bolded when $N=3$ and $N=4$ show better performance? This appears to contradict the claim on line 269 that the model achieves better results without increasing the number of negative samples. Could the authors please clarify?\n\n- DyGFormer[2] also considers recency and frequency of neighbors through a neighbor co-occurrence encoding scheme and patching technique. Could the authors clarify what differentiates TPSN from DyGFormer, particularly regarding how TPSN’s temporal pattern subgraph mining and multi-subgraph contrastive learning offer advantages over DyGFormer’s sequence-based Transformer approach?\n\n\n---\n\n[2] Yu, Le, et al. \"Towards better dynamic graph learning: New architecture and unified library.\" *Advances in Neural Information Processing Systems* 36 (2023): 67686-67700.\n\n[5] Huang, Shenyang, et al. \"Temporal graph benchmark for machine learning on temporal graphs.\" Advances in Neural Information Processing Systems 36 (2023): 2056-2073."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lFJ77ql3Vt", "forum": "tApEmMRIgi", "replyto": "tApEmMRIgi", "signatures": ["ICLR.cc/2026/Conference/Submission7198/Reviewer_vTit"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7198/Reviewer_vTit"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761230956917, "cdate": 1761230956917, "tmdate": 1762919351867, "mdate": 1762919351867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for inductive representation learning on temporal networks named TPSN. The approach extracts two types of temporal pattern subgraphs: TFS-Subgraph, which uses Time First Search to capture recent interactions, and FFS-Subgraph, which uses Frequency First Search to capture high-frequency interactions. The method performs adjacent edge reconstruction based on triadic closure laws and employs multi-subgraph contrastive learning. Experiments on Reddit and Wikipedia datasets demonstrate improvements over baselines in link prediction and node classification tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important problem to learn representations in temporal graphs by capturing two distinct temporal patterns that reflect realistic social network dynamics.\n- The proposed method based on the temporal patterns is technically sound.\n- The experiments are conducted across multiple tasks (transductive/inductive link prediction, node classification) with detailed ablation studies validating the effectiveness of the proposed method."}, "weaknesses": {"value": "- The evaluation is restricted to only two datasets (Reddit and Wikipedia), which leads to two concerns: \n  1. The graphs are not diverse enough. Testing on diverse temporal networks (e.g., financial, biological, transportation) would strengthen generalizability claims.\n  2. These two datasets are relatively small, 9-10k nodes, which cannot fully demonstrate the efficiency of the proposed method.\n- The paper lacks comparisons with more recent temporal GNN methods and subgraph-based approaches. The baselines are somewhat dated (most from 2018-2020), and missing comparisons with more recent methods such as DyGFormer [1] and GraphMixer [2].\n\n- While the approach is intuitive, the paper lacks theoretical analysis of why this particular combination of TFS and FFS patterns is optimal, how the reconstruction affects information flow mathematically, or convergence guarantees for the contrastive learning objective.\n\n- There is no complete complexity analysis of the proposed method (only the FFS-subgraph detection component contains the complexity of subgraph pattern mining).\n\n- The paper contains grammatical errors and unclear explanations.\n  1. The format of reference: Throughout the paper, citations lack proper punctuation between consecutive references, and the reference after text requires a bracket.\n  2. Some explanations are not clear. For example, the motivation for why edge reconstruction specifically helps with triadic closure learning is not clearly discussed. The results of attention analysis is not very clear.\n\n[1] Yu L, Sun L, Du B, et al. Towards better dynamic graph learning: New architecture and unified library[J]. Advances in Neural Information Processing Systems, 2023, 36: 67686-67700.\n\n[2] Cong W, Zhang S, Kang J, et al. DO WE REALLY NEED COMPLICATED MODEL ARCHITECTURES FOR TEMPORAL NETWORKS?[C]//11th International Conference on Learning Representations, ICLR 2023. 2023."}, "questions": {"value": "1. Complexity of the proposed method\n2. Experimental results on more diverse and larger-scale graphs\n3. Compareison with more recent baselines\n4. More clear explanations of model design and experimental results"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "glpGwNWUUQ", "forum": "tApEmMRIgi", "replyto": "tApEmMRIgi", "signatures": ["ICLR.cc/2026/Conference/Submission7198/Reviewer_WyES"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7198/Reviewer_WyES"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761416796775, "cdate": 1761416796775, "tmdate": 1762919351121, "mdate": 1762919351121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the TPSN framework for inductive representation learning on temporal graphs, aiming to capture temporal subgraph patterns through time-first and frequency-first mining, adjacency reconstruction, and multi-subgraph contrastive learning. The method is evaluated on Reddit and Wikipedia datasets for link prediction and node classification tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a relevant problem in temporal graph representation learning and provides a structured pipeline for subgraph-based pattern extraction.\n\n- The framework integrates multiple components—subgraph mining, positional encoding, and contrastive learning—into a unified design.\n\n- The experimental section includes comparisons across several benchmark datasets and ablation studies for individual modules"}, "weaknesses": {"value": "- The claimed methodological innovation is marginal, with many components (e.g., attention-based aggregation, triplet loss) borrowed from prior work such as TGAT and DynamicTriad without substantial conceptual advancement.\n﻿\n- Experimental settings are limited to small-scale datasets, leaving scalability, robustness, and generalizability untested.\nThe TFS and FFS subgraph detection algorithms appear to involve recursive neighborhood expansion and edge sorting. What is the actual runtime and memory overhead compared to conventional temporal motif mining? Are these steps parallelizable or optimized in implementation?"}, "questions": {"value": "- The TFS and FFS subgraph detection algorithms appear to involve recursive neighborhood expansion and edge sorting. What is the actual runtime and memory overhead compared to conventional temporal motif mining? Are these steps parallelizable or optimized in implementation?\n﻿\n- The adjacency reconstruction process seems to generate additional virtual edges. How does this affect the memory footprint and computational cost of the model, particularly in dense temporal networks?\n﻿\n- The framework incorporates multi-subgraph contrastive learning with triplet loss. Could the authors quantify the additional training cost (e.g., time per epoch or GPU memory) compared with TGAT or JODIE under the same hardware setup?\n﻿\n﻿\n- The claim that TPSN achieves better performance with fewer negative samples is interesting. Could the authors explain the trade-off between accuracy and time complexity when varying the number of negative samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MuQeo197O2", "forum": "tApEmMRIgi", "replyto": "tApEmMRIgi", "signatures": ["ICLR.cc/2026/Conference/Submission7198/Reviewer_DLTe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7198/Reviewer_DLTe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755020004, "cdate": 1761755020004, "tmdate": 1762919350242, "mdate": 1762919350242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TPSN, a novel GNN layer for temporal graphs, which is designed to explicitly encoding higher-order structures using an efficient subgraph-mining algorithm. \nThe main part of the work (Section 3) is devoted to the introduction of this encoding, which is then combined with a contrastive learning mechanism (Section 3.4).\nExperimental validation confirms the effectiveness of TPSN."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The modelling of high-order interactions in temporal graphs is a highly relevant topic.\n- The experiments are convincingly demonstrating the promising results of the method."}, "weaknesses": {"value": "- Section 3 is the central part, introducing the new architecture. It is however a quite lenghty text descripion, without a sufficiently clear formalization. This makes it hard to reproduce the work, and to understand its novelty and impact.\n\n- Part of the work is motivated by ideas typical of social interactions and complex systems. These claims are however not substantiated by references to the literature (see e.g. the second paragraph in the introduction), and no efforts are made to (at least partially) survey the existing work in this direction (see e.g. [5, 6]). \n\n- The related work and the considered baselines are not up-to-date (the newest architecture are DynAERNN and TGAT). In particular, the work almost discards an entire family of event-based architectures (TGL [1], APAN [2], DGNN [3], TGN [4], to mention a few). These choices severly limit the scope of the work, and make the experimental results not sufficiently convincing. \n\n\n[1] H. Zhou et al., TGL: A general framework for temporal GNN training on billion-scale graphs, Proc. VLDB Endow. (2022).\n\n[2]  X. Wang et al., APAN: Asynchronous propagation attention network for real-time temporal graph embedding, SIGMOD (2021).\n\n[3] Y. Ma et al., Streaming graph neural networks, SIGIR (2020).\n\n[4] E. Rossi et al., Temporal graph networks for deep learning on dynamic graphs (2020)\n\n[5] Liu, J., et al., Higher-order Structure Boosts Link Prediction on Temporal Graphs, arXiv (2025).\n\n[6] C. Battiloro et al., Generalized simplicial attention neural networks, IEEE TSIPN (2024)."}, "questions": {"value": "Apart from the points discussed above, there are several typographic errors (already in the abstract)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "prgkDwOhuR", "forum": "tApEmMRIgi", "replyto": "tApEmMRIgi", "signatures": ["ICLR.cc/2026/Conference/Submission7198/Reviewer_zZve"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7198/Reviewer_zZve"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989054527, "cdate": 1761989054527, "tmdate": 1762919349745, "mdate": 1762919349745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}