{"id": "frqrywj6Tv", "number": 9186, "cdate": 1758114481152, "mdate": 1759897738933, "content": {"title": "GeoDM: Geometry-aware Distribution Matching for Dataset Distillation", "abstract": "Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. \nHowever, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. \nIn this work, we propose a geometry-aware distribution-matching framework, called **GeoDM**, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. \nTo adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. \nOur theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.", "tldr": "", "keywords": ["Dataset distillation", "distribution matching", "manifold learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/46870ab4c892501e21c177c3151adf482cae8f37.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GeoDM, a geometry-aware dataset distillation method that performs distribution matching in a product space composed of Euclidean, hyperbolic, and spherical components. By learning per-geometry curvatures and weights and by adding a geometry-consistent optimal transport loss, the method aims to better preserve the intrinsic structure of real data than standard Euclidean distribution matching."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper argues clearly that real datasets may contain mixed geometric structures (flat, hierarchical, angular) and that matching only in Euclidean space can be limiting. Modeling in a mixed-curvature/product space is a natural response to this.\n\n2. The combination of multiple geometries, learnable curvature, and an OT-based alignment term is conceptually clean and aligned with the stated goal of structure-preserving distillation.\n\n3. The work provides a generalization argument showing that matching in a richer geometry can, under reasonable assumptions, lead to tighter approximation than purely Euclidean matching."}, "weaknesses": {"value": "1. All results are reported on classic benchmarks such as MNIST and CIFAR (10/100), mostly under very low IPC. There is no evidence on medium- or large-scale, higher-resolution, or more diverse datasets. As a result, it is unclear whether the proposed geometric modeling remains useful or practical when the data manifold is more complex, when backbones are deeper, or when the distilled set must support stronger augmentation.\n\n2. Running multiple geometries in parallel, learning curvatures, and adding an OT loss are nontrivial additions to a standard distillation pipeline. The paper does not provide a clear comparison of training time and memory usage against recent, efficient distillation methods.\n\n3. The method uses a fixed-dimensional split across Euclidean, hyperbolic, and spherical parts for all datasets. This makes the method stable on small benchmarks but raises questions about how well it adapts to larger, more structured data.\n\n4. The paper suggests that many real datasets are non-Euclidean, but it does not include experiments on clearly hierarchical, directional, or cross-domain data where the proposed product space would be most justified.\n\n5. It is difficult to tell how much of the gain comes from the geometry-aware OT term itself versus the move to a product manifold. Simpler variants (e.g., OT in the dominant geometry only) are not explored."}, "questions": {"value": "1. Can you provide results on at least one medium- or large-scale dataset (e.g., a 224×224 ImageNet setting) to show that the method does not break down or become too expensive at scale?\n\n2. What is the actual wall-clock time and memory overhead of GeoDM compared with a strong Euclidean distribution-matching baseline when using the same backbone and training schedule?\n\n3. Did you try making the allocation of dimensions to the three geometries learnable or data-dependent, and if so, what stability or performance issues arose?\n\n4. For datasets without obvious hierarchical or angular structure, do the learned weights tend to collapse to the Euclidean component, and if that happens, can the method skip the curved parts to save computation?\n\n5. Could a simpler OT variant (for example, OT only in the geometry with the largest learned weight) achieve similar performance with a lower cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "J4k4TGKosA", "forum": "frqrywj6Tv", "replyto": "frqrywj6Tv", "signatures": ["ICLR.cc/2026/Conference/Submission9186/Reviewer_32VR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9186/Reviewer_32VR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827752779, "cdate": 1761827752779, "tmdate": 1762920859357, "mdate": 1762920859357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Problem: The paper tries to tackle the limitation of dataset-distillation methods that perform distribution matching only in Euclidean latent spaces, which can miss non-Euclidean structure (e.g., hierarchical or directional/cyclical patterns) present in real data.\n\nMotivation: Under the manifold hypothesis, data may live on curved manifolds; embedding and matching in a space that can express Euclidean, hyperbolic, and spherical geometry should better preserve task-relevant structure than a flat space.\n\nProposed solution: (1) Perform distribution matching in a product Riemannian space combining Euclidean, hyperbolic, and spherical factors, implemented with a Riemannian CNN to produce geometry-aware features. (2) Use learnable curvatures for the non-Euclidean branches and map real/synthetic data into the weighted product space; align them with a DM objective (e.g., NCFM). (3) Add a geometry-aware optimal-transport loss computed in the product space to couple the factors and preserve class-conditional mass; include curvature regularization in the total loss.\n\nExperiments: On standard benchmarks, the method outperforms distillation baselines and include ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It explains why doing dataset distillation only in Euclidean space can miss real data geometry.\n- It proposes distribution matching in a product space (Euclidean + hyperbolic + spherical) so each type of structure can be represented.\n- The curvatures and the weights of the three geometries are learnable, letting the method adapt to each dataset.\n- A geometry-aware OT loss aligns real and synthetic data across the three components and avoids one component dominating.\n- Theory sounds, which tried to state that doing distribution matching in a product space (Euclidean × hyperbolic × spherical) gives a strictly tighter generalization-error bound than doing it in a single Euclidean space.\n- Experiment includes ablation studies."}, "weaknesses": {"value": "- Theory rests on specific assumptions. The analysis relies on “mild regularity” assumptions and constant-curvature product spaces (Euclidean, hyperbolic, spherical); real data may not fit these perfectly.\n- The model fixes the dimensionality of each manifold factor.\n- The method introduces learnable curvature, geometry weights, and an OT term with its own coefficient/regularization, hence more components and hyperparameters to manage.\n- Added complexity. The approach uses a product of three geometries with a Riemannian CNN plus an OT loss, which increases modeling and training complexity compared to standard Euclidean DM. Please provide experiments on the tradeoff between performance and complexity among proposed method and baselines.\n- Experiments are on MNIST (1, 28, 28), CIFAR-10 (3, 32, 32), and CIFAR-100 (3, 32, 32) only (small/medium scale). Please provide experiments on other large scale datasets.\n- Theoretical results are upper-bound guarantees (tighter than Euclidean); they do not directly quantify runtime/compute or guarantee gains on tasks beyond those tested.\n- The gain of using OT vs baseline is insignificant (71.8 vs 72.3) but the tradeoff is more complexities and higher training time because OT is not scalable. Please provide experiments to show the tradeoff clearly so readers can judge whether the ~0.5 pp gain justifies the added complexity."}, "questions": {"value": "Please address all my concerns in Weaknesses section. Besides, I have some additional questions:\n- How were manifold dimensions per branch chosen, and did you test making them learnable?\n- How sensitive are results to the curvature initialization and to the geometry-weight initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6nQpYxZtqV", "forum": "frqrywj6Tv", "replyto": "frqrywj6Tv", "signatures": ["ICLR.cc/2026/Conference/Submission9186/Reviewer_bAdL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9186/Reviewer_bAdL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933279512, "cdate": 1761933279512, "tmdate": 1762920858983, "mdate": 1762920858983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper extends dataset distillation by embedding real and synthetic data into a learned product space combining Euclidean, hyperbolic, and spherical geometries. It learns curvature and geometry weights and adds an OT term to preserve class-level mass. The goal is to respect non-Euclidean data structure during distribution matching. Experiments on several small benchmarks show consistent improvements."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- the motivation of the paper is very clear and intuitive, Euclidean latent spaces likely miss curvature.\n\n- the main idea is conceptually very intuitive to follow, a combination of several similar modules, and learnt weights."}, "weaknesses": {"value": "- The paper compares single geometry vs three, but omits two-geometry combinations (E+H, E+S, H+S) in ablation studies. Without this, the claim that all three curvatures matter remains unverified. \n\n- Some of the assumptions might be too unrealistic, for example, uniform algorithmic stability is unlikely satisfied by deep non-convex training. Empirical check to support the relevance of the theoretical terms will be necessary. \n\n- The method introduces many complicated components, and whether the performance gain is worth the complexities is of question. runtime or memory comparison is better provided for context of the practical gain introduced by the complicated method."}, "questions": {"value": "It will be good to offer empirical evidence for the three questions listed in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U0ls2z1KYB", "forum": "frqrywj6Tv", "replyto": "frqrywj6Tv", "signatures": ["ICLR.cc/2026/Conference/Submission9186/Reviewer_v4iA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9186/Reviewer_v4iA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064028615, "cdate": 1762064028615, "tmdate": 1762920858457, "mdate": 1762920858457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GeoDM, a geometry-aware dataset distillation framework that performs distribution matching in a product manifold combining Euclidean, hyperbolic, and spherical spaces. The main motivation is that existing distribution-matching methods operate solely in Euclidean spaces and fail to capture intrinsic geometric structures of data like hierarchical or cyclical patterns. The authors introduce learnable curvature parameters and weights for different geometries, along with an optimal transport loss to align real and synthetic data distributions. Theoretical analysis shows tighter generalization bounds versus Euclidean-only approaches, and experiments on several datasets demonstrate consistent improvements over state-of-the-art baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The connection between manifold hypothesis and dataset distillation is intuitive and clearly articulated. Figure 1 effectively demonstrates that data exhibits non-Euclidean geometric structure that Euclidean spaces fail to capture.\n\n2. The experiments cover multiple datasets, baselines, and ablation studies. The robustness across different distribution matching methods (DM, DSDM) and cross-architecture evaluation demonstrate generalizability.\n\n3. Theorems 4.1 and 4.2 provide mathematical justification for the approach, decomposing the error into statistical, stability, and geometric components, which offers insight into why geometry matters."}, "weaknesses": {"value": "1. The use of product manifolds, Riemannian CNNs, hyperbolic/spherical embeddings, and optimal transport are all well-established techniques. The main contribution is combining them for dataset distillation, which feels somewhat incremental. The paper would benefit from deeper insights into why this particular combination works.\n\n2. While GeoDM consistently outperforms baselines, the gains are often 1-3%, which may not justify the substantial increase in complexity (three geometry branches, learnable curvatures, OT loss). The computational cost is not discussed, but the method likely requires significantly more resources than Euclidean baselines.\n\n3. Assumption 4.1 is quite strong (e.g., assuming data lies on a mixed-curvature product manifold), and it's unclear how realistic this is for vision datasets like CIFAR-10. The paper claims this is \"empirically grounded\" but doesn't provide evidence that CIFAR-10 actually exhibits this structure beyond the 3D visualization in Figure 1.\n\n4. Several decisions appear arbitrary: Why fix dimensions (dE, dH, dS) rather than learn them? How sensitive is performance to these choices? The curvature regularization terms (Eq. 3) seem ad-hoc—why penalize deviation from the radius in these specific ways? What happens with different regularizers?\n\n5. How are dimensions allocated across geometries? What is the computational overhead compared to baselines? How does performance vary with different dimension allocations? The paper mentions fixing dimensions \"as varying dimensionality often introduces extra degrees of freedom\" but provides no empirical support."}, "questions": {"value": "1. How do you determine the split of dimensions across Euclidean, hyperbolic, and spherical components? Is there a principled way to set dE, dH, dS, or is it purely empirical? An ablation study on different dimension configurations would strengthen the paper.\n\n2. What is the training time and memory overhead of GeoDM compared to NCFM or other baselines? Given the modest accuracy improvements, understanding the cost-benefit tradeoff is important for practical adoption. Can you provide wall-clock time comparisons and discuss whether the gains justify the added complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wjqHiqW0jx", "forum": "frqrywj6Tv", "replyto": "frqrywj6Tv", "signatures": ["ICLR.cc/2026/Conference/Submission9186/Reviewer_6TC6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9186/Reviewer_6TC6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762121588480, "cdate": 1762121588480, "tmdate": 1762920858074, "mdate": 1762920858074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}