{"id": "V0jhErYl9k", "number": 16777, "cdate": 1758268574734, "mdate": 1759897219963, "content": {"title": "Label-Free Privacy-Preserving Learning for Zero-Shot Action Recognition", "abstract": "Traditional action recognition relies on labeled data and closed-set assumptions, limiting adaptability to novel actions and environments. Vision-Language Models (VLMs) offer a more flexible alternative through text-image alignment, enabling zero-shot action recognition. However, using raw video data poses privacy risks due to sensitive visual content. Privacy-Preserving Action Recognition (PPAR) aims to anonymize videos while preserving action-relevant semantics. Existing learning-based PPAR approaches often require both action and privacy annotations and retraining of recognition models on anonymized data, limiting their flexibility and compatibility with powerful pretrained VLMs. We propose LaF-Privacy, a novel label-free privacy-preserving framework for zero-shot action recognition. Our method is trained without any manual annotations, using two complementary objectives: preserving high-level action-relevant features and suppressing low-level appearance cues between raw and anonymized videos. We adopt a video transformer encoder for spatio-temporal learning and introduce an Action-Aware Masking Module (AAMM) to discard irrelevant regions, further enhancing privacy. LaF-Privacy enables direct use of pretrained VLMs for zero-shot inference on anonymized videos. Experiments on VP-UCF101 and VP-HMDB51 demonstrate that our approach achieves state-of-the-art trade-offs between privacy protection and zero-shot recognition performance.", "tldr": "We introduce LaF-Privacy, a label-free privacy-preserving framework for training anonymizers that preserve action semantics and support zero-shot recognition with VLMs.", "keywords": ["privacy protection", "action recognition", "label-free", "unsupervised", "zero-shot"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3bdbae360e4aa1333e035f32973a4b96016a1778.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a label-free privacy preservation method for zero-shot action recognition using VLMs. Their proposed method minimizes visual similarity while maintaining embedding similarity, maintaining performance on action recognition tasks. Notably, their method does not require action or private attribute labels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Handling VLM anonymization is a solid motivation that is underexplored in the field.\n2. The idea is clean, results show only a small utility decrease with moderate privacy increase."}, "weaknesses": {"value": "1. Anonymization performance is weak. Even with the cMAP justification, there is room for better anonymization. In prior (trained) methods, it is near impossible to make out private attributes. Even in some qualitative examples (Figure 7), some attributes are visually identifiable. Could a tradeoff curve be analyzed by scaling the relative privacy-utility weights?\n2. It is difficult to tell if the anonymizer is specific for the zero-shot action recognition task. The results should explore other types of VLM tasks such as retrieval or even a captioning task to see if general performance of the VLM is retained.\n3. The results without the AAMM appear to demonstrate natural masking, but the reasoning is unclear. The patch-level representations are regularized to have similar representations to the original patches. It would be helpful to see more analysis on how it learns to mask specific tokens."}, "questions": {"value": "1. Could a privacy objective be up-weighted to result in more \"self-masking\" (see W3) and eliminate the need for the additional masking module? Also, would this result in lower cMAP/lower visibility with some (ideally slight) decrease to utility?\n2. Does the natural masking before AAMM imply that those representations are already close to noise?\n3. Can this anonymizer + VLM be applied to additional tasks beyond zero-shot action classification?\n4. In the ablation where just the CLS/video token is used in the utility loss, how come the private attribute prediction scores increase? This is counterintuitive, since the visual patches are responsible for the privacy-preservation.\n5. How do the privacy results look on the VISPR dataset shown in prior anonymization works?\n6. Is the anonymization method robust against attacks? Meaning, can a model learn to reconstruct/denoise back to the original input after anonymization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3GpPsl0oCs", "forum": "V0jhErYl9k", "replyto": "V0jhErYl9k", "signatures": ["ICLR.cc/2026/Conference/Submission16777/Reviewer_nM16"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16777/Reviewer_nM16"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746715997, "cdate": 1761746715997, "tmdate": 1762926820328, "mdate": 1762926820328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LaF-Privacy, a label-free and privacy-preserving framework for zero-shot action recognition. It aims to anonymize videos while retaining action-relevant semantics so that pretrained Vision-Language Models (VLMs) (e.g., CLIP, X-CLIP, ActionCLIP) can perform zero-shot action recognition without retraining. The approach is built on a video transformer encoder with an Action-Aware Masking Module (AAMM) that dynamically masks uninformative or privacy-sensitive patches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A. Clever supervision design – Uses pretrained CLIP-style embeddings to preserve semantics without labels. The overall model is a  general and plug-and-play – Works with different VLMs (X-CLIP, ActionCLIP) without retraining them.\n\nB. Experimental results achieve overall good balance between action and privacy, in a zero-shot setting for action and supervised testing for privacy. \n\nC. The overall approach is simple to understand, though the building blocks seem huge."}, "weaknesses": {"value": "A. There seems to be an error on the Table 1 - privacy F1 of ours (0.632) of VP-HMDB51 should be the bold best rather than the underline 2nd best for the zero-shot session. Please confirm the number or correct it if wrong. \n\nB. The proposed method requires quite amount of learning (training) and, from Table 1, it seems the performance does not achieve the best on both the action recogntion and privacy hiding with this training efforts. For example, 2x down-sampling is achieving the best action recogntion and the blackening is the best model for privacy across datasets. As the paper mentioned, the proposed model is playing as a trade-off maker here. This reviwer would like to put up some challenge here:\n-  Can the similar trade-off be obtained through a mix of 2x-down sampling and blackening (with a variety of blackening levels), which is not only unsupervised but also learning free? \n    - If so, then the complicated training schema from this paper would seems unnecessary or at last less efficient. For example, 2 × downsampling + mild blurring or masking could plausibly approximate the “moderate visual difference” LaF-Privacy achieves — at far lower training cost. After all, LaF-Privacy’s F1 and cMAP are just 0.03–0.05 lower than each single transformation baseline.\n\nC. The training of privacy model is using the distorted frames, rather than the raw frames. This reviewer would like to point out that the VLM embedding might also have a chance carrying privacy semantic, which is mostly preserved due to the L2 loss of the Action Module. Since there is no re-traiing of VLMs, it is possible that VLM embeddings will leak privacy, which is over-looked by this study. This reivewer is wondering any thoughts from authors on this matter."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O1pI6Tshnn", "forum": "V0jhErYl9k", "replyto": "V0jhErYl9k", "signatures": ["ICLR.cc/2026/Conference/Submission16777/Reviewer_gHsn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16777/Reviewer_gHsn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759564814, "cdate": 1761759564814, "tmdate": 1762926819972, "mdate": 1762926819972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework called LaF-Privacy, designed to achieve label-free privacy-preserving learning for zero-shot action recognition. The method anonymizes videos without requiring any action or privacy annotations, enabling direct compatibility with pretrained VLMs for zero-shot inference. The framework consists of a video transformer encoder, an AAMM, and a multi-objective loss function that integrates visual dissimilarity maximization, action feature preservation, and masking regularization. Experimental results are reported on the VP-UCF101 and VP-HMDB51 datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper meaningfully combines zero-shot recognition and privacy preservation, addressing the dual practical demands of data privacy and model generalization in real-world scenarios.\n\nThe experimental design is generally comprehensive, including main experiments, ablation studies, cross-VLM evaluations, and visualization analyses, which evaluate the proposed approach from multiple perspectives.\n\nThe method requires no costly privacy annotations, lowering the practical barrier for deployment and demonstrating potential applicability.\n\nThe writing is clear, and the figures and tables effectively convey the core ideas and experimental outcomes."}, "weaknesses": {"value": "The paper does not provide a fair comparison with a broader range of unsupervised or self-supervised learning approaches, such as reconstruction-based or contrastive learning methods. This makes it difficult to discern whether the reported improvements are primarily due to the proposed architectural design or simply the strength of VLM-based representations.\n\nThe authors mention in the appendix that cross-VLM generalization performance degrades, but they do not provide an analysis of the underlying causes. While they claim the performance drop is acceptable, the magnitude of this decrease is non-negligible when compared to the reported performance gains. Furthermore, the evaluation lacks validation across a broader range of VLMs to assess generalization. If the framework is heavily dependent on a specific VLM and cannot transfer effectively to others, its practical applicability in diverse unsupervised scenarios would be severely limited—contradicting the paper's claimed contribution of being a flexible, label-free solution compatible with pre-trained models.\n\nAlthough the paper emphasizes the notion of an “SOTA trade-off,” the absolute reductions in privacy metrics (F1, cMAP) are relatively limited, particularly for attributes like “relationships”, where the improvement is marginal. This raises concerns about the robustness of the framework against stronger inference attacks, which the paper does not discuss in depth.\n\nAlthough the combination of \"label-free\" and \"zero-shot\" learning has practical value, the proposed method primarily integrates existing VLM and Transformer frameworks without introducing novel theoretical mechanisms or significant architectural innovations. Moreover, the training process heavily relies on pre-trained VLMs, making it susceptible to domain biases inherent in these models and thus limiting its applicability.\n\nThe design of the AAMM closely resembles the Learned Token Pruning (LTP) mechanism. While the application domain is different, the paper lacks sufficient novel theoretical justification or architectural innovation to substantiate its unique contribution in this context.\n\nThe privacy protection evaluation relies solely on F1 and cMAP metrics, lacking analysis against more targeted attacks."}, "questions": {"value": "Ablation results show that the model without AAMM (“Unmasked”) achieves similar performance. Could the authors further analyze specific scenarios where AAMM is crucial? Are there explicit cases demonstrating that dynamic masking offers significant advantages over a fixed masking ratio?\n\nThe current privacy evaluation relies on a ViT-based classifier trained on anonymized videos. Have the authors considered stronger reconstruction-based privacy attacks to assess the robustness of the proposed framework? For attributes such as “relationships”, which are difficult to protect, what are the authors’ potential improvement directions or future plans?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H6E1NFu1kh", "forum": "V0jhErYl9k", "replyto": "V0jhErYl9k", "signatures": ["ICLR.cc/2026/Conference/Submission16777/Reviewer_ix2d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16777/Reviewer_ix2d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889617577, "cdate": 1761889617577, "tmdate": 1762926819505, "mdate": 1762926819505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of preserving privacy in action recognition without access to labels. The paper uses an Action-Aware Masking Module (AAMM) to mask out irrelevant information from the decoded input video and uses a loss to push the masked output away from the original input. To ensure the action recognition performance is retained, a pre-trained VLM is used to align the representations before and after augmentation. A mask loss is used to maximize the amount of tokens that are masked in the process. Overall the paper achieves some improvement over generic baselines in the zero-shot setting such as downsampling, blackening, and blur."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The ability to perform privacy preservation on action recognition without requiring privacy labels is novel and interesting.\n\n- The modules used in the pipelined are thoroughly detailed for ease of replication. \n\n- The proposed method achieves an average improvement compared to the baselines over both classification accuracy and privacy preservation"}, "weaknesses": {"value": "- Ablation results reveal that the visual loss and applying the mask does not have a significant impact on the privacy results despite being the portion of the method designed to focus on privacy.\n\n- The cross-VLM results are not very convincing, with a significant drop suggesting that this method is not generalizable. In principle, masking the input should be a model-agnostic privacy technique.\n\n- In Figure 2, it is not apparent which modules are being trained and which are frozen.\n\n- There is a lack of clarity in figure captions. In figures 3 and 4 it is unclear what the takeaway is from the shown masks, as many of them look very similar.\n\n- There is no ablation on the scaling weights for the action and vision losses. It would be helpful to see the curve as these are varied to better understand the tradeoff between classification accuracy and privacy score.\n\n- Minor note: the filesize for this paper is very large, likely due to high quality images in the ablations, please reduce in future revisions."}, "questions": {"value": "- How  is the MLP in the AAMM initialized and how does it estimate token importance?\n\n- The baselines adopted in this paper do not show much of a drop in privacy score compared to raw data, while in the cited work [1] there is more of a significant change. What is the reason for this change in behavior?\n\n- How does the method perform on the VISPR1[2] dataset?\n\n[1] Ishan Rajendrakumar Dave, Chen Chen, and Mubarak Shah. Spact: Self-supervised privacy preserva tion for action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 20164–20173, 2022\n\n[2] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Towards a visual privacy advisor: Understanding and predicting privacy risks in images. In IEEE International Conference on Computer Vision (ICCV), 2017."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7gecRyR3NJ", "forum": "V0jhErYl9k", "replyto": "V0jhErYl9k", "signatures": ["ICLR.cc/2026/Conference/Submission16777/Reviewer_6Ub4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16777/Reviewer_6Ub4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995786977, "cdate": 1761995786977, "tmdate": 1762926818839, "mdate": 1762926818839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}