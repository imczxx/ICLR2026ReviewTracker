{"id": "aTyV6Omm6m", "number": 14470, "cdate": 1758236628472, "mdate": 1759897368276, "content": {"title": "AR Models can be Faster and More Accurate Parallel Decoders than Diffusion LLMs", "abstract": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts have primarily explored diffusion-based LLMs (dLLM) for parallel decoding to reduce latency while preserving model generation quality. However, non-diffusion approaches remain largely underexplored and it's unanswered whether AR models can be adapted as faster parallel decoders than dLLMs while maintaining generation quality. We present pcLLM, a progressive consistency distillation paradigm that transforms autoregressive (AR) models into efficient parallel decoders while preserving the causal inference property. pcLLM achieves $3.6\\times$ wall-clock speedup on coding benchmarks with minimal loss in performance. Based on pcLLM's trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to $4.2\\times$ higher token acceptance count per iteration and nearly $4\\times$ speedup, effectively trading additional compute for lower inference latency.", "tldr": "", "keywords": ["LLMs", "LLM inference", "parallel decoding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/53ffe53d4a3ab07b297ff3fb910013af59800f45.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces pcLLM, a novel training method called progressive consistency distillation that trains autoregressive (AR) models to function as efficient parallel decoders. The key contribution lies in three components designed to improve on prior work like CLLM: a progressive noise schedule, a new sequence packing strategy, and a noise-aware causal attention mask. This progressive approach aims to solve a key challenge in consistency-based training: the difficulty of accurately predicting an entire block of tokens when conditioned on long, unconverged (noisy) preceding text. The authors report significant speedups on coding benchmarks (up to 3.8x vs. standard AR with 1-3% accuracy difference). Compared to the strong CLLM baseline, pcLLM is up to 44% faster, but with a mixed impact on quality: it shows a 3.2% absolute accuracy drop on HumanEval while simultaneously achieving a 2.0% absolute gain on MBPP."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-structured and clearly motivated. The authors do a commendable job of positioning their work against prior methods (like CLLM) and explaining the technical rationale for the new components.\n- The progressive noise schedule and noise-aware causal masking are an interesting and notable evolution of the original CLLM approach, directly tackling the known challenge of training with large, noisy context blocks.\n- From a systems perspective, the addition of sequence packing is a clever and practical optimization that contributes to the method's efficiency.\n- The evaluation provides a thorough investigation of inference speedups relative to block size (as seen in Figure 4b). The authors demonstrate a clear, positive impact on speed (up to 3.8x vs. AR) and show their method is competitive against a strong baseline (CLLM). Notably, on the MBPP benchmark, the proposed pcLLM (MR) method is both faster (+27%) and more accurate (+2.0%) than the CLLM baseline.\n- The authors include a clear and effective ablation study (Table 4a) that successfully isolates the benefit of their proposed noise-conditioned mask, validating its impact on performance over other masking strategies.\n- The techniques presented, particularly the progressive distillation schedule, are promising and could be built upon by future work in Jacobian-based parallel decoding."}, "weaknesses": {"value": "- W1. A major weakness is the failure to compare against speculative decoding (SD). The authors' justification for omitting this key baseline is unconvincing. This comparison is critical because SD is a lossless method, guaranteeing output identical to the original AR model. In contrast, pcLLM is lossy, as its re-training process incurs a 3.2% absolute accuracy drop on HumanEval relative to that same original AR model. Without this baseline, the practical benefit of the paper's complex, lossy method over a simpler, lossless alternative is unknown.\n- W2. The main results in Table 1 are difficult to interpret, as the block size k used for the pcLLM and CLLM methods is not specified. The paper's own ablation studies show that k is a highly sensitive parameter that significantly impacts both speedup and accuracy. This omission is problematic because it is unclear if the 3.62x speedup on HumanEval and the 84.8 accuracy score were achieved at the same k. This makes it difficult for the reader to assess the method's true speed-versus-accuracy trade-off from the main results table.\n- W3. The claim that the method \"largely\" preserves performance is an overstatement. A 3.2% absolute drop on HumanEval (88.0% → 84.8%) is a very significant degradation in quality. Furthermore, the results are mixed, not a clear win (it wins on MBPP but loses badly on HumanEval), and the evaluation is limited to a single domain (coding), making it hard to generalize the findings to other tasks.\n- W4. The evaluation is missing several key analyses. First, there is no study on the effect of model size. It is unclear if the reported speedups will hold for larger, more practical models (e.g., 70B, 175B), which is a key consideration for the method's impact. Second, the paper fails to provide Pareto curves (speed vs. accuracy) for both pcLLM and CLLM. A full curve, generated by sweeping the block size k, is necessary for a fair, holistic comparison. The current tables only present a single operating point.\n- W5. The method's complexity, arising from both its multi-objective training and iterative inference engine, introduces overhead. This is confirmed by the ablation study (Fig 4b), which shows the method is slower than standard AR at small block sizes and hits a hard speedup ceiling. This high overhead and limited scalability call the method's practical utility into question."}, "questions": {"value": "- Could the authors provide a direct comparison (wall-clock speedup vs. accuracy) of pcLLM against a strong, lossless SD baseline to properly situate the paper's contribution?\n- What block size $k$ was used to generate the main results in Table 1? To better understand the trade-offs, could you show both speedup and HumanEval accuracy for pcLLM at various block sizes (e.g., k=8, 16, and 50)?\n- How do the reported speedups and quality trade-offs for pcLLM scale to larger, more practical models (e.g., 13B or 70B)?\n- To provide a fairer, more holistic comparison, would it be possible for the authors to provide speed-vs-accuracy Pareto curves for both pcLLM and CLLM, generated by sweeping the block size $k$? This would be more informative than the single data point in Table 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xkbME3wzjx", "forum": "aTyV6Omm6m", "replyto": "aTyV6Omm6m", "signatures": ["ICLR.cc/2026/Conference/Submission14470/Reviewer_hCKy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14470/Reviewer_hCKy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537955371, "cdate": 1761537955371, "tmdate": 1762924872111, "mdate": 1762924872111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces pcLLM, a training and inference framework that aims to turn autoregressive (AR) LLMs into parallel decoders via \"progressive consistency distillation\", coupled with Jacobi-style iterative decoding and inference-time optimizations (rejection recycling). Empirically, the paper reports 3.6-4.0x wall-clock speedups on coding benchmarks with modest accuracy drops and argues advantages over diffusion-based LLMs, and argues that the training curriculum is more suited than the original CLLMs training procedure, as learning to predict shorter blocks first is easier and more stable. No comparisons with speculative decoding are included."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**Significance**: the pcLLMs are faster than vanilla CLLMs, and current diffusion LLMs."}, "weaknesses": {"value": "## Summary of the weaknesses\n- The submission is difficult to read. I would not be able to implement the method, as multiple details are unclear or not discussed. There is no appendix in the submission, which would have been a good place to clarify the method. \n- Comparing pcLLMs to diffusion LLMs only, and not speculative decoding, a conscious choice of the authors (line 351), seems unjustified.\n\n\n## Detailed weaknesses\n1. Missing citations in the introduction for important milestones in discrete diffusion models (D3PM, SEDD, MDLM, MD4, RADD)\n2. It is unclear how the noise ratios $t_i$, introduced on line 184, are used. Given trajectories generated from the original cLLM, it seems the authors want to pick points along the consistency trajectory, such that each block has a certain noise level, close to the ratios $t_i$s. However, there is no guarantee that trajectories with the desired ratios exist. An alternative explanation, is that the authors *combine* different trajectories, by extracting blocks from different trajectories, and combining them into a training example. Can you clarify how the training examples are selected, in particular, such that each block has a desired ratio of noisy tokens?\n3. The sentence packing strategy is unclear. There are no explanations on the mechanism, simply a reference to the figure (lines 229, 230), which does not suffice to explain it.\n4. Experiments (line 233): \"*Collecting additional rounds of Jacobi trajectories from intermediate checkpoints help*\". This is not an adequate amount of details. What fraction of trajectories are sampled from the intermediate checkpoints? How frequently? Do you replace examples from the original cLLM when introducing the trajectories from the intermediate checkpoints? These are important questions that are not discussed at all.\n5. Experiments (line 235): You argue that the block sizes are increased progressively during training. How progressively? From line 316, it seems you double the block size from 16 to 32, however it is not clear at which point of training. Are you doubling the block size after half the total number of steps?\n6. **Missing important baselines** (Line 351): The decision to exclude speculative decoding from the comparison appears unjustified. Including it would have been more relevant than comparing against dLLMs, which are currently known to underperform autoregressive models in both performance and speed, particularly when KV-caching optimizations are not applied. Moreover, omitting comparisons with multi-token prediction methods such as Medusa also seems questionable. Your approach requires fine-tuning the model, and although Medusa introduces additional decoding heads, both methods aim to accelerate sampling from autoregressive models. Therefore, including at least one speculative decoding baseline, as well as Medusa, would make the evaluation more complete and fair.\n7. It is not clear what the trade-off is between speed-up and performance on coding benchmark is, when using different block sizes during sampling."}, "questions": {"value": "1. You introduce a distillation method for consistency LLMs, yet compare pcLLMs with *un-distilled* diffusion models. A fairer comparison would be between pcLLMs, and distilled diffusion LLMs, e.g. with SDTT [1], or Di4C [2]. Is there a reason why you did not compare with distilled dLLMs?\n2. Can you clarify how the training examples are selected, in particular, such that each block has a desired ratio of noisy tokens (cf weakness 2)\n3. What is the window size $w$? How does it compare to $n$? From line 183, it seems that $n$ already represented the length of the smaller blocks, does that mean that $n=w$ ? \n4. What is $t$, on line 192? Is the the value of $t_i$ in an arbitrary block (so that the argument holds for any block)?\n5. *Progressive distillation loss* (line 194): In your work, it seems you are using noisy blocks in the prefix. This is surprising, since cLLMs sample new blocks based on fully denoised blocks. In your case, you start generating future blocks before the previous ones are fully denoised, is that correct? It is surprising that such approach would be faster than generating blocks one-by-one, as once a block has converged, one can use KV-caching, hence lookup will be faster\n6. You mention CoT generation for reasoning model (last line of page 4). In Figure 4a (caption), you argue that the timings are performed with a prompt of length 128, and generating 256 tokens. This is rather short. Did you measure the efficiency of your approach for longer generation, e.g. generating 2048, 4096 tokens?\n\n\n\n[1]: Beyond Autoregression: Fast LLMs via Self-Distillation Through Time, Deschenaux and Gulcehre\n\n[2]: Distillation of Discrete Diffusion through Dimensional Correlations, Hayakawa et al."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GdML5jQLVi", "forum": "aTyV6Omm6m", "replyto": "aTyV6Omm6m", "signatures": ["ICLR.cc/2026/Conference/Submission14470/Reviewer_TYFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14470/Reviewer_TYFF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761689977901, "cdate": 1761689977901, "tmdate": 1762924871321, "mdate": 1762924871321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes pcLLM, a progressive consistency distillation approach that converts autoregressive models into more efficient parallel decoders built upon Jacobi decoding. The method introduces a progressive noise schedule during consistency distillation. At inference time, the paper proposes rejection recycling and multi-block decoding to reuse stable n-gram segments and refine multiple blocks in parallel. The approach achieves up to ~4× speedup while aiming to preserve generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The progression from preliminaries to the progressive consistency distillation method is smooth, and the intuition behind the progressive noise schedule is clearly explained.\n\nFigures and qualitative examples are effective, and several ablations (e.g., block size) help justify design decisions."}, "weaknesses": {"value": "The explanation of multi-block decoding and rejection recycling is hard to follow, especially in Algorithm 1. While Figure 3 illustrate the behavior, a clearer algorithm description would improve clarity.\n\nEvaluation is limited to HumanEval and MBPP, restricting generalization to broader reasoning or open-ended text generation tasks.\n\nSection 4.2 claims that pcLLM “consistently achieves better accuracy,” but on HumanEval pcLLM (84.8) is lower than AR and CLLM baselines (~87–88).\n\nLimited ablation on inference hyperparameters (e.g., spawn ratio r and number of active blocks K) makes the sensitivity of the decoding strategy unclear.\n\nNoise schedule ablations are minimal. The reverse progressive schedule mainly demonstrates failure; additional variations (e.g., different window sizes or non-linear schedules) could better support the choice of the linear progressive cyclic schedule.\n\nThe paper does not explicitly discuss limitations or potential failure cases."}, "questions": {"value": "Could the authors provide deeper intuition for why the linear progressive cyclic noise schedule is particularly effective?  Does such noise scheduling methodology possess any conceptual relations to diffusion-style denoising?\n\nBeyond the example in Figure 2, can you provide more direct quantitative evidence isolating the contributions of multi-block decoding and rejection recycling to the speedup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bKcThlvz1h", "forum": "aTyV6Omm6m", "replyto": "aTyV6Omm6m", "signatures": ["ICLR.cc/2026/Conference/Submission14470/Reviewer_XcmS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14470/Reviewer_XcmS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975813249, "cdate": 1761975813249, "tmdate": 1762924870859, "mdate": 1762924870859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes pcLLM, a training recipe that turns an autoregressive (AR) LLM into a faster parallel decoder without abandoning causal generation. The key idea is progressive consistency distillation: instead of learning to predict a large block of future tokens at once, the model is trained with a progressive noise schedule and a noise-aware causal mask so that it can reliably predict more correct tokens under noisy unconverged contexts. Building on the resulting trajectory properties, the authors introduce two inference schemes—rejection-recycling of verified n-grams and multi-block decoding—to further increase tokens accepted per iteration. On coding benchmarks (HumanEval, MBPP) with Qwen2.5-Coder-7B-Instruct, pcLLM reports ~3.6–3.8× speedups vs AR with small accuracy deltas, and ~4× with multi-block + rejection-recycling on H200, while outperforming several diffusion-LLM decoders in both speed and pass@1 at the same scale."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Decouple training a parallel-friendly AR model from inference tricks; relate to Jacobi decoding theory and consistency training. \n\n2. Progressive noise schedule reduces hardest long-noisy contexts; noise-aware causal masks + sequence packing cut passes from O(N) to O(1) for the loss, a practical speedup for training. \n\n3. Lossless inference schemes. Rejection-recycling and multi-block decoding match/verify with greedy acceptance while mining high-quality n-grams that emerge in pcLLM trajectories. \n\n4. Empirical gains (coding). Consistent 3.5–3.8× speedups at similar pass@1; ~4× with multi-block on H200; ablations show block-size effects and noise-schedule importance."}, "weaknesses": {"value": "1. Narrow evaluation. Claims are benchmarked only on HumanEval/MBPP. No natural-language generation, instruction-following, or math/CoT tasks where noisy context tolerance may differ. The introduction strongly motivates broad latency issues (e.g., CoT), but experiments don’t validate there. \n\n2. Baselines could be stronger. While Jacobi and CLLM are included, the paper downplays speculative decoding families (EAGLE/EAGLE-2/3, Medusa, OSD) even though these are the most widely deployed AR accelerators; some are cited but not comprehensively compared in equal settings (same model, context lengths, caching, batch). \n\n3. Systems reporting. Results emphasize TPS and speedup ratios; please detail absolute end-to-end latency (including KV-cache trim/clone, verification kernels, and multi-block overhead) and the FLOPs trade-off vs AR and dLLM baselines. Clarify whether speedups persist under long contexts and with prefix-cache enabled. (Parts are implied, but the precise accounting is unclear.) \n\n14470_AR_Models_can_be_Faster_\n\nReproducibility gaps: The text claims reproducibility, but training requires Jacobi-trajectory mining, progressive schedules, and custom masking. Without code/configs, there’s residual ambiguity (e.g., window size choices across rounds, exact LR/steps for second pass, acceptance thresholds in Algorithm 1). \n\n14470_AR_Models_can_be_Faster_\n\nGenerality claims: The title and framing (“faster and more accurate than diffusion LLMs”) are strong; the dLLM baselines are a subset and mostly at 7B. A balanced claim would qualify the result as “at comparable size on coding benchmarks.”"}, "questions": {"value": "1.How is wall-clock latency measured? Please report per-prompt P50/P90 latency including: forward pass, verification, n-gram matching, KV-cache trim/clone, and multi-block spawning/promotion. Also report batch-size sensitivity. \n\n2. Generalization beyond coding. Can you include results on at least one NL generation (e.g., summarization) and one reasoning/CoT benchmark to substantiate broader claims? Your intro motivates CoT latency prominently. \n\n3. Spec-dec baselines. Please add a careful comparison to strong speculative decoding variants (e.g., EAGLE-2/3, Medusa) on the same backbone and hardware, with clear acceptance rates and end-to-end latency. What is pcLLM’s relative FLOPs vs those methods? \n\n4. Ablations on masks/schedules. The linear progressive schedule wins in Table 4 for small-scale ablations; could you show the same with full-data training and larger blocks (e.g., 128/256) to confirm the trend holds? \n\n5. Accuracy trade-offs. On HumanEval, pcLLM (MR) keeps the same pass@1 as pcLLM but both are a bit below AR. Can you discuss when accuracy dips occur (e.g., long dependencies) and whether a small AR fallback on hard blocks recovers them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MeaASxPtLJ", "forum": "aTyV6Omm6m", "replyto": "aTyV6Omm6m", "signatures": ["ICLR.cc/2026/Conference/Submission14470/Reviewer_A7a7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14470/Reviewer_A7a7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094584553, "cdate": 1762094584553, "tmdate": 1762924870364, "mdate": 1762924870364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}