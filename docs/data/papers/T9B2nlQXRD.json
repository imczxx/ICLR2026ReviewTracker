{"id": "T9B2nlQXRD", "number": 11774, "cdate": 1758203719242, "mdate": 1759897555775, "content": {"title": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence‑Level RL", "abstract": "We propose FSPO (Fair Sequence Policy Optimization), a sequence-level reinforcement learning method for LLMs that enforces length-fair clipping on the importance-sampling (IS) weight. We study RL methods with sequence-level IS and identify a mismatch when PPO/GRPO-style clipping is transplanted to sequences: a fixed clip range systematically reweights short vs. long responses, distorting the optimization direction. FSPO introduces a simple remedy: we clip the sequence log-IS ratio with a band that scales as $\\sqrt{L}$. Theoretically, we formalize length fairness via a Length Reweighting Error (LRE) and prove that small LRE yields a cosine directional guarantee between the clipped and true updates. Empirically, FSPO flattens clip rates across length bins, stabilizes training, and outperforms baselines across model sizes and evaluation datasets, with the largest gains on the Qwen3‑8B‑Base model.", "tldr": "", "keywords": ["Reinforcement Learning", "LLMs", "Policy Gradient", "RLVR"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a84fc413e391f427dd5df92a947b4e07e86f59e2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a sequence-level reinforcement RL method designed for Large Language Models. The authors identify a mismatch in current methods (like GRPO and RLOO) that apply PPO-style fixed-range clipping to sequence-level importance-sampling rations. They argue that longer responses are clipped more frequently distorting the optimization direction. FSPO addresses this by clipping the sequence log-IS ratio using a dynamic band that scales with length.  FSPO is evaluated on math reasoning tasks using Qwen3 base models (1.7B and 8B), showing improved stability, flatter acceptance rates across length bins, and increased performance compared to the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "FSPO preserves IS semantics while restoring fairness via the length scaled band. Another strenght is the theoretical contribution - the paper formalizes the problem via lenght reweighting error and provides a theorem linking this to update direction fidelity. It grounds its solution in the asymptotic Gaussian law of sequence log-IS ratios. The method shows improvement over strong baselines across multiple benchmarks and model scales, with the most significant improvements on harder tasks (AIME24/25) and larger models (8B)"}, "weaknesses": {"value": "The evaluations domain is a bit limited as it only considers math reasoning tasks. And while there are some gains in performance they are not huge. However the principled approach is nice.  Another limitation might be the drift assumption as this could change in different experimental settings."}, "questions": {"value": "how do you expect the drift assumption to hold in significantly different experiment settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4EX0VpKC96", "forum": "T9B2nlQXRD", "replyto": "T9B2nlQXRD", "signatures": ["ICLR.cc/2026/Conference/Submission11774/Reviewer_rLLd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11774/Reviewer_rLLd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669085707, "cdate": 1761669085707, "tmdate": 1762922798438, "mdate": 1762922798438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FSPO which  propose a length-scaled clipping band in log-IS space to equalize acceptance rates across sequence lengths. Empirical results on math benchmarks (MATH500, AIME24/25) show FSPO outperforms existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. this paper address a critical issue in RLVR.\n2. method is intuitive and simple to implement."}, "weaknesses": {"value": "1. Author did not thoroughly study the clipping hyperparameters and their effect. This is crucial because the choice of clipping band can significantly influence variance reduction and fairness.\n\n2. Some assumptions may be too strict. For assumption 3.1, clipping may affect sequences with high variance and especially when the data is limited.  In such cases, clipping might distort the distribution and negatively impact learning."}, "questions": {"value": "1. As Figure 2 \"The scale gap between the theoretical and empirical curves is expected due to the asymmetry between the upper and lower clip ranges in implementation.\" Can adaptively adjust the clip band further improve performance e.g. reduce the gap? An adaptive strategy might better handle varying sequence lengths and variance. \n\n2. Figure 4 has very large variance which makes hard to make reasonable conclusion, can author consider plot it with confidence interval or std? This would help clarify the statistical significance of the observed patterns and better support the claims regarding length fairness.\n\n3. In Table 3, author list upper clip, lower clip and dual clip. Can author give more explanation on how this dual clip affect the fairness or stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XBmrA1soxw", "forum": "T9B2nlQXRD", "replyto": "T9B2nlQXRD", "signatures": ["ICLR.cc/2026/Conference/Submission11774/Reviewer_wUha"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11774/Reviewer_wUha"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860233363, "cdate": 1761860233363, "tmdate": 1762922797909, "mdate": 1762922797909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FSPO (Fair Sequence Policy Optimization), a novel sequence-level reinforcement learning method for large language models (LLMs) that addresses length bias in importance sampling (IS) weight clipping. The authors identify that fixed clipping ranges in existing methods like PPO/GRPO lead to systematic reweighting of short versus long responses, distorting optimization. FSPO introduces a √L-scaled clipping band on the sequence log-IS ratio to enforce length fairness, formalized through a Length Reweighting Error (LRE) metric. Theoretically, small LRE ensures a cosine directional guarantee between clipped and true updates. Empirically, FSPO is evaluated on mathematical reasoning tasks using Qwen3 models, showing improved performance over baselines on benchmarks like MATH500 and AIME24/25, with flattened clip rates across length bins and stabilized training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novelty: FSPO is the first method to explicitly address length bias in sequence-level IS clipping, filling a critical gap in RLVR literature.\n- Theoretical Foundation: The LRE metric and cosine guarantee provide a rigorous basis for length fairness, supported by Markov chain CLT.\n- Empirical Validation: Comprehensive experiments on math benchmarks show consistent improvements, with strong ablation studies and diagnostics.\n- Practicality: FSPO is easy to implement and compatible with existing RL components, making it accessible for real-world applications.\n- Reproducibility: Detailed configurations and open-source references facilitate replication."}, "weaknesses": {"value": "- Limited Scope: Experiments focus solely on mathematical reasoning tasks; generalization to other domains (e.g., code generation or tool-use) is not verified.\n- Compute Constraints: Hyperparameter tuning is limited due to resource costs, potentially affecting optimality across diverse settings.\n- Assumption Dependency: Theoretical guarantees rely on assumptions like bounded stratification, which may not hold in all scenarios.\n- Empirical Drift Simplification: Setting μ≈0 for drift terms is justified empirically but might not generalize to policies with large KL divergence."}, "questions": {"value": "- How does FSPO perform on non-mathematical tasks, such as code generation or dialogue, where length distributions may differ?\n- Could the √L scaling be adapted dynamically based on task-specific length variance, rather than using a fixed σ estimate?\n- What are the implications of the bounded correlation assumption (Assumption 3.2) in practice? Are there cases where it might fail?\n- How sensitive is FSPO to the choice of c (clip scale) across different model architectures or reward functions?\n- Have the authors considered combining FSPO with advanced advantage estimators for further gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3IN3J52GD2", "forum": "T9B2nlQXRD", "replyto": "T9B2nlQXRD", "signatures": ["ICLR.cc/2026/Conference/Submission11774/Reviewer_FCnS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11774/Reviewer_FCnS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898280134, "cdate": 1761898280134, "tmdate": 1762922797452, "mdate": 1762922797452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FSPO (Fair Sequence Policy Optimization), a sequence-level reinforcement learning method for large language models (LLMs) that addresses the issue of length bias in importance sampling (IS) weight clipping. The authors identify that fixed clipping ranges in existing sequence-level RL methods (e.g., RLOO, GSPO) disproportionately affect sequences of different lengths, leading to unstable training and suboptimal performance. FSPO introduces a length-scaled clipping mechanism. Theoretical analysis formalizes this via Length Reweighting Error (LRE), linking small LRE to directional fidelity in policy updates. Empirical results on mathematical reasoning tasks (MATH500, AIME24/25) demonstrate that FSPO stabilizes training, flattens clip rates across lengths, and outperforms baselines, especially on larger models (e.g., Qwen3-8B)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly articulates a critical underexplored issue—length-dependent bias in sequence-level RL clipping—and formalizes it through LRE, providing theoretical grounding.\n\n2. FSPO is a simple yet effective modification to existing methods, requiring minimal changes (e.g., plug-in log-space clipping) while maintaining compatibility with RL frameworks like GRPO.\n\n3. Experiments are comprehensive, covering multiple model sizes (1.7B/8B), benchmarks, and baselines. Diagnostic plots (e.g., clip fraction vs. length) convincingly validate the method’s fairness claims.\n\n4. FSPO achieves consistent gains, with notable improvements on harder tasks (AIME) and larger models, suggesting scalability and practical utility."}, "weaknesses": {"value": "1. The method relies on a tuned scaling factor, which may require pilot runs for new settings. The paper notes compute constraints limited hyperparameter search, raising questions about robustness.\n\n2. Baseline Comparisons: While FSPO outperforms RLOO/GSPO, ablations show that simply widening the clip range fails, but more analysis on why FSPO's scaling is optimal is needed."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z06uwsZZ5K", "forum": "T9B2nlQXRD", "replyto": "T9B2nlQXRD", "signatures": ["ICLR.cc/2026/Conference/Submission11774/Reviewer_UsBx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11774/Reviewer_UsBx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920640200, "cdate": 1761920640200, "tmdate": 1762922797118, "mdate": 1762922797118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}