{"id": "02cEkpURXH", "number": 20949, "cdate": 1758311939461, "mdate": 1759896950334, "content": {"title": "Early Layer Readouts for Robust Knowledge Distillation", "abstract": "Domain generalization (DG) aims to learn a model that can generalize to unseen i.e. out-of-distribution (OOD) test domain. While large-capacity networks trained with sophisticated DG algorithms tend to achieve high robustness, they tend to be impractical in deployment. Typically, Knowledge distillation (KD) can alleviate this via an efficient transfer of knowledge from a robust teacher to a smaller student network. Throughout our experiments, we find that vanilla KD already provides strong OOD performance, often outperforming standalone DG algorithms. Motivated by this observation, we propose an adaptive distillation strategy that utilizes early layer predictions and uncertainty measures to learn a meta network that effectively rebalances supervised and distillation losses as per sample difficulty. Our method adds no inference overhead and consistently outperforms canonical ERM, vanilla KD, and competing DG algorithms across OOD generalization benchmarks.", "tldr": "", "keywords": ["domain generalization", "knowledge distillation", "early layer readouts"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2bb11bab4ab35adbf1f2a9ad3d46d601f3b0111c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a KD–based training strategy for OOD generalization. The authors first argue that training compact student models via simple KD from a teacher with strong OOD performance can often surpass standalone algorithmic DG methods. They further note that prior OOD-oriented KD approaches predominantly focus on the teacher’s design or the teacher–student relationship, leaving the design of the student model underexplored. To address this, the authors introduce a forecaster that quantifies per-sample difficulty using auxiliary models built on the student’s internal representations together with uncertainty measures. The KD loss is then reweighted on a per-sample basis according to the predicted difficulty. Experiments on four DomainBed datasets with ResNet-18 demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper highlights a student-model training framework for KD in the context of OOD generalization, a direction that has been relatively underexplored.\n- It proposes per-sample weighting via a meta-model that leverages the student’s internal representations and uncertainty measures."}, "weaknesses": {"value": "- Unclear linkage between motivation and design: The paper starts from the observation that KD is effective for OOD but uniform weighting is brittle, yet the causal chain that justifies why early-layer readouts plus uncertainty lead to instance weighting as the essential solution remains vague. For each design choice (using early student layers; adopting entropy and margin; alternating training; applying post-hoc adjustment), the paper should clarify why it is fundamentally or theoretically effective under OOD conditions.\n- Instability of alternating training: The student, auxiliary models, and the forecaster are all trained progressively from the beginning. Naturally, the auxiliary model’s correctness on the student and the forecaster’s weight predictions start near random in the early phase. The paper does not deeply examine how this phase affects student learning, nor when the auxiliary/forecaster models begin to exhibit reliable behavior in terms of correctness and weight prediction.\n- Limited experimental scope: Validation is restricted to a small set of datasets within the DomainBed framework. The paper does not evaluate on more challenging datasets such as DomainNet, nor on correlation-shift settings (e.g., Colored MNIST) beyond diversity shift, and it does not report results across multiple seeds.\n- Limited baselines: The authors adopt only training-algorithm baselines for KD, but many OOD-focused methods that leverage weight averaging also exist such as DiWA[1], and these are known to be more effective than designing new training algorithms.\n- Missing sensitivity analysis for newly introduced hyperparameters: The two hyperparameters introduced to adjust the forecaster’s predictions, $\\varsigma^\\star$ and $\\mu^\\star$, appear to be fixed to 0.1 and 0.5, respectively, without justification. The paper should explain how they were chosen and provide a sensitivity analysis.\n- Typos and minor presentation issues: line 037 “treates,” line 038 “exisiting,” line 059 “consitently,” and line 260 likely should use “.” rather than “:”.\n\n[1] Rame, Alexandre, et al. \"Diverse weight averaging for out-of-distribution generalization.\" Advances in Neural Information Processing Systems 35 (2022): 10821-10836."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LrWms20vTu", "forum": "02cEkpURXH", "replyto": "02cEkpURXH", "signatures": ["ICLR.cc/2026/Conference/Submission20949/Reviewer_XXf7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20949/Reviewer_XXf7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760501931345, "cdate": 1760501931345, "tmdate": 1762939045087, "mdate": 1762939045087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an adaptive KD framework for domain generalization where a lightweight forecaster uses early-layer readouts (auxiliary heads) and uncertainty features (entropy, confidence margin) to reweight per-instance contributions of supervised loss vs. teacher KL during student training. The forecaster is trained interleaved with the student and discarded at inference, so deployment cost matches vanilla KD."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation & pragmatic aim. KD already provides strong OOD gains; adaptivity addresses per-sample variability and teacher bias without incurring inference overhead.\n2. Student-centric design. Focuses on when and how the student should trust the teacher, rather than only on teacher improvements; leverages early readouts as signals."}, "weaknesses": {"value": "1. The method feels incremental relative to prior KDDG work [1], offering an early-readout reweighting tweak rather than a clearly novel principle.\n2. Magnitude of gains is modest. Improvements over a strong KD baseline average about +1.0–1.2% and vary by dataset; some cells are unchanged (TEACHER NETWORK: ViT-L/16). It’s not always clear whether the effect size justifies a new module vs. careful KD tuning.\n3. The paper lacks a systematic hyperparameter sensitivity analysis (e.g., τ, α/β, μ*/σ*, interleaving schedule), leaving the robustness of the reported gains unclear.\n4. The evaluation uses few and relatively small benchmarks and omits larger DomainBed suites like DomainNet, limiting evidence for scalability.\n5. The comparison set is outdated, lacking strong recent KD baselines (e.g., RISE [2], BOLD [3]), which weakens the competitiveness claim.\n6. The paper offers only empirical validation and lacks theoretical grounding to explain why early-layer readout–driven reweighting should improve OOD generalization.\n7. Experiments use only a small student (ResNet-18) distilled from a very strong teacher (ViT-L/16); the authors should evaluate larger students (e.g., ResNet-50, ViT-B) to test whether gains persist—especially since improvements with ResNet-18 are already modest.\n\n[1] 2021 - ACM MM - Embracing the Dark Knowledge: Domain Generalization Using Regularized Knowledge Distillation\n\n[2] 2023 - ICCV - A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance\n\n[3] 2025 - IJCAI - Balancing Invariant and Specific Knowledge for Domain Generalization with Online Knowledge Distillation"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "emYIxBo6Yd", "forum": "02cEkpURXH", "replyto": "02cEkpURXH", "signatures": ["ICLR.cc/2026/Conference/Submission20949/Reviewer_JkxR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20949/Reviewer_JkxR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811585601, "cdate": 1761811585601, "tmdate": 1762939044405, "mdate": 1762939044405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses out-of-distribution (OOD) generalization in knowledge distillation by proposing an adaptive framework that uses early layer predictions to dynamically weight the loss components. The authors introduce a \"forecaster\" meta-network that leverages auxiliary classifiers at intermediate layers, along with uncertainty measures (entropy and confidence margin), to predict sample difficulty and reweight the balance between supervised loss and distillation loss on a per-instance basis. The method is evaluated on domain generalization benchmarks (OfficeHome, PACS, VLCS, TerraIncognita) and shows consistent improvements over vanilla KD (+1.0-1.2% average accuracy) while adding no inference overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strong empirical validation: Consistent improvements across 4 benchmarks (OfficeHome, PACS, VLCS, TerraIncognita) with both ResNet-152 and ViT-L/16 teachers (Tables 3, 6-13)\n\nNo inference overhead: The forecaster and auxiliary networks are discarded after training, maintaining deployment efficiency—a critical practical consideration clearly addressed\n\nThoughtful design choices: The post-hoc adjustment mechanism is well-motivated by Figure 3, which shows forecaster collapse without normalization. The ablation in Figure 2 demonstrates the importance of uncertainty features (AUC improves from 0.56 to 0.82)\n\nFair experimental setup: Uses DomainBed codebase, follows established protocols, compares against multiple baselines including best-performing DG algorithms per dataset (Tables 4-5)\n\nClear identification of vanilla KD's strength: The observation that vanilla KD often outperforms standalone DG algorithms (Section 1, Table 3) is valuable and underexplored in prior work"}, "weaknesses": {"value": "Limited architectural generality: The approach is tightly coupled to ResNet's residual block structure (4 auxiliary classifiers at specific stages). Critical concern: No evidence provided that this method generalizes to other architectures (Vision Transformers, EfficientNets, etc.) or modalities beyond image classification. This significantly limits practical applicability.\n\nModest improvements: Average gains of 1.0-1.2% over vanilla KD are consistent but relatively small. On individual domains, results are mixed (e.g., PACS in Table 11: some domains improve, others decline slightly). The cost-benefit tradeoff of added training complexity may not be compelling for practitioners.\n\nInsufficient theoretical justification: Why should early layer confidence specifically help with OOD generalization rather than just indicating general sample difficulty? The paper treats these as equivalent without rigorous justification. Early layers capture low-level features—the connection to domain shift is unclear.\n\nAd-hoc forecaster design: The forecaster architecture (1D conv + linear layer, Sections 4.2) appears hand-crafted for ResNet. Hyperparameters for post-hoc adjustment (µ*=0.5, ς*=0.1) are not justified or ablated. Algorithm 1 introduces multiple hyperparameters (Ts, Tf) without sensitivity analysis.\n\nLimited scope: Only evaluated on small-scale image classification benchmarks. No experiments on larger-scale datasets (ImageNet), other modalities (NLP, audio), or other student architectures beyond ResNet-18.\nIncomplete analysis:\n\nNo comparison with other sample reweighting approaches for KD\nTable 1 shows the method without adjustment fails, but is relegated to one dataset\nMissing computational cost analysis during training (forecaster adds overhead even if discarded later)"}, "questions": {"value": "Same as above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oKvq6yCVz8", "forum": "02cEkpURXH", "replyto": "02cEkpURXH", "signatures": ["ICLR.cc/2026/Conference/Submission20949/Reviewer_NoZD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20949/Reviewer_NoZD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940220135, "cdate": 1761940220135, "tmdate": 1762939043715, "mdate": 1762939043715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a student-centric, adaptive KD scheme that learns an instance-wise weight to balance cross-entropy vs. KL terms using a lightweight “forecaster” fed by early-layer readouts (stacked intermediate logits) plus uncertainty signals (entropy and a confidence margin). The forecaster is trained with a correctness-prediction objective and its outputs are stabilized via a batch-standardized sigmoid adjustment before modulating the student loss; training alternates between updating the student/auxiliary heads on train splits and the forecaster on a held-out validation split, and all auxiliaries are discarded at inference. Reported results indicate consistent OOD gains over vanilla KD and DG baselines across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Student-centric, instance-adaptive weighting grounded in early-layer signals and explicit uncertainty features is a clean, modular idea that avoids hand-crafted rules.\n\n2. No inference-time overhead: the forecaster and auxiliary heads are training-only, preserving deployment efficiency. \n\n3. Clear base loss formalization (CE + KD with temperature) and a simple correctness-based objective for the forecaster make the method easy to implement and analyze."}, "weaknesses": {"value": "1. You call (L^{(S)}*{\\text{tot}}) a “convex combination” yet allow (\\alpha,\\beta\\in\\mathbb{R}) and only later constrain (\\alpha+\\beta=1); non-negativity isn’t enforced. Moreover, the KD term is defined with temperature (\\tau) but the standard (\\tau^2) factor that keeps gradient magnitudes comparable to CE is omitted, and later the forecaster weight replaces ((\\alpha,\\beta)) as ((1-w*{\\text{adj}})) and (w_{\\text{adj}}) without any scale calibration. Together, these choices make the optimization ill-conditioned and sensitive to (\\tau). Please formalize the constraints (e.g., (\\alpha,\\beta\\ge0,\\ \\alpha+\\beta=1)) and the gradient scaling. \n\n2. The adjusted weight (w_{\\text{adj}}=\\sigma!\\left(\\varsigma^*,\\frac{z_f-\\mu_B}{\\sigma_B}+\\mu^*\\right)) depends on batch mean/STD ((\\mu_B,\\sigma_B)) and free hyperparameters ((\\mu^*,\\varsigma^*)). This injects stochastic, batch-size-dependent drift into the loss, offers no invariance guarantees, and couples optimization to mini-batch composition. A principled alternative would derive a calibration mapping from a proper scoring rule or isotonic/logistic calibration fitted on a disjoint set, with stability guarantees. At minimum, clarify how ((\\mu^*,\\varsigma^*)) are selected and provide conditions ensuring (w_{\\text{adj}}\\in(0,1)) does not collapse. \n\n3. The forecaster is trained as a binary classifier of student correctness on a “meta-validation” split while the student and auxiliary heads are frozen, then roles are swapped. This alternating scheme implicitly defines a bilevel problem, yet there is no convergence analysis, fixed-point characterization, or even a guarantee that the forecaster’s risk aligns with the downstream student objective under distribution shift between the training and validation partitions. Please either cast the method as bilevel optimization (and justify the alternating updates) or provide a stability argument for Algorithm 1’s cycle. \n\n4. Several mathematical details are unclear: (i) in Eq. (5) (p_K(\\cdot)) is described as “probability … classified as label (K)” while (K) also denotes the number of classes—this is ambiguous; (ii) the forecaster loss in Eq. (6) uses inner-product notation (\\langle \\cdot,\\cdot\\rangle) without specifying whether this is summed over the minibatch or averaged, and whether class imbalance is handled; (iii) the forecaster’s 1D convolution “over stacked intermediate logits” lacks a precise tensor layout (layer-major vs class-major) and dimensionalities, which matter for reproducibility. Tighten the notation, explicitly define shapes, and disambiguate symbols reused for different roles."}, "questions": {"value": "see the weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2cO0cHlmMu", "forum": "02cEkpURXH", "replyto": "02cEkpURXH", "signatures": ["ICLR.cc/2026/Conference/Submission20949/Reviewer_uDoj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20949/Reviewer_uDoj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972722281, "cdate": 1761972722281, "tmdate": 1762939042266, "mdate": 1762939042266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}