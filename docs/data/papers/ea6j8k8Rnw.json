{"id": "ea6j8k8Rnw", "number": 13478, "cdate": 1758218373131, "mdate": 1759897434568, "content": {"title": "Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation", "abstract": "Robotic manipulation with Vision-Language-Action models requires efficient inference over long-horizon multi-modal context, where attention to dense visual tokens dominates computational cost. Existing methods optimize inference speed by reducing visual redundancy within VLA models, but they overlook the varying redundancy across robotic manipulation stages. We observe that the visual token redundancy is higher in coarse manipulation phase than in fine-grained operations, and is strongly correlated with the action dynamic. \nMotivated by this observation, we propose Action-aware Dynamic Pruning (ADP), a multi-modal pruning framework that integrates text-driven token selection with action-aware trajectory gating. ADP introduces a gating mechanism that conditions the pruning signal on recent action trajectories, using past motion windows to adaptively adjust token retention ratios in accordance with dynamics, thereby balancing computational efficiency and perceptual precision across different manipulation stages. \nExtensive experiments on the LIBERO suites and diverse real-world scenarios demonstrate that our method significantly reduces FLOPs and action inference latency (e.g. 1.35× speed up on OpenVLA-OFT) while maintaining competitive success rates (e.g. 25.8% improvements with OpenVLA) compared to baselines, thereby providing a simple plug-in path to efficient robot policies that advances the efficiency and performance frontier of robotic manipulation.", "tldr": "", "keywords": ["Vision-Language-Actions", "Efficient Robotic Manipulations"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68ef8a95f4de9e5df430bfcb448dc32e3e37bc78.pdf", "supplementary_material": "/attachment/49addcc72a4ebf0f9a0858817c5e472ebbedd3b7.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the inference efficiency problem of Vision-Language-Action (VLA) models in robotic manipulation and proposes a method called Action-Aware Dynamic Pruning (ADP). The core observation is that the redundancy of visual tokens is not constant during long-horizon manipulation tasks—it varies across different operation stages: redundancy is higher during coarse movements, while fine-grained grasping requires more detailed visual information. Based on this insight, the authors design two mechanisms to tackle the issue."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is clearly structured, and the writing makes it easy for readers to follow.\n\n2. The experimental evaluation is relatively comprehensive."}, "weaknesses": {"value": "1. I still believe that the core contribution (i.e., gating + pruning) demonstrates only a moderate level of novelty since it is relatively intuitive rather than a fundamentally transformative architectural innovation."}, "questions": {"value": "1. In Figure 1, the word “pick” is mistakenly written as “picke.”\n\n2. Although the paper discusses existing VLA acceleration methods (such as attention token pruning and structured compression), I think the authors should more clearly explain how ADP differs from these approaches in terms of innovation and performance boundaries.\n\n3. I am also curious about the trade-off: since the paper introduces additional modules to learn how to prune tokens, at what level of token reduction does the time saved roughly offset the extra computation introduced by these modules?\n\n4. From my previous experience, using text to attend to visual features often results in attention being paid to irrelevant regions. Does your method address this issue?\n\n6. The authors propose to determine whether the current stage is “coarse” or “fine” based on recent changes in the end-effector’s trajectory. However, is this mechanism generalizable? Does it require manually set thresholds? And when the task type changes (e.g., when the motion involves mostly rotation rather than grasping), could this mechanism fail?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xbo7HWo2QY", "forum": "ea6j8k8Rnw", "replyto": "ea6j8k8Rnw", "signatures": ["ICLR.cc/2026/Conference/Submission13478/Reviewer_4B1i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13478/Reviewer_4B1i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645607267, "cdate": 1761645607267, "tmdate": 1762924095998, "mdate": 1762924095998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **Action-aware Dynamic Pruning (ADP), a plug-and-play pruning framework for Vision-Language-Action (VLA) models. ADP combines text-driven anticipatory pruning with an **action-aware dynamic gating strategy (to determine when pruning should be applied based on recent motion dynamics). The approach is validated on the LIBERO benchmark and real-world robot tasks, demonstrating notable FLOPs reduction (up to 1.35× speedup) while maintaining or even improving task success rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel problem formulation:\n- The paper identifies an underexplored property of robotic manipulation—action-aware visual redundancy—and systematically exploits it to design a dynamic pruning scheme. This insight bridges the gap between static pruning  and phase-dependent motion dynamics.\n\n2. Methodological clarity and rigor:\n- The paper provides clear mathematical formulations for both text-driven token importance and motion-based gating.\n- The windowed trajectory definition and dynamic decision function are well-motivated and interpretable.\n- The theoretical complexity analysis (Eq. 19–25) convincingly quantifies expected computational savings.\n\n3. Comprehensive evaluation:\n- The paper conducts thorough experiments on simulation and real robot setups.\n- Ablation studies isolate the contributions of each component.\n- Visualizations (Figures 4–8) effectively support the claimed interpretability of the pruning mechanism.\n\n4. Strong empirical results:\n- ADP consistently outperforms static baselines in both efficiency and accuracy trade-offs. Especially impressive real-world latency improvement (1.49×) with maintained success rate."}, "weaknesses": {"value": "1. While the gating mechanism is intuitive, the paper lacks an analysis of stability or convergence under fluctuating motion magnitudes. How sensitive is the dynamic switching rule (Eq. 16–18) to noise or suboptimal motion planning?\n\n2. The gating rules (mean or extrema) are empirical; there is no adaptive or learned thresholding. A sensitivity analysis of hyperparameters would strengthen the claim of robustness.\n\n3.The baselines are comprehensive but mostly training-free ones. Including training-aware compression methods (e.g., DeeR-VLA, Mole-VLA) in a fair fine-tuned comparison would make the argument of “plug-and-play” stronger.\n\n4.The ablation study primarily examines whether dynamic control is included, but does not vary pruning ratios adaptively within tasks. Showing per-stage pruning ratio curves would illustrate dynamic behavior more explicitly.\n\n5. The real-robot evaluation setup is described in the appendix, but lacks release of calibration or dataset details. Code availability and parameter settings for dynamic thresholds are not stated."}, "questions": {"value": "1. Have you tested how ADP behaves under sensor noise or inaccurate motion estimation?\n\n2. Could the gating rule be differentiable and trained end-to-end with reinforcement signals?\n\n3. How does ADP interact with temporal caching or diffusion-based decoders in future hybrid models?\n\n4. What happens when action magnitude and task difficulty are decoupled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wJtbyLj9xQ", "forum": "ea6j8k8Rnw", "replyto": "ea6j8k8Rnw", "signatures": ["ICLR.cc/2026/Conference/Submission13478/Reviewer_dNCJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13478/Reviewer_dNCJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788180032, "cdate": 1761788180032, "tmdate": 1762924095384, "mdate": 1762924095384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ADP, which is a pruning method for VLA. ADP adopts text-driven pruning and action-aware gating,"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper aims to tackle one of the crucial problems in robotics, namely the efficiency of deployment.\n\nThe idea is intuitive and the method show good empirical results."}, "weaknesses": {"value": "The design principal is kind of heuristic. Some rules (especially for the action-aware part) seem to be tailored for this pick-and-place. \n\nThe experiments are focused on the OFT model. Not sure whether this proposed method is compatible with other backbones.\n\nSee questions below"}, "questions": {"value": "(1)\tFor the text-driven pruning, the visual tokens with low text attention score are discarded. However, during VLA’s training, no explicit constraints are applied on the text to image attention. How to make sure the correctness of the pruned area?\n\n(2)\tSince the method is claimed to be a plug-and-play component, results on ADP with other backbone are welcome. Currently, in table 1, it seems ADP is only combined with OFT.\n\n(3)\tIn some cases, the pruned policy performs better than the baselines. It seems interesting. Is there any discussions on this part?\n\n(4)\tThe design principal is kind of heuristic. Some rules (especially for the action-aware part) seem to be tailored for this pick-and-place. Other experiments are needed to show the proposed principal is universal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "idmcWZRnyU", "forum": "ea6j8k8Rnw", "replyto": "ea6j8k8Rnw", "signatures": ["ICLR.cc/2026/Conference/Submission13478/Reviewer_Uych"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13478/Reviewer_Uych"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800248650, "cdate": 1761800248650, "tmdate": 1762924094564, "mdate": 1762924094564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a plug-and-play framework to improve the efficiency of Vision-Language-Action (VLA) models for robotic manipulation by dynamically pruning redundant visual tokens. The key idea is to combine text-driven token selection with an action-aware gating mechanism conditioned on recent end-effector motion. Experiments on the LIBERO benchmark and real-robot setups show that ADP reduces FLOPs and inference latency by up to 1.35× (on OpenVLA-OFT) with minimal accuracy degradation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Unlike prior pruning or compression methods (e.g., EfficientVLA, DeeR-VLA, VLA-Cache), ADP leverages **action-conditioned motion dynamics** to modulate token pruning during robotic manipulation. The insight that **visual redundancy varies with manipulation phase** (coarse vs. fine-grained motion) is both intuitive and unexplored, giving the paper a clear conceptual novelty.\n\n- The paper's primary strength is its core insight that visual redundancy in VLA models is action-aware and correlates with the manipulation phase. This is a novel observation that moves beyond static or text-only pruning strategies.\n\n- The method is validated with strong empirical results in simulation and the real world. In simulation, it provides a clear accuracy-to-speedup trade-off, achieving a 1.35x speedup with only a 2.7% drop in success rate (at 30% ratio). On a physical Jaco2 robot, it demonstrates a 1.49x reduction in latency."}, "weaknesses": {"value": "- The action-aware gating mechanism, while effective, is built on a stack of heuristics. This includes the choice of motion metric ($\\delta_i$ as Euclidean displacement), the specific gating rule (\"adjacent-extrema\"), the fixed window size ($\\omega=8$), and hard-coded reset rules (e.g., a two-window \"cold start\" and a forced reset after three pruned windows). It is unclear how these settings would generalize to new tasks, robots with different dynamics, or different control frequencies without tuning.\n\n- The pruning strategy relies on similarity scores from Layer 0. This is counterintuitive, as deeper layers are typically assumed to capture more fused semantic meaning, although the paper provides a good justification in Figs. 6 and 7.\n\n- The experiments primarily rely on OpenVLA-OFT as the base model and omit results on diverse backbones (e.g., Pi0, GrooT, SmolVLA). Code availability or hyperparameter details are not specified beyond window size and ratios (Sec. 5.1), which hinders reproducibility.\n\n- The paper notes possible degradation during fine manipulation (Sec. 4.2) but does not quantify how the dynamic switch may misfire or cause visual underrepresentation in complex scenes."}, "questions": {"value": "- Could the authors provide results or qualitative visualizations on cases where the gating rule failed (e.g., incorrect pruning during fine-grained manipulation)?\n\n- How sensitive is performance to the window length and retention ratio? An ablation over these parameters would clarify the generality of the dynamic policy.\n\n- The real-world experiments used a single fixed camera, whereas the simulation setup used multi-view pruning, including a wrist view. How does the action-aware gate perform without a wrist camera, whose motion dynamics would be identical to the end-effector trajectory? Does this change simplify or complicate the tuning of the gating threshold?\n\n- Could the **dynamic pruning controller** be made learnable (e.g., via reinforcement learning) instead of relying on hand-crafted thresholds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x8HPmVuhIf", "forum": "ea6j8k8Rnw", "replyto": "ea6j8k8Rnw", "signatures": ["ICLR.cc/2026/Conference/Submission13478/Reviewer_EgjS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13478/Reviewer_EgjS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887608459, "cdate": 1761887608459, "tmdate": 1762924093981, "mdate": 1762924093981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}