{"id": "5hQMbFaCJv", "number": 23017, "cdate": 1758338251689, "mdate": 1759896835834, "content": {"title": "Enhancing Logical Reasoning of Large Language Models via Phased Fine-Tuning", "abstract": "Large language models (LLMs) have not only achieved impressive progress in natural language processing tasks but also demonstrated remarkable performance in practical applications such as intelligent customer service. However, LLMs continue to demonstrate shortcomings in complex logical reasoning and decision-making capabilities. As one of the key elements in human intelligence, logical reasoning plays a crucial role in various tasks, including natural language understanding, intelligent question-answering, and knowledge graph construction. The deficiency of LLMs in logical reasoning significantly limits their applications, especially in domains requiring high accuracy and trustworthiness. To tackle this issue, we focus on propositional logic and introduce a logic QA-specific phased fine-tuning method to enhance the logical reasoning capabilities of LLMs, performing supervised fine-tuning from easy to hard. Based on the symbolic logical form complexity derived from disjunctive normal form (DNF) and the LLM logical reasoning complexity in propositional logic question-answering tasks, the difficulty of logic question-answering samples is automatically computed, and the training data is stratified based on the difficulty. A dedicated fine-tuning dataset for propositional logic is constructed, and experiments demonstrate the significant effectiveness of our method, especially in those tasks demanding strong logical reasoning ability.", "tldr": "", "keywords": ["Large language model", "Logical reasoning", "Fine-tuning"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a87ed849f0ba0d794d26e16ea408dcb68db5bf0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the logical reasoning capabilities of LLMs, by using curriculum learning method, a three-stage training framework from easy to hard was established, and it was proposed to use the combination of syntactic complexity and semantic understanding difficulty to distinguish the difficulty, achieving the division of three stages. Additionally, the authors trained models based on Llama-3.1 8B and Ministral-8B on a training set produced by the pipeline, and results show that the framework enhances logic reasoning abilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The combination of symbolic (DNF-based) and probabilistic (entropy-based) reasoning complexity is theoretically well-motivated and interpretable.  \n\n2. The phased fine-tuning design aligns with human-style curriculum learning and shows stable performance gains across datasets.  \n\n3. The method is evaluated on diverse benchmarks, showing both logical and general reasoning improvements."}, "weaknesses": {"value": "1. The paper only shows that different models prefer different $\\alpha$ values but does not analyze the underlying reasons or training dynamics, making this section observational rather than explanatory.\n\n2. The claimed “improved fine-tuning efficiency” is not supported by any efficiency analysis or quantitative evidence.\n\n3. This method is primarily tested on formal logic QA datasets. Its applicability to more naturalistic or open-domain reasoning tasks remains uncertain.\n\n4. No phase-wise results are provided to verify that each training stage contributes progressively.\n\n5. Minor formatting issue (line 258, Appendix B)."}, "questions": {"value": "1. How reliable is the LLM-based natural-to-symbolic translation process?\n\n2. How does the framework handle ambiguous or underspecified sentences during translation?\n\n3. How well would the method generalize to more naturalistic reasoning datasets?\n\n4. Line 420, in “sensitive analysis”, can you obtain more accurate conclusions by further analyzing through trying more model sizes and series?\n\n5. Line 296, the weight parameter  $\\alpha$  do not have specific values. Considering that “sensitivity analysis” were conducted later, weight parameters  $\\alpha $  have a significant impact on the final performance of the model. Therefore, it is necessary to specify what the weight parameters were for the experimental settings in other experiments.\n\n6. Figure 2 shows that the model accuracy will improve as the number of training steps increases, but it is clear that there is a significant turning point at 800 steps, which requires further analysis and explanation. Also, why was it chosen to stop at 982 steps? What will happen to the accuracy if training continues further?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WfPEhzhUt6", "forum": "5hQMbFaCJv", "replyto": "5hQMbFaCJv", "signatures": ["ICLR.cc/2026/Conference/Submission23017/Reviewer_MGEu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23017/Reviewer_MGEu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798676203, "cdate": 1761798676203, "tmdate": 1762942477790, "mdate": 1762942477790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limited logical reasoning capabilities of large language models (LLMs) by proposing a phased fine-tuning framework based on curriculum learning. The authors focus on propositional logic question-answering (QA) tasks and introduce a novel method for automatically measuring sample difficulty using two components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper introduces a principled approach to quantifying the difficulty of logical reasoning samples, combining both syntactic (DNF-based) and semantic (LLM-prediction-based) complexities. This is a clear advancement over heuristic or manual difficulty labeling, and the theoretical grounding in propositional logic and information theory is solid.\n2) The proposed method is extensively evaluated across a wide range of benchmarks, including both logical reasoning (e.g., FOLIO, LogicNLI) and general reasoning (e.g., MMLU, ARC) tasks.\n3) The paper provides theoretical justification for the proposed method."}, "weaknesses": {"value": "1) The entire pipeline is tightly coupled to propositional logic: DNF conversion, independence of atomic propositions, and entropy computation all break down once quantifiers, equality, or modal operators appear. The paper title promises “logical reasoning” in general, yet the method cannot handle even minimal first-order problems.\n2) The approach relies on an LLM to translate natural language premises and conclusions into symbolic form. Errors in this translation step could propagate into the difficulty measurement and fine-tuning process. The paper does not thoroughly analyze the impact of translation errors or propose robust solutions for handling them.\n3) The weight $(\\alpha\\)$ balancing $\\(C_{SL}\\)$ and $\\(C_{NL}\\)$  is shown to be dataset- and model-dependent, with no consistent optimal value. This suggests that the method may require careful tuning for new tasks or models, limiting its out-of-the-box applicability."}, "questions": {"value": "1) The paper explicitly limits itself to propositional logic. How would you extend the difficulty metric to first-order or modal logic without incurring exponential blow-ups during normal-form conversion?\n2) You provide a strong justification for using DNF over CNF due to computational explosion. However, the \"difficulty\" of a logical problem might also be influenced by its inherent structure, which could be more naturally captured by other forms (e.g., the number of nested implications). Did you explore any other canonical forms or syntactic complexity measures beyond DNF length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TkWicjN47r", "forum": "5hQMbFaCJv", "replyto": "5hQMbFaCJv", "signatures": ["ICLR.cc/2026/Conference/Submission23017/Reviewer_ewk3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23017/Reviewer_ewk3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822418815, "cdate": 1761822418815, "tmdate": 1762942477443, "mdate": 1762942477443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a difficulty measurement scheme for propositional-logic-structured problems in order to order training samples from easy to hard during fine-tuning. The method is based on a semantic complexity measure over the LLMs independent probability evaluations of each premise in the problem, and a structural complexity measure taken as the length of the problem in DNF form."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well laid out and mostly easy to follow\n2. Conceptually, the rationale behind the method, and the method itself, is mostly clear\n3. The method is benchmarked over a good variety of datasets\n4. Sensitivity analyses are provided for the hyper-parameters of the method"}, "weaknesses": {"value": "1. Some minor claims made in the paper are unsupported (Questions 3, 9, 13)\n2. Many of the formalisms presented in the methodology section are poorly defined or mostly extraneous to the method (Questions 3,4,5,6,7,8)\n3. Some elements of experimental design seem questionable: \n     a. Baseline selection is sparse, only comparing to the most naive approach (vanilla fine-tuning) (Question 10)\n     b. metric choice on some benchmarks is not self-explanatory, and no explanation is provided (Question 11)\n4. Implementation details are not provided: code is not provided, and the details of how the LLM's individual proposition probabilities are extracted are not provided (contextualization, prompting, post-model normalization over the support, ...)"}, "questions": {"value": "1. l. 269: can you define the w described as a condition to the independence? \n2. l. 269: to my knowledge, independence is not a natural concept in propositional logic. Can you define independence here?\n3. l. 303: \"indicating that the rankings [...] are not entirely consistent\". Can you explain how this is indicated? I see no indication of this. We would need to see some kind of (empirical) problem-wise score difference distribution to validate this claim.\n4. Section 3.2: you define the Difficulty Partial Order Principle (DPOP), arguing that problems which do not satisfy it are incomparable (l. 235). However, since you prove only that your later complexity measurement function is such that all rankings from the direct DPOP are preserved, your measurement function will also capture rankings which come from problem pairs which do not satisfy DPOP. This is clear from the fact that a problem with a single clause of 1000 variables will be seen as more complex than a problem with 5 clauses of 5,4,3,2,1 lengths respectively, whereas these two problems do not satisfy DPOP. Since later on you do not refer back to DPOP, what was the purpose of its introduction? It seems likely that most of the pairwise difficulty orderings determined by your measurement function will in practice not observe DPOP, since it is a fairly strict rule.\n5. I find your probabilistic framing of propositional logic in the logic QA setting somewhat confusing. If the context to a problem gives \"Tom reads Game Theory\" concretely, the probability p(P=T | context) should be 1. In a vacuum, if we are not given this premise, asking whether it is true that Tom reads Game Theory seems a strange question, since we do not know anything about Tom. Can you define your concept of probability measure here, and if/how it is conditioned on some given assumed-to-be-true context?\n6. Relatedly, your current definition of p(Q=T) at line 274 is defined recursively. If a context is provided such that the problem is well-defined in a propositional sense (i.e. Q is deducible by propositional logic), then presumably the recursive computation would get to a point where the probability of the atomic premises E would be either 0 or 1, and then this would lead to p(Q=T) eventually being either 0 or 1. This is all hypothetical, however, since the base measure of p(Q=T) is not defined, and I'm assuming it would be defined such that p(Q=T | Q=T) = 1. When you prompt the LLM for p(E), do you provide it the context of the problem? Typically, problems include some single-variable clauses (in CNF form) which define atomic truths with which one can then deduce the query using the provided multi-variable clauses (inference rules). \n7. Relatedly, given your definition of P(Q=T), which is computed over every proposition in the problem, are logically irrelevant propositions which were included in the problem not included in your computation? If my problem is \"John likes candy. If John likes candy he is young. Sarah hates candy. Q: John is young\", would your computation not include P(Sarah hates candy)? \n8. Relatedly, given your definition of P(Q=T), is it not true that P(Q=T) = P(\\neg Q = T), since the premises E of the question do not change with the query? If your computation of each atomic probability during the recursion is based on the contextual problem, then this would not be true as eventually you would have that p(Q =T | \\neg Q = T) = 0 instead of P(Q = T | Q = T) = 1. Again, this would demand some notion of conditioning on the given problem context, rather than assuming independence across premises/propositions. \n9. You mention that CNF conversion of logic resulted in \"exponential explosion\" (l. 217). I find this surprising, since typically problems are naturally described such that all listed premises must be simultaneously true. In cases where a single premise includes a conjunction in the antecedent, DeMorgans leads to a simple conversion to disjunction. In cases where a single premise includes a conjunction in the consequent, splitting the premise into two premises for each variable in the conjunction works fine. Can you explain further how this explosion occurs?\n10. Experimental results: why do you use exact match (EM) for FOLIO? To my knowledge, it is a multiple choice dataset where the choices are True, False, or Unknown. Why do you use acc_norm for LogiQA and MathQA? It seems that these datasets are labelled with a single correct answer. If they don't, should you not use some sort of multi-label classification metric, like Hamming distance or Acc@K?\n11. Experimental results: to what degree does your method improve over non-fine-tuned approaches, such as single in-context prompting and self-consistency? To what degree does your method improve over alternative curriculum schemes, such as the ones described in your related works section?\n12. In general, can you please link to specific appendices when referencing them in the main text, rather than a general \"See the supplementary material [...]\"?\n13. l. 255: \"The syntactic complexity of each DNF formula [...] is entirely determined by a sequence of integers\". This claim is neither substantiated in the paper, nor true. If this were true, two problems with identical sequences would be identically complex to solve. A vast literature on the measurement of empirical solution complexity of propositional logic problems exists, demonstrating that complexity is not this simple. For instance, see [1] \n14. While not necessary to the paper, an exploration of empirical measurements of propositional complexity via SAT-solver-based metrics (# conflicts, time-to-solve, tree-depth) would be interesting to see.\n\n\n[1]  Ansótegui, Carlos, et al. \"Measuring the Hardness of SAT Instances.\" AAAI. Vol. 8. 2008."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TEc0CX2cch", "forum": "5hQMbFaCJv", "replyto": "5hQMbFaCJv", "signatures": ["ICLR.cc/2026/Conference/Submission23017/Reviewer_Uno8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23017/Reviewer_Uno8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23017/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762026039520, "cdate": 1762026039520, "tmdate": 1762942477233, "mdate": 1762942477233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}