{"id": "XuQfB6RqSd", "number": 14365, "cdate": 1758233781425, "mdate": 1759897374843, "content": {"title": "Online Learning with Recency: Algorithms for Sliding-window Streaming Multi-armed Bandits", "abstract": "Motivated by the recency effect in online learning, we study algorithms for single-pass *sliding-window streaming multi-armed bandits (MABs)* in this paper. In this setting, we are given $n$ arms with unknown sub-Gaussian reward distributions and a parameter $W$. The arms arrive in a single-pass stream, and only the most recent $W$ arms are considered valid. The algorithm is required to perform pure exploration and regret minimization with *limited memory*. The model is a natural extension of the streaming multi-armed bandits model (without the sliding window) that has been extensively studied in recent years. \nWe provide a comprehensive analysis of both the pure exploration and regret minimization problems with the model.  For pure exploration, we prove that finding the best arm is hard with sublinear memory while finding an \\emph{approximate} best arm admits an efficient algorithm. For regret minimization, we explore a new notion of regret and give sharp memory-regret trade-offs for any single-pass algorithms. We complement our theoretical results with experiments, demonstrating the trade-offs between sample, regret, and memory.", "tldr": "", "keywords": ["streaming algorithms", "sliding window model", "multi-armed bandits", "online learning"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/105fe07a298b9d92e2e6207768510bc1024983c1.pdf", "supplementary_material": "/attachment/87c9f6c450c24e3f24f091b0c4b8347a93426ea8.pdf"}, "replies": [{"content": {"summary": {"value": "This paper studied a streaming multi-armed bandit problem. Instead of targeting a global objective, such as identifying the good arms or minimizing regret, the learner seeks to identify the good arms and minimize regrets in a sliding window containing $W$ arms. An exploration algorithm is proposed, and the regret minimization approach is developed based on it. Analytical results of the proposed algorithms are provided."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The theorems look reasonable, but I did not check the Appendix for details.\n3. Experimental results are presented."}, "weaknesses": {"value": "- No baselines are provided in the experiment section. Other algorithms, such as the track-and-stop and phase elimination, may be modified to address the sliding-window streaming MABs. Showing existing algorithms failing to achieve the local objective in the sliding window could highlight the significance of this work.\n\n- Though the theoretical results make sense to me, the technical challenge is unclear. Since the mean rewards are assumed to be bounded by $[0,1]$, to determine the $\\epsilon$-best arm, it is natural to divide the range into buckets of size $O(\\epsilon)$. Also, the dependence of sample complexity on the window size $W$ seems straightforward. Intuitively, to achieve a miss detection rate $\\leq \\delta$, each arm can be sampled to till the empirical mean is $\\epsilon$ close to the true mean with probability $1-\\delta/W$.\n\n- The regret minimization setup seems trivial to me, especially after the pure exploration algorihtm is established. One could also use other MAB algorithms, e.g., Thompson sampling or UCB, to minimize regret in each epoch. These algorithms can also be modified to be memory-efficient if $O(\\epsilon T)$ regret is allowed.\n\n-  The regret defined in separate epochs seems artificial, and the application is unclear. Why do existing models, such as arm-acquiring bandit [1], sleeping experts problem [2], mortal MABs [3], and nonstationary bandits, fail to address those scenarios? The paper could include the discussion.\n\n[1] Whittle, Peter. \"Arm-acquiring bandits.\" The Annals of Probability 9.2 (1981): 284-292.\n\n[2] Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping experts and bandits. Machine learning, 80(2):245–272, 2010.\n\n[3] Chakrabarti, Deepayan, et al. \"Mortal multi-armed bandits.\" Advances in neural information processing systems 21 (2008)."}, "questions": {"value": "Questions\n\n- Could you explain the technical challenge for this problem?\n- I think the phase elimination algorithm is a perfect fit for the sliding-window streaming MABs, since the bad arms are consistently eliminated from the candidate set. Why is it not mentioned in this paper?\n- Could you give more example applications of the epoch-wise regret minimization?\n\nSuggestions\n\nThe following suggestions are only for discussion. They do not need to be addressed.\n\n- The problem could be more interesting if the problem-dependent sampling complexity (and regret bound) were studied. And I think the phase elimination could be a very good approach in this setup.\n\n- The reverse version of the proposed problem might be more interesting. Given a fixed memory, what performance can be achieved? It is clear that the sliding-window characterization is a good one."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BeQEHGTphz", "forum": "XuQfB6RqSd", "replyto": "XuQfB6RqSd", "signatures": ["ICLR.cc/2026/Conference/Submission14365/Reviewer_hDQv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14365/Reviewer_hDQv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761606806882, "cdate": 1761606806882, "tmdate": 1762924785123, "mdate": 1762924785123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors studied pure exploration and regret minimization in the sliding-window streaming multi-armed bandit setting, an extension of the steaming multi-armed bandits model where only the most recent W arms (sliding window) are considered valid. This formulation captures recency effects that arise in applications where underlying trends shift over time or where data retention is limited for privacy reasons. For the pure exploration problems, the authors show that identifying the exact best arm requires $\\Omega(W)$ memory, while finding $\\epsilon$-approximate best arm can be achieved with only $O(1/\\epsilon)$ memory. For regret minimization, they introduce a new notion of sliding-window regret and establish a trade-off between memory and regret, demonstrating how performance degrades as memory constraints tighten."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper is the first to extend the sliding-window streaming model to the multi-armed bandit setting. For pure exploration problem, the authors established a upper bound for weak $\\epsilon$-approximation and a lower bound for strong $\\epsilon$-approximation that matches up to an extra $\\log n$ factor. For the regret minimization problem, the paper introduces a new notion of sliding-window regret that is defined epoch-wise and does not depend on the arm pulls chosen by the algorithm. Under this refined definition, the authors prove a strong lower bound, showing that achieving a total regret smaller than $O(T/W^2)$ is impossible with $o(W)$ space."}, "weaknesses": {"value": "A concern lies in the ambiguity of the scaling of the sliding window parameter $W$ which is a key component of the proposed model. For pure exploration, the lower bound is $\\Omega(W)$, while in the later sections, $W$ appears to be treated as a constant. In fact, the scaling of all terms need to be made clear in the definitions and bounds. This is also the case for regret minimization problem. The lower and upper bound there do not seem to match, but the mismatch is not clearly explained, and the scales of the two bounds are not directly comparable. The paper would benefit from a clear and consistent specification of how $W$ and other parameters scale throughout the analysis. \n\nThe presentation of the paper could be improved. The main contributions and results are repeated verbosely, while the key derivations and methodological insights are presented too tersely. Given the somewhat incremental nature of the work, it would strengthen the paper to reduce redundancy and instead provide more discussion, intuition, and sketch proofs to clarify why the proposed results are nontrivial and how the algorithm brings methodological advances."}, "questions": {"value": "1. In the motivation example of theatre visits, does the sliding windows correspond to a time period of 1-2 months and T the number of theatre visits? If so, T could be smaller than n-W+1 in this case. Could the authors clarify this interpretation?  \n2. For the strong $\\epsilon$ pure exploration case, any insights into why using a larger pulling size alone can improve the bound alone helps match the lower bound?   \n3. What is the assumption for arm distributions? The experiments seem to rely on Bernoulli arms, which may be overly simplistic for modeling streaming settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Bjt5iM1cwg", "forum": "XuQfB6RqSd", "replyto": "XuQfB6RqSd", "signatures": ["ICLR.cc/2026/Conference/Submission14365/Reviewer_XSQq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14365/Reviewer_XSQq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930164569, "cdate": 1761930164569, "tmdate": 1762924784665, "mdate": 1762924784665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper formalizes a sliding-window streaming MAB setting where arms arrive once in a stream and, at time $t$, only the most recent $W$ arrivals are considered \\emph{valid}. An algorithm may store a subset of past arms (subject to a memory budget) and, upon each arrival, decide whether to pull, store, or discard. The work studies two tasks:\n\n\n\\textbf{Pure exploration.} The authors show that \\emph{exact} best-arm identification within the active window requires $\\Omega(W)$ memory in a single pass (even with many pulls). In contrast, $\\varepsilon$-best identification is achievable with space $O(1/\\varepsilon)$ via a simple \\textsc{BUCKET} scheme that partitions rewards into $O(1/\\varepsilon)$ bins and keeps the newest representative per bin. Sampling complexity is $O\\!\\big(\\tfrac{n}{\\varepsilon^2}\\log \\tfrac{W}{\\delta}\\big)$ in the weak setting and $O\\!\\big(\\tfrac{n}{\\varepsilon^2}\\log \\tfrac{n}{\\delta}\\big)$ in the strong setting.\n\n\\textbf{Regret minimization.} Because naive regret is ill-posed (the learner can shift pulls across time), the paper introduces \\emph{epoch-wise regret}: split the horizon into $n-W+1$ epochs and enforce exactly $T/(n-W+1)$ pulls per epoch. Under this notion, $\\Omega(W)$ memory is necessary to achieve $o(T)$ regret; with $O(W)$ memory, the authors give an algorithm with regret $O\\!\\big(\\sqrt{W\\,(n-W)\\,T}\\big)$ (matching centralized $O(\\sqrt{nT})$ when $W$ is constant).\n\n\nExperiments on synthetic Bernoulli streams validate: (i) a smooth memory--quality trade-off for $\\varepsilon$-exploration and (ii) a sharp drop in regret once memory reaches $W$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Crisp model for recency: The sliding-window abstraction makes the ``recent-only'' constraint explicit and separates validity of arms from memory limits, enabling clean space/sample guarantees.\n\n2. Fundamental hardness: The $\\Omega(W)$ space lower bound for exact best arm (single pass) is clear and compelling, isolating an intrinsic barrier tied to the window size.\n\n3. Practical approximate exploration: The \\textsc{BUCKET} algorithm attains space $O(1/\\varepsilon)$ and near-optimal sample complexity using standard concentration, with an implementation that is simple and streaming-friendly.\n\n4. Well-posed regret notion: Epoch-wise regret resolves the definitional pitfall of naive windowed regret and supports sharp lower/upper bounds.\n\n5. Tight memory--regret frontier. The results pinpoint a phase transition: $o(T)$ regret needs $\\Omega(W)$ memory, while $O(W)$ memory suffices for $O\\!\\big(\\sqrt{W(n-W)T}\\big)$ regret; this situates the setting relative to classical $O(\\sqrt{nT})$ bounds.\n\n6. Empirical corroboration. Simulations reproduce the predicted phase transition (near memory \\(W\\)) and demonstrate the expected behavior of the \\textsc{BUCKET} scheme."}, "weaknesses": {"value": "1.  Motivation vs. storage model (expired arms can be kept): The model permits retaining any past arms in memory even after they leave the valid window. For privacy/retention-driven applications---a stated motivation---expired data would typically need to be deleted, likely tightening the achievable space/sample trade-offs. A variant that forbids storing expired arms (or charges for it) would better align with such use cases and reveal which guarantees survive under stricter retention.\n  \n2. Fixed pulls per epoch may limit policy expressiveness: Epoch-wise regret fixes exactly $T/(n-W+1)$ pulls in each epoch to avoid algorithm-dependent benchmarks. This is mathematically clean, but constrains adaptive allocation of effort across epochs (e.g., heavier exploration right after a suspected shift). A relaxed notion with bounded per-epoch variability could preserve well-posedness while covering common operational practices.\n  \n3. Experimental scope is narrow and fully synthetic: Experiments consider Bernoulli rewards and do not include head-to-head comparisons with strong non-windowed streaming heuristics (e.g., sliding-window UCB/TS used naively) nor real/semi-synthetic traces where recency is known to matter. Broader baselines and stress tests (varying $W$, non-stationary gaps) would better establish practical benefits.\n  \n4. Worst-case lower bounds may overstate typical memory needs: The $\\Omega(W)$ space lower bound for exact best-arm relies on a descending-means construction (Yao-style). In benign instances with large gaps, exact identification could be feasible with $o(W)$ memory. Presenting instance-dependent or average-case refinements would contextualize the worst-case message.\n  \n5. \\textsc{BUCKET} is gap-agnostic and can be conservative:\n The algorithm partitions $[0,1]$ into $O(1/\\varepsilon)$ bins and uses uniform per-arm sampling $s=\\Theta(\\varepsilon^{-2}\\log(\\cdot))$, independent of realized gaps. Gap-adaptive policies (e.g., racing/successive elimination adapted to the window) could reduce pulls substantially when many arms are clearly suboptimal. A comparison or ablation would help practitioners gauge efficiency.\n  \n6. Heavy-tailed rewards are not addressed:\n Analyses assume bounded or sub-Gaussian rewards. Many streaming logs are heavy-tailed; clarifying whether the results extend with robust estimators (median-of-means, Catoni) would widen applicability.\n  \n7. Limited practical guidance for tuning $\\varepsilon, W$, and memory: \nWhile the asymptotic rates are clear, practitioners will ask: given a memory cap $m$, how to pick $\\varepsilon$ (or bucket count) and expected sample budget to meet a target error/regret? A concise design table or rule-of-thumb derived from the theorems would increase usability.\n\n8. Claims about prior methods would benefit from direct baselines:\n The text argues earlier streaming MAB methods can output stale/invalid arms in recency-sensitive settings. A head-to-head empirical comparison (even if those baselines are naively adapted) would make this gap concrete and quantify the advantage."}, "questions": {"value": "Q.1. Privacy-consistent retention: The paper’s motivation includes data-retention and privacy considerations, yet the formal model allows the learner to retain arms even after they exit the valid window. This creates a gap between motivation and assumptions. Please clarify which results (e.g., the $\\Omega(W)$ space lower bound for exact best-arm and the $O(1/\\varepsilon)$-space $\\varepsilon$-exploration guarantees) continue to hold if expired arms must be dropped immediately, and which ones would need to change. A short lemma or an experiment under a “no-storage-after-expiration’’ constraint would make the scope precise.\n\nQ.2. Regret with bounded per-epoch variability: Epoch-wise regret fixes exactly $T/(n{-}W{+}1)$ pulls per epoch to avoid an algorithm-dependent comparator. While clean, this rigidity may not reflect realistic allocation patterns where effort is rebalanced after suspected shifts. It would strengthen the contribution to discuss a bounded-variation alternative and whether the main lower/upper bounds survive or fail under that relaxation; a succinct counterexample would be equally valuable.\n\nQ.3. Gap-adaptive exploration under windows: The proposed \\textsc{BUCKET} scheme is gap-agnostic: it fixes binning and sample counts regardless of realized difficulty. Many deployments exhibit large gaps where racing/successive-elimination saves pulls dramatically. Please consider a window-aware gap-adaptive baseline and report whether similar $O(1/\\varepsilon)$ space can be preserved while reducing samples on easy instances; an ablation would clarify the practical efficiency frontier.\n\nQ.4. Heavy-tailed rewards: The analysis relies on bounded or sub-Gaussian rewards, which can be violated by bursty or long-tailed streams. A brief discussion of robust mean estimators (median-of-means, Catoni) and the conditions under which the stated space/sample orders continue to hold would broaden applicability. Even a proof sketch or an appendix lemma would suffice.\n\nQ.5. Instance-dependent or average-case lower bounds: The $\\Omega(W)$ space lower bound for exact best-arm is worst-case (descending-means style). Practitioners will care about typical regimes where within-window gaps are large. Complementing the worst-case with instance-dependent lower bounds (in terms of the gap structure) or with empirical evidence that exact best-arm often needs $o(W)$ memory in benign instances would contextualize the hardness claim.\n\nQ.6. Baselines and stress tests: Practical relevance hinges on performance against strong streaming heuristics under a variety of conditions. Including sliding-window UCB/TS and reservoir-like policies, and stress-testing across $W$, $n$, and non-stationary gap profiles would help map out win/lose regimes and sensitivity. This would also illuminate whether the theoretical phase transition near memory $W$ is robust beyond the current synthetic setups.\n\nQ.7. Design guidance for practitioners: The theorems give clear asymptotic rates; translating them into a simple recipe would aid adoption. A small table that maps a memory budget $m$ to a recommended $\\varepsilon$, bucket count, and expected error/regret (with one worked example) would make the results immediately actionable for engineering teams.\n\nQ.8. Effect of retaining expired arms: To isolate the contribution of retention, an ablation across three regimes—(i) free retention of expired arms, (ii) memory-penalized retention, and (iii) no retention—would quantify how much of the observed gains stem from storing expired items. Reporting quality/regret versus memory in each regime would make this trade-off transparent.\n\nQ.9. Time/space complexity at stream scale: Streaming deployments are latency-sensitive. Reporting per-arrival processing time and memory footprints, and how they scale with $W$, $n$, and the number of buckets, would help readers judge deployability and identify bottlenecks or easy optimizations.\n\nQ.10. Relation to non-stationary bandits with persistent arms: A concise subsection clarifying the conceptual difference between “recency-validity over arms with single-pass space constraints’’ (this work) and “recency over rewards for persistent arms’’ (standard drifting-mean methods) would situate the contribution and indicate which techniques from that literature can or cannot be transplanted here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N94HVWwaYl", "forum": "XuQfB6RqSd", "replyto": "XuQfB6RqSd", "signatures": ["ICLR.cc/2026/Conference/Submission14365/Reviewer_hjY2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14365/Reviewer_hjY2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999261077, "cdate": 1761999261077, "tmdate": 1762924784154, "mdate": 1762924784154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}