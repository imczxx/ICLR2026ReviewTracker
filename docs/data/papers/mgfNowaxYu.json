{"id": "mgfNowaxYu", "number": 4540, "cdate": 1757702708793, "mdate": 1759898027567, "content": {"title": "Perch 2.0: The Bittern Lesson for Bioacoustics", "abstract": "Perch is a performant pre-trained model for bioacoustics. It was trained in supervised fashion, providing both off-the-shelf classification scores for thousands of vocalizing species as well as strong embeddings for transfer learning. In this new release, Perch 2.0, we expand from training exclusively on avian species to a large multi-taxa dataset. The model is trained with self-distillation using a prototype-learning classifier as well as a new source-prediction training criterion. Perch 2.0 obtains state-of-the-art performance on the BirdSet and BEANS benchmarks. It also outperforms specialized marine models on marine transfer learning tasks, despite having almost no marine training data. We present hypotheses as to why fine-grained species classification is a particularly robust pre-training task for bioacoustics.", "tldr": "", "keywords": ["bioacoustics", "audio", "sound"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/313ed46adf1c7da69721d612cdccdc027c0eb126.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Perch 2.0, an update to a pre-trained supervised model for (bird) bioacoustics. The key contributions include expanding the training data from exclusively bird species to a large multi-taxa dataset and adding a combination of existing training techniques: a prototype-based self-distillation, and an auxiliary source-prediction task. Perch 2.0 achieves state-of-the-art performance on different task types (retrieval, linear probing and pretrained out-of-the-box peformance) on the BirdSet and BEANS benchmarks. The paper also provides hypotheses for why supervised learning remains a dominant paradigm in the bioacoustics domain."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. *Clarity and motivation*: The paper is well-written and very easy to follow. The work is well-motivated by addressing real-world challenges faced by practitioners, such as the need for strong, generalizable embeddings from smaller models that do not require extensive fine-tuning.\n2. *Methodological combination:* The work combines existing techniques (self-distillation, source prediction, prototype learning) into a single training framework. This combination is well-suited to the problem of fine-grained, multi-class bioacoustic classification.\n3. *Comprehensive and practical evaluation:*  By testing the model on a diverse set of tasks, including pretrained out-of-the-box performance, one-shot retrieval, and few-shot linear, the authors mirror the practical ways such models could be deployed, providing a robust evaluation (and for further use in the future)."}, "weaknesses": {"value": "While the proposed method combination is interesting and the results are strong, the paper is limited by a lack of empirical validation. The core issue is an absence of ablation studies, which makes it impossible to attribute the performance gains to the specific contributions claimed by the authors (method-based, data-based, etc.).\n\n**1. Confounded contributions and lack of ablations:**\n\nThe core weakness is that the paper simultaneously introduces multiple changes to the previous model (a larger model backbone, more training data from new taxa, and several new training objectives,…). The central question of why the model has improved remains unanswered. For the work to be a strong scientific contribution, it would need to include ablations that isolate the impact of each component. Examples include:\n\n- *Data vs. method:* What is the performance gain from the new multi-taxa data alone, when training with the original Perch 1.0 methodology?\n- *Methodological components:* What is the individual contribution methods? The paper introduces a novel combination of methods but provides no empirical evidence that they are the actual drivers of the performance increase (e.g., mixup)\n- *Model scale:* What is the impact of moving from an EfficientNet-B1 to a B3 backbone? This choice feels arbitrary without a justification or comparison.\n\n**2. Insufficient baselines and reproducibility issues:**\n\n- *Missing baselines*:The evaluation would benefit from a category of baselines: general-purpose audio models pretrained on large datasets like AudioSet. Comparing would help contextualizing the performance of a domain model and support claims of SOTA performance.\n- *No code or release plan*: For a paper centered on a new model and its performance on new data compositions, the complete absence of code, model weights, or a concrete plan for their release degrades the quality. The paper does not state when or how the community will be able to access these resources, which limits the work's reproducibility (at least in the context of the review process now) and potential impact."}, "questions": {"value": "1. The paper's main weakness is the confounding of variables (new data, new architecture, new training objectives). Could you provide ablation studies that disentangle these factors? Specifically, can you demonstrate the individual performance contributions of the expanded dataset versus the new training objectives (self-distillation, source prediction, etc.) to understand what is truly driving the reported improvements?\n2. Could you justify the architectural choice of EfficientNet-B3 over other backbones?\n3. Could you please provide a concrete plan, including a timeline, platform and format, for the public release of the training/evaluation code, the model weights, and the exact data splits used in your experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wtzjgD8TCj", "forum": "mgfNowaxYu", "replyto": "mgfNowaxYu", "signatures": ["ICLR.cc/2026/Conference/Submission4540/Reviewer_kWia"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4540/Reviewer_kWia"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728263888, "cdate": 1761728263888, "tmdate": 1762917429694, "mdate": 1762917429694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Perch 2.0, a supervised audio classification model that incorporates three contributions: (1) a novel mixup augmentation variant, (2) a self-distillation mechanism using prototype-based learning, and (3) an auxiliary self-supervised objective. The model adopts an EfficientNet backbone with modifications to the training pipeline and is trained on a combined dataset comprising four existing datasets. The authors report state-of-the-art performance on benchmark evaluations, demonstrating improvements over previous approaches."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Clear presentation**: The paper is well-written and easy to follow.\n- **Comprehensive evaluation framework**: The inclusion of different model selection tasks in the evaluation is nice, as it helps to identify both strengths and limitations of the model.\n- **Pragmatic focus on supervised learning**: The decision to focus on supervised learning rather than following the current trend toward self-supervised methods is commendable. This work demonstrates that supervised approaches remain competitive and that self-supervised methods have not yet clearly surpassed them. This is an important contribution that encourages balanced research and prevents the field from prematurely abandoning promising supervised techniques."}, "weaknesses": {"value": "**Unclear novelty and insufficient differentiation from prior work**\n\nThe authors claim several contributions, including a novel mixup procedure, a self-distillation process, and a self-supervised auxiliary loss. However, the paper lacks clarity in distinguishing what constitutes genuinely novel contributions versus adaptations of existing techniques. For example, while the authors propose generalizing mixup to more than two components, they do not adequately discuss related work that already explores multi-component mixing strategies (e.g.,  [1]), nor do they clearly articulate how their approach differs from or improves upon these existing methods.\n\nThis issue extends to other claimed contributions as well. The paper reads more as a collection of independently developed techniques combined together rather than a cohesive framework with a clear design rationale. \n\nFor each component, it should be explicitly stated: \n\n- What is the actual novel contribution beyond existing literature?\n- What is the conceptual or empirical rationale for including this specific component?\n- How does it differ from and improve upon related work?\n\nWithout this clarity, it becomes difficult to assess the true technical contributions of the paper and understand the principled reasoning behind the proposed design choices.\n\n[1] Jang, Junwoo, Jungwoo Han, and Jinwhan Kim. \"K-mixup: Data augmentation for offline reinforcement learning using mixup in a Koopman invariant subspace.\" *Expert Systems with Applications* 225 (2023): 120136.\n\n**Missing rationale for key design choices**\n\nBeyond the novelty concerns, the paper lacks sufficient justification for its key design choices. While the authors incorporate multiple techniques from related work, they do not provide clear reasoning for *why* these specific design decisions should be beneficial or how they contribute to improved performance.\n\nCritical questions remain unanswered throughout the paper:\n\n- What is the rationale behind the specific mixup design? Why should this particular formulation be advantageous over alternatives?\n- What motivates the particular construction of the output heads? What benefits does this architecture provide?\n- More broadly, what is the underlying principle or insight that guides these design decisions?\n\nThis lack of motivation makes it difficult to understand whether the design choices are principled or simply empirically driven. The issue is compounded by an imbalanced presentation: while crucial design decisions lack proper justification, the paper dedicates excessive detail to minor implementation choices that appear neither novel nor particularly important. The authors should prioritize explaining the conceptual motivation behind their key contributions and reduce emphasis on less significant details.\n\n**Absence of empirical evidence and ablation studies**\n\nWhile the paper introduces numerous design choices, the evaluation is extremely limited and lacks any ablation studies to investigate these decisions. There is not a single experiment that systematically examines the individual contributions of the proposed components, making it impossible to assess which design choices actually benefit model performance and to what extent.\n\nGiven that this paper presents a combination of several techniques, this represents my strongest concern. When theoretical justification for a method is absent, rigorous empirical validation through ablation studies becomes essential. For example, there should be experiments that compare the generalized mixup variant against the vanilla mixup baseline to demonstrate the specific improvements their formulation provides.\n\nThis requirement applies to every key design choice in the paper. Without such ablations, the paper essentially presents an uncontrolled combination of techniques with no evidence about what actually drives the reported improvements. The claimed contributions cannot be properly validated, and readers have no guidance on which components are essential versus which might be redundant or even detrimental in other contexts.\n\n**Minor Issues:**\n\n- **Inconsistent caption placement**: The caption for Figure 1 is placed below the figure, while the caption for Figure 2 appears above. Caption placement should be consistent throughout the paper, preferably below all figures, following standard conventions.\n- **Improper citations**: Several references cite arXiv preprints when the work has actually been published at peer-reviewed conferences. Authors should update citations to reflect the published versions where available.\n- **Poor figure integration**: Multiple figures are included in the paper, but are never referenced in the main text. All figures should be explicitly referenced and discussed. Additionally, the purpose of Figure 1 is unclear. It appears to simply show an image of a bird without providing meaningful context or contributing to understanding the method. If figures do not serve a clear purpose, they should be removed or replaced with more informative visualizations."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6hw2J4NbKx", "forum": "mgfNowaxYu", "replyto": "mgfNowaxYu", "signatures": ["ICLR.cc/2026/Conference/Submission4540/Reviewer_fms8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4540/Reviewer_fms8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813796544, "cdate": 1761813796544, "tmdate": 1762917429443, "mdate": 1762917429443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Perch 2.0, an updated version of the bioacoustic\nclassification model Perch. Perch 2.0 includes more diverse training data,\nslightly modified architecture, and auxiliary loss functions. The training data\nincludes broader species coverage from Xeno-Canto, iNaturalist, Tierstimmenarchiv,\nand FSD50k. The model architecture is now based on EfficientNet-B3 (compared with\nEfficientNet-B0 in Perch 1.0) and includes a prototype-learning classifier head,\na source-prediction head, and a linear classification head. Training is conducted\nin two stages: First, only the linear-classification and source-prediction heads\nare used to compute the loss; then, the prototype-learning head is used to provide\nsoft targets for the linear-classification head, a form of self-distillation.\nHyperparameter tuning is conducted with Vizier using multiple\nvalidation tasks. The selected model is then evaluated on BirdSet and BEANS,\nachieving state-of-the-art results on both datasets. Subsequently, it is claimed\nthat \"supervision remains dominant in bioacoustics\" as shown by the strong\nperformance of Perch 2.0 and the benefit of fine-grained labels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- The model achieves state-of-the-art results on multiple datasets."}, "weaknesses": {"value": "While the developed model shows strong performance, the question remains: what\ncontributes to its strong performance? As multiple changes were made compared with\nthe BirdSet and Perch 1.0 baselines, it is hard to assess the importance of each\nindividual change. Most importantly, it is unclear how much the additional\ntraining data contributes to the performance increase relative to the\narchitectural changes and auxiliary losses. Ablation studies could help clarify\nthis. This is especially important because the authors conclude that\n\"supervision remains dominant in bioacoustics\"; however, it is unclear whether this\nconclusion still holds if Perch 2.0 is trained on the same data as BirdMAE.\n\nIn addition, the following weaknesses were identified:\n- Reproducibility is hindered because no training code is provided, and no fixed sets\n  of training data are used. Instead of training the model on BirdSet's Xeno-Canto\n  cut or iNaturalist's iNatSound dataset, the authors scrape the data anew.\n- While the model achieves state-of-the-art results, the improvements over\n  previous methods are relatively small.\n- The comparison on the BEANS benchmark lacks strong baselines as identified in\n  [1]. BirdMAE and the encoder of NatureLM-audio could also be tested with LP\n  and PP.\n- Tables in the appendix are hard to read, as the best and second-best results are\n  not highlighted.\n\n    \n[1] Foundation Models for Bioacoustics -- a Comparative Review\nhttps://www.arxiv.org/abs/2508.01277"}, "questions": {"value": "- How many seeds were used to validate the results? What are the standard\n  deviations?\n- What is the reasoning behind selecting 6 s and 5 s windows (lines 130–131)?\n- Why four prototypes per class (line 193)?\n- How much faster is training with softmax compared to a binary-sigmoid objective (line 207)?\n  Can you share some details?\n- What is your rationale behind setting the number of training steps? Did you\n  use early stopping?\n- \"General purpose audio benchmarks such as HEAR are still dominated by supervised and semi-supverised models...\" (l.424f.) What is your opinion on the recent results on the AudioSet benchmarks, where models like BEATs [2] or SSLAM[3] outperforming SL models?\n\n[2] BEATs: Audio Pre-Training with Acoustic Tokenizers https://arxiv.org/abs/2212.09058\n[3] SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes https://arxiv.org/abs/2506.12222"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H0MGq8Gk5W", "forum": "mgfNowaxYu", "replyto": "mgfNowaxYu", "signatures": ["ICLR.cc/2026/Conference/Submission4540/Reviewer_TSZg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4540/Reviewer_TSZg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848631465, "cdate": 1761848631465, "tmdate": 1762917429273, "mdate": 1762917429273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Perch 2.0 is a compact bioacoustic model trained on ~14 000 species and event classes to produce general-purpose audio embeddings. It keeps the backbone of Perch 1.0 but adds three elements: a prototype-based species head, a linear head trained through self-distillation from the prototype outputs, and a self-supervised source-prediction task. Strong data augmentations (generalized mixup, random windows) handle label noise.\nAcross 19 benchmarks, Perch 2.0 claims to achieve state-of-the-art transfer and few-shot performance while remaining small enough for field deployment. The authors argue that careful supervised training with auxiliary objectives can match or surpass much larger self-supervised audio models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper combines supervised training, prototype-based distillation, and auxiliary objectives in a clear, effective design.\n\n- Experiments cover a wide range of benchmarks and are technically solid.\n\n- The model transfers well across domains while staying compact and efficient.\n- Strong performance under linear probing shows the embeddings are general and practical to use.\n\n- The model architecture is optimized to be employed in real-world systems, so as to be as light as possible."}, "weaknesses": {"value": "- Unclear contribution of components to performance: The paper introduces several methodological components (e.g., multi-source mixup, self-distillation, and an auxiliary source-prediction loss). However, their individual contributions are not clearly isolated, as the paper does not provide controlled ablation studies. In particular, the role of the windowing strategy and the handling of label noise across heterogeneous sources remains insufficiently explained. This raises concerns regarding reproducibility and makes it hard to disentangle how components are contributing to the reported performance gains.\n\n- Missing engagement with contrasting literature: The paper does not currently cite or discuss [1], which provides a systematic comparative analysis of multiple models, including Perch 1.0, on the same BEANS and BirdSet benchmarks. That work concludes that self-supervised models such as BirdMAE generally outperform supervised approaches. Including and positioning the present findings in relation to [1] would strengthen the contribution by clarifying how this work complements existing results.\n\n- Dataset construction lacks some detail: the paper states that classes were manually mapped, but the procedure and criteria for this mapping are not fully described. More detail on how label merging/normalization was performed (e.g., handling synonyms) would help ensure reproducibility and interpret the consistency of supervision across sources.\n\n- Overall this is a good paper, with strong results and clear relevance to the field. The main gaps are the lack of ablation studies, some missing information on dataset construction and limited positioning relative to recent literature: addressing these points would substantially strengthen the clarity and credibility of the contribution. Additionally, a plan for code release is not mentioned but would further support reproducibility. With these additions, I would be in favor of a strong acceptance of the paper.\n\n[1] Schwinger, R., Zadeh, P. V., Rauch, L., Kurz, M., Hauschild, T., Lapp, S., & Tomforde, S. (2025). Foundation Models for Bioacoustics--a Comparative Review. arXiv preprint arXiv:2508.01277."}, "questions": {"value": "1. I understand that running full pretraining ablations may be computationally prohibitive within the rebuttal timeline, but additional clarification would be helpful. Could the authors share thoughts on the design choices and how they are expected to drive performance?\n\n2. The fixed 16 kHz input bandwidth excludes ultrasonic frequencies, yet the paper reports strong performance on marine mammals and bats. How does the model handle higher-frequency taxa such as dolphin or bat?\n\n3. Could you provide clarification or additional results showing how each architectural component affects performance? Specifically, how does windowing and noise in labels affect the final outcomes?\n\n4. The use of random vs. energy-peak 5 s windows is underexplained (section 2.1). What is the proportion of data processed by each method? What is their comparative effect on performance?\n\n5. Can you provide more details on how you handle normalization/merging of labels across different datasets ( Xeno-Canto, iNaturalist and the Tierstimmenarchiv)? \n\n6. Will the code be released upon acceptance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "90Y3tbCztC", "forum": "mgfNowaxYu", "replyto": "mgfNowaxYu", "signatures": ["ICLR.cc/2026/Conference/Submission4540/Reviewer_Xkp3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4540/Reviewer_Xkp3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992857930, "cdate": 1761992857930, "tmdate": 1762917429081, "mdate": 1762917429081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}