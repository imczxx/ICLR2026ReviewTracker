{"id": "qrMo6R7lOS", "number": 18052, "cdate": 1758283282597, "mdate": 1763544406859, "content": {"title": "ICaRus: Identical Cache Reuse for Efficient Multi-Model Inference", "abstract": "Multi model inference, where multiple task-specialized models collaborate to solve complex real-world problems, has recently emerged as a prominent paradigm, particularly in the development of agentic AI systems. However, in such scenarios, each model must maintain its own Key-Value (KV) cache for the identical prompt, leading to explosive memory consumption.\nThis explosive growth of KV caches forces LLM serving systems to evict previously stored caches, which in turn introduces significant recomputation overhead whenever the evicted caches are required again. \nMoreover, prefix caching is inherently infeasible across different models, forcing each model to recompute KV cache for the identical prompt, which leads to signficant overhead. To alleviate these issues, we propose Identical Cache Reuse (ICaRus), a novel architecture that allows multiple models to share identical KV caches across all layers. ICaRus is based on the key observation that a decoder-only Transformer can be conceptually decomposed into a logical encoder, which generates KV caches, and a logical decoder, which predicts output tokens from the KV caches. ICaRus fine-tunes only the logical decoder while freezing the logical encoder, enabling multiple models to share an identical KV cache. This eliminates cache memory explosion and unexpected evictions while also allowing cross-model reuse of KV caches for new input tokens, thereby removing redundant recomputation in multi model inference achieving both efficiency and scalability. Moreover, by incorporating lightweight adapters such as LoRA, ICaRus parallelizes KV cache generation and next-token prediction during decoding. ICaRus achieves comparable accuracy to task-specific fine-tuned model across a diverse set of tasks, while allowing multiple specialized models to fully share KV caches. ICaRus achieves up to $11.1\\times$ lower P95 latency and $3.8\\times$ higher throughput in multi agent scenarios with 8 different models, compared to prior multi model system.", "tldr": "We propose ICaRus, the first architecture that enables multi model to fully share KV caches across all layers, providing a principled solution to inefficiencies in conventional multi model serving.", "keywords": ["multi model inference", "agentic ai", "prefix caching", "fine-tuning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d940a61c80253377ca36a6a21dff7127918d761c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ICaRus, a novel architecture designed to optimize multi-model inference by enabling full Key-Value (KV) cache sharing across decoder-only Transformer models. The core innovation lies in conceptually decomposing a Transformer into a logical encoder (which generates KV caches) and a logical decoder (which predicts output tokens). By freezing the encoder and fine-tuning only the decoder using lightweight adapters (e.g., LoRA), ICaRus allows multiple specialized models to reuse identical KV caches, significantly reducing memory usage and recomputation overhead. The authors demonstrate that ICaRus achieves comparable accuracy to task-specific fine-tuned models while delivering up to 11.1× lower P95 latency and 3.8× higher throughput in multi-agent workflows."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The logical encoder-decoder decomposition is an interesting and insightful abstraction that enables cross-model KV cache reuse, a previously unsolved challenge.  \n* ICaRus shows substantial improvements in latency and throughput, especially in multi-agent workflows like ReAct and Reflexion, while surprisingly matches or even exceeds the performance of fully fine-tuned models in several benchmarks."}, "weaknesses": {"value": "* Limited scope of sharing. The proposed approach assumes problems are solved with multiple task-specialized models that **have different weights** and **are finetuned from the same base model**, such an assumption limits its applications. Is such a setting common? To my best knowledge, multi-agent systems typically use the same model for all roles, with only the system prompts customized for each.\n\n* Synthetic evaluation setup. The experimental setting for multi-model inference (Lines 318–320) relies on a synthesized workload that routes multi-turn requests in a round-robin fashion to different models. While this setup enables controlled benchmarking, it may not accurately reflect the dynamic and heterogeneous nature of real-world multi-agent systems, where agent interactions are often asynchronous, task-dependent, and influenced by external stimuli. This raises concerns about the generalizability of the reported latency and throughput improvements."}, "questions": {"value": "1. In Figure 3, does the decode phase update the **shared** KV Cache? If yes, does another role-specific decoder directly reuse the updated KV or it always requires the logical encoder (Figure 3(a)) to recompute the KV cache?\n\n2. Is ICaRus coupled with LoRA finetuning? Or it is compatible with other finetuning approaches (e.g., full parameter finetuning or prefix tuning)? Which adaptations are needed for other finetuning approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pag5O3Kk8z", "forum": "qrMo6R7lOS", "replyto": "qrMo6R7lOS", "signatures": ["ICLR.cc/2026/Conference/Submission18052/Reviewer_QNML"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18052/Reviewer_QNML"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760628117988, "cdate": 1760628117988, "tmdate": 1762927841734, "mdate": 1762927841734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To All Reviewers: Key Contributions of ICaRus"}, "comment": {"value": "We thank the reviewers for the insightful and constructive feedback.\nBefore addressing the detailed comments, we briefly emphasize the core value of ICaRus.\n\n---\n+ **A Paradigm Shift to Full Cross-Model KV Cache Sharing via a Logical Encoder–Decoder Architecture**\n\nTo the best of our knowledge, no existing architecture allows multiple models to fully share their KV caches, which has made collaboration between task-specialized expert models difficult and computationally expensive. **ICaRus is the first to enable multiple experts to fully share context without costly recomputation** by introducing the simple but powerful idea of logically decomposing a decoder-only Transformer into a shared logical encoder and task-specific logical decoders.\n\n---\n+ **Bridging Train and Serve: Training-Time KV Sharing for Robust Real-World Serving**\n\nICaRus explicitly **accounts for cross-model KV cache sharing already at training time** by freezing the logical encoder. This training strategy **not only enables high generation quality answer** even when multiple models fully share KV caches, **but also provides robustness in real-world serving**, since there is **no discrepancy between training and inference.** In contrast, prior recomputation-based approaches introduce a train–serve mismatch in how KV caches are handled, which makes their behavior riskier to rely on in production systems.\n\n---\n+ **Toward Scalable Multi Model Systems Beyond the KV-Cache Bottleneck**\n\nIn both academia and industry, systems are rapidly evolving from single-model setups to architectures in which multiple models are orchestrated together for complex tasks [1,2,3,4]. Several works have shown that orchestrating more expert models can lead to stronger overall capability [4, 5]. However, as the number of expert agents grows, efficiently managing each model’s KV cache has remained a major scalability bottleneck for multi-model systems. ICaRus addresses this limitation **by allowing the KV caches of even hundreds of agents to be managed within a single shared pool,** enabling a **qualitatively new regime of multi-model systems in which large numbers of specialized expert models can collaborate** to solve complex tasks that were previously out of reach for single-model or conventional multi-model architectures.\n\n---\n[1] Belcak et al., “Small Language Models Are the Future of Agentic AI”, arXiv, 2025\n\n[2] Spataro, “Introducing Microsoft 365 Copilot Tuning, Multi-Agent Orchestration, and More from Microsoft Build 2025”, Microsoft 365 Blog, 2025. \n\n[3] Shen et al., “Small LLMs Are Weak Tool Learners: A Multi-LLM Agent”, EMNLP 2024.\n\n[4] Subramaniam et al., “Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains”, ICLR 2025. \n\n[5] Kim et al., “The Cost of Dynamic Reasoning: Demystifying AI Agents and Test-Time Scaling from an AI Infrastructure Perspective”, arXiv, 2025."}}, "id": "vSRu8LSu4q", "forum": "qrMo6R7lOS", "replyto": "qrMo6R7lOS", "signatures": ["ICLR.cc/2026/Conference/Submission18052/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18052/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18052/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763536786180, "cdate": 1763536786180, "tmdate": 1763568216066, "mdate": 1763568216066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ICaRus introduces “Identical Cache Reuse” to let multiple task-specialized LLMs share the same KV cache for identical prompts, avoiding redundant memory use and recomputation in multi-model inference. It splits a decoder-only Transformer into a frozen logical encoder (that builds KV caches) and a fine-tuned logical decoder (that predicts tokens), so all models reuse one shared cache. Using lightweight adapters (e.g., LoRA), ICaRus achieves similar accuracy to task-specific fine-tuning but cuts P95 latency by up to 11× and boosts throughput up to 3.8× in multi-agent workflows."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Innovative idea that changes how multi-model share KV cache.\n2. Demonstrates good potential system improvement over previous methods by the new design.\n3. Promising path to efficient multi-model inference."}, "weaknesses": {"value": "1. Unclear accuracy comparison to workflows without any sharing or naive baselines. How does a base model perform if I do not use the new workflow?\n2. Training seems to be more complex. How robust is this training method? It will be more convincing to do more experiments on bigger models and more datasets."}, "questions": {"value": "1. How does the work deal with cases when agent finishes prefill and does some decoding? How to reuse the decode token for that part? Or how does system perform if that part is not used? Especially in long decoding scenarios like coding.\n\n2. How does the accuracy compare with not sharing at all? A simpler workflow without multi-model inference. Not sure if the dataset needs multi-model inference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "czv7OKtjLr", "forum": "qrMo6R7lOS", "replyto": "qrMo6R7lOS", "signatures": ["ICLR.cc/2026/Conference/Submission18052/Reviewer_eTzX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18052/Reviewer_eTzX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608071315, "cdate": 1761608071315, "tmdate": 1762927841361, "mdate": 1762927841361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ICaRus introduces “Identical Cache Reuse” to let multiple task-specialized LLMs share the same KV cache for identical prompts, avoiding redundant memory use and recomputation in multi-model inference. It splits a decoder-only Transformer into a frozen logical encoder (that builds KV caches) and a fine-tuned logical decoder (that predicts tokens), so all models reuse one shared cache. Using lightweight adapters (e.g., LoRA), ICaRus achieves similar accuracy to task-specific fine-tuning but cuts P95 latency by up to 11× and boosts throughput up to 3.8× in multi-agent workflows."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Innovative idea that changes how multi-model share KV cache.\n2. Demonstrates good potential system improvement over previous methods by the new design.\n3. Promising path to efficient multi-model inference."}, "weaknesses": {"value": "1. Unclear accuracy comparison to workflows without any sharing or naive baselines. How does a base model perform if I do not use the new workflow?\n2. Training seems to be more complex. How robust is this training method? It will be more convincing to do more experiments on bigger models and more datasets."}, "questions": {"value": "1. How does the work deal with cases when agent finishes prefill and does some decoding? How to reuse the decode token for that part? Or how does system perform if that part is not used? Especially in long decoding scenarios like coding.\n\n2. How does the accuracy compare with not sharing at all? A simpler workflow without multi-model inference. Not sure if the dataset needs multi-model inference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "czv7OKtjLr", "forum": "qrMo6R7lOS", "replyto": "qrMo6R7lOS", "signatures": ["ICLR.cc/2026/Conference/Submission18052/Reviewer_eTzX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18052/Reviewer_eTzX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608071315, "cdate": 1761608071315, "tmdate": 1763580377038, "mdate": 1763580377038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* The paper proposes a novel method to reuse KV Caches across different models to reduce redundant prefill operations. The method achieves up to 11.1× lower P95 latency and 3.8× higher throughput in agentic workflow across 8 different models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The method decouples the decoder-only model into a logical encoder and a token predictor, and freezes the encoder for fine-tuning. This makes the fine-tuning faster and more memory efficient. All LLMs also share the same logical encoder.\n* Instead of fine-tuning the entire decoder, lightweight adapters are used for fine-tuning for more efficiency to reduce the overhead of multiple KV accesses."}, "weaknesses": {"value": "* The paper mentions DroidSpeak as related work, which also performs sharing of the KV Cache across multiple LLMs, but points out that DroidSpeak requires some re-computation of sensitive layers. It would be useful to see a more direct comparison with DroidSpeak as a baseline and compare the cost of fine-tuning adapters in ICaRus vs selective re-computation in DroidSpeak.\n* The evaluation only considers a round-robin routing scheme, which deliberately increases the number of models being used in a session. It would be useful to see the evaluation under prefix-aware or KV-aware routing. Do the gains still hold in terms of the latency and throughput if reuse across models is not required?"}, "questions": {"value": "* In the round-robin evaluation setup, how many different models are used? I missed this detail and would encourage the authors to clarify.\n* In Figure 2, please clarify the insight as the training loss curves look almost identical and I am missing the takeaway."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G0bkZFjFPd", "forum": "qrMo6R7lOS", "replyto": "qrMo6R7lOS", "signatures": ["ICLR.cc/2026/Conference/Submission18052/Reviewer_1HaB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18052/Reviewer_1HaB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928325501, "cdate": 1761928325501, "tmdate": 1762927840900, "mdate": 1762927840900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The core idea of the work is to attempt to re-use the KV for the prefill phase, compared to the traditional LoRA based approach.The work instead finetunes a decoder style model to handle the sub tasks, using a light weight adapter."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. I think this is an interesting twist on traditional LoRA by choosing to change just the decoding side. \n2. provides a 11x decent speedup over the existing gpu cache system"}, "weaknesses": {"value": "1. I feel like the optimization provided by Icarus, while very impressive, feels a bit too incremental on existing LoRA systems.\n\n2. From Appendix A2.2, the experiments tested the case where each agent has it's own LoRA adapter. Due to arrival patterns and batching, It is unclear the amount of LoRA used per GPU might not be true in practice. \n\n3. The main alternatives the paper lists are either recompute or save. It is possible that swap is a valid strategy."}, "questions": {"value": "1. was swap space turned on for the experiments? Can all the KV cache be evicted/loaded back instead of recomputation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oNluIPWreP", "forum": "qrMo6R7lOS", "replyto": "qrMo6R7lOS", "signatures": ["ICLR.cc/2026/Conference/Submission18052/Reviewer_7YdR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18052/Reviewer_7YdR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762732381386, "cdate": 1762732381386, "tmdate": 1762927840090, "mdate": 1762927840090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}