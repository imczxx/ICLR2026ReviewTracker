{"id": "2rxgospB5s", "number": 20027, "cdate": 1758301666765, "mdate": 1759897005690, "content": {"title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought", "abstract": "Vision-language-action (VLA) models have emerged as the next-generation framework in robotics, integrating visual perception, language reasoning, and robotic control into unified systems. In this paper, we present \\textbf{dVLA}, a diffusion vision-language-action model with multimodal chain-of-thought. The dVLA optimizes visual reasoning, language comprehension, and robotic actions simultaneously through a unified diffusion-based objective. By harmonizing these modalities into a single cohesive framework, dVLA facilitates more effective cross-modal reasoning, enabling the model to generalize to novel instructions and objects. To ensure practical viability, we also integrate model acceleration methods that substantially decrease robot response times. Extensive evaluations in both simulation and the real world confirm that dVLA significantly outperforms current discrete and continuous VLA models, highlighting the potential of diffusion language model (DLM) based frameworks for robotics.", "tldr": "", "keywords": ["Vision-Language-Action Model", "Chain-of-Thought"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa217d992f9e187182d011d71a86b9e94dac122a.pdf", "supplementary_material": "/attachment/f85e0ab8d343baae142e8b56e3cb89ed0d0dd4ca.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces dVLA, a novel Vision-Language-Action (VLA) model built upon a discrete diffusion language model (DLM) backbone. The authors identify key limitations in existing VLA frameworks, namely the gradient conflicts arising from co-training with disparate objectives (e.g., for action prediction and vision-language understanding) and the architectural difficulty of integrating multimodal generation, such as visual Chain-of-Thought (CoT), into standard autoregressive models.  To address this, dVLA proposes a unified probabilistic framework where all modalities—including observations, language instructions, textual reasoning, visual subgoal images, and robot actions—are discretized into a shared token space and optimized jointly via a single diffusion-based denoising objective. The core contribution is a multimodal Chain-of-Thought training paradigm where the model is trained to reconstruct randomly masked tokens across all modalities, forcing it to learn consistent and physically grounded cross-modal representations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The core idea of unifying multimodal understanding and generation (including actions) under a single diffusion-based training objective is elegant. \n- The authors proactively address the primary drawback of diffusion models—inference latency—by incorporating and evaluating acceleration strategies. This shows a thoughtful consideration of the practical requirements for deploying robotic policies and strengthens the overall contribution.\n- The paper is easy to follow."}, "weaknesses": {"value": "- The multimodal CoT paradigm requires a complex data pipeline that involves generating visual subgoals (future frames) and textual reasoning (using a model like SEED-1.5VL). The paper could provide more details on the practicalities and potential costs or biases associated with this data generation process, which is a critical component of the overall framework.\n- The paper makes the strong claim that the model learns \"underlying physical laws\" because it can predict failure images. While a fascinating result, this could be an overstatement. The model may be learning to recognize and reconstruct common failure modes from the training data's statistics, rather than developing a true causal or physical understanding. This claim should be toned down or supported with more targeted evidence (e.g., generalization to novel failure types).\n- While the paper shows an example of predicting failure, it does not provide a broader analysis of the model's own failure modes. What are the common failure patterns of dVLA itself during evaluation? Does it fail on reasoning, visual grounding, or action precision? This would provide valuable insights for future work."}, "questions": {"value": "Please see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NROfXU4LWV", "forum": "2rxgospB5s", "replyto": "2rxgospB5s", "signatures": ["ICLR.cc/2026/Conference/Submission20027/Reviewer_pWAW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20027/Reviewer_pWAW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562910076, "cdate": 1761562910076, "tmdate": 1762932924913, "mdate": 1762932924913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces dVLA, a diffusion-based vision-language-action model that integrates multimodal chain-of-thought reasoning into robotic control. By unifying visual reasoning, textual understanding, and action prediction within a single diffusion framework, dVLA aims to address gradient conflicts and cross-modal inconsistency present in prior VLA systems. The model demonstrates strong results on the LIBERO benchmark and real-world manipulation tasks, with notable performance gains attributed to multimodal chain-of-thought reasoning and acceleration strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper proposes a unified diffusion-based objective for jointly optimizing vision, language, and action, which is a conceptually elegant and potentially impactful framework for robotic reasoning.\n2.\tThe authors conduct comprehensive experiments on both simulation (LIBERO) and real-world tasks, showing consistent improvements over several competitive baselines."}, "weaknesses": {"value": "1.\tA major weakness is that several recent works already integrate image/video generation and multimodal reasoning for robotic control, yet the paper fails to adequately discuss or compare against these, which undermines claims of novelty. Examples include:\n  1.  Zhu et al. Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets (RSS 2025)\n  2.  Li et al. Unified Video Action Model (RSS 2025)\n  3.  Cheang et al. GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation (Arxiv 2024)\n2.\tIt remains unclear whether the reported performance gains stem from the diffusion-based co-training of image generation or from the proposed multimodal chain-of-thought reasoning. Prior works have already demonstrated benefits from visual co-training, so an ablation or comparative analysis isolating these factors is needed.\n3.\tThe paper states that previous co-training approaches suffer from gradient conflicts but does not explicitly articulate how dVLA reconciles this issue beyond the unified objective. There is insufficient explanation of how knowledge preservation (e.g., for general VQA or world understanding) is maintained after robotics-specific fine-tuning.\n5.\tWhile the experiments are extensive, the evaluation scope focuses primarily on short to mid-horizon manipulation tasks. It would strengthen the paper to show that the proposed multimodal CoT scales to complex, compositional, or long-horizon planning scenarios.\n6.\tThe visual CoT examples, while compelling, appear anecdotal. More quantitative evaluation of reasoning correctness or alignment between predicted and executed subgoals would enhance the paper’s rigor."}, "questions": {"value": "1.\tThe paper could benefit from a clearer theoretical or empirical motivation for why diffusion modeling is particularly advantageous for multimodal alignment compared to autoregressive methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ia5KNAoZRv", "forum": "2rxgospB5s", "replyto": "2rxgospB5s", "signatures": ["ICLR.cc/2026/Conference/Submission20027/Reviewer_eZGi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20027/Reviewer_eZGi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624221494, "cdate": 1761624221494, "tmdate": 1762932924288, "mdate": 1762932924288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study VLAs, VLMs which have been adapted to output action data. They argue that typical co-training of VLAs tends to degrade either the general common sense knowledge or the ability to learn robot actions, with it difficult to learn both. Additionally, VLMs are able to do multimodal generation, an underutilized modality. The authors proposed training a VLA to predict subgoal images and subgoal text, given the current image and a high level language instruction. This is trained via masking tokens across all prediction modalities (image, text, action), training reconstruction from one modality to the others. This is called dVLA, since diffusion is used for predicting all modalities\n\nAdding subgoal image and text prediction increases the inference times of dVLA, so there is additional work on KV caching to improve the real world inference speed. Results are tested on a Franka robot."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It's been shown in other contexts that adding additional prediction tasks improves the amount of signal extractable from a data source, and so adding additional image and text prediction tasks before action prediction makes sense. The generated visual CoT images are interesting generations as well. The paper is clear on how many trials were done per eval, which is good to see. There are quite a few baselines. The inference improvements are the kind of useful work which is nice to see. Gains are seen in the hardest real-world task setup."}, "weaknesses": {"value": "As noted by the paper, this approach increases the inference time of the method, and although this is mitigated by the prefix attention + KV caching, these improvements could have also been added to the base model which does not do multimodal predictions. So although the paper shows improvement over not using visual CoT, it feels like there is a missing comparison to a larger base VLM with KV caching that does not predict intermediate subgoals.\n\nAt its core, the paper is about adding extra subgoal predictions on top of a FAST-based diffusion setup, and this just feels like a minor idea, given the past history of CoT in LLMs and subgoal prediction in robot learning, as well as the only minor improvements seen in LIBERO benchmark.\n\nSome typos (Vallina dVLA --> Vanilla dVLA)"}, "questions": {"value": "In Figure 2, are the visual and textual subgoals generated outputs of the model, or are they examples of what subgoals could be? It was not clear to me if \"illustrative example\" meant \"is an actual generation\", especially given some of the artifacting in the generated images of Figure 4.\n\nIf there are videos of real robot execution, that would also be appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B9TMRHyRT5", "forum": "2rxgospB5s", "replyto": "2rxgospB5s", "signatures": ["ICLR.cc/2026/Conference/Submission20027/Reviewer_qBYf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20027/Reviewer_qBYf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866067852, "cdate": 1761866067852, "tmdate": 1762932923603, "mdate": 1762932923603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces dVLA, a novel vision-language-action (VLA) framework for robotic control built upon a discrete diffusion language model (DLM). The central contribution is the use of a **unified diffusion-based objective** to jointly optimize visual reasoning, language comprehension, and robotic action generation. The authors frame this as a solution to the gradient conflicts  and architectural gaps  that arise in traditional auto-regressive models, especially when trying to integrate generative capabilities like visual \"Chain-of-Thought\" (CoT).\n\nThe dVLA model discretizes all input and output modalities—including current images, text instructions, robot state, future subgoal images (visual CoT), textual reasoning (textual CoT), and action chunks—into a unified token sequence. It is then trained with a discrete diffusion objective to reconstruct randomly masked tokens from any modality, given the unmasked ones. The authors demonstrate through experiments in both simulation (LIBERO)  and the real world (Franka robot)  that this multimodal CoT approach significantly outperforms existing VLA models. To address the inherent latency of diffusion models, the paper also introduces acceleration strategies, combining a prefix attention mask with KV caching to achieve a ~2x speedup for practical deployment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Novel and Sound Methodology:** The core idea of using a unified discrete diffusion objective to harmonize perception, reasoning (both visual and textual), and action is elegant and well-motivated. It directly addresses a clear and significant limitation in prior work: the difficulty of integrating generative reasoning (like predicting future images) with action prediction in a single, non-conflicting framework. Building this upon a discrete diffusion model (MMaDA)  and employing discrete tokenization for all modalities (including actions via FAST ) is a technically sound and modern approach.\n\n2.  **Strong Empirical Evaluation:** The paper provides a comprehensive set of experiments that validate the proposed method. dVLA achieves state-of-the-art (SOTA) performance on the LIBERO benchmark, outperforming a wide range of both continuous and discrete action policies. Furthermore, it demonstrates superior performance in a multi-task, real-world setting with four distinct tasks, showing the highest average success rate (65%) compared to all baselines.\n\n3.  **Compelling Ablation Study:** The authors effectively isolate the impact of their primary contribution, the multimodal CoT. The comparison between the full `dVLA` and \"Vanilla dVLA\" (which lacks the explicit CoT)  shows a substantial performance improvement: a +6.6% average success rate on LIBERO  and a +12.5% improvement in the real-world tasks. This provides strong evidence that the joint generation of visual and textual subgoals is a key driver of the model's success.\n\n4.  **Focus on Practicality (Acceleration):** The paper commendably addresses the primary practical drawback of using diffusion models for robotics: slow inference speed. The authors propose and evaluate two acceleration strategies (Prefix Attention Mask and KV Caching) , demonstrating a ~2x inference speedup with only a marginal drop in task performance. This is a crucial step for making the method viable in real-time settings."}, "weaknesses": {"value": "1.  **Unclear Explanation of Acceleration Method:** The description of the \"Prefix Attention Mask\" in Section 3.4 is confusing and difficult to follow. The text describes a \"blockwise causal attention mask with two blocks\"  and \"full bidirectional attention within each block\" , but then states \"tokens in one block restricted from attending to tokens in subsequent blocks\". This description is ambiguous. For the KV caching of the prefix (Block 1) to be useful, the generated tokens (Block 2) must be able to *attend to* the prefix tokens. The diagram in Figure 1 appears to show a standard prefix-LM, but the text description does not clearly align with this. This methodological detail is critical and needs to be clarified.\n\n2.  **Marginal SOTA and Baseline Confusion on LIBERO:** While dVLA achieves SOTA on LIBERO (96.4%) , the margin over the \"Discrete Diffusion VLA\" baseline (96.3%)  is only 0.1%. This is a razor-thin difference and suggests that the core benefit might come from using *any* discrete diffusion VLA, rather than the paper's specific multimodal CoT contribution. This is confounded by the \"Vanilla dVLA\" baseline (89.8% SR). It is not clear what the difference is between \"Vanilla dVLA\"  and \"Discrete Diffusion VLA\". Both appear to be discrete diffusion VLAs without the multimodal CoT, yet their performance is drastically different. The paper needs to explicitly define these baselines and directly address why the multimodal CoT provides only a 0.1% boost over a seemingly similar SOTA diffusion model, even though it provides a 6.6% boost over the \"Vanilla dVLA\" ablation.\n\n3.  **Contradictory Information on Textual CoT Usage:** There appears to be a contradiction regarding the use of textual CoT. In Section 3.5, the paper states, \"For simpler tasks, we omit language reasoning to further speed up inference\". However, Table 1 (LIBERO results) clearly shows `dVLA` using *both* textual and visual CoT (it has a \"✓\" in both columns). This implies the LIBERO tasks were *not* considered \"simpler tasks.\" This should be clarified: was textual CoT used for all LIBERO tasks? If so, how was this reasoning data generated, given that the SEED-1.5VL method was described as \"designed for long-horizon tasks such as bin picking\"?\n\n4.  **Limited Scale of Real-World Evaluation:** While the inclusion of real-world experiments is a significant strength, the evaluation scale is limited (10 trials per task). With these few trials, the 65% success rate (26/40) for `dVLA`  is promising but may not be statistically robust. For example, the 7/10 success on \"Bin Picking\" and \"Hang Cups\"  is good, but more trials would be needed to build stronger confidence in the method's reliability compared to baselines."}, "questions": {"value": "1.  **Prefix Mask Clarification:** Could you please provide a clearer explanation of the \"Prefix Attention Mask\" (Sec 3.4)? Does Block 2 ($[o_{\\mathfrak{g}oal},r,a_{chunk}]$)  attend to Block 1 ($[o,l, s]$)? If so, how should we interpret the text \"tokens in one block restricted from attending to tokens in subsequent blocks\"?\n\n2.  **Baseline Comparison (Vanilla dVLA vs. Discrete Diffusion VLA):** What is the exact architectural and training objective difference between your \"Vanilla dVLA\" baseline (89.8% LIBERO SR)  and the \"Discrete Diffusion VLA\" (Liang et al., 2025) baseline (96.3% LIBERO SR)? Why does the latter perform so much better, and why is the margin between your full `dVLA` (96.4%)  and \"Discrete Diffusion VLA\" only 0.1%?\n\n3.  **Textual CoT Usage on LIBERO:** Can you confirm if textual CoT was used for the LIBERO benchmark results in Table 1? If so, how does this align with the statement that language reasoning is omitted for \"simpler tasks\", and how was this textual reasoning data generated for the LIBERO dataset?\n\n4.  **Inference Procedure:** During inference, are the visual CoT, textual CoT, and action tokens all generated *simultaneously* in one parallel denoising process (as suggested by Fig 1 and the unified objective)? Or is the process sequential (as implied by Sec 3.3, \"generates two parallel outputs... Subsequently, dVLA grounds these... to produce a concrete and executable action\" )?\n\nI am open to raising the score if above concerns could be well explained and addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SGNKBFFgkx", "forum": "2rxgospB5s", "replyto": "2rxgospB5s", "signatures": ["ICLR.cc/2026/Conference/Submission20027/Reviewer_eGfQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20027/Reviewer_eGfQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926566945, "cdate": 1761926566945, "tmdate": 1762932923212, "mdate": 1762932923212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}