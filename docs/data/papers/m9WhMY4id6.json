{"id": "m9WhMY4id6", "number": 13949, "cdate": 1758225726449, "mdate": 1759897400706, "content": {"title": "Diffusion Modulation via Environment Mechanism Modeling for Planning", "abstract": "Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.", "tldr": "", "keywords": ["Reinforcement Learning", "Diffusion-based Reinforcement Learning", "Offline Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/884c523403141d44253d25795c04764825812007.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The author proposes DMEMM, a approach for trajectory generation in offline reinforcement learning which explicitly enforce consistency with the underlying environment dynamics. DMEMM integrates transition dynamics and reward functions into diffusion model training through three main components (a reward-aware diffusion loss, a transition-based auxiliary loss and a reward-based auxiliary loss). Experiments on D4RL locomotion and Maze2D tasks demonstrate effectiveness of DMEMM."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Experimental results demonstrate that DMEMM performs well on D4RL tasks.\n2. The motivation is highly intuitive and reasonable."}, "weaknesses": {"value": "1. The experiments rely solely on the D4RL benchmark, which has become saturated for many locomotion and maze-solving tasks. Evaluating the method on more recent or challenging benchmarks (e.g., OGBench) would better demonstrate its scalability and robustness.\n2. The strong results are demonstrated on tasks with relatively low-dimensional state and action spaces. A key component of DMEMM is the explicit transition model which is known to be challenging to scale to high-dimensional state or action spaces (e.g., OGBench humanoid maze). This raises a critical question about the method's applicability to these more complex domains.\n3. The evaluation is purely quantitative. The paper would be significantly strengthened by including visualizations that compare trajectories generated by DMEMM against baselines.\n4. Although this paper focuses on training-time dynamic consistency improvement, I suggest the authors include more related works on inference-time improvement methods. Here are some relevant papers:\n\n[1] SafeDiffuser: Safe Planning with Diffusion Probabilistic Models\n\n[2] Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic Detection of Infeasible Plans\n\n[3] Resisting stochastic risks in diffusion planners with the trajectory aggregation tree\n\n[4] Inference-Time Policy Steering through Human Interactions\n\n[5] Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lfE5Ee2EK4", "forum": "m9WhMY4id6", "replyto": "m9WhMY4id6", "signatures": ["ICLR.cc/2026/Conference/Submission13949/Reviewer_vn9X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13949/Reviewer_vn9X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760872071019, "cdate": 1760872071019, "tmdate": 1762924452686, "mdate": 1762924452686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose DMEMM, a diffuser-style offline RL planning algorithm with better consistency.\nIt improve the generated trajectory’s feasibility and optimality by adding extra transition dynamics and reward functions as loss term.\nwith above design, the paper outperforms baselines like diffuser, hd-da."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- the idea is simple and practice, can be applied to any diffusion framework\n- show reasonable performance gain compared to vanilla diffuser with detailed benchmark"}, "weaknesses": {"value": "- most of evaluated task is still low-dimensional and image-based task or real world evaluation is missing.\n    \n- the theoretical contribution is weak since it mainly restate diffusion reparameterization trick.\n    \n- in evaluation against other methods, the performance gain with extra design is relative small (around 3 point and sometimes no improvement)."}, "questions": {"value": "- since the dataset trajectory is already feasible, is transition loss redundant?\n    \n- can author explains why add transition loss can improve out of distribution dynamical feasibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zO1kXfm0do", "forum": "m9WhMY4id6", "replyto": "m9WhMY4id6", "signatures": ["ICLR.cc/2026/Conference/Submission13949/Reviewer_TDtu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13949/Reviewer_TDtu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704847363, "cdate": 1761704847363, "tmdate": 1762924451850, "mdate": 1762924451850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DMEMM is a diffusion-based offline RL planner that injects learned transition and reward models into both training (via modulation losses) and sampling (via dual guidance). The goal is to align denoising with MDP structure, improving transition consistency and reward alignment. Experiments on D4RL locomotion and Maze2D show consistent gains over strong baselines, with ablations attributing improvements to each component."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: Principled integration of transition-consistency and reward into loss and sampler; clear re-parameterization enabling modulation and guidance.\n- Quality: Consistent improvements on D4RL/Maze2D; ablations isolate the effect of weighting, transition/reward modulation, and guidance.\n- Clarity: Algorithms, objectives, and implementation details are well specified."}, "weaknesses": {"value": "- No manipulation or real-robot validation. The evaluation focuses on locomotion and 2D navigation; there are no manipulation-style benchmarks (e.g., Simpler, LIBERO etc.) and no on-hardware experiments. This leaves open questions about **complex dynamics**, **sim-to-real transfer**, and control latency.\n\n- Benchmark coverage & SOTA baselines. The study omits common diffusion manipulation baselines (e.g., Diffusion-policy-style chunked planners in manipulation, i.e. Diffusion Policy, MetaDiffuser, and LDGC) for comparisons. \n\n- On large mazes, the paper itself notes that hierarchical HD-DA outperforms DMEMM, suggesting challenges for long-horizon structure that are not yet addressed by the method.\n\n- Planning efficiency not quantified. The paper specifies reverse-diffusion steps and hardware, but does not report wall-clock planning frequency (Hz) or end-to-end latency, which is essential for deployment and for fair comparison to chunk-based diffusion planners."}, "questions": {"value": "1. **Real-robot feasibility.** Can you provide at least one on-hardware result (e.g., a simple pick-and-place on a Franka or UR arm) to demonstrate that DMEMM’s planning latency and stability suffice for real control loops?\n\n2. **Manipulation benchmarks.** Please add manipulation tasks in simulation (e.g., Simpler, LIBERO) and compare to strong diffusion planners tailored to manipulation. This is essential to support claims of broad applicability beyond locomotion/navigation.\n\n3. **Planning efficiency.** What is the per-step planning frequency (Hz) end-to-end (mean/median, with variance) under your reported settings (e.g., N=20 reverse steps)? How does it scale with horizon T, and how does it compare to chunk-based diffusion planners under matched hardware?\n\n4. **Long-horizon structure.** Large-maze results indicate gaps vs. hierarchical methods. Could you integrate subgoal/hierarchical decomposition or curriculum-guided horizons into DMEMM’s modulation framework to recover competitiveness on long-horizon tasks? \n\n5. **Role of learned models.** How sensitive is performance to the fidelity of **T** and **R**? Please show ablations that (i) vary their capacity/training data, (ii) introduce synthetic noise/mismatch, and (iii) report how modulation/guidance weights respond."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QOiH2WRH9e", "forum": "m9WhMY4id6", "replyto": "m9WhMY4id6", "signatures": ["ICLR.cc/2026/Conference/Submission13949/Reviewer_pnAw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13949/Reviewer_pnAw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740172001, "cdate": 1761740172001, "tmdate": 1762924451363, "mdate": 1762924451363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Diffusion is the main planning method in robotics. However, it is often applied without additional domain specific knowledge about what the input representation actually means, namely states and actions.  The authors introduce 3 additional loss terms that help in training diffusion systems such that when sampling at test time, you get superior trajectories with better rewards. This is achieved by first training transition and reward models.  These are then used to  introduce additional loss terms to guide the diffusion model training.  These three terms encourage:\n1. The state and action transitions to accurately reflect the learn transition model\n2. The resulting sampled trajectories to have a high reward, as measured by the learnt reward model\n3. Prioritize fitting trajectories that achieved a high reward.\n\nIn addition to the extra signals used at training time, they also introduce a guidance signal that guides the diffusion towards system both a high reward, but also towards likely transitions.  Both of these are again measured by the learnt transition and reward models.  \n\nThey then demonstrate that this approach yields state of the art performance on all tasks on D4RL and Walker2D.  For Maze2D, at the largest matched by a hierarchical diffusion planner but otherwise get the best performance.\n\nThey also show via ablation experiments that each part of the process above helps overall performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "## Originality\nThis is (to my knowledge) a novel fusion of the transition and reward model into the planning and guidance parts of the diffusion process.  \n\n## Quality\nThe aims of the research are clearly laid out, and the results are empirically validated.  Ablations provide evidence that each of the changes that they made were necessary.  \n\n## Clarity\nThe paper is clearly written, and could be reproduced from the descriptions provided.\n\n## Significance\nDiffusion is a workhorse of robotics control, and better training methods that result in higher success rates could lead to significant improvements in robotics applications."}, "weaknesses": {"value": "More experiments involving more difficult simulation environments like https://github.com/google-deepmind/aloha_sim would make a stronger case that their method is general enough to help with general robot control."}, "questions": {"value": "Have you noticed any pathological behavior if you mis-set the weightings between the different loss functions during training?  E.g., if the reward is too high, does it start to break the transition model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4hJ0Y7deHz", "forum": "m9WhMY4id6", "replyto": "m9WhMY4id6", "signatures": ["ICLR.cc/2026/Conference/Submission13949/Reviewer_cGZS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13949/Reviewer_cGZS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948343140, "cdate": 1761948343140, "tmdate": 1762924450421, "mdate": 1762924450421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}