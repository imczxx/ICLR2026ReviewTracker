{"id": "IAj49Jwc9l", "number": 17362, "cdate": 1758275052341, "mdate": 1759897179932, "content": {"title": "LS-CLIP: Autoencoder-Based Mining of CLIP's Inherent Local Semantics in Cross-Domain Image Retrieval", "abstract": "Contrastive Language-Image Pretraining (CLIP) excels in cross-domain image retrieval. However, existing methods often depend on extensive manual annotations for local supervision and neglect CLIP's native local-semantic capabilities. To address these problems, we propose an autoencoder-based approach named LS-CLIP, which is designed to mine local semantics in CLIP and realize cross-domain feature alignment.  First, we design a self-supervised Semantic Reconstruction Module (SRM) for local feature mining. Reconstructing the patch features of the Vision Transformer (ViT), SRM integrates global and local semantic perception, enabling it to adapt to retrieval tasks of different granularities. Second, we introduce Feature Moment Transfer (FMT). Through the reconstruction of cross-domain features via moment transfer, the stability of the feature space is enhanced. In addition, this module incorporates noise to reconstruct the data distribution, thereby improving the model's generalization ability. To accommodate diverse retrieval intents, we construct a dataset with rich textual descriptions and a wide range of scenarios, named CDIR-Flickr30k. Extensive experiments demonstrate that LS-CLIP significantly outperforms state-of-the-art baseline models in various metrics. Zero-shot evaluation confirms its strong generalization capability. Importantly, LS-CLIP can be applied as a plug-and-play model to CLIP variants, consistently delivering performance improvements.", "tldr": "LS-CLIP improves CLIP's performance on specialized tasks by using an autoencoder approach and feature moment transfer to mine local semantic information and enhance the model's generalization ability.", "keywords": ["Cross-Domain Image Retrieval; CLIP; Autoencoder; Local Semantics; Pluggable"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/efa931240b108355b1a15358380cee1c2632f10b.pdf", "supplementary_material": "/attachment/16b621c90f915a24164ba6a09ec06a56ec082074.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces LS-CLIP, a lightweight autoencoder-based module designed to mine local semantics from CLIP's patch features for Cross-Domain Image Retrieval (CDIR). The method comprises two main components: a Semantic Reconstruction Module (SRM) to reconstruct patch features for local detail mining and a Feature Moment Transfer (FMT) mechanism, inspired by style transfer, to enhance feature stability and generalization. The authors also contribute a new synthetic CDIR benchmark, CDIR-Flickr30k, which includes an object-to-image retrieval task ."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Plug-and-Play Efficacy**: The proposed module is lightweight, adding only 2M parameters , and demonstrates consistent performance improvements as a plug-and-play addition to various baselines (CLIP, FG-CLIP, Siglip2) .\n2. **Resource Contribution**: The new CDIR-Flickr30k dataset introduces a novel object-to-image retrieval task, providing a new resource for evaluating multi-granularity visual retrieval."}, "weaknesses": {"value": "1. **Limited Novelty**: The methodological contribution is incremental. The SRM is a standard autoencoder applied to patch features, conceptually similar to MAE , while the FMT module is a direct adaptation of the AdaIN mechanism from style transfer, used here as a regularizer.\n2. **Weak Justification for FMT**: The paper posits that the complex FMT perturbation enhances generalization. However, it lacks a clear justification for why this specific moment-transfer operation is superior to simpler and more common feature-space regularization techniques (e.g., additive noise).\n3. **Synthetic Benchmark Concerns**: All cross-domain queries in the CDIR-Flickr30k dataset are generated by other models (e.g., Pidinet, AnimateDiff, Grounding DINO). This raises a significant concern that the model is learning to invert the specific artifacts of these generators rather than learning a general, human-perceived understanding of cross-domain semantics."}, "questions": {"value": "1. What is the clear justification for using the complex, style-transfer-based FMT mechanism for regularization over simpler, more established methods like dropout or noise injection in the feature space?\n\n2. To properly ablate the method, can the authors provide results for a Base + FMT configuration (i.e., without the SRM module)?\n\n3. How can the authors validate that the model's strong performance on the fully synthetic CDIR-Flickr30k benchmark will translate to real-world, human-generated cross-domain data, and is not just an artifact of learning to invert the data generators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FXljTYmeH1", "forum": "IAj49Jwc9l", "replyto": "IAj49Jwc9l", "signatures": ["ICLR.cc/2026/Conference/Submission17362/Reviewer_mWzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17362/Reviewer_mWzY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760864659663, "cdate": 1760864659663, "tmdate": 1762927276310, "mdate": 1762927276310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LS-CLIP, a lightweight and plug-in local semantic mining framework built on CLIP for cross-domain image retrieval (CDIR) tasks such as sketch-to-image, cartoon-to-image, and mosaic-to-image retrieval."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Achieves substantial performance improvements in challenging cross-domain settings (e.g., sketch/photo, mosaic/photo) with minimal computational cost, advancing practical retrieval applications.\n\n- Contribution breadth: Provides not only an effective method but also a new dataset (CDIR-Flickr30k) that broadens evaluation coverage and supports future research in multi-style retrieval"}, "weaknesses": {"value": "## General ##\nThe paper suffers from major issues in writing and presentation. Many claims are incorrect, insufficiently supported, ambiguous, or irrelevant. Below, I outline several specific examples illustrating these problems. In its current form, the paper is not yet suitable for publication, particularly not at a top-tier venue such as ICLR:\n\n- Abstract clarity: The abstract is unclear and should be rephrased. It fails to convey the core ideas and goals of the paper, relying instead on vague technical statements. For example, phrases like \"Through reconstructing the patch features of the Vision Transformer (ViT), SRM integrates global and local semantic information…\" are too abstract and do not help the reader understand the module’s purpose. Similarly, the mention of \"Feature Moment Transfer (FMT)\" and \"stability of the feature space\" is confusing - it is not clear what is being reconstructed, what \"moment transfer\" means, or how it contributes to feature stability. Overall, the abstract leaves the reader more confused than informed.\n\n- Introduction – unsupported claim: The statement \"...among which Query-Based Image Retrieval (QBIR) is the most widely applied\" is misleading and lacks evidence. The authors should provide a reliable reference to support this claim. As far as is generally known, text-to-image search (e.g., Google Images) has been the most widely used Image Retrieval form for many years, more than image-to-image search. This needs clarification or revision.\n\n- Figure 1 explanation: the figure is poorly explained. CLIP does not include a \"cross-attention\" module but rather self-attention, so it is unclear what exactly is being visualized. Are the authors showing attention maps between the CLS token and patch tokens? What visualization method or tool was used? The mention of \"attention weights\" in line 91 adds to the confusion and requires clearer explanation.\n\n- Figure 1 caption: The caption is ambiguous due to the use of green, yellow, and red colors both for frame borders and in the heatmaps. The color references should be clarified to avoid misinterpretation.\n\n- Line 84 – missing context: The sentence \"we designed an autoencoder-based adapter named Semantic Reconstruction Module (SRM) based on MAE He et al. (2022)\" is insufficient. The authors should briefly explain what MAE (Masked Autoencoder) contributed or how it relates to their method, rather than simply citing it without context.\n\n- Line 92 – unsupported statement: The claim \"However, existing benchmarks are insufficient\" needs stronger justification. The provided explanation that \"FSCOCO only supports text or sketch queries\" is not convincing, as FSCOCO therefore was not intended for cross-domain image-to-image retrieval. Likewise, stating that \"FreestyleRet has low-quality images\" is not an adequate reason. The authors should discuss other relevant datasets, such as \"Sketchy\" [1], and explain more concretely why current benchmarks fail to meet their goals.\n\n- Conceptual inconsistency: The authors contradict themselves: at line 37, they define CDIR as \"finding relevant images in one visual domain based on query images from another visual domain\", but later (line 132), they describe \"image-text datasets as the foundation of VLM-based CDIR\". These two definitions are inconsistent and need reconciliation.\n\n- Line 133 – inaccurate phrasing: The sentence \"Datasets like LAION, COCO, and Flickr30K focus on contrastive text-image learning in the same domain, lacking support for cross-domain query types\" is conceptually incorrect. Datasets do not \"focus\" on learning methods - they only provide data. Furthermore, the phrase \"lacking support for cross-domain query types\" is unclear, as these datasets are inherently image–text paired datasets, not designed image-to-image retrieval (or for object detection, for example). The authors should refine this statement to accurately describe the datasets’ limitations.\n\n## Evaluation ##\n- Limited dataset validation: The authors claim improvements in cross-domain retrieval tasks, yet most of their evaluation is conducted only on the proposed CDIR-Flickr30k dataset. To convincingly demonstrate the method’s general effectiveness, results should be reported on additional, well-established cross-domain benchmarks. Relying solely on a self-introduced dataset limits the credibility and generalizability of the findings.\n- Missing key baseline: The paper frames the main task as image-to-image retrieval, yet uses CLIP, a model primarily designed for image–text alignment, as its core backbone. This choice is questionable. Models such as DINOv2, which are widely adopted for image-to-image tasks and often outperform text-aligned models in pure visual retrieval, represent a critical missing baseline. Including DINOv2 or similar self-supervised vision models would provide a fairer and more meaningful comparison.\n\n- Unclear evaluation focus: In Section 4.3.2, the authors present zero-shot text–image retrieval results. However, the primary goal of this work is cross-domain image-to-image retrieval. It remains unclear whether LS-CLIP improves general image retrieval performance, or if its gains are confined to specific setups involving CLIP. The authors should clarify whether their approach enhances retrieval in general, or only when coupled with CLIP for cross-domain adaptation.\n\n\n[1] Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James Hays. 2016. The sketchy database: learning to retrieve badly drawn bunnies. ACM Trans. Graph. 35, 4, Article 119 (July 2016), 12 pages."}, "questions": {"value": "See \"weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vy8ZX20wqV", "forum": "IAj49Jwc9l", "replyto": "IAj49Jwc9l", "signatures": ["ICLR.cc/2026/Conference/Submission17362/Reviewer_oaby"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17362/Reviewer_oaby"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837094971, "cdate": 1761837094971, "tmdate": 1762927274993, "mdate": 1762927274993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a lightweight framework that enhances CLIP’s local semantic understanding for cross-domain image retrieval. It proposes two modules: a Semantic Reconstruction Module (SRM) that mines local patch features through self-supervised reconstruction, and Feature Moment Transfer (FMT) that improves feature stability via cross-domain moment alignment. To better evaluate multi-granularity retrieval, the authors also build a new dataset, CDIR-Flickr30k, containing diverse query types such as text, sketch, and object images. Experiments show that LS-CLIP consistently improves performance and generalization across multiple CLIP variants while maintaining low computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies an important limitation of CLIP—its weak handling of local semantics—and attempts to address it in the context of cross-domain retrieval.\n2. The proposed method (LS-CLIP) is lightweight and can be integrated into existing CLIP models without significant computational overhead.\n3. The newly constructed CDIR-Flickr30k dataset broadens the evaluation perspective by including diverse query modalities such as sketch and object images.\n4. Experimental results are clearly presented and demonstrate consistent, if moderate, improvements across multiple CLIP variants and datasets."}, "weaknesses": {"value": "1. The overall contribution is incremental rather than fundamental; the method primarily refines feature representation without introducing new theoretical insights or a strong methodological innovation.\n2. The technical novelty is limited, both the autoencoder reconstruction and moment alignment components are standard techniques reused with minimal adaptation.\n3. The proposed CDIR-Flickr30k dataset lacks detailed description, justification, and validation, making it unclear whether it meaningfully challenges existing benchmarks.\n4. The paper focuses heavily on quantitative results but lacks qualitative or interpretive analysis to demonstrate why LS-CLIP captures “local semantics” more effectively.\n5. There is no convincing discussion of practical significance; the improvement margins are small and do not clearly justify the complexity of the additional modules.\n6. The writing sometimes overstates the contribution, giving the impression of conceptual novelty where the actual advance is modest."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iPFnb7kQrt", "forum": "IAj49Jwc9l", "replyto": "IAj49Jwc9l", "signatures": ["ICLR.cc/2026/Conference/Submission17362/Reviewer_1qJo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17362/Reviewer_1qJo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997003935, "cdate": 1761997003935, "tmdate": 1762927274322, "mdate": 1762927274322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight framework to enhance local semantic understanding of CLIP for cross-domain image retrieval. Two modules are proposed: the Semantic Reconstruction Module (SRM) mines local block features through self-supervised reconstruction, and the Feature Moment Transfer Module (FMT) improves feature stability through cross-domain moment alignment. To better evaluate multi-granularity retrieval, the authors also built a new dataset, CDIR-Flickr 30k, that includes multiple query types such as text, sketches, and object images. Experiments have shown that LS-CLIP continues to improve the performance and generalization of multiple CLIP variants while maintaining low computing costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper identifies an important limitation of CLIP: its handling of local semantics is weak, and it attempts to address this issue in the context of cross-domain retrieval.\n\n2. The proposed method LS-CLIP is lightweight and can be integrated into existing CLIP models without incurring significant computational overhead.\n\n3. The newly constructed CDIR-Flickr30k dataset broadens the evaluation perspective by including multiple query modalities such as sketches and object images."}, "weaknesses": {"value": "1. The method primarily refines feature representation without introducing new theoretical insights. The autoencoder reconstruction and moment alignment components are standard techniques reused with minimal adaptation.\n2. The paper could give more qualitative or interpretive analysis to demonstrate why LS-CLIP captures “local semantics” more effectively.\n3. Some experiments show that adding the proposed module actually leads to worse performance. For example, FG-CLIP and FG-CLIP+Ours in Table 1.\n4. Some statements overstate the contribution, e.g., “providing new insights for practical CDIR applications\"..., which could be more modest."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iPFnb7kQrt", "forum": "IAj49Jwc9l", "replyto": "IAj49Jwc9l", "signatures": ["ICLR.cc/2026/Conference/Submission17362/Reviewer_1qJo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17362/Reviewer_1qJo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997003935, "cdate": 1761997003935, "tmdate": 1763646098606, "mdate": 1763646098606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}