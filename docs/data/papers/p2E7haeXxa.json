{"id": "p2E7haeXxa", "number": 16625, "cdate": 1758266917086, "mdate": 1759897228843, "content": {"title": "OVRD: Open-Vocabulary Relation DINO with Text-guided Salient Query Selection", "abstract": "Open-Vocabulary Detection (OVD) trains on base categories and generalizes to novel categories with the aid of text embeddings from Vision-Language Models (VLMs).\nHowever, existing methods are insufficient in utilizing semantic cues from the text embeddings to guide visual perception, which hinders the performance of zero-shot object detection.\nIn this paper, we propose OVRD, an Open-Vocabulary Relation DINO with text-guided salient selections.\nSpecifically, we introduce text-guided salient query selection to choose image features most relevant to the text embeddings, along with their \ncorresponding reference points and masks, thereby providing additional semantic cues for guiding visual perception. \nBuilding upon this, the salient reference points are used to recover the relative spatial structure of the selected features, \nenhancing positional awareness in the salient transformer decoder. \nMoreover, to fully leverage both the semantic cues and the recovered spatial structure, we develop a self-attention model of semantic relationships to model sparse semantic relations in OVD scenarios to further guide visual perception.\nWe evaluate OVRD on public benchmarks in a zero-shot setting, achieving 37.0 AP on LVIS Minival, which performs favorably against the state-of-the-art methods.\nThe code is available at https://anonymous.4open.science/r/OVRD.", "tldr": "An open-vocabulary object detection model to explore relation modeling in open-vocabulary scenarios while enhancing multi-modal fusion through text-guided salient query selection.", "keywords": ["Multi-modal Learning", "Open-Vocabulary", "Object Detection"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/234872f67c649a12da4ad53ea33f8b48d59a3394.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Open Vocabulary Relation DINO (OVRD) with a text-guided salient query selection to choose image features most relevant to the text embeddings. OVRD captures the symmetric and fully-connected semantic relations with the aid of text-aware soft-mapping, and also models the directional and sparse relations to guide multimodal fusion and improve zero-shot detection performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, the manuscript is well-organized and clearly written."}, "weaknesses": {"value": "In the Introduction, the authors claim that recent methods [Cheng et al. (2024); Du et al. (2024); Wang et al. (2024)] insufficiently leverage semantic cues for guiding visual perception, and thus propose a text-guided mechanism as an enhancement. However, this argument lacks clarity and persuasiveness, as the cited works themselves fundamentally rely on text-guided or vision-language fusion strategies. The authors should provide a more precise and technically meaningful differentiation. Specifically, it is essential to clarify: What is the key limitation shared by the fusion strategies in [Cheng et al. (2024); Duetal. (2024); Wang et al. (2024)] that the proposed method overcomes?\n\nThe mathematical symbols in the manuscript need rigorous typesetting to enhance clarity and avoid ambiguity. It is common to use different fonts to distinguish scalars, vectors, and matrices. For instance, in the expression “I∈R^(H×W×3)”, I represents a matrix, while H and W are scalar dimensions.\n\nPlease provide visualizations to interpret the “Text-guided Salient Query Selection” module. Specifically, how do the selected salient regions correlate with the input text prompts? Can you also show a case where this process correctly identifies a novel object that a baseline method misses? Such evidence is key to validating the module's contribution."}, "questions": {"value": "The mathematical symbols in the manuscript need rigorous typesetting to enhance clarity and avoid ambiguity. It is common to use different fonts to distinguish scalars, vectors, and matrices. For instance, in the expression “I∈R^(H×W×3)”, I represents a matrix, while H and W are scalar dimensions.\n\nThe pipeline in Figure 2 (b) raises two questions: What is the source of the “Points” and “Masks”? Are they generated by an external model or an upstream process?  There is logical redundancy: if the exact mask has already been obtained, the bounding box can be directly obtained. Why do we need a separate branch for bounding box detection? Please clarify the role of masks in this framework.\n\nIn Table 1, the results of metric APr and APc are not analyzed in detail. Additionally, the values of the APr and APc metrics for rare and common categories are significantly lower than those of SOTA methods on both the LVISMiniVal and LVISVal datasets. The paper claims strength in semantic relation modeling in open-vocabulary scenarios. However, the results show severely low APr and APc. Without convincing analysis and improved performance, the core claim of semantic relation modeling is unsubstantiated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LM0xbieCQS", "forum": "p2E7haeXxa", "replyto": "p2E7haeXxa", "signatures": ["ICLR.cc/2026/Conference/Submission16625/Reviewer_41M2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16625/Reviewer_41M2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16625/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620741414, "cdate": 1761620741414, "tmdate": 1762926694029, "mdate": 1762926694029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OVRD, which improves open-vocabulary object detection performance by utilizing semantic cues from text embeddings to guide the model's visual perception."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**:\n- This paper's motivation is reasonable. Open-vocabulary object detection indeed needs to utilize cues from the text.\n\n**Clarity**:\n- This paper clearly explains its motivation and methodology."}, "weaknesses": {"value": "- The experimental results in this paper fail to verify that the proposed method can enhance visual perception by leveraging text information: (1) In Table 1, the proposed method OVRD does not demonstrate consistently superior performance over existing methods, particularly on the LVIS rare categories. (2) The paper's ablation study only compares AP metrics under different settings but lacks a specific analysis of using the information from the text embeddings. This fails to show a direct link between the changes in AP values and the utilization of text information. (3) The paper does not provide any visualization results or examples. (4) The experiments are only conducted on LVIS benchmark. Other benchmarks such as COCO-OVD should be included.\n- The paper's assertion that 'existing methods are insufficient in utilizing semantic cues from the text embeddings'  is insufficiently supported and lacks corresponding evidence. This is the paper's fundamental motivation. so it requires justification. I hope the authors can supplement this claim with more evidence, including experimental results, examples, or supporting references."}, "questions": {"value": "- The main experimental results in Table 1 are under the zero-shot setting, but open-vocabulary object detection is different from zero-shot object detection. Given that the title includes 'open-vocabulary', could the authors please explain why the zero-shot setting was used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FbnqZANMxa", "forum": "p2E7haeXxa", "replyto": "p2E7haeXxa", "signatures": ["ICLR.cc/2026/Conference/Submission16625/Reviewer_1gDH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16625/Reviewer_1gDH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16625/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842157647, "cdate": 1761842157647, "tmdate": 1762926693684, "mdate": 1762926693684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OVRD, an open-vocabulary object detection method built upon DINO that aims to improve zero-shot detection performance. The main contributions include: (1) Text-guided Salient Query Selection that selects text-relevant image features along with their reference points and masks, (2) vision rotary positional embeddings for positional awareness enhancement, and (3) Semantic Relation Self-Attention that models sparse and directional semantic relations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides good intuition for modeling semantic relations in open-vocabulary scenarios, and the proposed components work together in a logical manner to enhance multi-modal fusion and text-guided visual perception.\n- Comprehensive ablation studies. Table 2 demonstrates that each component contributes to the final performance, with detailed analysis of design choices."}, "weaknesses": {"value": "- Limited technical novelty. The text-guided salient query selection is a straightforward extension of OV-DINO, by adding reference points/masks selection in Eq. (2). RoPE is borrowed from existing work. The semantic relation modeling, while interesting, only provides +1.4 AP gain as shown in table 2. The overall contribution feels like combining existing techniques.\n- The re-evaluation of OV-DINO baselines described in section A.5 is problematic. The authors remove what they call \"post-training tricks\" which were actually design choices in the original OV-DINO method. This creates an unfair comparison where baseline are artificially lowered. The improvement over OV-DINO$^2$ is only 0.9 AP on LVIS Minival when comparing against the weakened baseline, making the true contribution unclear."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oSUO3895DF", "forum": "p2E7haeXxa", "replyto": "p2E7haeXxa", "signatures": ["ICLR.cc/2026/Conference/Submission16625/Reviewer_vZQg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16625/Reviewer_vZQg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16625/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938215236, "cdate": 1761938215236, "tmdate": 1762926693186, "mdate": 1762926693186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OVRD, a zero-shot open-vocabulary detector built upon the DINO framework. It introduces three main components: a Text-guided Saliency Query Selection (TSQS) mechanism, Positional-Aware enhancements (RoPE), and a Semantic Relation Self-Attention (SRSA) module. The authors report improved results over OV-DINO and Grounding-DINO on the LVIS benchmark, with ablations showing the contribution of each component. While the core idea of using text to guide visual attention is clear, the work's novelty is limited. The technical contributions feel more like an integration and fine-tuning of existing ideas from its predecessors rather than a significant methodological leap."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1、The paper correctly targets a central challenge in OVD: how to better leverage text priors to guide the model's query selection and visual attention.\n2、The authors rightly point out that many models fuse multimodal information too late (i.e., only at the classification stage). The SRSA module is a sensible attempt to address this by modeling semantic relationships earlier in the self-attention layers.\n3、The method is built directly upon the popular DINO/DETR family, making it easy for the community to understand, reproduce, and compare against."}, "weaknesses": {"value": "1、The novelty of the TSQS module is questionable. Language-guided query selection is already a core component of both Grounding-DINO and OV-DINO, making this contribution feel more like a minor, incremental improvement.\n\n2、The core idea of injecting a relation matrix into self-attention has been previously explored in works like Relation-DETR. While the specific implementation may be new, the underlying concept is not.\n\n3、The reported performance gains are quite small (+0.9 AP in some cases). More importantly, the authors admit to re-evaluating OV-DINO by removing some of its original techniques (e.g., template ensembling). This breaks the experimental protocol and results in an unfair, apples-to-oranges comparison."}, "questions": {"value": "1、The paper claims to select \"text-relevant\" queries, but the mechanism is opaque. It's not specified what the T_CLS module is (e.g., a linear layer, MLP?) or how it computes relevance from a simple feature norm.\n\n2、The paper mentions using RoPE but fails to specify where it's integrated into the architecture. More importantly, it lacks a direct ablation study to isolate RoPE's specific contribution to performance.\n\n3、he evaluation protocol is not clearly defined. The paper needs to explicitly state which version of the LVIS dataset is used and whether the standard \"Fixed AP\" metric is reported, making it difficult to verify if the setup aligns with key baselines.\n\n4、The ablation study is flawed, as it bundles multiple changes together. It's impossible to disentangle the true contribution of the proposed modules from known confounding factors, such as simply using more queries or longer text inputs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AVwMrC51SE", "forum": "p2E7haeXxa", "replyto": "p2E7haeXxa", "signatures": ["ICLR.cc/2026/Conference/Submission16625/Reviewer_Dm4g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16625/Reviewer_Dm4g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16625/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955831574, "cdate": 1761955831574, "tmdate": 1762926692680, "mdate": 1762926692680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}