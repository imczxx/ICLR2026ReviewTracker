{"id": "zHdKaF3ZM7", "number": 4784, "cdate": 1757766265204, "mdate": 1759898013236, "content": {"title": "Weight-Space Linear Recurrent Neural Networks", "abstract": "We introduce WARP (**W**eight-space **A**daptive **R**ecurrent **P**rediction), a simple yet powerful model that unifies weight-space learning with linear recurrence to redefine sequence modeling. Unlike conventional recurrent neural networks (RNNs) which collapse temporal dynamics into fixed-dimensional hidden states, WARP explicitly parametrizes its hidden state as the weights and biases of a distinct auxiliary neural network, and uses input differences to drive its recurrence. This brain-inspired formulation enables efficient gradient-free adaptation of the auxiliary network at test-time, in-context learning abilities, and seamless integration of domain-specific physical priors. Empirical validation shows that WARP matches or surpasses state-of-the-art baselines on diverse classification tasks, featuring in the top three in 5 out of 6 real-world challenging datasets. Furthermore, extensive experiments across sequential image completion, multivariate time series forecasting, and dynamical system reconstruction demonstrate its expressiveness and generalization capabilities. Remarkably, a physics-informed variant of our model outperforms the next best model by more than 10x. Ablation studies confirm the architectural necessity of key components, solidifying weight-space linear RNNs as a transformative paradigm for adaptive machine intelligence.", "tldr": "We introduce a simple yet powerful model that unifies weight-space learning with linear recurrence to redefine sequence modeling.", "keywords": ["physics-informed machine learning", "weight-space learning", "meta-learning", "deep sequence modeling", "linear recurrence", "test-time training"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67f9b1813785e9f7014ae00843c8dc0738982879.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors seek to incorporate domain priors and in-context reasoning into linear RNNs.\nThey show that (i) linear RNNs can act as effective hypernetworks, (ii) these RNNs can engage in in-context learning or physics-informed modeling, and (iii) these hypernetworks exhibit significant improvements over baseline models.\nModels are evaluated on forecasting / reconstruction, classification, and in-context linear regression tasks.\nBaseline models include gated RNNs, linear SSMs (S4), and transformers."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Fantastically written, very clear.\n- Related work and appendix are thorough.\n- Experiments show clear improvements of the proposed WARP RNN over baseline models, both in accuracy and compute.\n- Authors are clear about limitations, especially the important limitation that the hidden state transition matrix is dense."}, "weaknesses": {"value": "- The introduction does not (although the related work does) mention selective SSMs (e.g., Mamba), which have re-introduced nonlinearities into linear RNNs.\n- CelebA reconstruction seems a bit contrived as a task: the annotated S4 article referenced does indeed reconstruct MNIST and similar image data, but then proceeds to spoken digit data, which the authors did not try reconstructing.\nMoreover, CelebA face reconstructions are substantially corrupted.\nWhile I do not deny WARP's improvement in image reconstruction over baseline, I am unsure about the salience of image reconstruction as a metric for evaluating RNNs.\nI think there are more relevant tasks to measure long-range dependencies, such as Long-Range Arena benchmarks.\n- There are no experiments on text prediction or classification, which are some of the most relevant tasks for evaluating new RNN architectures - does WARP have limitations or inductive biases that prevent it from expressively processing text?\nIf so, the authors should state it, as this is an important limitation, especially to claims of in-context learning.\nIf not, then the authors should evaluate their model with text to support the expressivity of WARP.\n- WARP's connections to biology are not sufficiently explained in the main text, although the authors do discuss it further in the appendix."}, "questions": {"value": "I suggest the following:\n- Traditional long-range dependency experiments like Long-Range Arena benchmarks.\n- Text prediction and classification experiments to support claims of expressivity.\n- Connections to biology, especially in the main text, should be clarified.\n- As stated by the authors, future work should explore ways to reduce the density of hidden-state transition matrices used in WARP. \nComplex diagonal recurrences may be fruitful in this regard."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MIiGSYD6YS", "forum": "zHdKaF3ZM7", "replyto": "zHdKaF3ZM7", "signatures": ["ICLR.cc/2026/Conference/Submission4784/Reviewer_rzHm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4784/Reviewer_rzHm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761190017162, "cdate": 1761190017162, "tmdate": 1762917575732, "mdate": 1762917575732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes WARP, a novel class of recurrent neural networks (RNNs) that perform sequence modeling directly in weight-space, blending linear recurrence with non-linear decoding. Unlike standard RNNs, which maintain a hidden state that is a result of propagating the sequence through the network, the proposed model’s hidden state is instead equal to the parameter vector of a so called auxiliary (“root”) MLP and is updated over time via a linear recurrence. Each update is driven by consecutive input differences, and the auxiliary MLP provides nonlinear decoding using an input that encodes the canonical ordering of the sequence.  This formulation enables gradient-free adaptation and in-context learning during inference, as well as the injection of physics priors through explicit parameter constraints.\nExtensive experiments are conducted across time-series analysis, dynamical system reconstruction, and multivariate time-series classification. WARP shows consistent or superior performance to state-of-the-art baselines on most tasks. A physics-informed variant, WARP-Phys, achieves significant improvements on physical dynamics reconstruction tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel conceptual framing:  The idea of treating the recurrent hidden state of a linear state-space model as the weights of another neural network is both elegant and novel. It bridges ideas from fast weights, meta-learning, hypernetworks and structured state-space models while maintaining linear recurrence efficiency. Additionally, it offers a built-in support for gradient-free adaptation, in-context learning, and physics-informed modeling in a single architecture.\n2. Computational Efficiency: Once the model has learned from the context, the final root network can be extracted and reused to process subsequent queries without reevaluating the entire sequence, yielding significant computational savings compared to other in-context learning models. Furthermore, the proposed architecture leverages a dual training mode that combines linear recurrence with a parallel scan operator – a well-established technique in the State Space Model (SSM) literature – to accelerate state propagation. Together, these design choices lead to notable computational efficiency improvements.\n3. Strong empirical results: Competitive or superior performance on time-series analysis, especially Traffic Flow Forecasting (despite ignoring graph priors) and Image completion, as well as dynamical system reconstruction. The inclusion of a physics-informed variant further demonstrates the framework’s adaptability and potential for interpretability.\n4. Clarity and completeness:  The paper is well-written, includes high-quality figures, ablations, detailed appendices, and clear pseudocode."}, "weaknesses": {"value": "1. Scalability constraints:  The main bottleneck is the large transition matrix which scales quadratically with the number of root-network parameters. Experiments are thus limited to moderate model sizes, raising questions about feasibility for large-scale models.\n2. Limited theoretical grounding:  While the empirical evidence is compelling, the theoretical analysis of representational capacity and stability (e.g., under linear recurrence updates) still remains to be established. \n3. Computational cost reporting:  Although the recurrence is linear, updating and decoding weight vectors remains costly. Memory and compute scaling with model size are not fully quantified and are only provided in the appendix, but entirely missing from the main body of the paper.\n4. Limited setting for dynamical system reconstruction:  While the possibility of making the network physics-informed is compelling, the shown examples illustrate this for relatively simple systems with a small number of parameters. While the proposed method clearly allows for in-context learning, and hence does not need to retrain a network for each new dynamical system (from the same category), the setups are done for what appears to be noiseless input-output data, and a low number of parameters. One could, instead of learning the entire mapping of the system, learn only its phase or exponential mapping for any other sequence model that allows for ICL, in the same way as demonstrated in this work. This alternative formulation would serve as a fairer baseline for comparison.\n5. Novelty relative to prior work:  There is conceptual overlap with other concepts briefly outlined in the related work (e.g., fast weight RNNs), though the authors’ framing is distinctive. A more explicit comparison to those baselines would help to better position WARP’s contribution."}, "questions": {"value": "1. Have you evaluated WARP’s performance on noisy measurement scenarios for dynamical system reconstruction?\n2. Could you discuss related sequence modeling approaches in weight space and explain why these were not included as baselines in any of the tasks?\n3. For the Energy Prediction experiment: what are the current state-of-the-art results? In general, for all experiments, how were baselines tuned, and how much effort was spent on hyperparameter optimization? Also, performance of S4 is reported only on MNIST, and ConvCNP only on CelebA. Full performance tables in the appendix would improve transparency.\n4. Have you considered benchmarking WARP on long-range dependency benchmarks, such as Long Range Arena (LRA)? This could contextualize WARP’s capabilities relative to other established long-sequence models that were evaluated on these tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K7gWau6xml", "forum": "zHdKaF3ZM7", "replyto": "zHdKaF3ZM7", "signatures": ["ICLR.cc/2026/Conference/Submission4784/Reviewer_3Pnu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4784/Reviewer_3Pnu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920579154, "cdate": 1761920579154, "tmdate": 1762917574881, "mdate": 1762917574881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper offers a new weight-space learning technique that iterates weights of a feedforward neural network in a linear state space, enabling in-context learning. The authors evaluate WARP across diverse tasks including image completion, time series forecasting and classification, and dynamical system reconstruction, demonstrating competitive or superior performance compared to standard RNNs, state-space models (S4, Mamba), and Transformers. Notably, a physics-informed variant achieves order-of-magnitude improvements on physical system reconstruction benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core idea of parametrising RNN hidden states as weights of an auxiliary neural network is conceptually interesting and, to the best of my knowledge, novel. The authors test their method on a diverse set of domains and perform a large range of ablations to show the necessity of design choices. The writing is generally accessible, with good intuitive explanations, and goes far to place itself in the larger test-time adaptation literature."}, "weaknesses": {"value": "Claims are sometimes overstated and/or imprecise. For example, phrases like \"transformative paradigm for adaptive machine intelligence\" (Abstract, Conclusion) and \"redefine sequence modeling\" (Abstract) are not well-supported. The empirical results show WARP is competitive but not uniformly superior. \"Brain-inspired formulation\" (Abstract, page 2) refers only to using input differences, with citation to synaptic plasticity [16], but the connection is somewhat superficial - there is a rich literature concerned with modelling realistic synaptic plasticity rules which this approach to weight-space trajectory modelling does not engage with. \"Infinite-dimensional RNN hidden states\" (page 9, footnote 6) is misleading—the hidden state is finite-dimensional, though it parametrizes a function."}, "questions": {"value": "“Rather than relying on direct inputs, we draw inspiration from the human brain and compute signal differences to drive such recurrences.” - from where is the inspiration drawn? Did you try ablating this, i.e. just using x_t as the input to B? Similarily, did you try ablating the random noise applied to observations in AR mode? i.e. p_forcing = 0 (or 1, whatever means noise is never added)\n\nWhat is the operational difference between gradient-free adaptation and in-context learning? The definitions provided seem to be nested on page 2 (i.e. in-context learning implies gradient-free adaptation).\n\n\"This strategic initialisation also imposes a critical constraint wherein the initial hidden state θ_0 must encode semantically rich information applicable to the entire sequence.\" Could you give some clarity here - how can the weights encode information about a sequence prior to observing it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GyOp2eDthB", "forum": "zHdKaF3ZM7", "replyto": "zHdKaF3ZM7", "signatures": ["ICLR.cc/2026/Conference/Submission4784/Reviewer_rGWP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4784/Reviewer_rGWP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032542494, "cdate": 1762032542494, "tmdate": 1762917574578, "mdate": 1762917574578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes recurrent neural networks (RNNs) through a weight-space linear recurrence formulation that unifies several modern architectures — including continuous-time linear RNNs, state-space models (SSMs), and residual recurrent networks — under a single linear operator perspective.\n\nThe authors derive closed-form expressions for training dynamics and generalization in the overparameterized limit, showing that convergence properties and implicit regularization can be understood via the spectral structure of the recurrent Jacobian. They provide:\n\nA linearized weight-space recurrence model that approximates nonlinear dynamics by a low-rank operator with analytically tractable behavior.\n\nA demonstration that generalization error scales with spectral conditioning, extending kernel-based intuition from linear networks to recurrent architectures.\n\nEmpirical validation on synthetic sequence modeling and dynamical-system reconstruction tasks (mass–spring–damper, Lotka–Volterra, PEMS08 traffic), showing alignment between predicted and observed convergence trends.\n\nOverall, the paper contributes to a principled understanding of how weight-space geometry and recurrence interact to shape training efficiency and generalization."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper builds on well-established analyses of linear networks and extends them naturally to recurrent settings using a spectral-decomposition framework (Schur- and SVD-based). Derivations are internally consistent and clearly documented.\n\nThe proposed linear recurrence view elegantly bridges RNNs, residual-RNNs, and diagonal SSMs, helping clarify connections between recent model families.\n\nExperiments across several dynamical-system tasks (MSD, LV, traffic flow forecasting) confirm the predicted dependence of training speed and generalization on spectral conditioning and effective recurrence length.\n\nThe inclusion of both synthetic physics systems and real-world time-series (PEMS08) demonstrates breadth and internal consistency.\n\nMathematical exposition is detailed; hyperparameters and architectures are listed (Appendix D.4–D.6). Code release and ablation details are promised."}, "weaknesses": {"value": "1.  The analytic results rest on linear, Gaussian assumptions; nonlinear recurrence effects and gating dynamics are only discussed qualitatively. As such, predictive power for modern gated RNNs or structured SSMs is limited.\n\n2. The analysis centers on the infinite-width, overparameterized limit; it does not quantify where the asymptotic predictions break down for finite models.\n\n3. The authors reference Saxe et al. (2014) but omit more recent theoretical works on curriculum and transfer in RNNs (e.g., Rajan, Kepple & Engleken) and on gradient-flow analyses in recurrent kernels — literature directly related to their spectral-mode interpretation.\n\n4. Although the experiments match qualitative trends, they serve mainly as demonstrations rather than quantitative tests (e.g., no variance or uncertainty estimates, small sample sizes).\n\n5. Generalization is assessed by mean-squared error only; tasks with stochastic noise or long-term dependency tests (copy, addition, character-level modeling) would strengthen claims about recurrence depth and spectral bias."}, "questions": {"value": "1. How sensitive are your analytical predictions to non-normal dynamics (upper-triangular Hₕ) versus the diagonal “normal” case you ultimately focus on?\n\n2. Could your framework accommodate nonlinear activation perturbations (e.g., ReLU linearization) to estimate when linear approximation fails?\n\n3. Have you compared your spectral regularization predictions to empirical spectral shrinkage observed during training (e.g., spectrum compression in Wh)?\n\n4. Could you clarify how your results relate to implicit bias analyses in linear transformers or SSMs (e.g., Merrill & Sabharwal; Orvieto et al.)?\n\n5. Are there regimes where the recurrence’s spectral radius predicts too-rapid forgetting or instability, contradicting observed dynamics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NuwNC5fYaL", "forum": "zHdKaF3ZM7", "replyto": "zHdKaF3ZM7", "signatures": ["ICLR.cc/2026/Conference/Submission4784/Reviewer_jyKY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4784/Reviewer_jyKY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762099559596, "cdate": 1762099559596, "tmdate": 1762917573953, "mdate": 1762917573953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **WARP**, a framework that performs *linear recurrence in weight space* rather than in hidden-state space. At each step, the parameters of a small decoder network evolve linearly:\n\n$$\\omega_t = A \\omega_{t-1} + B(x_t - x_{t-1}), \\qquad y_t = \\text{MLP}_{\\omega_t}(\\varepsilon)$$\n\nwhere \\(A, B\\) are learned transition matrices and $\\varepsilon$ encodes position or context.  \n\nConceptually, this shifts recurrence from feature dynamics to parameter dynamics. It blends (i) the efficiency of linear RNNs/SSMs (e.g., S4, Mamba), (ii) the expressivity of nonlinear decoders, and (iii) in-context or gradient-free adaptation through weight evolution. The model is tested across image completion, time-series forecasting, dynamical-system reconstruction, and classification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. Original framing. The move to perform recurrence directly in parameter space is novel\nand quite elegant. It reads as a middle ground between hypernetworks and fast-weight\nRNNs, but with the analytical simplicity of a linear transition.\n\nS2. Range of results. The experiments span diverse domains - MNIST/CelebA completion,\nETT and PEMS forecasting, DSR, and UEA time-series classification. The UEA section\nis particularly strong: comparisons include modern SSMs like S5, Mamba, S6, NRDE,\nand NCDE, with WARP performing competitively across most tasks.\n\nS3. Interpretability and analogy. The weight updates via input differences evoke synaptic-\nplasticity rules, which gives the method a neat biological parallel and some explanatory\nappeal.\n\nS4. Presentation. The paper is clear, visually well-organized, and balances theory with\nintuition. Figures showing progressive reconstruction genuinely help convey how the recurrence behaves."}, "weaknesses": {"value": "W1. Benchmark depth. While broad, the benchmark is missing some of the newer SSMs\nthat define the current frontier. In particular, LinOSS (Rusch & Rus, 2024)—an oscilla-\ntory, long-sequence SSM—is cited but not compared. Given that LinOSS, FACTS, and\nGriffin all outperform S4 and Mamba on long forecasting tasks, excluding them makes\nthe SoTA claim weaker.\n\nW2. Scalability. The transition matrix $A \\in \\mathbb{R}^{D_\\omega \\times D_\\omega}$ scales quadratically with the size of the decoder, which will quickly become impractical. No structured or low-rank variants are\nexplored.\n\nW3. Theory gap. The paper is mostly empirical. There’s no discussion of spectral properties,\nstability, or representational capacity of the linear map in weight space.\n\nW4. Domain imbalance. Some domains (especially physics and image experiments) use\nsmall or older baselines (ConvCNP, GRU, Transformer). More recent adaptive or physics-\ninformed baselines like Neural Context Flows (ICLR 2025) or ZEBRA (2024) would\nstrengthen those sections."}, "questions": {"value": "• Include direct comparisons to LinOSS, FACTS, and Griffin.\n\n• Explore structured or low-rank A, B for scale.\n\n• Add runtime and memory tables.\n\n• Include a brief stability/spectral analysis.\n\n• Clarify what fundamentally distinguishes this from hypernetworks and fast-weight RNNs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lmMAYKkBmE", "forum": "zHdKaF3ZM7", "replyto": "zHdKaF3ZM7", "signatures": ["ICLR.cc/2026/Conference/Submission4784/Reviewer_T7rz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4784/Reviewer_T7rz"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4784/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762517894861, "cdate": 1762517894861, "tmdate": 1762917573426, "mdate": 1762917573426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}