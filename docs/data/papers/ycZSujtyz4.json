{"id": "ycZSujtyz4", "number": 11895, "cdate": 1758204514361, "mdate": 1759897548047, "content": {"title": "TACS-Guided Self-Alignment of LVLMs for Explainable Chest X-ray Analysis", "abstract": "Large vision–language models (LVLMs) hold promise for medical imaging but face two critical\nchallenges: dependence on curated human-annotated datasets for alignment and poor robustness to\nreal-world perturbations. We show that LVLMs can produce inconsistent outputs between original chest X-rays and WhatsApp-compressed versions that appear visually indistinguishable. Such failures raise serious concerns for mHealth platforms, where compressed or perturbed images are common in real-world diagnostic workflows. Moreover, current LVLMs often attribute lung abnormality predictions to irrelevant regions outside the lungs—a phenomenon we term out-of-lung saliency (OLS)—which is exacerbated by compression artifacts. These challenges highlight the urgent need for robust and explainable LVLMs in CXR diagnosis.\n\nTo address these issues, we propose Self-CXRAlign, a self-alignment framework that enhances\nexplainability robustness through multi-task learning (MTL)-driven supervised fine-tuning (SFT). Self-CXRAlign enforces explainability robustness, ensuring stability of predictions and attributions across original and perturbed images. Central to our method is the Inter-Task Attribution Conflict Score (TACS), a novel metric that guides the selection of auxiliary tasks to reduce attribution conflicts and mitigate negative transfer. By steering SFT with TACS, Self-CXRAlign achieves up to 80% reduction in OLS compared to naïve MTL, paving the way for explainable and trustworthy LVLM deployment in mHealth-driven chest X-ray analysis.", "tldr": "A novel technique for self-alignment of Large-Vision Language Models for Robust Chest X-ray Diagnosis", "keywords": ["lvlm", "vision language models", "self-alignment", "robustness", "explainability", "llm", "lmm"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6294adbe982d55410295cbee7ba8e83b4fcb42ad.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "We propose Self-CXRAlign, a self-alignment framework that improves explainability robustness via MTL-driven supervised fine-tuning. The method ensures prediction and attribution consistency across clean and perturbed CXR inputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written with a precise problem formulation. The proposed multi-task learning (MTL) framework for modeling inter-task relationships and identifying auxiliary tasks that avoid negative transfer is interesting and well motivated. The idea to use selected auxiliary tasks for self-alignment to improve LVLM robustness to compression artifacts addresses a practical failure mode for clinical imaging deployed over low-bandwidth platforms."}, "weaknesses": {"value": "1. Motivation relies on assumptions about perturbations. It is unclear how the authors decide which perturbations (types and strengths) are “optimal” or realistic for the target deployment (e.g., WhatsApp compression). The paper would benefit from a clearer justification for the chosen corruptions and how representative they are of real-world transmission artifacts.\n2. Training efficiency. The proposed approach requires training combinations of the primary task with each auxiliary task, this could be computationally expensive. More discussion or methods to mitigate training cost are needed.\n3. The evaluation should report clinically meaningful metrics commonly used in the domain (e.g., CheXbert-F1 for label prediction when applicable) and more robustness analyses (e.g., performance vs corruption strength, explanation faithfulness).\n4. Table3b seems missing, which is important to show the soundness of motivation for this work."}, "questions": {"value": "What is the strategy to generate explanation heatmaps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "l4gzzhxsHP", "forum": "ycZSujtyz4", "replyto": "ycZSujtyz4", "signatures": ["ICLR.cc/2026/Conference/Submission11895/Reviewer_CjUq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11895/Reviewer_CjUq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732678044, "cdate": 1761732678044, "tmdate": 1762922908505, "mdate": 1762922908505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TACS, a metric to assess post-hoc saliency map consistency across non-compressed and compressed images. Using TACS, the authors propose Self-CXRAlign, a method to find which sub-tasks promote explanation consistency without the need\nto train the model on all possible sets of sub-tasks, showing that Self-CXRAlign improves the report generation and OLS. Even though Self-CXRAlign seems promising, some methodological decisions and the lack of broader results raise concerns about the technique."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "· Making the saliencies consistent between compressed and non-compressed images results in better performance for report generation, which is an interesting finding.\n· The attribution vulnerability and TACS metrics introduced by the paper are valuable and can be used in other contexts."}, "weaknesses": {"value": "· The method's primary goal is to make the saliency maps consistent across compressed and non-compressed images. Nevertheless, these saliencies are generated by only observing the gradients or the attention scores at different levels of the network. It is\nunclear why the authors chose this approach instead of adapting the well-described methods in the literature, such as Grad-CAM, RISE, etc. References are also missing in this most important section of the work.\n\n· Since the authors are using this method to generate the saliencies, some metric to evaluate the faithfulness of the explanations should have been performed, i.e, to check if the highlighted pixels are important for the model's final prediction using metrics such as\nMoRF and LeRF.\n\n· As observed in Figure 3a, the OLS score is relatively high for compressed and non-compressed images. This might indicate that the model performs similarly for both compressed and non-compressed images, or that the generated saliency maps perform\npoorly.\n\n· Figures 1,2, and 5 should be improved.\n\n· The results are evaluated only when Fracture was selected as the primary task.\n\n· The results are not sufficient to support the goal of the method.\n\n· The work does not have a conclusion section."}, "questions": {"value": "· Why did the authors decide to generate the saliency maps using the described method? How can the authors check that these maps are meaningful?\n\n· In Section 3.3, the authors claimed that an attribution map for MTL does not exist in the literature, and then decided to take the average of the saliencies for each label as the final one. Can the authors clarify why it was done that way? Why not only have separated\nsaliencies for each label identified by the model?\n\n· Section 4.1 uses a surrogate model (vision-transformer) to find the best tasks that optimize the consistency among the saliencies for the compressed vs non-compressed. With the identified tasks, the LVLM is then fine-tuned. How are the saliencies generated\nfor the surrogate model? Section 3.1 only defines how to generate the saliencies for the LVLM.\n\n· Why is the PIP Score not reported in the results for Self-CXRAlign?\n\n· Self-CXRAlign is fine-tuned using Fracture as the primary task, and the sub-tasks are identified using the method. Table 1 presents the results for the reports generated for the Fracture label. Did the authors also fine-tune the other methods/models similarly? This\nwould make this comparison fairer to the SOTA alternatives.\n\n· Table 3 in the Appendix shows that the proposed method performs worse than the others in the accuracy and precision metrics. Why do the authors claim that it outperforms the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q3YibPqu67", "forum": "ycZSujtyz4", "replyto": "ycZSujtyz4", "signatures": ["ICLR.cc/2026/Conference/Submission11895/Reviewer_tgNJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11895/Reviewer_tgNJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894801286, "cdate": 1761894801286, "tmdate": 1762922908046, "mdate": 1762922908046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles robustness and explainability in Large Vision-Language Models (LVLMs) for chest X-ray (CXR) analysis. The authors identify two main issues: (1) model instability, where LVLMs produce inconsistent predictions for original versus WhatsApp-compressed images, and (2) poor explainability due to Out-of-Lung Saliency (OLS), models attributing lung findings to irrelevant regions. To address these, they propose Self-CXRAlign, a self-alignment framework that uses Multi-Task Learning (MTL) and Supervised Fine-Tuning (SFT). The core idea is the Inter-Task Attribution Conflict Score (TACS), which guides the selection of auxiliary tasks by favoring those with negative attribution correlations (i.e., conflicting tasks). The authors claim this reduces attribution vulnerability and improves robustness, reporting up to 30% reduction in OLS and 45% improvement in report generation metrics compared to SOTA LVLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Real-world robustness to image perturbations (e.g., messaging-app compression) is an important and underexplored limitation in medical LVLM deployment.\n- Formalizing Out-of-Lung Saliency (OLS) as a measurable failure mode is valuable for interpretability studies.\n- Novel concept: The idea of using inter-task attribution conflicts (negative TACS) to guide multi-task training is theoretically interesting.\n- The framework leverages existing model attributions rather than requiring additional manual labels, which is conceptually efficient."}, "weaknesses": {"value": "Despite its novelty, the paper suffers from serious validation and evaluation issues.\n\n- The authors train a LLaVA-CXR (full-MTL) model on all 14 tasks but never report its results on the full test set. Instead, they compare a TACS-guided model (9 tasks) against the 14-task models only on the 9-task subset, effectively comparing a specialist to a generalists. This looks unfair and invalidates the claim of a 45% improvement. The experiment should include full test results and fair baselines trained on equivalent subsets.\n- The benefit of the TACS metric is conflated with that of the Self-CXRAlign SFT pipeline. Missing a crucial control experiment: retraining a SOTA baseline (e.g., LLaVA-Rad) on the 9-task subset identified by TACS. If this baseline also improves, the SFT procedure adds no value beyond TACS filtering\n- The entire motivation that LVLMs fail on WhatsApp-compressed images is not empirically proven.\n- To establish instability, the authors should have evaluated baseline LVLMs on original vs. compressed CXRs over multiple inference runs (e.g., 5–10 inferences) and reported per-abnormality average, variance, and instability rates. Robustness should also have been tested on multiple perturbation types (different JPEG quality levels). Without this, the motivation remains unclear\n- The model explicitly drops five abnormalities (e.g., Pneumonia, Pneumothorax, Lung Lesion) that have positive TACS scores. This means the final model is blind to critical pathologies, which makes it unsafe for clinical deployment. The paper does not quantify this trade-off or explain how such omissions could be mitigated.\n- TACS-guided SFT is tested only on the authors’ own LLaVA-CXR model and on one dataset (MIMIC-CXR). There is no demonstration that TACS generalizes to other datasets (e.g., CheXpert, PadChest), which limits its scientific impact.\n- The paper does not report the sample counts or data distributions for the 14 abnormalities. Without this, we cannot tell whether TACS captures genuine attributional conflict or simply correlates with data imbalance."}, "questions": {"value": "- Please provide the full evaluation of LLaVA-CXR (full-MTL) on all 14 tasks, including original and WhatsApp-compressed versions, and a comparison with other SOTA models on the full test dataset.\n- What is the performance of the model on the five dropped tasks (Pneumonia, Pneumothorax, etc.)?\n- How can a model that omits these critical findings be considered clinically valid?\n- Why was a simple baseline retrained on the 9-task subset not included to isolate TACS’s effect?\n- Please report training-sample counts per task and analyze their correlation with positive/negative TACS scores."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6391aBzh13", "forum": "ycZSujtyz4", "replyto": "ycZSujtyz4", "signatures": ["ICLR.cc/2026/Conference/Submission11895/Reviewer_BmGJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11895/Reviewer_BmGJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923997737, "cdate": 1761923997737, "tmdate": 1762922907574, "mdate": 1762922907574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Self-CXRAlign, a self-alignment framework designed to improve the robustness and explainability of large vision–language models (LVLMs) for chest X-ray analysis. The method employs multi-task learning–driven supervised fine-tuning (SFT) guided by a new metric, the Inter-Task Attribution Conflict Score (TACS), to reduce attribution conflicts. Experiments show that Self-CXRAlign improves stability and reduces OLS, supporting more reliable LVLM deployment in mHealth settings."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses an important and practical problem in deploying LVLMs for medical imaging, emphasizing robustness and explainability in real-world clinical and mHealth settings."}, "weaknesses": {"value": "The paper’s overall structure is poorly organized and many key experiments are placed in the supplementary materials, while Section 3 is overly lengthy and cluttered with unnecessary formulas, reducing readability and focus on the main contributions.\nThe Related Work section lacks sufficient discussion of existing research on explainability analysis, missing connections to prior interpretability frameworks in medical LVLMs.\nIn the experimental part, the authors mention constructing a WhatsApp-compressed dataset but fail to reference or compare with CVPR 2025’s “CheXwhatsApp” work, which directly addresses similar challenges. Moreover, the use of the same dataset name in the supplementary material requires clarification.\nThe use of saliency and attention maps as evidence for explainability is limited; these methods are not fully appropriate for evaluating LVLM interpretability. More rigorous validation involving clinical experts would be necessary to support the claims of improved explainability."}, "questions": {"value": "1. Many key experiments are placed in the supplementary materials, and Section 3 contains lengthy formulaic derivations. Could the authors clarify why these details were not integrated into the main text and how they plan to improve the paper’s overall structure and readability?\n\n2. The Related Work section lacks discussion of explainable analysis methods. How does the proposed framework relate to prior studies on interpretability in LVLMs, and what are its unique contributions in this context?\n\n3. The paper mentions constructing a WhatsApp-compressed dataset but does not cite or compare with CVPR 2025’s “CheXwhatsApp” dataset. Could the authors explain the relationship between their dataset and that work, especially given the identical dataset name used in the supplementary materials?\n\n4. Saliency and attention maps are used as the main explainability evidence. Could the authors justify why these visualizations are sufficient for assessing LVLM interpretability, and whether any clinical expert validation was conducted to support the conclusions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GTxkrNEQL8", "forum": "ycZSujtyz4", "replyto": "ycZSujtyz4", "signatures": ["ICLR.cc/2026/Conference/Submission11895/Reviewer_NPeS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11895/Reviewer_NPeS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052523940, "cdate": 1762052523940, "tmdate": 1762922907209, "mdate": 1762922907209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}