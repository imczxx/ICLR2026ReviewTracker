{"id": "XAXT7A8EWh", "number": 12600, "cdate": 1758208890895, "mdate": 1763654830255, "content": {"title": "Post-Training Quantization for Video Matting", "abstract": "Video matting is crucial for applications such as film production and virtual reality, yet deploying its computationally intensive models on resource-constrained devices presents challenges. Quantization is a key technique for model compression and acceleration. As an efficient approach, Post-Training Quantization (PTQ) is still in its nascent stages for video matting, facing significant hurdles in maintaining accuracy and temporal coherence. To address these challenges, this paper proposes a novel and general PTQ framework specifically designed for video matting models, marking, to the best of our knowledge, the first systematic attempt in this domain. Our contributions include: (1) A two-stage PTQ strategy that combines block reconstruction-based optimization for fast, stable initial quantization and local dependency capture, followed by a global calibration of quantization parameters to minimize accuracy loss. (2) A Statistically-Driven Global Affine Calibration (GAC) method that enables the network to compensate for cumulative statistical distortions arising from factors such as neglected BN layer effects, even reducing the error of existing PTQ methods on video matting tasks up to 20%. (3) An Optical Flow Assistance (OFA) component that leverages temporal and semantic priors from frames to guide the PTQ process, enhancing the model’s ability to distinguish moving foregrounds in complex scenes and ultimately achieving near full-precision performance even under ultra-low-bit quantization. Comprehensive quantitative and visual results show that our PTQ4VM achieves the state-of-the-art accuracy performance across different bit-widths compared to the existing quantization methods. We highlight that the 4-bit PTQ4VM even achieves performance close to the full-precision counterpart while enjoying 8× FLOP savings.", "tldr": "", "keywords": ["Post-Training Quantization", "Video Matting"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a82a01855ed72ba331539441a60524a66e10ebe.pdf", "supplementary_material": "/attachment/834de0a43d980a685732151f34149cbb14e69253.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes PTQ4VM, a post-training quantization framework for video matting that aims to maintain temporal and spatial quality under low-bit quantization. The framework consists of three components: Block-wise Initial Quantization (BIQ) for stable block-level quantization; Global Affine Calibration (GAC) to compensate for post-quantization statistical shifts; and Optical Flow Assistance (OFA) to enforce temporal consistency using motion-guided priors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, easy to follow. \n2. The overall pipeline is simple, modular, and compatible with existing matting architectures, showing practical deployment potential."}, "weaknesses": {"value": "1. How does the proposed BIQ method handle feature dependencies across quantized blocks? If each block is optimized independently, could quantization errors accumulate or disrupt global feature consistency?\n2. The OFA module assumes accurate optical-flow alignment, but how robust is the framework when flow estimation is noisy or fails under motion blur and occlusion? Also, has the computational overhead of running optical-flow inference been measured?\n3. Although the paper claims to be “training-free,” both BIQ and GAC require calibration data. What happens when the deployment domain differs from the calibration domain (e.g., lighting, background, or motion changes)?\n4. The reported gains over existing PTQ baselines appear marginal. Based on ablation table, the effectiveness of OFA is quite unstable, does it caused by the unstable quality of flow estimation?"}, "questions": {"value": "See above weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WjlRn29WAR", "forum": "XAXT7A8EWh", "replyto": "XAXT7A8EWh", "signatures": ["ICLR.cc/2026/Conference/Submission12600/Reviewer_TTBj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12600/Reviewer_TTBj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12600/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779797089, "cdate": 1761779797089, "tmdate": 1762923449963, "mdate": 1762923449963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PTQ4VM, a post-training quantization framework designed for video matting models.\nIt consists of three main components: BIQ (Block-wise Initial Quantization), which stabilizes quantization by optimizing scale and rounding at the block level; GAC (Global Affine Calibration), which corrects distribution shifts after BN folding through learnable global scaling and bias factors; and OFA (Optical Flow Assistance), which introduces an optical-flow-based regularization to improve temporal consistency between frames. Experiments on RVM and MatAnyOne demonstrate that PTQ4VM achieves near-FP32 accuracy under 4-bit quantization without retraining, while significantly reducing computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper addresses a new and underexplored problem—post-training quantization for video matting—where temporal stability is as crucial as spatial accuracy. While PTQ has been well studied for image-based tasks, its extension to temporally dependent applications is novel and practically meaningful, showing potential for efficient deployment in real-time video systems.\n\n2. The overall framework design is coherent and systematic, combining block-wise, global, and temporal calibration stages (BIQ–GAC–OFA) in a way that builds logically from local to global stability. Each stage has a clear motivation and is supported by ablation experiments that justify its inclusion, giving the paper a well-engineered and reproducible structure.\n\n3. The method shows strong empirical performance, maintaining almost FP32-level accuracy even at 4-bit precision. This demonstrates that the proposed calibration techniques effectively reduce quantization errors, supporting the claim that accurate low-bit inference can be achieved without any retraining.\n\n4. PTQ4VM is evaluated on both RVM (CNN-RNN) and MatAnyOne (Transformer-based) architectures, suggesting the framework’s potential generality across different network structures. The consistent behavior across these models indicates that the proposed approach could be extended beyond matting to other temporally sensitive vision tasks."}, "weaknesses": {"value": "1. Although OFA is proposed as the key contribution to improve temporal consistency, the empirical improvement on DTSSD is limited or inconsistent across experiments. This raises doubts about how much OFA truly contributes to stability, and whether its effect depends on the quality of the optical flow model used during calibration.\n\n2. From a methodological standpoint, the work is largely incremental, extending ideas already explored in earlier PTQ methods such as BRECQ and bias correction. While the integration of these ideas into a single pipeline is well executed, it does not fundamentally change the underlying quantization paradigm or introduce new theoretical insights.\n\n3. The baseline comparison is somewhat outdated, as it only includes earlier methods like BRECQ and QDrop. More recent PTQ techniques would provide a stronger context for assessing the relative contribution of PTQ4VM and its empirical advantage.\n\n4. Although the paper claims that GAC stabilizes feature distributions, the evidence remains qualitative and lacks quantitative validation.\nAdditional analysis showing layer-wise mean or variance alignment before and after calibration would make the distribution-correction claim more convincing and scientifically grounded."}, "questions": {"value": "1.\n1-1. OFA is introduced to enhance temporal consistency, yet Table 2 shows several cases where DTSSD becomes worse after adding the module. Could the authors clarify whether OFA truly improves temporal stability or if the effect is within the variance of the metric?\n\n1-2. Since RAFT may not capture the fine-grained pixel motions that are critical in video-matting scenarios, have the authors considered using a more accurate flow estimator such as GMFlow [1], or FlowFormer [2] for warping? It would be informative to see whether OFA’s limited impact stems from the quality of the flow model itself.\n\n1-3. The paper employs DTSSD as the sole metric for temporal consistency, but this measure might not fully reflect perceptual flicker or long-term drift.Could the authors justify why DTSSD is the most appropriate choice, or consider including an additional metric?\n\n2. The comparison includes BRECQ and QDrop, which are relatively old. It would strengthen the paper to include or at least discuss more recent PTQ methods such as GPTQ [3], SmoothQuant [4], or OmniQuant [5] to better position the contribution.\n\n3. BIQ appears conceptually similar to BRECQ’s block-wise reconstruction. Could the authors clarify what concrete design difference—such as the optimization schedule, block partitioning, or objective weighting—makes BIQ more stable or effective?\n\n4. The paper claims that GAC stabilizes intermediate feature distributions, but the evidence is mostly qualitative. Quantitative measurements of layer-wise mean and variance before and after calibration would make this claim more convincing.\n\n[1] Xu, Haofei, et al. \"Gmflow: Learning optical flow via global matching.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n[2] Huang, Zhaoyang, et al. \"Flowformer: A transformer architecture for optical flow.\" European conference on computer vision. Cham: Springer Nature Switzerland, 2022.\n\n[3] Frantar, Elias, et al. \"Gptq: Accurate post-training quantization for generative pre-trained transformers.\" arXiv preprint arXiv:2210.17323 (2022).\n\n[4] Xiao, Guangxuan, et al. \"Smoothquant: Accurate and efficient post-training quantization for large language models.\" International conference on machine learning. PMLR, 2023.\n\n[5] Shao, Wenqi, et al. \"Omniquant: Omnidirectionally calibrated quantization for large language models.\" arXiv preprint arXiv:2308.13137 (2023)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LT4mj2Fp3L", "forum": "XAXT7A8EWh", "replyto": "XAXT7A8EWh", "signatures": ["ICLR.cc/2026/Conference/Submission12600/Reviewer_Ddxo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12600/Reviewer_Ddxo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12600/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807249757, "cdate": 1761807249757, "tmdate": 1762923449529, "mdate": 1762923449529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the performance degradation that occur when applying standard Post-Training Quantization (PTQ) methods to video matting models. To solve this, the authors propose PTQ4VM which integrates three key techniques: (1) Block-wise Initial Quantization (BIQ) for more stable and accurate optimization; (2) Global Affine Calibration (GAC) to correct statistical distribution shifts introduced by quantization; (3) Optical Flow Assistance (OFA), which uses temporal priors from adjacent frames to enforce smoothness during the calibration stage."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-drafted, with a clear, logical flow that makes the motivations and contributions easy to follow.\n2. The method delivers consistent accuracy gains over PTQ baselines under multiple bit-widths."}, "weaknesses": {"value": "1. Flow errors (fast motion, occlusion, camera shake) may misguide calibration. The method uses RAFT (accurate but heavy) during calibration—calibration-time compute and wall-clock cost are not reported. Sensitivity to using lighter flow or imperfect flow is not analyzed.\n2. The paper mentions that an \"appropriate block partitioning\" is used for the BIQ stage but does not go into detail about how these blocks are defined or if different partitioning strategies were explored."}, "questions": {"value": "1. The calibration set used in the experiments is quite small (256 images). How sensitive is the performance of PTQ4VM to the size and content of the calibration set? Would using a larger or more diverse calibration set lead to further improvements?\n2. Does OFA improve or degrade performance in scenarios with heavy occlusions or strong camera motion? Could you share failure cases and quantitative breakdowns?\n3. Are there constraints or kernel support issues on common deployment backends (TensorRT, TFLite, CoreML) for W4A4?\n4. Have you evaluated on additional video matting datasets or different frame rates/resolutions to test robustness to distribution shifts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8L2T1CvuXR", "forum": "XAXT7A8EWh", "replyto": "XAXT7A8EWh", "signatures": ["ICLR.cc/2026/Conference/Submission12600/Reviewer_PqqH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12600/Reviewer_PqqH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12600/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883155065, "cdate": 1761883155065, "tmdate": 1762923448998, "mdate": 1762923448998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an approach for post-training quantization tailored for video matting models. The task is to compress a pre-trained video matting model from full 32-bit precision to 8- or 4-bit while retaining as much quality as possible. The authors propose improvements over prior approaches:\n- A global affine calibration optimization to correct errors stemming from fusing batch normalization into weights before quantization. The quantization process introduces errors and shifts expected feature statistics, causing growing error in subsequent layers, and global affine calibration does global optimization to combat those shifts.\n- An optical-flow-based alpha warping motion consistency loss to better guide the quantization process to improve video matting quality.\n\nThe authors opt for a block-wise quantization granularity to balance quality with significant memory requirements of video matting models.\nThe evaluation across two video matting datasets and three quantization schemes shows that the proposed approach consistently outperforms existing ones in the quality of the resulting model. An extra evaluation on MatAnyone shows that the proposed method generalizes across the base model architectures."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors clearly explain the approach and the motivation of different components.\n2. The evaluations show that the proposed method convincingly outperforms the existing ones.\n3. The authors provide extensive ablation studies in the appendix.\n4. The quantization problem is important, especially for use cases like mobile video conferencing that uses video matting for the camera feed."}, "weaknesses": {"value": "Major weaknesses:\n1. The proposed optical-flow-based motion compensated alpha loss is hardly original. Video matting methods have been using it for their training for a while, so it’s a natural component to try in a quantization method tailored for video matting models.\n2. The evaluation is limited to one video matting model (RVM), plus the second one (MatAnyone) in a limited evaluation in the appendix. Evaluating on more video matting methods would allow to more confidently judge the generalizability of the proposed quantization approach.\n3. It would be good to see a subjective evaluation in addition to objective metric evaluation. The superiority of the quantized model to the FP32 model on some setups, as pointed out by the authors, suggests the limitations of the objective metrics in question.\n\nMinor weaknesses:\n1. Spaces are missing in many places around citations, e.g. line 54 “quantizationJacob”, line 58 “(2023)(QAT)”, line 80 “layersIoffe” etc.\n2. Figures 1 and 2 are not referenced in the text."}, "questions": {"value": "See weaknesses. Would be good to see:\n- Evaluation on more video matting methods.\n- Subjective evaluation.\n- Fix formatting errors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YKMV8XVa5v", "forum": "XAXT7A8EWh", "replyto": "XAXT7A8EWh", "signatures": ["ICLR.cc/2026/Conference/Submission12600/Reviewer_qJJp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12600/Reviewer_qJJp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12600/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927187169, "cdate": 1761927187169, "tmdate": 1762923448499, "mdate": 1762923448499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}