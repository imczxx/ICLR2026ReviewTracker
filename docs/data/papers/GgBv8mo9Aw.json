{"id": "GgBv8mo9Aw", "number": 8334, "cdate": 1758078941896, "mdate": 1759897791073, "content": {"title": "When Students Surpass Teachers: Hypergraph-Aware Knowledge Distillation with Spectral Guarantees", "abstract": "Many real-world systems involve complex many-to-many relationships naturally represented as hypergraphs, from social networks to molecular interactions. While hypergraph neural networks (HGNNs) have shown promise, existing attention mechanisms fail to handle hypergraph-specific asymmetries between node-to-node, node-to-hyperedge, and hyperedge-to-node interactions, leading to suboptimal structural encoding. We introduce \\textbf{CuCoDistill}, a novel framework that challenges fundamental assumptions in knowledge distillation by demonstrating that student models can systematically outperform their teachers through hypergraph-aware adaptive attention with provable spectral guarantees. Our approach features: (1) set-aware attention fusion that handles variable-sized hyperedge sets with approximation error bounds of $\\epsilon\\sqrt{|\\mathcal{V}|}\\max_i|\\mathcal{E}_i|$; (2) co-evolutionary unified architecture where teacher and student jointly discover structural patterns in a single forward pass; and (3) theoretically-grounded curriculum distillation based on hypergraph spectral properties. We prove that when student's constrained attention aligns with the hypergraph's intrinsic spectral dimension, superior generalization emerges through beneficial regularization. Extensive experiments across nine benchmarks show our students achieve up to 1.8\\% higher accuracy than teachers while delivering 6.25× inference speedup and 10× memory reduction, consistently outperforming state-of-the-art methods and establishing new efficiency-performance frontiers for hypergraph learning.", "tldr": "An asymmetric contrastive scheme where only the teacher processes both clean and perturbed views, fusing them via a learnable gating mechanism to produce high‐quality distillation targets.", "keywords": ["Hypergraph Learning", "Attention", "knowledge distillation", "Co-Distillation"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aefb360b4c7652e5baa0cee7344e6461c6187db6.pdf", "supplementary_material": "/attachment/1bd7f2736279c2ad4aa1659318cc0173945ee526.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a knowledge distillation (KD) method for learning on hypergraphs where a compact 'student' and a high-capacity 'teacher' hypergraph neural network are trained together so that they learn from each other at the same time. Through a step-by-step process guided by the hypergraph's spectral properties, the student's constrained attention mechanism acts as a beneficial regulariser, filtering noise and helping both models focus on essential structural patterns. This approach enables the student model to not only be significantly more efficient but to also provably outperform its teacher on large-scale and noisy datasets, challenging traditional assumptions in knowledge distillation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  This work reframes KD as a powerful regularization mechanism, proving both theoretically (Theorem 2) and empirically that a constrained student can systematically generalize better than its unconstrained teacher under specific, predictable conditions (e.g., noisy, feature-redundant data).\n2. There are formal guarantees for (i) spectral preservation of the attention mechanism, (ii) convergence under co-evolution + curriculum, and (iii) generalization benefits from curriculum with reduced hypothesis complexity; plus a complexity corollary that explains the measured efficiency. These align tightly with the architectural choices.\n3. The paper evaluates on nine diverse hypergraph datasets and provides robustness (feature/structural/label noise), sensitivity analyses (e.g., K-factor, temperature), and scaling studies (time/memory vs. N), which substantiate both accuracy and efficiency claims."}, "weaknesses": {"value": "1. All main results and ablations target node classification. That limits external validity for hypergraph tasks where higher-order relations matter most (e.g., hyperedge/link prediction, group recommendation, set expansion). Actionable: add at least one hyperedge prediction benchmark (e.g., on DBLP/IMDB subsets) and one inductive split to test transfer. Even a single well-designed hyperedge task would strengthen the “hypergraph-aware” claim.\n2. The ablation study in Table 3 is performed on three datasets where the student model is either superior or nearly on par with the teacher. To provide a more complete picture, it would be highly insightful to include an ablation study on a clean, well-structured dataset where the teacher clearly dominates (e.g., CC-Cora or DBLP-Conf). This would help answer a key question: Do components like co-evolutionary training still offer significant benefits (e.g., faster convergence) even when the final student accuracy doesn't surpass the teacher? This would strengthen the case for the framework's general utility beyond the specific 'student-superiority' scenario."}, "questions": {"value": "1. Under what conditions is the Frobenius-norm approximation bound in Theorem 1expected to be tight in practice?\n2. The curriculum combines time-varying quantiles and loss-weight schedules. Which individual component contributes most to stability and performance?\n3. In the set-level attention, which elements are learned versus fixed normalization?\n4. For dense regimes (e.g., IMDB), what preprocessing affects hyperedge size/degree distributions, and how might this interact with K-sparsification in evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "96iAmrzwbP", "forum": "GgBv8mo9Aw", "replyto": "GgBv8mo9Aw", "signatures": ["ICLR.cc/2026/Conference/Submission8334/Reviewer_MzmR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8334/Reviewer_MzmR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761034056599, "cdate": 1761034056599, "tmdate": 1762920256140, "mdate": 1762920256140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper claims to improving the performance of Hyper-Graph Neural Network by designing the attention mechanism for hyper-graph asymmetries, introducing constrained attention, and creating a co-evolve training mechanism."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "None."}, "weaknesses": {"value": "1. The organization is poor, which is confusing and hard to follow.\n\n2. The writing is poor and some of the key explanations are missed, for example, what is 'structural inductive bias'?\n\n3. The definition of the hyper-graph is confusing. For the hyper-edges, is it denotes the edges between nodes? Or the edges between hyper-nodes?\n\n4. Figure.1 is confusing and hard to understand. There are lots of meaningless texts with emphasis, such as the Unified Backbone. Moreover, the presentation of the data flow is a disaster, which is hard to understand. \n\n5. The annotations is confusing, e_i and e_j are used as the feature of nodes, however, the e is said to belongs to the edge set in the very initial definition.\n\n6. The proof of Theorem 1 is meaningless. There is no explanation where A_ours comes from and how the bound is computed.\n\n7. There are lots of unexplained variables, such as the w_i in Eq.(10).\n\n8. Lacks of discussion with related work, such as 'Distilling Knowledge from Graph Convolutional Networks. CVPR 2020', which is highly relative with this paper."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wHERsueiM9", "forum": "GgBv8mo9Aw", "replyto": "GgBv8mo9Aw", "signatures": ["ICLR.cc/2026/Conference/Submission8334/Reviewer_Swub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8334/Reviewer_Swub"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837673360, "cdate": 1761837673360, "tmdate": 1762920255671, "mdate": 1762920255671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an improved knowledge distillation framework for better hypergraph learning. The authors first point out some limitations or challenges of existing techniques, including the shortcomings of prior hypergraph attentions and the gap between distillation and hypergraph learning tasks. The proposed distillation framework contains three different parts. Part 1 focuses on improving the hypergraph attention via an adaptive multi-scale fusion (combining node-node and node-hyperedge attentions) to support a more comprehensive knowledge extraction (both global and local interactions). It also solves the variable-size challenge of hyperedges. The authors give a theorem to guarantee that this part can encode intrinsic knowledge of the vanilla hypergraph by bounding the gap between the proposed attention matrix and an ideal one. Part 2 proposes a co-trained teacher-student distillation framework, where the teacher is an attention-based GNN and the student is its dynamic top-k sparse variant. This part incorporates both attention alignment and embedding alignment for better performance. In this part, the authors also provide a theorem, showing that when K is greater than the effective spectral dimension of the vanilla hypergraph, the student can approach the teacher in a large probability. Part 3 incorporates contrastive learning and curriculum learning to further improve the above framework, where the authors use contrastive and distillation gaps to design a “difficulty” score for their curriculum, supporting easy-to-hard learning. The experiments are generally comprehensive, including performance comparisons among nine benchmarks from different domains, ablation studies, teacher-student comparisons, running time and memory comparisons. These experimental results show that the proposed framework can achieve better performance with reduced time and memory costs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1)\tBased on some experimental results, the proposed framework effectively improves the performance of hypergraph learning tasks, with reduced time and memory costs, among some benchmarks from different domains.\n\n2)\tThe authors provide theorems to show that their framework can learn intrinsic knowledge from the input hypergraph and the student can approach the teacher when K is large, showing that the framework has some theoretical merits.\n\n3)\tThe presentation of their specific methodology designs is clear (with clear mathematical formulas), which makes their method understandable.\n\n4)\tThe ablation studies are detailed and comprehensive."}, "weaknesses": {"value": "1)\tThe analysis of the ablation studies is missing. Please check Line 265. The submitted manuscript seems incomplete.\n\n2)\tThe title is misleading. After carefully reading the main text, from my point of view, the central aim of this paper is to propose a framework to improve the hypergraph learning performance, rather than study whether, when, and why the student can surpass the teacher (a critical question in the knowledge distillation domain). Thus, the title of this paper is very misleading, giving a sense that the authors propose a hypergraph-based solution to solve the above-mentioned general question in the knowledge distillation domain. While the authors attempt to discuss the student-teacher relationship regarding hypergraph tasks in this paper, it is not enough to highlight that contribution in the title.\n\n3)\tThe main motivation is unclear. The proposed framework contains three parts. Part 1 focuses on improving hypergraph attention. Part 2 focuses on a co-trained distillation framework. Part 3 focuses on a contrastive curriculum. From my point of view, Part 2 is directly related to the main aim of this work, since the authors attempt to use distillation to improve hypergraph learning. However, the motivation of incorporating Part 1 and Part 3 into the comprehensive framework is unclear. While they have merits and benefits in performance gain, whether the distillation must rely on them remains confusing. Thus, incorporating them significantly harms the generality and effectiveness of Part 2 and makes the holistic framework heavy. In summary, the three parts focus on different challenges, and do not align with the same main motivation. Besides, regarding distillation itself, why distillation matters in hypergraph learning still requires further elaboration.\n\n4)\tThe novelty seems limited. First, are the authors the first to introduce distillation (the main idea) to hypergraph learning? Second, the holistic distillation framework has three parts. According to the main text, I see limited novelty in each of them. For example, Part 1 combines local and global knowledge, which seems common in graph transformers. The embedding distillation and attention distillation in Part 2 can also be found in graph transformers or GNNs. Part 3 is interesting in defining a “difficulty” score via gaps for a curriculum. Yet, based on the structure of the paper presentation, it is not a main contribution of this paper. And I think the novelty still needs to be highlighted by contrasting it with some prior existing curriculum designs related to distillation or contrasting learning. In summary, the authors should clearly state which component is novel and highlight it with enough support.\n\n5)\tBoth co-trained distillation and sequential distillation have their own merits. The authors should clearly point out that the former one requires more memory."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LpldHDfNqF", "forum": "GgBv8mo9Aw", "replyto": "GgBv8mo9Aw", "signatures": ["ICLR.cc/2026/Conference/Submission8334/Reviewer_EjVN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8334/Reviewer_EjVN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982201448, "cdate": 1761982201448, "tmdate": 1762920254974, "mdate": 1762920254974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate a knowledge distillation framework for hypergraph neural networks (HNNs).\n\nTo this end, the authors introduce CuCoDistill, an attention-based distillation framework for HNNs.\n\nCuCoDistill combines contrastive learning and an attention mechanism to distill the teacher's knowledge to the student model in an effective manner.\n\nThrough experiments, the authors demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- S1. The authors conduct an in-depth analysis of the hypergraph density, which provides important insight into the use case.\n\n- S2. Embedding the similarity of the teacher model and student model is interesting."}, "weaknesses": {"value": "- **W1 [Theory]** While the authors present several theoretical results, I think the statements are not formal enough. For instance, what does it mean by structural encoding? Moreover, to my understanding, the attention matrix is a learnable component that is derived from the model output. Then, how can this be used for theoretical analysis, given that the learning process of the attention matrix depends on the model hyperparameters and training configurations?\n\n- **W2 [Research goal]** The authors criticize the limitations regarding the current usage of contrastive learning and attention mechanisms within the HNN domain. However, I cannot understand why the teacher-student-based distillation framework overcomes this limitation. What is the key research question of this work? Is it proposing a new HNN design or proposing a new distillation method? The key research question and its presentation should be further improved.\n\n- **W3 [Baselines]** The method only includes outdated HNNs as baselines, which were published in 2019. The authors need to compare the proposed method with more recent HNNs, such as [1, 2, 3].\n\n- **W4 [Incomplete manuscript]** The writing of Section 3.1 is incomplete\n\n- **[References]**\n  - [1] Chien et al., You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks, ICLR 2022\n  - [2] Wang et al., Equivariant Hypergraph Diffusion Neural Operators, ICLR 2023\n  - [3] Wang et al., From hypergraph energy functions to hypergraph neural networks, ICML 2023"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5fLZr8Pk65", "forum": "GgBv8mo9Aw", "replyto": "GgBv8mo9Aw", "signatures": ["ICLR.cc/2026/Conference/Submission8334/Reviewer_SoKY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8334/Reviewer_SoKY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990429599, "cdate": 1761990429599, "tmdate": 1762920254029, "mdate": 1762920254029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CuCoDistill, a highly novel and complex framework for knowledge distillation (KD) in Hypergraph Neural Networks (HGNNs). The authors address the failure of existing HGNN attention mechanisms to handle hypergraph asymmetries and the limitations of standard KD in preserving higher-order structures . The framework's core innovations include: (1) a hypergraph-aware adaptive attention mechanism with provable spectral guarantees; (2) a unified co-evolutionary architecture where teacher and student models train simultaneously rather than sequentially ; and (3) a spectral curriculum scheduler that dynamically adjusts learning difficulty based on hypergraph properties. The paper theoretically and empirically demonstrates the counter-intuitive finding that, under certain conditions (e.g., noisy datasets), the compressed student model can systematically outperform the larger teacher model ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework is innovative, particularly its \"co-evolutionary\" architecture and the theoretical demonstration that a student model can surpass its teacher. \n\n\n\n2. The work is theoretically deep, providing provable guarantees for its attention mechanism (Theorem 1) and formalizing the conditions for student superiority (Theorem 2) , lending rigor to its claims.\n\n\n\n\n\n3. The empirical results are good, showing state-of-the-art performance, efficiency gains (6.25x speedup, 10x memory reduction), and, crucially, validating the \"student surpasses teacher\" phenomenon on several large-scale, noisy datasets."}, "weaknesses": {"value": "1. The framework's complexity is extremely high, potentially hindering reproducibility and adoption. It integrates multiple complex components (multi-scale attention, co-evolution, spectral curriculum, multi-level KD losses ), creating a system that is very difficult to implement and tune.\n\n2. The claimed \"student superiority\" is highly conditional and not a general outcome. The results clearly show this phenomenon occurs only on large, noisy, or feature-redundant datasets (e.g., DBLP, IMDB, Yelp). On clean, well-structured datasets (e.g., CC-Cora), the teacher model remains superior, a critical nuance that limits the generality of the titular claim.\n\n3. The method introduces a very large number of new hyperparameters. The spectral curriculum (adaptive thresholds, loss weights $\\lambda(t)$) , attention mechanism (Top-K $\\alpha$) , and various loss component weights create a complex tuning space, even with the sensitivity analysis provided in the appendix."}, "questions": {"value": "1. The ablation study shows the \"Spectral Curriculum\" has the smallest individual impact (0.9-1.1%). Given its complexity (calculating dual difficulties, quantile thresholds), is this component truly necessary, or could a simpler regularization suffice?\n\n2. In the t-SNE analysis (Figure 4, ), the student embedding space for DBLP shows a worse silhouette score (0.327) than the teacher (0.614), yet the student model outperforms the teacher on the DBLP task (Table 1). This is counter-intuitive. Could the authors explain why degraded cluster quality in the embedding space leads to better classification accuracy in this case?\n\n\n3. There is a citation error in the baseline description (Section D.2.2). The text cites (Zhang et al., 2019b) for Hyper-SAGNN but then describes HyGCL-AdT (Qian et al., 2024) . This should be corrected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tkf7lkEQQj", "forum": "GgBv8mo9Aw", "replyto": "GgBv8mo9Aw", "signatures": ["ICLR.cc/2026/Conference/Submission8334/Reviewer_MvEx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8334/Reviewer_MvEx"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762161095244, "cdate": 1762161095244, "tmdate": 1762920253545, "mdate": 1762920253545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}