{"id": "qwA04SZeCa", "number": 2668, "cdate": 1757189553906, "mdate": 1759898134176, "content": {"title": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning", "abstract": "Embodied agents operating in household environments must interpret ambiguous and under-specified human instructions. A capable household robot should recognize ambiguity and ask relevant clarification questions to infer the user intent accurately, leading to more effective task execution. To study this problem, we introduce the Ask-to-Act task, where an embodied agent is tasked with a single or multi-object rearrangement task using an under-specified instruction in a home environment. The agent must strategically ask minimal, yet relevant, clarification questions to resolve ambiguity while navigating under partial observability. To address this challenge, we propose a novel approach that fine-tunes multi-modal large language models (MLLMs) as vision-language-action (VLA) policies using online reinforcement learning (RL) with LLM-generated rewards. Our method eliminates the need for large-scale human demonstrations or manually engineered rewards for training such agents. We benchmark against strong zero-shot baselines including GPT-4o as well as supervised fine-tuned MLLMs on our task. Our results show that our RL-finetuned MLLM outperforms all baselines by a significant margin ($10.4$-$16.5\\%$), generalizing well to novel scenes and tasks. To the best of our knowledge, this is the first demonstration of adapting MLLMs as VLA agents that can act and ask for help using LLM-generated rewards with online RL.", "tldr": "", "keywords": ["Embodied AI", "Reinforcement Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da99197d315e89ab526d2f3ded05efeb5790dfd1.pdf", "supplementary_material": "/attachment/81f03309c81efd455ea2168a6fa6930d3a017f21.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces ASK-TO-ACT, a benchmark and training paradigm for embodied agents that can recognize ambiguous, under-specified human instructions and ask clarifying questions before acting. To solve this challenge, the authors propose AUTOASK, a RL approach that fine-tunes multimodal large language models (MLLMs), specifically LLaVA-OneVision, into vision-language-action (VLA) policies capable of both acting and asking. Unlike imitation learning or manually engineered reward pipelines, AUTOASK employs LLM-generated dense rewards that evaluate the agent’s question quality and progress toward disambiguation. Experiments in the Habitat 3.0 simulator with 83 ReplicaCAD scenes show that AUTOASK outperforms strong zero-shot baselines (e.g., GPT-4o with ReAct) and supervised fine-tuned models by 10.4–16.5% in success rate and 8.8–20.0% in ambiguity-resolution efficiency on unseen scenes and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- ASK-TO-ACT introduces a realistic and underexplored problem, training embodied agents to ask clarification questions, bridging human-robot interaction and multimodal reasoning.\n\n- The use of LLM-generated per-step rewards provides a scalable alternative to manual reward engineering, enabling nuanced supervision for reasoning about ambiguity.\n\n- Results span multiple baselines (zero-shot, few-shot, SFT) and metrics (success rate, ARS, question ratio), showing consistent gains across unseen scenes and unseen tasks.\n\n- Architecture and RL training details are presented with explicit design choices (e.g., Perceiver downsampling for long visual histories, DD-PPO optimization)."}, "weaknesses": {"value": "- Results are limited to the Habitat simulator; no real-robot validation or human-in-the-loop evaluation is presented, leaving open questions about real-world transfer.\n\n\n- The LLM reward model accesses ground-truth environment state, which is unavailable in real deployments, potentially overstating feasibility.\n\n\n- There is little discussion of the variance or consistency of the reward signals produced by the LLM, which is critical for RL convergence."}, "questions": {"value": "- How would AUTOASK perform with real human responses (which can be noisy, incomplete, or contradictory) instead of simulated LLM-generated answers?\n\n- How stable are LLM-generated rewards across different prompts or LLM variants (e.g., Llama-3 vs GPT-4)? Could reward inconsistency destabilize policy learning?\n\n- Given reliance on privileged simulator state during training, what adaptations would be required to deploy this system on a real embodied platform like Spot or Stretch?\n\n- What are the common failure modes, e.g., over-questioning, irrelevant clarifications, or ambiguous follow-ups—and how might the model align its questioning strategy to human tolerance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zc23HLEmBh", "forum": "qwA04SZeCa", "replyto": "qwA04SZeCa", "signatures": ["ICLR.cc/2026/Conference/Submission2668/Reviewer_nK4L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2668/Reviewer_nK4L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760724860496, "cdate": 1760724860496, "tmdate": 1762916325465, "mdate": 1762916325465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces the ASK-TO-ACT task, where an embodied agent performs single- or multi-object rearrangement in a household environment based on an under-specified instruction. Accordingly,  this paper proposes a framework that fine-tunes multimodal large language models through online reinforcement learning guided by the reward generated from a large language model. The goal is to empower agents with the abilities of asking for help with human-in-the-loop feedback. Experiments show promising results on unseen scenes or tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed task, i.e., Ask-to-Act, is interesting. It shifts the focus from purely executing instructions to recognizing ambiguity and asking for help when needed.\n2. Using LLMs to generate rewords is interesting, which can reduce the dependency on expensive manual reward engineering.\n3. Experiments cover unseen scenes and tasks, which is important for embodied agents that need to generalize beyond training trajectories."}, "weaknesses": {"value": "1. Lacking real‐world evaluation.The experiments are conducted in simulated home environments . Real‐world environments are much complex.\n2. While the reward design for “useful” questions is interesting, defining what counts as “useful” can be subtle—especially in real-world environments with more types of ambiguity. The method relies on privileged state information to label questions as useful, which may not hold in real settings.\n3. Leveraging LLMs to generate rewards is interesting. However, it introduces concerns regarding the correctness and the robustness of these rewards—especially in large, complex environments or when the LLM provides incorrect assessments."}, "questions": {"value": "1. How can we know the quality of LLM-generated rewards to errors in object detection or partial observations?\n2. How does the proposed method compared to exploration-based strategies for deciding when to ask?\n3. How does the proposed method work in real-world environments?\n4. Can the agent handle multi-turn clarification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BuWvjVhOAm", "forum": "qwA04SZeCa", "replyto": "qwA04SZeCa", "signatures": ["ICLR.cc/2026/Conference/Submission2668/Reviewer_Tw6h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2668/Reviewer_Tw6h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761647560397, "cdate": 1761647560397, "tmdate": 1762916325327, "mdate": 1762916325327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ASK-TO-ACT, a task where embodied agents must resolve ambiguity in under-specified instructions by asking clarification questions. The authors propose AUTOASK, which fine-tunes multimodal large language models (MLLMs) as vision-language-action (VLA) policies using online reinforcement learning with LLM-generated rewards. The method is evaluated in Habitat 3.0 on tasks requiring agents to fetch and place objects while disambiguating user intent through natural language questions. AUTOASK outperforms zero-shot baselines and supervised fine-tuning approaches by 10.4-16.5% on unseen scenes and tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper demonstrates that subgoal rewards alone are insufficient for learning effective question-asking behavior. This motivates the need for dense, question-specific rewards.\n- The reward ablation study (Table 2) and analysis of question budgets provide valuable insights into the trade-offs between task success and user interaction burden.\n- AUTOASK outperforms both zero-shot GPT-4o baselines and behavior cloning supervised fine-tuning, demonstrating the effectiveness of RL training for this task.\n- The ASK-TO-ACT benchmark includes diverse ambiguity types (attribute recognition, spatial reasoning, object size, compositional ambiguity) that test different reasoning capabilities."}, "weaknesses": {"value": "- The authors claim their LLM-based reward generation is more scalable than manually designed rewards, yet their method requires extensive privileged information including \"task instruction, objects in environment, current location of objects, list of target objects and their current location, and desired placement location.\" This level of privileged access is arguably no more scalable than hand-crafted rewards and would not generalize to real-world settings where such information is unavailable.\n- The agent is limited to 9 predefined question templates, which fundamentally undermines the claim of natural language interaction. The authors acknowledge unconstrained question generation performs poorly but dismiss this limitation by suggesting larger models might help without empirical validation.\n- While the paper shows performance differences between policies trained with different rewards (e.g., subgoal rewards), it provides insufficient analysis of what specific behavioral changes lead to these improvements.\n- The heavy reliance on privileged environment information for reward generation suggests the method would not transfer to other embodied AI tasks without significant manual engineering per domain."}, "questions": {"value": "- Can the authors provide results using the LLM reward model without privileged information to substantiate scalability claims? How would the method perform if the LLM reward model only had access to the same partial observations as the agent, making it truly scalable to real-world deployment?\n- What specific behavioral patterns emerge in agents trained with subgoal rewards versus those with question-specific rewards? Understanding these differences would strengthen the paper's contributions.\n- Have the authors tested larger MLLMs (>7B parameters) with unconstrained question generation as suggested? This is critical for validating whether the approach can scale to more natural interactions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WqWxpYUlkI", "forum": "qwA04SZeCa", "replyto": "qwA04SZeCa", "signatures": ["ICLR.cc/2026/Conference/Submission2668/Reviewer_NUjL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2668/Reviewer_NUjL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939081602, "cdate": 1761939081602, "tmdate": 1762916325167, "mdate": 1762916325167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The papers proposes to use RL to fun-tune the VLA to VLA by LLM-generated rewards. Also, the paper focus on enable the RL tuned VLA to do multi-round ask-reasoning and act by ultilizing online RL."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to read and follow.\n\n2. The problem this paper focus, that under the case that the user give a unclear instruction or goal, the agent need to be able to do ask and reasoning then act in multiple round, is very practical questions in the community, even lots of the current fundation robot model only focus on train simple policies under assumptions that both the environment an dinstruction is determinstic and clear."}, "weaknesses": {"value": "1. The problem this paper focus is practical and meaningful, but potential concerns existed in LLM-generated rewards: Although ASK-TO-ACT aims to model human-robot clarification dialogues, the “user” responses are simulated by an LLM using privileged state. This avoids actual human ambiguity, noise, or miscommunication. Thus, the realism of the claimed human-robot interaction remains limited.\n\n2. The paper only has experiemt on simple sumulated env, but not real env, which for this specific problem, real env is the most emportant: All baselines are either handcrafted LLM agents (ReAct) or simple SFT on synthetic trajectories. In order to support the conclusion, the author might need to consider some baseline like human-feedback RL (RLAIF / RLHF) approaches, fundation models VLAs such as OpenVLA or Pi0, reward-learning baselines like Text2Reward or Eureka-style code-generated rewards applied directly."}, "questions": {"value": "See the above weakness. Besides those, I have further concern that:\n1. WIth the existed experiment design, I feel there still lacks some analysis of why RL with LLM-generated rewards helps. Section 5.3 shows the reward choice will affect the result, but overall lack interpretation of all the design and results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5f2UNH4w20", "forum": "qwA04SZeCa", "replyto": "qwA04SZeCa", "signatures": ["ICLR.cc/2026/Conference/Submission2668/Reviewer_w8Mf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2668/Reviewer_w8Mf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982880808, "cdate": 1761982880808, "tmdate": 1762916325027, "mdate": 1762916325027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}