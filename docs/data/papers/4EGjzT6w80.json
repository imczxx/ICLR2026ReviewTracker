{"id": "4EGjzT6w80", "number": 11298, "cdate": 1758195686823, "mdate": 1763508232572, "content": {"title": "Flow Matching with Semidiscrete Couplings", "abstract": "Flow models parameterized as time-dependent velocity fields can generate data from noise by integrating an ODE.\nThese models are often trained using flow matching, i.e. by sampling random pairs of noise and target points $(x_0,x_1)$ and ensuring that the velocity field is aligned, on average, with $x_1-x_0$ when evaluated along a time-indexed segment linking $x_0$ to $x_1$. \nWhile these noise/data pairs are sampled independently by default, they can also be selected more carefully by matching batches of $n$ noise to $n$ target points using an optimal transport (OT) solver.\nAlthough promising in theory, the OT flow matching (OT-FM) approach (Pooladian et al., 2023, Tong et al., 2024) is not widely used in practice. \nZhang et al. (2025), pointed out recently that OT-FM truly starts paying off when the batch size $n$ grows significantly, which only a multi-GPU implementation of the Sinkhorn algorithm can handle.\nUnfortunately, the pre-compute costs of running Sinkhorn can quickly balloon, requiring $O(n^2/\\varepsilon^2)$ operations for every $n$ pairs used to fit the velocity field, where $\\varepsilon$ is a regularization parameter that should be typically small to yield better results.\nTo fulfill the theoretical promises of OT-FM, we propose to move away from batch-OT and rely instead on a semidiscrete formulation that can leverage the fact that the target dataset distribution is usually of finite size $N$. The SD-OT problem is solved by estimating a dual potential vector of size $N$ using SGD; using that vector, freshly sampled noise vectors at train time can then be matched with data points at the cost of a maximum inner product search (MIPS) over the dataset.\nSemidiscrete FM (SD-FM) removes the quadratic dependency on $n/\\varepsilon$ that bottlenecks OT-FM. SD-FM beats both FM and OT-FM on all training metrics and inference budget constraints, across multiple datasets, on unconditional/conditional generation, or when using mean-flow models.", "tldr": "Propose a new method to train", "keywords": ["flow matching", "optimal transport", "semidiscrete optimal transport"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5c8409fc55c1ea5b184f27d6973e9d0db7502b4a.pdf", "supplementary_material": "/attachment/b6e0147a2d102267bbae8d9a1ec55b009f9815ec.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Semidiscrete Flow Matching (SD-FM), a novel approach to define the noise-to-data coupling in flow matching. The core idea is to perform a offline pre-computation to solve a global, semidiscrete Optimal Transport (OT) problem, which yields a dual potential vector g*. This vector g* implicitly defines a partition of the noise space, where each region in the partition corresponds to a unique data point. During the online FM training phase, each sampled noise vector is coupled to its corresponding data point by performing a Maximum Inner Product Search (MIPS) to identify its region. This \"pre-compute then query\" strategy is designed to improve the efficiency and scalability of OT-guided flow matching. The authors demonstrate significant performance gains on some benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Solid Theoretical Analysis: The paper provides a convergence analysis for the core algorithm, with a systematic comparison with existing optimal transport-based methods. This lends to theoretical importance to the proposed method. \n\n- Clarity of Exposition: The paper is well-written and logically structured. Figure 1, in particular, is highly effective. It intuitively illustrates the core methodological differences between I-FM, OT-FM, and the proposed SD-FM, allowing readers to quickly grasp the paper's central contribution."}, "weaknesses": {"value": "- Limited Experimental Scope: The method was only evaluated on two small-scale benchmarks. This is a significant omission, as the problem of computational inefficiency‚Äîwhich the paper claims to solve‚Äîis most critical on large-scale, high-dimensional datasets like ImageNet 256. The absence of such experiments makes it difficult to validate the method's scalability and practical utility.\n\n- Limited Baseline Comparisons: The set of baseline methods is narrowly restricted to only two types of flow-matching models. The paper lacks a comparison against current state-of-the-art (SOTA) generative models in the related domain. To properly contextualize its contributions, the method must be benchmarked against SOTA approaches in terms of both generation quality and efficiency.\n\n- Insufficient Analysis of the Key Hyperparameter Œµ: The vast majority of experiments are conducted with Œµ=0 (hard assignment). The parameter Œµ controls the stochasticity of the coupling (soft vs. hard assignment) and is highly likely to directly influence the diversity of the generated model. The paper lacks any investigation into how Œµ mediates the trade-off between generation quality (e.g., FID/precision) and diversity (e.g., recall)."}, "questions": {"value": "In table 2, the generation quality is better at $\\epsilon = 0$, while some theoretical results are only valid when $\\epsilon > 0$, e.g., Theorem 2, Proposition 3. Could you provide some explanation for this phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eC2yuvCCKE", "forum": "4EGjzT6w80", "replyto": "4EGjzT6w80", "signatures": ["ICLR.cc/2026/Conference/Submission11298/Reviewer_WFDC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11298/Reviewer_WFDC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489265317, "cdate": 1761489265317, "tmdate": 1762922444053, "mdate": 1762922444053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To overcome the computational cost but fulfill the theoretical promises of OT-FM, this paper leverages the entropy regularized semidiscrete OT (SD-OT) and proposes a SD-OT coupling. In the coupling stage, the SD-OT problem is solved by estimating the dual potential $g$ by SGD; then in the FM training stage, noise samples are pared to data samples by maximum inner product search (MIPS, when $\\epsilon = 0$) or SoftMax (when $\\epsilon > 0$), using the learned $g$. They also provide the convergence analysis of SGD for SD-OT. The experiments using ImageNet, PetFace and CelebA show that the proposed SD-OT coupling is better than I-FM and OT-FM in main metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-motivated by practical drawbacks of OT-FM, and the general coupling method SD-FM can potentially integerate into many diffusion-/ flow-based models. The convergence analysis justify the case for both $\\epsilon = 0$ and $\\epsilon > 0$. The validity of unregularized $\\epsilon = 0$ case avoids the choice of tuning parameter."}, "weaknesses": {"value": "1. The paper mainly adapts SD-OT from Genevay et al., 2016 in FM setting, which may raise some novelty issues.\n2. The scalabililty of SD-OT precompucate for large $N$ remains a concern.\n3. In section 2 (Background and Related Work), are \"$\\ldots$\" in subtites typos?"}, "questions": {"value": "1. Although $\\epsilon = 0$ is valid, how sensitive are results to $\\epsilon$? Also, what's the sensitivity for fitted $g$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OX5IVa8KZU", "forum": "4EGjzT6w80", "replyto": "4EGjzT6w80", "signatures": ["ICLR.cc/2026/Conference/Submission11298/Reviewer_JxQX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11298/Reviewer_JxQX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761490356256, "cdate": 1761490356256, "tmdate": 1762922443392, "mdate": 1762922443392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Semidiscrete Flow Matching (SD-FM), a novel approach that addresses the computational bottlenecks of Optimal Transport Flow Matching (OT-FM). By reformulating the noise-data pairing problem as a semidiscrete optimal transport problem from a continuous noise distribution to a discrete dataset, SD-FM fits a dual potential vector via SGD during a precomputation phase. This allows each newly sampled noise vector to be matched to a data point at training time through a simple maximum inner product search, effectively eliminating the quadratic dependency on batch size that plagues batch-OT methods. Extensive experiments demonstrate that SD-FM outperforms both standard FM and OT-FM across various datasets and training settings, achieving superior results in unconditional/conditional generation tasks under all tested inference budget constraints."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a fundamentally different approach to optimal transport in flow matching by leveraging the semidiscrete formulation, effectively circumventing the quadratic complexity that has limited practical adoption of OT-FM methods.\n- Beyond the algorithmic contribution, the work provides thorough theoretical analysis including convergence guarantees for both regularized and unregularized settings and extends the Tweedie formula to the semidiscrete case, demonstrating strong mathematical rigor.\n- The experimental design covers diverse scenarios including unconditional/conditional generation, super-resolution, and mean-flow models, with consistent improvements across different inference budgets, particularly notable in low-NFE regimes.\n- The decoupling of precomputation from training, combined with efficient MIPS-based matching, offers a viable path for scaling OT-based methods to larger datasets while maintaining performance benefits."}, "weaknesses": {"value": "- While the 12-hour precomputation on 2.56M samples is reasonable, the paper lacks systematic analysis of how the method scales to modern large-scale datasets (e.g., >10M samples). The O(N) training cost, though better than quadratic, may still become prohibitive for massive datasets.\n- The comparison framework omits important contemporary approaches for improving flow straightness (e.g., Reflow, MinibatchOT variants), making it difficult to assess SD-FM's relative advantages in the broader landscape of efficient flow matching methods.\n- The paper provides limited insights into hyperparameter selection, particularly regarding the regularization parameter Œµ. Practical recommendations for choosing Œµ based on dataset characteristics (dimensionality, size, noise levels) would significantly enhance reproducibility and usability.\n- The experimental validation focuses primarily on lower-resolution datasets, leaving open questions about the method's effectiveness on high-resolution generation tasks (e.g., 256√ó256 and above) that are increasingly relevant in practical applications."}, "questions": {"value": "See the \"Weakness\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8LeYlXP9uE", "forum": "4EGjzT6w80", "replyto": "4EGjzT6w80", "signatures": ["ICLR.cc/2026/Conference/Submission11298/Reviewer_rkmY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11298/Reviewer_rkmY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791461387, "cdate": 1761791461387, "tmdate": 1762922442894, "mdate": 1762922442894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Semidiscrete Flow Matching to tackle the training computational cost of OT-FM. It studies a semidiscrete OT problem from a continuous noise to a discrete data distribution. This is solved in two stages: a one-time precomputation of a dual potential vector, followed by a MIPS lookup during training to pair noise with data. Theoretical convergence analysis and empirical results show SD-FM beats I-FM and matches OT-FM at a fraction of the computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of reframing OT-FM as a continuous-to-dataset semidiscrete problem is novel and practically efficient. The associated convergence criterion is also a new contribution.\n2. The paper is well-written. The motivation is clear. Figures and tables effectively communicate the method and its advantages."}, "weaknesses": {"value": "1. The paper alludes to fast approximate MIPS but does not investigate the trade-off between approximation error and final model quality, measured by FID. For large $N$, what is the impact of the approximation error from MIPS?\n2. The paper lacks validation on high-dimensional data. Current diffusion models often denoise in latent spaces, which have high data dimensions."}, "questions": {"value": "1. Figure 5 presents a nice analysis of diversity control through guidance samples. Could you provide more extensive analysis on how different parameters affect the sample diversity?\n2. When applied to datasets of billions of samples, what is the additional memory overhead of SD-FM compared to I-FM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dsoHapVWTt", "forum": "4EGjzT6w80", "replyto": "4EGjzT6w80", "signatures": ["ICLR.cc/2026/Conference/Submission11298/Reviewer_48uQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11298/Reviewer_48uQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897771615, "cdate": 1761897771615, "tmdate": 1762922442612, "mdate": 1762922442612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Semidiscrete Flow Matching to tackle the training computational cost of OT-FM. It studies a semidiscrete OT problem from a continuous noise to a discrete data distribution. This is solved in two stages: a one-time precomputation of a dual potential vector, followed by a MIPS lookup during training to pair noise with data. Theoretical convergence analysis and empirical results show SD-FM beats I-FM and matches OT-FM at a fraction of the computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of reframing OT-FM as a continuous-to-dataset semidiscrete problem is novel and practically efficient. The associated convergence criterion is also a new contribution.\n2. The paper is well-written. The motivation is clear. Figures and tables effectively communicate the method and its advantages."}, "weaknesses": {"value": "1. The paper alludes to fast approximate MIPS but does not investigate the trade-off between approximation error and final model quality, measured by FID. For large $N$, what is the impact of the approximation error from MIPS?\n2. The paper lacks validation on high-dimensional data. Current diffusion models often denoise in latent spaces, which have high data dimensions."}, "questions": {"value": "1. Figure 5 presents a nice analysis of diversity control through guidance samples. Could you provide more extensive analysis on how different parameters affect the sample diversity?\n2. When applied to datasets of billions of samples, what is the additional memory overhead of SD-FM compared to I-FM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dsoHapVWTt", "forum": "4EGjzT6w80", "replyto": "4EGjzT6w80", "signatures": ["ICLR.cc/2026/Conference/Submission11298/Reviewer_48uQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11298/Reviewer_48uQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897771615, "cdate": 1761897771615, "tmdate": 1763636171270, "mdate": 1763636171270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to Reviewers and AC"}, "comment": {"value": "We thank all reviewers for their time and feedback. We thank the AC and SAC for handling our paper.\n\n**Here is a summary of the salient points we addressed in the rebuttal:**\n\n* **SOTA methods were already present in our initial draft**: Our contribution is on improving the *noise-data coupling* component of flow matching (`L.41`) . \n  *  We already compared at great lengths with the **SOTA** method of providing such couplings (using Sinkhorn on large batches of $n\\approx 500k$). We provide timing results that were not available in [Zhang et al,25], and show that our algorithm clearly dominate OTFM in both compute cost (time per pair, `x-axis`) and performance (FID, `y-axis`), in `Fig. 4, 7, 8`. \n  * Additionally, we had already shown that SD-OT couplings can be added to, and improve, a most recent **SOTA** approach (MeanFlow, Oral @ Neurips 25) in low-NFE unconditional generation in `Fig. 6`.\n    \n* **Dimensionality of datasets covered in experiments**: Our experiments covered datasets as large as ImgN-64x64 in pixel space ($d=64 \\times 64 \\times 3 = 12,288$), and ImgN-256x256 in latent space ($d=32 \\times 32 \\times 4 = 4096$). While one might be tempted to discount ImgN-64 as low-resolution, it has, in fact, *higher dimension* than latent space ImgN-256 used e.g. in MeanFlow. We focus on *both* pixel and latent space generation because pixel space is known to be more challenging. Using pixel space, we can evaluate progress on the FM training pipeline, ablating the role of the encoder/decoder. We do of course advocate using latent space in for large data spaces, and, remarkably, we believe SD-OT converges faster in latent-space (thanks to the Gaussian-like structure of the prior), as illustrated in a new Figure 23.\n\n* **Ablating the role of ùúÄ**: Many of our results already ablated ùúÄ. They point to the benefit of choosing ùúÄ as small as possible, and we recommend end-users to choose $ùúÄ=0$ as default in `L. 471`. We have added a series of parameter wide ablations ($ùúÄ, ùúè$, cost) in `Fig. 19~22` for ImgN-64 that confirm the superiority of using a SDOT coupling that is sharp (ùúÄ=0), recovers data marginal (low ùúè) and is optimal w.r.t squared Euclidean cost, for better FID / curvature.\n    \n* **Scalability to large dataset size $N$**: Our current implementation handled  $N$=2.5M in $d=12k$. $N$ can be increased further using PCA to reduce dimension. Specifically, the cost of computing $\\mathbf{g}$ scales linearly with $N$ (dataset size), $K$ (# of SGD iterations), and $d$ (dataset dimension) as $KNd$. Any reduction in $d$ via PCA unlocks the ability of using large $N$. Note that this comes at no cost to the performance of the flow model, which is still trained in the full-dimensional space. For very large datasets, we recommend splitting them into *mega-batches* of a size that would depend on dimension (could be e.g. $10\\sim 50M$) and compute the dual vector for each mega-batch independently.\n\n**Additions  to our revision include the following items:**\n\n* **Positioning of our theoretical contributions on SD-OT** `L.192-197` clarify the novelty of our results relative to Genevay et al., 2016, highlighting the importance of the unbiased convergence criterion and the theoretical analysis of SGD.  \n* **Clarified that MIPS is exact when using ùúÄ = 0**. `L.286 `. In that case, the task of assigning noise to data amounts to an `argmax` lookup. We clarify that our results all used _exact_ lookup. Approximations could bring this to sublinear time but are left for future work.\n* **Clarification of datasets for Experiments `Section 5`** Dataset names are included in subsection headings, emphasizing that our experimental results cover unconditional and conditional generation, superresolution, guidance, and one-step generation, and a range of datasets and resolutions up to 64x64 in pixel space and 256x256 in latent space.  \n* **Recommendation on the choice of ùúÄ** In `L.470-472` we are now more upfront about our recommendation to pick $ùúÄ = 0$ by default since this effectively eliminates the hyperparameter and is equivalent to solving for exact semidiscrete OT.\n* **Additional experimental results** To answer points raised by reviewers, we have added new experiments and ablations to the Appendix (`L.1782` and after).  \n  * `Fig 17`, we show the stability of the optimal dual potential $\\mathbf{g}_ùúÄ^\\\\star$ to the choice of $ùúÄ \\geq 0$ in the case of ImgN64. These results support our earlier conclusions, namely that $ùúÄ = 0, 0.01$ differ minimally, while $ùúÄ = 0.1$ has a mild impact on the solution.  \n  * `Fig 18`, we provide an additional illustration on how the selection frequency of images from the ImageNet64 ($N$ = 2.5M) changes as we solve OT. This shows that images are sampled more and more evenly as we get closer to the OT solution.\n  * `Figs 19, 20, 21, 22`, we show ablations on (i) the convergence criterion ùúè for solving SD-OT, (ii) the choice of cost, and (iii) the choice of ùúÄ."}}, "id": "ew0Kp9YkKd", "forum": "4EGjzT6w80", "replyto": "4EGjzT6w80", "signatures": ["ICLR.cc/2026/Conference/Submission11298/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11298/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission11298/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763748863126, "cdate": 1763748863126, "tmdate": 1763748863126, "mdate": 1763748863126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}