{"id": "Ii4UcmxbmJ", "number": 16911, "cdate": 1758270176934, "mdate": 1763671702715, "content": {"title": "K^*-means: a parameter-free clustering algorithm", "abstract": "Clustering is a widely used and powerful machine learning technique, but its effectiveness is often limited by the need to specify the number of clusters, k, or by relying on thresholds that implicitly determine k. We introduce k*-means, a novel clustering algorithm that eliminates the need to set k or any other parameters. Instead, it uses the minimum description length principle to automatically determine the optimal number of clusters, k*, by splitting and merging clusters at the same time as optimizing the standard k-means objective. We prove that k*-means is guaranteed to converge and demonstrate experimentally that it significantly outperforms existing methods in scenarios where k is unknown. We also show that it is accurate in estimating k, and that empirically its runtime is competitive with existing methods, and scales well with dataset size.", "tldr": "An new algorithm that functions like  -means except doesn't require knowing  , nor setting any threshold or other parameters", "keywords": ["clustering", "minimum description length"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ea5f90d8072bfbb47ea88231c01559788c5e993b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes k*means, a parameter-free variant of k-means that automatically determines the number of clusters through the minimization of an objective (eq. 1) that involves a k-means loss term and an MDL term. The interesting aspect of the method is that it includes split and merge operations in addition to typical k-means operations that increase or decrease k always ensuring that the proposed objective is improved. Therefore, it is guaranteed that this dynamic method will converge to a minimum of the objective.\nActually, the method extends x-means which is an old incremental method (only cluster splitting takes place) based on BIC/MDL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The method automatically estimates the number of clusters k.\nS2. It does not make use of any hyperparameter\nS3. All operations ensure the minimization of the objective function."}, "weaknesses": {"value": "W1. Although MDL is theoretically justified, it is not considered efficient or robust in practical finite-sample settings. For this reason the MDL-based x-means algorithm has been first replaced by a Gaussianity-based criterion (g-means algorithm) and later by a unimodality-based criterion (dip-means algorithm). \nW2. Related work omits significant contributions in number of clusters estimation, such as g-means, dip-means and  more recently the Uniforce and DipDeck algorithms to mention some of them.\nW3. The experimental part ignores the majority of well-known methods that conduct automatic number of clusters estimation. \nW4. Since the method starts with a single cluster, the usefulness of the merging operation is questionable."}, "questions": {"value": "Q1. Experimental comparison ignores several well-known or state-of-the-art methods for number of clusters estimation (see W2). Some methods are available in the ClustPy library.\nQ2. The method should be tested on datasets with imbalanced clusters and/or not clearly separated clusters.\nQ3. More complex datasets should be considered in the clustering comparison. In the present work dimensionality reduction takes place \nto tackle increased dimensionality, thus the clustered datasets are low-dimensional.\nQ4. The method tends to underestimate the number of clusters. This is due to the fact that the MDL term is relatively big resulting in conservative estimations. This is in my opinion the main drawback of the proposed approach.\nQ5. How often is merging actually triggered in the experiments? Could you provide ablation results (with vs. without merging) to demonstrate its impact?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VnwvXb1VY3", "forum": "Ii4UcmxbmJ", "replyto": "Ii4UcmxbmJ", "signatures": ["ICLR.cc/2026/Conference/Submission16911/Reviewer_WBJp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16911/Reviewer_WBJp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650387527, "cdate": 1761650387527, "tmdate": 1762926938385, "mdate": 1762926938385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "7Ke0DYQBwh", "forum": "Ii4UcmxbmJ", "replyto": "Ii4UcmxbmJ", "signatures": ["ICLR.cc/2026/Conference/Submission16911/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16911/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763671701655, "cdate": 1763671701655, "tmdate": 1763671701655, "mdate": 1763671701655, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an extension of k-means that infers the appropriate number k of clusters. The algorithm is a heuristic method to minimize the description length of the input data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed approach is sensible and the algorithm works quite well in the experiments, which are designed reasonably well."}, "weaknesses": {"value": "- This paper could have been written 20 years ago. It does not really engage with the current research in machine learning that is most relevant to ICLR. \n\n- The algorithm is fundamentally heuristic. The proof of convergence says only that the method reaches a local optimum after a finite number of steps. The bound on performance in Appendix D is potentially more interesting. However, the bound is not stated as a formal theorem, and the proof is long enough that this reviewer has not checked it. Lines 826 and 942 appear to say that the proof is merely about the initialization, and therefore not about the new algorithm in a substantive way. Equation 5 is complicated enough that it is not obvious when the RHS provides a trivial bound and when it is non-trivial.\n\n- Since the algorithm is heuristic, the authors should describe how it is better than obvious variations on the same theme. In particular, it seems inefficient to consider splitting every center at every iteration. A different heuristic would be to consider splitting just the highest-cost center. However, something else missing in this submission is a discussion of time complexity (Table 3 is only empirical). At first sight, the \"assign\" step is much more expensive than the \"update\" step\", which itself is much more expensive than the splitting and merging operations, because the latter involve computations on scalars only (not vectors of length d). So perhaps MDL would be minimized better and faster with more splitting and merging operations, and fewer \"update\" steps, and/or \"update\" steps that operate only on points with changed centers.\n\n- The subroutines INITSUBCENTROIDS and KMEANSSTEP are described generally, but not given explicitly as pseudocode\n\n\nMissing comparisons to related research:\n- Unsupervised learning using MML by JJ Oliver, RA Baxter, CS Wallace, ICML, 1996 \n- P.Kontkanen, P.Myllymaki, W.Buntine, J.Rissanen, H.Tirri, An MDL Framework for Data Clustering. In Advances in Minimum Description Length: Theory and Applications, edited by P. Gr ̈unwald, I.J. Myung and M. Pitt. MIT Press, 2005\n- A novel split and merge EM algorithm for gaussian mixture model by Y Li, L Li, 2009 Fifth International Conference on Natural Computation"}, "questions": {"value": "125: This line is misleading because under certain conditions, BIC and MDL are equivalent.\n\n161: Explain briefly why and how the Kraft-McMillan inequality is relevant and used.\n\n162, 271: What does \"noend\" mean?\n\n172: What does the notation [ ] mean in the RHS [{...}, {...}]?\n\n186: What does max(X) mean given that X is a set of high-dimensional points, not of scalars?\n\n203: How sensitive is the algorithm to the value of m? Why is this the best value of m?\n\n336: Should be ImageNette and not ImageNet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oWOMAwNPAd", "forum": "Ii4UcmxbmJ", "replyto": "Ii4UcmxbmJ", "signatures": ["ICLR.cc/2026/Conference/Submission16911/Reviewer_WhHw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16911/Reviewer_WhHw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761715092468, "cdate": 1761715092468, "tmdate": 1762926937939, "mdate": 1762926937939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a parameter-free clustering algorithm named k*means, which aims to eliminate the need for manual specification of the cluster number k and does not introduce any other parameters. The authors provide a theoretical proof of the algorithm's convergence within a finite time. Furthermore, through comprehensive experiments on diverse multimodal datasets, they demonstrate that k*means outperforms most established baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "They introduce k∗means, an entirely parameter-free clustering algorithm\nFor carefully constructed synthetic data, k∗means can infer the true number of clusters, and shows much higher accuracy than existing methods;"}, "weaknesses": {"value": "The performance advantage of the proposed algorithm over existing methods appears to be marginal. As shown in Table 3, the accuracy of \nk*means does not demonstrate a significant improvement across most datasets.\n\nThe synthetic experiments are limited to clusters generated from multivariate normal distributions with fixed variance. \nThe algorithm's robustness remains unverified under more complex and challenging data scenarios, such as non-convex clusters (e.g., ring-shaped), uneven cluster densities, or datasets with a high proportion of noise (e.g., >20%).\n\nThe literature review primarily contrasts the method with traditional parameter-free algorithms. However, it omits comparisons with recent advancements in this direction (e.g., PFMVKM) as well as other relevant classical algorithms (e.g., DPC). This omission may lead to an incomplete and potentially less convincing performance evaluation.\n\nA theoretical analysis of the algorithm's time and space complexity is absent, which is crucial for understanding its scalability and practical utility.\n\nMinor Concerns:\n\nSome algorithms included in the empirical comparison (e.g., DPMM in Table 3) are not introduced in the Related Work section.\n\nThe algorithm name \"K-Means++\" is incorrectly written as \"k++means\" in multiple instances throughout the manuscript and should be corrected.\n\nTable 1: There is excessive blank space above the X-Means row.\n\nTable 5: The table appears to be cut off and is not displayed fully.\nThe same issue occurs with the equation on Page 17."}, "questions": {"value": "NaN"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iZb2vFj0U6", "forum": "Ii4UcmxbmJ", "replyto": "Ii4UcmxbmJ", "signatures": ["ICLR.cc/2026/Conference/Submission16911/Reviewer_WSHK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16911/Reviewer_WSHK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978615606, "cdate": 1761978615606, "tmdate": 1762926937296, "mdate": 1762926937296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}