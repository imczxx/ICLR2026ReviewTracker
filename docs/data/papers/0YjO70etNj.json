{"id": "0YjO70etNj", "number": 15043, "cdate": 1758247153266, "mdate": 1759897333814, "content": {"title": "Mirror Descent-Ascent for mean-field min-max problems", "abstract": "We study two variants of the mirror descent-ascent algorithm for solving min-max problems on the space of measures: simultaneous and sequential. We work under assumptions of convexity-concavity and relative smoothness of the payoff function with respect to a suitable Bregman divergence, defined on the space of measures via flat derivatives. We show that the convergence rates to mixed Nash equilibria, measured in the Nikaidò-Isoda error, are of order $\\mathcal{O}\\left(N^{-1/2}\\right)$ and $\\mathcal{O}\\left(N^{-2/3}\\right)$ for the simultaneous and sequential schemes, respectively, which is in line with the state-of-the-art results for related finite-dimensional algorithms.", "tldr": "", "keywords": ["Mean-field optimization", "min-max games", "Bregman divergence", "Adversarial learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8e2ef04598e35c2c12a88e74ba149475d007776.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the convergence properties of simultaneous and sequential mirror descent for solving min-max games. The paper shows that under some technical assumptions, the convergence rates (in terms of NI error) are $O(N^{-1/2})$ in the simultaneous case and $O(N^{-2/3})$ in the sequential case. An example of training GANs is implemented with both variants of MD, and the behavior corroborates the faster convergence rate of sequential MD."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The main technical results are clearly presented and while the notation is quite heavy, the authors do a good job of making the paper readable.\n- The convergence result for sequential MD is the first such result for mean-field min-max games, and the proof technique is quite interesting (if reliant on several technical assumptions).\n- The motivating example of GANs is clearly presented and the experimental results nicely corroborate the theoretical statements."}, "weaknesses": {"value": "- A concern is how extensible the convergence results are -- it is not made clear what other problem settings would satisfy the required technical assumptions to obtain a fast convergence rate. In particular, Assumption 3.4 seems like a very restrictive condition on the second variation, and I believe it would improve the paper to show more examples of divergences and min-max problems that satisfy all technical assumptions.\n- Beyond GANs and mean field neural nets, not much is said about the other concrete applications of the framework. Considering that prior work has shown connections and examples ranging from Sinkhorn and EM algorithms to reinforcement learning, a natural question which is not investigated is how sequential MD performs in these settings. I would be more positive on the paper if it can be shown that sequential MD is a superior method for other applications. As it stands, the theoretical results are novel but the significance and applicability of the results are not convincing.\n- The notation in the paper is quite dense (and somewhat unavoidable given the topic), but giving some additional clarifications on the assumptions and also providing more detailed proof sketches (in particular emphasizing the way the proofs diverge from standard techniques) would help readers who might be unfamiliar with the material."}, "questions": {"value": "- The MD variants studied are Euler discretizations of the Fisher-Rao gradient flow in continuous time. I am curious if other discrete-time algorithms which can be obtained by taking different discretizations? Does the continuous-time convergence rate given insights into the behavior of its discretizations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vy1p3QoZXT", "forum": "0YjO70etNj", "replyto": "0YjO70etNj", "signatures": ["ICLR.cc/2026/Conference/Submission15043/Reviewer_dZnt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15043/Reviewer_dZnt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761500906713, "cdate": 1761500906713, "tmdate": 1762925366592, "mdate": 1762925366592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies mirror descent (MD) algorithms on spaces of measures. It provides analysis for both the simultaneous and sequential (a.k.a. alternating) versions of MD for general convex-concave objectives and general objectives, generalizing existing results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The generalization of existing results to this setting requires, at least in my understanding, a significant amount of technical work, and the paper seems to do this well. The paper is also generally well-written and clear."}, "weaknesses": {"value": "### Issue with the spaces under consideration\nI have trouble understanding on which spaces assumptions and statements are valid.\n\nIn particular, it is not clear how Assumption 3.4 can be satisfied for the entropy regularizer with the current wording.\nIndeed, Assumption 3.4 requires the existence of a constant $L_{h^* }$ such that the inequality l368 holds uniformly over all bounded functions.\nBut the constant $L_{h^* }$ given by Prop. G.15 depends on the functions themselves (through their sup norm).\nMoreover, in this proof, the authors refer to (Lascu et al., 2025, Lemma A.2) for the Lipschitzness of $\\phi$ but I could not find this result in the lemma mentioned (which is several pages long). I also do not see how to obtain such a pointwise Lipschitz bound.\n(This issue is the main reason behind my current rating.)\n\nMoreover, in example 1.3, the authors appear to have to restrict the space of probability measures to those with density wthin bounded distance from a reference distribution. This seems to be restrictive compared to (Hsieh et al., 2019) for instance.\n\n### Lack of examples of divergence\nIn its comparison to previous work, this paper insists on the generality of the divergences considered compared to previous works which were only on KL divergence.\nHowever, most of the assumptions are verified only for the entropy regularizer, see Asm 2.1 and 3.4.\nMoreover, for the examples 1.2 and E, the choice of divergence is not discussed, and the reader is refered to Remark 2.2. If I understand correctly, this means that these examples are only valid when $h$ satisfies the inequality l286. If this is the case, this should be made more explicit in the paper and this assumption should be better discussed.\n\n\n### Minor\n- l260-267 make it sound as if the form of the update is novel while, unless I am mistaken, it is standard in mirror methods.\n- The appendix is quite long, it could benefit from a table of contents and an introductory section explaining its organization."}, "questions": {"value": "Could the authors address \"Issue with the spaces under consideration\" and \"Lack of examples of divergence\" in the section above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mTtXa8WyZS", "forum": "0YjO70etNj", "replyto": "0YjO70etNj", "signatures": ["ICLR.cc/2026/Conference/Submission15043/Reviewer_pJx5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15043/Reviewer_pJx5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662094747, "cdate": 1761662094747, "tmdate": 1762925366093, "mdate": 1762925366093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the simultaneous and sequential mirror descent-ascent (MDA) algorithms in solving convex-concave min-max problems with infinite-dimensional action spaces.\nFor simultaneous MDA, they show that the averaged iterate converges at a rate of $O(1/\\sqrt{N})$, where $N$ is the number of iterations.\nFor sequential MDA, they show that the averaged iterate converges at a rate of $O(1/N^{2/3})$.\nBoth convergence rates match the known rates for the finite-dimensional bilinear case.\nThey also implement many numerical experiments to prove the efficiency of the algorithms in its applications, e.g., training GANs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "*   The paper seems to be the first paper to study the sequential MDA for games with infinite action spaces;\n*   This paper establishes theoretical foundations for the faster convergence of the sequential MDA than its simultanuous counterpart;\n*   The paper is fairly well-written and polished."}, "weaknesses": {"value": "*   It looks like the new result on the sequential MDA is obtained by the parallel analysis in the discrete-case. It would be better if the authors can point out the main differences between the discrete-time and continuous-time proofs;\n*   It seems that in the statement of their main theorems, they missed some key assumptions:\n    *   It appears that they need the reference function $h$ to be Legendre, i.e., the norm of the derivative of $h$ goes to infinity as the iterate approaches the boundary. For example, they may need that assumption to get the first-order optimality conditions in Eq. (20). If correct, this is a big limitation that should be called out in many places. Currently, the paper repeatedly emphasizes being for \"general Bregman divergences.\"\n    *   The stepsizes need to be set depending on the total number of iterations $N$ for both algorithms, while it seems that they did not include that setup in the statements of Theorem 2.4 and Theorem 3.6.\n*.  The GANs motivation does not feel very compelling. Having a probability distribution over network parameters feels highly unrealistic. How would you train this?"}, "questions": {"value": "*   Is my understanding of part 3 correct?\n*  How do you imagine realistically training a probability distribution over parameters in a neural network?\n\nMinor points:\n*   In Assumption 1.5, it would be clearer if the authors clearify the notations $D_{F(\\cdot, \\mu)}(\\nu', \\nu)$ and $D_{F(\\nu, \\cdot)}(\\mu', \\mu)$. As $D_h$ is used to denote Bregman divergence in the paper, maybe they can consider use alternative notations for the second-order derivatives if possible;\n*   The motivation of MDA formulation in Section 1.6 can be simplified in my opinion, as it aligns with the general gradient-based optimization methods;\n*   Remark 2.5 is not entirely right in my opinion, since the usual $1/\\sqrt{N}$ rate does not require relative Lipschitzness.\n*   In much of the related literature, \"sequential\" algorithms are called \"alternating\" rather than \"sequential\".\n*   Typos:\n    *   p18 l929: due [to] Assumption 3.5 or [by] Assumption 3.5.\n    *   Why are $\\mu^\\*,\\nu^\\*$ included in Theorem 2.4? As far as I can tell they play no role?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ET4kM4BWLm", "forum": "0YjO70etNj", "replyto": "0YjO70etNj", "signatures": ["ICLR.cc/2026/Conference/Submission15043/Reviewer_kmEj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15043/Reviewer_kmEj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835932907, "cdate": 1761835932907, "tmdate": 1762925365706, "mdate": 1762925365706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript studies mirror descent/ascent algorithms for nonlinear convex–concave problems defined on the space of probability measures. The authors consider two variants of a saddle-point optimization algorithm: simultaneous and sequential. They leverage relative smoothness and Lipschitzness properties of the convex–concave objective to prove a $1/\\sqrt{N}$ bound for the simultaneous algorithm. Additionally, they use Hessian Lipschitzness and an $L_{\\infty}$ assumption on derivatives to improve the convergence rate to $1/N^{2/3}$ for the sequential algorithm. The results extend earlier results for bilinear objectives and can be extended to training mean-field neural networks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written.\n- It tackles the technical arguments needed to extend optimization guarantees for bilinear objectives on probability spaces to nonlinear convex–concave objectives.\n- It empirically verifies its theoretical results."}, "weaknesses": {"value": "- The analysis assumes both Lipschitzness and smoothness, yet the convergence of the simultaneous algorithm is $1/\\sqrt{N}$, which may be restrictive for nonlinear objectives. The cited analogous result (Bubeck 2015 Theorem 5.1) assumes only Lipschitzness. That said, the authors clearly explain the proof bottleneck, which I appreciate.  \n- The implementation requires an internal sampler to trace the algorithm’s trajectory, but the approximation error introduced by this sampler is not characterized."}, "questions": {"value": "Do you think it is possible to derive a $1/N$ rate in the relative-smoothness case using an algorithm similar to (Bubeck 2015, Section 5.2.3)? What are the main technical challenges? If not, is there an impossibility result—e.g., a matching lower bound—establishing that faster rates are unattainable under these assumptions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yXlmdnWaGo", "forum": "0YjO70etNj", "replyto": "0YjO70etNj", "signatures": ["ICLR.cc/2026/Conference/Submission15043/Reviewer_xMxA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15043/Reviewer_xMxA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762218076794, "cdate": 1762218076794, "tmdate": 1762925365247, "mdate": 1762925365247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}