{"id": "5K9XW9gQ5r", "number": 6064, "cdate": 1757951957884, "mdate": 1759897937173, "content": {"title": "UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning", "abstract": "Unified multimodal large language models such as Show-o and Janus have achieved strong performance across both generation and understanding tasks. However, these models typically rely on large-scale datasets and require substantial computation during the pretraining stage. In addition, several post-training methods have been proposed, but they often depend on external data or are limited to task-specific customization. In this work, we introduce UniRL, a self-improving post-training approach. Our approach enables the model to generate images from prompts and use them as training data in each iteration, without relying on any external image data. Moreover, it enables the two tasks to enhance each other: the generated images are used for understanding, and the understanding results are used to supervise generation. We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize the models. UniRL offers three key advantages: (1) it requires no external image data, as all training samples are generated by the model itself during training; (2) it not only improves individual task performance, but also reduces the imbalance between generation and understanding; and (3) it requires only several additional training steps during the post-training stage. We evaluate UniRL on top of Show-o and Janus, achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and models will be released.", "tldr": "a post-training self-improving method for unified multimodal models using SFT and GRPO", "keywords": ["multimodal unified models", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/57136c5929707aa59d11a7e2743b185873936fef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes UniRL, a self-improving post-training framework for unified multimodal large language models (e.g., Show-o, Janus) to address two key limitations of existing methods: heavy reliance on external image data and persistent imbalance between text-to-image generation (T2I) and multimodal understanding (MMU) tasks. UniRL operates without external image data by leveraging the model’s own generated images as training data in each iteration. It enables mutual enhancement between T2I and MMU-generated images are used to train understanding, while understanding results (e.g., answer accuracy) supervise generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. UniRL eliminates the need for external image data by using the model’s real-time generated images as training samples. This addresses a critical pain point of existing post-training methods (which often rely on large external datasets like JourneyDB) and reduces data acquisition costs, making it more scalable.\n\n2. Unlike methods that optimize T2I or MMU in isolation, UniRL designs a closed-loop system where T2I and MMU reinforce each other. This not only improves individual task performance (e.g., Show-o’s GenEval score rises from 0.60 to 0.77 via SFT) but also explicitly reduces task imbalance—a long-overlooked issue in unified multimodal models.\n\n3. The proposed bidirectional metric Accuracy_MMU|T2I and Accuracy_T2I|MMU fills a gap in existing evaluations, which often focus on isolated task performance. This metric quantifies how well T2I and MMU align (e.g., whether a correctly generated image leads to a correct answer), providing a more holistic measure of unified model performance."}, "weaknesses": {"value": "1. UniRL only targets basic visual attributes (counting, color, position) and ignores complex multimodal tasks (e.g., mathematical reasoning, abstract scene understanding, or object interaction inference). This restricts its applicability to real-world scenarios where models need to handle diverse, high-level tasks.\n\n2. The credit assignment from MMU rewards to T2I in the decoupled setting is described at a high level; more details on stability tricks, baseline comparisons, and failure cases for SFT are needed."}, "questions": {"value": "1. In Table 4, UniRL (SFT) AccuracyMMU|T2I for “Single.” is 0.11. Is this a typo or a real phenomenon? If real, what causes such a stark collapse limited to this subtask? Similarly, Table 3 shows UniRL (SFT) “Single” = 0.08 for MMU. Could you share per-class confusion, prompt/QA distributions, and seed variance to rule out evaluation or data-pipeline issues?\n\n2. To address general capability decline, you propose \"incorporating a broader set of understanding categories\" and \"scaling up the model size.\" For the former, what specific new categories (e.g., math word problems, logical syllogisms) would you prioritize, and how would you design prompts/QA pairs for them to ensure alignment with both T2I and MMU? For the latter, do you have preliminary results or projections on how model scaling (e.g., increasing parameter count from Show-o’s base size) would impact text-based reasoning and numerical calculation performance? Would scaling alone be sufficient, or does it require paired adjustments to the training framework?\n\n3. MMMU’s strength lies in cross-discipline reasoning (e.g., combining visual cues with text-based math). Your current T2I-MMU mutual enhancement relies on basic visual attributes—how would you modify UniRL to enable the model to generate images that support text-based reasoning tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rfNNVETYwD", "forum": "5K9XW9gQ5r", "replyto": "5K9XW9gQ5r", "signatures": ["ICLR.cc/2026/Conference/Submission6064/Reviewer_Xvrj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6064/Reviewer_Xvrj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902534375, "cdate": 1761902534375, "tmdate": 1762918439923, "mdate": 1762918439923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "UniRL introduces a self-improving post-training framework for unified multimodal models (e.g., Show-o, Janus) that jointly optimizes text-to-image (T2I) generation and multimodal understanding (MMU) using supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO). The core innovation is a closed-loop pipeline where the model generates images from prompts, answers questions about them, and uses the feedback to improve both tasks without external data. Besides, a bidirectional evaluation metric (Accuracy_MMU|T2I, Accuracy_T2I|MMU)  is introduced to quantifies task alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed self-improving framework is technically sound. By using generated data for iterative training, no external data is required and the mutual reinforcement of T2I and MMU tasks addresses the generation-understanding imbalance pervasive in unified models.\n2. The integration of SFT and GRPO  pave the way for further performance improvement in the context of unified models.\n3. The overall presentation is clear and easy to follow.\n4. The analysis of SFT v.s. GRPO provide valuable insights."}, "weaknesses": {"value": "1. In table 2 and table 4, the performance of Count. underperforms than that of the baseline model. However, the prompts construction and evaluation pipeline  in figure 1 and figure 2  show that the Count. performance could be improved with the proposed framework. Why the performance is worser? More evaluation details should be included to clarify this.\n2. The evaluation only foucuses on basic visual attributes (counting, color, position) but neglects complex reasoning (e.g., physics, temporal dynamics). Accordingly, the performance improvement on GenEval could be \"hacked\" by the constructed prompt suit for iterative self-improving. In other words, the performance gains might be attribute to the iterative optimization on the isomorphic prompt set instead of actual improvement. Are there any ood results to better reflect the effectiveness of the proposed method?\n3. More recent unified models should be involved for evaluation and comparison such as BAGEL, SHOW-O2, BLIP-3O, etc.\n4. More evaluation on multimodal understanding benchmarks such as MMMU, GQA, MME, etc. , should be included, current results in table 3 only provide limited evaluation scope for the author's claim on improving generation and understanding simutaneously."}, "questions": {"value": "I would consider raise my score if authors could address my concerns, especially on the evaluation results and details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I would consider raise my score if authors could address my concerns, especially on the evaluation results and details."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FSdnzPwjp2", "forum": "5K9XW9gQ5r", "replyto": "5K9XW9gQ5r", "signatures": ["ICLR.cc/2026/Conference/Submission6064/Reviewer_hxCv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6064/Reviewer_hxCv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908017914, "cdate": 1761908017914, "tmdate": 1762918439418, "mdate": 1762918439418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a self-improving post-training method (UniRL) for unified MLLMs (u-MLLMs). UniRL consists of both SFT and RL stages, and enables u-MLLMs to generate their own synthetic training data without relying on external datasets. Moreover, the paper introduces a new metric to quantify task imbalance. Experiments on Show-o and Janus demonstrate performance improvements in both tasks over baseline u-MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **No external training data requirement:** The proposed UniRL uses only model-generated images for post-training, making it more practical and scalable, eliminating the need for expensive real-world data collection and annotation. \n\n- **Valuable consistency evaluation metric:** The proposed bidirectional metrics, Accuracy(MMU|T2I) and Accuracy(T2I|MMU), is a useful and well-conceived to quantify the alignment between generation and understanding tasks. \n\n- **Insightful analysis:** This paper provides clear observations explaining why GRPO outperforms SFT for generalization while SFT excels for T2I in end-to-end settings."}, "weaknesses": {"value": "- **Synthetic data reliability and quality control:** While self-generated training data of u-MLLMs is data-friendly, the quality and diversity of generated synthetic data can not be guaranteed. Without guarantees of generated data, the self-training loop risks reinforcing generation biases or error accumulation over iterations. How do you ensure that the self-generated images remain sufficiently diverse and informative across iterations? Is there any mechanism to prevent the accumulation of low-quality or mode-collapsed samples?\n\n- **Limited evaluation and comparison:** The experiments primarily focus on comparisons against baselines, lacking more comprehensive evaluations with leading u-MLLMs of comparable parameter scales and other GRPO-trained u-MLLMs.\n\n- **Reward function design is too empirical:** The reward rules are hand-crafted keyword matching. It lacks universality and may introduce bias toward specific phrasings or synonyms."}, "questions": {"value": "1. How does UniRL's performance scale with model size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9dhSfDcCju", "forum": "5K9XW9gQ5r", "replyto": "5K9XW9gQ5r", "signatures": ["ICLR.cc/2026/Conference/Submission6064/Reviewer_LME2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6064/Reviewer_LME2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979004270, "cdate": 1761979004270, "tmdate": 1762918438792, "mdate": 1762918438792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce UniRL, a self-improving post-training framework for unified multimodal large language models (u-MLLMs) that perform both text-to-image generation (T2I) and multimodal understanding (MMU). In this way, the model is updated by the its self-generated data: the model generates images as well as answers questions about these images, and then optimizes itself using either SFT or GRPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a real gap in current u-MLLMs, i.e., the lack of mutual improvement between generation and understanding, and argues convincingly that post-training is an efficient place to address it.\n- It is elegant and practically attractive for models to bootstrap its own samples. \n- Text is easy to follow."}, "weaknesses": {"value": "- Limited novelty: The overall pipeline is a well-known paradigm in self-training and reinforcement learning.\n- Narrow scope: The method is validated only on GenEval-like attributes, which represent low-level visual features. It remains unclear whether the approach generalizes to more complex open-domain tasks (e.g., human actions, reasoning, visual grounding, or captioning).\n- The automatic reward relies on keyword or attribute matching. Without semantic or visual similarity measures (e.g., CLIP-based scoring or GPT-based judging), the reinforcement signal is weak and may not scale.\n- Although UniRL improves GenEval and balance metrics, the absolute gains are moderate, and the experiments are limited to small model variants. The method does not appear to change the frontier of multimodal learning."}, "questions": {"value": "Please refer to the weaknesses.\n\n- How sensitive is performance to the reward function design and group size K in GRPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "51hjweDVGJ", "forum": "5K9XW9gQ5r", "replyto": "5K9XW9gQ5r", "signatures": ["ICLR.cc/2026/Conference/Submission6064/Reviewer_2jf8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6064/Reviewer_2jf8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055344287, "cdate": 1762055344287, "tmdate": 1762918438207, "mdate": 1762918438207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}