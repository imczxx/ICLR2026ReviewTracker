{"id": "fejDLlOKCl", "number": 6372, "cdate": 1757974947738, "mdate": 1763404240963, "content": {"title": "Learning to Reason for Factuality", "abstract": "Reasoning Large Language Models (R-LLMs) have significantly advanced complex reasoning tasks but often struggle with factuality, generating substantially more hallucinations than their non-reasoning counterparts on long-form factuality benchmarks. However, extending online Reinforcement Learning (RL), a key component in recent R-LLM advancements, to the long-form factuality setting poses several unique challenges due to the lack of reliable verification methods. Previous work has utilized automatic factuality evaluation frameworks such as FActScore to curate preference data in the offline RL setting, yet we find that directly leveraging such methods as the reward in online RL leads to reward hacking in multiple ways, such as producing less detailed or relevant responses. We propose a novel reward function that simultaneously considers the factual precision, response detail level, and answer relevance, and applies online RL to learn high quality factual reasoning. Evaluated on six long-form factuality benchmarks, our factual reasoning model achieves an average reduction of 23.1 percentage points in hallucination rate, a 23% increase in answer detail level, and no degradation in the overall response helpfulness.", "tldr": "We propose the first online RL recipe for long-form factuality, to learn reasoning strategies that enhances the factuality of LLMs.", "keywords": ["Factual Reasoning", "reasoning for factuality", "long-form factuality", "hallucination reduction"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0c02b8664c3c79534fda4679a59a5a3550873cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles hallucinations in large reasoning R-LLMs. The authors observe that standard online RL methods underperform on long-form factuality when they directly use existing evaluation metrics (e.g., VeriScore) as rewards, because these metrics are susceptible to reward hacking (e.g., producing shorter, less detailed, or irrelevant yet factually correct content). To address this, the authors propose a composite reward that integrates: (i) factual precision, (ii) response detail (the number of correct claims), and (iii) answer relevance assessed by an LLM-as-a-judge. Experiments on six long-form factuality benchmarks indicate that this reward design improves performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a fine-grained reward function to mitigate hallucinations in R-LLMs, addressing the limitations of relying solely on automatic evaluation methods (e.g., VeriScore), which can incentivize less detailed or less relevant responses.\n2. The authors present extensive evaluations across diverse datasets, along with ablation studies that isolate and validate the contribution of each reward component."}, "weaknesses": {"value": "1. The answer-relevance reward appears closely tied to the quality of responses produced by the reference model, yet the paper provides insufficient discussions about the chosen reference model. Please elaborate on this dependency and its impact on results.\n2. The contribution may be incremental relative to prior factuality-alignment work. Clarifying the conceptual novelty and quantifying gains over closely related methods would help.\n3. The paper lacks comparisons with prior alignment methods. Many factuality-alignment approaches developed for base LLMs can be readily adapted to reasoning LLMs and should be included as baselines."}, "questions": {"value": "1. Could you include case studies and an error analysis of the proposed approach? In particular, does introducing the answer-relevance reward lead to new forms of reward hacking? For example: Producing overly long responses that mix on-topic content with irrelevant material to inflate relevance scores; Repeating informative paragraphs to appear more detailed or consistent."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rOabEnEzZF", "forum": "fejDLlOKCl", "replyto": "fejDLlOKCl", "signatures": ["ICLR.cc/2026/Conference/Submission6372/Reviewer_qp9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6372/Reviewer_qp9F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734578308, "cdate": 1761734578308, "tmdate": 1762918662110, "mdate": 1762918662110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how to improve long-form factuality in reasoning-style LLMs through online reinforcement learning.\nThe authors design a composite reward that balances factual precision, detail level (number of supported facts), and answer relevance judged by an LLM-as-a-judge.\nThey implement an online VeriScore system that enables claim-level factual verification within a few seconds, making it practical for RL training.\nUsing GRPO optimization on Llama-3.1-8B-Instruct, the model significantly increases factual precision and factual detail while maintaining helpfulness.\nAblation experiments show that removing the relevance term causes reward hacking, while combining all three rewards yields more balanced outputs.\nOverall, the work demonstrates an effective way to train LLMs for factual reasoning without sacrificing answer quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly identifies the challenge of long-form factuality in reasoning LLMs and motivates the need for online RL training.\n\n- The authors implement an efficient online version of VeriScore, reducing verification time to a few seconds per response.\n\n- The experiments cover six factuality benchmarks, showing consistent gains in both factual precision and supported facts."}, "weaknesses": {"value": "The answer relevance reward depends on another LLM’s judgment, which may introduce bias from the judge model.\n\n- The results may also be sensitive to the choice of the reward LLM, yet the paper does not clearly specify which model or size was used as the judge.\n\n- The authors mention that FactScore leads to less detailed answers, but the paper provides limited explanation of how detail level is precisely measured or how the model avoids generating irrelevant but correct statements.\n\n- Because both relevance and detail are evaluated by an LLM, and the RL objective directly optimizes those same LLM-based rewards, human evaluation would be necessary to validate helpfulness and factuality.\n\n- I think the title \"Learning to Reason\" may be misleading, as the approach uses outcome-based reinforcement learning with rewards applied only to final answers, making it unclear whether the model actually learns better reasoning chains rather than optimizing end results.\n\n- Minor issue: Figure and table references should be checked. Figure 1 is not clearly cited in the text."}, "questions": {"value": "- The answer relevance reward relies on another LLM's judgment. Could this introduce bias from the judge model? Did the authors observe or mitigate such bias? \n\n- How sensitive are the results to the choice of the reward LLM? What exact model and size were used as the judge? \n\n- The paper mentions that FActScore leads to less detailed answers. How is detail level precisely measured? How do the authors ensure the model is not producing irrelevant but correct statements? \n\n- If relevance and detail are both evaluated by an LLM, and the RL objective optimizes those same LLM-based rewards, should the evaluation include human assessment to validate helpfulness and factuality? \n\n- The paper claims the method avoids reward hacking, but could it simply shift the exploitation to new composite metrics? Are current evaluations sufficient to confirm this claim? \n\n- The approach appears to use outcome-based RL, with rewards only on final answers. Does the model actually learn better reasoning chains, or just optimize for end results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7n2DmpLKx8", "forum": "fejDLlOKCl", "replyto": "fejDLlOKCl", "signatures": ["ICLR.cc/2026/Conference/Submission6372/Reviewer_sgvb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6372/Reviewer_sgvb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875610741, "cdate": 1761875610741, "tmdate": 1762918661076, "mdate": 1762918661076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an online reinforcement learning approach to improve factuality in reasoning LLMs while maintaining response detail and relevance. The method introduces a three-component reward function that simultaneously optimizes factual precision, response detail level, and answer relevance (via LLM-as-a-judge), addressing reward hacking issues where models generate overly short or off-topic responses. The authors implement a scalable VeriScore variant (24x speedup) to enable real-time factuality evaluation during GRPO training. Evaluated on six long-form factuality benchmarks, the trained model achieves 23.1% higher precision and 23% more factual statements compared to the base model. The approach maintains overall response quality with >50% win rate against the base model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, I appreciate the research question and motivation of this paper, which tackles an important and timely problem with clear significance. The authors observe that state-of-the-art reasoning models (DeepSeek-R1, QwQ-32B) exhibit significantly higher hallucination rates than their non-reasoning counterparts (10-13 percentage points worse on average, Table 1) is both surprising and concerning. This finding challenges the implicit assumption that \"more reasoning=better quality\" and highlights a genuine problem that the community needs to solve."}, "weaknesses": {"value": "1. The motivation of the paper is appealing in that it aims to address the issue that previous methods for long-form factuality evaluation have not considered the relevance between the question and the corresponding answer. However, the implementation is somewhat disappointing: it merely compares whether the optimized model's responses are better than those of the base model. But what if the base model's answer is itself irrelevant to the question? This approach does not directly solve the stated problem. Rather, it implicitly assumes that outperforming the base model automatically means being closer to the correct answer. The motivation suggests designing an absolute judgment of relevance, *i.e.*, whether an answer is relevant. But the actual method degenerates into a relative comparison of whether an answer is better than that of the base model.\n\n2. The hyperparameters in Equation (2), such as $\\lambda$ and $\\mu$, have a substantial impact on model performance, yet the paper does not include a sensitivity analysis. Only three discrete values (0, 0.01, 0.1) are tested. Among the three factors, including factual accuracy, factual detail, and factual relevance, it remains unclear which one is the dominant factor influencing the results.\n\n3. Equation (2) appears to disregard the role of $y_{\\text{cot}}$, although I believe the factuality of long CoT is central to reasoning models. The factual consistency within these reasoning steps directly affects the factual correctness of the final answer. The paper does not explain why it considers the content of $y_{\\text{cot}}$ negligible. Could this omission exacerbate factuality issues in reasoning models, creating situations where the final answer is correct but the reasoning process is factually flawed?\n\n4. Earlier work has explored similar ideas to accelerate existing factuality evaluation methods, such as VeriFastScore[1].\n      \n      [1] VeriFastScore: Speeding up long-form factuality evaluation."}, "questions": {"value": "Please address my concerns in the above Weaknesses section, and then I will accordingly improve my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "E5kN5XVvGk", "forum": "fejDLlOKCl", "replyto": "fejDLlOKCl", "signatures": ["ICLR.cc/2026/Conference/Submission6372/Reviewer_huhz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6372/Reviewer_huhz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987523948, "cdate": 1761987523948, "tmdate": 1762918660606, "mdate": 1762918660606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the factuality problem in Reasoning LLMs (R-LLMs), models trained via reinforcement learning to produce long chain-of-thought (Long CoT) reasoning traces (e.g., OpenAI-o1, DeepSeek-R1). The authors observe that such models, despite excelling at math/coding reasoning, hallucinate significantly more than non-reasoning LLMs on long-form factual benchmarks. They propose a RL framework for factual reasoning, centered on a novel reward function combining Factual precision (verified via a scalable variant of VeriScore), Response detail level, and Answer relevance (using an LLM-as-a-judge). They optimize this composite reward using Group Relative Policy Optimization (GRPO). Experiments on six long-form factuality benchmarks (LongFact, FAVA, AlpacaFact, Biography, FactBench, FACTORY) show the effectiveness of the method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper tackles an important issue: factuality in reasoning LLMs, extending the RL-for-reasoning paradigm beyond purely verifiable domains.\n\n2. The authors systematically diagnose failure modes (over-precision → short answers; spurious detail → irrelevant verbosity) and explicitly design against them."}, "weaknesses": {"value": "1. The overlap with concurrent factual-RL papers (Li & Ng (2025) and Ren et al. (2025)) could be better delineated; distinguishing features beyond “long-form” need clearer articulation.\n\n2. The reward is empirically motivated but lacks formal justification or convergence discussion, e.g., how the multi-objective reward interacts with GRPO stability.\n\n3. Given that the core claim is reduction in hallucination, human-verified factuality on a subset would strengthen the argument considerably."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0MdSSatjST", "forum": "fejDLlOKCl", "replyto": "fejDLlOKCl", "signatures": ["ICLR.cc/2026/Conference/Submission6372/Reviewer_Zeq6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6372/Reviewer_Zeq6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129998070, "cdate": 1762129998070, "tmdate": 1762918660268, "mdate": 1762918660268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all the reviewers and the AC for reviewing our work!\n\n## Strengths\nWe are delighted to see that most reviewers acknowledge that our work investigates an important issue (Zeq6, huhz, sgvb), and our finding that state-of-the-art reasoning models (DeepSeek-R1, QwQ-32B) exhibit significantly higher hallucination rates than their non-reasoning counterparts is both surprising and concerning, highlighting a genuine problem that the community needs to solve (huhz).\n\nIn addition, reviewers praise the comprehensive design of our reward function as novel and systematically designed (Zeq6), addressing reward hacking issues (huhz), an effective way to train LLMs for factual reasoning without sacrificing answer quality (sgvb), and addressing the limitations of relying solely on automatic evaluation methods (qp9F).\n\nFurthermore, reviewers find our experiments across 6 different long-form factuality datasets extensive and convincing.\nThe authors present extensive evaluations across diverse datasets, along with ablation studies that isolate and validate the contribution of each reward component. (qp9F)\nThe experiments cover six factuality benchmarks, showing consistent gains in both factual precision and supported facts. (sgvb)\n\n## Commonly Raised Questions\nWe appreciate the reviewers for pointing out several presentation issues that could lead to major confusion or misunderstanding. In addition to the detailed responses to each reviewer, we would like to address some of the commonly raised concerns.\n\n### Our contributions\nSeveral reviewers brought up the point that we should better clarify our contributions and novelty. Our key contributions are as follows:\n\nIn this work, we show that existing SoTA R-LLMs suffer from significantly more hallucinations than their non-reasoning counterparts, highlighting an important open problem that has been overlooked hitherto.\nWe further propose the first Online RL recipe for long-form factuality with a novel reward formulation, and show that it achieves significant improvements over the base model as well as offline methods such as DPO (e.g. FLAME and FactTune).\n\n### On the difference between short-form and long-form factuality\nSome reviewers asked to further clarify the difference between short-form and long-form factuality, which is a defining distinction between this paper and a few recent works on short-form factuality RL (Li and Ng, 2025; Ren et al., 2025), and why long-form factuality is much more challenging.\n\nIt is straightforward to design a reward function for short-form QA, as the correctness of the answer can be easily checked by simple heuristics (e.g. exact match) or a LLM-judge. \nIn contrast, questions that require long-form responses are often much more open-ended, and there is not a single definition of “correctness” for a given response. For example, consider the following 3 responses for the example question: “Tell me about the Apollo 13 mission.”\n1. The entire Wikipedia page for the Apollo 13 mission.\n2. A one-sentence response: “The Apollo 13 mission was aborted due to an explosion on board.”\n3. The entire Wikipedia page for the Apollo 12 mission.\n\nAll three responses are 100% factually correct, but compared to response 1, response 2 is much less detailed, whereas response 3 is irrelevant to the question. As a result, it is much more challenging to design an effective reward formulation for long-form factuality that improves the factuality of a LLM’s responses without reducing either the detail level or the answer relevance.\n\nOne of our main contributions is that we propose the first Online RL recipe with a novel reward function for the more general and challenging long-form factuality problem."}}, "id": "4222yDyhkV", "forum": "fejDLlOKCl", "replyto": "fejDLlOKCl", "signatures": ["ICLR.cc/2026/Conference/Submission6372/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6372/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission6372/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763415751511, "cdate": 1763415751511, "tmdate": 1763415751511, "mdate": 1763415751511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}