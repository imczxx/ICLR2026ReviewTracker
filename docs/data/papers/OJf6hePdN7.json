{"id": "OJf6hePdN7", "number": 13838, "cdate": 1758223498271, "mdate": 1759897409300, "content": {"title": "Evolutionary Multi-Agent Reinforcement Learning for Crisis-Aware Demographic Policy Optimization", "abstract": "Demographic systems face unprecedented challenges from simultaneous crises. Conventional statistical demography techniques and agent-based models often struggle to capture nonlinear inter-regional interactions during periods of severe socio-economic disruption. To address this, we propose MADDPG-EVO-DGM, a hybrid algorithm that integrates multi-agent deep reinforcement learning with evolutionary optimization and meta-learning principles to model regional demographic processes under multiple crisis scenarios. Each region is treated as an autonomous agent learning to steer demographic policy levers, while periodic evolutionary “boosters” overcome local optima via population-based perturbations of actor network parameters. Additionally, a Darwin–Gödel Machine-inspired meta-learning mechanism adapts the booster triggers, enabling self-improvement in the learning process. We evaluate MADDPG-EVO-DGM on a simulation environment calibrated with real demographic data for eight federal regions of the Russian Federation over the period 2000–2025 and subject to ten concurrent crisis scenarios (e.g., pandemic, geopolitical conflict, economic collapse). Experiments demonstrate significantly faster convergence and improved performance over a baseline MADDPG: the hybrid approach achieves a higher final average reward (252.57 vs 243.07) and $3.4\\times$ lower convergence variance ($\\sigma=0.24$ vs $0.80$), indicating more reliable training. It also exhibits qualitative performance jumps of $+68\\%$ during evolutionary phases and maintains 35–45\\% greater resilience under crisis shocks compared to the baseline. To our knowledge, this is the first application of multi-agent reinforcement learning to large-scale demographic modeling under crises, opening new possibilities for evidence-based, crisis-resilient population policy design. Code, data and logs are provided to ensure reproducibility.", "tldr": "An adaptive evolutionary booster for MADDPG that triggers when learning stalls, improving reward and population-stability metrics in a crisis-aware demographic environment built from real regional data.", "keywords": ["Multi-Agent Reinforcement Learning", "Evolutionary Reinforcement Learning", "Adaptive Evolutionary Booster", "MADDPG", "Crisis-Aware Demographic Modeling", "Policy Optimization", "Population Stability Metric", "Real-World Regional Data", "Meta-Learning", "PBT Comparison"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4d23b918307f619fbb3a531117db86dc585894e.pdf", "supplementary_material": "/attachment/bff57b3cb2fa4e6aef2367f82495021f3d97fd98.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces MADDPG-EVO-DGM, a hybrid multi-agent reinforcement learning (MARL) algorithm that combines Multi-Agent Deep Deterministic Policy Gradient (MADDPG) with evolutionary optimization and meta-learning principles.\nThis approach is applied to large-scale demographic modeling under multiple concurrent crises, treating regions as autonomous agents that learn to optimize policy levers in a simulated environment calibrated to real data."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Integration of MARL with demographic crisis modeling.\n\n- Meta-learning adaptation introduces self-modification."}, "weaknesses": {"value": "- Numerous studies [1] have employed evolutionary algorithms to tune model parameters for optimizing the training process, rendering this approach less innovative.\n\n- The demographic environment, while incorporating multiple regions and their interactions, remains relatively simplistic. The crisis parameters are fixed, and each action spans one year, which may be excessively long for optimal control yet too brief to effectively reflect demographic policy impacts.\n\n- The experimental results are overly simplistic, lacking comparisons with advanced baseline methods. Additionally, there is no in-depth analysis of the proposed design or the demographic scenarios explored.\n\n- Figure 1 is confusing and appears inconsistent with the approach outlined in the paper. For instance, the use of LLMs is not referenced in Section 3. Furthermore, the distinction between the two \"crisis modeling\" components in the figure is unclear. Lastly, why there is an arrow pointing from 'MARL' to 'crisis modeling', rather than the reverse?\n\n[1] Bai, Hui, Ran Cheng, and Yaochu Jin. \"Evolutionary reinforcement learning: A survey.\" *Intelligent Computing* 2 (2023): 0025."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4A9xxVeH6h", "forum": "OJf6hePdN7", "replyto": "OJf6hePdN7", "signatures": ["ICLR.cc/2026/Conference/Submission13838/Reviewer_kGpr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13838/Reviewer_kGpr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761204936159, "cdate": 1761204936159, "tmdate": 1762924363032, "mdate": 1762924363032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MADDPG-EVO-DGM to optimize regional demographic policies under severe crisis conditions. To address sparse rewards and suboptimal local optima, the paper introduces an evolutionary booster and Darwin-Gödel Machine based meta-learning on top of a MADDPG baseline. The method is evaluated in a custom, crisis-aware simulation environment calibrated with 25 years of real demographic data for eight federal regions of the Russian Federation and ten concurrent crisis scenarios. The results show that the proposed approach converge significantly faster, achieve a higher final average reward, and demonstrate a 3.4x lower convergence variance than the baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper applies modern MARL techniques to large-scale, national demographic policy optimization, especially in the context of concurrent crises. This is a highly novel and socially significant problem, and the paper provides a strong proof-of-concept.\n2. The authors have clearly invested significant effort in building a simulation environment that is both complex and grounded. \n3. The core idea of an evolutionary booster is an effective solution to the exploration problem. The results in Figure 2 compellingly demonstrate that this hybrid approach is crucial for success."}, "weaknesses": {"value": "1. The entire complexity of regional demographic policy is compressed into a single, continuous 1D action (\"abstract policy lever\"). This is a massive simplification. Real-world policy involves a high-dimensional, complex action space (e.g., separate budgets for healthcare, migration programs, infrastructure). It is unclear if the learned policies are truly meaningful or just an artifact of this 1D control problem.\n2. The MADDPG-EVO-DGM method requires roughly 20x more computation per training step than the baseline MADDPG. This enormous cost makes the method's practical scalability, especially to the 89 regions motivated in the introduction, highly questionable.\n3. Figure 1 is confusing.\n4. The improvements over baselines are not very significant."}, "questions": {"value": "1. How to validate the fidelity of the simulation?\n2. See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ws4EKbpGQg", "forum": "OJf6hePdN7", "replyto": "OJf6hePdN7", "signatures": ["ICLR.cc/2026/Conference/Submission13838/Reviewer_qPZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13838/Reviewer_qPZ9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528986079, "cdate": 1761528986079, "tmdate": 1762924361902, "mdate": 1762924361902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MADDPG–EVO–DGM, a new method based on multi-agent reinforcement learning and meta-learning, applied to demographic policy design and crisis handling. MADDPG–EVO–DGM augments the existing MADDPG algorithm with periodic evolutionary boosters modulated via meta-learning. The method is studied on a novel simulation environment based on real-world demographic data, finding that MADDPG-EVO-DGM: (i) performs moderately better than MADDPG with significantly faster and more stable convergence; and (ii) significantly more robust than MADDPG in extreme crisis scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Quality:\n- the claim that MADDPG-EVO-DGM improves over MADDPG is supported by a direct, experimentally sound comparison that finds MADDPG-EVO-DGM to have moderately increased performance with significantly better sample efficiency and training stability\n- the importance of evolutionary boosters is further validated on a wide set of MARL algorithms in the appendix\n- reproducibility is supported by the supplementary material, which provides hyperparameters and code\n\nOriginality and significance:\n- while the contribution is specific to the presented domain (demographic policy design), it still presents a new method that combines existing MARL, evolutionary and meta-learning methods in a meaningful way\n- the presented method, simulation environment and results, both quantitative and qualitative (including the supplementary material) are a meaningful contribution to demographic policy design\n\nClarity:\n- overall, the manuscript is clear and the layout of the content is predictable\n- the contribution of the paper is clear and properly contextualized both in the introductory and related work sections\n- the idea of employing adaptive evolutionary boosters to avoid local minima is intuitive and sound"}, "weaknesses": {"value": "Quality:\n- while the experimental setup is described in adequate detail, the number of runs used to compute the variance of the training curves does not seem to be reported. Since this detail can greatly affect the significance of the corresponding results, especially in (MA)RL, it should be reported in the main text, ideally in the caption of the figures showing the convergences curves\n- it is mentioned that only 8 regions where simulated due to computational constraints, but there is no discussion on why this is the case (e.g. identifying bottlenecks), and how/if this would change in real-world applications\n\nOriginality:\n- the novelty of MADDPG-EVO-DGM lies mainly in the adaptiveness/sparse frequency of the \"evolutionary boosters\", i.e. it is mostly an original combination of existing methods\n\nSignificance:\n- while MADDPG-EVO-DGM is novel as a method, it is only applied to an arguably niche MARL domain (demographic policy design) on a novel environment, preventing this contribution from being relevant for other domains to which MARL is applicable\n- while it is clear that evolutionary boosters improve existing MARL methods, the results lack a direct comparison with previous non-MARL methods for demographic modeling, or a proper discussion (in the section that discusses experimental results) of why they would not be applicable at all to the proposed experimental setup (if this is the case). This would greatly enhance the significance by complementing the differences discussed in the related work section with direct evidence.\n\nClarity:\n- the description of the environment can be improved, especially for readers that are not necessarily familiar with demographic modeling. For example, it might not be immediately clear why an agent that only optimizes the given reward (eq. 2) would not be quickly drawn to perform very large actions, since there seems to be no explicit penalty for it. Discussing this, ideally with a simple example, would better motivate the use of MARL in this domain"}, "questions": {"value": "- how many runs where used to compute the average and deviation of the training curves in all experiments?\n- are previous non-MARL methods for demographic modeling applicable to the same experimental setup? How do they compare against MARL and MADDPG-EVO-DGM?\n- what exactly makes the problem of demographic policy design complex enough to require being modeled as a full Markov game to be solved with general-purpose (meta-)MARL algorithms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4ODybS1MSG", "forum": "OJf6hePdN7", "replyto": "OJf6hePdN7", "signatures": ["ICLR.cc/2026/Conference/Submission13838/Reviewer_nUvP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13838/Reviewer_nUvP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937376523, "cdate": 1761937376523, "tmdate": 1762924360769, "mdate": 1762924360769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}