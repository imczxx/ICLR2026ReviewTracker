{"id": "kIIG4Km1lu", "number": 13778, "cdate": 1758222368350, "mdate": 1759897413463, "content": {"title": "BRIDGE: Bi-level Reinforcement Learning for Dynamic Group Structure in Coalition Formation Games", "abstract": "The challenge of coalition formation games lies in efficiently navigating the exponentially large space of possible coalitions to identify the optimal partition. While existing approaches to solve coalition formation games either provide optimal solutions with limited scalability or approximate solutions without quality guarantees, we propose a novel scalable and sample-efficient approximation method based on deep reinforcement learning. Specifically, we model the coalition formation problem as a finite Markov decision process and use deep neural network to approximate optimal coalition structures within the full and abstracted coalition space. Moreover, our method is applicable to bi-level optimization problems in which coalition values are determined by the policies of individual agents at a lower decision-making level. This way, our approach facilitates dynamic, adaptive adjustments to coalition value assessments as they evolve over time. Empirical results demonstrate our algorithm's effectiveness in approximating optimal coalition structures in both normal-form and sequential mixed-motive games.", "tldr": "", "keywords": ["coalition formation games", "Bi-level reinforcement learning", "multi-agent reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b8355562d708741a67c75d780778c3f2c9d6189.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel bi-level RL framework called BRIDGE to solve the CSG problem. It models CSG as a two-level hierarchy: an upper-level \"Leader\" agent uses RL to find the optimal partition of agents, while lower-level \"Follower\" agents learn policies within those coalitions to determine their value. This main contribution allows BRIDGE to jointly optimize coalition structures and agent strategies, enabling it to solve dynamic Markov games where coalition values are not fixed. The method is shown to outperform existing approximate algorithms on both classic CSG benchmarks and mixed-motive Markov games."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper formulates the CSG problem as a bi-level RL task. This \"Leader-Follower\" paradigm is a significant conceptual contribution that bridges the gap between classic CSG and multi-agent RL.\n2. BRIGE solves CSG in mixed-motive Markov games (e.g., LBF environment). This extends CSG beyond normal-form games, enabling the optimization of coalition structures where the value is dynamic and dependent on learned agent strategies.\n3. The authors cleverly designed the upper-level MDP. Specifically, the action encoding for merging coalitions scales linearly with the number of agents, rather than quadratically which is crucial for tackling larger problem sizes. \n4. The approach is supported by a solid theoretical analysis in the appendix. Lemma C.1 formally proves that the leader's RL objective aligns with the CSG objective under optimal follower policies."}, "weaknesses": {"value": "1. The entire framework's theoretical justification (Appendix C, Lemma C.1) relies on the lower-level \"followers\" finding the optimal policy $π_{f}^{*}$ within the optimal Nash Equilibrium (NE) set. However, the practical implementation uses a MADDPG-variant, which is well-known to have no guarantees of converging to any NE, let alone the optimal one in a multi-equilibrium setting. \n2. The paper heavily emphasizes the O(N) scalability of the leader's action space, but it largely ignores the scalability of the follower problem, which can be exponentially complex.\n\nIf the authors can address these concerns, I would be very happy to raise my score."}, "questions": {"value": "1. The results for the normal-form games at N=100 in Figure 2 are impressive. Were these models trained from scratch? If so, what was the total training overhead (considering the bi-level nested loops, even for value lookups)?\n2. The paper claims the flattened adjacency matrix representation is \"equivariant,\" but also mentions that stability is ensured by \"fixing the agent ordering.\" This seems contradictory, as feeding a flattened vector into a standard MLP is not permutation equivariant. Could you please clarify this claim? Have you considered using a truly permutation-equivariant architecture, such as a Graph Neural Network (GNN), to process the coalition state?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KAna8NFV3Z", "forum": "kIIG4Km1lu", "replyto": "kIIG4Km1lu", "signatures": ["ICLR.cc/2026/Conference/Submission13778/Reviewer_QoQT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13778/Reviewer_QoQT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751077086, "cdate": 1761751077086, "tmdate": 1762924312096, "mdate": 1762924312096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies Coalition Formation Games (CFGs) and proposes a scalable, sample-efficient approximation method grounded in deep reinforcement learning. The authors model coalition formation as a finite MDP and train neural networks to approximate value functions over both the full and an abstracted coalition-structure space, indirectly yielding coalition partitions. They argue the approach supports dynamic, adaptive updates to coalition values and can be used in bi-level optimization settings where coalition values depend on lower-level agent policies. Empirically, the method is evaluated on both normal-form and mixed-motive Markov games."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The overall paper structure is clear, and the theoretical background is sufficiently motivated and well organized.\n\n2. Framing coalition formation as an MDP and learning value functions over full and abstracted spaces is interesting.\n\n3. Positioning the method for bi-level optimization problems is potentially impactful for multi-agent learning scenarios where coalition values evolve with lower-level policies.\n\n4. Claims of dynamic re-assessment over time are compelling for non-stationary or mixed-motive settings."}, "weaknesses": {"value": "1. The authors have missed the discussion on a key recent baseline (SALDAE). The related-work section omits SALDAE, a 2024 state-of-the-art baseline. Please add a principled comparison clarifying (i) the fundamental differences between your approach and SALDAE, and (ii) why your method should obtain performance gains.\n\n2. Preliminaries require cleanup of notation and definitions. There are inconsistencies in symbols (e.g., the number of agents denoted as n vs. N) and missing definitions (e.g., symbol T in Equation 1). Please unify notation throughout and define all symbols on first use. A small glossary/table of notation would help readability.\n\n3. The scope is limited to disjoint coalitions, with overlapping coalitions unaddressed. The paper defines CFGs as partitioning agents into disjoint groups. Overlapping coalitions are an active subarea. Clarify whether your formulation and algorithm can be extended to overlapping settings (e.g., by modifying the state/action representation or value aggregation), and discuss expected challenges (double-counting of utility, credit assignment, or computational implications). If not applicable, state assumptions explicitly.\n\n4. Bi-level Markov Games: horizon choice and sensitivity. The horizon is set to $N-1$ without justification. Please explain the rationale (theoretical or empirical) and add an ablation showing performance vs. horizon (e.g., shorter, $N-1$, longer), including runtime and stability impacts.\n\n5. There are some issues on the choince of baselines. (i) The current baselines exclude exact methods. Since one aim of the work is to propose an approximation with quality guarantees and the authors claim the ability to derive optimal coalition structures, it is important to include one or two representative exact solvers—even if they scale only to small instances—to compare the effectiveness (e.g., solution quality) and efficiency (e.g., runtime) of the proposed approximation against optimal solutions.; (ii) Apart from the 2024 SALDAE baseline, other baselines appear dated. Please include more recent approximate/planning-based or learning-based competitors.\n\n6. Please demonstrate the value of the bi-level architecture. Add a direct comparison to a single-level decision framework (e.g., your method with the lower level removed or replaced by a fixed surrogate) to show when and why bi-level modeling is beneficial. Include both quality (final utility/return) and cost (runtime, samples) metrics."}, "questions": {"value": "Please refer to the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4JnlerOwwJ", "forum": "kIIG4Km1lu", "replyto": "kIIG4Km1lu", "signatures": ["ICLR.cc/2026/Conference/Submission13778/Reviewer_z4ek"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13778/Reviewer_z4ek"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896960402, "cdate": 1761896960402, "tmdate": 1762924311445, "mdate": 1762924311445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of coalition formation games by formulating the problem as a MDP and decomposing the actions of the \"coalition structure chooser\", which plays the role of the leader, and the follower(s), whose goal is to play the right actions given the coalition it is in. The paper claims that empirically their approach performs better than existing methods and can \"generalize\" to a larger number of agents even if trained on a small number of them."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is easy to follow."}, "weaknesses": {"value": "- My primary consideration is that this paper's technical novelty is very light. Virtually any problem, including most combinatorial NP-hard problems can be formulated as \"a MDP\" where policies are essentially performing some kind of local search and rewards are the improvement in some objective. This includes say, problems like MAX-SAT or coloring problems, or even just a simple subset-search problem, which would seem like special cases of this coalition problem. As such, it seems to me that using RL is essentially throwing the kitchen sink at a difficult problem with little technical insights nor novelty.\n- To expand on the earlier point, while empirically \"performing better\" than existing methods is welcome, the revaluation was in my opinion too simple. It seems to me that more than BRIDGE being a generally performant method, existing evaluation benchmarks simply contain a lot of structure that the RL based approach is able to exploit. To this end, could the authors comment on how well BRIDGE performs if all $J^C_f = 0$ except for one particular $C^*$?\n- In Figure 2, how many epochs of training were performed? Could the authors give a brief classification of competing methods in the main paper? Are these heuristics or also ML based methods? How was computational cost normalized across competing methods? Was training time included in computational costs? While *inference times* were stated in appendix B3, is this not is an unfair comparison to classical methods which do not involve training? If \"training times\" were not included as part of computational costs, could one not simply precompute the solution to the fixed game and report an inference time of 0? [Note I am not referring to the \"generalization\" setting in Table 1\". \n- I found the section on mixed-motive games extremely unconvincing. The main problem here is that once the problem enters the mixed-motive (general-sum) regime, the problem becomes ill-posed. How are agents in a coalition assumed to behave? The most natural assumption is the Nash equilibrium, such that no individual agent is incentivized to deviate. But, it is well known that in general-sum games, the Nash equilibrium may not be unique and equilibrium selection must be performed. It is also rarely the case that MARL based methods (like MADDPG) converge to Nash, since even the tabular problem is PPAD hard and inapproximable. It is my understanding that the authors simply took \"convergence\" of the individual agents' policy to mean some kind of stability in the coalition-level game (see line 467, \"well-converged followers), which isn't necessarily true in general. \n- I also found the claim that BRIDGE generalizes to a larger number of agents better to be poorly supported. First, the authors show a marked performance against a random policy. This is obviously a poor baseline to compare against, and indeed, it is surprising that a random policy can even obtain 20% of the optimal, suggesting in fact, that the payoff structures are in fact quite simple to begin with.\n- Can the authors elaborate on how the utilities were practically generated? For instance, the authors use the agent based method (Rahwan, 2012), which states that $v(C) = \\sum_{i \\in C} p^i_C$, but here, $p^i_C$ is drawn from a uniform distribution (presumably, independently of all other $p^i_{C'}$). In some of the experiments, the authors claim to scale up to near 80-90 agents. This means there are approximately $2^80$ coalitions and hence that many $p^i_C$, which is obviously practically impossible to enumerate. Were these generated upfront, or on-demand?\n- This is a comparatively minor point on architecture design. In Section B1 and 4.1, the authors represent coalition structure as a NxN adjacency matrix where each edge is 1 if and only if two vertices are in the same coalition. I agree this representation is \"nice\" because of permutation invariance between agent labels. However, the representation of the \"merge\" actions (see Figure 6) does not seem to be permutation invariant, since every timestep the number of coalitions drops by one and the only actions allowed (yellow vector) are the first-#-of coalitions remaining. Wouldn't a simpler action representation be to simply have 1's on the vertices which would be merged? And if two vertices are already in the same coalition then the action does nothing. In this case, there would still be permutation invariance."}, "questions": {"value": "See the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m4FX5xicaP", "forum": "kIIG4Km1lu", "replyto": "kIIG4Km1lu", "signatures": ["ICLR.cc/2026/Conference/Submission13778/Reviewer_XJMV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13778/Reviewer_XJMV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989208140, "cdate": 1761989208140, "tmdate": 1762924310923, "mdate": 1762924310923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a novel algorithm called Bridge that tackles the Coalition Structure Generation (CSG) problem, which is the problem of how a group of agents should organize into subgroups to maximize collective gains. The proposed method formulates the problem as a bi-level reinforcement learning optimization problem, where an top-level leader agent aims to obtain the optimal coalition structure, and the follower agent aims to find the best strategy for each agent under the current coalition structure. The paper evaluates Bridge on three traditional CSG problems, showing superior performance than the current leading approximate methods for the CSG problem, and shows generalization capabilities with varying agent populations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written, with clearly stated motivation, problem formulation, and evaluation setup.\n- The use of bi-level reinforcement learning is well-justified, as the problem is naturally hierarchical.\n- Theoretical analysis is provided, including proofs demonstrating the existence and uniqueness of optimal policies under the proposed training objectives."}, "weaknesses": {"value": "- For the proposed algorithm, the reward for the leader agent inherently depends on the optimization of the follower agents. This might be very computationally expensive, can potentially be unstable, and slow to converge, making the algorithm challenging to tune in practice.\n- The generalization experiments across varying numbers of agents are only demonstrated on a small scale (from 5 to 10 agents). How well does the method generalize when the number of agents changes more substantially?"}, "questions": {"value": "- Are there experiments on reusing the follower’s policy network for each outer-loop optimization? In what scenarios might such inertia hinder or help performance?\n- What assumptions are required to guarantee the convergence of the proposed algorithm?\n- Since the leader agent’s actions are limited to merging coalitions, the states are not recoverable as the algorithm cannot split a coalition once formed. Would this affect the method’s overall performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o7O477i5Dg", "forum": "kIIG4Km1lu", "replyto": "kIIG4Km1lu", "signatures": ["ICLR.cc/2026/Conference/Submission13778/Reviewer_qNH2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13778/Reviewer_qNH2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762122177850, "cdate": 1762122177850, "tmdate": 1762924309985, "mdate": 1762924309985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}