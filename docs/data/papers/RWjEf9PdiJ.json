{"id": "RWjEf9PdiJ", "number": 6247, "cdate": 1757961949899, "mdate": 1763172158171, "content": {"title": "TokMem: Tokenized Procedural Memory for Large Language Models", "abstract": "Large language models rely heavily on prompts to specify tasks, recall knowledge and guide reasoning. However, this reliance is inefficient as prompts must be re-read at each step, scale poorly across tasks, and lack mechanisms for modular reuse. We introduce TokMem, a tokenized procedural memory that stores recurring procedures as compact, trainable embeddings. Each memory token encodes both an address to a procedure and a control signal that steers generation, enabling targeted behavior with constant-size overhead. To support continual adaptation, TokMem keeps the backbone model frozen, allowing new procedures to be added without interfering with existing ones. We evaluate TokMem on 1,000 tasks for atomic recall and multi-step function-calling for compositional recall, where it consistently outperforms retrieval-augmented generation while avoiding repeated context overhead, and fine-tuning with far fewer parameters. These results establish TokMem as a scalable and modular alternative to prompt engineering and fine-tuning, offering an explicit procedural memory for LLMs.", "tldr": "", "keywords": ["Tokenized Procedural Memory", "Memory tokens", "Continual adaptation", "Large language models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c85f4e031e14cc448ffdb43fc9090855212243d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the problem of efficiently storing and applying frequently-used procedures (such as calling certain tools) in large language models as opposed to using lengthy prompts or retrieval mechanisms. Specifically, the paper proposes TokMem, which encodes procedural knowledge as trainable continuous memory tokens. The paper evaluates on two settings: using TokMem for single tasks in Super-Natural Instructions and compositionally invoking on function-calling tasks. The results suggest that TokMem outperforms retrieval-augmented generation and parameter-efficient fine-tuning while using fewer parameters."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The experimental setting covers multiple models across different scales. The experiment also covers both single task setting and compositional setting.\n\nThe results demonstrate good empirical results of TokMem, showing its improvements over LoRA finetuning.\n \nThe proposed approach is carefully designed. For instance, TokMem uses the renormalization technique to prevent new tokens from dominating routing."}, "weaknesses": {"value": "I feel the paper needs substantial improvements in terms of clarity.\n* The paper heavily uses the term \"procedure\", but does not give a precise definition of \"procedure.\"\n* In particular, it's unclear whether the response in line 129 represents a fixed set for each procedure or can vary.\n* The implementation details of memory tokens are ambiguous. Are they essentially implemented as newly introduced special tokens added to the vocabulary?\n* I am also not so sure about the routing mechanism. Does the model predict a newly added special token that indexes a procedure during inference?\n* DC (decoupled embeddings) seems to also play an important role in some results but it is mainly covered in Appendix.\n\nThe choice of baselines mainly uses LoRA finetuning. It would be beneficial to include full fine-tuning as a reference to discuss the trade-offs between the number of parameters updated and performance.\n\nThe paper misses discussion of important related work on prompt compression. Methods like Gisting (Mu et al., 2023) and AutoCompressor (Chevalier et al., 2023) that compress prompts into tokens share similar motivations and should be\n\nI'm curious whether new special tokens are necessary. What if we used short natural language descriptions with special token wrappers like <call>tool name</call>, but similarly only tuned representations of these tokens? This approach wouldn't require adding new embeddings for every new procedure."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xmBYjbgHt0", "forum": "RWjEf9PdiJ", "replyto": "RWjEf9PdiJ", "signatures": ["ICLR.cc/2026/Conference/Submission6247/Reviewer_BfBf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6247/Reviewer_BfBf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761442591603, "cdate": 1761442591603, "tmdate": 1762918570537, "mdate": 1762918570537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TokMem, a tokenized memory that stores recurring procedures as compact, trainable embeddings. In particular, frequently reused procedures are “compressed” and stored by encoding them into an internalized memory token. To support continual adaptation, TokMem keeps the backbone model frozen, allowing new procedures to be added without interfering with existing ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is generall well-written and easy to follow.\n\n- The proposed method is compared to multiple valid baselines, with complementary experiments to understand the design choices.\n\n- Memory tokenization naturally reduces context size and supports continual learning without interfere\n\n- The backbone is frozen during the training, offering effciency and avoiding catastrophic forgetting."}, "weaknesses": {"value": "The method requires an additional adapation stage for compositional tasks, which seems to break the 'frozen backbone' claim? Also, is it fair to compare 'TokMem + adapt' to fine-tuning? While it is true that the stored procedures are modular via independent memory tokens, the comparison of TokMem without adaptation to fine-tuning in Table 3 seems to suggest limited capability of composing the modular procedures."}, "questions": {"value": "1. Is TokMem+DC using two tokens for each procedure/task? If yes, is it a fair comparison to TokMem? And can you further extend to more tokens?\n\n2. Why is renormalization needed, what is the intuition behind? I see it's helpful empirically but not sure if I understand the reason at Line 150-151. In particular, why new embeddings would develop inflated norms?\n\n3. Clarity: I found the following points confusing to me, requiring more time to catch the ideas.\n\n    - At Line 214-215, it is mentioned that both RAG and TokMem are with 'explicit' memory routing. IMO TokMem is doing 'implicit' routing since the memory token is chosen via next-token prediction, unlike RAG or MoE who has dedicated routing mechanism/architectural component. Can you clarify?\n    - Are the training examples formulated by inserting memory tokens between query and response of the original datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lGGoqme4W2", "forum": "RWjEf9PdiJ", "replyto": "RWjEf9PdiJ", "signatures": ["ICLR.cc/2026/Conference/Submission6247/Reviewer_unGL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6247/Reviewer_unGL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761512733627, "cdate": 1761512733627, "tmdate": 1762918570116, "mdate": 1762918570116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method augmenting a language model with a bank of trainable “procedure” embeddings. The model is trained on sequences of query tokens with interleaved \"procedure\" embeddings and response tokens.\n\nThe method is evaluated on \"Atomic Memory Recall\" and \"Compositional Memory Recall\" setups."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "At present I’m unable to identify clear strengths because the core method and its usage at inference are unclear. As a result I can’t assess the contribution or empirical value with confidence.\n\nConceptually, representing procedures as tokens could be interesting."}, "weaknesses": {"value": "The paper is highly unclear and have multiple problems, for example:\n1. The inference process is not described at all. During training, the model learns to predict a sequence of [query tokens]+interleaved [memory token][procedure tokens] -- what happens during inference? Are we appending procedure tokens to the input? In such case, it contradicts the motivation where the method is presented in opposition to e.g. RAG methods that inflate the context window. If the method is not injecting textual tokens of the \"procedures\", then it's another PEFT method, and should be compared to different PEFT baselines, including prompt-tuning, as well as other dynamic adapter approaches like Mixture-of-LoRAs.\n2. As the inference is unclear, so is the evaluation. For the \"Atomic Memory Recall\" setup, the tasks from Super-Natural Instructions dataset are regarded as the \"procedures\" in the memory bank. What is exactly evaluated here? Given test query, model predicts the memory token, then we append the text tokens of corresponding \"procedure\" and evaluate that with Rouge-L?\n3. Model is trained on data in the sequential order, which is contrary to the standard of shuffling the data for _stochastic_ gradient descent.\n4. Fine-tuning baseline includes only LoRA applied to query & key projections of the attention layers. It should involve also full fine-tuning, or at least LoRA applied to all linear layers, and not an arbitrary subset of them.\n5. What was the procedure of selecting hyperparameters for training? For example, learning rate for training LoRAs seems to be too low. Moreover, optimal learning rate usually differs across model sizes -- here the authors provided only a single value.\n6. \"Routing\" is mentioned multiple times in the paper but never introduced or defined.\n7. The method is described (e.g. in the abstract) as keeping the backbone frozen. However, in the second evaluation setup the model is initially finetuned before applying the TokMem.\n\nAs the authors claim in the Appendix, the paper was written with the help of ChatGPT, which might partially explain the level of presentation of the paper.\n\nAfter clarifications, I’m open to revising my score."}, "questions": {"value": "All the questions are listed in the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kyJinyIAQ5", "forum": "RWjEf9PdiJ", "replyto": "RWjEf9PdiJ", "signatures": ["ICLR.cc/2026/Conference/Submission6247/Reviewer_3EcY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6247/Reviewer_3EcY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940685814, "cdate": 1761940685814, "tmdate": 1762918569732, "mdate": 1762918569732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TokMem, a trainable memory module that can augment existing language models and steer their behavior. The memory module is represented as a bank of memory tokens that can be retrieved and appended to the context on the fly, and each memory token is trained to be associated with a response. Only the memory embeddings are updated during training while the base model remains frozen.\nThe experiments include popular approaches, such as ICL, RAG,  FT, and reply memory, and test them on Super Natural Instructions and APIGen, representing atomic and compositional memory."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a novel approach to managing memory for language models by fine-tuning just a memory module. The results are solid with comprehensive comparisons with different baselines. The design choices are solid and backed by ablation studies. It’s also great that the paper consider different types of memories like atomic and compositional memory."}, "weaknesses": {"value": "In terms of the experimental settings, the original Super Natural Instruction paper uses unseen tasks for testing, but this paper uses the same tasks for training and testing. As a result, it’s unclear how the TokMem can help existing language models generalize to new tasks. Do you have evaluations on applying TokMem to the seen unseen tasks?\n\nThe presentation could benefit from how the memory tokens are used during inference. (see below for some questions on the procedure)\n\nTypo: line 312 “fine-tine” → “fine-tune”"}, "questions": {"value": "How are the memory tokens retrieved during inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Kvpyf3RYv", "forum": "RWjEf9PdiJ", "replyto": "RWjEf9PdiJ", "signatures": ["ICLR.cc/2026/Conference/Submission6247/Reviewer_9FXM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6247/Reviewer_9FXM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953444312, "cdate": 1761953444312, "tmdate": 1762918569359, "mdate": 1762918569359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}