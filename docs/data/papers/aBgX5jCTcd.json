{"id": "aBgX5jCTcd", "number": 19531, "cdate": 1758297008048, "mdate": 1759897034335, "content": {"title": "STEP-VQ: Sequence-model Agnostic Frame-level Inference with VQ-VAE for Model-based Reinforcement Learning", "abstract": "Model-based reinforcement learning (MBRL) from pixels often encodes frames into discrete latent variables that form tokens for sequence model backbones to learn world model dynamics. Previous work adopts two main approaches, each facing distinct limitations. Categorical bottlenecks enable fast frame-level prediction by flattening spatial features into categorical distributions, but suffer from explosive parameter growing with resolution and code dimension. Conversely, vector-quantised variational autoencoder (VQ-VAE) methods achieve parameter efficiency through codebook quantisation but require slow token-level autoregressive prediction within frames, shifting computational complexity to the dynamics model and producing longer sequences that slow training and inference.\n  \n  We propose STEP-VQ, a novel frame-level VQ-VAE-based world model that enables prediction of entire frames through single forward passes. STEP-VQ follows the latent-imagination paradigm with two components: a world model (VQ-VAE + sequence model) and a behaviour policy. The approach is sequence-model agnostic, working with both Mamba-2 and Transformer architectures without modifications. Our key insight is that fine-grained spatial structure preservation may be unnecessary for effective behaviour learning in latent space, as temporal dynamics can implicitly capture spatial patterns through frame-level prediction. We provide rigorous theoretical analysis grounded in variational inference, showing how our training objective emerges from evidence lower bound (ELBO) optimisation and why Kullback--Leibler (KL) divergence formulations enable superior performance through bidirectional optimisation.\n  \n  On Atari-100k, STEP-VQ achieves competitive performance whilst dramatically improving efficiency: 11.2× faster training than a strong VQ-VAE based baseline, 4× parameter reduction compared to categorical bottlenecks, and growing advantages at higher resolutions (+27.4\\% mean improvement at 96×96). STEP-VQ reaches superhuman performance on 9 games versus 8 for categorical methods, with KL divergence providing 24.5\\% improvement over cross-entropy baselines. These results demonstrate that frame-level discrete quantisation offers a practical path to efficient, scalable MBRL using modern sequence architectures.", "tldr": "", "keywords": ["Model based reinforcement learning", "VQ-VAE", "Representation learning", "Sequence Modeling"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b132deec96dca29af2e365f85e4b980d0a66808.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a method for efficient world modeling that combines a VQ-VAE autoencoder with a sequence model to enable frame-level prediction of discrete latent representations, avoiding token-level autoregression. The approach models temporal dynamics by predicting the full grid of VQ code distributions for the next frame in a single forward pass and aligns these predictions with encoder posteriors through a KL-divergence-based dynamics loss derived from variational principles. Evaluated on the Atari 100K benchmark, STEP-VQ achieves performance comparable to categorical and autoregressive VQ-based baselines while providing 11× faster training, 4× fewer parameters, and improved scalability at higher resolutions, maintaining efficiency across both Transformer and Mamba-2 sequence model architectures."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper demonstrates a substantial training speedup compared to IRIS-like autoregressive VQ-VAE world models, effectively addressing a known computational bottleneck in VQ-based MBRL.\n\n2. The authors provide a well-structured categorization of existing approaches (categorical bottlenecks vs. VQ-VAE) and clearly identify the trade-offs motivating the proposed method.\n\n3. The inclusion of both Transformer and Mamba-2 sequence models in experiments enhances the robustness of the evaluation and shows the method’s compatibility with different sequence modeling paradigms."}, "weaknesses": {"value": "1. **Unclear methodological exposition:** The description of the proposed approach is mathematically dense and lacks intuitive implementation detail. The role of distributions and losses is not directly linked to concrete architecture design, and the core pipeline (as shown in Figure 6) should appear in the main text to aid understanding.\n\n2. **Limited performance gain over categorical bottlenecks:** While efficiency improves compared with IRIS-like methods, the method does not show clear performance advantages compared to strong CB-based baselines such as DreamerV3 or STORM. Additionally, computational cost comparisons with these CB models are missing, leaving STEP-VQ's relative benefit uncertain.\n\n3. **Insufficient detail in high-resolution comparisons:** The implementation setup of categorical baselines in higher-resolution experiments is under-specified, which weakens the claim that STEP-VQ consistently outperforms CB methods at larger resolutions.\n\n4. **Restricted scalability analysis:** Although the paper highlights resolution scalability, the experiments stop at 96x96 inputs, far below the resolutions addressed by recent models like Dreamer4 [1] (up to 640x360), limiting the evidence for true scalability across visual domains.\n\n[1] Hafner, Danijar, Wilson Yan, and Timothy Lillicrap. \"Training agents inside of scalable world models.\" arXiv preprint arXiv:2509.24527 (2025)."}, "questions": {"value": "See the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7h5GdfkCXL", "forum": "aBgX5jCTcd", "replyto": "aBgX5jCTcd", "signatures": ["ICLR.cc/2026/Conference/Submission19531/Reviewer_zxCC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19531/Reviewer_zxCC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801815100, "cdate": 1761801815100, "tmdate": 1762931420447, "mdate": 1762931420447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes STEP-VQ, a method combining VQ-VAE with frame-level prediction for model-based RL. While STEP-VQ achieves 11.2× speedup over IRIS and maintains comparable performance on the Atari-100k benchmark with fewer parameters. However, given the mediocre performance, limited baseline comparisons, narrow experimental scope, unvalidated theoretical assumptions, and limited novelty, **I recommend rejection**."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper proposes replacing cross-entropy with KL divergence for VQ-based methods and demonstrates a 24.5% performance improvement with theoretical justification."}, "weaknesses": {"value": "* The experimental performance is underwhelming. There already exist many model-based RL methods that achieve both faster overall training time and superior performance. \n* The paper compares against very limited baselines, I suggest comparing with state-of-the-art VQ-based methods such as Simulus [1], REM [2], and $\\Delta$-IRIS [3], as well as other categorical-based methods like TWISTER [4], DyMoDreamer [5], and STORM [6].\n* The evaluation is restricted to Atari-100k only. The paper lacks validation on other standard MBRL benchmarks such as Crafter, DeepMind Control Vision.\n* The core theoretical assumption $I(z_t[i, j]; z_t[i', j'] | h_t) ≈ 0$ is never empirically measured or validated.\n* **Limited Novelty**: Independent prediction of each token in discrete latent world models has already been explored by many works (e.g., REM, Simulus, Transformer World Model [7]), and most categorical-based methods also independently predict each token.\n\n**References**\n\n[1] Uncovering Untapped Potential in Sample-Efficient World Model Agents.\n\n[2] Improving Token-Based World Models with Parallel Observation Prediction.\n\n[3] Efficient World Models with Context-Aware Tokenization. \n\n[4] Learning Transformer-based World Models with Contrastive Predictive Coding\n\n[5] DyMoDreamer: World Modeling with Dynamic Modulation\n\n[6] STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning\n\n[7] Improving Transformer World Models for Data-Efficient RL"}, "questions": {"value": "* **Absolute training time**: What is the actual wall-clock training time (in hours) for a single Atari-100k environment under the experimental settings presented in this paper?\n* **Unfair capacity comparison**: STEP-VQ uses a codebook size of 64 while the CB baseline uses categorical classes of 32. Could the performance improvement simply be attributed to this larger representation capacity rather than the method itself?\n* **High-resolution scalability insights**: The improved performance at 96×96 resolution seems intuitive given the parameter scaling properties. What unique insights or theoretical understanding do the authors provide beyond the obvious architectural consequence?\n* **Vector-based observations**: Would STEP-VQ outperform CB in environments with vector-based observations (e.g., DeepMind Proprio Control) where spatial quantization may not be necessary?\n* **Comparison with SOTA**: What advantages does STEP-VQ offer compared to the current state-of-the-art baselines on the Atari-100k benchmark (e.g., TWISTER, EfficientZero V2 [1], EDELINE [2])?\n\n**References**\n\n[1] EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data\n\n[2] EDELINE: Enhancing Memory in Diffusion-based World Models via Linear-Time Sequence Modeling"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "S04Vy1ZiNe", "forum": "aBgX5jCTcd", "replyto": "aBgX5jCTcd", "signatures": ["ICLR.cc/2026/Conference/Submission19531/Reviewer_VmE4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19531/Reviewer_VmE4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830563888, "cdate": 1761830563888, "tmdate": 1762931420066, "mdate": 1762931420066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STEP-VQ, a model-based reinforcement learning approach combining vector-quantized VAEs with sequence models to enable frame-level prediction instead of slower token-level autoregression. The key idea is that temporal redundancy between frames can compensate for the loss of fine-grained spatial modelling, thus achieving both efficiency and scalability. The authors claim large speedups over prior VQ-based methods (e.g., IRIS), improved parameter efficiency over categorical bottlenecks, and competitive performance on Atari 100K using both Mamba-2 and Transformer architectures."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- This paper replaces token-level autoregression with frame-level prediction, yielding large training speedups (up to ~11x) while keeping VQ-VAE parameter efficiency.\n- The method works with both Mamba-2 and Transformers, showing comparable performance without architecture-specific changes."}, "weaknesses": {"value": "- The dynamics model remains unclear. In Section 2.3.1, the same function $f_\\psi$ appears to serve multiple roles: in Eq. (3), it computes a hidden vector from latent codes $z_t$, but in Eq. (4), it seems to output a 3D tensor of logits indexed spatially. Even Figure 6(b) does not fully clarify the mapping between these representations. Two possible interpretations are: (i) $f_\\psi$ maps between vector and 3D tensor representations through convolutional and transposed-convolutional layers (less likely), or (ii) each latent code is processed independently by the same model (more likely, but the notation in Section 2.3.1 would then be inconsistent). This ambiguity should be clarified.\n- Line 93: The statement about \"explosive parameter scaling\" may be overstated. The scaling depends on whether encoder output dimensions $H', W'$ grow with input size $H, W$, since additional downsampling could mitigate the effect. Clarifying this assumption would improve accuracy.\n- The KL balancing loss resembles the one in DreamerV2 (Hafner et al., 2020, Algorithm 2). This work should be cited explicitly, and the differences between the two formulations should be discussed. Also, the equation in line 253 lacks a label.\n- Evaluation setup:\n  * The abstract states: \"STEP-VQ reaches superhuman performance on 9 games versus 8 for categorical methods, with KL divergence providing 24.5% improvement over cross-entropy baselines.\" While this highlights per-game counts and a within-method loss comparison, Tables 1-2 show only small mean gains alongside notably lower medians (Mamba: mean +5.4%, median -35.6%; Transformer: mean +3.5%, median -21.7%), suggesting heavier-tailed outcomes and wins concentrated in fewer games. I recommend rephrasing the claim to reflect these results, and explicitly clarifying that the \"24.5% improvement\" refers to KL vs. cross-entropy **within** STEP-VQ, not versus categorical baselines.\n  * Only three seeds per game are used, which is insufficient given the known high variance in Atari 100K. At least five seeds are typically considered a minimum.\n  * Section 3 does not clearly state which architectures (Mamba or Transformer) are used in the results being compared. This makes it impossible to compare to the other results at lower resolution.\n  * Figure 3 would be more informative if it included the lower-resolution baselines, as done in Tables 6-7. This would better illustrate scalability trends across resolutions.\n- A relevant related work is missing: Robine et al. \"Smaller World Models for Reinforcement Learning.\" Neural Process Lett 55, 11397–11427 (2023).\n- Minor issues:\n  * Line 156: The encoder is referred to as $Enc_\\phi$ here but later as the probabilistic model $q_\\phi$. The relationship between these should be clarified or unified in notation.\n  * Line 191: The additional variable $z^\\star_t$ seems unnecessary. Reusing $z^q_t$ would simplify the presentation.\n  * Line 196: The indexing notation (e.g., [:, 1:L]) looks like Python slicing, which implies exclusive upper bounds. Using $z^\\star_{2:L}$ and $\\hat{z}_{1:L-1}$, consistent with 1-based time indexing elsewhere, would improve clarity.\n  * Figure 2(a): The legend is confusing.\n\nOverall, the paper makes a promising step toward more efficient world models, but the technical description of the dynamics model and evaluation methodology need clarification before publication. Further experiments (e.g., with more seeds or RNN variants) would strengthen the empirical case."}, "questions": {"value": "- Have the authors considered evaluating a recurrent (RNN-based) dynamics model?\n- Please clarify the dynamics model as described in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nxsocHoiK1", "forum": "aBgX5jCTcd", "replyto": "aBgX5jCTcd", "signatures": ["ICLR.cc/2026/Conference/Submission19531/Reviewer_3CbD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19531/Reviewer_3CbD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19531/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917610914, "cdate": 1761917610914, "tmdate": 1762931419613, "mdate": 1762931419613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}