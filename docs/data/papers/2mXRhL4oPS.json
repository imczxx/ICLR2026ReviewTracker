{"id": "2mXRhL4oPS", "number": 22121, "cdate": 1758326398962, "mdate": 1763471846292, "content": {"title": "Toward Monosemantic Clinical Explanations for Alzheimer’s Diagnosis via Attribution and Mechanistic Interpretability", "abstract": "Interpretability remains a major obstacle to deploying large language models (LLMs) in high-stakes settings such as Alzheimer’s disease (AD) progression diagnosis, where early and explainable predictions are essential. Traditional attribution methods suffer from inter-method variability and often produce unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability lacks direct alignment with model inputs/outputs and does not provide importance scores. We propose a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. Our approach evaluates six attribution techniques, refines them using a learning-based explanation optimizer, and employs sparse autoencoders (SAEs) to map LLM activations into a disentangled latent space that supports clearer and more coherent attribution analysis. Comparing latent-space and native attributions, we observe substantial gains in robustness, consistency, and semantic clarity. Experiments on IID and OOD Alzheimer’s cohorts across binary and three-class tasks demonstrate that our framework yields more reliable, clinically aligned explanations and reveals meaningful diagnostic patterns. This work advances the safe and trustworthy use of LLMs in cognitive health and neurodegenerative disease assessment.", "tldr": "", "keywords": ["mechanistic-interpretability", "attributional-interpretability", "neurodegenerative-disease", "LLM", "sparse-autoencoders", "explanation-optimizer"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ceb5cd5709a4c33c8a82acd7bb2a2852238c38b3.pdf", "supplementary_material": "/attachment/89b86f3e5858193ea5885c6cbbcc98b7edde7151.pdf"}, "replies": [{"content": {"summary": {"value": "The paper applies classical attribution methods and mechanistic interpretability methods in the form of Sparse Autoencoders to clinical Alzheimer's text data. The applied methods are benchmarked and quantitatively evaluated based on robustness metrics, sparsity, and a proposed meta-rule based on UMAP representations of the saliency maps."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper compares attribution methods with SAE-based mechanistic interpretability on clinical text data.\n- The evaluation includes both in-distribution (ID) and out-of-distribution (OOD) data."}, "weaknesses": {"value": "**W1:** The paper lacks a clear focus between being a benchmark study or an application-oriented study of diverse XAI methods in the context of clinical Alzheimer's text data. As a benchmark paper, a single thematically limited set of ID and OOD data is insufficient to draw meaningful conclusions. As an application paper, the work fails to derive clinically interesting insights through the proposed approach. The manuscript would benefit from focusing on one direction, which would also improve readability and provide a more coherent narrative structure.\n\n**W2:** The clinical relevance remains unclear. The motivation for applying mechanistic and attribution methods together to clinical data is not established. The work neither generates new methodological knowledge nor addresses a specific medical problem (Section 3.4 only describes the XAI results without interpretation). The intended use case is never stated, whether these explanations should assist physicians during diagnosis, enable extraction of novel biomarkers, or serve another clinical purpose. The paper reads as a demonstration of XAI methods applied to Alzheimer's clinical data without clear scientific or clinical objectives.\n\n**W3:** The UMAP projection of attribution maps raises methodological concerns. It remains unclear how semantic relationships, complexity, or faithfulness can be meaningfully evaluated in the projected space, where only similarity between maps is preserved. I assume, that similar information could potentially be obtained through various distance metrics without dimensionality reduction.\n\n**W4:** The evaluation relies exclusively on RIS/ROS metrics, which assess only robustness. Other important evaluation criteria for saliency map methods, such as faithfulness and complexity, are not considered.\n\n**W5:** Table 1 is excessively detailed, hindering the extraction of meaningful insights."}, "questions": {"value": "- Why should polysemantic representations of concepts within the model bias attribution methods?\n- How are the concepts derived from the SAE representations, and how is it evaluated whether they \"make sense\"?\n- Line 363: Why should lower complexity in the saliency map result in more robust explanations as indicated by RIS/ROS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "US9iUrFJHn", "forum": "2mXRhL4oPS", "replyto": "2mXRhL4oPS", "signatures": ["ICLR.cc/2026/Conference/Submission22121/Reviewer_ufP6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22121/Reviewer_ufP6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760827431279, "cdate": 1760827431279, "tmdate": 1762942073747, "mdate": 1762942073747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comment to the reviews."}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback, which has greatly helped us improve the clarity and focus of the manuscript. Across the revision, we substantially clarified the methodological scope of our work and improved both scientific precision and clinical relevance. First, we corrected misleading claims about novelty by rewriting the contribution statement to emphasize that our use of sparse autoencoders is methodological rather than architectural, and we added new empirical evidence—including an external, model-agnostic interpretability audit—to concretely demonstrate monosemanticity and clinical coherence. We clarified dataset composition, class-construction rationales, and the IID/OOD generalization design, updating both the manuscript and supplementary sections to ensure clinical consistency. We addressed concerns about hyperparameter selection by documenting the full validation-based tuning process, and we strengthened theoretical clarity by explaining the role of the UMAP-based geometry constraint. To improve readability, we rewrote the Abstract, Introduction and Contributions, simplified Table 1, reorganized and expanded Sections 3.4 and 3.4.1 (clinical practice), and clearly articulated the practical clinical workflow enabled by our approach—showing how clinicians can inspect subject-level explanations and identify high-yield cognitive biomarkers. We explicitly distinguished our method from model-editing techniques such as ROME/MEMIT, clarified that no LLM parameters are altered, and reframed our framework as a post-hoc interpretability pipeline. We added detailed explanations of why polysemantic representations bias attribution methods and how the SAE disentangling yields more coherent features, and we justified the evaluation metrics while acknowledging future extensions. Overall, the revisions strengthened conceptual clarity, methodological rigor, clinical grounding, and the coherence of the paper’s narrative. All changes in the updated version of the main paper and supplementary material are highlighted in blue. Thank you"}}, "id": "oOyrDJFdiF", "forum": "2mXRhL4oPS", "replyto": "2mXRhL4oPS", "signatures": ["ICLR.cc/2026/Conference/Submission22121/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22121/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22121/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763465212017, "cdate": 1763465212017, "tmdate": 1763465212017, "mdate": 1763465212017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the use of model editing techniques to improve the monosemanticity (i.e., interpretability and disentanglement) of clinical representations in large medical models. The authors argue that current medical foundation models (e.g., ClinicalBERT, BioGPT) produce entangled representations that make interpretation difficult and hinder trust in high-stakes clinical applications.\n\nTo address this, the paper proposes a monosemantic model editing framework, where latent dimensions corresponding to clinical concepts are modified via targeted interventions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles the critical issue of interpretability and trustworthiness in medical foundation models.\n- Applying monosemanticity which is  a concept originally from mechanistic interpretability to clinical representation learning is an interesting direction.\n- The proposed editing procedures (erasure and steering) are consistent with recent model editing literature and appropriately adapted for clinical text."}, "weaknesses": {"value": "- The model editing methods used are largely adapted from existing general-domain approaches (ROME, MEMIT, causal mediation), with limited algorithmic innovation.\n- While interpretability metrics are reported, there is insufficient discussion of clinical validity, i.e., whether the identified concepts align with real-world medical semantics.\n- The experiments are limited to a few clinical NLP benchmarks (MIMIC-III, PubMedQA), lacking evaluation on multimodal or longitudinal EHR data.\n- The abstract and introduction are somewhat lengthy and overly descriptive. I suggest clearly articulating the paper’s unique contribution early to help readers grasp the main innovation."}, "questions": {"value": "- How are the edited neurons selected and validated for semantic consistency?\n- Does the editing process generalize across different models (e.g., from BioBERT to PubMedGPT)?\n- How robust are the edits to retraining or fine-tuning? Do the monosemantic features persist?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dlVUYsQmpy", "forum": "2mXRhL4oPS", "replyto": "2mXRhL4oPS", "signatures": ["ICLR.cc/2026/Conference/Submission22121/Reviewer_Mirq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22121/Reviewer_Mirq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761333994354, "cdate": 1761333994354, "tmdate": 1762942073437, "mdate": 1762942073437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method to enhance the interpretability of Large Language Models (LLMs) used for diagnosing neurodegenerative dementia. Their approach combines three key components:\n1. Feature disentanglement: A sparse autoencoder is applied to the LLM's features to create a \"monosemantic feature space,\" aiming to isolate individual concepts, hence improving their interpretability.\n2. Attribution aggregation: Multiple attributional interpretability methods—including Feature Ablation, Layer Activations, Layer Conduction, Layer Gradient-SHAP, Layer Integrated Gradients, and Layer Gradient x Activation—are combined. This aggregation is performed using an encoder-decoder model, implemented as either a Diffusion Explanation Optimizer (DEO) or a Transformer Explanation Optimizer (TEO).\n3. Visualisation: The dimensionality of the resulting attributions is reduced using UMAP to provide visual feedback.\n\nThe method is developed and evaluated on data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) that includes cognitive normal (CN), mild cognitive impairment (MCI), and Alzheimer's disease (AD) subjects. It is also applied to the BrainLat dataset that includes CN, frontotemporal dementia (FTD), and AD subjects. Both binary and three-class classification tasks are implemented."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- **Important objective**: The focus on improving the interpretability of LLMs is highly relevant and important, particularly in high-stakes domains like healthcare where understanding model decisions is critical.\n- **Sensible and original methodology**: The proposed three-step approach appears both logical and original. The combination of feature disentanglement via a sparse autoencoder, the aggregation of multiple attribution methods using a dedicated optimiser (DEO/TEO), and final visualisation with UMAP constitutes a novel and well-structured pipeline for enhancing interpretability.\n- **Promising results**: The experimental results demonstrate that the method successfully achieves its stated objectives, showing evidence of increased sparsity in the feature representations while maintaining stability, which is a key goal for interpretability."}, "weaknesses": {"value": "- **Unclear use of the data**: The description of the ADNI cohort is confusing and raises concerns. The authors mention that they consider subjects who are cognitively normal, have early MCI and late MCI (Section B.1.1). However, for binary classification, they suddenly refer to \"AD subjects.\" It is unclear if these are distinct from the LMCI group or how they are defined. Also, the rationale for selecting the input features appears to be driven purely by technical convenience rather than clinical knowledge of disease progression. This group definition issue extends to the BrainLat dataset, which includes FTD patients. The application of labels like (L/M/C) to this cohort, as seen in Table 1, does not make clinical sense and suggests a problematic, non-clinical approach to data handling.\n- **Poor presentation and clarity**: The experimental section is very difficult to follow. The reader is forced to constantly switch between the main text and the supplement to understand the methodology. The tables are overcrowded with insignificant numbers, and the figures are not intuitively interpretable, which hinders the assessment of the results.\n- **Unclear practical application**: The paper fails to articulate a clear use case for the proposed method. It remains abstract how a clinician or researcher would practically apply this interpretability framework to gain insights into a specific diagnosis or the model's decision-making process. The utility and practical workflow are not sufficiently demonstrated."}, "questions": {"value": "- The paper imposes the constraint that the first and second components of each embedding vector must be equal (Equation 4). Could you please re-explain the motivation for this specific constraint? What problem does it solve or what specific property of the embedding space does it enforce?\n- Could you clarify whether hyperparameter tuning was performed using a separate validation set or directly on the test set?\n- Please provide details on how the ADNI and BrainLat datasets were split into training, validation, and test sets. Specifically, what were the sizes of each split, and was the splitting stratified by key variables?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dFIpgPPu8q", "forum": "2mXRhL4oPS", "replyto": "2mXRhL4oPS", "signatures": ["ICLR.cc/2026/Conference/Submission22121/Reviewer_14Zh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22121/Reviewer_14Zh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768723142, "cdate": 1761768723142, "tmdate": 1762942073167, "mdate": 1762942073167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This method  proposed a unified interpretability framework that integrates attributional and mechanistic perspectives via monosemantic feature extraction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This method used mechanistic interpretability to make trustworthy AI models for Alzheimer’s diagnosis.\nBy combining a Transformer Explanation Optimizer with a Sparse Autoencoder, the method aims to reveal more insightful and more consistent explanations of what the model has learned."}, "weaknesses": {"value": "1)The proposed monosemantic bottleneck is not supported by a novel algorithm, and its interpretability is not clearly demonstrated through the architecture. The paper repeatedly claims that the SAE features are “monosemantic” and “clinically meaningful,” but authors do not provides intuitive examples or insights to substantiate these claims. The connection between SAE latent features and clinical interpretability remains theoretical — it is hard to prove that these latent features correspond to coherent medical concepts verified by clinicians. For instance, the transformation pipeline (input → SAE → output) is not empirically illustrated or analyzed to reveal what each feature actually represent\n2)Only 2 datasets ADNI and BrainLAT  are used. Thus, the method does not show that LLM actually performs meanful reasoning on clinical data instead of memorizing structured text patterns\n3)λ₁–λ₅ are hand-tuned without ablation studies"}, "questions": {"value": "1)Could the authors clarify the causal relationship—if any—between the SAE’s latent representations and the clinical reasoning process of the model?\n2)Could the authors discuss potential dataset biases or artifacts that might inflate the interpretability or stability metrics?\n3)How were the λ₁–λ₅ hyperparameters chosen?\n4)Could authors validate this proposal with at least 1 more dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WEJXgsbvEW", "forum": "2mXRhL4oPS", "replyto": "2mXRhL4oPS", "signatures": ["ICLR.cc/2026/Conference/Submission22121/Reviewer_4q7S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22121/Reviewer_4q7S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762477427874, "cdate": 1762477427874, "tmdate": 1762942072884, "mdate": 1762942072884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}