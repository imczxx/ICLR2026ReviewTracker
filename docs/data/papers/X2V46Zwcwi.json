{"id": "X2V46Zwcwi", "number": 5410, "cdate": 1757907749605, "mdate": 1759897977302, "content": {"title": "From Faithfulness to Correctness: Generative Reward Models that Think Critically", "abstract": "Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.", "tldr": "The paper introduces a generative reward modeling framework for reinforcement learning that reasons from sentence-level faithfulness to correctness, focusing on open-domain question answering with supporting documents.", "keywords": ["reward models", "large language models", "reinforcement learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f8ec4421b9a071c23e80199fe03d84fac3c2a76.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a thinking supervised reward model named TRM for open domain QA with retrieved documents. TRM scores each answer sentence in three stages. First faithfulness to sources, then an explicit reasoning step, then a correctness decision. The model is trained with SFT on sentence level supervision and then with RL that uses a combined reward for faithfulness and correctness, with an extra boost for correctly finding incorrect sentences to handle class imbalance. Experiments on a Tencent WeChat search dataset and the CRUD dataset show higher F1 for detecting incorrect sentences and better answer selection than ORM and PRM. When used to train a policy with GRPO and a separate preference model for usefulness, the joint rewards improve correctness and usefulness versus a Qwen2.5 baseline and single reward ablations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Well defined sentence level supervision that enables finer error localization, with careful annotation protocol that yields the four cases faithful and correct, faithful but wrong, unfaithful but correct, and unfaithful and wrong. \n- Comprehensive baseline suite including ORM, PRM, and ablations TRM without reasoning, SFT only TRM, and RL trained TRM. Results consistently favor TRM variants on the main error detection metrics. \n- Policy training setup that separates correctness from usefulness by adding a preference model and broadcasting the preference reward to sentences. This is simple and practical, and the joint setup helps both dimensions in different test regimes."}, "weaknesses": {"value": "- The reward model dataset and the policy dataset are both sourced from the same family. The paper says test queries for policy are not seen by the reward model, yet both originate from the same search source which can share artifacts. A stronger split across domains and distributions would help. \n- The policy results use GPT 4.1 to label sentence correctness, and preference judgments also rely on GPT 4.1. This can create evaluation bias and circularity. More evaluation on that would be helpful.\n- The extra reward for catching incorrect labels and the weight between faithfulness and correctness are central. The paper fixes alpha and the preference weight, and presents one ablation. A sensitivity sweep and calibration analysis would strengthen the claim. \n- Since the preference model compares against a single anchor and the judge is a general LLM, the model could win by being longer rather than more helpful. The paper describes order swapping but not length control or toxicity checks. \n- Weak experiments. Very few experiments without standard RM benchmarks."}, "questions": {"value": "How robust is TRM to noisy or conflicting documents. Please report per case performance for the four categories faithful and correct, faithful but wrong, unfaithful but correct, unfaithful and wrong on out of distribution data. \n\nHow sensitive are results to alpha in the sentence reward and the beta that weights preference reward. Please add a sweep and show Pareto curves for correctness versus usefulness. \n\nDoes the explicit reasoning output improve calibrations or error explanations. For example, do faithfulness judgments reduce hallucinated citations or improve edit distance to references.\n\nCan TRM be distilled to a lighter model while retaining most of the gains. Provide a size versus performance plot. \n\nHow do you guard against verbosity gaming in the usefulness judge. Did you match token budgets or include length normalization. \n\nCan you compare the proposed method with baselines on standard RM benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lH1pODmq1C", "forum": "X2V46Zwcwi", "replyto": "X2V46Zwcwi", "signatures": ["ICLR.cc/2026/Conference/Submission5410/Reviewer_XPH6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5410/Reviewer_XPH6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796205110, "cdate": 1761796205110, "tmdate": 1762918045022, "mdate": 1762918045022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on how to use RLVR to improve the performance of open-domain question answering.\nIt introduces a Thinking-supervised Reward Model approach, which generates fine-grained sentence-level rewards from the dimensions of faithfulness and correctness.\nExperiments demonstrate that such reward signals effectively enhance the identification of incorrect sentences."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper focuses on how to improve the performance of open-domain question answering through reinforcement learning, which is a currently interesting and important issue.\n- The design of the fine-grained rewards is promising and may be a relatively good direction for improvement"}, "weaknesses": {"value": "- The reward for correctness in this paper heavily relies on LLM, which is limited by the knowledge of the LLM used. This may lead to inaccurate rewards.\n- The paper distinguishes between faithfulness and correctness but lacks a precise explanation. For example, the case in line 46: \"1984 was written by George Orwell in 1949\" is incorrect. In my understanding, it is also not faithful, as the document \"Novel 1984 was published in 1949\" does not directly support this answer. I believe this example does not clearly illustrate the difference between faithfulness and correctness.\n- What role does \"thinking\" play in the design of the reward model? Thinking may introduce significant inference latency. Is it suitable for a reward model? If thinking is not used and a faster model is employed instead, can similar results be achieved?\n- The annotated data used in this paper does not seem to be explicitly stated as open-sourced. How can reproducibility be ensured?\n- For the QA scenario, why does this paper only focus on cases involving supporting documents? It does not include cases without documents, which seems to be a more common usage scenario for LLMs currently."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MZp97b29nW", "forum": "X2V46Zwcwi", "replyto": "X2V46Zwcwi", "signatures": ["ICLR.cc/2026/Conference/Submission5410/Reviewer_ftTB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5410/Reviewer_ftTB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828836957, "cdate": 1761828836957, "tmdate": 1762918044747, "mdate": 1762918044747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Thinking-supervised Reward Model (TRM) to evaluate factual correctness in open-domain question answering.\nTRM decomposes judgment into three stages: faithfulness, reasoning, and correctness.\nIt is trained with supervised fine-tuning and reinforcement learning using faithfulness and correctness rewards.\nA two-stage human-annotated dataset separates faithfulness from correctness for each sentence.\nExperiments show TRM improves error detection over existing outcome- and process-based reward models.\nWhen combined with a preference model for usefulness, TRM enhances both correctness and usefulness of generated answers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles the important problem of correctness evaluation in open-domain QA, where factual verification is challenging.\n\n- It introduces TRM that explicitly separates faithfulness and correctness.\n\n- Experimental results demonstrate that TRM improves both sentence-level and answer-level error detection."}, "weaknesses": {"value": "- Several sections lack sufficient explanation of experimental settings or rationale behind hyperparameter choices. This may lead to concerns that the method and evaluation were not thoroughly investigated.\n\n- Overall, the paper would benefit from clearer organization and closer alignment between figures, tables, and textual analysis:\n\n1. The paper does not explain why alpha in Eq (2) is fixed at 0.5, nor does it include an ablation or sensitivity analysis to justify this choice.\n\n2. The data construction process lacks key details such as the total number of annotators, annotations per sentence, and inter-annotator agreement.\n\n\n3. Line 407-408: Correctness and usefulness evaluations rely solely on GPT-4.1 as the judge, without testing multiple evaluators or cross-model agreement.\n\n\n4. Line 416: Usefulness evaluation uses Qwen2.5-32B-Instruct as the only anchor, which may bias results. No alternative anchors or robustness tests are reported.\n\n\n5. Line 674-675: The 1:2 weighting between TRM and the preference reward model during policy training is not explained or experimentally validated.\n\n\n6. Line 315-316: Table 2a and 2b are referenced unclearly. The text does not specify which subtable corresponds to which result.\n\n\n7. Table 1 is placed far from the main discussion, making it difficult to connect the table with the analysis.\n\n\n8. Figure 3 appears in the paper without any in-text reference or discussion, which reduces interpretability."}, "questions": {"value": "- Why is alpha fixed at 0.5 in Eq (2)?\n\n- Lines 407–408: Why is GPT-4.1 the only evaluator? Would it be better to include multiple models for consensus?\n\n- Line 416: Why is Qwen2.5 used as the sole anchor for usefulness evaluation? Would using multiple anchors increase reliability?\n\n- Lines 674–675: Why is the reward weighting ratio set to 1:2?\n\n- Lines 315–316: What does Table 2a refer to? Which part of Table 2 corresponds to (a) and which to (b)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AhJQpZ2WbC", "forum": "X2V46Zwcwi", "replyto": "X2V46Zwcwi", "signatures": ["ICLR.cc/2026/Conference/Submission5410/Reviewer_sCaw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5410/Reviewer_sCaw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873648354, "cdate": 1761873648354, "tmdate": 1762918044462, "mdate": 1762918044462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "RL with Verifiable Rewards (RLVR) works well in domains like math and coding, where correctness is easily verified. In open-domain QA, verifying correctness is hard due to ambiguous or conflicting information. Existing work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, but not focused on improving factual accuracy, leading to errors. \n\nTo address this, authors proposed Thinking-supervised Reward Model (TRM). TRM evaluates answers in three steps: Given a query, answer, and supporting documents, TRM first evaluates the sentence-level faithfulness of the answer to the provided evidence. In the 2nd step, TRM will assess how does faithfulness inform correctness. Finally, TRM will check if the sentence is factually accurate. Results show that TRM outperforms baseline reward models (ORM and PRM) in detecting incorrect sentences and answers. Incorporating TRM into reinforcement learning improves correctness by up to 30.3% and usefulness by up to 35%. Ablation studies confirm the importance of the reasoning step."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The improvements on correctness and usefulness seems substantial.\n\nThe paper is clearly written and easy to follow"}, "weaknesses": {"value": "The evaluation is only on Tencent and CRUD datasets. Tencent dataset is closed source and not publicly available. The paper does not demonstrate generalizability across diverse domains, such as [HotpotQA](https://arxiv.org/abs/1809.09600), [AmbigQA](https://arxiv.org/abs/2004.10645).\n\nThe paper reports aggregate metrics but lacks qualitative analysis of where TRM fails (e.g., misjudging correctness despite high faithfulness). Please include error analysis and categorize common failure patterns.\n\nHow accurate is the judge? Usefulness is evaluated using LLM-as-a-judge (GPT 4.1), not human annotators. Please add human judges\n\nWhile TRM introduces structured reasoning, similar ideas of decomposing correctness and faithfulness exist in prior works on fact-checking and verifiable QA.\nAdlakha et al., 2024\nEvaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering\nMetropolitansky et al., 2025\nTowards Effective Extraction and Evaluation of Factual Claims"}, "questions": {"value": "The example is line 046 seems like an issue for retriever. It retrieves an irrelevant text “Novel 1984 was published in 1949”  as the context. But the author claimed that this is the error of the verification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SDioyzjfGo", "forum": "X2V46Zwcwi", "replyto": "X2V46Zwcwi", "signatures": ["ICLR.cc/2026/Conference/Submission5410/Reviewer_ZTiy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5410/Reviewer_ZTiy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947351543, "cdate": 1761947351543, "tmdate": 1762918043898, "mdate": 1762918043898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}