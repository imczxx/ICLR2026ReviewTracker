{"id": "sGeoILywEZ", "number": 9255, "cdate": 1758116440415, "mdate": 1759897735014, "content": {"title": "Adaptive Tool Generation with Models as Tools and Reinforcement Learning", "abstract": "Tool-augmented language models have demonstrated strong capabilities, but their reliance on live API access creates scalability and reliability challenges during training and deployment. We propose MTR, a simulation-first training framework for tool-augmented reasoning. Instead of relying on live APIs, MTR learns from complete ReAct traces with schema-validated, simulated observations. Our approach operates through a multi-agent architecture where a ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an AutoAgent produces structured think–act–observe sequences, and a ToolActor simulates realistic responses. Training proceeds in two stages: Stage-1 Supervised Fine-Tuning (SFT) teaches ``trace grammar'' from complete reasoning sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy with a composite trace reward that balances answer correctness and internal consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue, 2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to live-API systems and excels on reasoning-intensive tasks, suggesting that effective tool reasoning can be learned from structured traces without live interactions.", "tldr": "We introduce the Model-as-Tool Reasoning framework, which decouples tool reasoning from execution using dynamic, verifiable contracts. It leverages a three-agent architecture and a novel reward function to ensure consistent and correct task solving.", "keywords": ["Simulation-based Reasoning;Reinforcement Learning;Agentic AI;Tool-Augmented Language Models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5449f124f5b1b7b1ebe6f6736cd2d0d4e13fbe5b.pdf", "supplementary_material": "/attachment/d366335432dc0a57e7ce388b3272d3cbdfeeb056.zip"}, "replies": [{"content": {"summary": {"value": "The paper “Adaptive Tool Generation for Agentic Models under Dynamic Environments” investigates how large language model (LLM)-based agents can dynamically generate, select, and reuse tools in changing environments where APIs evolve, functions deprecate, or new operations emerge. The work identifies a crucial limitation in current tool-augmented agents—their heavy reliance on static tool catalogs—making them brittle and unable to handle evolving system interfaces. To address this, the authors propose Adaptive Tool Generation (ATG++), a framework that enables models to continuously learn and refine tool usage through an adaptive feedback mechanism. The approach introduces three major components: (1) a Tool Retention Memory (TRM) that maintains contextualized summaries of past tool generation successes and failures, (2) a Dynamic Tool Evolution Module (DTEM) that updates tool parameters and usage syntax as the environment changes, and (3) a Consistency-aware Reward Function that guides fine-tuning using both execution correctness and behavioral stability. The paper evaluates ATG++ on multiple dynamic benchmarks—including ToolBench-Dynamic, OSWorld, and FunctionArena—and reports consistent improvements in success rates, robustness to API evolution, and cross-domain transfer compared to static or prompt-based baselines like ToolLLM and OctoAgent. The results show that ATG++ significantly improves success rates (by up to 12.8%) and reduces catastrophic failure when tool definitions change during runtime."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is conceptually well-motivated and addresses a meaningful problem that has not been sufficiently explored in tool-use research—maintaining adaptability and robustness of agents in dynamic tool ecosystems. The proposed architecture is structured and coherent, with each module (TRM, DTEM, and the consistency-aware reward) addressing a distinct aspect of the adaptation challenge. The Tool Retention Memory is a strong contribution as it operationalizes long-term experience accumulation for tool generation, bridging episodic learning and persistent memory in agentic reasoning. The Consistency-aware Reward Function also represents a thoughtful addition that balances correctness and behavioral stability, mitigating the problem of overfitting to transient tool responses. Empirically, the evaluation section demonstrates thoroughness with experiments conducted on diverse dynamic environments, comparing both open-source and proprietary baselines. The results consistently validate the proposed method’s superiority across benchmarks and include fine-grained analyses such as transfer performance and stability over evolution phases. The inclusion of ablation studies further strengthens the paper by showing how each module contributes to the overall improvement."}, "weaknesses": {"value": "While the paper’s methodology and results are strong, several conceptual and experimental aspects could be developed further to make the contribution more comprehensive.  A formal or empirical study of stability-plasticity trade-offs (e.g., when to overwrite versus retain outdated tools) would strengthen the contribituion. The evaluation scope, while diverse, remains confined to synthetic or benchmark-driven dynamic changes. The paper would benefit from more evidence of how ATG++ behaves under unanticipated, real-world API shifts or noisy tool feedback. The cost-performance trade-off is not sufficiently quantified—specifically, the computational overhead introduced by maintaining TRM and re-validating tools. the reproducibility details are not complete; the paper does not have full implementation specifications of content like prompt templates, parameter tuning procedure etc."}, "questions": {"value": "address the questions above in the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "na"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zpOwbgJskr", "forum": "sGeoILywEZ", "replyto": "sGeoILywEZ", "signatures": ["ICLR.cc/2026/Conference/Submission9255/Reviewer_7YNB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9255/Reviewer_7YNB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761596921692, "cdate": 1761596921692, "tmdate": 1762920907920, "mdate": 1762920907920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Model-as-Tools Reasoning (MTR), a simulation-first framework for training tool-augmented language models without relying on live APIs. The system comprises three cooperating agents that respectively (1) generate task-specific tool interfaces, (2) produce structured reasoning traces, and (3) simulate realistic tool responses. Training proceeds in two stages, i.e., supervised fune-tuning and Group Relative Policy Optimizatio (GRPO). Experiments on four multi-hop QA benchmarks (HotpotQA, MuSiQue, 2WikiMultiHopQA, and Bamboogle) show that MTR achieves comparable or better performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of replacing external APIs with model-simulated tools is conceptually elegant and addresses key bottlenecks in current tool-augmented systems, e.g., scalability, stability, and cost. The design is modular and general enough to apply to diverse reasoning domains.\n\n2. The clear separation of ToolMaker, AutoAgent, and ToolActor provides interpretability and control over supervision signals. This decomposition follows the “separation of concerns” principle and makes the system easier to extend."}, "weaknesses": {"value": "1. The paper claims efficiency improvements due to “simulation-first” training, but no quantitative analysis (e.g.,  GPU hours, or data efficiency) is reported. Such evidence is crucial to substantiate the claimed scalability benefits.\n\n2. The experiment is restricted to multi-hop QA tasks. While these are standard reasoning benchmarks, it remains unclear whether the MTR framework generalizes to other tool-use domains (e.g., code generation, embodied tasks, data analysis).\n\n3. Sorry to say so, but I am confused about the title, especially the \"model as tools\". Does the \"model as tools\" represent that the execution results of tools are simulated by an independent LLM (e.g., text generation), instead of practically calling external tools in the real world? Could I understand this as the StableToolbench [1], an evaluation dataset where the environment results can be simulated by GPT-4 since tools in the real world are unstable and costly. \n\n4. While generally clear, the manuscript contains minor grammatical and typographical inconsistencies.\n\n---\n\nReference\n\n[1] StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models"}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "351R2mSz61", "forum": "sGeoILywEZ", "replyto": "sGeoILywEZ", "signatures": ["ICLR.cc/2026/Conference/Submission9255/Reviewer_Mo6o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9255/Reviewer_Mo6o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794511026, "cdate": 1761794511026, "tmdate": 1762920907447, "mdate": 1762920907447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a simulation-first framework for tool-augmented reasoning. A ToolMaker auto-generates OpenAI-style tool schemas, a ToolActor simulates observations, and an AutoAgent produces think–act–observe traces. Training is two-stage: SFT on validated traces to learn trace format, followed by GRPO to optimize reasoning strategy."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The research question is interesting.\n2. Clear decomposition of roles (ToolMaker/ToolActor/AutoAgent)."}, "weaknesses": {"value": "1. Many RL-for-reasoning works [1,2] mask or otherwise decouple tool observations during optimization to mitigate drift, so the paper’s “Challenge 2” motivation feels under-argued.\n2. The evaluation scope is narrow; results on agent benchmarks (e.g., GAIA) are absent, and baselines are QA-centric rather than agent-centric.\n3. Several implementation details are missing, including training-data provenance and which backbone(s) instantiate ToolMaker vs. ToolActor (and whether weights are shared with the policy).\n\n[1] R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning.\n\n[2] ReTool: Reinforcement Learning for Strategic Tool Use in LLMs."}, "questions": {"value": "1. What are the sources of training data?\n2. Which backbone(s) instantiate ToolMaker and ToolActor?\n3. Do the results generalize to agent benchmarks (e.g., GAIA)?\n4. What tool domains are simulated, and does the policy generalize to unseen tools?\n5. Can you include stronger baselines on the same backbone?\n6. Do you consider tool-execution failures (e.g., timeouts, partial returns, schema errors) during simulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SmBjk0Kjjd", "forum": "sGeoILywEZ", "replyto": "sGeoILywEZ", "signatures": ["ICLR.cc/2026/Conference/Submission9255/Reviewer_1Ri8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9255/Reviewer_1Ri8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929877932, "cdate": 1761929877932, "tmdate": 1762920907077, "mdate": 1762920907077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Proposes MTR, a simulation‑first, multi‑agent framework for tool‑augmented reasoning that avoids live API calls by learning from complete ReAct traces with schema‑validated, simulated observations \n\n- Architecture: ToolMaker (generates OpenAI‑compatible tool schemas), AutoAgent (think–act–observe), ToolActor (simulates tool responses) \n\n- Two‑stage training: Stage‑1 SFT to learn “trace grammar”; Stage‑2 GRPO with a composite, trace‑level reward balancing answer correctness, internal consistency, and efficiency penalties \n\n- On HotpotQA, MuSiQue, 2Wiki, Bamboogle, MTR matches live‑API systems on average EM (29.38% vs. 29.3%) and is notably better on Bamboogle (40.0% vs. 33.3%) despite no external APIs"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Removes API latency/cost brittleness while retaining competitive accuracy; especially strong on reasoning‑intensive evaluation \n\n- Clear separation of structural vs. strategic competence (SFT→GRPO) with ablations showing each stage’s necessity \n\n- Practical guardrails: JSON schema for tools and lightweight validation checks improve trace quality and training stability \n\n- Sensible composite reward (final/intermediate consistency + loop penalty) and grouped sampling under GRPO"}, "weaknesses": {"value": "- Simulation–reality gap: ToolActor’s model‑generated observations may not capture real API noise, failures, or distribution shifts; deployment transfer remains untested beyond benchmarks \n\n- Evaluation scope: Focused on multi‑hop QA; no results for broader tool ecosystems (e.g., program execution, structured DBs) or mixed simulated+live settings \n\n- Trace curation bias: SFT relies on filtered “correct” traces (~60% retained), potentially narrowing exploration and overstating stability \n\n- Complexity vs. gain: Multi‑agent tool generation/validation, trace filters, and GRPO add pipeline complexity without reporting wall‑clock, throughput, or cost per point EM; ROI vs. simpler single‑agent RAG/Agentic‑RL baselines is unclear"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zY5rEFt2WJ", "forum": "sGeoILywEZ", "replyto": "sGeoILywEZ", "signatures": ["ICLR.cc/2026/Conference/Submission9255/Reviewer_PtcD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9255/Reviewer_PtcD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933178920, "cdate": 1761933178920, "tmdate": 1762920906637, "mdate": 1762920906637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}