{"id": "QXc2NBJFHr", "number": 3582, "cdate": 1757481700808, "mdate": 1759898080230, "content": {"title": "The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge", "abstract": "Recent advances in feed-forward Novel View Synthesis (NVS) have led to a divergence between two design philosophies: bias-driven methods, which rely on explicit 3D knowledge, such as handcrafted 3D representations (e.g., NeRF and 3DGS) and camera poses annotated by Structure-from-Motion algorithms, and data-centric methods, which learn to understand 3D structure implicitly from large-scale imagery data. This raises a fundamental question: which paradigm is more scalable in an era of ever-increasing data availability? In this work, we conduct a comprehensive analysis of existing methods and uncover a critical trend that the performance of methods requiring less 3D knowledge accelerates more as training data increases, eventually outperforming their 3D knowledge-driven counterparts, which we term “the less you depend, the more you learn.” Guided by this finding, we design a feed-forward NVS framework that removes both 3D inductive biases and pose annotation reliance. By eliminating these dependencies, our method leverages great scalability, learning implicit 3D awareness directly from vast quantities of 2D images, without any pose information for training or inference. Extensive experiments demonstrate that our model achieves state-of-the-art NVS performance, even outperforming methods relying on posed training data. The results validate not only the effectiveness of our data-centric paradigm but also the power of our scalability finding as a guiding principle.", "tldr": "", "keywords": ["novel view synthesis", "feed-forward", "scaling behavior"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5c683ad182b7f4a166a0fd747a8d1a9ed44bbbe0.pdf", "supplementary_material": "/attachment/7736e9eb78f19f60bffe5692b40953ecb28303bb.zip"}, "replies": [{"content": {"summary": {"value": "The authors study generalizable novel view synthesis (NVS) from sparse, unposed 2D images and show that methods with less explicit 3D knowledge improve faster with larger data; they propose a framework that eliminates 3D inductive bias and pose annotations and learns implicit 3D awareness from 2D images, reporting photorealistic and 3D-consistent novel views comparable to methods that use posed inputs"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear empirical trend discovery, identifies and quantifies the data-scaling trend: models that depend less on explicit 3D knowledge improve faster as data scales, motivating a data-centric alternative to heavy 3D priors.\n\n2. The framework removes both explicit 3D architectural bias and pose supervision, which, if robust, simplifies pipelines and broadens applicability to unposed datasets.\n\n3. Practical impact in terms of enabling NVS without pose labels lowers annotation cost and allows training on large, in-the-wild photo collections.\n\n4. Empirical results claim parity with posed-input methods, suggesting the approach is not only elegant but effective on benchmarks.\n\n5. The paper reframes the trade-off between inductive bias and data scale, providing a clear hypothesis for future work and dataset collection strategies"}, "weaknesses": {"value": "1. The method’s advantages hinge on data scaling; performance in low-data regimes or niche domains may degrade relative to 3D-aware methods.\n\n2. Without explicit 3D representations or poses, ensuring geometric consistency across large camera motions or severe occlusions can be fragile; failure modes and long-range consistency are likely under-explored.\n\n3. Reported parity with posed methods may depend on specific datasets or evaluation metrics; results on highly diverse, real-world scenes or metric 3D accuracy may be weaker.\n\n4. Removing explicit 3D structure reduces model interpretability and makes diagnosing geometric errors or dataset biases harder.\n\n5. Achieving strong performance via data scaling may require substantial compute and careful curricula; the practical resource requirements may be high.\n\n6. Learning implicit 3D from 2D alone risks encoding photographic regularities that do not generalize to different capture conditions or object categories."}, "questions": {"value": "1. At what dataset size and diversity does the “less 3D knowledge” model overtake 3D-driven counterparts? Is there a measurable crossover point per dataset type?\n\n2. Which camera motions, occlusion patterns, or scene topologies cause the model to produce inconsistent geometry or view synthesis artifacts?\n\n3. How does the method perform on metric 3D/geometry benchmarks (e.g., depth/pose consistency) compared with explicit 3D methods?\n\n4. Does training on large web-scale data produce robust synthesis on specialized domains or is fine-tuning with some 3D bias still necessary?\n\n5. Can hybrid approaches (weak 3D priors, self-supervised pose signals) such as HawkI reduce data needs while retaining benefits of the minimalist design?\n\n6. What are the training compute, memory, and inference-time costs relative to NeRF-style or other 3D-aware baselines?\n\n7. Which minimal inductive elements (if any) materially help (positional encoding, multi-view consistency losses, rendering modules) and which are redundant?\n\n8. Does reliance on large web data introduce unwanted biases in scene content, lighting, or demographic representation that affect downstream uses?\n\n9. If we train on large in-the-wild data and evaluate on narrow-domain datasets to measure generalization and need for adaptation, how does it work?\n\n10. It will be good to add experiments with light-weight 3D cues (approximate poses, sparse depth) to quantify trade-offs between inductive bias and data size.\n\n11. It will be good to publish FLOPs, GPU hours, and inference latency to clarify practical deployment feasibility"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K5zspbzVrh", "forum": "QXc2NBJFHr", "replyto": "QXc2NBJFHr", "signatures": ["ICLR.cc/2026/Conference/Submission3582/Reviewer_KQEM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3582/Reviewer_KQEM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761680976913, "cdate": 1761680976913, "tmdate": 1762916843925, "mdate": 1762916843925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the trade-off between 3D bias-driven and data-centric design philosophies for Novel View Synthesis (NVS). The central hypothesis is \"the less you depend, the more you learn\": methods with weaker 3D dependencies exhibit superior scalability, accelerating in performance as training data increases.\n\nThe paper tests this hypothesis by conducting a systematic scalability analysis comparing existing methods (MVSplat, LVSM, NoPoSplat) on training subsets of varying sizes. They find that data-centric methods show greater performance gains with more data.\n\nBuilding on this insight, the paper proposes UP-LVSM, a novel, data-centric framework that operates in a fully unposed setting. This method learns to synthesize novel views from sparse images without any camera pose information or explicit 3D representations during training. The core technical contribution is the Latent Plücker Learner, a component that learns a latent pose space in a self-supervised manner, enabling viewpoint conditioning without ground-truth poses.\n\nExperiments show that UP-LVSM, when trained on a large dataset (66K scenes), achieves state-of-the-art performance, even outperforming methods that require ground-truth pose annotations. The paper argues that this validates their hypothesis and demonstrates that noisy 3D knowledge (such as SfM poses) can be a \"performance bottleneck\" at scale."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper's core finding—that at scale, it's better to learn 3D from 2D data than to rely on noisy, explicit 3D knowledge (like SfM) —is a significant and impactful claim, which is well-supported by the UP-LVSM results.\n\n2. The analysis in Section 3, which isolates the effects of 3D inductive bias and pose annotation dependence, is a valuable contribution in its own right. The use of dataset subsets to show performance trends (Figures 2, 4, and 5)  is very well done.\n\n3. The proposed UP-LVSM achieves SOTA performance on RealEstate10K, impressively outperforming LVSM (28.82 vs 27.60 PSNR)  despite LVSM having access to ground-truth input poses. This is a very strong result that validates the paper's hypothesis.\n\n4. The Latent Plücker Learner is a novel and well-designed component for learning a latent pose space without supervision. The design thoughtfully considers and addresses the risk of information leakage from the target view.\n\n5. The appendix is not an afterthought but contains crucial, high-quality analysis. Appendix E shows that LVSM are highly sensitive to pose noise, whereas UP-LVSMs are immune. Appendix D solves a key practical limitation of unposed methods.\n\n6. The paper is also exceptionally well written, structured and the figures and tables are all very informative."}, "weaknesses": {"value": "1. The paper's claim to remove \"3D inductive biases\" and operate without \"any 3D knowledge\"  is inaccurate. The Plücker ray embedding, which is foundational to the Latent Plücker Learner, is a strong 3D geometric prior. It encodes a line in 3D space. The claim should be more precise, e.g., \"without explicit 3D scene representations (like meshes or 3DGS) or camera pose annotations. \n\n2. A major practical drawback of unposed methods is the inability to control the camera for rendering. The paper presents a very simple and effective solution (fine-tuning a linear mapper) in Appendix D. For me personally, this is a crucial insight for making the method practical, yet it is absent from the main paper, potentially leaving readers with the impression that UP-LVSM is not controllable."}, "questions": {"value": "Do you have a sense of the upper bound? For instance, how does UP-LVSM (trained on 66K scenes) compare to a per-scene optimized 3DGS (a different kind of \"upper bound\")? \n\nIn Table 8, the 3D correspondence probing shows UP-LVSM (31.9) performing slightly worse than the off-the-shelf DINOv2 (36.8) at 0-15°. This is counterintuitive, as your model's encoder is fine-tuned on this 3D-aware task. Do you have an explanation for why the specialized model would be worse than the general-purpose one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pdfi1IQkKe", "forum": "QXc2NBJFHr", "replyto": "QXc2NBJFHr", "signatures": ["ICLR.cc/2026/Conference/Submission3582/Reviewer_n5sM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3582/Reviewer_n5sM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941840907, "cdate": 1761941840907, "tmdate": 1762916843654, "mdate": 1762916843654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper analyzes existing Novel View Synthesis (NVS) methods and discovers a core trend: methods that depend less on explicit 3D knowledge (poses, handcrafted 3D representations) benefit more from data scaling and eventually outperform 3D-biased approaches.\n\n- Based on this finding, the authors propose UP-LVSM, a feed-forward, fully data-centric NVS framework that learns implicit 3D structure directly from large-scale 2D images—without camera pose supervision or predefined 3D representations.\n\n- The method introduces a Latent Plücker Learner to infer camera geometry implicitly, enabling state-of-the-art performance in novel view synthesis from sparse, unposed images."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper demonstrates that removing explicit 3D priors allows performance to scale significantly with data, outperforming pose-supervised 3D knowledge-driven methods.\n\n- This paper has a pose-free & explicit-3D-free pipeline, a fully feed-forward transformer architecture that works without SfM poses, NeRF/3DGS priors, or handcrafted 3D structures—simplifying training and deployment.\n\n- Extensive experiments show state-of-the-art results and confirm the central hypothesis, providing both conceptual insight and practical contribution to scalable NVS."}, "weaknesses": {"value": "- I have watched the supplementary video. (1) The zoom-in and zoom-out distance is too short. (2) Also, compared to existing models, the method seems to only improve visual quality.\n\n- For datasets where GT is provided, the resolution appears to be very low. I am curious how the method performs on higher-resolution datasets. (If this dataset is the best available choice, I expect the authors to justify that in the rebuttal.)\n\n- The process of simply combining a Transformer with DINOv2 raises concerns regarding novelty. The authors should demonstrate where the novelty lies in this work. What is the novelty contribution of the Latent Plücker Learner?\n\n- The ablation study section is not intuitive. Merely describing components with text makes it difficult to understand what was included or excluded in each ablation setting."}, "questions": {"value": "Mentioned in the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GsUIHEXkmY", "forum": "QXc2NBJFHr", "replyto": "QXc2NBJFHr", "signatures": ["ICLR.cc/2026/Conference/Submission3582/Reviewer_WSHE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3582/Reviewer_WSHE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960314380, "cdate": 1761960314380, "tmdate": 1762916842972, "mdate": 1762916842972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors posit the principle “the less you depend, the more you learn”—arguing that reducing reliance on explicit 3D priors (like NeRF/3DGS representations or SfM-derived camera poses) improves scalability and generalization as training data scales. \n\nThis message extends upon LVSM and removes the training- and test- time pose requirements to make the framework even more data centric. The authors introduce a Latent Plücker Learner to infer camera poses in a self-supervised manner.\n\nThe paper presents empirical analysis across datasets (RealEstate10K, DL3DV, ACID, Objaverse) and compares UP-LVSM to bias-driven and pose-dependent methods (MVSplat, NoPoSplat, LVSM). Results show that UP-LVSM scales better with data and even surpasses 3D-supervised models in rendering fidelity and generalization."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is easy to follow and the authors promise to release code for reproducibility.\n\n- The paper evaluates across diverse datasets and metrics, providing ablations, scalability curves, and qualitative examples. The performance gains are consistent and very decent (e.g., +1 PSNR improvements over LVSM on large-scale data).\n\n- Removing the requirement of training- and test- time pose annotations have huge potential of scaling to much larger-scale datasets; hence this work (along with RayZer) opens up a lot of new possibilities."}, "weaknesses": {"value": "- UP-LVSM seems to have separately trained models on the RealEstate10k scene data and the Objaverse object data. It remains unclear to me if the latent plucker learning component can be trained on a mix of scene and Objaverse datasets. From the scalability perspective, it makes sense to have a method capable of ingesting all available data sources. \n\n- Tab. 4 is presented in a confusing way; it's unclear what baseline the performance gain is evaluated against. Moreover, I think it makes more sense to also include the absolute PSNR/SSIM/LPIPS values, rather than just providing the relative changes."}, "questions": {"value": "- in Tab. 8, it seems to me that DINOv2, though not explicitly trained on 3D tasks, seem to provide more accurate correspondence estimation than UP-LVSM which is trained on the 3D NVS task. I guess I find it a bit hard to understand. \n\n- in Tab. 5 and Fig. 7, how is the target pose provided to different methods? Did all the methods use the same ground-truth target pose? Or they consume poses estimated by UP-LVSM?\n\n- Line 1105 mentions that \"However, for the experiments reported in the main paper, we standardize all evaluations to the 224 ×224 setting, including all baseline comparisons.\" I wonder if this is fair to baselines if their provided checkpoint was trained only on 256x256."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0owutawuK3", "forum": "QXc2NBJFHr", "replyto": "QXc2NBJFHr", "signatures": ["ICLR.cc/2026/Conference/Submission3582/Reviewer_6twy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3582/Reviewer_6twy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762178103219, "cdate": 1762178103219, "tmdate": 1762916842712, "mdate": 1762916842712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}