{"id": "QNdf6wbjT3", "number": 6865, "cdate": 1757998924056, "mdate": 1759897887339, "content": {"title": "Correcting Influence: Unboxing LLM Outputs with Orthogonal Latent Spaces", "abstract": "A critical step for reliable large language models (LLMs) use in healthcare is to at-\ntribute predictions to their training data, akin to a medical case study. This requires\ntoken-level precision: pinpointing not just which training examples influence a de-\ncision, but which tokens within them are responsible. While influence functions\noffer a principled framework for this, prior work is restricted to autoregressive\nsettings and relies on an implicit assumption of token independence, rendering\ntheir identified influences unreliable. We introduce a flexible framework that in-\nfers token-level influence through a latent mediation approach for general predic-\ntion tasks. Our method attaches sparse autoencoders to any layer of a pretrained\nLLM to learn a basis of approximately independent latent features. Unlike prior\nmethods where influence decomposes additively across tokens, influence com-\nputed over latent features is inherently non-decomposable. To address this, we\nintroduce a novel method using Jacobian-vector products. Token-level influence\nis obtained by propagating latent attributions back to the input space via token\nactivation patterns. We scale our approach using efficient inverse-Hessian ap-\nproximations. Experiments on medical benchmarks show our approach identifies\nsparse, interpretable sets of tokens that jointly influence predictions. Our frame-\nwork enhances trust and enables model auditing, generalizing to any high-stakes\ndomain requiring transparent and accountable decisions", "tldr": "", "keywords": ["Influence Functions", "Sparse Autoencoder", "Interpretability", "LLM"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b5905a7ef87dc370d5acb27533d258d8986510e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a rather neat idea: use SAEs to help disentangle aspects of influence and apply it to the problem of training data attribution. It motivates the idea in the health domain, arguing that one needs to solve influence to tackle attribution to training data in this setting. The training method requires the transformer to be augmented with an SAE (If I understood correctly, this modified architecture is then used from pre-training onwards; but the impact of this on training is not discussed). The authors observe this can lead to unstable training, to which they provide some improvements (although measurements of stability were not provided). The contribution also introduces some new ways to approximate influence in this setting, and adds some additional justification for multi-token influence in their setup. It includes two small case studies, based on a GPT-2 scale model with SAEs. There is no quantitative evaluation or baseline comparison."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of using SAEs to help localise attribution is, I think, a genuinely novel idea. I appreciate also that the authors didn't make this a purely theoretical contribution, but actually tried their method, found instabilities in training, and explored some additional methods to make training more stable, such that they could finally analyse a dataset."}, "weaknesses": {"value": "The paper's introduction and abstract express the claim and value as a better way to handle data attribution for medical domains. But the paper doesn't include any baselines or quantitative evaluation. Since the paper is making approximations, and changing learning, there are several key things that need to be addressed, but are not in this manuscript: \n\n(a) Do the resulting models have comparable quality for computational cost and data needed to train the model? (SAEs are often themselves somewhat expensive to train, so this kind of modified architecture might be too impractical to ever really use?). \n\n(b) There is no baseline being compared to, and no real metric being suggested. The paper includes some anecdotal examples that help to illustrate what's happening, but doesn't include any baselines or validation of the strong claims for the paper's primary contribution. \n\n(c) There is no discussion of use Retrieval Augmented Generation (RAG), or long context, which in the context of medical domains seems like a much more natural approach to depend on, rather than model influence during pretraining. In a medical setting one is likely to want to use Retrieval Augmented Generation (RAG) methods more, to help minimise the risk of hallucination. In such a setting, the relationship to the input is more important than the training data. Thus, the claim that this work \"our approach offers a principled pathway toward transparent, trustworthy, and deployable LLMs for high-stakes domains\" is somewhat more grandiose than I think the reality. This work is still useful and good - but the introduction doesn't make any mention of the role of RAG in this problem domain. \n\n(d) There is no discussion of the computational cost of computing influence for all training data examples, or how this would scale. \n\nIn addition to these major issues, there are a number of overly-strongly stated claims/and presentation issues: \n\nLine 65: \"enabling more reliable influence estimation\": this claim is not substantiated by any evidence. This should be more precise, or justified appropriately. The difference in approximation method used in this paper vs past work may mean that the method in this paper is actually less reliable. I think the key point in the paper is that some of the theoretical aspects that make past methods more approximate can be removed. But without \n\nThere are critiques of influence w.r.t. its role as an attribution method (the key property the authors depend on for their motivation): namely that data in training interacts: any link between a test example and a single training data point will mix together aspects of style, word choice, semantics, tone etc. Even if we had perfect, efficient influence calculations, we would still not have solved the attribution problem. This strong claims about this work, which makes good but modest advances, such as \"our approach offers a principled pathway toward transparent, trustworthy, and deployable LLMs for high-stakes domains\", are somewhat over-claiming. \n\nThe case study visualizations are actually quite hard to read, and the claim that the latents \"each corresponding to a semantically meaningful concept\" doesn't seem to be clearly true in the case study examples (or maybe I'm missing something?). It would be significantly easier to understand if positive/negative contribution was indicated with color, and intensity with color intensity, or something like that. Right now, for example both red and blue can indicate positive. This makes scanning the diagrams very hard, and one has to constantly cross check the latent key. The captions also don't really tell you what the takeaway from the image is. It was also unclear if there's supposed to be some kind of separator between the symptoms and the diagnosis. Without formatting the examples in a natural way, one would expect the formatting oddity to itself play a confounding role. This is amplified by, in many cases, the examples seemed to miss the diagnosis part.\n\nLine 89: \"Connecting influence estimation to semantically meaningful features provides more actionable medical AI insights than traditional neuron-level attribution.\" -- it would be good to have at least one example to make such a claim. \n\nLine 107: Nit: \"Once training is complete and the model parameters ˆθ are fixed, we can analyze\ntheir local sensitivity to individual training samples.\" This can be done throughout training, not just at the end.\n\nLine 147: \"rendering the estimates unreliable.\": this is actually an empirical question which this paper does not validate. The effect on estimated reliability has not been tested. I think the strongest thing that can be said here is \"we would expect this to make estimates less reliable.\"\n\nLine 232: \"each corresponding to a semantically meaningful concept\": this is definitely an overclaim for SAE's latents: in an ideal world they would \"each corresponding to a semantically meaningful concept\", but in practice, of course the story is much messier. \n\nLine 484: \"Implementing this framework on larger-scale LLMs like LLaMA-3.1-1Bis a compelling future direction, but remains beyond the current computational scope.\": It's not clear why, given past work scaled up significantly larger, see: https://arxiv.org/abs/2410.17413v3"}, "questions": {"value": "1. If I understood the paper correctly, your approach involves modifying the training architecture to include SAEs from pre-training onwards. It would be great to clarify this, and if so, to also say something about the overall effect on perplexity of the model. \n\n\n2. The abstract and introduction frame the work as providing a solid theoretical foundation for influence over multiple tokens; but the method introduced is also an approximate method, like other approaches (e.g. gradient based methods like Tracin: https://arxiv.org/abs/2002.08484 ). It may be that I'm missing something, but I don't understand why the paper is being presented as having a better basis than other methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZVxu2rtTIR", "forum": "QNdf6wbjT3", "replyto": "QNdf6wbjT3", "signatures": ["ICLR.cc/2026/Conference/Submission6865/Reviewer_xvkx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6865/Reviewer_xvkx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953431522, "cdate": 1761953431522, "tmdate": 1762919119133, "mdate": 1762919119133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes computing training-data influence in a model’s latent space by inserting a sparse autoencoder (SAE) at an intermediate layer. Influence is attributed to SAE features rather than tokens, motivated by the claim that SAE latents are “more independent” and therefore better suited to influence assumptions. For sequence-level (non-decomposable) objectives, the method uses Jacobian–vector products (JVPs) to define neuron/feature-level influence and then maps those scores back to tokens by weighting with feature activations, yielding token-level ‘intensity’ visualizations. The experiments are positioned as proof-of-concept on smaller models and tasks, with qualitative dashboards rather than quantitative head-to-head results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method unifies three threads in interpretabiility - representation learning (SAE), influence estimation, and human-readable visualization all into a single pipeline that is easy to reason about.\n\n\n - The JVP formulation for feature-level influence on sequence-level objectives is a neat, general trick that could travel to other attribution settings.\n\n\n- Potential interpretability benefits: Latent-space attribution may expose recurring features rather than brittle surface tokens, which is appealing for error analysis when looking at output as a function of input tokens\n\n\n- Practical tooling: The visualization pipeline (feature influence × token activations) could potentially lead to model inspection tools that could be useful for downstream applications"}, "weaknesses": {"value": "- The paper does not motivate the need for added SAE complexity to token-level influence. What does SAE add to this exactly ? \n\n- Missing baselines: No quantitative comparisons against established TDA methods (TRAK, TracIn, Track-STAR) or non-gradient retrieval baselines (BM25, dense embeddings) on attribution metrics (MRR/Recall@k) and influence metrics (tail-patch sequence log-probs).\n\n- Visualization fidelity is untested: The token-highlight maps are qualitative and its not clear whether they are actually useful since there is not empirical validation. \n\n\n- Scale and robustness gaps: Results are on small setups; the paper does not show any analysis across seeds, checkpoints, SAE hyperparameters to validate whether the visualizations hold with varying these parameters.\n\n\n- The back-projection from features to tokens via activation weighting seemsheuristic. Could the authors clarify this?  There seems to be no analysis of sign/magnitude behavior under re-scaling, feature interference, or activation saturation, especially when multiple features co-activate."}, "questions": {"value": "- Could you help justify the multiplicative “feature influence × activation” mapping to tokens. How do you handle conflicting features, negative contributions, or activation scaling? Any normalization required to prevent feature dominance?\n\n\n- Do you anticipate any obstacles when moving from GPT-2-scale to modern LLMs (memory, JVP throughput, SAE training instability)? What would potentially be the compute costs and how do they compare to existing baselines?\n\n\n- In what regimes does SAE help most (e.g., non-decomposable objectives, long-range interactions) and where does it not help or hurt (e.g., highly lexical tasks)? Please include an error taxonomy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lmM6Y85aS9", "forum": "QNdf6wbjT3", "replyto": "QNdf6wbjT3", "signatures": ["ICLR.cc/2026/Conference/Submission6865/Reviewer_hzW3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6865/Reviewer_hzW3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972745287, "cdate": 1761972745287, "tmdate": 1762919118442, "mdate": 1762919118442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of unreliable token-level influence function attributions in LLMs. It argues that prior IF methods, which are critical for auditing models in high-stakes domains like healthcare, are theoretically unsound. This is because they rely on an implicit assumption of token independence. This renders the resulting influence scores unreliable for pinpointing which specific training tokens are responsible for a model's prediction. The paper proposes a flexible framework that infers token-level influence through a latent mediation approach, thereby correcting for the issue of token independence. The method attaches SAEs to an intermediate layer of a pretrained LLM to learn a basis of approximately independent latent features. It computes influence over this more stable latent space, rather than over the raw, correlated token space.\n\nThe core idea is to split the computation. First, an SAE finds a sparse, \"monosemantic,\" and approximately independent set of latent features. Second, the influence function is calculated over these latent features with respect to the downstream portion of the model. Because influence computed over these features is inherently non-decomposable (unlike in simple autoregressive losses), the authors use Jacobian-vector products (JVPs) to attribute influence to each latent feature. Finally, this latent-level influence is propagated back to the input space to get token-level attributions by combining the feature's influence score with the token's activation strength for that specific feature.\n\nThe empirical study involves qualitative case studies and visualizations on medical benchmarks (such as MedQA and a private Symptoms-Diagnosis dataset) using a GPT-2 model. The experiments visually demonstrate the framework's ability to trace a model's prediction on a test sample back to a sparse, interpretable set of latent features (\"neurons\") and the specific training tokens that activate them. These visualizations effectively show which training examples provide helpful (positive) or harmful (negative) influence, validating the method's utility for model auditing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a statistical flaw in prior token-level influence function (IF) methods. These methods implicitly assume token independence, but tokens in natural language are highly correlated. This assumption violation makes prior influence estimates unreliable.\n\n- The proposed solution is simple and intuitive; it uses Sparse Autoencoders (SAEs) to learn basis functions that approximate statistical independence. This directly addresses the assumption violation identified in prior work.\n\n- The qualitative analyses in the empirical studies demonstrate how the framework identifies interpretable medical features and traces predictions to specific training data."}, "weaknesses": {"value": "1. Insufficient quantitative evaluation. While the qualitative analyses in Figures 2 and 4 are appreciated, the paper would benefit significantly from systematic quantitative metrics. Specifically:\n(a) Baseline comparisons: How does this approach compare to the original Grosse et al. (2023) token-level method, gradient-based attribution (e.g., integrated gradients), or attention-based methods? Without such empirical comparisons, it is difficult to assess whether the added complexity provides a meaningful improvement.\n(b) Ground-truth validation: In a medical setting, can the identified influences be validated against expert annotations, clinical guidelines, or known causal relationships? The current evaluation relies on a few selected examples without a systematic assessment across multiple test cases.\n\n2. SAE identifiability and independence guarantees are not addressed. A fundamental limitation of SAEs is that they are unidentifiable—different training runs can produce different feature bases—and there is no guarantee that the learned features are truly independent. The paper's core theoretical claim (Remark 3.1) is that SAE features exhibit \"approximate independence,\" making them more suitable for influence estimation than tokens. However:\n(a) The paper provides no analysis of feature stability across different SAE training runs. If the features themselves are unstable, it is unclear how the resulting influence attributions can be reliable.\n(b) There is no empirical evidence comparing the correlation structure of the SAE feature space versus the raw token space to validate the \"approximate independence\" claim.\nWithout addressing these concerns, the theoretical justification for using SAEs remains unsubstantiated.\n\n3. Experimental details are insufficiently specified. Section 4 lacks crucial information needed for reproducibility and proper evaluation:\n(a) Dataset details: What is the \"private Symptoms-Diagnosis dataset\"? What is its size, composition, and collection methodology?\n(b) Experimental scope: For MedQA, how many test cases were analyzed, and how were they selected? The text mentions analyzing \"very few samples\" from a \"website dashboard,\" which suggests a risk of selection bias.\n(c) Architecture choices: Which layer l was instrumented with the SAE, and what motivated this choice? How many latent dimensions h did the SAE use? What sparsity level was targeted?\n(d) Hyperparameters: What values were used for λ₁ and λ₂ in Eq. 9? How sensitive are the results to these choices?\nThese omissions make it difficult to assess the method's generality or reproduce the results.\n\n4. The related work does not clearly position the contribution. The paper combines SAE-based analysis with influence function methodology, but the Related Work section (Appendix A) does not clearly articulate how this synthesis advances the current landscape. Instead, it reads as a broad survey, grouping citations by topic rather than providing a critical analysis that highlights:\n(a) The specific gap this work fills that neither SAE interpretability research nor influence function methods address independently.\n(b) How the combination is non-trivial or produces emergent capabilities beyond applying existing tools sequentially.\n(c) What limitations of prior work (beyond the token independence assumption) are being addressed.\nA more focused related work section explicitly contrasting this approach with closely related methods (e.g., Grosse et al. 2023, Marks et al. 2024) would better clarify the contribution's novelty and significance."}, "questions": {"value": "1. Regarding the \"Dynamic TopK\" activation (Section 3.1): this method is chosen because standard ReLU and TopK activations can \"lead to training instability and persistent dead latents.\" Could you elaborate on why \"Dynamic TopK\" is better at mitigating these specific issues than standard TopK? Can you demonstrate it empirically?\n\n\n2. Regarding the title \"Correcting Influence\": The title implies a correction is taking place. However, the method appears to be a measurement tool (a way to get a more reliable attribution) rather than an intervention (a way to fix the model's behavior). Could you clarify what is being \"corrected\"? Is it the measurement of influence (i.e., fixing the statistical flaws of prior methods), or do you see this as a step toward correcting the model's outputs (e.g., by identifying harmful influences for removal)?\n\nAs well as the questions in the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lykFdUEC8H", "forum": "QNdf6wbjT3", "replyto": "QNdf6wbjT3", "signatures": ["ICLR.cc/2026/Conference/Submission6865/Reviewer_UmqK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6865/Reviewer_UmqK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184371897, "cdate": 1762184371897, "tmdate": 1762919117932, "mdate": 1762919117932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}