{"id": "1VgUoPfl3z", "number": 3484, "cdate": 1757443244386, "mdate": 1759898087456, "content": {"title": "KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training", "abstract": "We present KnapFormer, an efficient and versatile framework to combine workload balancing and sequence parallelism in distributed training of Diffusion Transformers (DiT). KnapFormer builds on the insight that strong synergy exists between sequence parallelism and the need to address the significant token imbalance across ranks. This imbalance arises from variable-length text inputs and varying visual token counts in mixed-resolution and image-video joint training. KnapFormer redistributes tokens by first gathering sequence length metadata across all ranks in a balancing group and solving a global knapsack problem. The solver aims to minimize the variances of total workload per-GPU, while accounting for the effect of sequence parallelism. By integrating DeepSpeed-Ulysees-based sequence parallelism in the load-balancing decision process and utilizing a simple semi-empirical workload model, KnapFormers achieves minimal communication overhead and less than $1\\%$  workload discrepancy in real-world training workloads with sequence length varying from a few hundred to tens of thousands. It eliminates straggler effects and achieves $2\\times$ to $3\\times$ speedup when training state-of-the-art diffusion models like Flux on mixed-resolution and image-video joint data corpora. We attach the source code of our KnapFormer implementation in the supplementary materials.", "tldr": "Boost the training efficiency of diffusion transformer on heterogeneous data", "keywords": ["Diffusion transformers; Online load balancing; Distribute training; Efficiency"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0070698b01f9f39bdf5eab5788321bd7b4e7e41.pdf", "supplementary_material": "/attachment/d133ea4efe03028bb209fce223be80a2e712690c.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies the problem of load balancing problem in distributed training for diffusion transformer models. The paper proposes KnapFormer, which models the load-balancing problem as a multi-knapsack problem and assigns chunks of tokens to GPU groups so that the overall distributed training is load-balanced. The proposed method is especially useful when training data contains highly variable token lengths and the training nodes are heterogeneous. In simulated experiments, the proposed method could achieve 2 to 3 times speedups compared to the non-load-balanced version."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper studies an important and practical problem in distributed training and proposes a simple method to address the load-balancing issue.\n\n2. The proposed method shows significant speedups in the experiments compared to ordinary training without load-balancing."}, "weaknesses": {"value": "1. Since the paper models the load-balancing problem as a multi-knapsack problem, I think a more formal definition of the optimization objective should be presented. The approximation ratio of the given algorithm should also be stated.\n\n2. The proposed algorithm needs to globally move data across GPU nodes, which could potentially incur significant communication costs. Or, in other words, the proposed algorithm does not take into account the current location of the data and minimizes the communication.\n\n3. The idea of using a knapsack problem for load-balancing is not novel, although the application to the special case of heterogeneous GPU training is. Also, I think the proposed method is not restricted to training diffusion transformers, but the paper only addresses this setting.\n\nMinor:\n1. Some citations miss spaces before them in the formatting, e.g., line 130."}, "questions": {"value": "1. For equation 2, which is about adding the additional gamma parameter to model the memory-bound behavior of attention, it is not clear to me how this is justified. Basically, given that attention is memory-bound, why should its cost be modeled as gamma times its computational costs?\n\n2. The cost of every data sequence is individually estimated. Given the knapsack algorithm, the cost of a collection of data sequences on a GPU node is considered as the sum of the individual costs. Is this a reasonable assumption, or can authors give more justifications for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wBpZmJNm2O", "forum": "1VgUoPfl3z", "replyto": "1VgUoPfl3z", "signatures": ["ICLR.cc/2026/Conference/Submission3484/Reviewer_cww3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3484/Reviewer_cww3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761690399787, "cdate": 1761690399787, "tmdate": 1762916751139, "mdate": 1762916751139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "It proposes to combine workload balancing and sequence parallelism in distributed training of Diffusion Transformers.\nThis paper raises a concern whether it is a good fit to ICLR.  \nGreedy assignment has no optimality guarantees and can lag under rapidly shifting loads. Without theory or robustness analysis such as  competitive ratios, worst-case bounds, sensitivity to load noise, the contribution seems quite limited."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes an online redistribution of tokens to reduce stragglers when multimodal inputs create large sequence-length variance across GPUs\n- By integrating DeepSpeed-Ulysees-based sequence parallelism in the load-balancing decision process, it achieves minimal communication overhead and less than 1% workload discrepancy\n- Claimed performance improvements depend on fast intra-node links (NVLink) and a particular parallelism layout. It seems the payoff is infrastructure-dependent, thus less scientifically general.\n- Its fit to ICLR is not strong."}, "weaknesses": {"value": "- It benefits most when fast intra-node links (e.g., NVLink) are available since cross-node bandwidth/latency can erode the gains. \n- It does not look like a good fit to ICLR.  A load-balancing/throughput optimizer tied to a specific training stack reads as systems engineering, which typically fits MLSys/OSDI/EuroSys/SOSP better than ICLR.\n- Claimed results depend on fast intra-node links (NVLink) and a particular parallelism layout. \n- If improvements come purely from token routing and communication scheduling, not from changes to model, objective, or optimization dynamics. \n- Its results seem to emphasize a particular favorable setup.\n- Greedy assignment has no optimality guarantees and can lag under rapidly shifting loads. Without theory or robustness analysis (e.g., competitive ratios, worst-case bounds, sensitivity to load noise), the contribution can feel ad-hoc.\n- If adoption requires deep integration with specific setup such as DeepSpeed-Ulysses internals, and it raises questions about practical reproducibility outside that stack."}, "questions": {"value": "Property under rapidly shifting loads?\nWhat about the extension to other more general setups?\nHow do results change when inter-node bandwidth/latency is the bottleneck (e.g., no NVLink)?\nThe load assignment is greedy multi-knapsack. Any approximation or competitive guarantees? Worst-case bounds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W7Dp8OsBV9", "forum": "1VgUoPfl3z", "replyto": "1VgUoPfl3z", "signatures": ["ICLR.cc/2026/Conference/Submission3484/Reviewer_CBLp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3484/Reviewer_CBLp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834722463, "cdate": 1761834722463, "tmdate": 1762916750472, "mdate": 1762916750472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes KnapFormer, a lightweight online load balancer designed to mitigate token-level workload imbalance during distributed training of Diffusion Transformers (DiT). The core idea is to formulate token redistribution as a multi-knapsack assignment problem, assigning sequences to logical “compute bags” across GPUs to minimize per-device compute variance. A gamma-corrected workload model refines latency prediction by adjusting the quadratic attention cost term to match empirical latency.\nThe method integrates with DeepSpeed-Ulysses sequence parallelism using only one all-to-all communication per redistribution and a reversible mapping for downstream collation. Experiments on 32 × H100 GPUs show up to 2×–3× speedups in joint image–video pretraining scenarios and < 1% workload discrepancy across GPUs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-motivated system problem: The introduction clearly articulates token-length heterogeneity as a bottleneck in DiT training, connecting it to variable-length text inputs and varying visual token counts in mixed-resolution and image-video joint training.\n\n2. Methodological clarity: The paper provides explicit pseudocode-style descriptions and API examples, showing integration points both outside and inside transformer blocks. This improves reproducibility and readability.\n\n3. Minimal communication design: The load balancer operates via a single all-to-all collective, which is elegant and practically scalable for large GPU clusters. Besides, it does not require additional communication over DeepSpeed-Ulysses, making it easy to adopt without modifying attention kernels.\n\n4. Empirical validation and relevance: Table 1 presents three settings—low-res, mixed-res, and joint image–video pretraining—with metrics (WIR, FBL, TPS, HFU) that directly measure efficiency. The g8n4 topology reduces Forward-Backward Latency from 8.05 s → 3.44 s and improves HFU from 12.69% → 29.74%. These results demonstrate substantial throughput gains on realistic setups."}, "weaknesses": {"value": "1. Empirical evaluation limited to synthetic workloads: Section 4.1 explicitly notes that results are from “a training simulator for text-to-{image, video} diffusion training”. While realistic distributions are simulated, no actual end-to-end training curves (e.g., loss vs. steps) or validation throughput on real datasets are provided. This limits evidence that the gains translate to actual pretraining pipelines.\n\n2. Missing baseline comparisons with concurrent work: In Related Work (L160-L200), KnapFormer is compared only conceptually to MAGI-Attention (Zewei & Yunpeng 2025). However, Table 1 contains no quantitative comparison with existing balancing or routing methods such as MAGI-Attention or token-based adaptive batching systems. \n\n3. No ablation on the γ-correction factor: Figure 2 (L205-L215) introduces a fitted γ = 0.385 (H100) but the experiments later use γ = 0.49 (Table 1 note). The impact of this discrepancy or sensitivity to γ is never analyzed. A single fit per hardware may oversimplify modeling of latency variance across layers or data scales.\n\n4. Statistical rigor lacking. Table 1 reports single measurements (e.g., “TPS = 107.54 K”) without variance or confidence intervals. For efficiency metrics sensitive to random token distributions, multiple runs would strengthen reliability."}, "questions": {"value": "1. On real-world generalization: Since experiments use synthetic data, could you report results on an actual multimodal dataset (e.g., LAION-5B subsample or a video dataset) to demonstrate performance in non-simulated settings?\n\n2. On γ-parameter robustness: The γ used in experiments (0.49) differs from the earlier fitted 0.385. Was this re-fit on different workloads, or fixed arbitrarily? Please provide a short analysis showing how sensitive latency prediction and balancing performance are to γ.\n\n3. On communication cost quantification: You claim a “single all-to-all collective” suffices. Could you provide measured communication overhead (e.g., ms per iteration or % of total training time) to support that it’s negligible relative to compute?\n\n4. On ablations: In Table 1, g8n4 achieves the best results. Could you include an experiment showing how non-uniform topologies (e.g., g8n2 + g4n4) affect throughput?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2W1y9DXRur", "forum": "1VgUoPfl3z", "replyto": "1VgUoPfl3z", "signatures": ["ICLR.cc/2026/Conference/Submission3484/Reviewer_cJtD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3484/Reviewer_cJtD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903235518, "cdate": 1761903235518, "tmdate": 1762916750008, "mdate": 1762916750008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "KnapFormer proposes an online, sequence-chunk level load balancer for large distributed training of Diffusion Transformers (DiT). The method: (a) gathers per-sequence length metadata across ranks, (b) uses a (greedy) multi-knapsack style assignment to place sequences into logical “compute bags” (groups of 1..G GPUs), (c) splits sequences assigned to multi-GPU bags into contiguous chunks and performs a single all-to-all to redistribute chunks, (d) integrates with Ulysses/DeepSpeed sequence-parallel attention layouts (switching layouts via intra-bag all-to-alls) so attention can run efficiently (FlashAttention kernels), and (e) reverses the mapping after backprop so loss/outputs align. They augment the standard transformer FLOP model with a fitted γ factor to better predict latency and drive the knapsack weights. Experiments use a training simulator on 32 H100s for several multimodal scenarios (low-res, mixed-res, image+video) showing large reductions in workload imbalance and 2×–3× throughput gains in highly heterogeneous settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Practical problem & clear motivation.** Token / visual-token heterogeneity is real in multimodal diffusion training; addressing stragglers is valuable for throughput and cost. The paper identifies an important engineering bottleneck and proposes an end-to-end solution. \n    \n- **Simple, usable design.** The compute-bag abstraction and compact topology spec (e.g., g1n32+g2n16...) are intuitive and likely easy to plug into existing PyTorch/DeepSpeed pipelines; API snippets strengthen reproducibility claims. \n    \n- **Integration with existing sequence-parallel machinery.** Rather than reinventing distributed attention, they leverage Ulysses + FlashAttention, which helps keep per-block overhead low and allows use of high-performance kernels. This design choice improves the feasibility of adoption. \n    \n- **Low communication frequency.** The approach only does redistribution twice (pre-forward and post-backward) and uses single all-to-all collectives, which is appealing compared to per-layer token routing approaches (MoE or per-block rebalancing).\n    \n- **Empirical gains.** In highly heterogeneous settings the method shows substantial improvements in workload imbalance and tokens/sec (reported up to ~2–3×), demonstrating potential impact on training cost/time."}, "weaknesses": {"value": "- **Simplifying assumptions in workload model.** The latency model is FLOP-based with an empirical γ correction. While practical, it is hardware and kernel specific; the paper fits γ for H100 only and does not show sensitivity to γ, batch size, or kernel implementations. If γ changes (different GPU, FlashAttention version, different head/channel layouts), the knapsack decisions could be suboptimal. There is little robustness analysis.\n    \n- **Algorithmic / theoretical gaps.** The assignment uses a greedy knapsack heuristic without formal guarantees or complexity analysis. For very large N or rapidly changing sequence distributions, greedy may perform poorly; there is no analysis of worst-case behavior, convergence of per-round imbalance, or how frequently planning must run.\n    \n- **Communication & memory costs understated.** Claims of “single all-to-all” per redistribution underplay the cost when bags are large or cross-node. The paper reports that some bag topologies (e.g., g8n4) are best, but a detailed breakdown of communication time, peak memory for temporary packed tensors, and how these scale with bag size / cross-node links is missing. There is also no explicit treatment of how optimizer or activation checkpointing memory interacts with the extra buffers needed for routing. \n    \n- **Scope of applicability not fully characterized.** The method assumes homogeneous attention masks per datum and contiguous chunking of sequences. For models that use irregular attention masks (sparse/unified multimodal masks), sliding windows, or for architectures that cannot easily run FlashAttention on full sequences, applicability is unclear. Authors note this but do not provide solutions or experiments."}, "questions": {"value": "1. **Real cluster runs:** Can you provide wall-clock results from actual 32-H100 experiments (not simulator) showing the measured all-to-all costs, communication breakdown, and end-to-end epoch time? If these are in the supplement, please point to them. \n    \n2. **γ sensitivity:** How sensitive is the balancer to the choice of γ in Eq. (2)? If γ is misestimated by, say, ±20%, how much does TPS / WIR degrade? Have you tried cross-GPU (A100 vs H100) or kernel/version changes? \n    \n3. **Greedy solver behavior:** Why choose the greedy knapsack heuristic vs established approximate solvers (multi-dimensional knapsack, ILP relaxations)? Can you provide worst-case complexity and typical runtime for planning per step (N sequences, G GPUs)? Is planning ever a bottleneck? \n    \n4. **Frequency of rebalancing:** How often is plan_routing invoked in practice? Per iteration, per N iterations, or only when distribution changes? What is the overhead of repeated planning and how is stability of assignments ensured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Gyj2cYrv2W", "forum": "1VgUoPfl3z", "replyto": "1VgUoPfl3z", "signatures": ["ICLR.cc/2026/Conference/Submission3484/Reviewer_f64N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3484/Reviewer_f64N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985091839, "cdate": 1761985091839, "tmdate": 1762916749774, "mdate": 1762916749774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}