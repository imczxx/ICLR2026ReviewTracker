{"id": "kc5jbYHedw", "number": 6654, "cdate": 1757991441399, "mdate": 1759897902702, "content": {"title": "Inferring brain plasticity rule under long-term stimulation with structured recurrent dynamics", "abstract": "Understanding how long-term stimulation reshapes neural circuits requires uncovering the rules of brain plasticity. While short-term synaptic modifications have been extensively characterized, the principles that drive circuit-level reorganization across hours to weeks remain unknown. Here, we formalize these principles as a latent dynamical law that governs how recurrent connectivity evolves under repeated interventions. To capture this law, we introduce the Stimulus Evoked Evolution Recurrent dynamics (STEER) framework, a dual-timescale model that disentangles fast neural activity from slow plastic changes. STEER represents plasticity as low-dimensional latent coefficients evolving under a learnable recurrence, enabling testable inference of plasticity rules rather than absorbing them into black-box parameters. \nWe validate STEER with three benchmarks: synthetic Lorenz systems with controlled parameter shifts, BCM-based networks with biologically grounded plasticity, and longitudinal recordings from Parkinsonian rats receiving closed-loop DBS. Our results demonstrate that STEER recovers interpretable update equations, predicts network adaptation under unseen stimulation schedules, and supports the design of improved intervention protocols. By elevating long-term plasticity from a hidden confound to an identifiable dynamical object, STEER provides a data-driven foundation for both mechanistic insight and principled optimization of brain stimulation.", "tldr": "", "keywords": ["brain plasticity", "long-term stimulation", "recurrent dynamics"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38b1e328c5c9c53c05117fa94883751e6b1f7b4e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The original idea and intuition behind this paper are excellent. \n\nSTEER represents recurrent connectivity as a low-rank CP tensor decomposition and learns a stimulus-conditioned dynamical law $z_{k+1}=g_{\\theta}(z_k,\\bar{u}_k)$  governing how low-dimensional motif coefficients evolve across sessions. Identifiability is promoted through unit-norm and orthogonality constraints and an optional sign mask enforcing Dale’s principle. The approach is validated on increasingly realistic tasks: synthetic Lorenz systems, a Bienenstock–Cooper–Munro (BCM) plasticity model, and a longitudinal Parkinson’s disease DBS dataset.\n\nSeveral essential details are relegated to the appendix and the presentation undermine the overall quality of the work. Some of those choices make me dubious on the validity of the model.\n\nI am giving the paper an overall 4 \"marginally below the acceptance threshold\" but will consider increasing that grade if the weaknesses are adresses."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "This work is well motivated and conceptually strong and addresses a fundamental challenge in neuroscience of understanding how long-term stimulation reshapes neural circuits. The proposed framework introduces a dual-timescale formulation that separates fast within-session dynamics from slow plasticity adaptation.\n\nThe paper is innovative. By employing a low-rank decomposition of the recurrent connectivity tensor, STEER enforces structure and identifiability, leading to interpretable motif-level representations of plasticity. \n\nThe experimental design is particularly strong. The authors carefully structure their validation in increasing order of biological realism: starting from the synthetic Lorenz system, progressing to a controlled Bienenstock–Cooper–Munro (BCM) plasticity model, and culminating with real longitudinal deep-brain stimulation (DBS) data in Parkinsonian rats."}, "weaknesses": {"value": "The presentation needs a brush\n\nClarity:\n-\"session\" is never really well defined in the beginning and is such a vague term. Does itrefers to a single day, trial block, animal or experiment ? \n- section 4.1. Lorentz equation should be in main text, not hidden in the appendix\n- l299-204.  the are no motivation behind equation 10\n- Figure 4 is too small \n\nMotivation \n-  In the Lorentz experiment, it's not clear why this specific synthetic plasticity is chosen; they appear arbitrary.  A brief justification would be okay and would help readers evaluate whether the benchmark reflects plausible plasticity.  (you already justify parameters range, does the same justification work for the function form ?)\n\nConceptually:\n- you are presenting the performances on the states but the evaluation of plasticity (evaluation of the parameters) changes from benchmark to benchmark\n- similarly you don't provide systematic comparison with other methods"}, "questions": {"value": "* Identifiability and factor uniqueness: The authors reduce CP scaling/sign indeterminacy by constraining motif factors to unit‑norm and penalising orthogonality.  Do these constraints guarantee a unique solution, or could different factor orderings or scalings still yield the same connectivity?  How do they interpret the motifs biologically when multiple equivalent factorizations exist?\n\n* Report performance on the inferred plasticity, not just on state prediction. The paper emphasises within‑session predictive accuracy ($R^2$) and explained variance on held‑out trajectories, but these metrics speak only to how well the model predicts neural activity.  For a method whose main goal is to infer the slow plasticity rule, one would also expect quantitative assessments of how accurately the latent plasticity dynamics $z_{k+1}=g_\\theta(z_k,\\bar{u}_k)$ and the motif coefficients $c_k$ recover the ground truth.  In the Lorenz and BCM benchmarks, the authors report a dynamical‑similarity score (DSA = 0.63) and correlations with BCM thresholds, but there is no systematic evaluation of plasticity inference across rank choices or hyperparameters.  Likewise, in the DBS experiment they assess alignment between $\\Delta c$ and functional connectivity rather than the fidelity of the inferred slow law.  Expanding these analyses to provide error metrics on the inferred plasticity would strengthen the claim that STEER recovers the underlying rule."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZCzPWqZ1EN", "forum": "kc5jbYHedw", "replyto": "kc5jbYHedw", "signatures": ["ICLR.cc/2026/Conference/Submission6654/Reviewer_YUkh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6654/Reviewer_YUkh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752806162, "cdate": 1761752806162, "tmdate": 1762918965911, "mdate": 1762918965911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper developed a framework, named Stimulus-Evoked Evolution Recurrent dynamics (STEER), a structured model that separates fast within-session activity from slow across-session adaptation. The method explored how recurrent connectivity evolves under repeated interventions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper treats long-horizon plasticity as a latent dynamical law, rather than unstructured parameter drift.\n2. The model separates fast within-session dynamics and slow across-session evolution."}, "weaknesses": {"value": "1. The dynamical systems have input weights and readouts, which can also encode some information of connectivity. Therefore, it is still unsure if the connectivity recovered by the model is true or believable. (This may have been claimed by the author in the limitation part, but it remains a substantive concern.)\n2. The authors stated that the proposed method enforces an identifiable separation between fast within-session responses and slow network reconfiguration. But there’s no theorem or ablation demonstrating uniqueness of learned dynamics."}, "questions": {"value": "1. If you learn fast dynamics at first, then learn slow dynamics of these fast ones, will the results be different? Learning jointly requires hyperparameter search such as lambda slow, are the results sensitive to these parameters? \n2. Could the author define delta W? (Figure 4D) Since there are many Ws in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SQde6wkXRX", "forum": "kc5jbYHedw", "replyto": "kc5jbYHedw", "signatures": ["ICLR.cc/2026/Conference/Submission6654/Reviewer_sTJk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6654/Reviewer_sTJk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766334763, "cdate": 1761766334763, "tmdate": 1762918965240, "mdate": 1762918965240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STEER, a dual-timescale recurrent framework for inferring long-term neural plasticity rules from longitudinal stimulation data. STEER models fast neural activity within sessions using a low-rank recurrent network, while slow structural changes across sessions are captured by a learnable latent dynamical system driven by stimulation input. Experiments on synthetic benchmarks and Parkinson’s DBS data show that STEER allows disentangle short- and long-term dynamics, recovers biologically interpretable motifs, and generalizes to unseen stimulation protocols."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Motivation:** The paper addresses an underexplored area by extending short-term plasticity modeling toward the longer timescales of circuit reorganization. The proposed framework offers a structured, data-driven way to describe how stimulation may gradually reshape network connectivity.\n2. **References:** The related research is carefully reviewed, linking established neuroscience findings on Hebbian and homeostatic mechanisms with recent machine learning approaches for recurrent dynamics and meta-learning. This grounding strengthens the biological and methodological motivation.\n3. **Evaluation:** The method is tested on two synthetic datasets (Lorenz and BCM) and one real Parkinson’s DBS dataset. The experiments provide supporting evidence that the model can capture slow network adaptations and generate interpretable patterns, though further validation would be beneficial."}, "weaknesses": {"value": "1. **Evaluation:** Results on the BCM and Parkinson’s DBS datasets appear modest, and there is a visible mismatch between the ground truth in Fig. 4(a) and the model output in Fig. 4(b), suggesting partial recovery of the connection change. \n\n2. **Baselines:** Only one baseline (MD-SSM) is evaluated on the BCM simulation. Including other relevant machine learning approaches discussed in the related work would strengthen the empirical comparison.\n\n3. **Benchmark:** The evaluation spans two synthetic and one real dataset. Additional simulations or real neural datasets would better demonstrate the method’s generalization and robustness across experimental settings."}, "questions": {"value": "1. Could the authors discuss how choices such as rank, learning rate, or regularization impact performance? What might explain the prediction gap between the ground truth in Fig. 4(a) and the inferred results in Fig. 4(b)?\n\n2. How does the method scale to larger networks or longer time series? Please comment on computational cost and potential limitations for larger-scale simulations.\n\n3. Since the model infers latent dynamics and connectivity from observed activity, how does it avoid learning spurious correlations due to partial observations and noises?\n\n4. More implementation details (e.g., initialization, optimizer settings, runtime) would be helpful for reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lHYb39LUEV", "forum": "kc5jbYHedw", "replyto": "kc5jbYHedw", "signatures": ["ICLR.cc/2026/Conference/Submission6654/Reviewer_B3ZH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6654/Reviewer_B3ZH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968120219, "cdate": 1761968120219, "tmdate": 1762918964521, "mdate": 1762918964521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STEER, a method to infer the dynamical rules underlying long-term evolution of recurrent connectivity over sessions/hours/days from neural data. The framework learns low-dimensional latent coefficients for the neural dynamics underlying a session, which themselves evolve over slower timescales – thus allowing the inference of plasticity rules explicitly. The framework is trained by jointly optimising for reconstruction fidelity within a session, regularisation for consistency of inferred plasticity rule across sessions, and also regularisation for smoothness. The authors validate their framework on three tasks – two synthetic tasks involving learning a Lorenz system with varying coefficients and synthetic data generated with a specific, known plasticity rule; and one real dataset of neural recordings from rats undergoing DBS treatment for Parkinson's disease. The main claim is that STEER disentangles effectively the within-session dynamics from cross-session dynamics, allowing better extraction and interpretability of the rules/changes underlying cross-session variability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The framework seems principled in its design and the approach overcomes disadvantages of certain prior meta-learning approaches by not assuming a specific functional form of the underlying learning rule (e.g., not restricted to just variants of Oja's rule).\n* The presentation and figures are mostly clear.\n* The experimental evaluations span both synthetic tasks and real data, which is important for such works."}, "weaknesses": {"value": "* The use of DSA is good but you only report a single DSA value of 0.63 with no baseline or control/chance value. DSA is a relative metric, as emphasised in the original paper. Thus, it is not possible to know whether a DSA of 0.63 is good without a baseline comparison. Could the authors provide a baseline on shuffled values of the implicit factor, for example, and show that the actual inferred dynamics have a higher DSA score with the true dynamics?\n* In Fig. 4f I am not sure you can claim that you perform comparably to MD-SSM without a statistical significance test. The avg. score for STEER seems lower even when considering error bars. What are the error bars over – sessions or seeds? If the result of a stat. test is not significant then that validates the claim of comparable performance. I would also ask that the plot y axis range be restricted so it's clearer what the difference in performance is.\n* There is no quantification for Fig. 4d where it is claimed that STEER better recovers the $\\Delta \\mathbf{W}$ compared to MD-SSM. As far as I'm concerned, the block structure of both the MD-SSM nor STEER weight changes do not particularly resemble the ground truth weight changes. Could the authors comment on this? The value scales are also quite different and while I understand that exact values/scale may not matter, maybe they need to be normalised so the plots can be compared better?\n* There is no comparison against the meta-learning approaches mentioned in the introduction. While I can understand the difference between these methods, I think it is important to see clear evidence to back up the claim that they cannot model the effects of DBS, for example, and also that STEER can recover plasticity rules as those methods do in short-term settings.\n* Minor nit: in lines 130-131 you cite Bredenberg et al. and Kepple et al. as fitting synaptic plasticity rules through gradient-based optimisation on observational data. While Bredenberg et al. does do this, to my knowledge, they do not do it on real data. Meanwhile, Kepple et al. do not fit parameters for learning rules – they mainly evaluate different learning rules and curricula in an in silico setting on the basis of learning speeds (and classifying between these on the basis of such observations), again not working with real data to my knowledge."}, "questions": {"value": "* Minor typo in Fig. 1a: \"Brian\" -> \"Brain\".\n* Why is the evolution of coefficients jagged and not smooth in the Lorenz system plots?\n* What is the difference between Fig. 4 and App. Fig. 1? Why are the results so different qualitatively?\n* Could the authors provide additional information/motivation on the consistency and smoothness terms? How are the coefficient $\\lambda$s tuned?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TqUBX4mE7o", "forum": "kc5jbYHedw", "replyto": "kc5jbYHedw", "signatures": ["ICLR.cc/2026/Conference/Submission6654/Reviewer_TgyK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6654/Reviewer_TgyK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152692050, "cdate": 1762152692050, "tmdate": 1762918964144, "mdate": 1762918964144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}