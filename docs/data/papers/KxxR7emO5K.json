{"id": "KxxR7emO5K", "number": 18858, "cdate": 1758291561884, "mdate": 1759897077332, "content": {"title": "OmniCVR: A Benchmark for Omni-Composed Video Retrieval with Vision, Audio, and Text", "abstract": "Composed video retrieval requires retrieving a target video given a source video and a textual modification instruction, demanding fine-grained reasoning over multimodal transformations. Existing benchmarks primarily emphasize vision–text alignment and overlook the rich semantic signals in audio, such as speech, music, and environmental sounds, which are often decisive for understanding. To address this limitation, we introduce OmniCVR, a large-scale benchmark for omni-composed video retrieval that integrates vision, audio, and text as first-class modalities. OmniCVR is built through a scalable pipeline that combines segmentation, omni-modal annotation, and dual validation by both large language models and human experts. The benchmark features vision-centric, audio-centric, and integrated queries, with integrated queries forming the majority to better reflect real-world multimodal scenarios. We further propose AudioVLM2Vec, an audio-aware extension of VLM2Vec that incorporates explicit audio semantics, achieving state-of-the-art performance and exposing fundamental gaps in current multimodal retrieval systems.", "tldr": "", "keywords": ["Composed Video Retrieval; Multimodal Benchmark; Audio-Visual Queries"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/81c3361a127ec39b81b817909ced063726414207.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the OmniCVR benchmark for composed video retrieval, encompassing vision, audio, and text modalities. A key novelty of OmniCVR is its pioneering inclusion of the audio modality as a modification target, distinguishing it from previous text-video and composed video retrieval benchmarks. Furthermore, the paper proposes a corresponding AudioVLM2Vec framework, which effectively highlights significant limitations of existing models when evaluated on the newly-constructed OmniCVR benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. As a benchmark paper, this work contains the basic elements of a newly constructed benchmark. It covers essential aspects such as the limitations of previous benchmarks, the construction pipeline and its analysis, and the presentation of several baselines evaluated on the new benchmark.\n\n2. The identified problem, i.e., the absence of an audio stream in previous Composed Video Retrieval (CVR) benchmarks, is indeed a prevalent issue in existing literature. This establishes a clear and compelling motivation for the current work.\n\n3. The observed poor performance of several baselines when evaluated on the newly-introduced OmniCVR benchmark effectively highlights the limitations of existing models and, by extension, underscores the necessity and value of benchmarks that incorporate the audio modality."}, "weaknesses": {"value": "1. My primary concern relates to the modification texts in the current OmniCVR benchmark. As shown in Table 2, the average length of queries is 52.6 words, which seems excessively long compared to previous literature, as illustrated in Figures 1 and 2. The modification texts appear to contain too many details, especially for integrated-type retrieval, which, in turn, makes the task easier to some extent.\n\n2. This observation is further supported by the performance of the newly proposed AudioVLM2Vec framework. By incorporating audio signals into LLMs, AudioVLM2Vec achieves substantial gains of +28.5% and +64.8% in overall and audio-centric retrieval, respectively. These results are obtained without training on the provided training set, correct? The extremely lengthy modification texts make this task much easier, resembling a text-video retrieval scenario where the source videos play a less significant role.\n\n3. I am also confused by the statements regarding long-form videos in Lines 212, 482, and 602. The average length of the source videos is only 11.8 seconds, which can hardly be categorized as long-form. Additionally, why is the real long-video benchmark MLVU introduced in Line 130? It appears that MLVU is not directly related to the composed video retrieval task.\n\n4. Are there any performance comparisons for vision-centric retrieval? The main differences appear to arise from the audio-centric retrieval results. How do previous works perform on the visual-centric category within the OmniCVR benchmark?"}, "questions": {"value": "Please see the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3djdC42LCa", "forum": "KxxR7emO5K", "replyto": "KxxR7emO5K", "signatures": ["ICLR.cc/2026/Conference/Submission18858/Reviewer_H76e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18858/Reviewer_H76e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761226768483, "cdate": 1761226768483, "tmdate": 1762930826745, "mdate": 1762930826745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniCVR, a large-scale benchmark for composed video retrieval that treats visual, audio, and textual modalities as equally important. It addresses the oversight of audio in existing benchmarks by including audio-centric and integrated queries, where integrated queries dominate to reflect real-world multimodal scenarios. OmniCVR is constructed via a scalable pipeline involving video segmentation, omni-modal annotation, and dual validation (using LLMs and human experts). Evaluations show that current models struggle with audio-centric tasks, prompting the proposal of AudioVLM2Vec—an extension of VLM2Vec that explicitly incorporates audio semantics. This model achieves state-of-the-art performance, revealing critical gaps in handling audio-dependent queries and underscoring the necessity of audio-aware multimodal retrieval systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: This work first define and benchmark \"audio-inclusive composed video retrieval.\" This effectively eliminates the key limitation of prior CVR benchmarks: the neglect of audio as a first-class semantic modality. The proposed AudioVLM2Vec also offers a novel, pragmatic architecture for audio infusion.\n\nClarity: The paper is easy to follow and understand.\n\nSignificance: The paper proposes \"audio-inclusive composed video retrieval\", which enables direct future research on audio-visual-text reasoning and is an essential tool for developing genuinely holistic video understanding models."}, "weaknesses": {"value": "1. While the paper's introduction of audio-centric queries is commendable, its treatment of \"audio\" remains oversimplified. A truly comprehensive audio-centric benchmark must consider a wider array of acoustic, semantic, and para-linguistic dimensions and provide specialized evaluations for each. The benchmark misses the opportunity to define and evaluate based on these essential audio dimensions:\n\n     (a)  Para-linguistic Features in Speech:  (i)Emotion & Tone, (ii) Speaker Characteristics \n\n     (b) The lexical content in Speech\n\n     (c) Hierarchy of Environmental Sounds: \"Environmental Sound\" is a vast category that should be broken down into:   (i) Nature Sounds (wind, rain, animals), (ii) Mechanical Sounds (engines, machinery, alarms), (iii) Urban Soundscapes (traffic, crowds, construction), (iv). Foley/Action Sounds (footsteps, glass breaking, doors closing)\n\n     (d) Temporal Dynamics: This involves rhythm, pace, and intensity over time. A query could be: \"Find the same scene but where the background music has a slow, steady beat instead of a fast, erratic one.\"\n\n2. Lack of Specialized Evaluation per fine-grained Dimension: The paper's aggregated \"Audio-Centric\" results (Table 4) are misleading. A model might excel at modifying speech content but fail completely at modifying musical timbre or emotional tone. This critical diagnostic information is lost.\n\n3. In the same way, Limited Exploration of Audio-Visual Interactions: The benchmark and proposed method treat audio and vision as largely separate streams that are fused late (e.g., by concatenating text descriptions). It does not explicitly model or evaluate the complex, synergistic relationships between audio and vision, such as causal links (a person's mouth movements and their speech). The query text may be that \"Take the source video of a man speaking and replace his audio with a different language while also modifying the video to show his lips moving out of sync.\""}, "questions": {"value": "1. The paper contains many figures and tables that are not explicitly cited or referenced in the text.\n\n2. Efficiency Benchmarking: Report the inference latency and FLOPs for AudioVLM2Vec compared to other baselines, highlighting the cost of the audio-to-text step.\n\n3. Inadequate Ablation Studies: The paper fails to provide sufficient ablation analysis to disentangle the contributions of different components in AudioVLM2Vec. Critical questions remain unanswered: How much performance gain comes from audio transcription versus the fusion mechanism? How does the model perform on non-speech audio domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Nothing"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QlW3biFwk5", "forum": "KxxR7emO5K", "replyto": "KxxR7emO5K", "signatures": ["ICLR.cc/2026/Conference/Submission18858/Reviewer_Too4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18858/Reviewer_Too4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928296161, "cdate": 1761928296161, "tmdate": 1762930825967, "mdate": 1762930825967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce OmniCVR, a composed video retrieval benchmark that incorporates audio as a key modality alongside vision and text. The authors argue that the queries included in the dataset are representative of real-world queries, and target audio in addition to vision. Additionally, the authors extend the VLM2Vec retrieval method to simultaneously model audio, which they name AudioVLM2Vec. This approach achieves SoTA performance in multimodal retrieval."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper accurately identifies that audio is an under-utilized modality in video retrieval. We do lack benchmarks that target this modality.\n\n- In addition to dialogue, the dataset also centers non-language audio like music and sound effects, which historically have been even less focused on than dialogue.\n\n- The dataset is large enough to serve as a training set in addition to validation.\n\n- AudioVLM2Vec achieves impressive performance compared to existing methods. Its development is sensible and is described clearly. The chosen comparison methods are representative of the current research space.\n\n- The paper is generally sound and well-written.\n\n- The limitations and future work section is detailed and thoughtful."}, "weaknesses": {"value": "- The video-text retrieval benchmarks section in the related works segment is quite small. There have been many more video retrieval benchmarks, some of which center audio. Describing the differences between this dataset and others that center audio (VaTeX, MultiVENT 2.0, etc.) would be informative for those evaluating the novelty of the dataset.\n\n- It would be nice to describe the genres and types of content focused on in the dataset in more depth."}, "questions": {"value": "- What key contributions does this dataset make compared to existing video retrieval benchmarks that center audio? What is the importance of composed video retrieval tasks?\n\n- What genres of video are included in the dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X7FgOSMHI2", "forum": "KxxR7emO5K", "replyto": "KxxR7emO5K", "signatures": ["ICLR.cc/2026/Conference/Submission18858/Reviewer_2XpL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18858/Reviewer_2XpL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961436249, "cdate": 1761961436249, "tmdate": 1762930825504, "mdate": 1762930825504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniCVR, a new benchmark designed to evaluate omni-composed video retrieval, where the system retrieves a target video given a source video and a textual modification instruction that may involve changes in visual content, audio, or both.\nThe authors describe a large-scale data construction pipeline that combines LLM-assisted multimodal annotation (Qwen2.5-Omni, Gemini 2.5 Pro) with validation across multiple modalities. They also propose AudioVLM2Vec, an embedding model that integrates audio information via textual transcriptions to better handle audio–visual composition. The benchmark aims to capture fine-grained multimodal reasoning, extending beyond traditional text–video retrieval tasks.."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an underexplored and ambitious problem where compositional video retrieval jointly involves visual, auditory, and textual reasoning. By integrating audio-based instructions alongside visual composition, it broadens the conventional scope of video retrieval tasks. This is a valuable and forward-looking addition for multimodal benchmarks.\n\n- The benchmark creation process is well-documented and systematically structured. The use of multiple LLMs for multimodal captioning, filtering, and validation makes the dataset large, diverse, and reproducible. This methodological transparency is a clear strength.\n\n- The implementation details, model configurations, and evaluation metrics are clearly presented. The writing quality is high, and the paper provides good contextualization against prior work ."}, "weaknesses": {"value": "- The task assumes that, for a given source video and a detailed compositional instruction, another video exists in the corpus that satisfies all the described changes. Many examples involve complex spatial, temporal, or auditory transformations that are implausible to find in any real-world dataset. Such a setup feels conceptually closer to conditional video generation than retrieval. The paper does not convincingly justify why retrieval is a meaningful or achievable approach for these highly specific instructions.\n\n- If OmniCVR truly captures omni-compositional reasoning, models trained on it should exhibit strong transfer to conventional video retrieval datasets (e.g., MSR-VTT, VATEX, Dense-WebVid-CoVR). Yet the paper does not test this. The absence of out-of-domain evaluation makes it impossible to assess whether the learned representations generalize beyond the synthetic distribution or simply overfit to the compositional language space introduced by the benchmark. \n\n- While the benchmark emphasizes audio–visual–text reasoning, the proposed model (AudioVLM2Vec) primarily uses text transcriptions of audio, not raw acoustic embeddings. This simplifies audio reasoning into another text-conditioning signal, limiting the extent to which the benchmark truly evaluates multimodal understanding.\n\n- The paper does not present qualitative examples or human evaluations showing that retrieved videos actually match the intended compositional modifications. Without such validation, it is unclear whether the benchmark’s retrieval results are semantically meaningful."}, "questions": {"value": "1. Many of the benchmark’s compositional instructions describe modifications that seem infeasible to find in existing datasets (e.g., changing lighting, replacing sound events, or altering temporal composition). Could the authors clarify why they chose to formulate this as a retrieval problem rather than as conditional video generation? Do they believe that realistic corpora can ever contain such compositional matches at scale?\n\n2. If the benchmark truly captures omni-compositional reasoning, models trained on OmniCVR should generalize to more conventional video retrieval datasets such as MSR-VTT, VATEX, or ActivityNet Captions. Have the authors tested such out-of-domain performance? If not, how can they claim that the benchmark teaches general multimodal retrieval rather than overfitting to its own synthetic compositions?\n\n3. Could the authors provide example showing that the retrieved videos actually reflect the compositional modifications specified in the text?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9tO6Lb0Qlx", "forum": "KxxR7emO5K", "replyto": "KxxR7emO5K", "signatures": ["ICLR.cc/2026/Conference/Submission18858/Reviewer_F9DJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18858/Reviewer_F9DJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762165138600, "cdate": 1762165138600, "tmdate": 1762930824945, "mdate": 1762930824945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}