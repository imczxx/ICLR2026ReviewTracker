{"id": "rE64yoFKY9", "number": 15847, "cdate": 1758256057217, "mdate": 1759897277957, "content": {"title": "Boosting Targeted Adversarial Transferability: A Generative Approach Guided by Core Target Samples", "abstract": "Adversarial examples generated on one model can often be transferred to other unseen models, but achieving high targeted transferability remains challenging due to overfitting—especially under single-surrogate constraints. In this work, we propose BAT, a generative approach that Boosts targeted Adversarial Transferability by training the generator to align its outputs with a curated set of high-confidence \\textit{core target samples}. These samples—either selected from real data or synthesized from noise—serve as guidance across both output and feature spaces. To mitigate overfitting without requiring multiple surrogates, BAT employs an ensemble of frozen discriminators derived via pruning from a single pretrained surrogate model. BAT is applicable whether both the generator's training (source) and the evaluation images come from the target models’ training domain or exhibit a domain shift; it remains effective even without real target-class images during training. Extensive experiments on ImageNet-1K show that BAT notably outperforms existing $\\ell_{\\infty}$-constrained targeted attacks. We also provide theoretical bounds that reveal how ensemble size influences transferability, aligning with observed empirical trends.", "tldr": "This paper proposes BAT, a generative method that improves targeted adversarial transferability using pruned surrogate ensembles and core target guidance.", "keywords": ["Targeted Adversarial Attack", "Transferable Attack", "Generative AI", "Deep Neural Networks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf1ef79a91265da0e36bc26faab8524bced9d318.pdf", "supplementary_material": "/attachment/5164424d20433578157cab7d00fc1ded79671686.zip"}, "replies": [{"content": {"summary": {"value": "he paper introduces BAT, a generative targeted-transfer attack that aligns adversarial examples to a small “core” set of high-confidence target samples in both output and intermediate feature spaces. To overcome single-surrogate overfitting, BAT builds a self-ensemble of frozen discriminators by pruning a single pretrained model, avoiding extra training while injecting boundary diversity. Three variants handle data availability: BAT-BS (best real target images), BAT-CS (confidence-crafted target references), and BAT-CN (references synthesized from noise), covering both P=Q and P≠Q regimes. On ImageNet-1K and a Painting domain shift, BAT yields notably higher targeted TSR than strong iterative and generative baselines, with a simple theory linking transferability to ensemble size and its diminishing returns."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear, modular method: dual-space alignment to high-confidence core targets plus pruned-ensemble discriminators to mitigate single-surrogate overfitting.\n- Broad applicability: supports domain match and shift and target-data-guided vs. target-data-free training (BAT-CN works without real target images).\n- Strong empirical gains: consistent TSR improvements over state of the art across many victim architectures, including evaluations against robustly trained models.\n- Ablations isolate contributions of ensemble size and core-set selection and show monotonic improvements from baseline to full BAT variants.\n- Theory offers lower/upper bounds explaining the role of ensemble size and why gains saturate, matching empirical trends."}, "weaknesses": {"value": "- Statistical reporting is light: no multi-seed confidence intervals or seed-paired comparisons; robustness claims could be sensitive to randomness\n- Threat-model breadth could be expanded: emphasis on $\\ell_\\infty$ at a fixed $\\epsilon$; limited coverage of ℓ2 or structure-preserving perturbations in the main text\n- The pruning-diversity assumption is plausible but under-analyzed: lack of explicit diversity metrics (e.g., gradient/decision disagreement) vs. multi-surrogate ensembles\n- While theoretical support for a tighter $\\ell_2$ constraint than $\\ell_\\infty$ holds, empirical testing would more strengthen the claim\n- Limited discussion of compute/latency for training and inference, especially as |Ds| and core-set size scale"}, "questions": {"value": "- How stable are the reported gains across multiple seeds, and can you provide confidence interval range comparisons vs. the strongest baseline?\n- Can you quantify ensemble diversity induced by pruning (e.g., gradient cosine, boundary disagreement) and relate it to TSR?\n- Do the main trends persist for a second norm (ℓ2) and a simple spatial/color threat without heavy retuning?\n- What are the compute/latency costs for training and inference as |Ds| and core-set size vary, and where is the best accuracy/efficiency frontier?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "noY25oTu6b", "forum": "rE64yoFKY9", "replyto": "rE64yoFKY9", "signatures": ["ICLR.cc/2026/Conference/Submission15847/Reviewer_Q6HX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15847/Reviewer_Q6HX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760949274774, "cdate": 1760949274774, "tmdate": 1762926070216, "mdate": 1762926070216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents BAT,  a generative attack framework for targeted adversarial transfer under the constraint of a single surrogate. BAT is based on aligning both output and intermediate feature spaces of generated adversarial examples to those of a curated set of high-confidence core target samples. The key contribution is the creation of a diverse, frozen discriminator ensemble by pruning a single pretrained surrogate, thereby avoiding the need for multiple trained surrogates. The approach is evaluated in both domain-matched and domain-shift scenarios and supports both target-data-guided and target-data-free settings. Through extensive experiments and theoretical analyses, BAT is shown to improve targeted transferability over prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work has a comprehensive evaluation  under both no-shift and domain-shift settings  across diverse architectures.\n\n2.  Detailed ablations (Figure 5, Table 6) help EXPLAIN where performance gains come from,  diversified discriminators, core target selection.\n\n3.  The section 3 and algorithm 1 and 2 in the appendix and clear and easy to understand.\n\n4. The self-ensemble is simple and effective."}, "weaknesses": {"value": "1. The description of the generator architecture is under-specified.  Since it is an important component of the proposed work,  the authors should provide a detailed description.\n\n2. It is easy to convert some untargeted transferable attacks (e.g., admix-DT ) to the targeted ones. The authors should compare with them\n\n\n[1] Xiaosen Wang, Xuanran He, Jingdong Wang, and Kun He. Admix: Enhancing the transferability of adversarial attacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16158–16167, 2021."}, "questions": {"value": "1. Augmentation strategies can also be viewed as a form of self-ensemble. Could data-level self-ensemble via various augmentations achieve better performance compared to the model-level self-ensemble achieved by pruning? if data-level self-ensemble and model-level self-ensemble are combined, can the TSR be further improved?\n\n2. While the paper presents an interesting framework for boosting targeted transferability, I remain concerned about the level of novelty. The idea of ensembling surrogates to improve transferability is well established (e.g.,[2]); the use of feature-space alignment is also explored (e.g., [3]);  The notion of target samples as anchors is also explored (e.g., TTAA). So is this work a recombination of existing techniques into the generative targeted-transfer setting? I am not sure whether I understand correctly.\n\n\n[2] Hung-Jui Wang, Yu-Yu Wu, and Shang-Tse Chen. Enhancing targeted attack transferability via diversified weight pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2904–2914, 2024a.\n\n[3] Zhipeng Wei, Jingjing Chen, Zuxuan Wu, and Yu-Gang Jiang. Enhancing the self-universality for transferable targeted attacks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12281–12290, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S9UT6YEViV", "forum": "rE64yoFKY9", "replyto": "rE64yoFKY9", "signatures": ["ICLR.cc/2026/Conference/Submission15847/Reviewer_Np53"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15847/Reviewer_Np53"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823110927, "cdate": 1761823110927, "tmdate": 1762926069714, "mdate": 1762926069714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BAT, a generative method to improve targeted adversarial transferability by training a generator to align perturbations with curated high-confidence core target samples, guided in both output and feature spaces. To reduce overfitting without multiple surrogates, BAT uses an ensemble of frozen discriminators obtained by pruning a single pretrained surrogate, and it remains effective under domain shift and even without real target-class images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Multiple BAT variants are developed and evaluated, offering a clear view of design choices and their impact. \n2. Against competitive baselines, BAT demonstrates consistent gains, supporting the claimed effectiveness.\n3. On ImageNet-1K, BAT surpasses existing ℓ∞-constrained targeted attacks and is supported by theoretical bounds linking ensemble size to transferability, matching empirical trends."}, "weaknesses": {"value": "1. The rationale for why randomly pruning a frozen surrogate yields effective discriminator ensembles is under-explained; a deeper analysis or ablation would clarify mechanism and sensitivity.\n2. The preliminaries section is cluttered and difficult to follow; tighter structure and clearer notation would improve readability.\n3. Criteria for identifying “high-confidence” target samples are insufficiently specified."}, "questions": {"value": "1. What mechanism makes randomly pruned, frozen discriminator ensembles effective for targeted transfer?\n2. How well do the theoretical bounds track empirical gains with ensemble size, does BAT generalize across model families beyond the surrogate, and what are the main failure modes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mbEoD5f7sQ", "forum": "rE64yoFKY9", "replyto": "rE64yoFKY9", "signatures": ["ICLR.cc/2026/Conference/Submission15847/Reviewer_b9rh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15847/Reviewer_b9rh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974169634, "cdate": 1761974169634, "tmdate": 1762926069199, "mdate": 1762926069199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BAT (Boosting Adversarial Transferability), a generative framework that enhances targeted adversarial transferability by training generators to align outputs with curated high-confidence core target samples. The method employs an ensemble of frozen discriminators derived via pruning from a single pretrained surrogate model, eliminating the need for multiple surrogates. Theoretical bounds are provided to explain how ensemble size influences transferability. Experiments demonstrate consistent improvements in targeted success rates across various settings, including no domain shift, domain shift, and against robust models.​"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Innovative Single-Surrogate Ensemble Approach: The paper introduces a novel method to create diverse discriminator ensembles through pruning of a single pretrained model, eliminating the need for multiple distinct surrogate models while still achieving strong transferability. This addresses a key limitation of existing methods that require access to multiple models.​\nTheoretical Foundations with Practical Insights: The work provides rigorous theoretical analysis establishing lower and upper bounds on targeted transferability, revealing how ensemble size trades off with performance. This theoretical framework aligns well with empirical observations and offers valuable guidance for practical implementation.​"}, "weaknesses": {"value": "1. Your experimental evaluation primarily focuses on transfer scenarios using ResNet and DenseNet architectures. However, the increasing adoption of vision transformers (ViT) in computer vision applications necessitates a more comprehensive analysis of cross-architecture transferability. Could you please conduct additional experiments to address: (1) ResNet-to-ViT transferability across multiple ViT variants (e.g., ViT-Base, ViT-Large, DeiT); (2) ViT-as-surrogate transferability to traditional convolutional networks.\n\n2. Universality under domain shift: Beyond the single Painting↔ImageNet experiment, could you provide additional results on other cross-domain pairs (e.g., Sketch, Photo, synthetic datasets) to substantiate that the “core-sample + self-ensemble” strategy remains robust across a wider spectrum of distributional discrepancies?"}, "questions": {"value": "See weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cmn5Io4Yr6", "forum": "rE64yoFKY9", "replyto": "rE64yoFKY9", "signatures": ["ICLR.cc/2026/Conference/Submission15847/Reviewer_EqLf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15847/Reviewer_EqLf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983384868, "cdate": 1761983384868, "tmdate": 1762926068815, "mdate": 1762926068815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}