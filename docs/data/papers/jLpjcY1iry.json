{"id": "jLpjcY1iry", "number": 15088, "cdate": 1758247572268, "mdate": 1759897329574, "content": {"title": "LDT: Layer-Decomposition Training Makes Networks More Generalizable", "abstract": "Domain generalization methods can effectively enhance network performance on test samples with unknown distributions by isolating gradients between unstable and stable parameters. However, existing methods employ relatively coarse-grained partitioning of stable versus unstable parameters, leading to misclassified unstable parameters that degrade network feature processing capabilities. We first provide a theoretical analysis of gradient perturbations caused by unstable parameters. Based on this foundation, we propose Layer-Decomposition Training (LDT), which conducts fine-grained layer-wise partitioning guided by parameter instability levels, substantially improving parameter update stability. Furthermore, to address gradient amplitude disparities within stable layers and unstable layers respectively, we introduce a Dynamic Parameter Update (DPU) strategy that adaptively determines layer-specific update coefficients according to gradient variations, optimizing feature learning efficiency. Extensive experiments across diverse tasks (super-resolution, classification) and architectures (Transformer, Mamba, CNN) demonstrate LDT's superior generalization capability. Our code is available at ***.", "tldr": "We propose the Layer-Decomposition Training (LDT) strategy, which effectively mitigates feature distribution perturbations caused by misclassified unstable layers in existing methods through layer-wise separation of stable and unstable layers.", "keywords": ["Domain generalization"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3aeced69b36888391f34ae73e6494682883eb41.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Layer-Decomposition Training (LDT), a fine-grained, layer-wise method to improve domain generalization by explicitly separating “stable” and “unstable” layers based on gradient variance measured after a warm-up. Training uses two copies of the model with cross-freezing (stable layers train in the primary network; unstable layers train in the auxiliary network), while frozen counterparts are updated via EMA. A Dynamic Parameter Update (DPU) strategy adapts EMA coefficients per layer according to each layer’s relative variance rank. Experiments span super-resolution (DRealSR across cameras), classification (VLCS), and semantic segmentation (Cityscapes/BDD/Mapillary), and multiple backbones (CNN/Transformer/Mamba). Reported gains are strongest on SR, with modest training-time overhead and identical inference memory to baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear problem framing**: The paper targets a concrete weakness of coarse parameter partitioning (backbone vs. head) in prior work (LP-FT, DeFT) and motivates finer granularity using measured layer-wise gradient variance. The dual-branch cross-freezing setup is sensible and well-aligned with the goal of mitigating unstable-to-stable gradient interference.\n2. **Method simplicity & generality**: LDT and DPU are architecture-agnostic and slot into standard fine-tuning loops with minimal code complexity (once gradients are collected). Conceptually straightforward; likely portable to many vision backbones.\n3. **Empirical signal on SR**: On DRealSR, LDT and especially LDT+DPU improve PSNR across several target domains; the Canon branch shows a large jump ($\\approx$ +1.9 dB over baseline). The ablations comparing variance vs. mean vs. random partitioning support the central design choice."}, "weaknesses": {"value": "1. Metric inconsistencies: On SR, SSIM sometimes decreases even when PSNR increases (e.g., Pan and IMG branches in Table 1). The paper does not analyze why nor whether perceptual quality suffers. This matters for claims of “superior generalization capability.”\n\n2. Hyperparameter sensitivity: The ratio of unstable layers (Ratio_U) is central but lacks guidance: how is it chosen, how sensitive are results, and does it vary by architecture/task? The normalized-variance vs variance choice is argued for simplicity, yet Table 2 shows Var/Mean sometimes wins; the trade-off merits deeper discussion.\n\n3. Writing/Presentation issues：Several language glitches (e.g., “pm” instead of “±”), accidental filename text (“panasonic 132.png”), and inconsistent naming (“Layer-Decomposition” vs. “Layer-decoupled” in Appendix) distract from the contribution and could impede reproducibility.\n\n4. Novelty & Relation to Prior Work: Relative to LP-FT/DeFT, the novelty is finer-grained, data-driven layer partitioning and ranked EMA coefficients. This is incremental but non-trivial: the idea that some backbone layers can be less stable than head layers is interesting and empirically supported. Comparisons to other stability-oriented training schemes (e.g., sharpness- or noise-based regularizers) would situate the contribution better."}, "questions": {"value": "1. How exactly is gradient variance computed per layer? Is it over per-parameter gradients aggregated by mean/std? Over mini-batches or single examples? Any normalization by parameter scale?\n\n2. DPU coefficients. Did any layers receive W=1? If so, are those layers effectively frozen? Could you share the distribution of learned W across layers and runs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0uZR5H6PFl", "forum": "jLpjcY1iry", "replyto": "jLpjcY1iry", "signatures": ["ICLR.cc/2026/Conference/Submission15088/Reviewer_kBcr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15088/Reviewer_kBcr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888327109, "cdate": 1761888327109, "tmdate": 1762925413312, "mdate": 1762925413312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new training strategy, LDT (Layer-Decomposition Training), to improve domain generalization performance. It critiques existing methods for their coarse-grained partitioning of networks into 'stable' backbones and 'unstable' heads. LDT, in contrast, identifies stable/unstable layers at a fine-grained, individual layer level based on gradient variance. LDT uses a dual-branch (primary/auxiliary) network structure with a cross-freezing mechanism to isolate gradient interference from unstable layers, preventing it from disrupting the updates of stable layers. Furthermore, it introduces the DPU (Dynamic Parameter Update) strategy. Instead of using a fixed coefficient for parameter updates (EMA), DPU dynamically adjusts the update coefficient for each layer based on its variance level. This allows low-variance (stable) layers to learn more efficiently and high-variance (unstable) layers to be stabilized more effectively. The authors demonstrate that LDT achieves superior generalization performance compared to existing methods across diverse vision tasks (e.g., Super-Resolution, Classification, Semantic Segmentation) and various architectures (CNN, Transformer, Mamba)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Intuitive Problem Definition: The paper clearly points out the problem with existing methods (like DeFT) that simply separate the 'backbone' and 'head'. Fig 2(a) visually demonstrates that some backbone layers are even more unstable than the head, strongly justifying the core idea that a fine-grained, 'layer-wise' separation based on 'gradient variance' is necessary.\n2. Excellent Versatility (Task- & Architecture-Agnostic): The proposed methodology was successfully applied not only to low-level vision tasks (Super-Resolution) but also to high-level vision tasks (Classification, Segmentation). Furthermore, it demonstrated consistent performance improvements across diverse models, including CNN, Transformer, and the recent Mamba architecture, proving that LDT is a robust generalization 'strategy' not limited to specific conditions.\n3. Robust Experimental Validation: The ablation studies (Tables 1 and 2) are very well-designed, clearly isolating and verifying the effectiveness of LDT's core component (gradient variance-based partitioning) and the additional contribution of DPU (LDT+DPU > LDT > Baseline). This demonstrates that each proposed component makes a tangible and significant contribution."}, "weaknesses": {"value": "1. Increased Training Cost: LDT uses a dual-branch architecture, which significantly increases training memory (approx. 1.3x based on Table 3) and training time (approx. 1.8x) compared to the baseline. This could be a practical barrier for training large-scale models.\n2. Sensitivity to Key Hyperparameter: The method introduces a new key hyperparameter, the 'unstable layer ratio ($Ratio^U$)'. According to Table 9 in the appendix, performance is sensitive to this ratio (optimal around 0.4-0.5), requiring careful tuning when applying it to new tasks or datasets.\n3. Fixedness of Layer Selection: The distinction between stable and unstable layers is determined only once—immediately after the warm-up stage—and remains fixed throughout the entire training process. However, as training progresses, the stability of certain layers may change dynamically."}, "questions": {"value": "1. The DPU update coefficients (Eq. 10) are determined by a linear mapping based on 'rank' rather than the variance values themselves. This method (e.g., 0.99 + 0.01 * Rank) appears somewhat empirical. I am curious if you considered other functions that directly utilize the actual variance values (e.g. proportional to the normalized variance), and if so, why the current rank-based approach was found to be optimal.\n2. The increased training cost from the dual-branch setup could be prohibitive for larger models. Are there potential ways to reduce this cost while retaining the benefits of LDT, such as parameter sharing between the two networks or knowledge distillation?\n3. The stable/unstable status of the layers is fixed throughout the entire training process. If a dynamic LDT were applied, for instance, by re-measuring the gradient variance and updating the partitions periodically, could this lead to further performance improvements? Or, conversely, would it risk harming training stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uOrUhHTEQ8", "forum": "jLpjcY1iry", "replyto": "jLpjcY1iry", "signatures": ["ICLR.cc/2026/Conference/Submission15088/Reviewer_cWBr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15088/Reviewer_cWBr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981850052, "cdate": 1761981850052, "tmdate": 1762925412860, "mdate": 1762925412860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Layer Decomposition Training (LDT), a new domain generalization framework that improves network robustness by performing fine grained layer- wise separation of stable and unstable parameters. Existing works such as LP-FT and DeFT only distinguish between backbone and prediction head, which the authors argue is too coarse and leads to misclassification of unstable parameters.\n LDT first identifies unstable layers by measuring gradient variance across training samples and then adopts a dual branch cross freezing mechanism to isolate gradients between stable and unstable layers. To further enhance adaptability, a Dynamic Parameter Update (DPU) strategy is introduced, which adaptively adjusts EMA coefficients per layer based on gradient fluctuation magnitude.\n Extensive experiments are conducted across multiple domains including super resolution, classification, and semantic segmentation and across architectures such as CNNs, Transformers, and Mamba models. Results show consistent generalization improvements over state-of-the-art methods like DeFT and START."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea of layer wise decomposition of parameters guided by gradient variance is both conceptually novel and practically meaningful. Prior works (LP-FT, DeFT) only perform module level partitioning, whereas this work introduces a more granular and data driven approach. The Dynamic Parameter Update (DPU) is an innovative extension to EMA based stabilization, turning the update coefficient into a layer adaptive parameter, which enhances stability and learning efficiency. Theoretical motivation is well supported: the authors provide a gradient correlation analysis between stable and unstable layers and formal proofs (Appendix B) showing how unstable gradients propagate perturbations."}, "weaknesses": {"value": "Although the paper provides a theoretical derivation of gradient interference, the proof is limited to a simplified two module case (stable vs. unstable). Extending this to multi layer interactions or real architectures would make the analysis more convincing. The dual network training doubles the model instances during training, increasing memory from 15.27 GB to 20.25 GB (Table 3). Although inference cost is unaffected, a deeper analysis on scalability to larger networks (e.g., ViT-L or Swin-L) is missing."}, "questions": {"value": "DPU assigns update coefficients based on sorted variance rankings. Would a continuous mapping (e.g., normalization based scaling instead of discrete ranking) further improve stability? and could the authors explore a single network variant that integrates DPU like adaptation without duplicating the model? This might broaden applicability to resource constrained environments.\n\nIn today's era of large models, can this method be applied to large model architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KKVJtXRRVD", "forum": "jLpjcY1iry", "replyto": "jLpjcY1iry", "signatures": ["ICLR.cc/2026/Conference/Submission15088/Reviewer_e1yc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15088/Reviewer_e1yc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984920677, "cdate": 1761984920677, "tmdate": 1762925412447, "mdate": 1762925412447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Layer Decomposition Training (LDT) for domain generalization by detecting “unstable” layers identified via high gradient variance and separating their influence from “stable” layers. LDT trains two parallel copies of the network with cross-freezing and an EMA pathway to prevent interference, and introduces a Dynamic Parameter Update that adapts each frozen layer’s EMA coefficient according to its variance rank. The method is architecture and task agnostic, delivering gains on super-resolution, classification, and segmentation without any added inference overhead."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper methodology of using the Top-n layers with highest gradient variance as the unstable layer is easy to follow and easy to implement in practice."}, "weaknesses": {"value": "- The first concern is novelty. Dual-branch training already exists [1]; the difference here is a layer-wise variant that selects “unstable” layers via a top-n gradient-variance threshold, which feels ad-hoc.\n- The experimental evidence is limited to medium-scale vision benchmarks, with no large-scale tasks (e.g., ImageNet classification), making it hard to assess scalability and practical impact.\n- The method is presented as task-agnostic, yet there are no natural-language experiments (e.g., WikiText-103 language modeling), which would help demonstrate broader applicability.\n- Reported gains over DeFT [1] are modest (e.g., Table 4), suggesting limited improvement relative to prior dual-branch approaches.\n- Sections 2.2.3 and 2.4 are unnecessarily long relative to their content, which impairs readability. Greater emphasis should be placed on more baseline comparisons against other domain generalization methods.\n\nReferences:\n\n[1] Jaehyun Pahk, Donghyeon Kwon, Seong Joon Oh, and Suha Kwak. Decoupled finetuning for domain generalizable semantic segmentation. ICLR, 2025."}, "questions": {"value": "- Can you extend the evaluation to larger-scale and non-vision settings (e.g., ImageNet classification and an NLP benchmark such as WikiText-103), and outline any adaptations needed to apply LDT in these regimes?\n- Can you expand the comparisons with other domain generalization methods on datasets beyond DRealSR benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "etRPiyCJoW", "forum": "jLpjcY1iry", "replyto": "jLpjcY1iry", "signatures": ["ICLR.cc/2026/Conference/Submission15088/Reviewer_GLbA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15088/Reviewer_GLbA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987295897, "cdate": 1761987295897, "tmdate": 1762925411899, "mdate": 1762925411899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to mitigate the influence of unstable parameters on stable parameters for domain generalization. The authors proposed a fine-grained layer-wise partitioning strategy to improve parameter update stability, named Layer-Decomposition Training (LDT). LDT separates and cross-froze the stable and unstable layers during training, thereby mitigating the perturbations from unstable layers. Besides, the authors propose Dynamic Parameter Update (DPU) strategy to adjust the update coefficients of diverse layers according to their fluctuation amplitudes. Experiments on super-resolution, classification, and semantic segmentation demonstrate that LDT could enhance the generalization performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "[+] The paper is easy to follow. \\\n[+] The detail of the method and experiment is well described. \\\n[+] The related work is detailed."}, "weaknesses": {"value": "Major weakness:\n\n[-] The explanations for the experimental results are insufficient. This is particularly evident in Figure 2, Table 4, and Appendix Figure 5.\n\n[-] The authors claim that \"the same parameter update coefficient would inevitably result in information loss and counterintuitive behavior.\" Please clarify what is meant by \"information loss\" and provide evidence to verify this phenomenon.\n\n[-] The number of benchmarks used across different tasks is limited. Expanding the benchmark set would help better validate the effectiveness of the proposed approach.\n\n[-] Regarding Table 2:\n(1) The performance differences among the methods—particularly between Mean, Var/Mean, and Var—are not significant. Please explain the possible reasons for this.\n(2) Why does the Var/Mean method, which is supposed to better reflect parameter fluctuations without magnitude interference, underperform compared to Var?\n\n[-] In Appendix Table 7, the average performance of ResNet50 + DeFT (0.7978) is lower than that of ResNet18 + DeFT (0.7992). Please explain this counterintuitive result.\n\n[-] The authors selected layer partitioning ratios of 0.4 or 0.5 for unstable layers. Is this ratio also optimal for other tasks, such as classification and semantic segmentation?\n\nMinor weakness:\n\n[-] Typographical errors should be corrected to ensure clarity. For instance, \"0.8746 pm 4.79...\" in Table 1 should be revised to \"0.8746 ± 4.79...\"."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NzrYQboLX6", "forum": "jLpjcY1iry", "replyto": "jLpjcY1iry", "signatures": ["ICLR.cc/2026/Conference/Submission15088/Reviewer_xH6P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15088/Reviewer_xH6P"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762057965695, "cdate": 1762057965695, "tmdate": 1762925411564, "mdate": 1762925411564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}