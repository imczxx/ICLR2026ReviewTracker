{"id": "uxKK4uJgLw", "number": 13002, "cdate": 1758212627813, "mdate": 1763696063410, "content": {"title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving", "abstract": "Large language models (LLMs) demonstrate significant reasoning capabilities, particularly through long chain-of-thought (CoT) processes,\nwhich can be elicited by reinforcement learning (RL). \nHowever, prolonged CoT reasoning presents limitations, primarily verbose outputs due to excessive introspection. \nThe reasoning process in these LLMs often appears to follow a trial-and-error methodology rather than a systematic, logical deduction.\nIn contrast, tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling reasoning as an exploration within a tree structure. \nThis reasoning structure facilitates the parallel generation and evaluation of multiple reasoning branches, allowing for the active identification,\nassessment, and pruning of unproductive paths. \nThis process can potentially lead to improved performance and reduced token costs.\nBuilding upon the long CoT capability of LLMs, we introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a rule-based reward.\nToTRL is designed to guide LLMs in developing the parallel ToT strategy based on the sequential CoT strategy. \nFurthermore, we employ LLMs as players in a puzzle game during the ToTRL training process. \nSolving puzzle games inherently necessitates exploring interdependent choices and managing multiple constraints,\nwhich requires the construction and exploration of a thought tree, providing challenging tasks for cultivating the ToT reasoning capability. \nOur empirical evaluations demonstrate that our ToTQwen3-8B model, trained with our ToTRL,\nachieves significant improvement in performance and reasoning efficiency on complex reasoning tasks.", "tldr": "", "keywords": ["Large Language Models", "Tree of Thoughts", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ac841c5cd70d106ca8f1a5423a3c934b20f5452.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Tree-of-Thoughts Reinforcement Learning (ToTRL), an on-policy RL framework designed to move an LLM from linear chain-of-thought (CoT) reasoning to tree-of-thoughts (ToT) reasoning. The method comprises three main components: \n\n1. Policy optimization: The policy is trained using a clipped ratio objective, optionally augmented with a KL divergence term to a reference policy.\n\n2. Two-stage training: Stage 1 (“no-thinking mode”) uses a special prompt template to suppress the model’s usual CoT trace and induce explicit ToT steps within markup tags. Stage 2 (“thinking mode”) trains the model in standard generation mode to internalize the learned ToT behaviors for inference.\n\n3. Task-driven learning: The policy is trained via puzzle games that benefit from branching search, including 6×6 Sudoku and alphametic puzzles.\n\nThe resulting ToTQwen3‑8B model is evaluated on both in-distribution puzzles and several out-of-distribution logic tasks, using accuracy as the primary metric. Results show consistent improvements over several ~7–9B baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper addresses the well-known inefficiency and verbosity of long CoT reasoning by enabling branching exploration with a global perspective, aligning with prior work in ToT and graph-based reasoning.\n\nS2: The use of a rule-based validator combined with an exact-match reward is straightforward to reproduce for puzzle tasks and eliminates dependence on human-labeled rationales, reflecting trends in O1/R1-style RL frameworks.\n\nS3: Empirical results show that ToTQwen3‑8B achieves higher accuracy with fewer thinking tokens than the Qwen3‑8B baseline across multiple tasks, offering a notable practical advantage in efficiency and computational cost."}, "weaknesses": {"value": "W1: Equation (1) resembles a PPO-style clipped objective with an optional KL term to a reference model. Calling it REINFORCE may obscure the actual optimization method used.\n\nW2: The exact set-equality reward (Eq. 5) is brittle; success may hinge on precise formatting or extraction of answers, which could inflate performance or reduce reproducibility.\n\nW3: Comparisons omit relevant search-based alternatives, including: (i) self-consistency over CoT traces, (ii) explicit ToT BFS/MCTS as in the original ToT paper, (iii) RAP (planning with MCTS), and (iv) TS-LLM (AlphaZero-style value-guided search).\n\nW4: It is unclear whether all baselines were evaluated with identical token budgets, early-stop rules, and “thinking mode” support. Baselines lacking special thinking channels may be disadvantaged, making cross-model comparisons potentially unfair.\n\nW5: The training tasks are restricted to puzzles with exact-rule validators. Real-world reasoning tasks (coding, planning, open-ended writing, tool use) often require partial credit, multi-step execution, debugging, environment interaction, or human judgment. Training only on puzzles may not stress the full range of reasoning, error exploration, and backtracking required in practical scenarios."}, "questions": {"value": "Q1: Is Eq. (1) implemented as PPO with ε‑clipping? If so, why call it REINFORCE? \n\nQ2: What is the rollout size n per prompt, sampling temperature/top‑p, and maximum tokens per “thinking” segment? How is the stop instruction inserted when the thinking budget is reached, and how is a partial tree summarized before answering? \n\nQ3 Did you try partial‑credit rewards (e.g., counting correct solutions, Sudoku constraint satisfaction), or step‑level validators? \n\nQ4: Did every baseline receive the same token budget and stop rule? Several baselines lack special “thinking modes”, how did you ensure fairness?  \n\nQ5: How does the method perform on open-ended reasoning tasks as described in W5, beyond puzzles?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x4Ak66VQId", "forum": "uxKK4uJgLw", "replyto": "uxKK4uJgLw", "signatures": ["ICLR.cc/2026/Conference/Submission13002/Reviewer_6YF1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13002/Reviewer_6YF1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630512818, "cdate": 1761630512818, "tmdate": 1762923749111, "mdate": 1762923749111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The article introduce tree-of-thoughts RL (ToTRL) framework to guide LLMs to develop parallel ToT capabilities beyond sequential CoT. After ToTRL training process, the LLMs can solve puzzle games better, including in-domain and out-of-domain ones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Introducing parallel thinking patterns into reasoning LLMs sounds a reasonable effort. \n\nThe ToTQwen3-8B model shows significant performance gains on a variety of logic puzzles."}, "weaknesses": {"value": "Since the authors are still leveraging the CoT prompt for mathematical problems, it is unclear to me why it improves OOD mathematical tasks. Can you provide analysis as to why it also helps mathematical tasks?\n\nI am particularly curious why ToTQwen3-8B can “explore the solution space more effectively and efficiently” as the authors mentioned, given that ToT is often very costly. Some experiment setting details of Figure 3 in section 3.5 are unclear. How do you set the budgets as exactly (2^c) k tokens? Do you set a budget for each method and truncate the thinking length?"}, "questions": {"value": "“Initially, as illustrated in Figure 1, the LLM undergoes training to perform ToT reasoning in a non-thinking mode. The non-reasoning mode is achieved by introducing blanks between reasoning tags, which compels the model to suspend its conventional reasoning processes.” This is not explained clearly, even after referring to Figure 1. \n\nWhat does the separation line in Table 3 mean? Are there fundamental differences between the above 2 models and the middle 3 models?\n\nThe authors mention “Collectively, these efforts demonstrate the significant potential of internalizing ToT capabilities within the LLM itself, moving towards more autonomous reasoning.” I find the paper *Autonomous Tree-search Ability of Large Language Models* proposed the notion of autonomous ToT reasoning ability two years ago, and I believe there were relevant efforts in the literature. The authors can also consider including a discussion of the literature in this direction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GFiGSBh3oY", "forum": "uxKK4uJgLw", "replyto": "uxKK4uJgLw", "signatures": ["ICLR.cc/2026/Conference/Submission13002/Reviewer_dHrF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13002/Reviewer_dHrF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928084933, "cdate": 1761928084933, "tmdate": 1762923748699, "mdate": 1762923748699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed to apply RL to Tree-of-thoughts (ToT) with two-stage training, named ToTRL. This approach aims to narrow the gap between linear COT and parallel TOT generations, hence at the first stage the reasoning trace from the original model is turned off, then at the second stage both COT and TOT thoughts are turned on with RL for training. The paper tunes the model on two puzzle tasks for adaptation to the improved TOT reasoning patterns."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important problem of how to design an effective training procedure of improving parallel thinking techniques like TOT."}, "weaknesses": {"value": "- The motivation to adapt CoT to ToT reasoning is not well justified. It remains unclear in what sense is the linear COT unsuitable under the TOT setting, and whether the gain from 2-stage training is just due to extended-training.\n- It doesn't seem to be convincing that by applying RL on only two puzzle tasks, the model performance can be improved over a wide range of reasoning tasks. The claim of the title that training on puzzle tasks can unlock the potential of ToT is very broad and needs deeper justification.\n- The experimental result analysis did not reveal whether ToTRL truly improved tree search quality (diversity and depth)."}, "questions": {"value": "1. In the 2nd stage of ToTRL (\"thinking mode\"):\n- How do thoughts between <think> and </think> differ from <tot> and </tot>? It seems what <tot> captures is just a summary of <think>, rather than novel thoughts that could further improve tree search quality.\n- If thoughts between <think> and </think> come from the 1st stage (\"no-thinking mode\"), then since the base model did not go through ToT training yet, the thought quality is expected to be bad? This is exactly the problem the paper wants to address, not sure how effective it is to use these thoughts directly for 2nd stage training.\n\n2. For fair comparisoin, it seems the baseline models in experiments should also adopt ToT style reasoning, rather than using on their native reasoning paths.\n\n3. Was the improvement from ToTRL just an artifact of prolonged training, or it truly improved the quality of search trees? There isn't analysis on the difference between tree quality before and after ToTRL."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8orCUOONcK", "forum": "uxKK4uJgLw", "replyto": "uxKK4uJgLw", "signatures": ["ICLR.cc/2026/Conference/Submission13002/Reviewer_NCNz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13002/Reviewer_NCNz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931670666, "cdate": 1761931670666, "tmdate": 1762923748256, "mdate": 1762923748256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper adds reinforcement learning to the tree-of-thoughts (which is a generalization of CoT) to support better LLM reasoning for solving games/puzzles/reasoning tasks. CoT is linear (ie sequential) and ToT can support branching exploration of multiple pathways. The authors' contribution is to add an on-policy RL on top of a rule-based reward system to help the LLM transition from sequential CoT to parallel, tree-structured reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is extremely well written and details are fleshed out to support reproducibility\n- The experimental results show significant improvements based on a Qwen model that the authors have trained/fine-tuned.\n- The authors also demonstrate their approach in a test-time-scaling experiment and show that the learned policy is good to explore the search space better."}, "weaknesses": {"value": "- In the beginning of the paper, the authors mention that \"Initially, the LLM is trained to perform ToT reasoning in a non-thinking mode, leveraging more moldable thinking patterns to activate ToT reasoning. Once the LLM has developed a degree of ToT reasoning ability in the non-reasoning mode, it undergoes\nfurther training in the reasoning mode.\" This wasn't re-referred back later in the paper. Can you show/demonstrate examples of these patterns that activate ToT? Can you show ablation results showing the necessity of this initial reasoning in non-thinking mode? Is it because the CoTs are not \"faithful\"?"}, "questions": {"value": "- Please think of an additional experiment to address my question above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JAMRZe5Yp6", "forum": "uxKK4uJgLw", "replyto": "uxKK4uJgLw", "signatures": ["ICLR.cc/2026/Conference/Submission13002/Reviewer_R5RJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13002/Reviewer_R5RJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952424408, "cdate": 1761952424408, "tmdate": 1762923747805, "mdate": 1762923747805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}