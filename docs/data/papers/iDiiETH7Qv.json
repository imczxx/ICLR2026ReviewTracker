{"id": "iDiiETH7Qv", "number": 5143, "cdate": 1757855437608, "mdate": 1759897992123, "content": {"title": "Sheaves Reloaded: A Direction Awakening", "abstract": "Sheaf Neural Networks (SNNs) are a powerful algebraic-topology generalization of Graph Neural Networks (GNNs), and have been shown to significantly improve our ability to model complex relational data. While the GNN literature proved that incorporating directionality can substantially boost performance in many real-world applications, no SNNs approaches are known with such a capability. To address this limitation, we introduce the Directed Cellular Sheaf, a generalized cellular sheaf designed to explicitly account for edge orientations. Building on it, we define a corresponding sheaf Laplacian, the Directed Sheaf Laplacian $L^{\\widetilde{\\mathcal{F}}}$, which exploits the sheaf's structure to capture both the graph’s topology and its directions. $L^{\\widetilde{\\mathcal{F}}}$ serves as the backbone of the Directed Sheaf Neural Network (DSNN), the first SNN model to embed a directional bias into its architecture. Extensive experiments on twelve real-world benchmarks show that DSNN consistently outperforms many baseline methods.", "tldr": "", "keywords": ["directed sheaf neural network", "directed graphs", "directed cellular sheaves"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d091f859bb0fe4c36d0f740f35a31b2b02b382ef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper extends Sheaf Neural Networks (SNNs) to directed graphs by introducing the Directed Cellular Sheaf and the corresponding Directed Sheaf Laplacian (DSL), which explicitly encode edge orientation through complex-valued, direction-aware restriction maps. Building on this framework, the authors propose the Directed Sheaf Neural Network (DSNN), enabling principled learning on directed and heterophilic graphs. Experiments on synthetic and real-world datasets demonstrate that DSNN effectively captures directional dependencies and outperforms existing SNN and GNN models where edge directionality is crucial."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a mathematically principled extension of SNNs to directed graphs through the Directed Cellular Sheaf and Directed Sheaf Laplacian.\n\n2. It effectively captures asymmetric and directional relationships while maintaining robustness to heterophily.\n\n3. The experiments demonstrate consistent performance gains on both synthetic and real-world directed graph datasets."}, "weaknesses": {"value": "1.  Most compared models are from 2020–2022, with only one from 2024. The evaluation lacks more recent direction-aware or topology-based GNNs, which weakens the empirical evidence for DSNN’s claimed advantages.\n\n2. The paper does not clearly justify why Sheaf Neural Networks are the right framework for addressing heterophily or extending to directed graphs. Given the recent shift toward graph foundation models and unified architectures, it remains unclear whether adapting SNNs is the most effective or timely direction, rather than developing more generalizable approaches."}, "questions": {"value": "1. The introduction should better articulate why extending SNNs remains valuable in 2025. Specifically, it should discuss the advantages of combining SNNs with GNNs for handling directed graphs and heterophily, and explain why these structural refinements are still meaningful in the era of more unified and general graph learning paradigms.\n\n2. The comparison set is mostly limited to models from 2020–2022, with only one 2024 method included. More recent GNNs addressing heterophily and oversmoothing (from 2024–2025) should be added to strengthen the empirical evaluation and demonstrate DSNN’s relevance against state-of-the-art models.\n\n3. The paper does not include baselines from graph prompting, graph foundation, or pre-trained graph models, which have recently become dominant in node classification and general graph learning. Including such comparisons would position the work more clearly within the modern landscape of graph representation learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OicNXhNKXa", "forum": "iDiiETH7Qv", "replyto": "iDiiETH7Qv", "signatures": ["ICLR.cc/2026/Conference/Submission5143/Reviewer_uTjh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5143/Reviewer_uTjh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761470976165, "cdate": 1761470976165, "tmdate": 1762917907118, "mdate": 1762917907118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Directed Cellular Sheaf, which incorporates edge orientation into cellular sheafs. Based on the novel introduction, the paper proposes the Directed Sheaf Laplacian that serves as the backbone of the Directed Sheaf Neural Network. Experimental results validate the efficacy of the proposed GNN, with theoretical analysis on the properties of the Directed Cellular Sheaf."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The topics of Sheaf Neural Networks and directed graphs are significant.\n- The proposed Directed Cellular Sheaf admits satisfactory properties.\n- The proposed Directed Sheaf Neural Network seems to work well in experiments.\n- Complexity analysis is provided together with implementation code."}, "weaknesses": {"value": "- It is not clear what $\\tilde{F}^0$ means from Definition 1 without a clear reference or prior definition.\n- The description of DSBM is not clear and without a reference in the main text. Should refer to [1].\n- Some citation styles are not correct. For example, \\citep should be used for the reference at the end of line 291.\n- Some more works can be considered for comparison, e.g., [2] and [3].\n-  The concept can benefit from some motivating examples of the Laplacian. Also, there is a figure sheaf.png in the anonymous github link that may help with illustration.\n\nReference:\n\n[1] He, Y., Reinert, G., & Cucuringu, M. (2022, December). Digrac: Digraph clustering based on flow imbalance. In Learning on Graphs Conference (pp. 21-1). PMLR.\n[2] Badea, T. A., & Dumitrescu, B. (2025). Haar-Laplacian for directed graphs. IEEE Transactions on Signal and Information Processing over Networks.\n[3] Lin, L., & Gao, J. (2023, June). A magnetic framelet-based convolutional neural network for directed graphs. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-5). IEEE."}, "questions": {"value": "What does $\\tilde{F}^0$ mean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YmUmK5KLgn", "forum": "iDiiETH7Qv", "replyto": "iDiiETH7Qv", "signatures": ["ICLR.cc/2026/Conference/Submission5143/Reviewer_YHDx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5143/Reviewer_YHDx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631591998, "cdate": 1761631591998, "tmdate": 1762917906761, "mdate": 1762917906761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Directed Cellular Sheaves and a corresponding Directed Sheaf Laplacian, enabling Sheaf Neural Networks to incorporate edge directionality, a missing capability in current SNNs. The authors prove key spectral properties and show that this formulation recovers classical sheaf Laplacians and magnetic Laplacians as special cases. They further propose DSNN, demonstrating consistent gains on both real-world graphs and synthetic directional SBM settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**1. Principled directional sheaf formulation**\n\nThe paper introduces directed cellular sheaves and a corresponding directed sheaf Laplacian, providing the first rigorous sheaf-theoretic framework for directed graphs and addressing a clear limitation of existing SNNs.\n\n**2. Solid theoretical foundation**\n\nThe authors prove Hermiticity, PSD spectrum bounds, and show that the proposed operator recovers classical sheaf Laplacians and magnetic Laplacians as special cases, demonstrating a sound and unifying mathematical design.\n\n**3. Comprehensive experimental results**\n\nAcross both real-world and synthetic benchmarks, the model outperforms existing SNNs and competitive direction-aware GNNs, with especially strong results in heterophilic and direction-dominated settings, validating the benefits of directional sheaf modeling."}, "weaknesses": {"value": "**1. Limited intuition for the directional mechanism**\n\nWhile the mathematical construction is provided, the paper provides limited high-level insight into **how and why** the complex restriction maps enhance directional information flow in practice. The introduction of the complex phase feels algebraically motivated rather than guided by an intuitive model of directional propagation.\n\n**2. Scope of experimental evaluation**\n\nThe evaluation focuses primarily on small-to-medium-scale datasets. There is no demonstration on larger real-world directed benchmarks (e.g., OGB-ArXiv, and arxiv-year). \n\n**3. Ablations could be deeper**\n\n3.1. The effect of stalk dimension $d$\n\n3.2. Sensitivity to direction sparsity or unreliable edge orientation (i.e., direction noise)\n\n3.3. Effect of learning vs. fixing the phase $q$\n\n**4. Writing clarity**\n\nThe definition and construction of the directed cellular sheaf are mathematically sound but presented in a dense, notation-heavy manner. Adding intuitive explanations, intermediate steps, and conceptual guidance (e.g., how complex phases encode directional flow at a high level) would make the framework more accessible and easier to follow for a broader audience beyond sheaf specialists."}, "questions": {"value": "1. Is $q$ learned or tuned per dataset? If tuned, how stable is performance across $q$ values?\n\n2. Does DSNN maintain benefits in settings where directional edges are sparse or only weakly informative?\n\n3. How does performance degrade if a portion of edge directions are flipped or randomized?\n\n4. Why the random split is adopted for the node classification instead of the widely-used public splits? Performance on small homophilic and heterophilic benchmarks can vary noticeably with random seeds, so it would be useful to justify this choice and clarify whether public splits are also tested."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fOcvN1yF8V", "forum": "iDiiETH7Qv", "replyto": "iDiiETH7Qv", "signatures": ["ICLR.cc/2026/Conference/Submission5143/Reviewer_47m4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5143/Reviewer_47m4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839119005, "cdate": 1761839119005, "tmdate": 1762917906435, "mdate": 1762917906435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Directed Cellular Sheaf and corresponding Directed Sheaf Laplacian (DSL) and  Directed Sheaf Neural Network (DSNN), extending Sheaf Neural Networks (SNNs) to directed graphs through complex-valued restriction maps. The authors prove that the DSL is Hermitian, positive-semidefinite, and upper-bounded by 2, and that it generalizes both the classical Sheaf Laplacian and the Magnetic/Sign-Magnetic Laplacians. Empirically, DSNN consistently outperforms GNN and SNN baselines on node-classification and direction-prediction tasks across 12 real-world and several synthetic datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Novelty and rigor: The formulation is original and mathematically principled. The theoretical results are sound and clearly proven.\n* Unifying framework: DSNN subsumes NSD, MagNet, and SigMaNet as special cases.\n* Empirical performance: DSNN variants achieve top performance on 10/12 node-classification and 8/10 direction-prediction benchmarks, with large margins on heterophilic and directed graphs.\n* Writing quality: The paper is nice to read."}, "weaknesses": {"value": "* Empirical scope: Experiments focus on small-to-medium graphs; scalability to large or temporal directed graphs remains untested.\n* Ablations: Only sensitivity to q is analyzed. Additional studies on stalk dimension d or learned restriction-map architectures would strengthen the empirical section.\n* Baselines: Comparison omits some recent direction-aware or heterophilic GNNs (e.g., DiGCL, DPGNN).\n* Minor clarity issues: Dense notation and a few typos (“SNNs approaches”).\n* Limited intuition: The geometric meaning of complex restriction maps and the global parameter q could be discussed more intuitively; why complex numbers versus real skew-symmetric forms?"}, "questions": {"value": "1. Could q be made learnable per edge or per graph?\n2. Have you examined the spectral properties or phase distributions of learned complex restriction maps?\n3. Could you comment on DSNN’s scalability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RgXYlHh0Cs", "forum": "iDiiETH7Qv", "replyto": "iDiiETH7Qv", "signatures": ["ICLR.cc/2026/Conference/Submission5143/Reviewer_Xiwg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5143/Reviewer_Xiwg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992502010, "cdate": 1761992502010, "tmdate": 1762917906201, "mdate": 1762917906201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}