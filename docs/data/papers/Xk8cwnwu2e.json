{"id": "Xk8cwnwu2e", "number": 23820, "cdate": 1758348852095, "mdate": 1759896795343, "content": {"title": "Efficient Message-Passing Transformer for Error Correcting Codes", "abstract": "Error correcting codes (ECCs) are a fundamental technique for ensuring reliable communication over noisy channels. Recent advances in deep learning have enabled transformer-based decoders to achieve state-of-the-art performance on short codes; however, their computational complexity remains significantly higher than that of classical decoders due to the attention mechanism. To address this challenge, we propose EfficientMPT, an efficient message-passing transformer that significantly reduces computational complexity while preserving decoding performance. A key feature of EfficientMPT is the Efficient Error Correcting (EEC) attention mechanism, which replaces expensive matrix multiplications with lightweight vector-based element-wise operations. Unlike standard attention, EEC attention relies only on query-key interaction using global query vector, efficiently encode global contextual information for ECC decoding. Furthermore, EfficientMPT can serve as a foundation model, capable of decoding various code classes and long codes by fine-tuning. In particular, EfficientMPT achieves 85% and 91% of significant memory reduction and 47% and 57% of FLOPs reduction compared to ECCT for $(648,540)$ and $(1056,880)$ standard LDPC code, respectively.", "tldr": "We propose a novel efficient message-passing decoder for error correcting codes based on the proposed efficient error correcting attention module.", "keywords": ["Channel coding", "Error correcting codes", "Transformer-based decoder", "Message-passing decoder", "Neural decoder", "Transformer", "Efficient attention module"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/273c18b3836fa84ea824e5f1f6324b8e9ac4e9e3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an efficient variant of a transformer-based ECC decoder. The main difference from the previous methods is an efficient attention module, which is based on query and key components without a value component."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper conducted extensive experiments and shows improvement in memory size and computational complexity compared to previous works."}, "weaknesses": {"value": "The contribution of the present paper, in comparison with the recent works on Transformer-based ECC decoding by Choukroun and Park, seems incremental, as it largely reproduces the same elegant algorithm with  a minor technical adjustment.\n\nThe paper is not clearly written. It is focused on the many low-level technical details and doesn't explain the intuition and motivation \nof the proposed method."}, "questions": {"value": "Can the proposed attention module architecture be useful to transformer applications other than ECC?\n\nIn line 181 it is said that standard methods have complexity o(n^2).  What is the complexity of the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1fXgFb7rBI", "forum": "Xk8cwnwu2e", "replyto": "Xk8cwnwu2e", "signatures": ["ICLR.cc/2026/Conference/Submission23820/Reviewer_GA3B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23820/Reviewer_GA3B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760599253817, "cdate": 1760599253817, "tmdate": 1762942820671, "mdate": 1762942820671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an improved transformer-based decoder for error correction codes. The key novelty is the replacement of matrix multiplications in the attention mechanism with a vector-wise operations using a global query vector. EfficientMPT maintains decoding performance comparable to CrossMPT, while significantly reducing the parameter-count, FLOPs, and GPU usage. This is especially evident at large blocklengths - the efficiency improvements allow training at these lengths, without super-large memory usage."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. EEC attention is well-motivated, and is a natural extension of the ideas in CrossMPT: the global query vector effectively condenses magnitude vector into one vector, rather than relying on a query matrix. The resulting updates resembles message-passing decoding between syndrome info and magnitude info, with simple updates using information from the other modality.\n\n2. The empirical results are very strong. While maintaining the same performance as CrossMPT, efficientMPT achieves significant reductions in memory and computational complexity with a simple architectural change.\n\n3. The main practical contribution is that it allows scaling transformer-based decoding to long blocklengths, which was infeasible via previous methods. FLOPs scale linearly with n, unlike other methods (quadratic dependence?)\n\n4. EfficientMPT can act as a foundation model - unseen codes can be handled via lightweight finetuning.\n\n5. I like the experiment in Figure 7, supports the understanding that a very string inductive-bias based on the PCM has been imposed on the attention mechanism.\n\n6. The evaluation is thorough - compares with good decoders for canonical codes (SCL for polar rather than BP, etc)"}, "weaknesses": {"value": "No major weaknesses.\nWhile transformer-based decoders (including this paper) are still sub-optimal to classical codes/decoders used in practice, this paper improves efficiency of ECCT - which is a major step in the right direction."}, "questions": {"value": "1. How does training from scratch compare to finetuning the foundation model? Specifically, can you add FEfficient-MPT results for LDPC(1824,1520) in Figure 8a? (and/or train-from-scratch in 6c)\n\n2. The BER performance for 5G-LDPC codes is missing. Can you please add this for completeness. I'd expect a similar performance as CrossMPT? What is the gap in performance/complexity to the decoders currently employed in 5GNR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2J4NXrR8U5", "forum": "Xk8cwnwu2e", "replyto": "Xk8cwnwu2e", "signatures": ["ICLR.cc/2026/Conference/Submission23820/Reviewer_Cs5s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23820/Reviewer_Cs5s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761413122700, "cdate": 1761413122700, "tmdate": 1762942820396, "mdate": 1762942820396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a transformer based decoder for error correcting codes. The proposed decoder use a lightweight attention module that replace the costly attention mechanism, in the Masked self attention module and masked cross-attention module.\nThe proposed EEC decoder run over the magnitude and syndrome separately with the proposed simpler attention module. \n\nThey get very good results that reduce the number of parameters, memory usage, and FLOPs, without compromising the decoding performance achieved by CrossMPT."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed decoder is very efficient with respect to previous methods in terms of memory usage and FLOPs\n\n2. The proposed EfficientMPT decoder get competitive results compare to previous methods such as ECCT and CrossMPT\n\n3. The EfficientMPT algorithm can also serve as foundation model, a single model can be generalizes to unseen codes with fine-tuning, and get better results than BP on long LDPC codes.\n\n4. The proposed lightweight attention in the decoder using a global query vector and embedding the parity-check matrix, yields a simpler decoder."}, "weaknesses": {"value": "1. The proposed algorithm evaluate only on simple AWGN channel, and not on non-Gaussian channels such as Rayleigh channel\n\n2. The proposed foundation needs fine-tuning in order to operate well on larger codes.\n\n3. For Polar codes, the classic SCL decoder still get better results than the proposed EfficientMPT decoder,"}, "questions": {"value": "1. Can you check the results of the proposed decoder on non-AWGN channel?\n\n2. Can you suggest ways to reduce the gap to the SCL results? maybe some changes in the architectural or at loss level?\n\n3. Please run the simulation on all codes at it appears in the paper of [Choukroun & Wolf (2022)], for example in Table 1 you only test two BCH code, instead of 4 codes and larger ones"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cgoVfirFi5", "forum": "Xk8cwnwu2e", "replyto": "Xk8cwnwu2e", "signatures": ["ICLR.cc/2026/Conference/Submission23820/Reviewer_2sfM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23820/Reviewer_2sfM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743017456, "cdate": 1761743017456, "tmdate": 1762942820132, "mdate": 1762942820132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces EfficientMPT, a transformer-based decoder for error-correcting codes that targets computational and memory bottlenecks in existing transformer-based decoders. The method introduces an efficient attention mechanism, replacing standard matrix multiplications with vector-based element-wise operations and integrating the parity-check matrix (PCM) directly into the attention module. EfficientMPT is designed to be position- and length-invariant, enabling it to serve as a foundation model for ECC decoding. Experimental results show significant reductions in GPU memory, FLOPs, and parameter count, while maintaining bit error rate (BER) performance comparable to existing transformer-based decoders."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Effective handling of longer codes, overcoming a key constraint of earlier transformer-based ECC decoders.\n- Position and length invariance, supporting its use as a foundation model.\n- Thorough assessment across multiple code types."}, "weaknesses": {"value": "- Limited interpretability analysis; it remains unclear how decoding decisions are made compared to traditional belief propagation.\n- Missing comparisons with neural Tanner graph-based decoders.\n- The analysis of polar codes would be strengthened by a more prominent and direct comparison with SCL decoders, particularly systematic SCL decoders when evaluating BER.\nFurther Suggestions\n- Please verify the dimensions of magnitude and syndrome embedding matrices after multiplication by H and Hᵀ in Figure 2(c) (green multi-head patches).\n- The sentence “The lifting process generates LDPC codes of size (52×Z,10×Z)” appears to describe the PCM dimensions rather than the code parameters (n, k). Please clarify."}, "questions": {"value": "- The sentence “The magnitude embedding is then added to the resized syndrome embedding, which is resized from (n−k)×d to n×d by multiplying the PCM H” is unclear upon first reading—specifically, whether H is in binary or BPSK form. While later sections resolve this, earlier clarification would be helpful.\n- Is any sparsity-inducing regularization applied to the trainable PCM in Figure 7(b)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xlx4AtNZWk", "forum": "Xk8cwnwu2e", "replyto": "Xk8cwnwu2e", "signatures": ["ICLR.cc/2026/Conference/Submission23820/Reviewer_v8pc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23820/Reviewer_v8pc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998263532, "cdate": 1761998263532, "tmdate": 1762942819781, "mdate": 1762942819781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}