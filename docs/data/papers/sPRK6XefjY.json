{"id": "sPRK6XefjY", "number": 24739, "cdate": 1758359847200, "mdate": 1759896751571, "content": {"title": "On the Lipschitz Continuity of Set Aggregation Functions and Neural Networks for Sets", "abstract": "The Lipschitz constant of a neural network is connected to several important properties of the network such as its robustness and generalization. It is thus useful in many settings to estimate the Lipschitz constant of a model. Prior work has focused mainly on estimating the Lipschitz constant of multi-layer perceptrons and convolutional neural networks. Here we focus on data modeled as sets or multisets of vectors and on neural networks that can handle such data. These models typically apply some permutation invariant aggregation function, such as the sum, mean or max operator, to the input multisets to produce a single vector for each input sample. In this paper, we investigate whether these aggregation functions are Lipschitz continuous with respect to three distance functions for unordered multisets, and we compute their Lipschitz constants. In the general case, we find that each aggregation function is Lipschitz continuous with respect to only one of the three distance functions. Then, we build on these results to derive upper bounds on the Lipschitz constant of neural networks that can process multisets of vectors, while we also study their stability to perturbations and generalization under distribution shifts. To empirically verify our theoretical analysis, we conduct a series of experiments on datasets from different domains.", "tldr": "", "keywords": ["set aggregation functions", "Lipschitz continuity", "stability"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/74505badd4d9e18ffe82f06b1f109662d46544e7.pdf", "supplementary_material": "/attachment/599ff37925ff21b6ae7603af8ac673b913de6408.zip"}, "replies": [{"content": {"summary": {"value": "This paper discusses the Lipschitz properties of aggregation functions for multisets. The main results is that each of the three common aggregations: (a) mean (b) sum and (c) max are Lipschitz with respect to only one of the three multiset metrics \n\nmean--> EMD\n\nmax--> Hausdorff\n\nsum--> matching\n\nThe explicit constants are also computed. In addition, results for attention based aggregations are provided, implications for perturbations and distribution shifts are discussed and empirical evidence for the theorems is provided."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written, and from what I checked the theoretical results seem correct. The main result should be of some impact to the 'permutation invariant learning' community. Perhaps, the take home message is this: while from the lens of pure injectivity, the well known xu et al. paper argued that sum>mean>max, from the lens of Lipschitzness, each has its own distinct advantage."}, "weaknesses": {"value": "While the paper does conduct some nice experiments, there is no clear bottom line to the paper from a practical point of view. E.g., one could hope this paper would substantiate a claim that for some type of tasks, sum pooling is better, and for others, max pooling is more accurate, or more robust."}, "questions": {"value": "Honestly, these are not questions but suggestions for the next version of the paper (hopefully, the camera ready version). Feel free to ignore some or all of these points for the rebuttal. But please do take this into account when editing the next version of the paper. \n\n**Bigger comments*\n* EMD and the Hausdorff distance are not a metric on multisets of $\\leq M$ elements because for X={a,a}, Y={a}  we have EMD(X,Y)=0=Hausdorff(X,Y). This should be mentioned when these metrics are introduced. \n*  Related to this: some of your results follow from the arguments in xu et al., which are nicely summarized in figures 2-3 and in the schematic statement sum>mean>max. Mamely, it is established that mean cannot separate certain multisets (e.g., X and Y above) if they are equal as distributions, and that max cannot separate multisets which mean can, namely multisets which are different as distributions but are the same if multiplicities are ignored. It would be good to explain this, and this would also emphasize that the other direction, namely that sum is not Lipschitz with respect to the other metrics, is not known and is a truly new result.\n* It would be good to add references to support your choice of distances on multisets. Namely, to show these choices are natural and not something you invented. For example, the matching metric, perhaps the least known of the three, is used in the Chuang paper you cited in a different context, and in [1]. \n* It seems to me that an easy corollary of Theorem 3.1 is that none of these three metrics are bi-Lipschitz equivalence. Thus, no aggregation can be simultaneously Lipschtiz with respect to all of them\n* In Lemma 3.2, why don't you discuss the Hausdorff metric?\n* Bullets 1-2 of Lemma 3.2 follow immediately from proposition 3.2, I would comment on that.\n* I am missing some discussion of Theorem 3.3. Namely, what you get for mean and max is more or less the extension from theorem 3.1 which you might expect. For sum pooling you don't get this, which initially seems like a surprise. Why is this? After looking in the proof my understanding is that this is because the inner function can map non-zero elements to zero (in your proof, near zero). This too should be explained. This also seems a bit superficial to me: I would suggest to define the matching metric so that if X has $n_1$ elements and Y has $n_2$ elements, then the term $|n_2-n_1|$ would be added for the unmatched elements. Wouldn't this solve the issue?\n* What do we learn from Proposition 3.6? Could you add some discussion?\n* Section 3.4: I understand that Lipschitz results automatically lead to theoretical results on generalizaiton, still, I couldn't understand this section I think it should be rewritten. \n* Line 459-461: I think when you write \"insensitive\" you actually mean \"sensitive\". In my understanding insesnsitive=robust. \n \n\n\n**Minor comments**\n* In line 51, the sentence \"input data might correspond...\" is strange. I would say, sometimes, data is permutation invariant and should be treated as a multiset. \n* Line 119, for the sake of formal correctness, you might want to mention that sum pooling in injective when coupled with a  function applied to  each point. \n* Proposition 2.3 seems obvious to me, if you agree I would add some sort of remark about that\n* Theorem 3.3 starts with \"Let X,Y\" but X and Y do not appear elsewhere in the theorem. \n* Line 404: \"Therefore, the results of both Theorem 3.1 and Lemma 3.2 hold\". I would rephrase, the experiment corraborates the theorem, but it does not prove it is true. What proves the theoremsare true, is, well, the proofs of the theorems. \n* Line 418-420: You discuss Lipschitz constants of the affine layer, what about the activation?\n* A.3.1: In the proof, say that this if for the F for which the EMD is realized. \n*Line 771: \"Then we have\" doesn't make sense, should be \"where we choose\" or something like that\n\n[1] On the Holder Stability Of Multiset and Graph Neural Network, Davidson and Dym, ICLR 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "O4bnpYoVc5", "forum": "sPRK6XefjY", "replyto": "sPRK6XefjY", "signatures": ["ICLR.cc/2026/Conference/Submission24739/Reviewer_AMSr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24739/Reviewer_AMSr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760949448938, "cdate": 1760949448938, "tmdate": 1762943181894, "mdate": 1762943181894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically analyzes the Lipschitz continuity of three representative aggregation functions for multiset inputs (SUM, MEAN, MAX) with respect to three order-invariant multiset distances (EMD, Hausdorff, Matching), and provides exact constants. Based on these results, the authors derive a Lipschitz upper bound for set neural networks composed of “MLP → Aggregation → MLP”, and further discuss the applicability of existing W₁-based generalization bounds under distribution shift. Numerical experiments are conducted on ModelNet40 and Polarity, visualizing the practical validity of the derived bounds through scatter plots and correlation coefficients."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Presents a clear overview of which combinations of {SUM/MEAN/MAX} × {EMD, Hausdorff, Matching} are Lipschitz continuous, and additionally provides strengthened results for fixed cardinalities as lemmas.\n- Proposes a simple yet useful composition rule that allows direct derivation of Lipschitz upper bounds for set neural networks.\n- Demonstrates the effectiveness of the analysis across both image processing and natural language domains."}, "weaknesses": {"value": "- The main results focus on correlation plots, but lack comparisons of task performance (accuracy) and ablation studies (e.g., classification accuracy differences among SUM/MEAN/MAX).\n- It is unclear what practical benefits this work brings to neural networks for set functions.\n- The Matching distance is a metric only when “no zero vector is included” (Proposition 2.2), which may be inconsistent with real-world preprocessing (e.g., padding)."}, "questions": {"value": "- When Proposition 2.2’s assumption is violated in practice (e.g., zero vectors introduced by padding), how can the proposed theory be applied or extended?\n- In what kinds of practical situations would it be appropriate to use Mean, Max, or Sum, respectively? For instance, depending on conditions such as (a) a few outliers vs. widespread small noise, (b) fixed vs. variable set cardinality, or (c) small-element sets (e.g., fashion) vs. large-element sets (e.g., point clouds, documents).\n- This study explores suitable distance functions for each aggregation type (Sum, Mean, Max) under different scenarios. As a potential future direction, could this approach be further generalized by incorporating the Hölder mean (see arXiv:2403.17410)?\n\n(Other Comments)\n- Overall: It would be preferable to end mathematical expressions with a comma or period.\n- Line 41: might loose → might lose\n- Line 114: The table should include a title.\n- Line 147: Hausdorff distance → Hausdorff Distance\n- Line 356: Haudorff → Hausdorff\n- Figures 1–2: The axes are too small to read. It would also help to indicate within the figure which columns correspond to sum, mean, and max."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xzAHJ58NwK", "forum": "sPRK6XefjY", "replyto": "sPRK6XefjY", "signatures": ["ICLR.cc/2026/Conference/Submission24739/Reviewer_y3oZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24739/Reviewer_y3oZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761692385260, "cdate": 1761692385260, "tmdate": 1762943181688, "mdate": 1762943181688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "It has been demonstrated that some of the key properties of neural network models (such as their generalisation or robustness to corruption) can be controlled by (a bound on) their Lipschitz constant. This article addresses the question of whether specific neural networks, designed to process collections of vectors (sets or multisets) whose order is irrelevant to the task at hand, are Lipschitz continuous with respect to a given distance. The authors examine three known possible distances between unordered multisets, the Haussdorf distance, the Earth mover distance (EMD) and the matching distance, as well as three order-independent aggregation functions at the heart of common neural architectures for multisets, namely the sum, mean and max functions. The authors then thoroughly investigate all combinations of distances and aggregation functions, determine the possible Lipschitz continuity and, if continuous, the limits of the Lipschitz constants of these aggregation functions and the neural network constructed from them. Finally, the article analyses the theoretical limits of two proof-of-concept applications, namely the classification of a point cloud representing shapes (the ModelNet40 dataset) and the classification of film reviews from a set of unordered words (the Polarity dataset)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "-   The authors study the key properties of neural networks for unordered multisets, a context that appears to have been little studied to date.\n-   They provide a comprehensive study of the Lipschitz continuity of these networks and their aggregation functions with respect to three known distances (EMD, Haussdorf, matching distance), and provide new bounds on the Lipschitz constant when available. I'm not expert in this topic but the results seem novel.\n-   Where available, they show theoretically and numerically how the bounds of the Lipschitz constant can characterise the robustness of these neural networks in the face of certain data corruptions (addition of elements, perturbation of elements).\n-   Based on a result from (Shen et al., 2018), they also explain how these limits can be used to characterise the generalisation error of such a network in the event of a change in distribution.\n-   They provide a convincing numerical analysis of aggregation functions and the neural networks that rely on them. This analysis is performed on two datasets, and a comparison between the theoretical Lipschitz upper bounds and the numerical upper bounds is provided."}, "weaknesses": {"value": "Overall, the article is interesting and easy to read despite its technical nature and the diversity of all the aggregation function<->distance associations considered. I have listed the following errors that the authors should correct. True weaknesses are labeled as such in the list below.\n\n-   (weakness) In Sec. 3.1, while the numerical treatment is clear and legit, I didn't get why the authors need 3 trained different neural networks to simply test the Lipschitz continuity of the aggregation function. This part should be better motivated. Is it a way to artificially generate a lot of multiset examples? Or to generate vector multiset of a fixed dimension? A priori, I don't see any connection between the way these vectors are generated and the Lipschitz continuity: these vectors could be generated by any means, the Lipschitz continuity of the aggregation function that is fed with them is independent, except if the continuity is restricted to a family of specific vector but this is nowhere considered in this work.\n-   (weakness) L1102: \"f1 and f2 are fully-connected layers\" This is part is a bit obscure. First the f1 is applied to vectors in the definition of vX and then in this line, it is defined by its application to scalars. If it is applied componentwise to vectors it must be explained. But then, the affine transformation is a bit special and seems to involve a diagonal matrix diag(a1, ..., an). This part needs improvements.\n-   (weakness/error) Proof of Prop 3.6: Section A.8, L1433-1424: The proposed matrix F does not respect the constraints in L1439 and L1442. The only matrix I see that could respect it is $F_{ij} = 1/(n\\*(n+1))$ whatever the value of i and j; but maybe I'm wrong.\n-   The 3 claims of Thm 3.1 do not use explicitly the multisets X and Y introduce at the beginning of the theorem statement; Same remark for Thm 3.3\n-   Page 4, sentence \"As discussed above, the SUM function is theoretically more powerful than the rest of the functions.\": the word \"powerful\" is a bit vague in this sentence. What was introduced earlier is rather the possibility for the aggregation to be \"injective\"; this is not directly related to a concept of powerfulness or efficiency.\n-   Page 5, line 239: \"g\" does not strictly operate on a multiset with the double-brace notation, it is rather that g is invariant under any permutation of its input and I guess the notation g({{..}}) rather means that invariance. Such a meaning could be inserted in Sec. 2.1 \"Notation\"\n-   I guess this is a bit too implicit but it should be (re)said the Lipschitz constant over $f_{MLP1}$ and $f_{MLP2}$ are associated to the standard L2-L2 continuity of these functions\n-   Page 6: while interesting, the analysis of the attention mechanism (and the negative result regarding its continuity) would deserve to be announced with a few words in the intro. As it, it looks like an extra element that comes out of the blue.\n-   L297-298: \"Incorporating l2 attention into the definition of $f_{ATT}(X), unfortunately, does not make it Lipschitz.\" -> Is it a fact or something that has been proved by the authors? Explain\n-   Proposition 3.6: there is an error in the proof of point (1), see below\n-   L356-357: \"Haudorff\"\n-   L458-459: how the value 0.2 has been determined in the width of the uniform noise? That is a magical value.\n-   L459-462: Regarding the sensitivity of NNmax vs NNmean in the different considered scenarios, could this sensitivity be backed by the previous theoretical analysis?\n-   L617: \"denote the permutation\" -> \"denote one of permutation\"; there could be several minimizing permutations with the same score.\n-   L770: \"Then, we have\" -> The fact that v1 = u1 ...; is not a consequence, but a choice to find a counterexample; adapt the sentence accordingly, here and in the other 5 other similar proofs later.\n-   L835: define $d_{ij}$ (I guess it's just $\\|v_i - u_j\\|$)\n-   L835: the inequality is in the wrong direction; the sum with the entries $F_{ij}$ is bigger than the norm of the difference of means; same error later in another proof, L926\n-   L1135: You should remove this line or place a minimum over all possible matrix F here since the next bound using F\\* is then bigger if F\\* is the matrix minimizing the EMD rather than this minimization\n-   L1458-1460: The statement and equation \"We thus obtain dH(W,X') = h(X',X) = ...\" is wrong, but it seems anyway unnecessary for the proof of point (2).\n\nMinor/cosmetic:\n-   if not already the case, everywhere avoid denoting norms with the double bar, use rather \"backslash bars\" in latex"}, "questions": {"value": "-   Can you provide a correction to the proof of Prop. 3.6?\n-   Can you better explain the need of neural networks to generate multisets in Sec. 3.1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aQY2kJisRO", "forum": "sPRK6XefjY", "replyto": "sPRK6XefjY", "signatures": ["ICLR.cc/2026/Conference/Submission24739/Reviewer_gLAF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24739/Reviewer_gLAF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757336948, "cdate": 1761757336948, "tmdate": 1762943181413, "mdate": 1762943181413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyses the Lipschitz continuity of the set aggregation functions MEAN, SUM, and MAX. The EMD, Hausdorff distance, and matching distance are analysed, and Lipschitz bounds are derived for the cases of multisets having the same and different cardinalities. These results are then extended to the Lipschitz continuity of a neural network based on MLPs and an aggregation function. The authors show that a modified attention mechanism is not Lipschitz-bounded and use their results to derive bounds on stability under perturbations and distribution shifts. Numerical experiments are provided for two tasks and linked to the theory."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors provide a significant contribution to the analysis of Lipschitz continuity of aggregation functions. The paper is well structured, and the results build on each other, from the fundamental results in Table 1 to the bounds on input perturbations. A small set of benchmarks nicely accompanies the theoretical results."}, "weaknesses": {"value": "The paper would benefit from explaining related literature and the connections to it better. Based on the theoretical and experimental results, the conclusions for practitioners should be better spelled out (details see below)."}, "questions": {"value": "- Most theorems exclude sets that contain the zero vector. Explain why, whether it could be mitigated, and its effect on practice. \n- Please better explain the usage of the analysed network structures for state of the art (SOTA) or practice in the context of transformers or GNNs (not just one sentence in line 63 and two citations from 2017).\n- Please relate your work more to the cited Fourier Sliced-Wasserstein embedding (line 70) and the importance of bi-Lipschitz continuity and injectivity of aggregation functions.\n- Please describe the theoretical advantages of the sum aggregator better (line 119).\n- Why were those three distance metrics chosen?\n- The reader would benefit from a summary of when each metric is relevant and which aggregation function should be chosen in practice. Please relate it to the initial motivation of robustness and include experimental results, such as loose bounds.\n- Please relate the attention aggregation to the mean or the max.\n- Please unify notation, e.g., activation functions in line 234 and line 284."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HpqUntQhui", "forum": "sPRK6XefjY", "replyto": "sPRK6XefjY", "signatures": ["ICLR.cc/2026/Conference/Submission24739/Reviewer_ZjJJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24739/Reviewer_ZjJJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819253448, "cdate": 1761819253448, "tmdate": 1762943181133, "mdate": 1762943181133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}