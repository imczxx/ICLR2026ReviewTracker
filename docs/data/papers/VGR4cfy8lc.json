{"id": "VGR4cfy8lc", "number": 9614, "cdate": 1758130383425, "mdate": 1759897708982, "content": {"title": "Rethink Mini-batch Gradient: Cascade Momentum", "abstract": "During foundation model training, mini-batch stochastic gradient descent alleviates memory constraints; however, the resulting increase in gradient variance induces sharp oscillations in the loss curve, slowing convergence. Conventional momentum algorithms overlook the limitation introduced by mini-batch training; their ideal assumption is that momentum propagates smoothly over time. Yet, in practice, momentum is almost restricted to gradients within a single epoch, so cross-epoch information is severely diminished and cannot continuously suppress oscillations. For the first time, we theoretically analyze the momentum degradation problem under mini-batch gradients. To address this, we propose \\textbf{Cascaded Momentum}, which splits momentum into an \\textbf{Inner momentum} that rapidly smooths mini-batch gradients within each epoch and an \\textbf{Outer momentum} that accumulates historical gradient trends across epochs to provide inertial guidance to subsequent epochs. This two-level mechanism simultaneously attenuates noise and accelerates convergence with virtually no additional cost.", "tldr": "", "keywords": ["Optimization", "Momentum", "Mini-batch"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/793118ba8f1a7ff2c96650a990823fd8d887f565.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies momentum “forgetting” across epochs under mini-batch SGDM: the momentum is dominated by within-epoch gradients and does not preserve longer-horizon trends, causing loss oscillations and slower convergence. The authors propose Cascaded Momentum (CM): use an inner momentum to smooth per–mini-batch noise within an epoch and an outer momentum to carry gradient trends across epochs. Updates linearly combine the two; at epoch end, the outer momentum is refreshed from the inner one. The method is designed to add negligible compute/memory overhead. The theory presents convergence results for SGDM with mini-batches and for CM with mini-batches under explicit step-size and hyperparameter conditions (notably a coupling between \\\\(\\\\kappa\\\\) and \\\\(\\\\lambda\\\\)). Experiments on ResNet-18 over CIFAR-100 and Tiny-ImageNet (200 epochs; batch sizes 32/64/128/256) show smoother loss and higher accuracy for small batches, while acknowledging CM is not superior at larger batches (128/256)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The author's motivation is great. The problem of unstable training caused by mini-batch does exist and needs to be solved.\nA relatively systematic derivation of convergence is presented"}, "weaknesses": {"value": "**1.Lack of genuine novelty**\n\nThe proposed Cascaded Momentum (CM) is essentially a nested combination of an outer exponential moving average (EMA) across epochs and an inner EMA within mini-batches.  This design strongly resembles the Lookahead Optimizer (Zhang et al., 2019) when the Lookahead step size equals one epoch, and also overlaps with well-known EMA-based update schemes used for smoothing gradients and reducing computational overhead.  However, the authors do not cite or discuss these prior methods, giving the misleading impression that the two-level EMA or cascaded update mechanism is an original contribution.  As a result, the methodological innovation appears marginal.\n\n**2.Insufficient experimental validation**\n\nThe experiments are limited to two small-scale datasets (CIFAR-100 and Tiny ImageNet) with a single backbone (ResNet-18).  This setting is far from the paper’s claimed motivation—addressing mini-batch degradation in large-scale foundation model training.  The experiments therefore fail to demonstrate the claimed scalability or practical relevance of CM under realistic conditions.\n\n**3.Contradictory empirical trends**\n\nIn the reported results, Adam performs substantially worse than SGD, which contradicts well-established observations in standard training regimes, where Adam or AdamW typically outperform SGD, especially on vision benchmarks.  This raises concerns about experimental rigor, hyperparameter tuning, or reproducibility.\n\n**4.Lack of convincing evidence of real-world benefit**\n\nGiven the weak experimental setup, small model scale, and absence of large-batch or high-noise scenarios, the proposed CM method’s practical advantages remain unsubstantiated.  The work reads more like a minor reparameterization of existing momentum schemes rather than a meaningful rethinking of mini-batch optimization."}, "questions": {"value": "- What are the essential differences between the proposed Cascaded Momentum and existing methods such as Lookahead or EMA-based momentum?\n\n- Theoretical analysis seems similar to standard momentum proofs.  Which part of it is actually new?\n\n- Why are experiments limited to small datasets and a single small model, while the motivation targets large-scale training?\n\n- Adam performs much worse than SGD in your results—was Adam properly tuned?\n\n- Is CM effective on larger models or realistic large-batch training setups?\n\n- How was κ chosen, and how sensitive are results to this parameter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WnahRQ21dv", "forum": "VGR4cfy8lc", "replyto": "VGR4cfy8lc", "signatures": ["ICLR.cc/2026/Conference/Submission9614/Reviewer_Yj4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9614/Reviewer_Yj4m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760926031571, "cdate": 1760926031571, "tmdate": 1762921153744, "mdate": 1762921153744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Cascade Momentum, which introduces an additional momentum term across epochs to enhance the stability and convergence. Specifically, the method decomposes the conventional momentum into two parts: an inner momentum that operates within each epoch and an outer momentum that preserves gradient information across epochs. This design aims to mitigate the loss of gradient direction consistency that occurs when mini-batches are shuffled, improving optimization stability, especially under small batch sizes. The authors provide theoretical analysis showing the convergence properties of the proposed method and conduct experiments on CIFAR-100 and Tiny ImageNet, demonstrating that Cascade Momentum achieves better performance compared to standard momentum-based optimizers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a simple but effective two-level momentum design that retains gradient information across epochs to address the loss of optimization continuity in mini-batch training.\nExperiments show the proposed method achieves improved stability and convergence compared to standard momentum-based optimizers especially when the batch size is small.\nThe paper is clearly structured, with well-defined notations and an easy-to-follow presentation."}, "weaknesses": {"value": "Some assumptions, such as the claimed loss of momentum across epochs, are not theoretically or empirically proved or explained enough.\n\nThe experiments are limited and do not fully support some of the conclusions, with weak evidence of generalization beyond small datasets and architectures.\n\nIt remains uncertain how the outer momentum should be tuned or whether the method maintains its advantage under large-batch or diverse training settings."}, "questions": {"value": "1. The paper assumes that adjacent mini-batches are “highly correlated,” but this depends heavily on the dataset and data-loading strategy. With proper shuffling and large datasets, such correlation is often weak. Can the authors provide empirical evidence or measurements supporting this assumption?\n2. Carrying momentum across epoch boundaries is not inherently problematic. Isn’t this how standard SGDM operates successfully in practice? Can the authors clarify why they view this as a drawback rather than a desirable continuity property?\n3. In Figure 2, the authors attribute the loss spikes to the decay of gradient information in the momentum buffer when the batch size changes. Could this phenomenon instead result from learning-rate or batch-size mismatch, or from changes in optimization direction caused by batch-size change?\n4. The paper does not include experiments varying the outer momentum coefficient κ (Algorithm 2, line 11), which is critical for understanding the role and sensitivity of the proposed mechanism. Can the authors provide ablations or sensitivity analyses showing how performance changes with different κ values?\n5. The paper states that accuracy becomes “less competitive” under large batch sizes, but specific numbers are not reported. Does this mean the method performs similarly, slightly worse, or fails to converge? Are there explanations for this degradation, and should the method be considered only for small-batch cases?\n6. Can the authors include additional experiments on more diverse tasks, such as reinforcement learning or language model training? The current experiments are limited to small-scale image classification and seem insufficient to demonstrate the general effectiveness of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qj0k48bBd9", "forum": "VGR4cfy8lc", "replyto": "VGR4cfy8lc", "signatures": ["ICLR.cc/2026/Conference/Submission9614/Reviewer_gKxR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9614/Reviewer_gKxR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718216883, "cdate": 1761718216883, "tmdate": 1762921153390, "mdate": 1762921153390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the concept of a \"momentum degradation problem\" in the mini-batch momentum mechanism. The authors argue that the high frequency of intra-epoch mini-batch updates saturates the momentum buffer, effectively erasing long-term, cross-epoch gradient information. To address this, they propose Cascaded Momentum (CM), a simple and low-cost dual-buffer system. An \"Inner momentum\" smooths high-frequency noise within an epoch and is then reset. An \"Outer momentum\" accumulates the final state of the inner momentum at the end of each epoch, preserving a stable, long-term gradient trend that is used to guide the next epoch's updates. The authors provide a theoretical convergence analysis for both standard SGDM and their proposed SGD-CM in the non-convex, mini-batch setting. Empirical results on image classification tasks using ResNet-18 show the outperformance of SGD-CM against conventional optimizers in the mini-batch setting."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper raises an interesting question about the potential downsides of high-frequency mini-batch updates on the long-term memory of momentum.\n\n2. The proposed SGD-CM algorithm is a simple and intuitive improvement that directly addresses the identified problem by explicitly separating momentum into two timescales. A rigorous convergence analysis for SGDM and SGD-CM under the mini-batch setting is also provided."}, "weaknesses": {"value": "1. The paper's core argument is not clearly clarified. The logical chain connecting the theory of \"momentum degradation\" to its claimed empirical consequences is difficult to follow. The writing would benefit from a clearer, more direct explanation of why the loss of long-term information is an inherent problem and how the specific phenomena observed (like loss spikes) are a direct result of this problem, as opposed to other known factors. Moreover, while the existence of the theoretical analysis is a strength, its conclusions are difficult to understand in an intuitive way.\n\n2. The paper's primary empirical evidence for its core problem, presented in Figure 2, is unconvincing. The author claims that the observed loss spikes upon changing the batch size are caused by the \"momentum degradation\" making the optimizer's buffer stale. However, much simpler and more established explanations exist. For example, decreasing the batch size increases gradient variance and has the similar effect as increasing LR [1,2]. With a fixed learning rate, this change makes the LR effectively too large for the new noise regime, which can cause the optimizer's instability and loss spikes. The paper fails to provide critical comparison experiments to disentangle these effects. Without this, the author's claim that momentum degradation is the primary cause of the spike is unsubstantiated. The central claim, while interesting, lacks sufficient evidence.\n\n3. The main experimental results in Figures 3 and 4 show that the proposed SGD-CM offers only a minor performance improvement over existing, standard optimizers (like SGDM) on two datasets (CIFAR-100 and Tiny ImageNet). The empirical gains are not so compelling. To be more convincing, the paper would need to demonstrate more significant advantages or show that these advantages hold across a much wider variety of tasks and model architectures.\n\n\n**Reference**\n\n[1] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. arXiv:1706.02677, 2017.\n\n[2] Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, Quoc V. Le. Don't Decay the Learning Rate, Increase the Batch Size. International Conference on Learning Representations (ICLR), 2018."}, "questions": {"value": "The method introduces a new key hyperparameter $\\kappa$ for the outer momentum. The paper provides very little analysis of this. How sensitive is the optimizer's performance to the choice of $\\kappa$? I think an ablation study is needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z5C0JaE0LK", "forum": "VGR4cfy8lc", "replyto": "VGR4cfy8lc", "signatures": ["ICLR.cc/2026/Conference/Submission9614/Reviewer_qe2S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9614/Reviewer_qe2S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724162816, "cdate": 1761724162816, "tmdate": 1762921153123, "mdate": 1762921153123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Cascaded Momentum (CM) to solve the \"cross-epoch momentum degradation\" problem in mini-batch training, where high-frequency gradient updates \"drown out\" long-term trends.\n\nCM decouples momentum into:\n1.  Inner momentum: Smooths high-frequency noise *within* an epoch.\n2.  Outer momentum: Accumulates and propagates long-term trends *across* epochs.\n\nThis dual-level mechanism, with virtually no additional overhead, significantly enhances training stability, convergence speed, and performance, especially in small-batch settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper study a new interesting problem. \n2.Better performence in small batchsize.\n3.With virtually no additional overhead."}, "weaknesses": {"value": "1.Limited efficacy in large-batch, hindering its applicability to modern LLM training.\n2.Increased hyperparameter tuning complexity due to the introduction of the outer momentum coefficient and its potential co-dependencies."}, "questions": {"value": "Do you believe this \"inner/outer\" decoupling concept could be generalized to adaptive optimizers like AdamW？\nlarge batch size is the most important part, it would be great if you could prove that this method works for it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5Ku1NEoe5d", "forum": "VGR4cfy8lc", "replyto": "VGR4cfy8lc", "signatures": ["ICLR.cc/2026/Conference/Submission9614/Reviewer_p2uT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9614/Reviewer_p2uT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836752490, "cdate": 1761836752490, "tmdate": 1762921152515, "mdate": 1762921152515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the limitation that momentum in standard optimizers is confined to gradients within a single epoch, causing cross-epoch information to decay and fail to suppress oscillations effectively. To overcome this, they propose Cascaded Momentum (CM), which introduces two components: an Inner Momentum to quickly smooth mini-batch gradients within each epoch, and an Outer Momentum to accumulate gradient trends across epochs, providing long-term inertial guidance for optimization"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper combines experimental validation with theoretical analysis, offering a comprehensive examination of the proposed method."}, "weaknesses": {"value": "Overall, the paper is not very convincing.\n\n1. Although the authors argue that cross-epoch information loss is a key problem, the paper lacks strong evidence showing that preserving such information substantially improves optimization.\n2. From a theoretical perspective, it remains unclear why SGD-CM would outperform standard SGDM. The claimed robustness to batch size variation seems marginal and insufficiently justified.\n3. The experiments are limited to small datasets and models, casting doubt on whether the method can scale to large-scale deep networks with hundreds of billions of parameters, as claimed (line 27-28).\n4. The paper does not report the additional memory cost introduced by the new optimizer."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0n01gol4c9", "forum": "VGR4cfy8lc", "replyto": "VGR4cfy8lc", "signatures": ["ICLR.cc/2026/Conference/Submission9614/Reviewer_YbYx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9614/Reviewer_YbYx"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876536474, "cdate": 1761876536474, "tmdate": 1762921152258, "mdate": 1762921152258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}