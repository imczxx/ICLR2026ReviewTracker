{"id": "zgs0L72R4c", "number": 17841, "cdate": 1758281112495, "mdate": 1759897150845, "content": {"title": "CLoD-GS: Continuous Level-of-Detail via 3D Gaussian Splatting", "abstract": "Level of Detail (LoD) is a fundamental technique in real-time computer graphics for managing the rendering costs of complex scenes while preserving visual fidelity. Traditionally, LoD is implemented using discrete levels (DLoD), where multiple, distinct versions of a model are swapped out at different distances. This long-standing paradigm, however, suffers from two major drawbacks: it requires significant storage for multiple model copies and causes jarring visual \"popping\" artifacts during transitions, degrading the user experience. We argue that the explicit, primitive-based nature of the emerging 3D Gaussian Splatting (3DGS) technique enables a more ideal paradigm: Continuous LoD (CLoD). A CLoD approach facilitates smooth, seamless quality scaling within a single, unified model, thereby circumventing the core problems of DLOD. To this end, we introduce CLoD-GS, a framework that integrates a continuous LoD mechanism directly into a 3DGS representation. Our method introduces a learnable, distance-dependent decay parameter for each Gaussian primitive, which dynamically adjusts its opacity based on viewpoint proximity. This allows for the progressive and smooth filtering of less significant primitives, effectively creating a continuous spectrum of detail within one model. To train this model to be robust across all distances, we introduce a virtual distance scaling mechanism and a novel coarse-to-fine training strategy with rendered point count regularization. Our approach not only eliminates the storage overhead and visual artifacts of discrete methods but also reduces the primitive count and memory footprint of the final model. Extensive experiments demonstrate that CLoD-GS achieves smooth, quality-scalable rendering from a single model, delivering high-fidelity results across a wide range of performance targets.", "tldr": "", "keywords": ["Level of Detail", "3D Gaussian Splatting", "Neural Scene Representation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/347405f9a455698b4a1e059828fa52805b54476f.pdf", "supplementary_material": "/attachment/dac3c325a5d5793191bf911b4c510bec1dab6f08.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents CLOD-GS, a continuous level-of-detail framework for 3D Gaussian Splatting that enables smooth, pop-free quality scaling within a single model. Each Gaussian learns a distance-dependent opacity decay, allowing fine-to-coarse detail control without discrete model swaps. A coarse-to-fine training strategy with virtual distance scaling and sparsity regularization ensures robustness across viewing scales. CLOD-GS reduces Gaussian count and memory while maintaining or improving rendering quality on benchmarks like BungeeNeRF and Tanks & Temples."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The LoD topic is meaningful.\n2. The paper is well written and clearly organized."}, "weaknesses": {"value": "**Major**\n\n1. The coarse-to-fine training strategy is similar to that used in Octree-GS.\n2. No demo has been submitted; therefore, the performance of the proposed method cannot be effectively demonstrated.\n3. This is an LoD paper, which should be related to large-scale reconstruction for the topic to be meaningful. However, the paper only conducts experiments on small-scale scenes, and the reported metrics are similar to those of the baselines. I believe these results could easily be surpassed by tuning the hyperparameters. Also, I thinks the authors should compare with hierarchical-3dgs and more lod baselines.\n\n**Minor**\n\n1. The paper lacks references to key related work on Level of Detail (LoD) rendering. The authors should consider citing the following works:\n    - *Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian*\n    - *A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets*\n    - *Horizon-GS: Unified 3D Gaussian Splatting for Large-Scale Aerial-to-Ground Scenes*\n    - *Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System for Real-Time Rendering of Composed Scenes*"}, "questions": {"value": "please see the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uT51gnuC0R", "forum": "zgs0L72R4c", "replyto": "zgs0L72R4c", "signatures": ["ICLR.cc/2026/Conference/Submission17841/Reviewer_yMt5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17841/Reviewer_yMt5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760956983552, "cdate": 1760956983552, "tmdate": 1762927674939, "mdate": 1762927674939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CLoD-GS, a continuous level-of-detail (CLoD) mechanism embedded directly into the 3D Gaussian Splatting (3DGS) representation. Instead of maintaining multiple discrete LoD models (which incurs storage overhead and produces visual \"popping\" during switching), this work introduces a learnable distance-dependent decay factor for each Gaussian, allowing opacities to vary smoothly with viewpoint distance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Continuous control without model duplication, avoids DLoD storage and popping artifacts.\n\n2. Minimal parameter overhead (~1.6% per Gaussian), practical for deployment.\n\n3. Unified training strategy that naturally encourages Gaussian sparsity at distant views."}, "weaknesses": {"value": "1. The method assumes that viewing distance is the dominant factor in determining perceptual relevance. However, perceptual saliency often depends on texture frequency, geometric edges, semantic relevance, and shading sensitivity, not merely spatial distance.\n\n2. The paper does not provide sensitivity analyses showing whether these parameters generalize across scene types.\n\n3. Since the decay factor is learned independently for each Gaussian, the simplification behavior lacks global structural organization."}, "questions": {"value": "1. Could semantic or frequency-based saliency be introduced alongside distance? Would this improve LoD behavior in highly textured scenes?\n\n2. How stable is the learned decay factor across camera trajectory distributions? Does it overfit camera placement biases?\n\n3. Could this be extended to dynamic scenes where Gaussian parameters evolve over time?\n\n4. The opacity threshold scaling seems hand-tuned. Have you explored adaptive thresholds learned per scene?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XaJftD0cNH", "forum": "zgs0L72R4c", "replyto": "zgs0L72R4c", "signatures": ["ICLR.cc/2026/Conference/Submission17841/Reviewer_XUcB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17841/Reviewer_XUcB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653857703, "cdate": 1761653857703, "tmdate": 1762927674355, "mdate": 1762927674355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CLoD-GS, which augments each 3D Gaussian with a learnable distance-decay parameter to modulate opacity continuously as viewpoint distance changes. This creates a single representation that supports smooth quality–performance trade-offs at test time. A complementary training strategies and regularizers are incorporated for effective learning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed distance-adaptive opacity gives a smooth transition within a single representation, avoiding multi-copy asset storage and popping artifacts of DLoD.\n2. The virtual distance scale offers a continuous speed-quality curve, which is practical for real-time budegets.\n3. This paper is well-written and easy to follow."}, "weaknesses": {"value": "1. Prior efforts have designed sophisticated LoD for 3DGS [A], which are very effective even in large-scale scenes; a direct, side-by-side comparison with [A] is missing and would strengthen the empirical case and clarify trade-offs.\n2. The effectiveness of distance-driven opacity in regions with high-frequency textures or potential aliasing—central issues in LoD—is not convincingly demonstrated.\n3. Because opacity modulation retains the finest Gaussians and culls softly, rasterization workload may remain high. A report of FPS and visible-splat counts may be included to demonstrate the efficiency of the proposed method.\n4. Statements such as \"treating preliminary 3DGS as the key innovation\" and \"consistently achieves state-of-the-art rendering quality\" appear stronger than the presented evidence supports.\n\n[A] A hierarchical 3d gaussian representation for real-time rendering of very large datasets. [SIGGRAPH 24]"}, "questions": {"value": "1. Does the distance-driven opacity exhibit hole artifacts under aggressive thresholds or grazing views? Some visual or quantitative results could demonstrate the generalizability of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y2ZrLtvpTa", "forum": "zgs0L72R4c", "replyto": "zgs0L72R4c", "signatures": ["ICLR.cc/2026/Conference/Submission17841/Reviewer_Uu5C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17841/Reviewer_Uu5C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900130204, "cdate": 1761900130204, "tmdate": 1762927673831, "mdate": 1762927673831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a simple yet effective way to make 3D GS models smoothly adjust their rendering quality without switching between discrete versions. Traditional discrete lod methods require multiple stored models, leading to high memory use and visible popping when changing between levels. The paper overcomes this by introducing a learnable distance-based opacity control that allows each Gaussian to fade smoothly as the camera moves farther away. Combined with a coarse-to-fine training process that encourages the model to use fewer points at larger viewing distances, this approach produces a single unified model capable of continuous, artifact-free LoD scaling. Experiments on several public datasets show that CLoD-GS achieves comparable or better visual quality than existing methods, while using about 30–40% fewer Gaussians and memory. The method adds minimal computational cost, integrates easily into existing pipelines, and provides high-quality, scalable rendering results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a simple, learnable distance-based opacity mechanism that enables continuous LoD in 3D Gaussian Splatting without adding complex structures or large overhead.\n- Experiments show consistent improvement in visual quality and memory efficiency across multiple datasets and methods.\n- The paper is well written, provides detailed implementation settings, ablation studies, and public datasets, making the work easy to understand and verify."}, "weaknesses": {"value": "- The paper lacks deeper analysis of why the proposed opacity function and training design work optimally or how parameters affect convergence and quality.\n- Experiments focus only on static scenes. How about generalization to dynamic or large-scale environments remains unexplored?\n- Only briefly mentions similar approaches without in-depth experimental or conceptual comparison."}, "questions": {"value": "Same as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i6fDryptdr", "forum": "zgs0L72R4c", "replyto": "zgs0L72R4c", "signatures": ["ICLR.cc/2026/Conference/Submission17841/Reviewer_cpAX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17841/Reviewer_cpAX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907606850, "cdate": 1761907606850, "tmdate": 1762927673414, "mdate": 1762927673414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}