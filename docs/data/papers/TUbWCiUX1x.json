{"id": "TUbWCiUX1x", "number": 14309, "cdate": 1758232615090, "mdate": 1759897377678, "content": {"title": "Scale-Wise VAR is Secretly Discrete Diffusion", "abstract": "Autoregressive (AR) transformers have emerged as a powerful paradigm for visual generation, largely due to their scalability, computational efficiency and unified architecture with language and vision. Among them, next scale prediction Visual Autoregressive Generation (VAR) has recently demonstrated remarkable performance, even surpassing diffusion-based models. In this work, we revisit VAR and uncover a theoretical insight: when equipped with a Markovian attention mask, VAR is mathematically equivalent to a discrete diffusion. We term this reinterpretation as Scalable Visual Refinement with Discrete Diffusion (SRDD), establishing a principled bridge between AR transformers and diffusion models. Leveraging this new perspective, we show how one can directly import the advantages of diffusion—such as iterative refinement and and reduce architectural inefficiencies into VAR, yielding faster convergence, lower inference cost, and improved zero-shot reconstruction. Across multiple datasets, we show that the diffusion-based perspective of VAR leads to consistent gains in efficiency and generation. To facilitate further research, we will make the code and models public.", "tldr": "", "keywords": ["Visual Autoregressive Generation", "Discrete Diffusion"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b6c223e9be316c87b189ae9342833d655923077.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an interesting and important theoretical connection, linking the Visual Autoregressive (VAR) model with discrete diffusion processes. The core argument is that when VAR is conditioned only on the previous scale in a Markovian fashion (which the authors term SDD), it becomes mathematically equivalent to a structured discrete diffusion model with deterministic transitions. Building on this theoretical perspective, the authors import standard techniques from diffusion models, such as Classifier-Free Guidance and token resampling, to construct the final SRDD model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Novel Theoretical Bridge:** The paper's primary contribution is establishing a theoretical bridge between scale-wise autoregressive models (VAR) and discrete diffusion models. The explicit claim that \"VAR, when equipped with a Markovian attention mask, is mathematically equivalent to a discrete diffusion process\" is a novel perspective that offers a new way to understand VAR's mechanics.\n\n2.  **Effective Use of CFG:** The paper compellingly demonstrates through experiments that its Markovian variant (SDD/SRDD) can utilize Classifier-Free Guidance (CFG) more stably and effectively. Unlike the original VAR, which sees performance collapse under strong guidance, the proposed model exhibits a stable saturation behavior similar to diffusion models, validating the practical utility of the theoretical connection.\n\n3.  **Inherited Scalability:** The experiments suggest that the proposed SRDD model inherits the strong parameter scalability of VAR, with performance improving consistently as model size increases."}, "weaknesses": {"value": "1.  **Seriously Unfair Baseline Comparison (Fatal Flaw):** This is a fundamental flaw. In the Table 1 comparison, the authors explicitly state that the DiT-L/2 baseline has not converged during the training process on two unconditional generation tasks. Furthermore, the captions for Figure 12 and Figure 13 in the appendix reinforce that LDM and DiT-L/2 also didn't converage on other datasets. Comparing a novel model against unconverged baselines is methodologically invalid. It renders all claims of superiority over DiT and LDM unsubstantiated. The authors should provide results from a fair comparison against fully converged and properly tuned baselines.\n\n2.  **Insufficient Experimental Validation:** All experiments are conducted at a moderate resolution ($256 \\times 256$) and on medium-scale datasets. The authors acknowledge this in the limitations section, attributing it to a \"modest GPU budget.\" Therefore, while the model shows a good scaling *trend*, its actual performance and efficiency on truly large-scale, high-resolution datasets (e.g., the full ImageNet 1K) remain unproven.\n\n3.  **Unclear Articulation of VAR's Flaws in the Introduction:** The introduction fails to fundamentally articulate the \"design flaws\" of the original VAR model. The authors mention \"design inefficiencies\" and that conditioning on all previous scales is \"redundant,\" but largely support this by citing \"concurrent works.\" The paper should argue more clearly from first principles why the full autoregressive conditioning is a *fundamental* flaw, rather than just presenting it as an observation made by others."}, "questions": {"value": "1. **On the Justification of \"Mathematical Equivalence\" and the \"Deterministic Limit\"**\n\nThe authors' claim of \"mathematical equivalence\" heavily relies on the \"limiting case of a deterministic transition.\" This assumption seems to apply to the forward process (which the authors map to deterministic downsampling).\n\nHowever, the reverse process for both standard diffusion models and the generative (reverse) process for SDD, $p_{\\theta}(x_{t-1}|x_t)$, is inherently probabilistic, as it is a learned distribution. When deriving the loss function equivalence (Equations 6 to 7), the authors appear to base this on the deterministic assumption.\n\n- Why is it reasonable to use this \"deterministic transition\" limit when analyzing the *learned*, *probabilistic* reverse generation process?\n\n- Does this assumption oversimplify the problem, thereby weakening the rigor of the core \"mathematical equivalence\" claim?\n\n2. **On Novelty—Theoretical Explanation vs. Architectural Contribution**\n\nThe core architectural change from VAR to SDD is replacing the full causal attention mask with a Markovian one (attending only to the previous scale). The authors also note that concurrent work has already observed this inefficiency in VAR.\n\nDoes this imply that the sole novelty (sole novelty) of this paper is merely providing a new \"discrete diffusion\" theoretical explanation for a known (or simple) architectural variant? Or is there a deeper architectural contribution beyond simply changing the attention mask that this reviewer has missed? The primary contribution appears to be the theoretical perspective itself, which enables the principled application of diffusion techniques (CFG, MR). Please clarify. Thank you."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6mAq2IPmEM", "forum": "TUbWCiUX1x", "replyto": "TUbWCiUX1x", "signatures": ["ICLR.cc/2026/Conference/Submission14309/Reviewer_p1EH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14309/Reviewer_p1EH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815789069, "cdate": 1761815789069, "tmdate": 1762924746400, "mdate": 1762924746400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* This paper focuses on bridging the gap between Visual Autoregressive Generation (VAR) models and discrete diffusion models. The authors make a key theoretical insight: when VAR is equipped with a Markovian attention mask (conditioning only on the immediate previous scale rather than all prior scales), it is mathematically equivalent to a discrete diffusion process. They term this reinterpretation Scalable Visual Refinement with Discrete Diffusion (SRDD).\n\n* Building on this equivalence, the paper leverages diffusion-based principles to address VAR’s architectural inefficiencies. Specifically, SRDD integrates three core diffusion-inspired strategies: (1) classifier-free guidance (CFG) (optimized to a weight of 5.0 to avoid over-conditioning), (2) token resampling (Masked Resampling, MR) (resampling tokens with prediction probabilities < 0.01 for 15–25 iterations), and (3) scale distillation (pruning redundant intermediate scales while retaining high-resolution stages to reduce inference cost)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1/ This paper is written very clearly, allowing me to quickly grasp the details of the work and the optimizations implemented. It elaborates on the connection between visual autoregressive modeling (based on next-scale prediction) for VAR and discrete diffusion, and optimizes token resampling based on this relationship.  \n\n2/ The mathematical derivations and reasoning in this paper are quite intuitive and make sense to me."}, "weaknesses": {"value": "I believe this paper has the following issues:  \n1/ This work bears significant similarities to the accepted paper at CVPR 2025, *Hmar: Efficient hierarchical masked auto-regressive image generation*. I think the authors need to clearly explain the differences between the two works.  \n\n2/ Building on the first point, the optimizations proposed in this paper are mostly engineering-level improvements—such as CFG optimization, token resampling, and distillation—with limited innovation and insufficient technical contributions.  \n\n3/ All experiments are conducted on a very small scale(for example minimagenet), making it difficult to compare the proposed method with other approaches.\n\nThis paper gives me the impression of being more like an analysis report—it analyzes the relationship between VAR and discrete diffusion, yet lacks novel insights. All the methods mentioned in the paper have been presented in previous works (such as HMAR, MaskGIT, etc.)."}, "questions": {"value": "None\nI am familiar with the relevant papers and methods, have basically understood this paper, and I have no questions about it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OR390n7oHb", "forum": "TUbWCiUX1x", "replyto": "TUbWCiUX1x", "signatures": ["ICLR.cc/2026/Conference/Submission14309/Reviewer_ECCX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14309/Reviewer_ECCX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868673182, "cdate": 1761868673182, "tmdate": 1762924745945, "mdate": 1762924745945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper re-examines Visual Autoregressive Generation (VAR), a popular framework for image generation in the lens of diffusion. By design, VAR encodes different scales of the same image into a sequence of discrete latents, then uses an autoregressive model to model the sequence. The authors argue that a modified \"Markovian\" version of VAR, which predicts the next image scale based only on the immediate previous scale, is mathematically equivalent to a special case of discrete diffusion model where the transition is deterministic. The paper, in addition, shows that inference techniques applied in diffusion are applicable to \"Markovian\" VAR and obtains superior performance than the original VAR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper elucidates a connection between autoregressive scale generation and discrete diffusion."}, "weaknesses": {"value": "The arguments presented for VAR being discrete diffusion are more conceptual than mathematical. In addition, the formulation can be better explained and mathematical notation should be aligned to that of diffusion.\n\nIt's unclear to me why block-wise attention VAR is equivalent to discrete diffusion while VAR is not. \n\nThe paper claims that the superior performance of the Markovian attention in VAR is more fundamental (turning VAR to a diffusion model) than a mere regularization effect. However, I find it not convincing enough. (see questions)"}, "questions": {"value": "1. Could the authors clarify whether $I_0$ is the latent of original image at the highest scale or the downsampled image at the lowest scale? From the notation, it seems to be the lowest scale as it defined $I_n \\in R^{n \\times n}$, but then it also says $M(n) \\in R^{n^2 \\times N^2}$ is a downsampling operation.\n\n2. The authors describe the transition $M(n)$ as \"a non-linear deterministic downsampling operation\". Is there a more concrete form of $M(n)$ or it only needs to satisfy $R_n=I_n − (I_{n−1})_{\\uparrow(n)}$which is dependent on the upsampling network learned? Authors suggest that \"the exact transformation is provided in the supplementary material\", which I failed to find in the appendix.\n\n3. Could the author provide a clear definition of \"Markovian\" VAR in the paper?\n\n4. Could the author explain why Markovian attention VAR is equivalent to discrete diffusion while VAR is not? Using a Markovian attention pattern doesn't make the network strictly Markovian $p(R_{n}|R_{n-1})$, since the tokens from previous scale aggregates information from all previous scales. Therefore, the Markovian attention VAR still models $p(R_{n}|R_{n-1},\\dots, R_0)$ which is exactly the same as VAR.\n\n5. Could you explain why Markovian VAR results in a higher ELBO than autoregressive formulation? (line 253-255)\n\n6. line 314, is it a typo $beta_2=0.05$?\n\n7. How do the experiments demonstrate the better performance from Markovian attention is not a regularization effect? In the scaling experiment, the gap shrinks as the model gets bigger. The author also admits that the all ablations are on reduced datasets which is a setting strongly favors a regularized model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JQWgskbtDl", "forum": "TUbWCiUX1x", "replyto": "TUbWCiUX1x", "signatures": ["ICLR.cc/2026/Conference/Submission14309/Reviewer_YJyd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14309/Reviewer_YJyd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998079196, "cdate": 1761998079196, "tmdate": 1762924745604, "mdate": 1762924745604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that Visual Autoregressive Generation (VAR) becomes mathematically equivalent to a discrete diffusion process if you replace its block-causal mask with a Markovian attention mask (each scale conditions only on the previous one). Framed as Scalable Discrete Diffusion (SDD)—and with confidence-aware masked resampling and classifier-free guidance (SRDD)—the reinterpretation lets the authors import standard diffusion tricks (iterative refinement, guidance, scale pruning) directly into VAR. Empirically, SRDD improves sample quality and efficiency across 256×256 benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper gives a clean connection from scale-wise AR to discrete diffusion. \n2. Based on this connection, several training techniques proposed in discrete diffusion can be introduced to improve the performance of the Markovian version of VAR.\n3. SRDD improves over VAR across datasets and is better in/out-painting and super-resolution without task-specific training."}, "weaknesses": {"value": "1. Experiments are limited to small datasets; comparisons against current strong scale-wise AR or modern discrete-diffusion generative models at similar compute are absent.\n2. No precise theorem that states conditions under which the Markovian VAR is exactly a discrete diffusion.\n3. There are several typos and formatting errors in the paper. Please polish the paper writing."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "toJsx9g3In", "forum": "TUbWCiUX1x", "replyto": "TUbWCiUX1x", "signatures": ["ICLR.cc/2026/Conference/Submission14309/Reviewer_4Y57"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14309/Reviewer_4Y57"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153837811, "cdate": 1762153837811, "tmdate": 1762924745243, "mdate": 1762924745243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}