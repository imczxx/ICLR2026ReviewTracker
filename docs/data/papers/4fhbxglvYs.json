{"id": "4fhbxglvYs", "number": 342, "cdate": 1756736031208, "mdate": 1759898266788, "content": {"title": "Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding", "abstract": "Real-world images often suffer from spatially diverse degradations such as haze, rain, snow, and low-light, significantly impacting visual quality and downstream vision tasks. Existing all-in-one restoration (AIR) approaches either depend on external text prompts or embed hand-crafted architectural priors (e.g., frequency heuristics); both impose discrete, brittle assumptions that weaken generalization to unseen or mixed degradations. To address this limitation, we propose to reframe AIR as learned latent prior inference, where degradation-aware representations are automatically inferred from the input without explicit task cues. Based on latent priors, we formulate AIR as a structured reasoning paradigm: (1) which features to route (adaptive feature selection), (2) where to restore (spatial localization), and (3) what to restore (degradation semantics). We design a lightweight decoding module that efficiently leverages these latent encoded cues for spatially-adaptive restoration. Extensive experiments across six common degradation tasks, five compound settings, and previously unseen degradations demonstrate that our method outperforms state-of-the-art (SOTA) approaches, achieving an average PSNR improvement of 1.68 dB while being three times more efficient. Code will be released upon publication.", "tldr": "We propose reframing all-in-one restoration using latent priors and structural reasoning cues to effectively handle both seen and unseen degradations.", "keywords": ["all-in-one restoration", "latent encoding", "prompt-free AIR", "image enhancement"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e019a20b434172f8aa5ccec7bc449cce2855c461.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes DAIR, a degradation aware all-in-one restoration approach which aims to tackle unknown degradations. Existing methods require human manual inputs, prompts or hand-crafted priors to provide degradation information to the model, which presents challenges when dealing with unseen degradations. DAIR is built to automatically tackle unseen and compound degradations with a 'which, where, what' approach. The authors propose latent prior encoding to identify 'which' features from the encoder are required for restoration. A degradation map is then constructed which dictates 'where' the model should restore followed by cross-modal fusion to determine the content to reconstruct. A decoder (3WD) uses this information to produce the restored output. DAIR achieves SOTA performance."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The paper tackles an important problem of existing all-in-one restoration approaches which is addressing unknown degradations.\n3. The idea of pre-training a VAE to learn degradation representations is interesting.\n4. DAIR achieves SOTA performance with significant boosts in performance.\n5. The overall approach has many components whose importance is validated by an ablation study."}, "weaknesses": {"value": "See questions for ways to address major weaknesses.\n\n**Major**:\n\n 1. Lack of experiments to demonstrate the core motivation of the architecture: The authors show the importance of their proposed modules in Table 6. However, this does not support the ‘which, where, what’ approach, i.e., the core motivation of the work.\n\n 2. Lack of benchmarking on unseen task restoration: There are only experiments on unseen mixed degradations in the main paper, which does not support the claim in the abstract and introduction. \n\n 3. The YOLOv12 experiment is claimed as a contribution but has no other mention in the main paper. If it is to be claimed as a contribution, the experiment must be moved from the supplementary to the main paper.\n\n**Minor**:\n1. Citation style: Brackets are missing around the citations in many instances.\n2. To the best of my knowledge, PromptIR does not use manual prompt and is blind. So, that needs to be changed throughout the paper."}, "questions": {"value": "**More ablations (major weakness 1)**: \n1. Is there a benefit in using the luminance and chrominance encoders? How is performance affected if the VAE feature is directly used?\n2. Can the authors provide experiments showcasing the ‘which’ and ‘what’ features? For instance, can the authors provide experiments to show that the output of LPE is ‘which’ features to use for decoding (and similarly for ‘what’)?\n3. Can the authors provide experiments to confirm that the VAE captures degradation priors?\n\n**Lack of benchmarking (major weakness 2)**: The real-world experiments on non-compound degradations need to be moved to the main paper. Additionally, can the authors provide quantitative comparisons with other methods for these tasks? Also, can the authors provide results on real mixed degradations (such as LOLBlur [1])?\n\n**Fig. 5**: In Fig. 5, what features were used to plot the t-SNE?\n\n[1] Zhou, Shangchen, Chongyi Li, and Chen Change Loy. \"Lednet: Joint low-light enhancement and deblurring in the dark.\" European conference on computer vision. Cham: Springer Nature Switzerland, 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bH4lioscou", "forum": "4fhbxglvYs", "replyto": "4fhbxglvYs", "signatures": ["ICLR.cc/2026/Conference/Submission342/Reviewer_SuKu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission342/Reviewer_SuKu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761502205497, "cdate": 1761502205497, "tmdate": 1762915497640, "mdate": 1762915497640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DAIR reframes all‑in‑one image restoration (AIR) as learned latent prior inference. Guided by these learned priors, the method reasons about which features to route, where to restore, and what content to reconstruct (the “which–where–what” paradigm). Results illustrated in the manuscript seem good but limited."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Moving from prompts/architectural biases to latent prior inference gives a coherent, task‑agnostic design that maps naturally to which–where–what reasoning.\n2. The 3WD module relies on element‑wise gating for linear (in spatial size) complexity, avoiding quadratic attention overhead. This yields favorable compute/param budgets in multi‑task settings.\n3. Notable improvement in downstream detection (YOLOv12‑L), with 34.9 mAP, supporting practical impact beyond pixel metrics."}, "weaknesses": {"value": "1. The experimental setting is not standard compared with other all-in-one methods. The authors shall add additional two settings in Promptir or Perceive-ir.\n2. The main fig for the whole framework is bad and confusing. For example, I can't figure out where is the input for degradation map in (c) referred in every stage of (a).\n3. The questions of 'which, where and what' is good but lacks some convincing proofs or experimental results. It makes the manuscript more like a story made on the framework rather than solving key problems with insights.\n4. As is stated by the authors that the proposed method do not rely on textual or degradation prompts, the loss of SupCon for VAE pretraining actually involves degradation label, which means the method still need supervision from degradation information."}, "questions": {"value": "1. The same as weakness1, the compared method like AdaIR do the different settings, and why the authors choose a unique setting. Have doubts on the performance of normal experimental settings.\n2. Typos in line 277-278 shall be fixed.\n3. The training is conducted on a single Nvidia 3060 with a large scale of data. I have doubt on the training cost like day length for that.\n4. Further studies would help: frequency cues inside degradation map, LC luminance‑only vs. luminance+chrominance, and latent fusion variants.\n5. Why do the authors choose desnowing and low-light enhancement for single tasks? What about other single tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mExpXTbdPE", "forum": "4fhbxglvYs", "replyto": "4fhbxglvYs", "signatures": ["ICLR.cc/2026/Conference/Submission342/Reviewer_jDsD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission342/Reviewer_jDsD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756906265, "cdate": 1761756906265, "tmdate": 1762915497473, "mdate": 1762915497473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DAIR, a degradation-aware, all-in-one image restoration framework that eliminates manual prompts and hand-crafted priors. It reframes AIR as latent prior inference: a VAE-style encoder infers multi-scale latent codes and a global descriptor from the degraded image; a learnable degradation map (DM) tells the network where to restore; a µ-guided latent fusion decides what to restore; and a lightweight 3WD (which–where–what decoding) module performs spatially adaptive reconstruction with claimed linear complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem reframing: Casting AIR as latent prior inference is a clean, unified alternative to prompt-driven or frequency-heuristic approaches.\n2. Well-structured reasoning design: The which–where–what decomposition (latent-prior selection, degradation maps, and µ-conditioned fusion) is conceptually coherent and easy to follow.\n3. Efficiency claims: 3WD replaces quadratic attention with element-wise modulation, yielding linear-in-pixels complexity and large reported speed/memory savings."}, "weaknesses": {"value": "1. The conceptual gap to earlier non-prompt AIR (e.g., contrastive degradation embeddings, frequency mining) could be articulated and empirically isolated more sharply; the VAE-based latent prior may read as an incremental integration of known ideas.\n2. The encoder is pre-trained on mixtures (including some compound degradations) and then frozen. How this compares, apples-to-apples, to baselines’ pre-training or data exposure is unclear; fairness details (data volume, augmentations, early stopping) should be expanded.\n3. The complexity discussion focuses on the inner attention form. End-to-end wall-clock and memory benchmarks (same HW, resolution, batch size) vs. SA/CA across multiple image sizes would better substantiate the claimed 85–257× speedups.\n4.  “Unseen” tests are limited to two combos (haze+snow, low-light+rain). It would help to include more OOD types (e.g., underwater, compression artifacts, lens flare) and real-capture datasets to support robustness claims."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1X8cEAFgXV", "forum": "4fhbxglvYs", "replyto": "4fhbxglvYs", "signatures": ["ICLR.cc/2026/Conference/Submission342/Reviewer_9byp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission342/Reviewer_9byp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806972196, "cdate": 1761806972196, "tmdate": 1762915497374, "mdate": 1762915497374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DAIR, a unified framework that reframes all-in-one restoration as latent prior inference rather than prompt-following or hand-crafted inductive biases. A VAE-style latent prior encoder infers multi-scale degradation codes directly from a degraded image; these codes drive a three-part “which–where–what” reasoning pipeline: (i) which features to route (latent-prior-modulated encoders), (ii) where to restore (a learnable degradation map combining spatial features with FFT cues), and (iii) what to reconstruct (a µ-guided latent fusion that adaptively scales/shifts structural and chromatic branches). A lightweight 3WD decoder performs element-wise, degradation-guided attention with linear complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear which–where–what decomposition and a linear-time 3WD decoding mechanism; thoughtful integration of luminance/chrominance branches plus FFT-based cues for degradation localization. \n2. Strong empirical coverage: common, compound, and unseen degradations; consistent average PSNR/SSIM gains; tangible downstream benefits on object detection.\n3. Solid ablations (latent priors, DM, µ-fusion, 3WD) and efficiency analysis (FLOPs, linear attention vs. SA/CA) that support the design choices."}, "weaknesses": {"value": "1. Many benchmarks are synthetic (e.g., SOTS, Rain100L/CCD, compound datasets). Real-capture datasets (wild rain/snow/low-light, RAW/ISP pipelines) are underrepresented; robustness to sensor noise/ISP and compression artifacts remains unclear.\n2. Heavy reliance on PSNR/SSIM; limited perceptual/user-study evidence. Including LPIPS, MUSIQ/NIQE, or human preference tests would better validate perceptual gains and avoid PSNR-overfitting.\n3. While prior AIR (PromptIR/ADAIR/DFPIR/DiffUIR) are covered, results against task-specialized SOTA per degradation (e.g., latest LLIE, dehaze, derain, deblur transformers/diffusions) are not uniformly comprehensive on each dataset split. \n4. The encoder pretraining mixes single/compound degradations, then is frozen; risk of domain overfitting or latent collapse is only qualitatively addressed. Quantitative analyses of latent separability/stability across seeds and cameras would help.\n5. Degradation maps are compelling but lack quantitative localization validation (e.g., overlap with synthetic corruption masks, perturbation or pointing-game metrics). Similarly, how µ-fusion decisions correlate with true degradation semantics is not rigorously measured.\n6. Linear 3WD is attractive, but end-to-end latency on 2–8K images, memory usage under tiling, and comparison to windowed attention are not reported."}, "questions": {"value": "1. Do latent priors transfer to real-capture datasets (e.g., LLIE in the wild, real rain/snow) and RAW pipelines? Any results on smartphone benchmarks or cross-sensor generalization without finetuning?\n2. How stable are the learned {xℓ, µ} across runs/seeds? Provide cluster separability (t-SNE/UMAP) with degradation labels, NMI/ARI, and center/variance over seeds; any evidence of latent collapse under heavy compound degradations?\n3. On synthetic corruptions with known masks, what is the IoU/ROC of DM against ground truth per degradation (rain streaks, haze veil, snow flakes, low-light masks)? Does DM remain calibrated under unseen mixes?\n4. Sensitivity to FFT features (remove magnitude/phase), LC branching (luma-only vs chroma-only), FiLM parameterization (γ/β forms), and temperature/β in VAE loss? Please include variance across 3–5 runs.\n5. Could you report LPIPS/MUSIQ or a user study on a subset? Any failure cases (e.g., color shifts, ringing, detail hallucination) compared to PromptIR/ADAIR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ww5bwBIVoh", "forum": "4fhbxglvYs", "replyto": "4fhbxglvYs", "signatures": ["ICLR.cc/2026/Conference/Submission342/Reviewer_ivbb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission342/Reviewer_ivbb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900805988, "cdate": 1761900805988, "tmdate": 1762915497250, "mdate": 1762915497250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}