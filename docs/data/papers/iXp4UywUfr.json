{"id": "iXp4UywUfr", "number": 15737, "cdate": 1758254676460, "mdate": 1759897285428, "content": {"title": "SA-ResGS: Self-Augmented Residual 3D Gaussian Splatting for Next Best View Selection", "abstract": "We propose Self-Augmented Residual 3D Gaussian Splatting (SA-ResGS), a novel framework for stabilizing uncertainty quantification and enhancing uncertainty-aware supervision in next-best-view selection for active scene reconstruction. SA-ResGS improves both the reliability of uncertainty estimates and their effectiveness for supervision by generating Self-Augmented point clouds (SA-Points) via triangulation between a training view and a rasterized extrapolated view, enabling efficient scene coverage estimation. While improving scene coverage through physically guided view selection, SA-ResGS also addresses the challenge of under-supervised Gaussians, an issue exacerbated by sparse and wide-baseline views, by introducing the first residual learning strategy tailored for 3D Gaussian Splatting. This targeted supervision mechanism enhances gradient flow in high-uncertainty Gaussians by combining uncertainty-driven filtering with dropout- and hard-negative-mining-inspired sampling. Our contributions are threefold: (1) a physically grounded view selection strategy that promotes efficient and uniform scene coverage; (2) an uncertainty-aware residual supervision scheme that amplifies learning signals for weakly contributing Gaussians, improving training stability and uncertainty estimation across scenes with diverse camera distributions; (3) an implicit unbiasing of uncertainty quantification as a consequence of constrained view selection and residual supervision, which together mitigate the conflicting effects of wide-baseline exploration and sparse-view ambiguity in NBV planning. Experiments on Mip-NeRF 360, Deep Blending, and Tanks and Temples datasets demonstrate that SA-ResGS outperforms state-of-the-art baselines in both reconstruction quality and view selection robustness.", "tldr": "SA-ResGS enhances next-best-view selection in 3D Gaussian Splatting by combining physically grounded view selection with residual supervision, improving uncertainty estimation and reconstruction under sparse-view settings.", "keywords": ["3D Gaussian Splatting", "Active view selection", "Next Best View", "Residual Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60dc37cb64576e5cf8129228831970e029975c3a.pdf", "supplementary_material": "/attachment/17be1a39a5d3a222e94b479e33d707bcf4467a36.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SA-ResGS, a method for active scene reconstruction with 3D Gaussian Splatting that combines a geometry-aware prefilter for next-best-view (NBV) selection with an uncertainty-guided residual supervision strategy during training. The prefiltering stage builds “self-augmented points” by establishing correspondences between a trained camera view and a lightly extrapolated render; these points populate a voxelized, hash-encoded coverage space that favors candidate views offering genuinely new surface coverage before any uncertainty scoring is applied. The learning component renders both a full image and a subset image that prioritizes Gaussians considered uncertain (based on opacity/scale), which is intended to inject stronger gradients into weakly supervised splats without modifying the renderer. Experiments on Mip-NeRF 360 and a mixed set from Deep Blending / Tanks & Temples report small but sometimes consistent gains on PSNR/SSIM, with less consistent improvements on LPIPS. A runtime table suggests that candidate selection becomes faster due to the prefilter, although the per-iteration training cost increases because of the residual supervision procedure."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s main strength is the attempt to stabilize early NBV decisions by introducing a physically grounded coverage prefilter that does not depend on potentially unreliable uncertainty estimates. This is a clean and reusable idea that could be integrated into other active-reconstruction pipelines. The residual supervision mechanism is also practical, as it works at the level of data selection without modifying the renderer, and it is easy to plug into existing 3DGS training code. Finally, the candidate selection phase appears to benefit from the prefilter, which would be a useful engineering contribution."}, "weaknesses": {"value": "A major weakness of this work is that several core design choices are heuristic and insufficiently analyzed: \n\n1. The coverage prefilter relies on a hash-encoded voxel grid that can suffer from collisions and is sensitive to voxel size, yet no robustness study regarding the parameter is provided. \n\n2. The uncertainty-guided residual supervision selects Gaussians using opacity/scale; however, the author did not discuss the superiority against Fisher-infomation based uncertainty. [2,3]\n\n3. The pipeline uses MASt3R for dense correspondences without justifying this choice against strong alternatives. For example, VGGT [1] provides dense, geometry-aware matches/tracks and might be more stable under larger baselines or yield denser, less noisy SA-Points in low-texture areas; conversely, lighter matchers such as LoFTR/LightGlue [4] could reduce runtime and memory at some quality cost. Without a head-to-head comparison, it’s unclear whether the reported gains stem from SA-ResGS itself or from the particular strengths/quirks of MASt3R. In short, the generalizability of the method is undermined by not showing results with at least one alternative dense correspondence backbone.\n\n4. Most ablations are confined to Mip-NeRF 360; there is little evidence that the component-level gains carry over to Deep Blending or Tanks & Temples under the same protocol. The selection interval (the number of iterations of adding a view) is not varied or studied, even though it can materially affect both quality and runtime. In coverage estimation, the voxel size and thresholds are used without ablation, raising concerns about portability beyond the evaluated settings.\n\nThe experimental scope is not well aligned with common practice in active NeRF/NBV evaluation. The paper focuses primarily on Mip-NeRF 360, whereas much prior work, such as ActiveNeRF and FisherRF, treats NeRF-Synthetic as a primary benchmark specifically to enable comparisons across methods. Without results on NeRF-Synthetic under the standard protocol (dataset split, selection cadence, and training schedule), it is difficult to assess whether the reported gains carry over to the object-centric dataset. The paper should include NeRF-Synthetic results with the same NBV schedule and strong baselines. \n\nThe uncertainty-calibration experiments are weakly supported. The AUSE analysis is shown on a single scene and does not include detailed uncertainty–error correlation plots across multiple datasets. \n\nThe quantitative improvements are modest and inconsistent across metrics, raising questions about the practical impact of the method. While PSNR/SSIM sometimes tick up, LPIPS is not consistently improved and even degrades on some scenes. The paper also reports results with a single fixed random seed and does not provide standard deviations or confidence intervals, so it is hard to know whether the observed deltas are statistically meaningful or within run-to-run noise. \n\nThe qualitative evidence is similarly underwhelming. The supplementary visualization video shows results that are incremental and often visually hard to distinguish from the baseline; for example, on the Bonsai scene the differences are subtle at best. The paper does not provide region-wise quantitative metrics for these highlighted areas (e.g., masked PSNR/LPIPS on the zoom boxes), which would make the visual comparisons more diagnostic. \n\nThe runtime analysis is split and, therefore, unconvincing. The candidate-selection phase appears faster due to prefiltering, but per-iteration training becomes more expensive with residual supervision. Without an end-to-end latency comparison for a complete active reconstruction pipeline on the same hardware, with the same number of added views, selection interval, and total iterations, the practical benefit remains unproven. \n\n[1] Wang, Jianyuan, et al. \"Vggt: Visual geometry grounded transformer.\" CVPR. 2025.\n\n[2] Jiang, Wen, Boshu Lei, and Kostas Daniilidis. \"Fisherrf: Active view selection and mapping with radiance fields using fisher information.\" ECCV, 2024.\n\n[3] Li, Ruiqi, and Yiu-ming Cheung. \"Variational multi-scale representation for estimating uncertainty in 3d gaussian splatting.\" NeurIPS 2024. \n\n[4] Sun, Jiaming, et al. \"LoFTR: Detector-free local feature matching with transformers.\" CVPR. 2021."}, "questions": {"value": "1. Could you report results on the standard benchmark of active radiance field reconstruction, the NeRF-Synthetic dataset?  \n\n2. How sensitive is performance to the coverage-grid parameters (voxel size, dilation radius) and hashing choices? A small robustness study would help demonstrate that the gains are not brittle.\n\n3. Why did you choose opacity/scale as the uncertainty proxy for the residual subset? Did you evaluate alternatives such as Fisher-information based uncertainty, and if so, how do they compare?\n\n4. The method relies on MASt3R correspondences with extrapolated renders, especially early in training. Could you justify this design choice over methods like VGGT and LoFTR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FmeDVt4AMh", "forum": "iXp4UywUfr", "replyto": "iXp4UywUfr", "signatures": ["ICLR.cc/2026/Conference/Submission15737/Reviewer_2qKx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15737/Reviewer_2qKx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779479667, "cdate": 1761779479667, "tmdate": 1762925976368, "mdate": 1762925976368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SA-ResGS (Self-Augmented Residual 3D Gaussian Splatting), a framework designed to improve uncertainty-aware active mapping and next-best-view (NBV) selection in 3D reconstruction. The authors propose a Self-Augmented Point (SA-Point) mechanism, which triangulates correspondences between a real image and an extrapolated rendered view to produce physically grounded geometry priors for efficient scene-coverage estimation and robust NBV selection. To complement this, they introduce the first residual learning strategy for 3D Gaussian Splatting, which enhances training stability by explicitly reinforcing supervision on uncertain or weakly contributing Gaussians through uncertainty-guided sampling and dual-render residual supervision. Together, these innovations aim to stabilize uncertainty quantification, mitigate vanishing gradients, and improve reconstruction quality under sparse or wide-baseline conditions. Comprehensive experiments and ablations on Mip-NeRF 360, Deep Blending, and Tanks & Temples datasets demonstrate consistent gains in reconstruction fidelity, uncertainty calibration, and computational efficiency over existing NBV and uncertainty-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\n The paper makes a notable and original contribution by integrating physical constraints and a residual learning strategy into the active 3D Gaussian Splatting (3DGS) paradigm. The introduction of Self-Augmented Points (SA-Points) for physically grounded next-best-view selection represents a creative and conceptually elegant solution to the long-standing problem of unreliable uncertainty estimation under sparse-view settings. Moreover, the residual supervision mechanism—the first of its kind for 3DGS—extends the notion of residual learning from image-based networks to the 3D splatting domain, addressing vanishing gradient issues in an entirely new context. This combination of geometry-aware and uncertainty-driven reasoning is both novel and insightful, setting a promising new direction for active neural rendering research.\n\nClarity:\n The paper is well-organized and communicates its ideas clearly. Each component of the proposed framework is described with solid intuition and supported by algorithmic detail, figures, and pseudocode-level explanations that make the method straightforward to reproduce. The authors provide extensive implementation details in both the main text and appendix, including triangulation procedures, voxel hashing, and uncertainty filtering strategies. The logical flow from motivation to methodology and experiments is easy to follow, and the paper succeeds in making a technically complex idea accessible to a broad audience in 3D vision and rendering."}, "weaknesses": {"value": "While the proposed framework is conceptually strong and methodologically sound, the experimental results reveal certain limitations that warrant attention. Specifically, although SA-ResGS achieves clear improvements over several baselines, its quantitative performance does not consistently reach state-of-the-art levels in all metrics—particularly SSIM and LPIPS—suggesting room for further refinement in perceptual quality and structural consistency. Additionally, some experimental descriptions contain minor typographical errors and ambiguous phrasing which I will list in the questions. Addressing these clarity issues would further strengthen the empirical credibility and overall impact of the work."}, "questions": {"value": "1. The paper states that SA-Points mitigate the bias of uncertainty signals caused by incomplete or under-constrained geometry by providing a surface-aware guidance mechanism independent of uncertainty estimation. However, since SA-Points are triangulated using correspondences between a training view and an extrapolated rendered view, wouldn’t their accuracy still depend on the completeness and quality of the underlying geometry? Could the authors clarify how the method ensures stability of SA-Points when the 3DGS geometry is still immature in early training stages?\n\n2. In Table 1(a), the SSIM score of 0.610 is highlighted as the best, though it is not the highest value in the column. Could the authors clarify whether this is a typographical error or if there is a specific reason (e.g., statistical significance, dataset subset, or reporting convention) for highlighting this value?\n\n3. In Table 1(b), the ActiveNeRF baseline is missing, despite being included in Table 1(a). Was this omission intentional due to implementation or dataset compatibility issues, or could the authors provide results for completeness? Including this comparison would strengthen the empirical evaluation.\n\n4. The method does not achieve state-of-the-art results on SSIM and LPIPS metrics, which partially measure perceptual fidelity and structural quality. Could the authors elaborate on the potential causes for this performance gap, and whether tuning the residual weighting parameters or uncertainty thresholds might narrow the difference?\n\n5. The reported result for †+SA-ResGS in Table 2 differs from the corresponding “Ours” entry in Table 1(a), though both appear to represent the full model. Could the authors explain whether these configurations differ or if this discrepancy is due to averaging over different runs or datasets? A clarification would help ensure consistent interpretation of the ablation and main results.\n\n6. Recent NeRF and 3DGS-based active mapping approaches that leverage uncertainty—such as ‘Naruto: Neural Active Reconstruction from Uncertain Target Observations’ and ‘ActiveGAMER: Active Gaussian Mapping through Efficient Rendering’—are not discussed in the related work section. Including a comparison or discussion of these closely related methods would provide valuable context, highlight the distinctions of SA-ResGS, and better situate the proposed framework within the evolving landscape of uncertainty-aware active reconstruction research."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HdZ5XUlvKO", "forum": "iXp4UywUfr", "replyto": "iXp4UywUfr", "signatures": ["ICLR.cc/2026/Conference/Submission15737/Reviewer_zhib"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15737/Reviewer_zhib"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896726984, "cdate": 1761896726984, "tmdate": 1762925975843, "mdate": 1762925975843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SA-ResGS, a novel framework for active 3D reconstruction and next-best-view (NBV) selection built upon the 3D Gaussian Splatting (3DGS) paradigm. The key challenge addressed is the instability of uncertainty estimation and sparse geometric coverage during the early training phase of active 3DGS.\n\nThe authors introduce two main components:\n\nSelf-Augmented Points (SA-Points):\nA mechanism that synthetically perturbs the current camera viewpoint to create a virtual stereo pair. Dense correspondences (via MASt3R) between the current and perturbed views are triangulated into pseudo-3D points, which approximate surface coverage. These points are projected to candidate viewpoints, and a hash-based coverage score is computed to identify views that reveal new, unobserved regions.\n\nResidual Supervision for 3DGS (ResGS):\nA dual-path rendering supervision scheme in which one full rendering is complemented by a sub-rendering containing randomly sampled and high-uncertainty Gaussians. Both renderings are supervised against the ground truth, effectively amplifying gradients in uncertain or under-fitted regions.\n\nExperiments on Mip-NeRF360, Deep Blending, and Tanks & Temples show consistent PSNR/SSIM improvements over FisherRF and ActiveNeRF, with faster convergence and better uncertainty calibration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Conceptually novel SA-Points mechanism:\nThe idea of generating pseudo-3D points from a synthetically perturbed virtual view for coverage-aware NBV selection is original and well-motivated. It bridges geometric reasoning and learning-based active view planning in a lightweight manner.\n\nStability improvement without heavy computation:\nThe method improves early-stage training stability and uncertainty estimation without adding expensive optimization or extra 3D supervision.\n\nPractical training refinement:\nThe residual supervision elegantly balances global consistency and local refinement, improving reconstruction detail while maintaining stable gradients.\n\nComprehensive experiments:\nThe paper evaluates across multiple benchmarks, ablates both SA-Points and ResGS, and demonstrates performance and efficiency gains over strong baselines."}, "weaknesses": {"value": "ResGS innovation is incremental:\nWhile effective, the residual supervision is conceptually similar to uncertainty-weighted or hard-example reweighting schemes known in NeRF and 3DGS literature; the novelty lies more in integration than in principle.\n\nLack of robustness and generalization analysis:\nNo experiments test how SA-Points or ResGS behave when correspondence quality or uncertainty estimation deteriorates.\n\nAblation depth:\nIt remains unclear whether the gains are primarily due to SA-Points’ geometric guidance or simply more stable sampling. A finer ablation (e.g., varying perturbation magnitude or replacing MASt3R with a lighter matcher) would help clarify."}, "questions": {"value": "On SA-Points reliability:\nHow sensitive is the coverage estimation to correspondence errors from MASt3R? Have the authors tested using noisier or lower-quality matchers?\n\nOn manual parameters:\nWhat perturbation magnitude is used to synthesize the virtual viewpoint, and how was it chosen? Could this magnitude be adaptive to scene scale or depth range?\n\nOn ResGS interpretation:\nCan the authors clarify how their residual supervision differs conceptually from existing uncertainty-guided or sample-reweighting methods?\n\nOn robustness:\nIf the dense matching fails (e.g., in dynamic or textureless regions), does the coverage estimation degrade gracefully or bias the NBV selection?\n\nOn generalization:\nCould SA-ResGS handle unseen domains (e.g., moving scenes, outdoor environments), or is the method tied to static and well-textured indoor data?\n\nAdditional related works:\nActiveGAMER: Active gaussian mapping through efficient rendering\nNARUTO: Neural active reconstruction from uncertain target observations"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VfLXjAQl6O", "forum": "iXp4UywUfr", "replyto": "iXp4UywUfr", "signatures": ["ICLR.cc/2026/Conference/Submission15737/Reviewer_bGUu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15737/Reviewer_bGUu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974359474, "cdate": 1761974359474, "tmdate": 1762925975198, "mdate": 1762925975198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SA-ResGS introduces a novel framework for next-best-view selection in 3D Gaussian Splatting that addresses uncertainty quantification instability in sparse-view reconstruction. The method generates Self-Augmented Points (SA-Points) via triangulation between training views and extrapolated renders, enabling physically-grounded view selection that reduces dependence on uncertain early-stage model predictions. It proposes the first residual learning strategy for 3DGS, rendering both full-scene and uncertainty-guided Gaussian subsets (90% random + 10% most uncertain) to amplify gradients for weakly-supervised Gaussians, mitigating the vanishing gradient problem.\nThe two-stage selection pipeline (physical prefiltering via hash-encoded voxel dissimilarity, then uncertainty ranking) decouples view planning from training dynamics. Experiments on active view selection demonstrate that SA-ResGS outperforms state-of-the-art baselines in both reconstruction quality and view selection robustness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It clearly points out three specific limitations of existing NBV methods, which is valuable and insightful for readers.\n\n2. The paper introduces an innovative approach to decouple view selection from early-stage uncertainty estimation through Self-Augmented Points (SA-Points). Addresses a real limitation as the early-stage uncertainty estimates in 3DGS are unreliable due to sparse geometry and training instability."}, "weaknesses": {"value": "1. The paper positions itself as contributing to sparse-view 3D reconstruction but only compares against view selection methods, does not compare against specialized sparse-view reconstruction methods for example FSGS, SparseGS, DNGaussian, MVPGaussian, RegGaussian. Need to clarify how much the 'Next Best View Selection' useful, for example, given 20 images, carefully select views (SA-ResGS) + standard training compared with uniform sampling +  strong regularization (FSGS, SparseGS, ...), also need add experiments to carefully select views (SA-ResGS) + all other different sparse-view reconstruction methods to verify that the view selection contribution is orthogonal to regularization methodology.\n\n2. The paper claims to introduce \"the first residual supervision framework for 3DGS\", but several prior works have proposed to address the vanishing gradient problem in 3DGS, such as pixelSplat, PAPR, DropGaussian as the paper mentioned in related work, but haven't show experiments for 'ResGS' with other methods, also no ablation showing advantage of uncertainty-guided dropout over random dropout (DropGaussian). The \"first residual supervision\" claim appears overstated. The actual novel contribution—uncertainty-guided structured dropout—is not empirically validated against simpler random dropout baselines.\n\n3. In table2, fixed-order view selection is better than dynamically updated view selection, this is counterintuitive, as dynamic selection should adapt to training progress and outperform fixed sequences. The explanation in paper ‘training improvements alone are insufficient, especially under high computational uncertainty quantification errors’ is very limited, not provides any quantitative analysis, mechanism explanation, or uncertainty visualization to support this claim. And the ablation lacks dynamic_selection + standard_training, making it impossible to determine whether the problem stems from dynamic selection itself, residual supervision interference, or their interaction."}, "questions": {"value": "Please refer to the Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KI3tmLDqsC", "forum": "iXp4UywUfr", "replyto": "iXp4UywUfr", "signatures": ["ICLR.cc/2026/Conference/Submission15737/Reviewer_EVNA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15737/Reviewer_EVNA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038751641, "cdate": 1762038751641, "tmdate": 1762925974709, "mdate": 1762925974709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}