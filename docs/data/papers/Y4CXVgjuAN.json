{"id": "Y4CXVgjuAN", "number": 5702, "cdate": 1757927758635, "mdate": 1763545826427, "content": {"title": "UniCode: A Framework for Generating High-Quality Competitive Coding Problems", "abstract": "The reliance of competitive coding benchmarks on static, human-authored problems creates significant challenges, including data contamination and limited scalability. To address these issues, we introduce \\textbf{UniCode}, a novel framework that automatically generates high-quality algorithmic problems alongside robust, contamination-resistant test cases. Inspired by biological evolution that creates better and diverse offspring, our framework leverages Large Language Models (LLMs) to systematically diversify problems through three strategies: single-problem extension, same-type fusion, and cross-type fusion. A key innovation is our stress-driven test case synthesis pipeline, which generates reliable test suites without requiring a canonical ground-truth solution. This pipeline combines brute-force grounding for small-scale inputs with a consensus-based validation mechanism for large-scale inputs to ensure high correctness and coverage. We demonstrate effectiveness of our framework by curating a benchmark of 492 problems and evaluating 19 state-of-the-art LLMs. The results reveal that {UniCode} is highly challenging and discriminative, with the top-performing model, o4-mini, achieving a pass rate of only 70.3\\%. Our framework provides a scalable and reliable solution for generating dynamic evaluation datasets in coding domain.", "tldr": "", "keywords": ["Code generation", "Generative evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44becacdf9b493a5af63b264f5340ecacd8b96a4.pdf", "supplementary_material": "/attachment/986fdefadfa138701f4194ad3bee6030261a468e.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents UniCode, a framework for automatically generating competitive programming problems and test cases to reduce data contamination and improve scalability of code benchmarks. It expands seed problems through single-problem extension, same-type fusion and cross-type fusion, and builds reliable test sets via a stress-driven pipeline. The resulting dataset includes 492 verified problems.\n\nExperiments on 19 code models show strong discriminative power: even the best model reaches only 70.3% pass@1 and drops notably on adversarial cases. Models generalize well to paraphrased tasks but fail on structurally novel ones, suggesting limited algorithmic generalization. UniCode thus offers a scalable way to evaluate reasoning in code LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A well-designed, end-to-end test generation pipeline that avoids reference solutions. The per–test case majority voting between brute-force and optimized LLM solvers produces reliable labels while still preserving challenging instances.\n\n2. Practical dataset construction with clear procedures for input synthesis, difficulty calibration, problem taxonomy, and explicit time/memory budgets per test, making the benchmark genuinely usable rather than merely conceptual.\n\n3. A strong stance on contamination and robustness: the evolutionary problem generation (single-problem extension, intra-/cross-type fusion) yields structurally novel problems beyond paraphrases, mitigating leakage and probing true algorithmic generalization.\n\n4. High discriminative power across 19 models with transparent reporting (rand vs. adv), clearly exposing brittleness to adversarial inputs and the gap between linguistic robustness and algorithmic generalization."}, "weaknesses": {"value": "1. Limited ablations: while the pipeline is carefully engineered, the necessity of each stage is not well justified. The paper does not probe key choices such as per–test case vs. per–problem voting, solver diversity, or the 20/20/10 suite composition; adding an “LLM-pass” alongside Rand/Adv would also clarify the marginal value of LLM-synthesized inputs.\n\n2. Quality assurance remains narrow: per-testcase voting can still admit residual mislabeled cases, and the human validation sample is small"}, "questions": {"value": "Regarding the 20/20/10 suite composition, beyond the exact ratios, could you justify the necessity of including G_llm? Please add a G_llm-Pass metric (analogous to RandPass/AdvPass) to quantify its marginal value, and include a small sensitivity check showing results with and without G_llm inputs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LCUBWu74jN", "forum": "Y4CXVgjuAN", "replyto": "Y4CXVgjuAN", "signatures": ["ICLR.cc/2026/Conference/Submission5702/Reviewer_xybC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5702/Reviewer_xybC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571563837, "cdate": 1761571563837, "tmdate": 1762918207789, "mdate": 1762918207789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents UniCode, a generative evaluation framework for creating novel competitive-programming problems with corresponding test suites, aiming to reduce contamination and improve scalability over static benchmarks. It diversifies tasks via three strategies—extension, same-type fusion, and cross-type fusion—and constructs test suites without canonical solutions through stress-testing, majority voting, and LLM adjudication. The resulting dataset includes 492 problems across 15 tags, evaluated on 19 LLMs, with the best achieving ~70% pass@1. Human evaluation reports 98% solvability, and ablations show improved correctness (94.5%) and coverage (86.0%) over baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It addresses contamination and saturation issues in code benchmarks by proposing a generative evaluation framework that automatically produces new, diverse programming tasks.\n- The multi-stage test-suite construction (stress-testing, filtering, majority voting, adjudication) effectively removes the need for canonical solutions and yields high correctness and coverage.\n- The benchmark demonstrates strong discriminative power across 19 models, supported by human validation and ablation analyses indicating robustness and scalability."}, "weaknesses": {"value": "- The paper does not discuss or compare with Evol-Instruct and its derivative WizardCoder, which pioneered LLM-based progressive task evolution. The proposed single-problem extension is conceptually similar to Evol-Instruct’s “in-depth evolution,” undermining the novelty of the generation component. \n- The entire framework relies heavily on a single closed-source model (o4-mini) for both task generation and final adjudication. This creates potential “generator bias,” as the benchmark might inadvertently favor models with similar capabilities or architectures.\n- The framework depends on seed problems from TACO, yet does not analyze whether these seeds or their variants exist in common pretraining corpora. Without such an overlap check, the contamination-resistance claim remains unverified."}, "questions": {"value": "- Why are Evol-Instruct and WizardCoder not cited or compared? How is your single-problem extension fundamentally different from their progressive-evolution mechanism?\n- Have you tested alternative (preferably open-source) generators or adjudicators? How does this affect solvability rates or model rankings?\n- What are the dominant sources of the 5.5% incorrect test cases—faulty brute-force baselines, consensus failures, or adjudication errors?\n- Does “cross-type fusion” always yield truly hybrid algorithmic problems, or are they often decomposable into sequential subproblems from the seed set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I8xkNwf6C6", "forum": "Y4CXVgjuAN", "replyto": "Y4CXVgjuAN", "signatures": ["ICLR.cc/2026/Conference/Submission5702/Reviewer_2L99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5702/Reviewer_2L99"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721509871, "cdate": 1761721509871, "tmdate": 1762918207319, "mdate": 1762918207319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is devoted to introduction of a novel code generation benchmark. The methodology includes a step of verification via small input brute-force solver.\n\nPersonally, I like the idea of creating such a benchmark, although I do not see how it solves an issue of \"vast memorization of training data\", since this benchmark in 6 months will be in training dataset for every new model. \n\nAnother important issue - it is unclear how small input solver helps with big input, since in programming competitions it is usually the case, that some algorithm is not able to work with bigger input for the same problem. The authors report 98% of solvability of generated problems, although they evaluated only 50 problems, that means that one problem gives 2% error.\n\nIn closing, I would like to say that the proposed methodology seems to be interesting, but the benchmark itself does not solve an issue of previously introduced benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "novel methodology to generate problems"}, "weaknesses": {"value": "Personally, I like the idea of creating such a benchmark, although I do not see how it solves an issue of \"vast memorization of training data\", since this benchmark in 6 months will be in training dataset for every new model. \n\nAnother important issue - it is unclear how small input solver helps with big input, since in programming competitions it is usually the case, that some algorithm is not able to work with bigger input for the same problem. The authors report 98% of solvability of generated problems, although they evaluated only 50 problems, that means that one problem gives 2% error."}, "questions": {"value": "a) Please describe how your benchmark solves a mentioned issue?\n\nb) Please evaluate the generated problems more thoroughly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RUuy48nBEH", "forum": "Y4CXVgjuAN", "replyto": "Y4CXVgjuAN", "signatures": ["ICLR.cc/2026/Conference/Submission5702/Reviewer_bwy7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5702/Reviewer_bwy7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836492887, "cdate": 1761836492887, "tmdate": 1762918206987, "mdate": 1762918206987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UniCode, a novel framework for automatically generating high-quality and contamination-resistant algorithmic coding problems and test cases to overcome the limitations of static, human-authored benchmarks. Drawing inspiration from biological evolution, UniCode leverages Large Language Models (LLMs) to diversify problems through single-problem extension, same-type fusion, and cross-type fusion, producing more varied and challenging tasks. Using this framework, the authors curate a benchmark of 492 algorithmic problems and evaluate 19 state-of-the-art LLMs, finding that UniCode poses a substantial challenge."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The motivation for automatically generating high-quality and contamination-resistant algorithmic coding problems and test cases to overcome the limitations of static, human-authored benchmarks is convincing.\n\nThe evaluation covers 19 LLMs, which is intensive."}, "weaknesses": {"value": "Test generation metrics: only coverage and correctness are adopted. The fault detection capability of the tests is not investigated, which is quite important: a correct test case with a high coverage can be useless in detecting bugs if the test oracle is weak.\n\nThe abstract motivates the work from two aspects: data contamination and limited scalability. I did not find data contamination analysis in this paper, it is therefore difficult to judge the superiority of the proposed benchmark in terms of data contamination mitigation. A helpful solution is to compare the performance of LLMs in newly generated problems and old existing competitive problems with similar difficulty, and compare their results. If LLM's performance is significantly lower on the generated problems, if will demonstrate the consequence of data contamination and further motivate the proposed approach.\n\nThe paper has many arbitrary choices without sufficient justification and explanation. For example, why did you select single problem extension, same-type fusion, and cross-type fusion? Why not other strategies? Why did you assemble a final test suite S of 50 cases\nwith a fixed composition: 20 random, 20 adversarial, and 10 LLM-synthesized inputs, why this distribution?\n\nThe approach adopts o4-mini-medium to generate a set of candidate problems, which may cause bias towards o4-mini's performance in comparison to other LLMs."}, "questions": {"value": "How are the three strategies (single problem extension, same-type fusion, and cross-type fusion) selected?\n\nHow do the generated problems compare to the existing competitive problems with similar difficulty when running against different LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fJLVtlSBJB", "forum": "Y4CXVgjuAN", "replyto": "Y4CXVgjuAN", "signatures": ["ICLR.cc/2026/Conference/Submission5702/Reviewer_tfss"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5702/Reviewer_tfss"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954390788, "cdate": 1761954390788, "tmdate": 1762918206713, "mdate": 1762918206713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}