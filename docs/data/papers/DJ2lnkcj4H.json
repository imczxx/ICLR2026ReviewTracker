{"id": "DJ2lnkcj4H", "number": 23117, "cdate": 1758339824107, "mdate": 1759896831679, "content": {"title": "Signal Preserving Weight Initialization for Odd-Sigmoid Activations", "abstract": "Activation functions critically influence trainability and expressivity, and recent work has therefore explored a broad range of nonlinearities. However, activations and weight initialization are interdependent: without an appropriate initialization method, nonlinearities can cause saturation, variance collapse, and increased learning-rate sensitivity. We address this by defining an odd–sigmoid function class and, given any activation \\(f\\) in this class, proposing an initialization method tailored to \\(f\\). The method selects a noise scale in closed form so that forward activations remain well dispersed up to a target layer, thereby avoiding collapse to zero or saturation. Empirically, the approach trains reliably without normalization layers, exhibits strong data efficiency, and enables learning for activations under which standard initialization methods (Xavier, He, Orthogonal) often do not converge reliably.", "tldr": "", "keywords": ["Weight Initialization", "Activation Function", "Signal Propagation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b53c73a1ce3117419f7a7e6dacda2bdce6f0c55.pdf", "supplementary_material": "/attachment/4a95575bbfa90c62e0b8e7158532a809839409cf.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the initialization of neural networks and proposes a new strategy of initialization when using sigmoid-like activation functions, denoted by $f$. The entire study is based on the fixed points of $f$: when $f$ belongs to a given set of sigmoid-like functions, there exists a critical input scaling $\\omega > 0$ such that $x \\mapsto f(a x)$ has only one fixed point ($0$) when $a < \\omega$, and three fixed points ($0, \\xi_a, -\\xi_a$) when $a > \\omega$.\n\nThis paper proposes an initialization strategy based on this critical number $\\omega$, which depends on the shape of $f$, which is meant to achieve several goals:\n1. with well-chosen input scalings ($a > \\omega$), the pre-activations are guaranteed not to converge to 0 in probability (Theorem 4.6);\n2. the outputs of the network meet a specific requirement (Theorem 4.7).\n\nFinally, some experiments in simple cases (MNIST, Fashion-MNIST) show the superiority of this initialization strategy compared to Xavier, He and Orthogonal."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main idea is easy to follow.\n\nIt is still interesting to design initialization distributions for specific activation functions. Moreover, a complete study of the fixed-point distribution obtained after a large number of layers is still missing in the initialization of neural networks literature.\n\nThe proposed initialization distribution can be easily used."}, "weaknesses": {"value": "## Motivation\n**Discussion about former works.** The paper states that it proposes a method that \"keeps activation distributions well dispersed across layers, mitigating collapse to zero or saturation.\" The paper should provide evidence that former initialization strategies fail to address these issues. At least, the paper must explain the limitations of former initialization strategies. Please note that the \"Edge of Chaos\" works [2, 3] (cited in the paper) already address the issues of \"collapse to zero\" or \"saturation\".\n\n## Clarity\nThis paper contains several notations and terms that are not properly defined. This is a major issue, since it prevents the reader to fully understand some parts of the paper, for instance Theorem 4.7.\n\n**Negative rate $p$.** I did not understand what this \"negative rate\" is and why it is so important. I only understand that it is related to the output of the model.\n\n**Scalar surrogate $\\pi_L$.** Same issue: what is it? What is the link with calibration?\n\n**[Non-critical] Variance $\\sigma_z$.** What is it? It seems that this notation comes from [1], but it is not defined in this paper (it should be removed...).\n\n**[Non-critical] Functions $\\Phi_m, \\Phi_m^{\\alpha}$.** These notations should be replaced with $\\Phi^m$ and $\\Phi^m_{\\alpha}$, since they consist of functions $\\phi_a$ combined $m$ times. \n\n**Non-readable figures.** Figure 2 is very difficult to read: the text is too small.\n\n## Significance of Theorem 4.6\nI do not understand why Theorem 4.6 is significant whatsoever. It seems that the goal is to show (ii), stating that the probability that the model outputs sth. away from $0$ is bounded by below. However:\n1. the \"gains\" $(a_1, \\cdots, a_m)$ are related to the outputs of the neurons of the layers $(1, \\cdots, m)$, so the assumption that $(a_1, \\cdots, a_m)$ are Gaussian and independent does not hold (even if $a_i$ is Gaussian conditionally to the inputs of layer $i$);\n2. the neural network is assumed to have $N_l = 1$ neuron per layer, which is obviously not true.\n\nOverall, the restricted hypotheses of Theorem 4.6 are not discussed: we do not know if they are actually restrictive, and we do not know if the claims hold in practice (with dependent activations and several neurons per layer).\n\n## Preceding work\nThis papers is entirely based on [1]: same notation, same basic ideas (e.g., rewriting of the propagation used in Eqn. (1)), and same focus on the fixed points. While [1] focuses on $\\tanh$, this paper extends the study on [1] to a set of activation functions. This inspiration from [1] is entirely endorsed in the current paper, and there is nothing wrong about that.\n\nHowever, the significance of the current paper compared to [1] can be discussed. As such, the initialization strategy proposed here is based on only two new contributions:\n1. the study of $\\tanh$ is extended to a set of $\\tanh$-like functions;\n2. the tuning of the input scaling is now founded on a heuristic (and not a numerical estimation), which allows us to compute it analytically.\n\nThe significance of Theorem 4.6 the clarity of Theorem 4.7 are discussed above.\n\n## References\n\n[1] Robust weight initialization for tanh neural networks with fixed point analysis, Lee et al., ICLR 2024.\n\n[2] Exponential expressivity in deep neural networks through transient chaos, Poole et al., NeurIPS 2016.\n\n[3] Deep information propagation, Schoenholz et al., ICLR 2016."}, "questions": {"value": "What are the limitations of preceding works (e.g., Edge of Chaos)? Please be precise.\n\nCould you clarify the \"negative rate $p$\", \"$\\pi_L$\" and Theorem 4.7?\n\nDiscuss the hypotheses and the conclusion of Theorem 4.6. Limitation? Possibility to loosen the hypotheses? Does the conclusion hold in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dwUKtnBHcG", "forum": "DJ2lnkcj4H", "replyto": "DJ2lnkcj4H", "signatures": ["ICLR.cc/2026/Conference/Submission23117/Reviewer_Wx7z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23117/Reviewer_Wx7z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655988993, "cdate": 1761655988993, "tmdate": 1762942516379, "mdate": 1762942516379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a weight initialization method for feed-forward neural networks (FFNNs) whose activation functions belong to a class referred to as odd-sigmoid functions.\nAn odd-sigmoid activation function denotes a monotonic, origin-symmetric nonlinearity such as tanh.\nBuilding upon the analysis of fixed points of activation functions (similar to the discussion in Lee et al., 2025), the authors propose a weight initialization strategy that ensures the variance of activations in a layer remains above a certain threshold.\nExperiments on MNIST and Fashion-MNIST demonstrate that the proposed initialization leads to higher accuracy during the early training epochs compared to conventional initialization schemes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper introduces a novel weight initialization method grounded in the fixed-point analysis of activation functions.\n* It provides a theoretical analysis of the variance of activations in FFNNs initialized with the proposed scheme."}, "weaknesses": {"value": "* Some theoretical assumptions discussed in the paper seem to deviate from realistic conditions (see the Questions section below).\n* The proposed method is not applicable to activation functions that are not origin-symmetric, such as ReLU.\n* The evaluation metrics used in the experiments differ from common benchmarks, making it unclear how practically useful the proposed approach is."}, "questions": {"value": "* l.057: The paper states that “the proposed initialization reduces reliance on normalization layers such as Batch Normalization, thereby lowering the burden of hyperparameter tuning (e.g., depth/width selection).”\n  However, it is unclear how the absence of normalization layers is related to reduced dependency on depth or width selection.\n\n* In Proposition A.2 and Corollary A.3, the coefficient $a_n$ is assumed to be positive.\n  However, according to Eq. (1), this assumption does not seem to hold in general. Could the authors clarify this point?\n\n* In Theorem 4.6, the assumption $N_\\ell = 1$ is introduced.\n  Does this imply that the width of every layer in the FFNN is set to one?\n  If so, what general insights can be drawn from this theorem for more realistic FFNNs with $N_\\ell > 1$?\n\n* What does the term “width proxy” in Figure 4 represent?\n\n* What learning rate was used for the Adam optimizer?\n\n* Regarding Table 1:\n\n  * Since the initialization involves random sampling, the results should be reported as averages and standard deviations over multiple random seeds.\n  * Why are only the first five epochs evaluated? It would be more informative to include results over a standard number of training epochs.\n\n* Do “MLP” and “FFNN” refer to the same architecture throughout the paper?\n\n* For Table 2: Training for only ten epochs with extremely limited data (e.g., 100 or 500 samples) seems unrealistic.\n  What kind of practical implication can be drawn from such results?\n\n* In Figure 5, the authors consider cases where even models with Batch Normalization fail to train properly.\n  Therefore, the claim that the proposed method enables BN-free training does not appear to be justified based on these results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "c2xtXMzMs7", "forum": "DJ2lnkcj4H", "replyto": "DJ2lnkcj4H", "signatures": ["ICLR.cc/2026/Conference/Submission23117/Reviewer_4CNB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23117/Reviewer_4CNB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890471391, "cdate": 1761890471391, "tmdate": 1762942516058, "mdate": 1762942516058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a weight initialization method designed for a special class of activation functions called odd–sigmoids. These are functions that are smooth, bounded, strictly increasing and odd. The goal of this initialization is to ensure signal preservation: as activations pass through deep layers, they neither collapse to zero nor saturate, without needing normalization layers like BatchNorm."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The exposition seems quite clear and accessible, the derivations are straightforward, the notation consistent and the ideas and goals quite clear."}, "weaknesses": {"value": "- The theoretical contribution seem to heavily overlap with results on signal and variance preservation (e.g., Poole et al 2016; Schoenholz et al. 2016; Hayou et al.  2019) within a subclass of activations.  \n\n- The paper seems to only studY forward signal propagation and ignores gradients, Jacobian conditioning, or correlation dynamics, see for example the work by Pennington et al.\n\n- The experiments are limited to MNIST and Fashion-MNIST with vanilla MLPs and quite small depths.  Reported improvements over Xavier, He, or Orthogonal initialization seem modest and could lie within statistical variation?\n\n- From a theory perspective comparison with a number of prior works appears to be missing, particularly Hayou et al. (2019) and Murray et al. (2021) where there appears to be a clear conceptual overlap with regard to analyzing the role of the activation.\n\n\nLinks to some of the papers mentioned.\n- https://openreview.net/pdf?id=H1W1UN9gg\n- https://proceedings.neurips.cc/paper_files/paper/2017/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf\n- https://proceedings.mlr.press/v97/hayou19a.html\n- https://www.sciencedirect.com/science/article/pii/S1063520321001111"}, "questions": {"value": "- It is not clear to me what the analysis of the recursion $x_{n+1} = f(a_n x_n)$ and the ``pitchfork bifurcation'' add beyond established initialization principles such as initialization on the edge of chaos? How does this proposed framework differentiate itself from prior `edge-of-chaos or dynamical isometry analyses?\n\n- Can you contrast and compare your theoretical results with Hayou et al. (2019) and Murray et al. (2021)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Not applicable."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IPy5tiyaIN", "forum": "DJ2lnkcj4H", "replyto": "DJ2lnkcj4H", "signatures": ["ICLR.cc/2026/Conference/Submission23117/Reviewer_iEW8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23117/Reviewer_iEW8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939707015, "cdate": 1761939707015, "tmdate": 1762942515852, "mdate": 1762942515852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}