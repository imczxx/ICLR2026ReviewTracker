{"id": "7HT5i6w6qU", "number": 7663, "cdate": 1758031141937, "mdate": 1759897840713, "content": {"title": "TinyEye: Sharpening Visual Reasoning of Tiny Models with Offline Policy Optimization", "abstract": "Multimodal reasoning with small vision–language models (VLMs) is increasingly important in real-world applications, yet their limited capacity makes optimization and alignment especially challenging. In this paper, we propose a holistic framework for offline policy optimization to sharpen the visual reasoning capabilities of small models. At its foundation is TinyEye-Data, a large-scale corpus of two million reasoning trajectories distilled from state-of-the-art VLMs across 68 verifiable tasks, which provides diverse and reliable binary supervision entirely in the offline setting. We instantiate the framework through a four-stage pipeline: (1) native-resolution warm-up for robust vision–language alignment, (2) instruction tuning on TinyEye-Data to establish a broad reasoning foundation, (3) annealed rejection sampling to mine hard cases and refine supervision, and (4) Discriminative Direct Preference Optimization (DDPO), a new margin-based objective that formulates policy learning as reward classification and resolves the likelihood displacement issues of DPO. Stages (3) and (4) together form the core of verifiable offline reinforcement learning, where rejection sampling refines signals and DDPO optimizes the policy against them. The resulting model, TinyEye-2B, achieves state-of-the-art results across diverse reasoning benchmarks, reaching 50.3% on MMMU, 55.2% on MathVerse, and 63.9% on HallBench, outperforming other models of comparable scale by significant margins.", "tldr": "An extensive recipe to sharpen the visual reasoning capabilities of small models with offline policy optimization", "keywords": ["VLM", "reasoning", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e246fbae39c5284cd856426cfbcaa4f2510bc262.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a four-stage offline pipeline culminating in a core contribution, “Discriminative Direct Preference Optimization (DDPO),” which reframes DPO’s pairwise objective into a margin-based binary classification of response. DDPO is claimed to “resolve the likelihood displacement issue of DPO” and to “guarantee” monotonic reinforcement of positives while suppressing negatives under binary rewards. Empirically, DDPO is reported to beat a DPO baseline.\n\nOther than that, the authors also proposed a TinyEye-Data dataset containing 2M reasoning trajectories from advanced VLMs. Combined with traditional VL alignment training (stage 1), SFT (stage 2) and rejection sampling (stage 3), the resulting model TinyEye-2B is able to achieve competitive results on various reasoning benchmarks.\n\nOverall, I see limited technical contribution in this work and lean to recommend rejection pending author’s discussion."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear end-to-end pipeline. The four-stage offline recipe (native-resolution warm-up → large instruction-tuning → annealed rejection sampling → Stage-4 preference training) is well structured and practical for small VLMs.\n\n2. Competitive results. DDPO shows meaningful gains over the stated DPO baseline on several benchmarks in the Stage-3 → Stage-4 transition.\n\n3. Claimed mitigation of DPO’s “likelihood displacement.” The paper motivates a concrete pathology and offers a principled surrogate expected to avoid it."}, "weaknesses": {"value": "1. While the DDPO seems to be the core contribution of this paper, we see mixed evidence of effectiveness vs. DPO on benchmarks. Table 4 shows DDPO isn’t uniformly superior: on MathVista, DPO (68.0) edges DDPO (67.9). The paper emphasizes DDPO’s strong HallBench gains (binary classification-like), which might reflect task-objective alignment rather than a general advantage. \n\n2. TinyEye-Instruct/Reason are said to be “compact and verifiable,” but the paper provides sparse details on sources, license, overlaps, or leakage control, which is important for fair comparisons and reproducibility."}, "questions": {"value": "1. Can you provide targeted experiments that provoke likelihood displacement and show DDPO avoids it while DPO fails, holding all else equal (same pairs, same ref, same tuning)?\n\n2. What $\\alpha$ works best, how is it chosen, and how do outcomes change across $\\alpha$?\n\n3. Could you add DPO-Positive / SimPO / ORPO baselines, tuned comparably, to demonstrate DDPO’s advantages are not limited to default DPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KcZoeAH3GZ", "forum": "7HT5i6w6qU", "replyto": "7HT5i6w6qU", "signatures": ["ICLR.cc/2026/Conference/Submission7663/Reviewer_kzE3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7663/Reviewer_kzE3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760809036352, "cdate": 1760809036352, "tmdate": 1762919730726, "mdate": 1762919730726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of insufficient visual reasoning capabilities in small vision-language models (VLMs) for real-world edge deployment, where existing post-training methods (Supervised Fine-Tuning/SFT, Direct Preference Optimization/DPO, Reinforcement Learning/RL) suffer from overfitting, likelihood displacement, and high computational costs. To solve this, the authors propose TinyEye, a unified offline policy optimization framework designed to enhance the reasoning performance of compact VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.TinyEye-Data: A large-scale verifiable corpus of 2 million reasoning trajectories distilled from state-of-the-art teacher VLMs\n2.Empirical Validation: The resulting 2B-parameter model, TinyEye-2B, achieves state-of-the-art performance across diverse benchmarks: 50.3% on MMMU (multimodal reasoning), 55.2% on MathVerse (math reasoning), and 63.9% on HallBench (general multimodal QA)."}, "weaknesses": {"value": "1. The proposed method in the paper is only tested on a 2B-parameter model. It lacks experiments on 7B-parameter models, which are commonly used in both research and practical applications. This makes it hard to tell if the method works effectively for 7B-parameter models too.  \n2. The training process requires TinyEye-Data, which has 2 million reasoning trajectories. Using such a large amount of data may be inconvenient in real-world scenarios (e.g., situations where data is scarce). However, the paper does not discuss how to solve this problem.  \n3. The paper does not study how different image compression qualities (e.g., how much an image is compressed) affect the model’s reasoning results. In practice, images are often compressed, so it is unclear whether the model can still perform well in reasoning when dealing with compressed images."}, "questions": {"value": "1. Has the proposed method been compared with other RLHF methods such as GRPO on the same dataset?\n2. Could you provide the impact of different data compression ratios on reasoning performance?\n3. Could you show the performance of a 7B-parameter model on these benchmarks — for example, comparing with similar 7B models like *m2-Reasoning*, which used less than 2M training samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fcGHwRx0tP", "forum": "7HT5i6w6qU", "replyto": "7HT5i6w6qU", "signatures": ["ICLR.cc/2026/Conference/Submission7663/Reviewer_bWcL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7663/Reviewer_bWcL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761481278175, "cdate": 1761481278175, "tmdate": 1762919729975, "mdate": 1762919729975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TinyEye, a visual–language reasoning framework designed for tiny multimodal models (around 2B parameters). The method introduces a four-stage offline training pipeline that includes a novel Discriminative Direct Preference Optimization (DDPO) to replace DPO for better stability under binary reward settings. The authors also build a large-scale, verifiable reasoning dataset (TinyEye-Data) distilled from multiple teacher models. Experiments demonstrate significant improvements in multimodal reasoning benchmarks compared to existing open-source models of similar scale."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and well structured; the motivation is well grounded in the “likelihood displacement” problem of DPO.\n- Empirical results are strong: TinyEye-2B outperforms competitive open-source models such as InternVL-3 and Qwen2.5-VL, even though these models (might) use much larger training resources.\n- The authors provide clear empirical evidence and intuitive explanations showing that DDPO improves training stability and alignment for small models."}, "weaknesses": {"value": "- Since the base LLM is Qwen3-1.7B, it would be fair to include a direct comparison with InternVL-3.5-2B, which also builds on Qwen3-1.7B. Qwen2.5-VL and InternVL-3 seem somewhat outdated as baselines.\n\n- The full training pipeline (Stage 1–4) is quite complex, involving multiple data generation and filtering steps, which could limit scalability and reproducibility.\n\n- It would strengthen the work to include a comparison with online RL-based methods such as GRPO under a controlled setup.\n\n- As DDPO is model-agnostic, it would be useful to test whether it generalizes to other mainstream VLM families (e.g., Qwen-VL, InternVL series) to confirm its robustness."}, "questions": {"value": "In Table 1, could you explain why the model performs so well on MathVision and WeMath? Is it because the training data includes similar tasks or distributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kMbVzbiYKC", "forum": "7HT5i6w6qU", "replyto": "7HT5i6w6qU", "signatures": ["ICLR.cc/2026/Conference/Submission7663/Reviewer_dBHH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7663/Reviewer_dBHH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841524843, "cdate": 1761841524843, "tmdate": 1762919729548, "mdate": 1762919729548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TinyEye, a holistic offline policy optimization framework to improve visual reasoning in compact VLMs. It introduces TinyEye-Data (2M verifiable reasoning trajectories across 68 tasks), a four-stage training pipeline—(1) native-resolution warm-up for robust vision–language alignment, (2) large-scale instruction tuning over TinyEye-Instruct/Reason, (3) annealed rejection sampling with tandem repeat avoidance (TRAS) to mine hard cases and prevent collapse-like degeneracy, and (4) Discriminative Direct Preference Optimization (DDPO), a margin-based, sign-preserving objective tailored to binary rewards that avoids DPO’s likelihood displacement. The resulting 2B model achieves SOTA among small VLMs on multiple benchmarks (e.g., 50.3 MMMU val, 55.2 MathVerse, 63.9 HallBench), with ablations showing consistent gains from each stage and DDPO outperforming DPO, especially on binary-evaluated tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Well-motivated offline pipeline for small models: coherent coupling of distillation, rejection sampling, and discriminative preference optimization that avoids expensive and unstable on-policy RL.\n\nVerifiable, diverse data at scale: TinyEye-Data spans 179+93 datasets with task-specific verification (symbolic/numeric checks for math, VLM-as-judge for open-ended), plus multi-teacher distillation and pass@8 difficulty estimation.\nPractical safeguards against collapse: tandem repeat detection and TRAS during sampling; shortest-chain aggregation to favor concise, effective reasoning.\n\nNovel objective with theory: DDPO reframes DPO’s relative likelihood into binary reward classification with a sign-preserving margin; appendix provides derivation linking to GSPO and argues away DPO’s additive-shift degeneracy.\n\nStrong empirical results for 2B scale: competitive or superior to 2B–4B baselines across multimodal reasoning, textual math, and general VQA; clear improvements at each stage; thinking vs. no-thinking ablation demonstrates CoT value."}, "weaknesses": {"value": "Limited novelty: The proposed four-stage training pipeline offers practical value in engineering integration and feasibility, but its methodological originality appears limited from an academic standpoint. Each stage largely relies on combinations of existing paradigms and hyperparameter tuning, making it difficult to pinpoint substantive breakthroughs in theoretical framing, learning objectives, or training dynamics. Consequently, its potential to inspire and transfer to subsequent research remains to be further demonstrated.\n\nQuestionable fairness of distillation: The training data are heavily distilled from stronger teacher models. Although such a setup can be expected to improve performance, it also complicates attribution: the current experiments do not systematically compare different distillation configurations and methods, making it unclear whether gains primarily stem from teacher capability transfer or from the proposed training mechanisms and objectives. This undermines the strength of the methodological claims. It is advisable to include comparable baseline methods at the teacher/distillation level to demonstrate the non-triviality of the approach.\n\nInsufficient baselines: The comparisons are concentrated in Stage 4 and are reasonably thorough against standard DPO, but the first three stages lack systematic, side-by-side evaluations against alternative alignment schemes, SFT/RFT recipes, or interchangeable components. At the algorithmic level, the work also omits equal-data, equal-compute comparisons with a broader set of offline preference/policy optimization methods (e.g., SimPO, ORPO, DPO-Positive, Alpha-DPO). This limits both the external validity of the conclusions and the clarity of attribution. A unified-protocol, multi-method comparison is recommended.\n\nLarge-scale inclusion of public benchmarks in training: TinyEye-Data incorporates a substantial number of widely used community benchmarks as training sources. While using benchmark data for proof-of-concept studies can be understandable, directly employing them as large-scale training corpora may erode these benchmarks’ credibility and validity as independent evaluation and comparison tools, thereby impacting the long-term benchmarking ecosystem. This practice is especially contentious in the context of training open-weight models.\n\nLimitations of purely offline strategies: While end-to-end offline policy optimization improves stability and cost controllability, it also introduces typical issues of distributional mismatch and limited reachability [1]. For multimodal tasks, textual evidence in offline trajectories may not be reliably grounded by the current student model to the visual inputs, potentially exacerbating vision–language hallucinations and semantic mismatches [2]. The authors are encouraged to discuss these issues in the paper.\n\n[1] Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models\n[2] Semi-off-Policy Reinforcement Learning for Vision-Language Slow-Thinking Reasoning"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PgRzvHND7P", "forum": "7HT5i6w6qU", "replyto": "7HT5i6w6qU", "signatures": ["ICLR.cc/2026/Conference/Submission7663/Reviewer_r5LZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7663/Reviewer_r5LZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982635735, "cdate": 1761982635735, "tmdate": 1762919729131, "mdate": 1762919729131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors focus on the training techniques of existing LVLMs, including the supervised fine-tuning, offline reinforcement learning, and online reinforcement learning, and claim the urgent need for a discriminative, stable, and efficient training method. Then, authors propose a unified offline policy optimization framework with corresponding datasets called TinyEyes. Experiment are conducted on a training-from-craft LVLM with the proposed method and data. The trained TinyEye-2B demonstrates competitive results on math, language, and general scenarios with learning LVLMs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  A well-distilled 2M data including the instruction part and reasoning part are proposed, which may contribute to the community if open-sourced.\n2. The overall results are competitive among LVLMs whose parameters are less than 4B."}, "weaknesses": {"value": "1. The motivation is not clear.  This paper proposes to solve the problem of post-training for small models. While the shortcut problem of naive reinforcement learning exists for small models, the low-margin problem are common for all sizes of models. Meanwhile, the efficiency     claim should be further claimed, as the offline method like DPO requires a large amount of preference annotations.\n\n2. Though four contributions are listed in the introduction, the core contribution is not highlighted. Most of the motivation and background part focus on the post-training techniques. But the first and second contribution mentioned are the overall pipeline to train a LVLM with processed data. \n\n3. The techniques novelty is limited. The explicit methods used in the proposed framework are widely used and explored in existing LVLMs. Similar procedure to the framework is also commonly observed in leading LVLMs like Keye, Mimo, Sail, etc.. The most important part in the paper is the DDPO. However, the core design of DDPO has limited and incremental novelty and is more like an engineering-level optimization.\n\n4. The experiments can not support the claim. Or say what is the main claim for this paper? Table 4 only compares the DDPO with DPO without other recent studies in DPO."}, "questions": {"value": "1. This paper may require careful re-organization to highlight the main claim and contribution to show the real problem to be solved. Most designs show limited relevance with small parameters models.\n\n2. Though the trained TinyEye-2B model has competitive results, how to reach such performance is not clear besides the stage gains, more data or higher quality data or training objectives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bwzuGmpfWQ", "forum": "7HT5i6w6qU", "replyto": "7HT5i6w6qU", "signatures": ["ICLR.cc/2026/Conference/Submission7663/Reviewer_Zz4q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7663/Reviewer_Zz4q"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7663/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762257625454, "cdate": 1762257625454, "tmdate": 1762919728771, "mdate": 1762919728771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}