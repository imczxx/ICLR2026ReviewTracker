{"id": "V1vOjoWcvc", "number": 3055, "cdate": 1757323662516, "mdate": 1763543238123, "content": {"title": "Towards understanding multimodal in-context learning", "abstract": "Multimodal Large Language Models (MLLMs) often exhibit in-context learning (ICL) capability, yet the mechanisms governing its formation remain poorly understood. Specifically, how architectural choices and training data statistics jointly shape the emergence of this ability is still an open question. To understand this, we aim to reverse-engineer multimodal ICL by training transformers on controlled synthetic classification tasks with varying multimodal data statistics and architectural choices.\nOur investigation begins by re-evaluating foundational principles of unimodal ICL within modern transformer architectures. While many prior findings hold, our experiments reveal two surprising discoveries. First, Rotary Position Embeddings (RoPE), a standard component in modern LLMs, can impair the formation of ICL circuits. Second, we find that larger models require stronger statistical cues in the training data for ICL to emerge.\nExtending our analysis to the multimodal regime uncovers a fundamental learning asymmetry: once a primary modality develops the core ICL circuit from statistically diverse data, a secondary modality can achieve comparable performance with far less data complexity. In contrast to the unimodal case, we show that model scaling consistently improves multimodal ICL.\nAdditionally, we find that a high-quality pretrained encoder for the secondary modality is critical. Through mechanistic analysis with progress measurements tracking the formation of ICL circuits, we show that both unimodal and multimodal ICL rely on a common induction-style circuit that copies the label from the in-context exemplar that matches the query; multimodal training mainly refines this behavior rather than introducing new circuitry. We validate some findings on real MLLMs and provide practical implications.\nThese results provide a concrete, mechanism-level account of ICL in modern multimodal transformers.", "tldr": "", "keywords": ["in-context learning", "multimodal learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/db16d03d594975182c83407234ab182abbc96a88.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates multimodal in-context learning (ICL) mechanisms through controlled synthetic experiments on transformers trained with Gaussian Mixture Models. The key findings include: (1) RoPE impairs ICL by disrupting induction heads, (2) scaling unimodal models raises the data complexity threshold for ICL emergence, favoring memorization when data complexity is fixed, (3) in multimodal settings, a primary modality develops the core ICL circuit with secondary modalities then requiring minimal data complexity, and (4) scaling improves multimodal ICL unlike the unimodal case. The authors introduce \"progress measurements\" that track attention patterns and validate selected findings on Qwen2.5-VL and IDEFICS."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written, and the exposition is clear. \n- The observation about RoPE and its negative impacts on induction heads, is interesting and appears to be genuinely novel.\n- The controlled synthetic experiments are well designed, with quantifiable progress measurements to successfully isolate factors affecting ICL emergence in the chosen regime, extending prior mechanistic work on induction heads to multimodal settings."}, "weaknesses": {"value": "- The biggest weakness is that although benefits of cross-modal alignment are discussed, the paper entirely omits any analysis of how two modalities actually interact (information transfer, cross attention circuits) during ICL inference, which is often the key driver in modern LLMs (see plethora of literature on improving multimodal ICL). It remains unclear whether genuine cross-modal reasoning emerges or whether the language model simply performs ICL on features that include visual information. \n- The empirical validation on actual MLLMs is limited, with just one benchmark and minimal mechanistic probing. For the trends that are validated, these would be largely predictable even without any mechanistic analysis and as such, it is unclear if the key findings hold in practice. \n- More importantly, the findings may not generalize beyond the data-scarce regime studied. For instance, the negative effects of RoPE or scaling effects might change or even become irrelevant with appropriate data scaling, as in practice, larger models with RoPE still demonstrate excellent ICL performance. This issue is not discussed in the paper. \n- Minor typo: Line 422: \"two main components\" but only lists 1\n\nOverall: While this paper makes solid strides towards understanding multimodal ICL, some claims appear to be under-substantiated due to missing cross-modal interaction analysis and limited real MLLM validation. If the authors can argue why this is not the case on either front, the reviewer can lean towards accepting."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I7uHzMvUBO", "forum": "V1vOjoWcvc", "replyto": "V1vOjoWcvc", "signatures": ["ICLR.cc/2026/Conference/Submission3055/Reviewer_dWNT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3055/Reviewer_dWNT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792226902, "cdate": 1761792226902, "tmdate": 1762916532400, "mdate": 1762916532400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response Part 1: Summary of the reviews and new experiments"}, "comment": {"value": "We thank all reviewers for their thoughtful and constructive feedback. We appreciate that multiple reviewers recognize:\n\n- **The careful, controlled experimental design**: synthetic Gaussian mixture data with explicit control enables clear causal statements about how data statistics and architecture affect ICL (qRdn, edWe, dWNT).\n- **The novel architectural insight** that RoPE and related relative PEs can suppress induction circuits in low-data-complexity regimes (emphasized in strengths by all reviewers).\n- **The multimodal asymmetry finding**: a primary modality installs a reusable ICL circuit, enabling a secondary modality to achieve strong ICL with much lower data complexity (qRdn, edWe, vYWv).\n- **The quantitative progress measurements** (PHStrength, IndStrength, TLA, CLA) that are predictive of ICL accuracy and allow us to **track circuit formation for multimodal ICL** over training (qRdn, dWNT).\n\nWe are grateful for these assessments and have revised the paper to make these contributions and their intended scope more explicit.\n\n## New Experiments\n\nTo address reviewer concerns and strengthen our empirical validation, we conducted several new experiments and have updated the revisio (All updates in the manuscripts are presented in red text):\n\n1. **Extended Omniglot Validation (Appendix A.3.1)** (Reviewer qRdn, edWe, vYWv, dWNT): We now vary class diversity, burstiness, and class-frequency skew on Omniglot images, confirming our learning asymmetry transfers to real images.\n\n2. **Positional Encoding Sensitivity Analysis (Appendix A.2.2)** (Reviewer qRdn, dWNT): We varied context length and data complexity across APE, RoPE, and Hybrid PE. APE consistently outperforms RoPE at fixed complexity, but the gap narrows at very high complexity, confirming RoPE imposes an \"induction tax\" that requires sufficient data to overcome.\n\n3. **Model Scaling at Fixed Data Complexity (Appendix A.2.3)** (Reviewer qRdn, vYWv): We provide explicit ICL-IWL trade-off tables showing that at fixed data complexity, larger models favor memorization over ICL, explaining the rising thresholds in Fig. 2a.\n\n4. **Cross-Modal Interaction Analysis (Sec. 4.4.2, Appendix A.3.4)** (Reviewer dWNT): We add more analysis on modality integration. In addition, modality-zeroing ablations show ICL drops from 0.967 to 0.336 (M2 zeroed) and 0.063 (M1 zeroed), confirming the model uses both modalities rather than M1-only strategies.\n\n5. **Early-Fusion Joint Training (Appendix A.3.5)** (Reviewer vYWv): Training both modalities from scratch shows the primary-modality role depends on sequence geometry—when labels are adjacent to M2 tokens, the induction circuit anchors on M2 instead of M1.\n\n6. **Discussion added (Sec. 4.5)**: We included a new section discussion to connect our mechanistic insights to real MLLMs."}}, "id": "yIVMzB5EX1", "forum": "V1vOjoWcvc", "replyto": "V1vOjoWcvc", "signatures": ["ICLR.cc/2026/Conference/Submission3055/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3055/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3055/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763541591520, "cdate": 1763541591520, "tmdate": 1763566027946, "mdate": 1763566027946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the mechanisms underlying in-context learning (ICL) in both unimodal and multimodal  models. Through controlled synthetic experiments, the authors explore how architectural choices (e.g., use of RoPE) and data statistics influence the emergence of ICL. They report several key findings: (1) RoPE can hinder ICL circuit formation; (2) larger models require stronger statistical cues for ICL to emerge; and (3) in multimodal settings, once one modality learns the ICL mechanism, a secondary modality can achieve comparable behavior with much less data diversity. The work further argues that multimodal ICL relies on the same induction-style circuit as unimodal ICL, refined rather than replaced by multimodal training. Some of the findings are validated with Qwen and IDEFICs models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Understanding the mechanism of ICL, particularly in multimodal settings, is an important and underexplored area.\n\n* he paper is generally well-written, with findings and methodology clearly presented.\n\n* The identified role of RoPE and the modality asymmetry provide potentially valuable directions for future work."}, "weaknesses": {"value": "* The claim that \"Larger models consistently exhibit reduced ICL\" appears to contradict evidence from large LLMs, where scaling tends to improve ICL emergence. \n\n* ICL and zero-shot performance should be monitored together. The paper should discuss whether the observed effects might stem from general model capability rather than ICL-specific mechanisms. For example, low/high ICL performance might be due simply to high/high model performance.\n\n* Experimental details lacking: How many in-context examples (“shots”) were used? What modalities were considered, and how the authors define different modalities? What were the model sizes and dataset scales used in each experiment?\n\n* The validation on Qwen and IDEFICS is insufficient to substantiate most of the claims. Showing that the model scale correlate with ICL performance (which shown in previous work, including e.g. the IDEFICS paper) and the strenfght of induction heads are minor part of the paper claims. \n\n* Similar findings about preimary modality bias (dominance of a “primary” modality) have been reported before [1]. The paper should better position its contributions relative to these works.\n\n[1] Baldassini, Folco Bertini, et al. \"What makes multimodal in-context learning work?.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "questions": {"value": "* How joint (e.g., early-fusion) multimodal training might affect the observed asymmetry?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lCkpcdQqqb", "forum": "V1vOjoWcvc", "replyto": "V1vOjoWcvc", "signatures": ["ICLR.cc/2026/Conference/Submission3055/Reviewer_vYWv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3055/Reviewer_vYWv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947067908, "cdate": 1761947067908, "tmdate": 1762916532017, "mdate": 1762916532017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how ICL emerges and operates in MLLMs, aiming to uncover the mechanisms behind this ability at the circuit level.\nUsing controlled synthetic multimodal classification tasks, the authors systematically vary data statistics and architectural components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of synthetic Gaussian mixture data allows precise manipulation of multimodal statistics, which strengthens causal claims.\n\n2. Identification that RoPE suppresses induction circuits.\n\n3. Discovery of asymmetry between primary and secondary modalities and how pretraining on one modality installs transferable ICL circuits."}, "weaknesses": {"value": "1. The work largely follows existing analyses from prior studies, offering limited novelty.\n\n2. Experimental validation under real settings is limited, reducing confidence in the result’s generalizability."}, "questions": {"value": "1. How is the swapped-label implemented? \n\n2. It is not immediately clear why the ICL–IWL balance performs best when α₂ ≈ 1 based on the figure. Could the authors provide further justification?\n\n3. The subgraphs in the bottom portion of Figure 6 are difficult to interpret. Can the authors clarify what each represents and how they relate to the main texts?\n\n4. What exactly is meant by “raw high-dimensional feature”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BNeeAfbRLu", "forum": "V1vOjoWcvc", "replyto": "V1vOjoWcvc", "signatures": ["ICLR.cc/2026/Conference/Submission3055/Reviewer_edWe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3055/Reviewer_edWe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986254118, "cdate": 1761986254118, "tmdate": 1762916531293, "mdate": 1762916531293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper systematically studies how in-context learning (ICL) emerges in multimodal transformers using a controlled synthetic setup built from Gaussian mixtures. By varying data complexity factors and introducing quantitative diagnostics—PHStrength, IndStrength, TLA, and CLA—the authors trace the formation of induction-style attention circuits. The key findings are: (1) rotary and other relative positional encodings weaken ICL formation; (2) scaling increases the data complexity threshold for unimodal ICL, promoting memorization; (3) multimodal ICL is asymmetric, with the primary modality bootstrapping learning for the secondary; and (4) pretrained encoder quality is crucial for strong multimodal ICL. Results are validated on large models like Qwen2.5-VL and IDEFICS. The work provides a clear, mechanistic view of how architecture, scaling, and representation quality interact to produce ICL behavior in multimodal transformers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Careful, controlled experimental design — synthetic GMM data + control over K, ε, B, α gives clear causal evidence for how data statistics drive ICL in both uni- and multimodal regimes. This leads to mechanistic progress measurements — PHStrength, IndStrength, TLA, CLA are well-motivated, quantitatively predictive, and allow the authors to track circuit formation over training. \n\nClear novel architectural insight: RoPE harms induction circuits — the paper demonstrates that RoPE (and ALiBi) consistently reduce ICL accuracy vs absolute PEs and produce more diffuse attention that weakens previous-token / induction heads. T\n\nImportant multimodal asymmetry finding — showing that a decoder pretrained on a high-diversity primary modality can bootstrap ICL such that the secondary modality needs far less diversity/burstiness is an intuitive and practically useful result for dataset and architecture design."}, "weaknesses": {"value": "Synthetic → real generalization limited — while synthetic control is powerful, results hinge on idealized GMMs; the real-data validation is limited (Qwen2.5-VL analysis and a small Omniglot probe). Broader real-world tests are needed to confirm generality.\n\nPositional encoding recommendation could be risky in practice — the paper shows RoPE/ALiBi hurt ICL in these tasks, but RoPE brings other benefits (length generalization, training stability). The manuscript does not fully quantify tradeoffs (e.g., effect on other tasks, or hybrid encodings), which limits actionable guidance. \n\nScaling analysis might conflate capacity vs data regime — the unimodal result (“larger models need more complex data to show ICL”) is interesting, but the experiments use fixed data budgets. It remains unclear whether larger models trained with proportionally more data would still favor in-weight memorization. The compute/data scaling frontier isn’t fully explored."}, "questions": {"value": "How might these findings inform training recipes for large production MLLMs (positional encoding choice, pretraining mix, encoder pretraining)? The paper gives implications — could you make them more prescriptive?\nHow sensitive are the RoPE-vs-absolute results to context length and dataset complexity? Is there a regime where RoPE still dominates (e.g., much longer contexts)?\nIn unimodal scaling experiments, if you scale data proportionally with model size, does the ICL threshold still increase? Please report model×data scaling curves."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BM74HSflfl", "forum": "V1vOjoWcvc", "replyto": "V1vOjoWcvc", "signatures": ["ICLR.cc/2026/Conference/Submission3055/Reviewer_qRdn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3055/Reviewer_qRdn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762789884218, "cdate": 1762789884218, "tmdate": 1762916530768, "mdate": 1762916530768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}