{"id": "FdaUyJLlzM", "number": 23315, "cdate": 1758342041099, "mdate": 1759896821526, "content": {"title": "rePIRL: Learn PRM with Inverse RL for LLM Reasoning", "abstract": "Providing process rewards has been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. \nIn LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy.\nHowever, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability.\nIn this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies.\nSpecifically, we design a dual learning process that updates the policy and the PRM interchangeably.\nOur learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs.\nWe theoretically show that our proposed learning framework can unify both online and offline PRM learning methods with additional assumptions, justifying that rePIRL can learn PRMs with minimal assumptions.\nEmpirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing PRM learning methods. \nOur ablation studies further show the effectiveness of our key designs.", "tldr": "", "keywords": ["Inverse RL; LLM reasoning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/038345d18a088a7dcd95fbe07d6b5b2554619c99.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to learn a process reward from expert demonstrations via inverse reinforcement learning. Specifically, the paper parameterizes the trajectory distribution using energy model and solves a maximum likelihood estimation problem to learn the process reward. The algorithm alternates between reward update and policy update, which is a standard practice of IRL. The experiment uses Qwen-2.5-3B to demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper aims to address the issue of absence of process reward, which is an important problem for RL to train language models."}, "weaknesses": {"value": "1. The idea of using IRL to learn a reward function from expert demonstrations is not new, please see the reference [1,2]. What is your difference?\n\n2. The theoretical part of this paper is simple copy-paste from literature without mentioning them. For example, the claim in Theorem 1 is just soft policy. This result and also proof have been provided in literature, with continuous version provided in [3] and discrete version provided in [4]. You proof in Appendix B.1 has the same logic and is a simplification of the proof in [4]. The propositions are just simple extension of Theorem 1. I highly suggest the authors explicitly mention the reference, revise Section 3.3, and point out that the theoretical results come from the reference [3,4], otherwise, it is misleading.\n\n3. I find the definition of $p(o_t|s_t,a_t)$ in line 197 problematic. This energy model only depends on the current reward $r_\\phi(s_t,a_t)$ which does not make sense. In RL, the optimal action is the action that has the highest Q-value, i.e., that can maximize the long-term return (i.e., cumulative reward). That says, the energy model should depend on the Q-value. If it only depends on the current reward, it means that the action is myopic, i.e., it only wants to maximize the instant reward instead of the long-term return. This kind of action, in general, cannot be the optimal action, unless your reward function is a Q-function which is difficult to learn/define in practice.\n\n[1] Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment\n\n[2] From Demonstrations to Rewards: Alignment Without Explicit Human Preferences\n\n[3] Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\n\n[4] Infinite Time Horizon Maximum Causal Entropy Inverse Reinforcement Learning (the TAC version)"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KCc3QDjGzl", "forum": "FdaUyJLlzM", "replyto": "FdaUyJLlzM", "signatures": ["ICLR.cc/2026/Conference/Submission23315/Reviewer_TGya"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23315/Reviewer_TGya"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760983621602, "cdate": 1760983621602, "tmdate": 1762942601446, "mdate": 1762942601446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an inverse RL based method for learning a process reward model for LLM reasoning. The method is based on maximum entropy IRL and an importance weighting method is proposed to approximate the partition function in the gradient. The authors showed connection to various methods such as DPO, DQO, etc. Experiments are conducted on several math and coding benchmarks, showing improvements on some. In the ablations, the authors show the effectiveness of their method for best-of-N sampling using an outcome reward model and training using only the learned process reward model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The proposed importance weighting method for the IRL algorithm is potentially a nice algorithmic trick."}, "weaknesses": {"value": "* The connection with other SOTA method is a bit superfluous. The main commonalities is just that all methods are more or less based on maximum entropy RL, where the connections are widely known.\n* IRL for LLM fine tuning has actually been proposed by multiple papers now e.g. [1, 2]. Not citing or comparing with them is missing a lot of context. Especially the proposed algorithm is very similar to [2].\n* Considering the paper is algorithmic, it's missing some ablation experiments on the algorithmic design choices, for example using the importance weight vs taking more inner policy optimization steps. It is also widely known that this adversarial style IRL method can be unstable. How has that affected the proposed method, if at all?\n\n[1] [Wulfmeier, Markus, et al. \"Imitating language via scalable inverse reinforcement learning.\" Advances in Neural Information Processing Systems 37 (2024): 90714-90735.](https://arxiv.org/abs/2409.01369)\n\n[2] [Li, J., Zeng, S., Wai, H. T., Li, C., Garcia, A., & Hong, M. (2024). Getting more juice out of the sft data: Reward learning from human demonstration improves sft for llm alignment. Advances in Neural Information Processing Systems, 37, 124292-124318.](https://arxiv.org/abs/2405.17888)"}, "questions": {"value": "* For AIME2024, why all baselines preformed significantly worse than the base model? Why rePIRL only matched the base model - should it ever surpass it?\n* In Figure 1a, why should we expert PRM trained models to perform better on BON sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aGbFS7vZ4C", "forum": "FdaUyJLlzM", "replyto": "FdaUyJLlzM", "signatures": ["ICLR.cc/2026/Conference/Submission23315/Reviewer_wZzT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23315/Reviewer_wZzT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761424594269, "cdate": 1761424594269, "tmdate": 1762942601089, "mdate": 1762942601089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces rePIRL, an inverse reinforcement learning (IRL) framework designed to learn process reward models (PRMs) for LLM reasoning. The authors contend that current PRM methods are limited by strong assumptions, such as requiring expert reward functions, or by intrinsic issues like entropy collapse. rePIRL addresses this by learning a PRM from expert trajectories using a dual learning process that interchangeably updates the policy and the PRM, utilizing custom techniques to scale traditional IRL to LLMs. The paper theoretically demonstrates that this framework can unify existing online and offline PRM learning methods, arguing that it operates with minimal assumptions. The method's effectiveness is validated empirically on math and coding datasets, where it reportedly outperforms existing PRM learning approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core contribution is the application of an IRL-inspired framework to learn PRMs. This is well-motivated, as it bypasses the need for explicit token-level reward annotations or access to an expert policy for MCTS, requiring only expert trajectories.\n\n2. The paper provides a strong theoretical contribution by integrating several SOTA methods (DPO, DQO, MCTS, PRIME) into its framework as special cases that require additional assumptions. This analysis in Section 3.3 rigorously supports the central claim that rePIRL is a more general PRM learning method.\n\n3. The method is tested on seven distinct math and coding benchmarks against relevant baselines, including BC, MCTS, and PRIME .\n\n4. The ablations effectively validate the design choices. The \"PRM-only\" training experiment is particularly strong, demonstrating that the learned reward signal alone (without outcome rewards) can significantly improve performance over the base model, confirming the quality of the recovered reward function."}, "weaknesses": {"value": "1. While rePIRL achieves SOTA average performance among the tested methods, the absolute improvements are modest. For example, on the math benchmarks, rePIRL achieves a 33.5% average, compared to 31.7% for vanilla RLOO and 30.7% for MCTS. These small margins raise questions about the practical utility of the method relative to its complexity.\n\n2. The proposed dual learning algorithm is significantly more complex than the baselines. It requires simultaneously training a policy and a PRM , managing rollouts from both , and computing importance sampling weights. This complexity may present a high barrier to implementation and stable training.\n\n3. The framework's \"minimal assumptions\" claim is traded for a strong dependency on high-quality expert trajectories. The experiments relied on generating four expert demonstrations per problem using Claude-3.7-sonnet, which is a powerful, proprietary model. This reliance on an expensive and extensive set of expert data is a significant practical limitation."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KT7Sf5g0Sz", "forum": "FdaUyJLlzM", "replyto": "FdaUyJLlzM", "signatures": ["ICLR.cc/2026/Conference/Submission23315/Reviewer_tf3f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23315/Reviewer_tf3f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990040213, "cdate": 1761990040213, "tmdate": 1762942600857, "mdate": 1762942600857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces rePIRL, a novel framework for learning Process Reward Models (PRMs) in large language models (LLMs) for multi-step reasoning tasks. The approach centers on adapting Inverse Reinforcement Learning (IRL) to recover token-level reward signals from expert trajectories, without requiring step-wise annotations, preference labels, or access to the expert policy itself. The method alternates between updating a policy (via maximum entropy RL) and a reward model, employing importance sampling to render the IRL objective tractable for LLM-scale problems. Theoretically, the authors demonstrate that several existing PRM learning methods (e.g., PRIME, Math-Shepherd, DPO, DQO) can be viewed as special cases of their framework under more restrictive assumptions, thereby positioning rePIRL as a more general approach. Empirically, rePIRL outperforms baselines on both mathematical (AIME, AMC, Math-500) and coding (Leetcode, LiveCodeBench) benchmarks when fine-tuning a Qwen2.5-3B model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a substantial contribution by successfully adapting and scaling classical IRL to the challenging domain of LLM reasoning. While IRL is well-established in robotics and control, its application to LLMs is novel and non-trivial due to the enormous state and action spaces involved. The theoretical unification of existing online and offline PRM methods under a single framework is both creative and insightful, clearly underscoring the minimal-assumption nature of rePIRL. The technical execution is sound: the derivation of the learning objective—including the use of importance sampling to circumvent the intractable partition function—is rigorous, and comprehensive theoretical proofs (provided in the appendix) rigorously connect rePIRL to prior work. The experimental evaluation is thorough, spanning multiple challenging benchmarks and including ablations on policy update methods and test-time scaling. The results consistently demonstrate superior performance compared to strong baselines."}, "weaknesses": {"value": "While the paper includes several important baselines, the empirical comparison could be further strengthened by incorporating a broader array of recent PRM or preference-based methods, particularly those that similarly operate under minimal supervision. Although the paper ablates the policy learning algorithm, it offers less insight into the PRM's architecture and specific design choices. Given that the reward model is a core component, an ablation study of its capacity or architectural variations would be valuable for understanding its impact on final performance and training stability."}, "questions": {"value": "Please refer to \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dtonMLD2K9", "forum": "FdaUyJLlzM", "replyto": "FdaUyJLlzM", "signatures": ["ICLR.cc/2026/Conference/Submission23315/Reviewer_ZTdJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23315/Reviewer_ZTdJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997943847, "cdate": 1761997943847, "tmdate": 1762942600678, "mdate": 1762942600678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}