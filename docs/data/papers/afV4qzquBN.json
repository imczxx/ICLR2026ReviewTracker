{"id": "afV4qzquBN", "number": 21034, "cdate": 1758313080587, "mdate": 1759896945947, "content": {"title": "COLD-Steer: Steering Large Language Models via In-Context One-step Learning Dynamics", "abstract": "Activation steering methods enable inference-time control of large language model (LLM) behavior without retraining, but current approaches either capture suboptimally steering signals from labeled examples or require hundreds to thousands of examples to optimize using specific procedures for each behavioral target. We introduce COLD-Steer, a training-free framework that steers LLM activations by approximating the representational changes that would result from gradient descent on in-context examples. Our key insight is that the effect of fine-tuning on a small set of examples can be efficiently approximated at inference time without actual parameter updates. We formalize this through two complementary approaches: (i) a unit kernel approximation method that updates the activations directly using gradients with respect to them, normalized across examples, and (ii) a finite-difference approximation requiring only two forward passes regardless of example count. Experiments across a variety of steering tasks and benchmarks demonstrate that COLD-Steer achieves upto 95\\% steering effectiveness while using 50 times fewer samples compared to the best baseline. COLD-Steer enables real-time adaptation to new steering objectives and facilitates accommodating diverse perspectives without extensive demonstration data, which we validate through our experiments on pluralistic alignment tasks. Our framework opens new possibilities for adaptive, context-aware model control that can flexibly address varying loss-driven human preferences through principled approximation of learning dynamics rather than specialized training procedures.", "tldr": "We introduce COLD-Steer, an optimization-free, sample-efficient activation steering framework that leverages the in-Context One-step Learning Dynamics of given examples to steer LLM behavior during inference.", "keywords": ["Steerable Generation", "Large language models", "Representation Engineering", "Test-time Intervention", "Learning Dynamics"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/70c5fbb055d245b1df9f47d356d473a0047e4508.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors proposed a training-free activation steering method, termed COLD-Steer. The core idea is that in-context examples of a desired behavior implicitly define the direction of gradient descent in activation space; thus, by simulating this “one-step learning dynamic,” the model can be “steered” without retraining."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The theory bridge activation steering with learning dynamics is elegant. It unifies contrastive (CAA, DiffMean) and parameter-tuning (ReFT) perspectives under one gradient-based formulation.\n\n2. The authors performed broad evaluations which spans multiple LLMs and diverse downstream tasks (e.g., bias mitigation, hallucination reduction, refusal, sycophancy, pluralistic alignment). The COLD-Steer also shows competitive accuracy than the baselines. \n\n3. Avoids expensive backpropagation during steering with 10–50× fewer labeled examples."}, "weaknesses": {"value": "1. Limited theoretical rigor in approximations. The unit kernel assumption (κ = 1) oversimplifies eNTK behavior and may obscure causality. \n\n2. Some important details are missing. No clear separation of effects from η (steering magnitude) or layer choice; lack of sensitivity or robustness testing. \n\n3. Most results are on small/medium LLMs (7B). No evidence the method scales to larger scale-level or multi-modal models.\n\n4. COLD-Steer relies on in-context examples to approximate the “one-step learning dynamics.” This inherently depends on the number and quality of examples that can fit into the context window (ICL window). Current LLMs (e.g., Llama-2-7B-chat) have a limited token context."}, "questions": {"value": "Q1. Include layer-wise ablation is necessary, which layers yield maximal steerability vs. stability?\n\nQ2. The authors acknowledge this limitation briefly (“future work should develop more sophisticated approximations of the neural tangent kernel”) but provide no empirical study on how κ or layer l affect the approximation quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kEDQZPqu7r", "forum": "afV4qzquBN", "replyto": "afV4qzquBN", "signatures": ["ICLR.cc/2026/Conference/Submission21034/Reviewer_QoRx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21034/Reviewer_QoRx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914086954, "cdate": 1761914086954, "tmdate": 1762940615413, "mdate": 1762940615413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents COLD-Steer, a training-free and sample-efficient method for steering large language models at inference time. It approximates how model representations would change after a single gradient update on a few in-context examples, enabling behavioral control without retraining. Two variants are proposed: COLD-Kernel-Steer, which aggregates gradient signals using a simple kernel, and COLD-FD-Steer, which uses a finite-difference approximation requiring two forward passes. Experiments on CAA, BiPO, and OpinionsQA show that COLD-Steer achieves similar or better control than prior methods with 10–50 times fewer examples."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Interesting idea of approximating learning dynamics to perform activation steering.\n- Training-free and efficient compared to fine-tuning or parameter-tuning approaches.\n- Works with few examples and across different LLM families.\n- Strong empirical results on several behavioral control tasks.\n- Both variants are complementary, with COLD-FD providing more consistent results than COLD-Kernel, though at the expense of computational efficiency."}, "weaknesses": {"value": "- Theoretical justification of approximations (unit kernel, finite difference) is limited.\n- While examples of COLD-steered generations are given and discussed, the paper could benefit from more interpretability analysis of how activations are actually changed."}, "questions": {"value": "- How sensitive is COLD-Steer to the choice of steering layer and the η multiplier?\n- Could kernel approximations beyond the unit kernel improve stability without major cost?\n- COLD-FD reduces memory use by clipping small parameter updates, keeping only about 4% of parameters with significant changes. Could you provide more detail on how the clipping $\\theta_\\text{thresh}$ threshold is chosen and how it affects steering performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pqAVkimCt9", "forum": "afV4qzquBN", "replyto": "afV4qzquBN", "signatures": ["ICLR.cc/2026/Conference/Submission21034/Reviewer_wgtt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21034/Reviewer_wgtt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918469458, "cdate": 1761918469458, "tmdate": 1762940614163, "mdate": 1762940614163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes COLD-Steer, a framework for steering large language model (LLM) activations by approximating the representational changes that would result from gradient descent on in-context examples. It introduces two variants:\n- COLD-Kernel Steer: Uses kernel approximation to estimate gradients.\n- COLD-FD Steer: Employs finite-difference approximation to estimate gradients.\nThe approach is claimed to be training-free, data-efficient, and unifying existing contrastive steering methods. Additionally, it is presented as being applicable across a diverse set of steering tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong theoretical motivation and a unifying perspective that generalizes existing methods. It would be valuable to further elaborate on the connections to other approaches such as [1,2,3].\n- Includes computational complexity analysis, but a more explicit comparison with the complexity of existing methods would strengthen the contribution.\n- Extensive experimental setup, covering selection and open-generation tasks, distribution shifts, computational efficiency, and qualitative outputs.\n- Compares against a broad range of baselines, demonstrating the method’s effectiveness across diverse scenarios.\n\n[1] Refusal in Language Models Is Mediated by a Single Direction\n[2] Controlling Language and Diffusion Models by Transporting Activations\n[3] Angular Steering: Behavior Control via Rotation in Activation Space"}, "weaknesses": {"value": "See Questions"}, "questions": {"value": "- Line 40: claims existing methods use between 250 to 1000 examples, but [1] uses as few as 64. This counterexample should be addressed.\n- Figure 1 (left): Why does the contrastive method significantly decrease in accuracy as the number of samples increases? Which experiments demonstrate this phenomenon?\n- Lines 191–193: claims that using a unit kernel yields strong empirical performance. Some discussion to explain this observation would be helpful.\n- Table 2: COLD-FD performs much better than ReFT(mlp), which is a more complex method. What explains this? COLD-FD only approximates the gradient, whereas ReFT performs actual gradient descent.\n- Table 2 (top) and Table 5: COLD-Kernel is a generalization of DiffMean/CAA ([2]), but performs worse. Why is this the case?\n- Table 2 should include an average (avg) column for easier comparison across methods.\n- More evaluation on different LLM families and sizes is needed. Table 3 only shows results for Gemma 2 9B and Mistral 7B on the selection task. Other tasks lack cross-model comparison. Including a more diverse set of sizes would better demonstrate generalization.\n- Lines 306–307: \"Steering is applied to all prompt token representations (rather than the final token only), which yields consistently better performance.\" Does this mean steering is applied to the input token representations? If so, is it applied sequentially during steering vector computation as in [3]? Does this setup apply to both selection and open-ended tasks?\n- Line 309: is η the same across all methods? Do all methods perform best at η = 1? Please provide full grid search results for all methods.\n- Lines 311–312: \"For open-ended generation, we intervene only at the first generated token to guide continuation, while limiting the compounding effects of steering.\" Is this strategy used for the proposed methods only or all baselines? Many existing methods steer on all generated tokens [1,2,3,4]. An empirical comparison would help justify this decision.\n- Table 4: lacks comparison to other methods. Please include baseline results.\n- The paper lacks robustness evaluation: does the method inadvertently affect untargeted behaviors while steering?\n\nReferences:\n[1] Refusal in Language Models Is Mediated by a Single Direction  \n[2] Steering Llama 2 via Contrastive Activation Addition  \n[3] Controlling Language and Diffusion Models by Transporting Activations  \n[4] Angular Steering: Behavior Control via Rotation in Activation Space"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QBdWc1BGBi", "forum": "afV4qzquBN", "replyto": "afV4qzquBN", "signatures": ["ICLR.cc/2026/Conference/Submission21034/Reviewer_toPp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21034/Reviewer_toPp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989182935, "cdate": 1761989182935, "tmdate": 1762940613695, "mdate": 1762940613695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tries to fill the gap between in-context learning and parameter efficient tuning by approximating the change in the representations when finetuning on in-context examples at inference time. They propose two training-free approaches to achieve this goal, a unit kernel approximation method and a finite-difference approximation method. The proposed methods are tested on multiple choice data as well as open-ended generations. They are able to achieve up to 95% performance with 10-50 times fewer examples compared to several steering and parameter efficient tuning approaches. In addition, the proposed approaches do not require pairs of positive and negative examples in contrast to other activation engineering approaches such as Contrastive Activation Addition (CAA)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper tackles an important and interesting problem.\n\n- The work is grounded in prior literature and does a good job telling a coherent and concise story.\n\n - The proposed approaches are theoretically grounded.\n\n - There are several in-depth experiments that evaluate the proposed approaches in terms of effectiveness, generation quality, behavioral shift quality, and efficiency."}, "weaknesses": {"value": "The evidence for the effectiveness of the kernel based approach is lacking. According to Figure 3, COLD-kernel approach doesn’t seem to be very effective on several tasks. Section 4.4 (maintaining pluralistic views) seems to be an afterthought to hide this weakness, but it seems a different task than shifting behavior, which is the main claimed goal of the paper."}, "questions": {"value": "How important are the number of examples in the quality of approximations? It seems that for certain tasks, the number of examples does not influence the results while for others the difference in accuracy is significant and sometimes more examples even hurts performance. What are your intuitions?\n\nDoes “Base” in Table 3, 4, and 6 refer to no in context examples and no training? That seems to be the weakest baseline across all the methods you have considered as baseline. Why not compare it to DiffMean or ReFT?\n\nCan you provide more intuition about why the kernel based steering preserves subgroup distributional properties better than the finite difference method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aJhhWbLAKf", "forum": "afV4qzquBN", "replyto": "afV4qzquBN", "signatures": ["ICLR.cc/2026/Conference/Submission21034/Reviewer_xSG1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21034/Reviewer_xSG1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762075778688, "cdate": 1762075778688, "tmdate": 1762940612848, "mdate": 1762940612848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}