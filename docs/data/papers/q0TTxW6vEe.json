{"id": "q0TTxW6vEe", "number": 6503, "cdate": 1757987291747, "mdate": 1759897910791, "content": {"title": "Sample-Efficient Pruning Model Selection via Lasso", "abstract": "We study the problem of selecting a pruned neural network from a set of candidates generated by various pruning methods. The goal of a learner is to identify a near-optimal model that achieves low generalization error. Although model selection techniques such as cross-validation are widely used in practice, they often fail to provide guarantees on generalization error or offer only asymptotic guarantees. To address these limitations, we propose an algorithm that jointly selects a pruned network and updates its parameters using an $L_1$-regularization, thereby encouraging sparsity while ensuring low generalization error. For a given error tolerance $\\epsilon$, we establish a sample complexity lower bound of $\\Omega\\left(\\frac{1}{\\epsilon^2} \\log M\\right)$, where $M$ is the number of candidate models, demonstrating that our algorithm remains sample-efficient even when the candidate pool is large. Extensive numerical experiments confirm both the practical effectiveness and the theoretical guarantees of the proposed method.", "tldr": "We propose a novel elimination-based algorithm that identifies a near-optimal pruned network from a pool of pruned neural networks.", "keywords": ["model selection", "neural network pruning", "generalization error", "sample complexity"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61a6e43c36d5c7b6a6c15ea9eeb974ef06f56df5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes L1Sel, a sample-efficient model selection framework for pruning neural networks under the PAC learning framework. The method combines an elimination-based procedure with L1-regularized selection to identify the best-performing pruned model from a set of candidates using limited data. The authors further introduce L1Sel+, which extends the approach with backward elimination to enhance sparsity by pruning redundant connections in the final layer. Theoretical results establish finite-sample guarantees, and experiments across classification and regression tasks demonstrate improved generalization and efficiency over existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper is built on a clear theoretical foundation. It formulates the pruning model selection problem within the PAC learning framework and provides formal finite-sample guarantees. The theoretical grounding is relatively new in pruning research and offers a valuable perspective for understanding model compression from a generalization viewpoint.\n\n(2) The experimental results are comprehensive and consistent. The method is validated across multiple classification and regression tasks, showing improvements in generalization and sparsity over several recent baselines. These results demonstrate the robustness and versatility of the proposed approach."}, "weaknesses": {"value": "(1) Some abbreviations and technical terms appear without clear definitions or appropriate references upon first use, which may hinder readability for non-specialist readers. A more careful and consistent presentation of notation and terminology would improve clarity.\n\n(2) The L1Sel+ algorithm described in Section 4.4 appears conceptually similar to, or potentially covered by, the prior work DepGraph (Fang et al., 2023). The resemblance between their redundant-parameter elimination mechanisms raises concerns about the novelty and distinct contribution of this component.\n\nReference:\n[1] Fang, Gongfan, et al. DepGraph: Towards Any Structural Pruning. CVPR 2023."}, "questions": {"value": "(1) Could the authors elaborate on the necessity of introducing L1Sel+? If the pruned models can still be further sparsified after being selected by L1Sel, this may indicate that the initial pruning algorithms were not fully effective. In particular, when more advanced parameter grouping techniques such as DepGraph are employed, redundant parameter groups should have already been avoided. The additional backward-elimination step in Sec. 4.4 therefore appears somewhat unimportant, serving mainly as an extra safeguard rather than one of the core contributions.\n\n(2) Could the authors provide more comprehensive ablation studies? It would be valuable to examine how different design choices affect the performance of L1Sel, for example, by integrating different base pruning algorithms, by varying the resource or budget allocation strategy, or by using alternative estimators or elimination criteria. Such analyses would help clarify the robustness and general applicability of the proposed framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WnOAfW4dK4", "forum": "q0TTxW6vEe", "replyto": "q0TTxW6vEe", "signatures": ["ICLR.cc/2026/Conference/Submission6503/Reviewer_6WXA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6503/Reviewer_6WXA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761830724, "cdate": 1761761830724, "tmdate": 1762918871729, "mdate": 1762918871729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript tackles model selection problem among many pruned nets. Instead of the usual cross-validation hand-waving, they formalize selection via a new metric defined as inter-model excess risk $\\mu(m, \\hat{\\mathbf{w}}\\_m^n)-\\mu(m^\\*, \\mathbf{w}\\_{m^\\*}^\\*)$. They proposed L1Sel, an elimination-based procedure: iteratively throw out clearly sub-optimal candidates using confidence bounds, then, on the survivors, refit only the last layer with L1 (classic Lasso) and pick the one with the smallest regularized empirical risk. They prove finite-sample PAC results: to get inter-model excess risk $\\leq \\varepsilon$ with prob $\\geq 1-\\delta$, regression needs $\\tilde{\\Omega}((1 / \\varepsilon^2) \\log (d M))$ under standard boundedness/sub-Gaussian/compatibility assumptions; classification needs $\\tilde{\\Omega}((1 / \\varepsilon^2) \\log M)$ (ignore log d). Some numerical comparisons are reported on the MNIST/CIFAR datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-  The studied problem setting is important in practice. Against cross‑validation, they get similar or slightly better generalization error and lower runtime on both MNIST/MLPNet and CIFAR‑10/Resnet‑20. The proposed method increases last‑layer sparsity without hurting accuracy (Table 2)\n\n\n\n- The presentation of the paper has improved compared to the version I reviewed previously. The construction of events, oracle inequality, and the final error decomposition are standard analysis frameworks and correctly chained in appendix, yielding a rate that recovers textbook Lasso behavior when $M=1$"}, "weaknesses": {"value": "- The work presents inter‑model excess risk as *newly introduced* (p. 2-3), but it is just the cross‑class analogue of excess risk; i.e., *regret* to the best model in the pool. It’s perfectly fine to use, but it should not be claimed as pure conceptual novelty towards communities\n\n- Bounded labels $y$ (assumption 1), sub-Gaussian noise defined w.r.t. each model's true parameter $\\theta_m^*$ (assumption 2), and compatibility (assumption 3) are standard in Lasso theory for a single model, but simultaneously enforcing them across $M$ heterogeneous, pruned neural feature maps is a big ask in my view; the paper gives no discussion of when this is remotely realistic for CNN/ResNet features \n\n- The work assumes a positive global lower bound on the Fisher factor's smallest eigenvalue along the line segment between $\\Theta^{*}$ and $\\Theta$. That rules out near-saturated logits and is not defensible for general pruned nets. Would you consider replacing with a e.g. restricted strong convexity or local curvature condition under a margin/overlap assumption, and show how $\\kappa_0$ depends on the data distribution?\n\n\n\n\n\n\nIdentified some typos that not affect the rating,\n\n- p.3 \"revisit the some definitions\", remove the\n\n- p.6 \"at sparsity rate of\", use rates\n\n- p. 22 and 33, \"Lemam\" should be Lemma\n\n- p.35, \"Combininig\""}, "questions": {"value": "- The comparison to CV (Table 1) ignores nested CV and says CV can favor models that overfit folds, which is true only when one deploy naive CV for selection; the standard remedy could be e.g. nested CV\n\n- On the speed advantage, we don't know whether you warm‑start Lasso across epochs, which could have impacted runtimes"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ABKFHLkUjo", "forum": "q0TTxW6vEe", "replyto": "q0TTxW6vEe", "signatures": ["ICLR.cc/2026/Conference/Submission6503/Reviewer_6hQw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6503/Reviewer_6hQw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800856015, "cdate": 1761800856015, "tmdate": 1762918871367, "mdate": 1762918871367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a model selection algorithm that also has pruning effect on the nodes of the last layer and weights directly leading to them. The algorithm, named L1Se1, is a two-stage method: the first is a model selection method with a bound on minimizing ``inter-model excess risk'', and the second step is a lasso on the last layer with removing penultimate weights leading into linear terms removed by lasso. The selection method has a PAC-learning style probabilistic bound on sample efficiency to minimize the inter-model excess risk. Empirical validations provide proof-of-concept workability of this method."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Overall, this manuscript is a very readable composition, especially with sufficient explanation of preliminary material in section 2 for the readers without adequate exposure to bounding risk in probabilistic manner. This leads nicely to the point where the authors propose a new metric of ``inter-model excess risk''. The design of the new metric works nicely with the subsequent PAC-style high-probability bounds. This, along with detailed proofs in the appendix, makes the theoretical sections of this work readable to broader audience. \n\nThe theoretical setting for deriving sample complexity bounds based on empirical processes, confidence bounds and regret-like objectives give solid foundations, as they are standard approaches enjoyed for decades in learning theory and reinforcement learning literatures."}, "weaknesses": {"value": "The theoretical foundation, despite its solidness, shows age --- the bound-derivation framework itself may be the cause of this work's research direction. In my opinion, this is what constricts the impact of this work, by splitting the whole problem of neural network pruning into the (feature vector set) model selection (from a given candidate pool of potentially pruned models) and the sparse linear model fitting by lasso. \n\nIn contrast to the constricted range, the contribution statements of this work, especially in the abstract and the introduction, is misleadingly broad (compared to what is stated in conclusion). For example, 'we propose a novel pruning model selection framework that identifies a near-optimal pruned neural network having low generalization error' can be read in a logically incorrect manner. Also, empirical results, such as the pruning rates, sparsity rates, and accuracies are presented in potentially misleading viewpoints. Additionally, the newly presented metric seems to have some weaknesses to be translated to practical impact in model pruning. I will specify more of these via questions below."}, "questions": {"value": "As far as I understand, this manuscript contains a method to first select a model with high probability bounds using the inter-model excess risk whose definition and computation relies on a given candidate pool (in elimination step of Alg. 1) and then perform lasso on the last layer weights only, followed by pruning the weights leading into the lasso-sparsified last layer nodes (in 'selection' step of Alg. 1). \n\nMy first question is on the practical meaning of theoretical correctness from the elimination step. The probabilistic correctness of the elimination step seems to depend on not only the sample size $N$ but also other factors, such as containing good enough candidates in the initial model pool. Empirical validations were made on 100 models in the pool. Definition of inter-model excess risk seems to pick the best-among-in-the-given-pool. How would this translate to actual application of the proposed method (e.g. model pruning)?\n\nSecond question extends from this perspective, on the meaning of the important terms 'sparsity rates' and 'pruning rates' used throughout the experiment section. For example, what kind of models are generated when '[you] generate\n20 randomly pruned classification models at sparsity rates of 50%, 60%, 70%, 80%, and 90%'? Are these models generated by other methods to fill in the candidate pool? On the other hand, what are the 'pruning rate' in Tables 2 and 3? How are the rates relevant to benchmark baselines as well as the proposed algorithm?\n\nThird question is on the scope of sample-efficiency in the proposed framework? I think this is a serious question as the title starts as 'Sample efficient pruning model selection [...]'. There seems to be two bounds -- one on the risk itself, polynomial in the training sample size, and one on the sample complexity of algorithm logarithmic on model pool size. There seem to be lots of practically important terms impacting the bound, for example, the max epoch count $H$ and corresponding schedule of regularization parameters and confidence radii. Is there any prescient goldilocks requirements on setting those tunable parameters for this method to enjoy practical benefits? Additionally, I am afraid that the title and contribution statements overstate/mislead as if the pruning aspect contributing to sample efficiency. It seems to me that this algorithm uses lasso to 1) create sparse model in the last layer with the goal of making some weights in penultimate layer ineffective (hence limiting pruning targets up to penultimate layers), and 2) increase performance by sparse model fitting which is direct effect of lasso as well.\n\nLast question: what are the limitation of the proposed framework as a pruning method? One aspect I noticed is that its native pruning capacity is on the last layer only (which includes incoming edges to the last layer nodes, applicable to both Alg. 1 and Alg. 2). The proposed method may be used to have broader model-wide by using other pruning methods to provide candidate pools (or use randomly pruned models as presented in the empirical validations), but this does not mean the proposed method may come short of becoming an effective standalone pruning method, especially when the target models are models with depth larger than 2 or 3. Also, I wonder using 100 random pruned models as a candidate pool and the selected benchmark pruning methods show useful/valid comparisons. Potential of pruning method should also contain how well it prunes a whole model from scratch, rather than sparse-fitting given (semi-pruned) models to boost performance and pluck more last-layer incoming weights and gain boost in pruning ratio -- the results in Table 2 look very biased."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8EjvvBbdjT", "forum": "q0TTxW6vEe", "replyto": "q0TTxW6vEe", "signatures": ["ICLR.cc/2026/Conference/Submission6503/Reviewer_G9WW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6503/Reviewer_G9WW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973458522, "cdate": 1761973458522, "tmdate": 1762918870805, "mdate": 1762918870805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of selecting and fine-tuning one model from a pool of pretrained and pruned neural networks. Each candidate model is treated as a fixed pruned backbone with a trainable linear head. The authors define an inter-model excess risk to formalize how far a selected model's generalization error is from that of the best candidate, and they propose an elimination-based algorithm, L1Sel, which uses L1-regularized ERM to iteratively discard suboptimal models. The paper presents theoretical guarantees on the sample complexity of achieving a target inter-model excess risk, with dependence logarithmic in the number of candidate models, and presents empirical results on several small-scale datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The formulation of model selection across a pool of pruned networks as a statistical learning problem is conceptually clean. The inter-model excess risk provides a coherent metric for comparing generalization performance across candidates. Based on this theoretical framework, the authors provide finite-sample bounds and a sample complexity result that scales only logarithmically with the number of models.\n\nThe proposed method L1Sel combined with L1 regularization is straightforward simple and a scalable approach to Neural Network model selection is an interesting direction."}, "weaknesses": {"value": "The motivation and experimental setup do not align with realistic pruning and fine-tuning pipelines. The paper assumes a scenario where a large pool of pretrained and pruned networks are selected by training only a linear head—which is not how pruning is typically used. In practice, obtaining a pruned model is typically a two-step process: we start from dense pretrained models and perform pruning as part of fine-tuning, rather than choosing among already-pruned ones. Moreover, \"fine-tuning\" here reduces to linear probing, which is known to be inferior to full-model fine-tuning in most realistic cases. Also in the context of linear probing, it is uncommon to prune the last linear layer, which further questions the relevance of this setup.\n\nAlmost all theoretical content is deferred to the appendix, and the main body only contains one theorem, which is for regression, even though most experiments are conducted on classification tasks. As a result, it is difficult to assess what is actually novel beyond standard Lasso model-selection analysis. If the main contribution is theoretical, it should be articulated clearly in the paper body, including a discussion of how it extends or differs from known results.\n\nThe experiments are conducted on relatively simple datasets (MNIST, CIFAR-10, small regression tasks) with small models, where most approaches already perform well. These settings do not convincingly demonstrate the value of the proposed method. More challenging datasets and larger, modern architectures are needed to make the empirical case. Furthermore, the paper uses random pruning to generate model pools. This produces a wide range of model qualities, making the selection problem artificially easy. The paper should evaluate on pools of realistic, high-quality pruned models obtained by state-of-the-art structured pruning techniques to assess whether L1Sel provides any meaningful advantage. Baselines such as CV-5, CV-10, TS, PARC, and SFDA are only listed in tables but not explained in the main text. Readers unfamiliar with these methods cannot interpret what they do or why they are relevant comparators. In general, the related work section has to be discussed in the main text."}, "questions": {"value": "- Can you provide a clear high-level description of your main theorem in the main text and explain how it differs from standard Lasso excess risk bounds?\n\n- How sensitive is the algorithm to hyperparameters such as the epoch schedule, $\\lambda$, and $\\alpha$? Any heuristics or tuning strategies?\n\n- Can you evaluate L1Sel on realistic candidate pools from actual structured pruning methods or larger pretrained models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m2FDdRkhoj", "forum": "q0TTxW6vEe", "replyto": "q0TTxW6vEe", "signatures": ["ICLR.cc/2026/Conference/Submission6503/Reviewer_oCNt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6503/Reviewer_oCNt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997643994, "cdate": 1761997643994, "tmdate": 1762918869971, "mdate": 1762918869971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}