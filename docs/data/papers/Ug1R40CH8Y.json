{"id": "Ug1R40CH8Y", "number": 6659, "cdate": 1757991497024, "mdate": 1759897902424, "content": {"title": "Reinforcement Learning for Generalized Label Aggregation", "abstract": "The rise of large language models (LLMs) as annotators has introduced new opportunities and challenges for label aggregation in data annotation pipelines. While traditional aggregation methods are designed for human crowd workers with independent judgments, they fall short when applied to LLM-generated annotations that exhibit high correlation patterns and provide rich explanatory justifications. To address these challenges, we introduce RFAgg, a reinforcement learning framework that dynamically aggregates LLM annotations by jointly modeling both labels and their corresponding justifications. To train RFAgg, we construct the AGG dataset by collecting question-answer pairs generated by different LLMs across various datasets. Then, RFAgg first uses LLMs to generate multiple aggregation responses containing reasoning tokens and final answers for each input, and then uses our proposed aggregation reward functions to update the model via the policy optimization algorithm.  Experiments demonstrate that RFAgg significantly outperforms classical and recent aggregation methods. Most notably, it serves as a general aggregation model, generalizing well to out-of-domain and previously unseen tasks. Despite being trained only on limited classification tasks, RFAgg achieves an average improvement of 2.45\\% on diverse objective tasks and 5.2\\% on the Alpaca 2.0 subjective task compared to its base model. We will publicly release the AGG dataset and our source code.", "tldr": "We train a language model to expertly aggregate conflicting labels and justifications from other LLMs, creating a highly accurate and general-purpose aggregator.", "keywords": ["Label Aggregation", "LLM", "Data Annotation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80c80798a4db7451c9438914f27e85584e8d19fb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel RL-based aggregation of LLM-generated data annotations. The authors constructed a dataset containing annotations and explanations using Qwen2.5-7B. Then, the authors used a recently developed RL framework, GRPO, to obtain an effective aggregator. The main focus of the authors was the development of the RL reward functions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The research problem is practically interesting. I agree with the authors that the LLM-generated annotations are different from human-generated ones, and therefore, it is necessary to develop more effective aggregators.\n2. The RL model of the aggregation problem is interesting and promising.\n3. The authors constructed a new dataset for the research. The dataset contains well-known and widely-used previous datasets."}, "weaknesses": {"value": "1. The problem setting is to aggregate annotations from different LLMs or one LLM with different personalization (Sec. 3.1). However, the dataset AGG was constructed using Qwen2.5-7B only. Although the authors mentioned a “highly scalable strategy” which was “significantly more practical”, I don’t see how that is true.\n\n2. The reward functions are not well-explained. For example, I don’t see how the format reward and accuracy reward are calculated.\n\n3. The authors decided to use GRPO as the reinforcement learning framework without clear reasons. Although they have mentioned stability, sample efficiency, etc., the argument is quite high-level and unconvincing. The details of the GRPO framework (Eq. 4) are not explained either. \n\n4. The concept of “difficulty of an instance” is not intuitive. I don’t understand why the number of available annotations reflects difficulty. For example, an instance may have a large number of highly consistent annotations. I don’t think this case is difficult.\n\n5. In the experiments, the authors used only the Qwen2.5-7B model. Considering the fact that the dataset AGG is generated using exactly the same model, I’m curious what would happen if the annotator LLMs are diverse and the aggregator LLM is different."}, "questions": {"value": "See detailed comments (especially the ones in weaknesses)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zhjbv6lnlf", "forum": "Ug1R40CH8Y", "replyto": "Ug1R40CH8Y", "signatures": ["ICLR.cc/2026/Conference/Submission6659/Reviewer_EUzf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6659/Reviewer_EUzf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914441277, "cdate": 1761914441277, "tmdate": 1762918969819, "mdate": 1762918969819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RFAGG, a RL framework for aggregating annotations generated by LLMs. Unlike traditional aggregation methods that assume independent human annotators, RFAGG explicitly models both the labels and their accompanying justifications produced by LLMs.\n\nExperiments on AGG and several out-of-domain datasets show that RFAGG substantially outperforms traditional aggregation baselines and untuned LLM aggregators. The framework achieves gains of up to 5.2% on AlpacaEval 2."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Reformulates label aggregation as a reinforcement learning problem, going beyond classical probabilistic or consensus-based formulations.\n\n2. The proposed AGG dataset systematically covers multiple modalities and annotation types, providing a valuable benchmark.\n\n3. The appendix includes prompts, data splits, and ablations that make replication feasible."}, "weaknesses": {"value": "1. Outdated baselines, mostly pre-LLM (≤ 2019).\nThe experimental comparisons in Table 1 include only traditional label aggregation methods such as Majority Voting, Dawid–Skene (1979), CATD (2014), and several Bayesian variants from 2019 (BWA, IBCC, EBCC). These baselines represent the pre-LLM era of aggregation research. Since 2020, many new approaches have emerged that leverage LLM capabilities — e.g., self-consistency, debate-style or multi-agent aggregation, explanation-aware adjudication, and LLM-as-judge verification frameworks — which have substantially improved benchmark performance. The paper does not discuss or compare against any of these post-LLM aggregation methods, leaving its empirical positioning incomplete.\n\n2. Table 1 compares only against a basic LLM (Qwen2.5-7B).\nThe only LLM baseline considered is the untuned Qwen2.5-7B-Instruct, which serves both as annotator and aggregator. No stronger or specialized LLMs are included. Consequently, it is unclear whether the reported gains hold for modern high-performing LLMs or under diverse architectures.\n\n3. Limited discussion of recent LLM-alignment and reward-learning baselines.\nGiven that RFAGG uses reinforcement learning (GRPO) for aggregation, comparisons with newer alignment-oriented techniques — such as Direct Preference Optimization (DPO), Reinforcement Learning from AI Feedback (RLAF), or explanation-aware SFT — would strengthen the methodological justification. Without these, the necessity of using RL over simpler preference-based objectives remains partly unsubstantiated."}, "questions": {"value": "The paper evaluates on widely used benchmarks such as AGNews, SST2, and MMLU. While these are well-established, their difficulty and ceiling performance have shifted dramatically with modern LLMs. For instance, models on MMLU-Pro already reach around 93%, whereas RFAGG reports only 78% on MMLU (Table 1).\n\nCould the authors clarify why they chose to evaluate only on the original MMLU instead of more challenging or recent variants such as MMLU-Pro, MMLU-STEM, or DeepBench?\n\nHow should we interpret the 78% figure in a landscape where even base LLMs far exceed that on newer leaderboards?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wFL6nyAGa6", "forum": "Ug1R40CH8Y", "replyto": "Ug1R40CH8Y", "signatures": ["ICLR.cc/2026/Conference/Submission6659/Reviewer_3jDf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6659/Reviewer_3jDf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971223576, "cdate": 1761971223576, "tmdate": 1762918969492, "mdate": 1762918969492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an RL framework (RFAGG) for LLM annotations aggregator. The aggregator conditions on the input plus K annotations (label + justification) and generates a reasoned aggregation. Rewards combine format correctness, answer accuracy, and an entropy-weighted bonus for high-disagreement items; optimization uses GRPO with a curriculum over annotation count. The authors construct an AGG dataset (mostly text classification/multiple-choice; some reasoning and a vision-inspection suite) using Qwen-2.5 variants to simulate diverse annotators. RFAGG outperforms majority vote and probabilistic crowd models on in-domain tests and shows generalization to other domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Clear problem framing: aggregating correlated LLM “votes” with usable rationales, not just labels.\n+ Reward design is intuitive and targeted at the true goal (accuracy + conflict resolution), not just imitation.\n+ Simple, plausible training recipe (GRPO + curriculum over #annotations).\n+ Broad empirical sweep with consistent gains over MV/DS-style baselines; nice to see open-ended (AlpacaEval 2) and a vision domain.\n+ Ablations support the components (RL > SFT, reward shaping, curriculum)."}, "weaknesses": {"value": "- Reward details under-specified. The entropy threshold τ, reward scaling, and sensitivity analyses are not clarified. It would be good to see how performance varies with τ and with different ways to compute annotation entropy when labels are open-ended.\n- The LLM-aggregator baseline is just “untuned Qwen-2.5-7B,” which might be too weak. Using a stronger (e.g. a larger proprietary model) / reasoning-optimized model or a stronger straw-man that explicitly mirrors the three reward factors (format compliance, answer accuracy, and disagreement/entropy handling) would add confidence.\n- Compute / cost / stability. No training-time or sampling-cost details (e.g., group size for GRPO, #rollouts) are given here; reproducibility depends on those knobs."}, "questions": {"value": "Apart from my concerns in the weaknesses:\n- How is τ chosen in the entropy bonus, and how sensitive are results to τ and reward scaling? Are there any per-task tuning?\n- In the curriculum, what are typical kmin/kcurr and how many steps per stage? Does performance keep improving as K (number of annotations) grows, or does it saturate?\n- Does RFAGG assume binary labels? How does it handle multi-class or ordinal labels? Are richer, non-binary inputs (e.g., annotator probabilities/confidence) expected to improve performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5HEzoGxEAK", "forum": "Ug1R40CH8Y", "replyto": "Ug1R40CH8Y", "signatures": ["ICLR.cc/2026/Conference/Submission6659/Reviewer_ZfEw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6659/Reviewer_ZfEw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989201408, "cdate": 1761989201408, "tmdate": 1762918968977, "mdate": 1762918968977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an LLM based method for label aggregation, i.e., creating a label for an input which has been labeled/annotated by multiple LLMs. The approach is based on training an LLM using RL to jointly model the label as well as the justification, based on those provided by the annotator LLMs. For this, the paper proposes a reward function which has components addressing the aggregation format, accuracy and complexity. The paper combines an existing RL objective (GRPO), with a curriculum training on those inputs with less number of annotations (and hence simpler) to be aggregated. The paper also introduces an AGG datasets whose training dataset is constructed from eight well known benchmark datasets and the test data combines 5 other datasets covering a broader spectrum of tasks. The annotator pool was simulated in the experiments using different personalities of prompts to one of two specific models. The paper claims significant improvements of their method over previous non-LLM label aggregation methods, and also using general purpose LLMs for aggregation.\n\nI wish to clarify that I have previously reviewed this paper, and while there are some improvements from the previous version, the paper has not been significantly changed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles the important problem of label aggregation using general purpose LLMs which are increasingly being used for specialized computational and modeling tasks. The RL and curriculum based method is potentially useful."}, "weaknesses": {"value": "1. The exposition of the proposed method in the paper is somewhat lacking in detail and clarity: the various rewards (e.g. format reward and accuracy reward)  are not defined formally, and it is not clear how they are obtained (is an LLM used?). While Appendix  C has a description of the MDP, there is no reference to this appendix from the main paper.\n2. The overall approach uses an existing RL training (GRPO) with some fairly natural rewards and a curriculum. No analytical analysis or justification is provided.\n3. Further, from the ablations in Fig. 3, it seems that removing either the curriculum or complex rewards does not diminish the performance significantly."}, "questions": {"value": "1. Missing references from the main paper to relevant appendices.\n2. Use parenthesized citations in Section 2, and on line 55.\n3. The results in Table 3 seem implausible - all seven baselines have exactly the same performance on ARC."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5kEUKHhA2w", "forum": "Ug1R40CH8Y", "replyto": "Ug1R40CH8Y", "signatures": ["ICLR.cc/2026/Conference/Submission6659/Reviewer_udxn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6659/Reviewer_udxn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762349272173, "cdate": 1762349272173, "tmdate": 1762918968481, "mdate": 1762918968481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}