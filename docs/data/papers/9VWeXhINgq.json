{"id": "9VWeXhINgq", "number": 22398, "cdate": 1758330552779, "mdate": 1762945582605, "content": {"title": "ShreddingNet: Coarse-to-Fine Restoration for Multi-Source Shredded Manuscripts", "abstract": "As an important research task of human cultural heritage, the restoration of artworks and calligraphy is of great significance. Seldom existing works have taken the multi-source (*i.e.*, fragments are not ensured to be from the same piece of artworks) fragment-oriented restoration task into account.In this paper, we introduce a restoration algorithm for shredded artworks based on a coarse-to-fine two-stage pipeline.This is an algorithm that can handle the multi-source shredded artworks restoration problem without any restrictive conditions or strong assumptions,and it admits a linear time complexity and robustness to stains, mold, and contour defects. In the proposed coarse matching stage, the algorithm compares the features of each fragment, generating candidate matching pairs. Although a significant number of erroneous matching pairs persist in the candidate set, erroneous matches between fragments from different source images are often rare, enabling high-accuracy clustering of fragments belonging to the same image.In the introduced fine-grained matching stage, the algorithm filters out erroneous matching pairs from the candidate set, producing more precise final matching pairs for global assembly.Experiments conducted on more than 4,000 images from two datasets demonstrate the average reconstruction F1-score achieves 98.37%, which is 5.72% higher than the current state-of-the-art method, confirming the method’s effectiveness and robustness.Source code is available in the supplementary material.", "tldr": "This work designs a neural network model for multi-source manuscript restoration, employing a coarse-to-fine pipeline to reduce the time complexity, measured by the number of model invocations, to linear.", "keywords": ["manuscript restoration; coarse-to-fine pipeline; computer vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/cb52c9610c44ce7b9a4b7333c86592075a700793.pdf", "supplementary_material": "/attachment/a6cbfc345d17fc0484c30102971c3d296e03b77e.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents ShreddingNet, a coarse-to-fine pipeline for restoring multi-source shredded manuscripts. A coarse stage extracts texture/contour features per fragment, fuses them via cross-attention, and keeps Top-K candidate neighbors; label-propagation clusters fragments by source, assuming cross-source false matches are rare. A fine stage performs cross-attention at contour level and uses a lightweight CNN scorer plus RANSAC to validate matches, followed by MST-style global assembly. On large synthetic datasets (including degraded cases), the method reports strong precision/recall and claims near-linear scaling by constraining candidate pairs. The contribution is a clean integration of dual-modality features, graph clustering, and attentive matching tailored to multi-source reconstruction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Pros:\n(1) well-motivated task (multi-source) with a clear, modular design; \n(2) solid headline gains over classic baselines; \n(3) ablations indicate both clustering and CNN scoring matter; \n(4) pragmatic assembly strategy."}, "weaknesses": {"value": "Cons:\n(1) external validity,evidence is largely synthetic; real-world variability (aging, illumination, non-rigid tears) is under-tested; \n(2) “linear time” is argued via candidate scaling, not end-to-end runtime/memory curves across fragment counts; \n(3) the rarity of cross-source false matches needs quantitative stress-tests;\n (4) comparative breadth, newer transformer/diffusion assemblers and significance testing are thin; \n(5) presentation, more schematics/figures (failure modes, clustering dynamics, assembly flow) would substantially improve clarity and trust."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qxn0ftw7UT", "forum": "9VWeXhINgq", "replyto": "9VWeXhINgq", "signatures": ["ICLR.cc/2026/Conference/Submission22398/Reviewer_5Hkc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22398/Reviewer_5Hkc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761671102055, "cdate": 1761671102055, "tmdate": 1762942200702, "mdate": 1762942200702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "t7ZzJl3kvh", "forum": "9VWeXhINgq", "replyto": "9VWeXhINgq", "signatures": ["ICLR.cc/2026/Conference/Submission22398/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22398/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762945581808, "cdate": 1762945581808, "tmdate": 1762945581808, "mdate": 1762945581808, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ShreddingNet, a two-stage, coarse-to-fine pipeline for multi-source fragment restoration, where fragments may originate from several different images. The method first performs a coarse matching using contour-texture fused graph features and cross-attention ResGCNs to find candidate fragment pairs efficiently, then clusters fragments by source through label propagation. Then, a fine-grained matching stage with stacked cross-attention and a CNN-based scorer refines pair alignments and reconstructs each source image via maximum-spanning-tree assembly. This design reduces complexity from quadratic to linear in fragment count and requires no prior knowledge of outlier ratios. On large synthetic datasets based on art_2192 and pex_2000 and degraded hard variants, ShreddingNet achieves SOTA performance. In addition, it runs substantially faster than prior SOTA methods such as PairingNet or JigsawNet."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a well-motivated pipeline for multi-source fragment restoration. The resulting linear-time complexity and ability to work without prior knowledge of the number of sources are good.\n2. The writing is clear, well-structured, and technically precise, with definitions, equations, and figures that make the workflow easy to follow."}, "weaknesses": {"value": "1. The coarse matching stage relies on the empirical observation that \"wrong matches are intra-source.\" No detailed justification or overall error rate analysis is provided. This leaves the clustering reliability insufficiently analyzed.\n2. From table 1, there is a noticeable gap between the fine-grained stage and the global assembly stage. This suggests that much of the high performance comes from graph-level correction rather than a robust pairwise matcher. This reliance may hide systematic fine-grained stage errors or over-smoothing.\n3. The pipeline mainly combines known graph fragment encoding, cross-attention fusion, and CNN-based scoring. And due to the above two weaknesses, the pipeline's novelty is limited."}, "questions": {"value": "Weakness 1, 2, and:\nThe assumption that \"wrong matches are intra-source,\" if violated, will propagate errors throughout the pipeline. The paper would benefit from explicit error-correction mechanisms or uncertainty quantification to detect when this assumption fails. For hard-variant datasets, how much of the performance drop originates from the initial assumption's breakdown rather than the subsequent module? Moreover, how robust is the assumption under realistic conditions such as repeated patterns, high texture homogeneity, or large missing regions that obscure contour cues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZIzu5cRaPo", "forum": "9VWeXhINgq", "replyto": "9VWeXhINgq", "signatures": ["ICLR.cc/2026/Conference/Submission22398/Reviewer_na2X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22398/Reviewer_na2X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815524246, "cdate": 1761815524246, "tmdate": 1762942200393, "mdate": 1762942200393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a system to recover the original image from a set of image segments, which can be used for artwork restoration. The pipeline is composed of two stages: the coarse matching stage first extracts image segments' global features by aggregating the edge image features and contour geometry features, and retrieves related image pairs. Then, a fine matching stage is applied to find pixel-level contour correspondences among the retrieved image pairs and filter the incorrect image pairs. The proposed method is evaluated on real-world datasets and shows its effectiveness in improving the image segment matching accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The pipeline is reasonable and well-designed. The two-stage coarse-to-fine matching strategy is effective in improving the image segment matching accuracy while improving the efficiency.\n2. The overall writing is clear and easy to follow. The figures are well-organized and helpful in understanding the proposed method."}, "weaknesses": {"value": "1. The paper only cites prior works on fragment feature extraction and matching works. However, the idea of coarse retrieval via global features and then finding pixel-level correspondences is widely used in other related fields, such as image retrieval and image matching.\nThe major difference is that this work focuses on finding correspondences on image contour segments rather than full images; however, the high-level idea is similar.\nThe reviewer thinks that the authors should reference these related fields.\nIncluding but not limited to the following works:\nFrom Coarse to Fine: Robust Hierarchical Localization at Large Scale;\nSuperGlue: Learning Feature Matching with Graph Neural Networks;\nSemi-dense feature matching with transformers and its applications in multiple-view geometry;\nNetVLAD: CNN architecture for weakly supervised place recognition;\n\n2. It's not clear how the proposed method handles the contour defects, such as broken or noisy contours. In real-world scenarios, the image segments may have imperfect contours due to damage or noise. \nSince the fine matching stage relies on accurate contour correspondences for recovering the transformation between image segments, the contour defects may lead to incorrect matching results.\nThe reviewer suggests that the authors discuss how to handle these contour defects in the paper and provide some analysis on how these defects affect the matching performance."}, "questions": {"value": "1. Contour binary maps are traded as images or some other data structure for feature extraction?\n\n2. How many points are used for contour? It seems that using every contour pixel will lead to a significant number. Line 191 mentions that contour points exceeding L are truncated. Why use truncation instead of sampling? It seems that truncation may lose important shape information.\n\n3. What are the benefits of incorporating image features? It seems that contour shape is more important for image segment matching. Can you ablate the image feature extraction module to see how it affects the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wWUlYkqHFe", "forum": "9VWeXhINgq", "replyto": "9VWeXhINgq", "signatures": ["ICLR.cc/2026/Conference/Submission22398/Reviewer_ZsZ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22398/Reviewer_ZsZ5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951157840, "cdate": 1761951157840, "tmdate": 1762942200103, "mdate": 1762942200103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ShreddingNet, a two-stage, coarse-to-fine pipeline for multi-source fragment reassembly. Stage-1 extracts contour–texture bimodal features, builds a fragment similarity matrix, keeps Top-K neighbors per fragment, and runs graph clustering (LPA) to separate sources; Stage-2 applies an inter-fragment cross-attention decoder to score candidate pairs, estimates local transforms (via RANSAC), and assembles globally with a maximum-spanning-tree strategy. The authors claim linear time complexity with respect to fragment count (by capping candidates per fragment) and robustness to stains, mold, and contour defects, reporting strong results on large synthetic datasets"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "• Well-structured, practical pipeline.\nThe decomposition—coarse Top-K pruning + source clustering, followed by fine pair scoring + MST assembly—is clean and addresses both scalability and accuracy in a realistic way. The architecture description is clear, and the use of cross-attention for feature fusion and inter-fragment interaction is appropriate to the fragment domain.\n\n• Graph clustering to separate sources.\nLeveraging the observation that most erroneous coarse matches remain intra-source enables effective LPA clustering without specifying the number of sources, a good fit for multi-source restoration.\n\n•Efficiency story is plausible and backed by timing plots.\nBy constraining candidates to B×K, model invocations grow linearly with fragments; the timing regressions vs. image counts (Fig. 5) empirically show near-linear behavior on the two datasets.\n\n•Comprehensive ablations and module-level reporting.\nThe paper reports module-wise precision/recall (CM/FM/SE/GA) and ablates clustering, the fine stage, and the score network; it also defines evaluation metrics (including ARI for clustering) carefully."}, "weaknesses": {"value": "• Evidence for “linear time complexity” is empirical and conditional.\nThe claim hinges on a fixed K and the assumption that Top-K retains sufficient true neighbors as the number of fragments grows. The paper does not provide a formal argument for recall@K stability or bounds relating K, feature margins, and fragment counts. The linear fits in Fig. 5 are encouraging but do not constitute a complexity guarantee (they also depend on hardware and implementation constants). Clarify under what distributional conditions Top-K recall remains bounded away from zero and how worst-case adversarial configurations behave. \n\n• External validity is limited by heavy reliance on synthetic data.\nTraining and testing are primarily synthetic; the real-world evaluation comprises two small case studies produced by printing/tearing/scanning a few images. It is unclear how the method fares on genuine museum scans (paper translucency, bleed-through, nonplanarity, illumination gradients). A larger real-world benchmark (≥50 images) or collaboration with a heritage institution would substantially strengthen the claims. \n\n• Comparative coverage may be incomplete.\nThe baselines (rule-based, JigsawNet, PairingNet) are adapted for multi-source reassembly, and one relevant prior (LLMCO4MR) is excluded due to missing artifacts. The field also includes recent GNN/OT/transformer puzzle solvers and diffusion-based approaches that might be competitive after adaptation. Please expand baseline breadth or justify omissions with implementation and fairness details. \n\n• Clustering reliability and failure cases are under-analyzed.\nLPA is sensitive to graph edge noise and can produce inter-source bridges if coarse similarities are “sticky.” The paper argues errors are mostly intra-source, but does not quantify the rate and impact of bridge edges nor give sensitivity curves of ARI vs. K and vs. corruption level. Provide precision–recall for clustering labels (not just assembly edges) and analyze failure modes (e.g., content-similar sources). \n\n• Score network calibration and domain shift.\nA lightweight CNN + MLP scores similarity matrices; however, there is no discussion of calibration (ECE/Brier) or transfer to different materials/inks/languages. Since the score threshold is a key hyperparameter, present calibration plots and threshold sensitivity across domains (art vs. pex; normal vs. hard). \n\n• Efficiency accounting across methods.\nTiming includes all stages, but RANSAC cost can dominate in some pipelines; the paper argues its coarse-to-fine design reduces the number of RANSAC calls relative to PairingNet. Please provide per-stage time breakdowns and standard deviations, and clarify pre/post-processing overheads to ensure fair timing comparisons."}, "questions": {"value": "• Coarse features and fusion (Sec. 3.2).\n1) The paper limits contour points to L and asserts no truncation by “adjusting L,” but on pex 2000, L=2400—what is the distribution of contour point counts, and what is the runtime/memory sensitivity to L? Report histograms and ablate L vs. accuracy/time.\n2) Cross-attention fusion averages two directional attentions. Why average rather than concatenate + projection or gated fusion? A small ablation here would help.\n\n• Candidate selection & K.\nThe K-sweep in Appendix I is useful; consider plotting recall@K for true neighbors in Stage-1 separately from downstream F1, to quantify pure coarse recall. Also show fragment-count–normalized K (e.g., K as a fraction of B) to discuss scalability.\n\n• Clustering (LPA).\nPlease report ARI vs. K on both datasets and under corruption (hard settings). Include bridge-edge rate (probability an inter-source edge survives after coarse matching).\n\n• Fine-stage interaction (Sec. 3.3).\n1) The cross-attention decoder depth is n=2. Provide a depth sweep (n = 1, 2, 4) and head count sweep; show trade-offs vs. runtime. \n2) The valid mask on M removes off-contour entries—explain mask construction under contour defects (holes, extrusions).\n\n• Score estimation.\nBeyond the CNN vs. rule-based contrast, please provide ROC curves and AUC distributions by difficulty, plus calibration curves.\n\n• RANSAC details\nWhat is the inlier threshold and minimal set? Do you estimate affine, similarity, or homography? Provide robustness to outlier ratios and ablate RANSAC iterations.\n\n• Global assembly (MST).\nThe MST weight mixes score and transform—how are these normalized and combined? Provide a weighting ablation and analyze cycles/contradictions when multiple high-score edges disagree geometrically.\n\n• Metrics\nYou already include multiple seeds for Table 1 (great). Please add mean ± std for timing in Fig. 5, and run significance tests (paired) against the strongest baseline on overlapping test subsets.\n\n• Dataset realism.\nSynthetic stains/mold/contour defects are well documented, but they may not capture paper warp, nonplanarity, translucency. Consider simulating projective/distortion fields and illumination gradients, or at least discuss limitations. \n\n• Real-world cases.\nThe two case studies suggest promise but are too small for statistical conclusions. Please release all scans, masks, and outputs to enable third-party replication; add a mini-benchmark (≥50 images) if feasible. \n\n• Fairness in baseline adaptations.\nYou replace JigsawNet’s HLM assembly and choose arbitrary trees for PairingNet (no learned scores). Document these choices and report time/quality with at least one alternative fair setting per baseline to guard against adaptation bias. \n\n• Reproducibility materials.\nThe paper claims code/models/data in the supplementary and on HF; ensure exact scripts (training, evaluation, data generation), random seeds, and failure cases are included."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DCPg1qTsA9", "forum": "9VWeXhINgq", "replyto": "9VWeXhINgq", "signatures": ["ICLR.cc/2026/Conference/Submission22398/Reviewer_Qhgm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22398/Reviewer_Qhgm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965934692, "cdate": 1761965934692, "tmdate": 1762942199545, "mdate": 1762942199545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}