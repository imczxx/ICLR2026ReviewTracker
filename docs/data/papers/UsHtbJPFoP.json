{"id": "UsHtbJPFoP", "number": 23741, "cdate": 1758347825569, "mdate": 1759896799372, "content": {"title": "Learning in Context, Guided by Choice:  A Reward-Free Paradigm for Reinforcement Learning with Transformers", "abstract": "In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods require explicit reward signals during pretraining, limiting their applicability in real-world scenarios where rewards are ambiguous, difficult to specify, or expensive to collect. To overcome this limitation, we propose a new learning paradigm, *In-Context Preference-based Reinforcement Learning* (ICPRL), where both the pretraining of TMs and their deployment to new tasks rely solely on preference data, thereby eliminating the need for reward supervision. Within this paradigm, we study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a proven strategy in ICRL, remains effective in ICPRL for training TMs to predict optimal actions using preference-based context datasets. To improve data efficiency, we further propose alternative frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without relying on optimal action labels or reward signals. Empirical evaluations on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong generalization to unseen RL tasks, achieving performance on par with ICRL methods trained with full reward supervision.", "tldr": "", "keywords": ["in-context learning", "in-context reinforcement learning", "transformers", "preference-based learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72c091a5f2eca36d519cc5b0696ec31c39c22996.pdf", "supplementary_material": "/attachment/fb0f065c35a6c71261291b297ec22575106e8350.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a method for learning transformer-based control policies that rely on in-context learning abilities to generalize to new tasks and learn from offline data. The proposed approach ICPRL relies on conditioning the transformer policy on preference labelled trajectories both during pre-training and at deployment to encourage the policy to infer good versus bad behavior without requiring access to an explicit reward signal. After introducing this approach, the paper identifies that it does a poor job of learning from the preference data. Therefore, the paper introduces a modified approach that relies on a DPO-like objective when preference feedback is available per step and preference sample conditioned reward model when preference feedback is at the trajectory level. The results are presented for the per step and the trajectory level preference feedback in Darkroom (per step only) and MetaWorld. In some settings the proposed approach outperforms while in others it matches or falls short of the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper applies the proposed approach to two preference feedback settings: at the step level and at the trajectory level.\n- The paper is clear and easy to follow.\n- The primary modification that is proposed is simple and not overly complex or convoluted.\n- In MetaWorld, the ICRG results are stable across data quality conditions or increasing with data quality whereas other methods lack an consistent performance increase as data quality improves."}, "weaknesses": {"value": "- Given the baselines provided, it is not easy to identify the benefits of the including preference labelled data in the reward nor policy's context. Examples of baselines that would help to make this clear include, training a reward model on the preference data and doing ICRG without the proposed preference dataset conditioning and using the proposed reward modelling approach to train a policy without conditioning on the preference samples.\n- There are no results indicating how noisy preference labels impact reward and policy learning.\n- The bulk of experiments and results are not in the main body of the paper. Instead 8/9 pages are dedicated to set up, despite the paper claiming the experiments \"prove\" the benefits of the proposed method. More of the experiments should be in the main body.\n- The experiments are describing a proving the proposed method is generalizable. Experiments don't \"prove\", they demonstrate, show, suggest, or provide evidence.\n- The contribution of the proposed \"In-Context Preference Optimization\" is not clear. It seems like an application of DPO.\n- The paper presents its contributions as more complex than what they appear to be in practice. The main contributions seem to be applying DPO to per step preferences, learning a reward function conditioned on preference labelled examples, and doing exactly ICRL with the addition of conditioning on preference samples. The simplicity of this is hidden in the presentation.\n- Instead of SAC, a meta-learning or multi-task algorithm that learns from the ground truth reward should be used.\n- The experimental set up is not clearly detailed. For example, the number of tasks used for training versus evaluation is not clear nor which sets of tasks were used in each split."}, "questions": {"value": "- What was the multi-task set up for MetaWorld? Which tasks were trained on? Which were evaluated on?\n- Why are you reporting episode reward instead of success rate for MetaWorld?\n- How many random seeds were used?\n- Are the results in Figure 2 and Figure 3 means over tasks? If so, what is standard error from the mean? If not, what were the episode rewards computed over?\n- It is said that the context datasets are of low, medium, or high quality. What does this mean when the context dataset are preference samples? Does the quality description cover the trajectories or the label quality? If label quality, how was that manipulated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GfEeJcOgEu", "forum": "UsHtbJPFoP", "replyto": "UsHtbJPFoP", "signatures": ["ICLR.cc/2026/Conference/Submission23741/Reviewer_EAD1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23741/Reviewer_EAD1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763657607, "cdate": 1761763657607, "tmdate": 1762942786911, "mdate": 1762942786911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores extending in-context reinforcement learning (ICRL) with supervised pretraining on preference data instead of explicit reward signals. The authors provide methods for learning from both per-step preference and trajectory-wise preference. The experiments show the proposed method performs on par with or exceeds baselines that use explicit reward signals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The writing is clear and easy to understand.\n- The proposed methods are novel extensions of the existing ICRL frameworks to the preferential data domain.\n- The proposed methods remain competitive in the absence of explicit reward signals."}, "weaknesses": {"value": "- It seems each experiment only contains one run, which raises concerns about statistical rigour.\n- I am concerned about the practicality and robustness of the in-context reward estimation and reward relabelling approach. It involves two stages of in-context learning: one for learning the in-context reward estimator and the other for learning the in-context policy."}, "questions": {"value": "- Is the in-context policy learning independent from the in-context reward estimator learning, or does it use the relabelled data by the estimator for training?\n- Why do the authors choose a regularized objective in this case? If the ultimate goal is to maximize returns, I don't see why one should penalize deviations from the behaviour policy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dvNI3rc1v2", "forum": "UsHtbJPFoP", "replyto": "UsHtbJPFoP", "signatures": ["ICLR.cc/2026/Conference/Submission23741/Reviewer_ejrF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23741/Reviewer_ejrF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976103370, "cdate": 1761976103370, "tmdate": 1762942786742, "mdate": 1762942786742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents in-context preference reinforcement learning (ICPRL), a form of multi-task reinforcement learning where the adaptation to different tasks is done in-context (ie. changing the inputs to the transformer model), and the reward is derived (implicitly or explicitly) from binary preferences.\n\nThe paper proposes two implementations of ICPRL: in the first one the preferences are considered over state-action pairs (I-PRL), in the second one the preferences are considered over trajectories (T-PRL). Additionally, three pertaining strategies for I-PRL and T-PRL are explored: 1) DP2T where the preferences are simply provided in context, 2) ICPO (only for I-PRL) where preferences are used to directly optimise the transformer model (analogously to Direct Preference Optimisation), and 3) ICRG (only for T-PRL) where a multi-task reward function is learnt from trajectory preferences.\n\nExperiments with MetaWorld and DarkRoom show that I-PRL+ICPO and T-PRL+ICRG are competitive with the ICRL baseline, but without the need for explicit reward functions or high-quality reward transitions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The paper is well written, provides a good motivation, and is easy to follow (though see nitpicks below).\n* The derivation of ICPO is very interesting and so is its connection to DPO.\n* The paper tackles an challenging problem (multi-task RL) through creative means (mixing in-context learning with preference-based reinforcement learning)\n* The theoretical derivation of the paper is _very_ thorough with many useful appendices (though note I did not have time to review all of the appendices).\n* The paper explores thoroughly two very different approaches to in-context preference reinforcement learning: I-PRL and T-PRL."}, "weaknesses": {"value": "* **W1**: The form of multi-task learning employed in the experiments is too narrow. For both DarkRoom and MetaWorld, \"multi-task\" boils down to different initial and goal states. Indeed, it was precisely this issue that motivated MetaWorld [1] (quoting from the abstract, emphasis mine):\n\n> However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. **When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks**. Therefore, if the aim of these methods is enable faster acquisition of entirely new behaviors, we **must evaluate them on task distributions that are sufficiently broad** to enable generalization to new behaviors.\n\nThe current experiments on MetaWorld are a necessary baseline, but the impact of ICPRL would be much more significant if the method worked well on the MT-10 portion of MetaWorld tasks.\n\n* **W2**:  Though the related works section is really well redacted, it is missing a discussion of how ICPRL differs from goal-conditioned reinforcement learning. Similarly, connections to meta-learning should be explored.\n\n* **W3**: Some key ablations are missing, specifically: the number of preferences used during pre-training, the number of examples given in-context, and the accuracy of the learnt reward function for ICRG (see questions below for more details).\n\n* **W4**: The effect of human-gathered preferences (say for T-PRL) is not studied. Without such a study the applicability of ICPRL is reduced to settings where preferences may be automatically obtained.\n---------\n\n[1] Yu et al. (2019) \"Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning\" CoRL."}, "questions": {"value": "### Questions (in no particular order)\n\n* **Q1**: The paper states that AD (Laskin et al. 2002) \"assume $D^R = \\\\left\\\\{\\\\epsilon_{i,j}\\\\right\\\\}^J_{j=1}$, a set of trajectories  $\\\\epsilon_{i,j}$ collected by increasingly improving policies\", yet the reference cited seem to provide state-action preferences instead. Did I miss something in Laskin et al? If not, can you provide an example of an ICRL method in the literature that provides trajectory-level rewards?\n* **Q2**: For I-PRL how are the in-context state-action pairs sampled? Are they sampled independently? Or do they follow the state-action pairs of the preferred trajectory?\n* **Q3**: Are there any alternatives to the uniform policy for ICPO (line 345)? Could you really on a policy that maximises exploration of the state-space for instance?\n* **Q4**: For T-PRL+DP2T, is the binary preference inserted before or after the trajectories? If it's inserted after, is the preference always on the same position (ie are the trajectories always of the same length?)\n* **Q5**: For Figs 2 & 3, are these the results of a single run? What is the spread across different initialisations for the same task (same initial and goal states)? What is the spread across different tasks?\n* **Q6**: What is the effect of increasing/decreasing the size of the pre-training dataset on final performance?\n* **Q7**: How many examples are given in-context? What is the effect of increasing/decreasing in-context samples on final performance?\n* **Q8**: In Fig 2 (bottom, MetaWorld), why does increasing the quality of the trajectories result in a decrease in performance for I-PRL+DP2T?\n* **Q9**: in Fig 2 (bottom, MetaWorld) the performance of SAC looks poor. Did SAC actually converge during training?\n* **Q10**: what are the effects of varying the numbers of tasks for use in pertaining/testing? How did you come up with the current splits (for DarkRoom the ratio is 80/20, whereas for MetaWorld is 90/10).\n* **Q11**: related to the previous question and following up on lines 216-220, how many additional preference pairs are needed to get bring ICPRL to work a new task?\n\n-------\n\n### Nitpicks (do not affect rating, no need to follow up during rebuttal)\n\n* **N1**: Appendix B has repeated sections from the main paper verbatim. Please include only what was not added in the main text (and ideally try to fit the literature review in the main paper).\n* **N2**: Line 143: instead of \"classification-like\" I suggest \"cross-entropy where the optimal action $a^\\\\star$ has $p=1$.\n* **N3**: Are you certain that the reliability of labels can be _guaranteed_ by any LLM method? Do you have citation where this statement is proved?\n* **N4**:  I would merge sections 5 and 6, since DP2T is just another pertaining strategy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tdn2osk9w3", "forum": "UsHtbJPFoP", "replyto": "UsHtbJPFoP", "signatures": ["ICLR.cc/2026/Conference/Submission23741/Reviewer_8ssj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23741/Reviewer_8ssj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23741/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184507504, "cdate": 1762184507504, "tmdate": 1762942786410, "mdate": 1762942786410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}