{"id": "QoDOwjsbAq", "number": 5001, "cdate": 1757831257675, "mdate": 1759898001008, "content": {"title": "VisionReasoner: Unified Reasoning-Integrated Visual Perception via Reinforcement Learning", "abstract": "Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks. In this paper, we introduce VisionReasoner, a unified framework capable of reasoning and solving multiple visual perception tasks within a shared model. Specifically, by designing a unified reward mechanism and multi-object cognitive learning strategies, VisionReasoner enhances its reasoning capabilities to analyze visual inputs, and addresses diverse perception tasks within a unified model. VisionReasoner generates a structured reasoning process before delivering the desired outputs responding to user queries. Human evaluation reveals the reasoning process of VisionReasoner is faithful and reliable even without annotated reasoning train data. To rigorously assess unified visual perception capabilities, we evaluate VisionReasoner on ten diverse tasks spanning three critical domains: detection, segmentation, and counting. Experimental results show that VisionReasoner achieves superior performance as a unified model, outperforming the baseline Qwen2.5VL by relative margins of 29.1\\% on COCO (detection), 22.1\\% on ReasonSeg (segmentation), and 15.3\\% on CountBench (counting).", "tldr": "", "keywords": ["Reasoning Segmentation; Reinforcement Learning; Multi-model Large Language Models; Visual Perception"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8e98d332301840bc607c34cfae827a04c4d45dbf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a unified LVLM for different perception tasks such as detection, segmentation, counting, and more. Through jointly optimizing the reward formulated with different metrics for perception tasks using GRPO and DAPO, the model is able to outperform the base model significantly on various perception tasks. Meanwhile, the model also shows better performance on VQA tasks despite not being fine-tuned on those tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A unified LVLM for different perception tasks is proposed, which could benefit the broader community.\n\n2. The proposed method is simple yet effective.\n\n3. The experiments and ablation studies are comprehensive."}, "weaknesses": {"value": "1. The contribution seems somewhat limited, as the paper mainly focuses on designing effective rewards for various perception tasks.\n\n2. More comparisons between VisionReasoner and the base model on VQA tasks would be beneficial. It would also be interesting to understand whether there is any degradation in the model’s general capability since it is trained on perception-specific tasks."}, "questions": {"value": "What is the numerical performance of $mmmu_{val}$ for VisionReasoner?\n\nOn which tasks does it perform well, and on which tasks does it underperform compared with the base model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "na8i7rMsPl", "forum": "QoDOwjsbAq", "replyto": "QoDOwjsbAq", "signatures": ["ICLR.cc/2026/Conference/Submission5001/Reviewer_H7fg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5001/Reviewer_H7fg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428981778, "cdate": 1761428981778, "tmdate": 1762917816598, "mdate": 1762917816598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VisionReasoner, a unified RL-based LVLM that handles detection, segmentation, and counting in a single model via a “locate first (boxes + points), then segment/count” paradigm, while producing interpretable reasoning traces. It uses GRPO with a task-agnostic reward scheme (thinking/format and non-repetition rewards, plus multi-object IoU and L1 accuracy rewards) and efficient multi-object matching via batched computation and the Hungarian algorithm. The authors report zero-shot performance on 10 benchmarks, claiming substantial gains with only ~7k training samples and no degradation to VQA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unified multi-task framework design. \nThe paper successfully constructs a unified framework capable of handling three major categories of visual perception tasks—detection, segmentation, and counting—simultaneously. This unified design offers several notable advantages.\n\n2. Outstanding data efficiency and scalability. \nWith only 7,000 training samples, the VisionReasoner-7B model achieves strong performance, demonstrating impressive data efficiency and generalization capability."}, "weaknesses": {"value": "1. The experimental evaluation could benefit from a broader and more up-to-date set of baseline models. The paper mainly compares VisionReasoner with Shikra and Qwen2.5-VL; however, Shikra, as an early work from 2023, may not fully reflect the current progress of large vision-language models (LVLMs). Expanding the comparison to include more recent LVLMs could provide a fairer and more comprehensive assessment of VisionReasoner’s performance in the current landscape.\n\n2. Some implementation aspects could be described in greater detail. For instance, the distribution of training data across different task types is not fully specified, and the weighting or design rationale of the overall reward function is somewhat unclear. Providing additional clarification in these areas would enhance the work’s reproducibility and help readers better understand the factors contributing to the model’s performance."}, "questions": {"value": "1. Insufficient Transparency in Training Data Distribution\nThe paper reports using approximately 7,000 training samples but does not provide details on their distribution across different task types. Since the allocation of training data can significantly affect performance balance in multi-task learning, it is recommended to specify the number and proportion of samples for each task type, along with the criteria for data partitioning, to enhance reproducibility and interpretability.\n\n2. Limited Coverage of Large Vision-Language Models\nThe current experiments mainly compare with models such as Shikra and Qwen2.5-VL. However, Shikra is an early work, and the overall coverage of more recent and powerful large vision-language models (LVLMs) remains limited. Including additional representative LVLMs—such as LLaVA, MiniGPT-4, InstructBLIP, or Qwen-VL-Max—would provide a more comprehensive and up-to-date evaluation, thereby better demonstrating VisionReasoner’s competitiveness within a unified multi-task framework.\n\n3. Ambiguity in Reward Function Weight Design\nThe paper introduces multiple reward functions (e.g., format reward, IoU reward, L1 reward) but does not clearly explain the weighting scheme or its underlying rationale. Furthermore, the paper lacks quantitative analysis of the relative importance of these rewards (e.g., through ablation studies). It is recommended to include further experiments or discussions to strengthen the justification and interpretability of the reward design."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Oa9lbF4ZQF", "forum": "QoDOwjsbAq", "replyto": "QoDOwjsbAq", "signatures": ["ICLR.cc/2026/Conference/Submission5001/Reviewer_7Pxd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5001/Reviewer_7Pxd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821480464, "cdate": 1761821480464, "tmdate": 1762917816138, "mdate": 1762917816138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VisionReasoner, a unified framework for compositional multi-hop reasoning across a wide range of vision-language tasks. Unlike traditional task-specific methods, VisionReasoner decomposes complex queries into structured reasoning steps using a Unified Intermediate Representation (UIR) and a task-agnostic planner-executor architecture. The planner produces reasoning programs from natural language questions, and the executor interprets them over visual observations. The authors train the model on diverse reasoning tasks and demonstrate strong zero-shot generalization, outperforming task-specific models on multiple benchmarks without fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed architecture (planner + executor + UIR) offers a principled way to unify reasoning over different modalities and domains.\n2. The system generalizes across unseen reasoning tasks and domains with minimal or no task-specific supervision."}, "weaknesses": {"value": "1. The strongest recent baselines use retrieval-augmented reasoning where structured planning is implicit. The authors only compare with older systems like RAML and ReGrouP, not modern VLMs fine-tuned with in-context reasoning.\n2. The planner is the backbone of VisionReasoner, yet the accuracy of program generation is not reported. The author shall consider to include planner-only accuracy (e.g., execution success rate, semantic match with gold reasoning paths).\n3. The UIR uses a limited set of programmatic operations that must be predefined. This suggests limited compositional expressivity, especially when handling spatial, temporal, or logical reasoning beyond object-centric understanding.\n4. The executor module is assumed to interpret UIR programs correctly. There is no ablation showing how executor errors affect end-to-end performance, nor how robust the system is to ambiguous or poorly planned programs. The authors shall consider to include experiments around the accumualted error issue.\n5. While modularity aids interpretability, separating planner and executor may hurt end-to-end task performance. There's no attempt at joint fine-tuning, which could close the gap with SOTA models on certain benchmarks. I think joint fine-tuning would interest more potential audiences in the area."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UX1dK5Di6m", "forum": "QoDOwjsbAq", "replyto": "QoDOwjsbAq", "signatures": ["ICLR.cc/2026/Conference/Submission5001/Reviewer_9SJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5001/Reviewer_9SJ9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915802092, "cdate": 1761915802092, "tmdate": 1762917815219, "mdate": 1762917815219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a framework wherein a base vision-language model (VLM) is RL-postrained for 3 text-guided perception tasks: object detection, segmentation and counting. They utilize the Qwen-2.5-VL-7B model as the base VLM and SAM2 for segmentation, while GRPO is utilized for RL training on 7000 training samples collected from LVIS, RefCOCOg, gRefCOCO and LISA++. Rewards for RL include thinking format, answer format, 'non repeat' format, bboxes IoU reward, bboxes L1 reward and points L1 reward. For matching rewards for multiple objects, the authors propose a hungarian matching algorithm.\n\nResults show the authors proposed 'VisionReasoner' model outperforms existing large VLMs and the base Qwen2.5-VL-7B model on multiple benchmarks. Further analysis and ablations are also provided such as impact of RL algorithm, human analysis of reasoning process comparison of response lengths across dataset and results on visual question answering (which was not RL trained for)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed framework is straightforward and clear to understand -- usage of RL to postrain a VLM for 3 core perception tasks with multiple rewards to capture the perception tasks and reasoning lengths.\n2. The results show clear improvements of RL postraining (using GRPO objective) in improving results on appropriate benchmarks.\n3. Experiments include ablations and analysis to understand the method."}, "weaknesses": {"value": "1. The authors state that previous methods employ RL in a task-specific manner and utilize distinct reward functions for different tasks. However, in my opinion, authors in their work also seem to employ task-specific rewards for detection and point matching in addition to format rewards. \n2. A zero-shot chain-of-thought prompted baseline should be present as without it, it is currently unclear whether just directly prompting model to think step-by-step or breakdown the prompt can also be sufficient to obtain a decent performance boost on the base model for considered perception tasks. This is an important missing baseline in my view.\n3. Authors mention performing human evaluation on reasoning process but do not provide details on how this is done (e.g.  how many participants, the task setup, etc.)\n\nRelatively minor: \n\n4. Experiment details can be more extensive: e.g. \n- How many GPUs are used for training and how many epochs/steps for RL convergence? \n- Will code and models be open source for reproduction?"}, "questions": {"value": "Please refer to weaknesses section above. \n\nIn addition:\n1.  What are potential reasons for the RL-postrained result being less than the base model for Detection and segmentation on RefCOCO, RefCOCO+, RefCOCOg?\n2. Will code and models be open source for reproduction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a0nPAlaSO0", "forum": "QoDOwjsbAq", "replyto": "QoDOwjsbAq", "signatures": ["ICLR.cc/2026/Conference/Submission5001/Reviewer_ybFD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5001/Reviewer_ybFD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973697254, "cdate": 1761973697254, "tmdate": 1762917814789, "mdate": 1762917814789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}