{"id": "D8aUg8R1VG", "number": 12413, "cdate": 1758207629778, "mdate": 1759897511389, "content": {"title": "Conditional Diffusion Models with Classifier-Free Iterative Guidance", "abstract": "Classifier-Free Guidance (CFG) is a widely used technique for improving conditional diffusion models by linearly combining the outputs of conditional and unconditional denoisers. While CFG enhances visual quality and improves alignment with prompts, it often reduces sample diversity, leading to a challenging trade-off between quality and diversity. To address this issue, we make two key contributions.\nFirst, CFG generally does not correspond to a well-defined denoising diffusion model (DDM). In particular, contrary to common intuition, CFG does not yield samples from the target distribution associated with the limiting CFG score as the noise level approaches zero—where the data distribution is tilted by a power $w>1$ of the conditional distribution. We identify the missing component: a Rényi divergence term that acts as a repulsive force and is required to correct CFG and render it consistent with a proper DDM. Our analysis shows that this correction term vanishes in the low-noise limit.\nSecond, motivated by this insight, we propose a novel sampling procedure to draw samples from the desired tilted distribution. This method starts with an initial sample from the conditional diffusion model without CFG and iteratively refines it, preserving diversity while progressively enhancing sample quality. We evaluate our approach on both image and text-to-audio generation tasks, demonstrating substantial improvements over CFG across all considered metrics.", "tldr": "", "keywords": ["Diffusion models", "classifier-free guidance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a8b7fd74c2bab3ea1a408bd79c6f6a42cafc917.pdf", "supplementary_material": "/attachment/b1f72ba8739ae7db7581f4bba296218a26267003.zip"}, "replies": [{"content": {"summary": {"value": "The paper theoretically analyzes the bias of CFG with the aid of the Renyi divergence. To be more detailed, the authors describe quantitatively the discrepancy between scores of conditional distribution and CFG-guided one. Benefiting from this finding, the bias in CFG could be expressed as some function with respect to the noise schedule term $\\sigma$, enabling more precise control of guidance weight and trajectories in CFG-guided synthesis. Both qualitative and quantitative experiments confirm the efficacy of the proposed method."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The quantitative description of the bias in CFG is solid and insightful. The error analysis is also precise and of great soundness.\n- The motivation of the proposed method is clear and easy to follow. Reducing the guidance weight at the stage which may lead to huge bias indeed improves the synthesis performance.\n- The experimental results are comprehensive. Results under different settings consistently confirm the efficacy, which is greatly convincing."}, "weaknesses": {"value": "- Despite the solid theoretical part, the proposed pipeline is relatively not well-explored. First, as is reported in Fig. 3, increasing Repetitions fails to further improve the performance. Involving only one or two iterations seems a local correction or refinement and inconsistent with \"iterative guidance\", since most denoising steps before $T_0$ fall back to the native CFG with a smaller weight. Besides, as is claimed in Prop. 2, the discrepancy is an $O(\\sigma^2)$ term when $\\sigma$ is small. This seems to conflict with the results in Fig. 3, in which extremely small $\\sigma_*$ almost corrupts both FID and FD_DINOv2. Note that EDM2 does not need large CFG weight, why is performance so sensitive to $\\sigma_*$? Finally, to more properly use the current name \"iterative guidance\", could the authors come up with some pipelines to shorten the native CFG period and use more iterations and some dynamic $\\sigma_*$ and guidance weights to better follow the theory?\n- The current notation system is kind of hard to follow. Four notations $p$, $q$, $\\pi$, and $g$ all represent probabilities, making the understanding a bit difficult. Besides, what does the last term in ODE_Solver in Alg. 1 mean? From my understanding in the iterative stage the denoising process should be the same, then why use something like $(T-T_0)/R$ which seems like the average length of the denoising step? If this means the step length, then what is $k$ and $T_0+k$? When $R=1$, $T_0$ mod $R = 0$ and $k=T$. I try my best but cannot understand this part."}, "questions": {"value": "Could the authors provide more detailed ablation on the functionalities under both guidance scale and noise level? For example, effects of guidance scale under different noise level, or what if guidance scale increases but noise level decreases? The ablation of multi-factor could offer deeper insight of the pipeline and make it easy to use."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MKxotpKJlB", "forum": "D8aUg8R1VG", "replyto": "D8aUg8R1VG", "signatures": ["ICLR.cc/2026/Conference/Submission12413/Reviewer_M9WA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12413/Reviewer_M9WA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761113661532, "cdate": 1761113661532, "tmdate": 1762923307375, "mdate": 1762923307375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates improving the widely used classifier-free guidance (CFG) to simultaneously enhance sample quality and diversity. The authors first provide a theoretical analysis, formulating the gap between the ideal conditional score and the CFG score (with guidance scale w) using the Rényi divergence of order w. Separately, they introduce a two-stage sampling scheme called Classifier-Free Iterative Guidance (CFIG). In the first stage (a traditional CFG procedure), a probability flow ODE is integrated from a high noise level (sigma_max) using a moderate guidance scale w_0 to obtain diverse initial samples. In the second stage, these samples are repeatedly refined by being perturbed to a low noise level (sigma_*) and then denoised by solving a reverse SDE guided by a larger CFG scale w. The method is evaluated on class-conditional image generation and text-to-audio synthesis tasks, with ablation studies conducted on key hyperparameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Code is provided in the supplementary materials, which facilitates reproducibility.\n2. The paper studies classifier-free guidance (CFG), an important component in diffusion-based generative models that is widely applicable across different diffusion frameworks.\n3. A theoretical analysis is presented to characterize the gap between the ideal conditional score and the CFG score.\n4. A new sampling procedure is proposed to potentially improve generation quality."}, "weaknesses": {"value": "1. The connection between the proposed CFIG method (Section 4) and the theoretical analysis (Section 3) appears weak. None of the key parameters -- such as sigma_*, T-T_0, or R -- is quantitatively derived from the theoretical results. Moreover, the central idea of CFIG, namely iterative refinement on a weakly guided CFG results, seems only tenuously related to the theoretical observations in Section 3. The current presentation overcomplicates a rather straightforward method, which may obscure the core insights and hinder the reader’s understanding.\n2. The idea of *iteratively perturbing samples to higher noise levels and then re-denoising them* has been widely explored in prior diffusion sampling methods [1][2][3] for result refinement. The paper lacks a clear discussion of how the proposed CFIG method relates to, or differs from, these existing approaches.\n[1] Yu, Jiwen, et al. \"Freedom: Training-free energy-guided conditional diffusion model.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n[2] Bansal, Arpit, et al. \"Universal guidance for diffusion models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.\n[3] Ma, Nanye, et al. \"Inference-time scaling for diffusion models beyond scaling denoising steps.\" arXiv preprint arXiv:2501.09732 (2025).\n3. The experimental results are not convincing, and several important details remain unclear.\na. The performance improvement over INTERVAL-CFG in the image generation experiments is marginal and could easily fall within the range of randomness.\nb. According to Algorithm 1, CFIG appears to use a CFG denoiser to obtain the initial samples, implying a runtime at least comparable to that of CFG. However, the appendix (“Comments on runtime and memory”) claims that CFIG first generates an initial sample using the plain denoiser, resulting in a faster process. This inconsistency makes it unclear how CFG is actually applied in the first stage.\nc. The paper reports only the number of sampling steps when comparing with prior works. It would be more appropriate to report the number of function evaluations (NFE), as different sampling methods entail varying numbers of model forward passes during guided generation."}, "questions": {"value": "1.The paper presents many conclusions and propositions but lacks intuitive explanations to help readers grasp the core ideas. For instance, what specifically causes the gap (the gradient of the Rényi divergence) between the ideal score and the CFG score? Why is this score gap bounded by \\sigma^2? I suggest that the authors include the key steps or intuitions behind the proofs in the main text, rather than placing all derivations in the appendix.\n2.The text-to-audio generation experiment is not necessary for evaluating the proposed methods, though CFIG shows relatively better performance than in the image experiments.\n3.A more convincing ablation would be directly use the generated result of CFG/INTERVAL-CFG as the initial sample, and then apply the iterative resampling process to it to see to what extent can this process refine the initial sample. \n4.The iterative resampling procedure is, in principle, not tightly coupled with CFG. It would be interesting to investigate whether this iterative refinement is also effective for non-guided sampling processes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DcecRZZ8ta", "forum": "D8aUg8R1VG", "replyto": "D8aUg8R1VG", "signatures": ["ICLR.cc/2026/Conference/Submission12413/Reviewer_4nXS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12413/Reviewer_4nXS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991134107, "cdate": 1761991134107, "tmdate": 1762923307115, "mdate": 1762923307115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper theoretically analyzes the inconsistency problem in classifier-free guidance for conditional diffusion models and traces its root cause. The authors rigorously prove that the widely used linear combination of conditional and unconditional denoisers in CFG does not correspond to any valid denoising diffusion process. Through detailed mathematical derivation, they identify a missing gradient term of the Rényi divergence. Based on this analysis, the authors propose a new sampling procedure named Classifier-Free Iterative Guidance. The method first generates an initial sample using a weak or zero guidance scale, and then performs multiple rounds of re-noising and denoising under a normal guidance scale. This iterative process gradually refines sample quality while preserving diversity. Extensive experiments on several tasks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a clear and rigorous derivation showing CFG’s theoretical inconsistency and identifies the missing Rényi divergence term that explains its mode-collapse behavior.\n2. The proposed CFIG algorithm is easy to implement since it only modifies the sampling phase, requiring no model retraining. It integrates seamlessly with existing diffusion backbones, making it practically useful for real-world applications."}, "weaknesses": {"value": "1. Although the paper theoretically derives the Rényi divergence correction, it is not actually incorporated in the proposed algorithm. It's a bit of a shame that CFIG circumvents this problem by using a low-guided approximation instead of explicitly using the Rényi correction.\n2. CFG exhibits different problems in different modal generation tasks. The paper validates its performance on image and audio generation. I'd like to see its performance on more modalities, such as video generation tasks.\n3. What do “CFGIG” stands for in Figure 6 and Figure 7?\n4. In the image experiment, what is the R value of CFIG? Given that the quantitative metrics are comparable to INTERVAL-CFG without significant improvement, if R is greater than 1, does this imply that more denoising steps were used only to achieve performance similar to that of INTERVAL-CFG? If so, it would indicate a lack of cost-effectiveness in comparison.\n5. Following up on the previous question, there is a lack of experimental analysis regarding efficiency.\n6. Considering that DDMs are more commonly used for generation tasks, the absence of qualitative comparisons makes it difficult to evaluate their practical application value."}, "questions": {"value": "Please refer to the questions and suggestions in the “Weaknesses” part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MOs3X7dLvz", "forum": "D8aUg8R1VG", "replyto": "D8aUg8R1VG", "signatures": ["ICLR.cc/2026/Conference/Submission12413/Reviewer_UCMX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12413/Reviewer_UCMX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009146887, "cdate": 1762009146887, "tmdate": 1762923306885, "mdate": 1762923306885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}