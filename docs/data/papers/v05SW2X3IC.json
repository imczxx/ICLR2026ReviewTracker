{"id": "v05SW2X3IC", "number": 1774, "cdate": 1756917588951, "mdate": 1759898187663, "content": {"title": "Lossy Common Information in a Learnable Gray-Wyner Network", "abstract": "Many computer vision tasks share substantial overlapping information, yet conventional codecs tend to ignore this, leading to redundant and inefficient representations. The Gray-Wyner network, a classical concept from information theory, offers a principled framework for separating common and task-specific information. Inspired by this idea, we develop a learnable three-channel codec that disentangles shared information from task-specific details across multiple vision tasks. We characterize the limits of this approach through the notion of lossy common information, and propose an optimization objective that balances inherent tradeoffs in learning such representations. Through comparisons of three codec architectures on two-task scenarios spanning six vision benchmarks, we demonstrate that our approach substantially reduces redundancy and consistently outperforms independent coding. These results highlight the practical value of revisiting Gray-Wyner theory in modern machine learning contexts, bridging classic information theory with task-driven representation learning.", "tldr": "", "keywords": ["information theory", "learnable compression", "disentanglement", "learning theory", "representation learning", "computer vision", "neural networks"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0f7a22b170d83f33a7561eac72655c6e1044666.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a theoretically grounded and empirically validated framework that connects Gray–Wyner information theory with modern learnable architectures. While the architectural novelty is moderate and writing can be improved, the theoretical novelty, empirical consistency, and conceptual clarity make it a strong contribution."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe theoretical analysis appears rigorous and well-presented. I did not identify mistakes or inconsistencies in the proofs of Theorem 1 and Theorem 2.\n2.\tAddressing the challenge of distilling shared information in multi-task learning is both important and underexplored. The proposed learnable Gray-Wyner Network provides a natural and conceptually clear solution, supported by solid theoretical justification and empirical evidence.\n3.\tI appreciate the authors’ effort to draw inspiration from classical information-theoretic principles (i.e., the Gray-Wyner framework) and adapt them to modern machine learning contexts.\n4.\tThe empirical results are consistent with the theoretical analysis and convincingly demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1.\tMy main concern lies in the presentation of the paper. The notations are dense and difficult to follow, particularly in Sections 2.1 and 3.1. The motivation behind the proposed method is somewhat unclear. Reorganizing the theoretical sections with more intuitive explanations and a clearer logical flow would significantly improve readability.\n2.\tAlthough the method is designed to “disentangle” shared information, the paper lacks visualization or analysis to demonstrate that the learned representations indeed capture task-common semantics.\n3.\tIt would be beneficial to include error bars in the experimental results to reflect the variance and enhance the reliability of the reported performance."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "g1bd6BMHQK", "forum": "v05SW2X3IC", "replyto": "v05SW2X3IC", "signatures": ["ICLR.cc/2026/Conference/Submission1774/Reviewer_wYFu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1774/Reviewer_wYFu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761538607904, "cdate": 1761538607904, "tmdate": 1762915886136, "mdate": 1762915886136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a learnable three-channel encoder-decoder based on the Gray-Wyner network from information theory, designed to disentangle shared information and task-specific information in multi-task visual scenarios. The method achieves promising results across multiple benchmarks and settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. By analyzing the rate bounds of joint and independent encoding, the method clarifies the feasible conditions for disentangling shared information.  \n2. A Lagrangian-relaxed optimization objective is designed, allowing the amount of information in the shared channel to be dynamically controlled by the hyperparameter β, supporting a continuous trade-off from transmission-rate optimal (β=1) to reception-rate optimal (β=2).  \n3. Shared encoding layers extract common information, which is then split into private channels, ensuring representation compatibility while reducing redundancy.  \n4. The method supports direct integration with pre-trained task models, such as DeepLabV3+ and Faster R-CNN, achieving efficient encoding without the need to fine-tune the task networks."}, "weaknesses": {"value": "1. Theorem 1 provides upper and lower bounds on the amount of shared information, but it is not specified how these bounds can be approached in a practical learning framework. Does this require specific architectural constraints or training mechanisms to ensure tightness of the bounds?  \n2. The method relies on a Markov assumption that “each source contains no exclusive information that benefits non-corresponding tasks” (Section 2.1). In real-world multi-task learning, cross-task information transfer is often beneficial. This assumption may be overly strict, potentially limiting the applicability of the method.  \n3. The entropy model for private channels uses the shared channel as context (Section 3.3), which may lead to leakage of shared information into private channels. Are there mechanisms in place to ensure the purity of information separation?  \n4. The experimental tasks are relatively simple (classification, regression, segmentation). It remains unclear whether the method remains effective for tasks requiring fine-grained spatial reasoning (e.g., instance segmentation) tasks."}, "questions": {"value": "See the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eY7w5S5OHv", "forum": "v05SW2X3IC", "replyto": "v05SW2X3IC", "signatures": ["ICLR.cc/2026/Conference/Submission1774/Reviewer_czUz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1774/Reviewer_czUz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562444443, "cdate": 1761562444443, "tmdate": 1762915885955, "mdate": 1762915885955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the Gray–Wyner information theory framework into a learnable neural network for efficient multi-task and multi-view compression. It introduces a neural Gray–Wyner network that disentangles data into a common channel (shared information) and private channels (task-specific details), allowing controllable trade-offs between joint transmission efficiency and individual task performance. The authors derive new lossy common information bounds, connect them to Wyner’s and Gács–Körner’s classical measures, and design a trainable objective that optimizes this trade-off via a single parameter. THey experiment on synthetic data, a colorized version of MNIST,  and on real datasets including COCO and cityscapes. They show that the proposed approach achieves significant compression gains and better representation sharing compared to independent or naive joint codecs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Nice theoretical generalization of Grey Wyner coding to lossy compression and deep function approximators\n- Nice connection between multi-task representation learning and information theory\n- The authors Experiments on real world datasets like COCO and Cityscapes is appreciated and nicely complement the theoretical analysis\n- Thorough theoretical analysis and high quality proofs in a detailed appendix\n- Thorough reporting of architectures and parameters used"}, "weaknesses": {"value": "- A bit dense and difficult to follow, assumes alot of prereqs in info theory, perhaps consider adding an appendix of preliminaries to help readers from outside the direct field of GWC understand why this is meaningful and useful\n- Though they compare against some baseline codes (joint and independant) the work exists largely outside the realm of modern computer vision representation learning and multi-task benchmarks. Some of this can be forgiven because its largely a theory paper, but some connection or comparsion to other representation learning schemes out there in the literature might make this paper more relevant for the community.\n- Its unclear whether these experiments are dependant on the particulars of thenetwork architecure used, as its a bespoke set of networks and not a backbone that appears elsewhere, also not using modern tools like transformers hinders some of the relevance of the experiments. \n\n- nit: overlapping text in Theorem 1 like 175 + 176  D_1 overlaps with Z1 hat\n- Willing to improve my score"}, "questions": {"value": "It might be valuable for the community to have a better \"wrap-up\" of the paper. Its hard to see what this contributions enables in the space of deep learning practice that wasnt already possible with a less principled method. Does this enable predicting something fundamentally new or solving a new class of problems? Or is it just a theoretical refinement and crystalization of what practitioners have been heuristically doing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zSRTJg77k4", "forum": "v05SW2X3IC", "replyto": "v05SW2X3IC", "signatures": ["ICLR.cc/2026/Conference/Submission1774/Reviewer_nk8B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1774/Reviewer_nk8B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871675372, "cdate": 1761871675372, "tmdate": 1762915885391, "mdate": 1762915885391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}