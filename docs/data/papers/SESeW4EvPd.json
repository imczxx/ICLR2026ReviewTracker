{"id": "SESeW4EvPd", "number": 2984, "cdate": 1757312177472, "mdate": 1763004759902, "content": {"title": "Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation", "abstract": "Enabling robots to perform precise and generalized manipulation in unstructured environments remains a fundamental challenge in embodied AI. While Vision-Language Models (VLMs) have demonstrated remarkable capabilities in semantic reasoning and task planning, a significant gap persists between their high-level understanding and the precise physical execution required for real-world manipulation. To bridge this “semantic-to-physical” gap, we introduce GRACE, a novel framework that grounds VLM-based reasoning through executable analytic concepts (EAC)—mathematically defined blueprints that encode object affordances, geometric constraints, and semantics of manipulation. Our approach integrates a structured policy scaffolding pipeline that turn natural language instructions and visual information into an instantiated EAC, from which we derive grasp poses, force directions and plan physically feasible motion trajectory for robot execution. GRACE thus provides a unified and interpretable interface between high-level instruction understanding and low-level robot control, effectively enabling precise and generalizable manipulation through semantic-physical grounding. Extensive experiments demonstrate that GRACE achieves strong zero-shot generalization across a variety of articulated objects in both simulated and real-world environments, without requiring task-specific training.", "tldr": "", "keywords": ["Vision-Language-Model; Robotic Manipulation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/176fc9e9cea49b8a7d047aceca6efb524abae2d8.pdf", "supplementary_material": "/attachment/84ee505d19cded4ecbb75adfddcbc7dc80080cac.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a VLM-based framework for robot manipulation of articulated objects, that aims to balance generalization and precision. It proposes the \"executable analytic concept\" that represents an object's geometry, structure, and manipulation blueprint. Based on this, it leverages the VLM to parse high-level task instructions in natural languages into executable action plans related to the object."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a good summary of how VLMs are currently used in robot learning, and points out their struggles in balancing generalization and knowledge of real-world physics/dynamics/motion (Introduction paragraph 2-3). These high-level discussions sound very reasonable and provide a good motivation for the work.\n- It proposes a practical framework, with improved performance in several tasks."}, "weaknesses": {"value": "**Writing**\n\nTo my feeling, the writing focuses too much on high-level storytelling, but misses too many problem details. I think making these things clear is particularly important, so that there could be a clear boundary for what problems this work can solve and what problems it cannot -- given that generalization is a main claim in this paper.\nConcretely:\n\n*Section 3 Analytical Concepts*\n\nI think this section is important as the \"analytical concept\" is a core term proposed in this paper. However, I feel Section 3 fails to give a clear enough definition of it. The only concrete description of it is probably Figure 1. However, although Figure 1 provides a straightforward explanation of what it looks like, I think it doesn't actually give a clear term definition. What's missing:\n\t- What are exactly the components and variables in \"executable analytic concepts\"?\n\t- What are the problem settings it can model? (And what settings can it not model?) For example: In its geometric component, can it model freeform shapes represented as triangle meshes, or it has to be parametric shapes? In its manipulation component, what actions can be modeled, in addition to pull/push? The force has to be point-based (so that it's mostly for a parallel gripper instead of a dexterous hand)?\n\n*Section 4 Methodology -- Problem Formulation*\n\nSimilarly, I feel the problem formulation part is also not clear enough. For example:\n\t- It mentions that the important problem of \"bridging the gap between high-level human commands and low-level physical actions\". I understand that the \"high-level human commands\" mean the language instruction $l$ (which can be both arbitrarily long-horizon and under-specified). But what are the \"low-level physical actions\"? -- contact maps, gripper poses, etc.? Is it a single point-based force direction, or is it a motion sequence (with the temporal dimension)?\n\t- It says that the system is given a visual observation $O_t$. The subscript $t$ means time? So the observation is a temporal sequence?\n\n*Section 4 Methodology (4.1-4.3)*\n\nI also have the same feeling when reading the rest of Section 4.\n\nHow is the object parsed? I don't see a separate prompt, but just the task-parsing prompt in Appendix D. Or is object parsing part of this prompt (e.g., line 792-793)? If that's the case, that means the resultant object/part structures are actually **task-dependent**?\n\n\n**Experiments**\n\nIt seems most experiments are just done in simulation environments. I think real-world experiments are important, ideally with some variations in object shapes/appearances/backgrounds, as the paper claims generalization. The objects involved in the sim experiments are mostly common daily-life objects, and with the setups in Section 5.3 (Appendix B), I think more quantitative evaluations on real robots won't take too much effort to conduct."}, "questions": {"value": "See weaknesses. The questions mainly focus on an important point to be clarified: What are the classes of problems that the proposed framework can model and solve? Or in other words, the paper claims \"generalization\", then how \"general\" is the system?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7Rmb5DuBjC", "forum": "SESeW4EvPd", "replyto": "SESeW4EvPd", "signatures": ["ICLR.cc/2026/Conference/Submission2984/Reviewer_jLCA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2984/Reviewer_jLCA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595177304, "cdate": 1761595177304, "tmdate": 1762916480746, "mdate": 1762916480746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "pP13Z05ce2", "forum": "SESeW4EvPd", "replyto": "SESeW4EvPd", "signatures": ["ICLR.cc/2026/Conference/Submission2984/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2984/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763004759125, "cdate": 1763004759125, "tmdate": 1763004759125, "mdate": 1763004759125, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GRACE, a framework designed to bridge the gap between the high-level semantic understanding of Vision-Language Models (VLMs) and the precise, low-level control required for robotic manipulation. The core idea is the introduction of \"Executable Analytic Concepts\" (EACs), a library of pre-defined, mathematically formulated blueprints that encode the geometry, affordances, and manipulation semantics of object parts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well-written and clearly structured. The motivation for bridging the \"semantic-to-physical\" gap is well-articulated, and the proposed GRACE pipeline is explained in a logical sequence."}, "weaknesses": {"value": "1. Over-engineered and Handcrafted Core: The entire framework is critically dependent on a pre-defined, manually curated library of \"analytic concepts\". The paper provides no discussion on the creation, maintenance, or scaling of this library. This approach does not solve the generalization problem but rather shifts it from learning a policy to the laborious, non-scalable task of engineering a comprehensive set of mathematical blueprints for every conceivable object part and interaction. This fundamentally limits the system's applicability in unstructured environments, as it can only handle objects that conform to its pre-existing library. Consequently, the claim of \"zero-shot generalization\" is narrow; the system generalizes to new instances of known concepts, but not to novel object categories or affordances for which a concept has not been manually defined.\n\n2. Limited Novelty in Context of Existing Work: The idea of using structured representations to connect high-level reasoning with low-level control is a cornerstone of robotics research. The paper fails to sufficiently differentiate its approach from a large body of work that seeks to ground language in geometric and physical representations. For example, recent works use foundation models to reason about 3D scenes to generate motion plans or constraints directly, offering a more flexible paradigm than selecting from a rigid library (e.g., VoxPoser, ReKep from Huang et al. 2023, 2024). The proposed \"analytic concepts\" appear to be a hand-engineered analogue to learned representations like Neural Descriptor Fields (Simeonov et al., 2022), which also aim to capture object geometry and affordances in a structured, SE(3)-equivariant manner but do so by learning from data. The paper's dismissal of such data-driven approaches as suffering from \"instability\" is a strong claim that is not substantiated with direct comparative evidence.\n\n3. Brittleness of the Multi-Stage Pipeline: The proposed system is a sequential cascade of multiple, distinct modules: task parsing, object segmentation, concept selection, parameter estimation, and motion planning. Such pipelines are notoriously brittle, as errors from any single stage can propagate and cause catastrophic failure downstream. The authors themselves concede that the primary sources of failure are pose estimation and inverse kinematics, which underscores the challenges of integrating disparate symbolic and geometric components without end-to-end optimization."}, "questions": {"value": "1. Scalability of the Concept Library: Could the authors please provide more detail on the process and effort required to author a new analytic concept? For example, what would be involved in adding a concept for a \"toggle switch\" or a \"spray bottle pump\"? How many concepts were included in the library for the presented experiments, and how were they selected? A deeper understanding of this process is crucial for evaluating the practical scalability of the framework.\n\n2. Justification Over Learning-Based Alternatives: The central thesis is that these analytic concepts are superior to learned representations. Could the authors provide a more direct or theoretical justification for this? For instance, why is this approach preferable to one where a VLM outputs parameters for a learned representation, such as a Neural Descriptor Field (Simeonov et al., 2022) or a keypoint-based model (e.g., kPAM from Manuelli et al., 2019), which might offer greater flexibility in representing non-canonical object shapes?\n\n3. Handling Object Variation and Ambiguity: How does the system handle objects that do not neatly fit any of the pre-defined concepts in the library? For instance, how would it model an artisanal, irregularly shaped drawer handle? Does the parameter estimation module produce a high-error fit, or does the system have a mechanism to report a failure or lack of an appropriate concept?\n\n4. Clarification of \"Zero-Shot\" Learning: The paper states that it performs \"zero-shot generalization\" but also mentions that the system includes \"specialized MLP heads, each regressing a specific structural parameter\" which are trained on a dataset generated from PartNet-Mobility. This suggests the system is trained for a known set of concepts. Could the authors please clarify their definition of \"zero-shot\"? Does it refer to unseen object instances and poses, or to entirely unseen object categories and manipulation skills for which no analytic concept exists?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mvdBFSORt0", "forum": "SESeW4EvPd", "replyto": "SESeW4EvPd", "signatures": ["ICLR.cc/2026/Conference/Submission2984/Reviewer_FJ3j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2984/Reviewer_FJ3j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943774886, "cdate": 1761943774886, "tmdate": 1762916480529, "mdate": 1762916480529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes “Analytical Concepts”, an intermediate representation inferred by VLMs which can be leveraged to generate motion trajectories for manipulation without task demonstrations. It is mainly composed of three parts: geometric concept assets (for constructing geometric primitives), structural blueprint (for composing the geometric primitives to construct object instances using visual observations), and manipulation blueprint (for sampling grasp poses and post-grasp trajectories). The method is evaluated in both simulation (SimplerEnv and SAPIEN) and real world."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It is an appealing high-level idea to construct object-centric concepts for manipulation and to use VLMs for inferring these concepts using visual observations\n- The empirical results show that the proposed approach is competitive compared to baselines often trained on in-domain demonstration data."}, "weaknesses": {"value": "- While it’s an appealing high-level idea, the specific method introduced in this work appears to be very specific to articulated objects. For example, the whole process of “geometric concept assets” and “structural blueprint”, albeit with their novel names, seems to be equivalent to the common procedure used for 3D reconstruction for articulated objects. For this purpose, many prior works have explored using VLMs in this context (such as https://xiahongchi.github.io/DRAWER/). Furthermore, this process appears to be not meaningful at all for rigid objects (or can be considered equivalent to 3D mesh reconstruction too). It’s unclear the specific novelty or benefits of the proposed approach (apart from it’s using VLMs which has been widely explored).\n- The third module (Manipulation Blueprint) is also limited in its scope. It appears to always follow a fixed motion plan: collision-free motion to pregrasp pose → grasping → a post-grasp trajectory with push/pull wrench. While it’s perfectly valid if the scope is limited to only articulated objects, many practical manipulation tasks could involve other motions that are not being addressed, including non-prehensile manipulation, or more sophisicated post-grasp trajectory other than this point-vector parametrization."}, "questions": {"value": "- In the provided demo, a long-horizon task is shown including placing multiple items into a tray. While the paper mentioned that long-horizon task is done by decomposing the instruction into discrete steps, it’s unclear why the robot intentionally places items early on to leave space for objects later using the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bmzGG6fqBd", "forum": "SESeW4EvPd", "replyto": "SESeW4EvPd", "signatures": ["ICLR.cc/2026/Conference/Submission2984/Reviewer_GUzX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2984/Reviewer_GUzX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948271465, "cdate": 1761948271465, "tmdate": 1762916480161, "mdate": 1762916480161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new method that can generate physically executable actions in 3D environments.\n\nThe authors introduce Executable Analytic Concepts (EACs) — mathematically defined blueprints encoding: Object affordances (how something can be acted upon), Geometric constraints (shape, dimensions, layout), Manipulation semantics (e.g., “pull,” “twist,” “push”)\n\nThen the authors introduce GRACE (Grounding Reasoning through Analytic Concept Execution), which integrates EACs into a structured policy pipeline. It shows great performance in both simulation environments and real world experiemtns.\n\nThere are some concerns and weakness of this framework:\n1. It cannot handle challenging structure that cannot be handled by the EAC\n2. All the policies are open-loop, which lack of failure recovery capability compared with VLA models.\n3. Concept library coverage is finite and requires manual design\n4. Performance depends on accurate pose and point-cloud estimation. This multi-stage policy might be bottlenecked by vision models.\n5. Currently limited to two gripper primitives (push/pull), cannot handle cased like dexterous hand manipulation. \n\nAlso, there are many details that cannot be found in either main paper or supp materials, which makes the submission not sound enough."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a new concpet called EAC and the following pipeline, which bridges semantics and physics via mathematical blueprints.\n\n2. EACs are transparent and reusable, which can be used for some specific cased and easy to debug.\n\n3. It might be helpful for zero-shot generalization without task-specific training.\n\n4. This pipeline is explainable, enabling human-readable and explainable robot behavior."}, "weaknesses": {"value": "1. The framework struggles with complex or unconventional object structures that fall outside the representational capacity of the current Executable Analytic Concepts (EACs), reducing its effectiveness on highly irregular or deformable geometries.\n2. All policies in GRACE operate in an open-loop manner, lacking feedback-based correction or failure recovery mechanisms. This limits robustness compared with closed-loop Vision-Language-Action (VLA) models that can adapt dynamically during execution.\n3. The analytic concept library remains finite and relies on manual design, constraining scalability and adaptability to unseen object categories or novel task semantics.\n4. The overall performance heavily depends on accurate pose and point-cloud estimation. Errors propagated from perception modules can significantly degrade downstream policy execution, making the multi-stage pipeline susceptible to bottlenecks in visual grounding.\n5. The current implementation supports only two manipulation primitives—pushing and pulling—and is not yet capable of handling tasks requiring dexterous, multi-fingered hand control or fine-grained contact reasoning.\n\nAlso some details are missing:\n1. please give the full object list and pictures of the real world experiments in the supp materials for sound experiment report.\n2. please show the comparison of the baseline vla and your method, showcase why baseline method fails.\n3. There is not details and pictures showing how the simulation experiments are performed. Please include more details."}, "questions": {"value": "Please check weakness, provide the important details that are missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lcnRm2uKil", "forum": "SESeW4EvPd", "replyto": "SESeW4EvPd", "signatures": ["ICLR.cc/2026/Conference/Submission2984/Reviewer_4MGW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2984/Reviewer_4MGW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073891219, "cdate": 1762073891219, "tmdate": 1762916479653, "mdate": 1762916479653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}