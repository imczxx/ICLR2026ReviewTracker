{"id": "wY6kCiklVK", "number": 2316, "cdate": 1757057521082, "mdate": 1763110185728, "content": {"title": "Contrastive Self-Rewarding MLLM", "abstract": "Direct Preference Optimization (DPO) has been proven effective and efficient in aligning Multi-modal Large Language Models (MLLMs) with human preferences and improving multimodal understanding of MLLMs. However, most existing work relies heavily on either human annotations or auxiliary reward models to construct preference data, which limits their scalability and introduces potential inconsistencies between the reward model and fine-tuned MLLMs. This paper presents ConSR, a Contrastive Self-rewarded Preference Optimization framework that constructs contrastive inputs and frames the variation of the corresponding model outputs as self-reward signals. We perturb the visual input by degrading its fine-grained details and enriching it with semantic context, respectively, forming contrasts with the original visual input. The variation of the corresponding model responses reveals the model’s sensitivity to the visual inputs, which we exploit to rank and construct preference pairs without external supervision. In addition, we reformulate the DPO objective to mitigate its length bias, and reweight visual tokens to assign higher weights to more responsive tokens regarding visual cues. Extensive experiments across multiple visual understanding benchmarks demonstrate ConSR’s consistent superiority across diverse tasks.", "tldr": "We propose contrastive self-rewarding approach that leverage the probablities shift of responses under contrastive inputs as reward to construct prefrence dataset for preference optimization.", "keywords": ["Vision-Language", "Direct Preference Optimization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c69503e8c5766e4d771529cc2c0e5b48a4f61029.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ConSR, a way to align multimodal large language models without using human labels or external reward models. The main idea is to measure how much a model’s answer changes when the input image is either degraded or enriched. If the model reacts in a meaningful way to those changes, that behavior becomes a “self-reward” signal.\nConSR builds preference pairs from those signals and fine-tunes the model with a modified DPO loss that (1) normalizes for response length and (2) gives more weight to visually sensitive tokens.\nExperiments on LLaVA-OneVision and Qwen2.5-VL show clear and consistent improvements on both general multimodal benchmarks and hallucination benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The idea of using the model’s own probability shifts under contrastive inputs as a reward is creative and well motivated. It avoids external judges and keeps everything self-contained.\n2) The improvements are small but steady across tasks, which makes the method look robust.\n3) The experiments are thorough: multiple backbones, diverse benchmarks, plus solid ablations"}, "weaknesses": {"value": "1) The novelty is more in the combination than in each part. Self-rewarding and contrastive cues have both been seen before, so the real contribution lies in putting them together neatly.\n2) Most gains are within 1–2 points, which are meaningful but modest; it would help to show some qualitative or human evaluation to make the improvement more tangible.\n3) A small discussion about where the approach might break would strengthen the paper."}, "questions": {"value": "1)  How large is the overlap between the self-rewarded pairs and those that GPT-4 would consider preferred?\n2) Have you checked whether the method encourages shorter or longer answers beyond what length normalization controls?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DzouEc6l87", "forum": "wY6kCiklVK", "replyto": "wY6kCiklVK", "signatures": ["ICLR.cc/2026/Conference/Submission2316/Reviewer_zhtt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2316/Reviewer_zhtt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767981811, "cdate": 1761767981811, "tmdate": 1762916191172, "mdate": 1762916191172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "nLJKHEoigY", "forum": "wY6kCiklVK", "replyto": "wY6kCiklVK", "signatures": ["ICLR.cc/2026/Conference/Submission2316/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2316/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763110184854, "cdate": 1763110184854, "tmdate": 1763110184854, "mdate": 1763110184854, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a framework named ConSR (Contrastive Self-rewarded Preference Optimization) to address a key limitation in aligning Multimodal Large Language Models (MLLMs): the reliance of Direct Preference Optimization (DPO) on human-annotated or model-based preference datasets.\n\nThe core idea is to create a \"self-rewarding\" mechanism that bypasses the need for external supervision. The framework operates by:\n\n- **Constructing Contrastive Inputs**: Automatically perturbing the visual input by (1) degrading fine-grained details (e.g., blurring) and (2) enriching semantic context (e.g., adding descriptions).\n\n- **Generating a Self-Reward Signal**: The MLLM generates responses for both the original and perturbed inputs. The core hypothesis is that the magnitude of change in the model's response can serve as a reward signal.\n\n- **Constructing Preference Pairs**: This reward signal is used to automatically rank different model outputs, thereby algorithmically constructing preference pairs (e.g., \"this response > that response\").\n\n- **Optimization**: These auto-generated preference pairs are then used to fine-tune the MLLM using DPO. The authors also introduce modifications to the DPO objective to mitigate length bias and re-weight visual tokens."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Valid Motivation**: The paper tackles a significant and well-recognized problem in the field: the high cost and scalability bottleneck of acquiring high-quality preference data for alignment. The goal of creating an unsupervised preference alignment framework is well-motivated.\n\n2. **Technical Additions**: The authors demonstrate an understanding of DPO's limitations by including specific technical modifications, such as the adjustment for length bias and the re-weighting of visual tokens, which go beyond the high-level concept."}, "weaknesses": {"value": "This paper's foundational premise suffers from severe flaws, and its core methodology is questionable.\n\n1. **Fundamental Flaws of the Proxy Reward**: This is the most critical weakness. The paper's central hypothesis—that a model's sensitivity to visual perturbation is a valid proxy for human preference—***is a highly problematic and unsubstantiated leap of faith***.\n\n  - **Misalignment with Human Preference**: The authors provide no compelling evidence that this proxy metric (sensitivity) correlates with what humans actually value (e.g., factual accuracy, logical coherence, safety, or helpfulness). At best, this signal is **insufficiently aligned with human preference**.\n\n  - **Obvious \"Hackability\"**: This proxy reward is **transparently \"hackable\"**. A model could easily learn to \"game\" this signal by \"performing\" sensitivity—for example, by strategically outputting high-variance responses to any perceived perturbation—without making any genuine improvements to its core capabilities. The paper **completely fails to discuss or investigate this critical risk of reward hacking**.\n\n  - **Underexplored \"Black Box\" Signal**: The paper uses perturbation sensitivity as a reward, but **what this signal is *actually* rewarding remains entirely underexplored**. It is a \"reductonist\" approach, simplifying the complex, multi-dimensional nature of human preference into a single, simplistic metric of \"change.\"\n\n  - **\"Echo Chamber\" and Ungrounded Drift**: Using this internal, unexplored signal as a reward creates a classic **\"echo chamber\"**. Without any external, grounded truth as an anchor, the model is guided solely by its own internal artifacts. This will inevitably lead the model to amplify its own biases and flaws, causing it to \"drift\" from factual or useful grounding.\n\n  - **Self-Contradictory Evaluation**: The paper ultimately relies on expensive human evaluation to prove its method's efficacy. This is a **fundamental self-contradiction**: it demonstrates that the authors themselves cannot trust this \"self-rewarding\" signal to be a reliable substitute for human judgment, thereby exposing the signal's inherent unreliability.\n\n2. **Significant Lack of Novelty**: The core mechanism—using the difference in responses or logits between original and perturbed inputs as a reward signal—**is not novel**. This concept has been explored in prior work within the multimodal domain (e.g., POVID [1], VCD [2], SeVA [3], and more recently, VPPO [4], [5]). The paper's contribution appears to be repackaging this known technique and applying it to DPO, which constitutes a very limited methodological advance.\n\n3. **Outdated and Ineffective Baselines**: The paper's primary baseline is DPO. This is an increasingly outdated choice, as the field is rapidly moving towards online and on-policy alignment methods. Comparing against a baseline that no longer represents the state-of-the-art makes the claimed \"improvements\" far less compelling.\n\n4. **Insignificant and Statistically Questionable Improvements**: Many of the reported performance gains are marginal. They could very likely be attributed to statistical noise or normal evaluation variance. The authors fail to provide any statistical significance analysis, leaving the reader to conclude that the method may not offer any tangible, generalizable benefits.\n\n\n\n[1] Aligning Modalities in Vision Large Language Models via Preference Fine-tuning\n\n[2] Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding\n\n[3] Self-Supervised Visual Preference Alignment\n\n[4] SPOTLIGHT ON TOKEN PERCEPTION FOR MULTIMODAL REINFORCEMENT LEARNING\n\n[5] On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models"}, "questions": {"value": "1. **On Reward Hacking**: The proposed proxy reward seems trivially \"hackable.\" Did you conduct any experiments to test for this? For example, does the model learn to simply maximize response variance upon detecting any input perturbation, regardless of its core performance on the task? How can you guarantee this signal is not being exploited?\n\n2. **On Reward Justification**: Your core assumption is that perturbation sensitivity correlates with human preference. What evidence supports this beyond the final, indirect benchmark scores? What, exactly, do you believe this reward signal is incentivizing the model to learn?\n\n3. **On Novelty**: The mechanism of using perturbation-based response diffs as a signal has been seen in prior work (e.g., VCD, SeVA). Could you please precisely articulate the novel methodological contribution of ConSR, distinguishing it from these previous applications of a similar concept?\n\n4. **On Statistical Significance**: Many of your reported gains are marginal. Have you performed statistical significance tests (e.g., bootstrap or permutation tests) to confirm that these improvements are real and not simply evaluation noise?\n\n5. **On Baselines**: Why did you choose to compare against DPO, an increasingly outdated baseline, rather than more current state-of-the-art online or on-policy alignment methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KGopNKOwnU", "forum": "wY6kCiklVK", "replyto": "wY6kCiklVK", "signatures": ["ICLR.cc/2026/Conference/Submission2316/Reviewer_4o6s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2316/Reviewer_4o6s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892026487, "cdate": 1761892026487, "tmdate": 1762916190953, "mdate": 1762916190953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the problem of aligning multimodal large language models (MLLMs) without human preference data or external reward models. The key contribution is the Contrastive Self-Rewarding framework (ConSR) with visual perturbations to construct contrastive visual pairs, using differences in the model’s responses as self-supervised reward signals. A modified DPO objective with length normalization and visual-token reweighting is used to improve training stability and multimodal grounding. The paper is clearly written and experimentally evaluated on standard multimodal benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper has good motivation to study an important problem. \n- Introducing visual perturbation as a contrastive mechanism is a creative and intuitive extension of self-rewarding LLM frameworks.\n- The paper is clear, well-structured, and easy to follow, with a good set of benchmark coverage."}, "weaknesses": {"value": "- The paper explores only a limited set of visual editing methods (diffusion-based degradation). More perturbation types or systematic analysis are needed to justify that the method generalizes beyond specific image transformations. Generative methods could be explored to see visual variety and complexity changes in perturbation. Following from the previous point, there are no visual examples or detailed explanations showing how perturbations influence model outputs or why such perturbations help the model learn richer visual semantics. \n\n- The claimed “self-rewarding” property may not generalize beyond these carefully controlled perturbations; it is unclear whether the approach would remain effective on natural or unseen image variations.\n\n- The experiments in the main body of the manuscript only include comparisons with vanilla models as baselines. In Table 10, the authors further present results against other existing methods; however, the proposed approach does not demonstrate significant improvements over these baselines.\n\n- The paper would benefit from a clearer problem formulation and a more precise explanation of which dimensions of VLM performance the proposed approach targets.\n\n- The general framework has been extensively explored in prior work on VLM alignment. From my perspective, this paper mainly applies existing techniques and their minor extension, without introducing substantially new ideas.\n\n- The presentation quality needs improvement to meet academic standards. For example, the paper incorrectly refers to 'probability shift'—the correct concept is 'distribution shift'."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "d3TDQUwyF5", "forum": "wY6kCiklVK", "replyto": "wY6kCiklVK", "signatures": ["ICLR.cc/2026/Conference/Submission2316/Reviewer_A4LB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2316/Reviewer_A4LB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984182186, "cdate": 1761984182186, "tmdate": 1762916190525, "mdate": 1762916190525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ConSR, a contrastive self-rewarding framework for preference optimization in Multi-modal Large Language Models (MLLMs). Instead of relying on external reward models or human annotations, ConSR perturbs visual inputs (via degradation or semantic enhancement) to construct contrastive pairs and derives self-reward signals from model output sensitivity. A length-normalized DPO loss with token-level visual weighting mitigates length bias and enhances fine-grained visual grounding. Experiments on multiple benchmarks show consistent improvements over strong baselines such as LLaVA-OneVision and Qwen2.5-VL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Innovative Self-supervised Alignment: Introduces a creative, annotation-free preference optimization approach, significantly reducing dependency on human or external rewards.\n2. Sound Theoretical Design: The reformulated DPO loss effectively addresses known weaknesses of traditional DPO. Mathematical exposition is clear and reproducible.\n3. Comprehensive Experiments: Evaluations across 11 benchmarks with thorough ablations substantiate all key claims."}, "weaknesses": {"value": "1. Baseline Comparisons Outdated: Lacks direct comparison with the newest self-rewarding or contrastive alignment methods, such as CHiP, AdaViP, MIA-DPO, and MCM-DPO.\n2. Insufficient Analysis of Perturbation Design: Diffusion-based perturbation is empirically best but theoretically underexplained.\n\n\n[1] CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs\n[2] AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization \n[3] MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models\n[4] MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for Alt-text Generation"}, "questions": {"value": "1. How does ConSR scale to architectures with weaker visual grounding or video inputs?\n2. Can the self-reward mechanism lead to pathological or unstable learning behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x7L66v1x2w", "forum": "wY6kCiklVK", "replyto": "wY6kCiklVK", "signatures": ["ICLR.cc/2026/Conference/Submission2316/Reviewer_Pkf9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2316/Reviewer_Pkf9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2316/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762414943684, "cdate": 1762414943684, "tmdate": 1762916190215, "mdate": 1762916190215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}