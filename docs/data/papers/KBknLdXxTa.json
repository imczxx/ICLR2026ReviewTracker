{"id": "KBknLdXxTa", "number": 15956, "cdate": 1758257612764, "mdate": 1759897270895, "content": {"title": "From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics", "abstract": "Large language models now solve many benchmark math problems at near‑expert levels, yet this progress has not fully translated into reliable performance in real‑world applications. We study this gap through contextual mathematical reasoning, where the mathematical core must be formulated from descriptive scenarios.We introduce CORE-MATH, a benchmark that repurposes AIME and MATH-500 problems into two contextual settings: Scenario Grounding (SG), which embeds abstract problems into realistic narratives without increasing reasoning complexity, and Complexity Scaling (CS), which transforms explicit conditions into sub‑problems to capture how constraints often appear in practice. Evaluating 61 proprietary and open‑source models, we observe sharp drops: on average, open‑source models decline by 13 and 34 points on SG and CS, while proprietary models drop by 13 and 20.  Error analysis shows that errors are dominated by incorrect problem formulation, with formulation accuracy declining as original problem difficulty increases. Correct formulation emerges as a prerequisite for success, and its sufficiency improves with model scale, indicating that larger models advance in both understanding and reasoning. Nevertheless, formulation and reasoning remain two complementary bottlenecks that limit contextual mathematical problem solving. Finally, we find that fine‑tuning with scenario data improves performance, whereas formulation‑only training is ineffective. However, performance gaps are only partially alleviated, highlighting contextual mathematical reasoning as a central unsolved challenge for LLMs.", "tldr": "", "keywords": ["Large Language Models", "Mathematical Reasoning", "Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fac7c294888425bc1720c2d8d4464d5f8ef11ab2.pdf", "supplementary_material": "/attachment/0b0de5584c52f398c8ee898a7e94ad3fc256954e.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles a very relevant gap, i.e. models that perform well on abstract math often struggle when the same problems are presented in a realistic narrative. The authors build a benchmark by transforming existing items from AIME and a filtered MATH-500 subset into two contextualized variants. The first, Scenario Grounding (SG), wraps the same core math in a short story while keeping the solution unchanged. The second, Complexity Scaling (CS), conceals some explicit conditions behind small sub-problems, requiring the model to first identify what is needed before solving. They evaluate a large set of models and observe a clear drop from the original to SG and a further drop to CS. They also analyze where errors originate and introduce three formulation metrics (formulation accuracy, necessity, and sufficiency) to distinguish between “setting up the right math” and “doing the math.”. The authors test training strategies and find that simple SFT on scenario-style data gives measurable improvements without harming abstract performance. \n\nI see the contribution as timely and practically important. The benchmark design, although not perfect, addresses a real need: testing whether models can extract the correct structure from a story. The formulation-vs-reasoning framing is helpful for analysis, and the training results give a simple recipe that others can try. The novelty is, however, moderate (a new benchmark plus analysis), but the empirical scope and the focus on formulation make it valuable to the community, provided the evaluation is strengthened."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work isolates a phenomenon that has been widely reported, ie, strong models on abstract math often fail when conditions are embedded in text. The two-variant construction is simple and scalable, and the error analysis is quite illuminating, showing that setting up the math is frequently the key difficulty. I also appreciate that the authors do not stop at measurement, but also include initial training interventions that enhance contextual performance without compromising abstract skills. The paper is mostly clear, and the benchmark seems practical to adopt."}, "weaknesses": {"value": "My main concerns are about measurement validity and fairness. The SG/CS transformations would benefit from multiple independent annotators and a quantitative audit to ensure equivalence and controlled change. The formulation metrics rely on an LLM judge. The human validation sample is eventually too small, and there is no check with a different judge family or a symbolic/exact solver, where feasible. The open-source vs proprietary comparison lacks confidence intervals to assess robustness. Contamination or near-duplicate checks are not presented, which matters for items derived from widely circulated math sets. The SFT pipeline filters scenarios using one solver, which can introduce selection bias and may not generalize."}, "questions": {"value": "1) Could you report inter-annotator agreement and a small difficulty-parity audit for SG, and a clearer rubric for CS to show it does not change the task class? \n2) Would you consider re-scoring a stratified subset with a different judge family, some exact/symbolic checks where possible, and a larger blinded human pool?\n3) It would also help to provide confidence intervals or bootstraps, and to balance sampling across model groups (or clearly separate the summaries). \n4) Please comment on step-level near-duplicate analysis and on the sensitivity of the SFT gains to the choice of solver used for filtering."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "J5LKgQHx7F", "forum": "KBknLdXxTa", "replyto": "KBknLdXxTa", "signatures": ["ICLR.cc/2026/Conference/Submission15956/Reviewer_xYoD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15956/Reviewer_xYoD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396734966, "cdate": 1761396734966, "tmdate": 1762926167669, "mdate": 1762926167669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CORE-MATH, a benchmark for evaluating LLMs on contextual math reasoning. The benchmark converts problems from AIME and MATH-500 into scenario-based narratives and more complex narratives with embedded sub-problems, Complexity Scaling. The authors evaluate 61 and proprietary open-source LLMs and find that performance drops sharply when moving from abstract to contextual settings. Through error analysis and targeted experiments, the main limitation is identified as incorrect problem formulation rather than failures in step-by-step reasoning. The paper also examines how fine-tuning and formulation-specific training can reduce these errors and shows gains, but a large gap remains. The work demonstrates that contextual math reasoning is still an open challenge for current LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The research question is well motivated and the main finding is relevant. The paper provides a useful observation: the authors identify contextual complexity as a general bottleneck that limits current LLMs' reliability on multi-step reasoning.\n\n- The paper provide insightful error analysis. The detailed breakdown of error types (Figure 2) demonstrates that formulation errors predominate across architectures in contextual settings, which is often overlooked in prior work."}, "weaknesses": {"value": "- The paper builds several automatic annotation and categorization steps and relies on an LLM judge with only light human checks. The authors state that they use o1-mini to decide whether a model output is mathematically equivalent to the reference solution, and that manual checks on Qwen3-14B and Qwen3-32B show more than 90% agreement with human judgments. However, the paper does not report the exact sample sizes, selection protocol, prompts, or agreement statistics, e.g., Cohen's kappa, Pearson/Spearman correlation of scores, or confidence intervals. The heavy use of an LLM-as-judge without full validation details becomes a concern.\n\n- The error analysis uses GPT-5 to assign error categories to outputs from other models, but the paper does not provide the prompts or templates that define each category. Also, there is no study of judge bias or stability, e.g., prompt ablations, temperature sweeps, and no human expert evaluation. A small human audit with inter-rater agreement would be useful. \n\n- CORE-MATH draws from AIME-2024, AIME-2025, and a filtered subset of MATH-500, with each original item converted into two variants, SG and CS. This means the total pool starts from a relatively small number of sources. The paper probes robustness by adding two extra SG versions for AIME-2024 and reporting an averaged score, SG Avg@3, but it does not report standard deviation or standard error, and it does not repeat this check for CS or the other subsets. It is not clear whether that the results could be sensitive to specific paraphrases; Another concern is the limited scale in CORE-MATH, which makes results less convincing."}, "questions": {"value": "- The proposed SFT setup helps but does not remove the drop under CS. The paper positions this as a first step, but the current training recipe and analysis do not yet show a clear path to solve the remaining gap. What suggestions do the authors have to further address this research gap, or how would the authors justify the limited improvement through their method?\n\n- Typo: the caption reference \"Figure 4\" should be \"Table 5.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vHPSiR3MIH", "forum": "KBknLdXxTa", "replyto": "KBknLdXxTa", "signatures": ["ICLR.cc/2026/Conference/Submission15956/Reviewer_qQ6S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15956/Reviewer_qQ6S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945148782, "cdate": 1761945148782, "tmdate": 1762926167161, "mdate": 1762926167161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce CORE-MATH, a new benchmark that repurposes problems from established sources like AIME and MATH-500 into more realistic, narrative-driven scenarios. This is done through two variations: \"Scenario Grounding,\" which embeds problems in a narrative, and \"Complexity Scaling,\" which conceals conditions within sub-problems. Through an extensive evaluation of 61 models, the paper demonstrates a significant drop in accuracy on these contextual tasks. The primary cause of failure is identified as \"problem formulation\"—the inability to correctly extract the core mathematical structure from the narrative. The authors find that while model scale helps, it doesn't solve the issue, and that end-to-end fine-tuning on scenario-based data improves performance, whereas training a model solely for formulation is ineffective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work tackles the highly relevant and important gap between benchmark success and practical, real-world capability in LLMs, pushing research beyond abstract problem-solving.\n2. The CORE-MATH benchmark is well-designed. By building upon trusted sources (AIME, MATH-500) and systematically creating two distinct types of contextual challenges (SG and CS), the authors provide a controlled framework for analyzing this problem.\n3. The study is thorough, evaluating a wide array of 46 open-source and 15 proprietary models. The analysis goes beyond simple accuracy metrics, effectively identifying problem formulation as the key bottleneck through both qualitative and quantitative evidence."}, "weaknesses": {"value": "The primary weakness of the work lies in the lack of detail regarding the benchmark construction process, which is critical for ensuring the benchmark's validity and reproducibility. The paper states that an LLM (01-mini) was guided by structured prompts to generate the contextual variants, which were then reviewed by human experts. However, several key aspects unclear:\n1. Why choose o1-mini but not stronger models? Did the authors compare its performance with other frointer LLMs?\n2. What was the protocol for the human expert review to guarantee mathematical equivalence? How many experts reviewed each problem, and what was the procedure for resolving disagreements to ensure the final scenarios were valid?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "crq0B5eh5R", "forum": "KBknLdXxTa", "replyto": "KBknLdXxTa", "signatures": ["ICLR.cc/2026/Conference/Submission15956/Reviewer_YVi6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15956/Reviewer_YVi6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15956/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983557754, "cdate": 1761983557754, "tmdate": 1762926166502, "mdate": 1762926166502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates LLMs’ mathematical reasoning in contextual scenarios, where the underlying math must first be formulated from narrative descriptions before being solved. It introduces CORE-MATH, a benchmark that repurposes AIME and MATH-500 problems into two controlled variants: Scenario Grounding (SG), which embeds problems in realistic narratives without altering the core math, and Complexity Scaling (CS), which hides explicit conditions behind simple sub-problems to mimic how constraints appear in practice.\n\nAcross 61 models, the authors observe substantial accuracy drops from the original abstract problems to SG—and even larger drops on CS—implicating problem formulation as a primary failure mode. Training experiments indicate that fine-tuning on scenario data improves robustness, whereas a formulation-only pipeline is ineffective."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem is interesting, as it focuses on realistic contextualization of mathematical reasoning.\n\nThe paper introduces a new benchmark, CORE-MATH, which is built upon AIME 2024, AIME 2025, and MATH-500 datasets.\n\nThe proposed Scenario Grounding (SG) and Complexity Scaling (CS) strategies are effective for constructing contextual mathematical problems that test both problem formulation and reasoning capabilities."}, "weaknesses": {"value": "Is the mapping from the original problem to the narrative automatic or human-assisted?\n\nHow do the authors ensure that the mapping is accurate? The generated narrative may not exactly match the original mathematical problem, which could alter its meaning.\n\nThe Scenario Grounding (SG) and Complexity Scaling (CS) strategies are used for data construction. How do the authors guarantee that these transformations do not change the underlying problem semantics?\n\nWould it be possible to train models on similar data types so that their performance can be better preserved across contextualized settings?"}, "questions": {"value": "Since AIME and MATH datasets may have already been heavily used or augmented during LLM training, how would the results differ if the authors included other benchmarks, such as MathOdyssey?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XmBkMOj0S5", "forum": "KBknLdXxTa", "replyto": "KBknLdXxTa", "signatures": ["ICLR.cc/2026/Conference/Submission15956/Reviewer_MmP5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15956/Reviewer_MmP5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15956/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763235749133, "cdate": 1763235749133, "tmdate": 1763235827226, "mdate": 1763235827226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}