{"id": "iWEG5F8o9a", "number": 6034, "cdate": 1757951135511, "mdate": 1759897938645, "content": {"title": "ACDC: Adaptive Cloud-Device Collaboration for Efficient and Accurate Semantic Segmentation", "abstract": "Semantic segmentation is a vital task in computer vision with wide-ranging practical applications. Advanced segmentation models achieve high accuracy but face computational challenges for device-based deployment, while lightweight models often produce coarse predictions, potentially losing pixel-level details. To address these limitations, this paper introduces the Adaptive Cloud-Device Collaboration (ACDC) framework, which combines the efficiency of device-side models with the robust capabilities of cloud-side models. ACDC employs an adaptive uncertainty detection mechanism to capture pixel-wise distributional shifts, filtering challenging samples for fine-grained processing on the cloud, and fuses dual-granularity predictions to achieve precise results. The framework comprises three key modules: Device-Aware Adaptive Segmentor (DAS) for coarse segmentation and uncertainty detection using a two-stage Uncertainty Decoupler; Dynamic Cloud Augmentation Module (DCAM) for processing challenging samples and adaptive update; and Collaborative Fusion Engine (CFE) for dual-granularity integration. Extensive experiments demonstrate that ACDC improves segmentation accuracy with minimal data transmission, adapts to dynamic environments, and effectively identifies uncertain samples. Code is provided in the supplementary materials.", "tldr": "", "keywords": ["Cloud-Device Collaboration", "Semantic Segmentation", "Uncertainty Quantification"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6079ae9eeef13aceecd81fb4238e7e978a3c238e.pdf", "supplementary_material": "/attachment/5fabb313cd16bab45148ec46c8e30d04bae0f177.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ACDC, a framework for cloud–device collaborative semantic segmentation that aims to balance accuracy and efficiency. The framework consists of three main components:\n\n(1) A Device-Aware Adaptive Segmentor (DAS), which performs lightweight, coarse segmentation on devices and identifies uncertain samples through a two-stage Uncertainty Decoupler that distinguishes uncertainty due to model limitations and downsampling effects.\n(2) A Dynamic Cloud Augmentation Module (DCAM), which processes the uncertain samples on the cloud, providing fine-grained segmentation and adaptive updates to the device’s uncertainty matrix.\n(3) A Collaborative Fusion Engine (CFE), which merges coarse and fine-grained outputs through a dual-granularity fusion mechanism.\n\nExperiments on ADE20K, Cityscapes, Pascal VOC, and Pascal Context show consistent mIoU improvements compared to cloud-only and prior collaborative baselines (e.g., CDCCA, AMS, CEMA), while reducing latency and FLOPs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The ACDC framework integrates uncertainty estimation, cloud offloading, and multi-level fusion in a coherent architecture. Each module serves a clear functional role, and the overall system is well-motivated by practical deployment constraints.\n2. The two-stage Uncertainty Decoupler introduces a nuanced separation between model and sampling uncertainties. This addresses a genuine gap in existing collaboration approaches, which often use entropy or confidence-based heuristics that are not well-suited to pixel-wise tasks.\n3. The experiments are broad and well-organized, spanning multiple datasets and baselines. Quantitative results are supported by qualitative visualizations (Fig. 3). The framework demonstrates consistent accuracy improvements and measurable reductions in latency and communication load.\n4. The paper is well-written, the motivation is easy to follow, and the figures effectively illustrate complex components (especially Fig. 2)."}, "weaknesses": {"value": "1. While the components (uncertainty estimation, adaptive update, fusion) are each reasonable, the paper primarily integrates established ideas under a unified framework rather than introducing a fundamentally new theoretical concept. The contribution is mostly system-level engineering rather than algorithmic innovation.\n\n2. The proposed “uncertainty matrix” formulation uses contrastive objectives and Frobenius norms, but the intuition behind Eq. (6–9) remains underexplained. It is unclear how well this formulation captures epistemic versus aleatoric uncertainty or how sensitive it is to hyperparameters such as λ, γ, or the kernel width. A more rigorous or probabilistic interpretation would strengthen the contribution.\n\n3. Although the method claims minimal overhead, the paper provides limited quantitative evidence about the communication cost, training time, or memory footprint. Since efficiency is a central claim, a clear runtime–accuracy curve or ablation on upload bandwidth would make the results more convincing.\n\n4. All experiments are conducted on canonical segmentation datasets using a fixed HRNet–SegFormer pair. There is no evaluation under real-world dynamic or continual settings, despite the stated goal of adapting to “dynamic environments.” This weakens the paper’s central claim of adaptivity and practical relevance.\n\n5. The paper reports quantitative gains but provides little qualitative analysis of the uncertainty matrix or the behavior of the filtration rate.\nFor example, visualizing where and why the decoupler flags uncertainty would help validate its conceptual soundness.\n\n6. Competing uncertainty-based or probabilistic collaboration frameworks (e.g., Bayesian or ensemble-based segmentation) are not included in the comparison. Without these, it is difficult to attribute the gains specifically to the proposed uncertainty decoupling mechanism.\n\n7. The choice of HRNet (device) and SegFormer (cloud) seems tuned for complementary behavior. It is unclear whether ACDC’s performance persists if both sides use similar architectures or if the cloud model is only moderately stronger.\n\n\nOverall, the paper tackles a practically important problem—efficient semantic segmentation across device–cloud boundaries—and offers a technically solid, well-tested framework. The system is coherent and reproducible, and the performance gains are empirically verifiable. However, the theoretical underpinnings of the uncertainty modeling remain somewhat shallow, and the adaptivity claims are stronger than what the experiments actually demonstrate. In essence, this is a high-quality system paper with limited conceptual novelty but clear empirical value."}, "questions": {"value": "See weeknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c3TDAn7XC4", "forum": "iWEG5F8o9a", "replyto": "iWEG5F8o9a", "signatures": ["ICLR.cc/2026/Conference/Submission6034/Reviewer_Yb8V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6034/Reviewer_Yb8V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624150865, "cdate": 1761624150865, "tmdate": 1762918423387, "mdate": 1762918423387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Due to the significant disparity in computational resources between cloud and device, the authors propose ACDC to address the challenges of semantic segmentation in cloud-device collaboration, ensuring high segmentation performance while enabling efficient inference. The core concept is to enable device-based models to distinguish between easy and difficult samples, processing simple images rapidly, while the cloud-based model handles complex ones and returns results. Ultimately, the final fusion segmentation outcome is achieved through the transmission of edge information."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method has been validated across multiple datasets."}, "weaknesses": {"value": "1. The key limitations and key challenges in the introduction convey nearly identical meanings.\n\n2. The format of citations is wrong without spacing and parentheses.\n\n3. Excessive font formatting in the introduction, such as **bold**, *italic*, $\\underline{underline}$, and $\\texttt{texttt}$.\n\n4. No corresponding explanation follows the appearance of Eq. (3).、\n\n5. In Section 3.2, the statement “we propose the Device-Aware Adaptive Segmentor (DAS) and the Collaborative Fusion Engine (CFE) to achieve efficient, high-quality segmentation” is ambiguous. “Achieve efficient, high-quality segmentation” refers to the CFE.\n\n6. In Eq. (4), $s$ should be treated as the input to the $f_{down}$ function. What is the upsample function for $\\text{y}_\\text{coarse}$?\n\n7. Why is the last dimension of Eq. (4) denoted as $C$? Shouldn't $C$ represent the input dimension? Additionally, why is the uncertainty matrix also defined as $C \\times C$?\n\n8. What distinguishes $\\text{y}_\\text{coarse}$ from $p(x)$?\n\n9. What do $y$ and $\\hat{y}$ represent, respectively? How do they differ from $\\text{y}_\\text{coarse}$?"}, "questions": {"value": "Due to the numerous ambiguities and unclear points in the writing, I am unable to understand the author's approach accurately. Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l8dRgvvHUU", "forum": "iWEG5F8o9a", "replyto": "iWEG5F8o9a", "signatures": ["ICLR.cc/2026/Conference/Submission6034/Reviewer_mwcQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6034/Reviewer_mwcQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753360069, "cdate": 1761753360069, "tmdate": 1762918422935, "mdate": 1762918422935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces ACDC (Adaptive Cloud-Device Collaboration), a novel framework for efficient and accurate semantic segmentation that integrates device-side and cloud-side processing. The key idea is to leverage adaptive uncertainty detection to decide which samples should be offloaded to the cloud for fine-grained refinement.  Experiments on several semantic segmentation datasets demonstrate consistent improvements in mIoU, reduced latency, and minimized upload bandwidth compared to both device-only and prior cloud-device collaboration baselines. The manuscript also provides thorough ablations and theoretical grounding for the Uncertainty Matrix."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Rationality: The methodology is conceptually sound and well-motivated. The uncertainty modeling and adaptive updates are mathematically justified (with a detailed proof in the appendix).\n2. Novelty: The Uncertainty Matrix effectively models pixel-wise distributional shifts, outperforming prior entropy-based approaches. The DCAM’s weighted update mechanism allows continual adaptation under distribution shifts.\n3. Comprehensive evaluation: Demonstrated improvements across four major datasets with consistent gains in mIoU, latency, and FLOPs. Extensive analysis of filtration rate, module combinations, and distance metrics solidifies the claims that the three modules can be integrated with various segmentation backbones."}, "weaknesses": {"value": "1. Complex training pipeline: The optimization of the Uncertainty Matrix and adaptive updates might hinder reproducibility and real-time deployment feasibility.\n2. Theoretical depth: While the Uncertainty Matrix is intuitive, its probabilistic grounding could be connected more clearly to existing Bayesian or information-theoretic frameworks.\n3. Scope limitation: Focused solely on semantic segmentation; applicability to other pixel-level or multimodal tasks (e.g., depth estimation, panoptic segmentation) is not discussed. Comparisons with recent sparse refinement methods (e.g., SparseRefine) are missing."}, "questions": {"value": "1．How sensitive is the performance to the choice of filtration threshold (τₛ)? Can this threshold be learned dynamically rather than tuned manually?\n2. Could the Uncertainty Matrix be approximated or compressed to reduce its computational footprint on-device?\n3. Is there a feedback delay consideration in DCAM updates, and how does it affect online adaptation?\n4. How would ACDC perform under severe bandwidth constraints or intermittent connectivity?\n5. Could this framework be extended to multimodal segmentation (e.g., RGB + depth)?\n6. Why choose a lightweight convolutional layer in CFE instead of attention mechanisms for fusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cvGsmPZXkM", "forum": "iWEG5F8o9a", "replyto": "iWEG5F8o9a", "signatures": ["ICLR.cc/2026/Conference/Submission6034/Reviewer_Jsem"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6034/Reviewer_Jsem"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806940932, "cdate": 1761806940932, "tmdate": 1762918422643, "mdate": 1762918422643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ACDC, a cloud-device collaboration framework for semantic segmentation that balances on-device efficiency with cloud-based accuracy. It uses a lightweight device model for fast, coarse predictions and a heavyweight cloud model for high-accuracy processing.\nThe core of ACDC is a two-stage uncertainty decoupler that identifies samples from shifted distributions or in general more challenging ones and filters what to send to the cloud:\n\n(1) sample-level uncertainty: a learnable uncertainty matrix is proposed for identifying challenging samples (due to the limited capacity of the model ) and offloads them entirely.\n\n(2) pixel-level uncertainty: a standard per-pixel entropy check identifies uncertain pixels (due to downsampling) for local, on-device refinement.\n\nThe ACDC framework is completed by a Dynamic Cloud Augmentation Module (DCAM), which adaptively updates the device's uncertainty matrix to refine it on wrong predicted classes (as estimated by difference in pseudo-labels) and sends it back to the local device, and a Collaborative Fusion Engine (CFE), which efficiently fuses the coarse device mask with the fine-grained cloud mask. \n\nExperiments on 4 semantic segmentation datasets (ADE20K, Cityscapes, Pascal VOC, Pascal Context) show ACDC outperforms baselines in mIoU while minimizing data transmission."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Significance**\n- This work addresses a practical and important problem: enabling high-accuracy segmentation on resource-constrained devices. The cloud-device collaboration paradigm is a pragmatic and efficient solution for real-world applications where latency and local computational resources are critical bottlenecks.\n\n\n**Originality**\n- The Uncertainty Decoupler that separates uncertainty into \"model capability\" (sample-level) vs. \"information loss\" (pixel-level) seems novel to me.\n- The uncertainty matrix to estimate per image difficulty for a semantic segmentation model is interesting and novel\n\n**Quality**\n- The framework is validated on four standard benchmarks (ADE20K, Cityscapes, Pascal VOC, Pascal Context).\n- The authors report performance, latency and FLOPs gains, that give an interesting trade-off in the 10% filtration rate"}, "weaknesses": {"value": "**Unclear core contribution**\n- I find that the paper's core contribution, the uncertainty matrix and its associated loss function (Eq. 4-9), is mathematically dense and poorly motivated\n- The paper does not provide a clear, intuitive derivation for why the score function $\\mathcal{S}_{U}(x)$ (Eq. 7) or the smoothing function $\\mathcal{Q}(z)$ (Eq. 8) are correct formulations for this problem. They are presented without justification, making it difficult to understand what the learned matrix U is actually modeling. There is zoom-in on their form in Appendix A.4.1 but essentially details the equations and puts them together\n- For the final uncertainty score $u(x;\\mathcal{U}) = tr(q(x)\\mathcal{U}q(x)^{\\top})$ (Eq. 9) the paper just states it \"reflects the uncertainty\" without explaining why this specific metric is superior to simpler, well-understood metrics like Mahalanobis distance, which is also designed to measure distributional shifts. The ablation in Table 7 is insufficient as it doesn't explain why the proposed metric works better and the gaps with Mahalanobis are small.\n\n**Limited related work**\n- When reading the description of the uncertainty decoupler and how it aims to separate uncertainty from model limitation and information loss/ambiguity from downsampling one can easily get the feeling of \"reinventing the wheel\" from uncertainty estimation literature [a], [b], [c]. We already have a large body of works in formalizing and naming the types of uncertainty (epistemic/knowledge, aleatoric/data) and it's not clear why the paper prefers going on its own way with the definitions and the formalism.\n\n\n**Limited evaluation**\n- the paper mentions potential OOD images, that lie outside the training distribution, when motivating their method. However all experiments are conducted in in-distribution mode. The utility of uncertainty is more obvious in conditions of distribution shift, in particular on real-world applications such as this one proposed here.\n- I would recommend looking at different forms of distribution shift and there are lots of  ready to use variants compatible with Cityscapes that can be used: Cityscapes-C (with image corruptions) [d],[e], ACDC [f]  (with different weather conditions: snow, rain, night, fog), BRAVO [g] (with different shifts of distribution and perturbations).\n- Given that the method assumes that a large model can run on the cloud, it would be interesting to explore heavier backbones or uncertainty estimation methods on that side, in particular since this can improve the performance locally, by updating the uncertainty matrix. Currently only one backbone is considered for local and cloud.\n- Also it would be interesting to know what is the upper bound for this setting and how far/close is the current method from that. To this end the results of the cloud-only variant could be reported.\n\n\n**Small performance gains**\n- The proposed uncertainty decoupler module to select uncertain samples shows modest improvements compared to the simple entropy baseline (Table 2), between +0.15 (on Pascal) and +0.64 at best (on Cityscapes). As the evaluation does not containt shifted distribution samples it's difficult to say whether this is due to the limited difficulty of the data or the limited efficacy of the proposed module\n- Similarly, the adaptive update mechanism shows limited performance gains, in particular on the 5% filtration rate ((+0.08 to +0.17), where one would expect that the most difficult samples to be dealt with and the biggest boosts. On 10% and 20% the relative improvements area bit better.\n\n\n**Clarity**\n- It's not clear why the trace (Eq. 9) reflects the uncertainty\n- It's not clear how the baselines in Table 2 entropy and CEMA were computed, as they were originally proposed for image classification.\n\n\n**[Minor] Related work**\n- The use of class prototypes for estimation of distribution shift is slightly related with some methods on deterministic uncertainty estimation [h], [i]\n\n\n**[Minor] Misc.**\n- With natbib please use $\\citep$ accordingly for the references\n- The use of $c$ subscript is confusing as it's used for \"cloud\", \"coarse\"\n- K and C are both used for the number of classes\n- In Figure 1 the dataset on which the scores were computed is not mentioned\n- The layout and organization of the paper could be improved: the tables with results arrive way ahead of the experiments section\n- Figure 3 would need a column with the ground truth labels\n- The ACDC names of the method clashes with the dataset with the same name that is quite established in this area [f]\n\n\n**References:**\n\n[a] Kendall & Gal, What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?, NeurIPS 2017\n\n[b] Hullermeier & Waegeman, Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods, ML 2021\n\n[c] Seng et al., Reliable classification: Learning classifiers that distinguish aleatoric and epistemic uncertainty, IS 2014\n\n[d] Hendrycks & Dietterich, Benchmarking Neural Network Robustness to Common Corruptions and Perturbations, ICLR 2019\n\n[e] Franchi et al., Robust Semantic Segmentation with Superpixel-Mix, BMVC 2021\n\n[f] Sakaridis et al., ACDC: The Adverse Conditions Dataset with Correspondences for Robust Semantic Driving Scene Perception, ICCV 2021\n\n[g] Vu et al., The BRAVO Semantic Segmentation Challenge Results in UNCV2024, ECCV Workshops 2024\n\n\n[h] Macedo et al., Entropic Out-of-Distribution Detection, ICNN 2021\n\n[i] Franchi et al., Latent Discriminant deterministic Uncertainty, ECCV 2022"}, "questions": {"value": "This paper takes an interesting direction of study: a cloud-device collaboration framework for semantic segmentation that balances on-device efficiency with cloud-based accuracy. The idea itself make sense and the authors identify multiple of the good ingredients for such a solution: uncertainty estimation and decoupling, cloud-level prediction and refinement, local update and fusion of predictions. However I do have several concerns regarding the limited experiments, essentially all conducted in-distribution, and the limited performance gains, while using a fairly complex approach.\n\nMy current rating is leaning towards reject at this time, but I'm looking forward for the rebuttal.\n\nHere are a few questions and suggestions that could be potentially addressed in the rebuttal or in future versions of this work (please note that suggested experiments are not necessarily expected to be conducted for the rebuttal):\n\n1. Consider adding some results with data actually shifted from the train distribution, e.g., Cityscapes-C, ACDC, BRAVO subsets, etc.\n\n2. Add discussion on why the trace is a good proxy for uncertainty and in general a discussion on the reasoning of the proposed uncertainty estimation strategy.\n\n3. Add description of the implementation of the Entropy and CEMA baselines from Table 2.\n\n4. Add discussion on the relationship between ACDC and different types of uncertainty (epistemic, aleatoric)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hXFG9wF6mX", "forum": "iWEG5F8o9a", "replyto": "iWEG5F8o9a", "signatures": ["ICLR.cc/2026/Conference/Submission6034/Reviewer_yiy7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6034/Reviewer_yiy7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762199862487, "cdate": 1762199862487, "tmdate": 1762918422037, "mdate": 1762918422037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}