{"id": "Dxb6gBJHby", "number": 15007, "cdate": 1758246772019, "mdate": 1759897335953, "content": {"title": "LearnPruner: Rethinking Attention-based Token Pruning in Vision Language Models", "abstract": "Vision-Language Models (VLMs) have recently demonstrated remarkable capabilities in visual understanding and reasoning, but they also impose significant computational burdens due to long visual sequence inputs. Recent works address this issue by pruning unimportant visual tokens, achieving substantial computational reduction while maintaining model performance. The core of token pruning lies in determining token importance, with current approaches primarily relying on attention scores from vision encoders or Large Language Models (LLMs). \nIn this paper, we analyze the effectiveness of attention mechanisms in both vision encoders and LLMs. We find that vision encoders suffer from attention sink, leading to poor focus on informative foreground regions, while in LLMs, although prior studies have identified attention bias toward token positions, text-to-vision attention demonstrates resistance to this bias and enables effective pruning guidance in middle layers. \nBased on these observations, we propose $\\textbf{LearnPruner}$, a two-stage token pruning framework that first removes redundant vision tokens via a learnable pruning module after the vision encoder, then retains only task-relevant tokens in the LLM's middle layer. \nExperimental results show that our LearnPruner can preserve approximately 95\\% of the original performance while using only 5.5\\% of vision tokens, and achieve 3.2$\\times$ inference acceleration, demonstrating a superior accuracy-efficiency trade-off.", "tldr": "", "keywords": ["Vision-Language Model", "Token Reduction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ede456ab4d91775fa13d17dd66aae9c490126151.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a two-stage pruning framework designed to make VLM inference more efficient while preserving accuracy. The first stage employs a learnable module after the vision encoder to estimate and discard redundant visual tokens, and the second stage applies a text-guided pruning process in the LLM’s middle layers, selectively retaining only tokens that are relevant to the query through text-to-vision attention."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This study offers an insightful examination of the attention behavior in both the vision encoder and the LLM, pointing out the shortcomings of [CLS]-based attention and demonstrating how LLM attention can be effectively exploited for token selection.\n- Unlike prior works relying on attention heuristics ([CLS] or average attention), the learnable pruning module introduces a differentiable and data-driven importance predictor.\n- The clear organization and presentation make the paper easy to follow."}, "weaknesses": {"value": "- The concept of the learnable pruning module appears highly similar to the learnable image-token predictor proposed in Dynamic-LLaVA (ICLR 2025), which leverages image token features to predict a binary mask for selecting tokens during prefill computation. However, the current work does not include a proper comparison or discussion of this prior work. It would considerably strengthen the paper to include Dynamic-LLaVA as a baseline in the experiments and to explicitly discuss methodological differences and advantages, thereby clarifying the novelty and demonstrating the superiority of the proposed approach.\n  * https://openreview.net/forum?id=hzVpZDrW73\n\n- The paper briefly mentions a diversity-based token selection module to preserve background information during inference. However, the description of this component is insufficient and does not clearly explain how the module is implemented. Moreover, I was not able to find an ablation study quantifying its contribution, making it difficult to assess the actual impact of the diversity-based selection compared to the LPM.\n\n- Diversity-based vision token pruning methods are not properly discussed or compared. It would strengthen the paper to include these approaches as baselines in the comparison experiments to better demonstrate the superiority of the proposed method.\n  * DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models ( https://arxiv.org/abs/2503.02175 )\n  * DART: Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More ( https://arxiv.org/abs/2502.11494 )\n\n- I am concerned about the practical applicability of the proposed method in conjunction with modern high-performance inference engines and optimization techniques. The use of middle-layer text-to-vision attention in the second stage may not be directly compatible with implementations such as FlashAttention. Furthermore, introducing additional parameters for the LPM in the first stage and performing token pruning within intermediate layers in the second stage could require substantial implementation effort and modification for popular inference frameworks such as vLLM or TensorRT. It would be beneficial for the authors to discuss these integration challenges and clarify how their method can be efficiently deployed in real-world inference systems."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iZgszYLF2f", "forum": "Dxb6gBJHby", "replyto": "Dxb6gBJHby", "signatures": ["ICLR.cc/2026/Conference/Submission15007/Reviewer_ui91"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15007/Reviewer_ui91"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761397564772, "cdate": 1761397564772, "tmdate": 1762925337412, "mdate": 1762925337412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes LearnPruner, a two-stage pruning approach for VLMs. The proposed approach first removes redundant vision tokens with a learnable pruning module after the vision encoder, and then retains only task-relevant tokens in the LLM's middle layer. These are based on some analyses on the VLM's attention patterns: in visual encoder, [CLS] fails to adequately attend to salient foreground objects, while in LLM, text-to-vision attention can provide reliable guidance for token selection. Experiments with multiple models and a variety of datasets show the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The direction of making VLMs more efficiency is important for its real-world deployment.\n- The method is well-motivated by preliminary analyses.\n- The proposed approach is shown to obtain good performance."}, "weaknesses": {"value": "- It is unclear how well the Learnable Pruning Module can generalize to different tasks, that may be very different to the ones that have been seen in the training of the module.\n- The ablation study seems a little thin, where it would be better if more settings can be investigated. For example, what will be a good ratio of the compression of the vision encoder and the LLM? (The ratio is set to 3, but how would the overall efficiency be influenced if we have other settings?)"}, "questions": {"value": "- I'm wondering how some of the hyper-parameters (such as k=12-th layer of the LLM for the second stage) are selected? Will these settings be model- or benchmark-sensitive?\n- The approach seems to be query sensitive, if we would like to perform pruning in a query-agnostic way, will the proposed approach be adaptable to this setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VgrMzJiuHK", "forum": "Dxb6gBJHby", "replyto": "Dxb6gBJHby", "signatures": ["ICLR.cc/2026/Conference/Submission15007/Reviewer_FyhA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15007/Reviewer_FyhA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914206375, "cdate": 1761914206375, "tmdate": 1762925337000, "mdate": 1762925337000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LearnPruner is a two-stage token pruning framework for Vision-Language Models. It first uses a learnable module to remove redundant vision tokens, then prunes text-irrelevant tokens in LLM middle layers. It preserves 95% performance with only 5.5% tokens, achieving 3.2× faster inference and superior accuracy-efficiency trade-off."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is clear and easy to follow.\n2. Experimental results show strong performance on both LLaVA-1.5 and LLaVA-Next."}, "weaknesses": {"value": "1. Experiments on LLaVA-1.5 and LLaVA-Next provide limited persuasiveness; please include comparisons on Qwen2.5-VL.\n2. The design of Remove Text-Irrelevant Content does not appear novel, it seems to be a straightforward reuse of existing ideas.\n3. Additional training is required compared with training-free methods."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n4rzKZE1jV", "forum": "Dxb6gBJHby", "replyto": "Dxb6gBJHby", "signatures": ["ICLR.cc/2026/Conference/Submission15007/Reviewer_uDiD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15007/Reviewer_uDiD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937575818, "cdate": 1761937575818, "tmdate": 1762925336320, "mdate": 1762925336320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a new token pruning pipeline, LearnPruner, with a learnable module to determine token importance. Despite the prior concern with the bias in vision-to-vision attention, the proposed method leverage the more robust text-to-vision attention for the second stage token pruning. In the experiments, the proposed pruning framework is able to better preserve accuracy compared many prior baselines while providing similar benefit in computational cost reduction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The learnable module provides a more robust way to acquire the visual token importance.\n- The overall performance is improved compared to prior baselines."}, "weaknesses": {"value": "- There is no failure case analysis.\n- Ablation study on the LearnPruner training: what will happen if more or fewer training data samples are used?\n- Extra ablation study on the diversity-based token selection is needed.\n- The use of text-to-vision attention remain a bottleneck in the pruning process. The challenge of attention bias and drifting is not tackled with."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DnnCDNyTUR", "forum": "Dxb6gBJHby", "replyto": "Dxb6gBJHby", "signatures": ["ICLR.cc/2026/Conference/Submission15007/Reviewer_iSWN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15007/Reviewer_iSWN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962308769, "cdate": 1761962308769, "tmdate": 1762925334276, "mdate": 1762925334276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}