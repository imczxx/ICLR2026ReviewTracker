{"id": "GMxQHTyO2T", "number": 15488, "cdate": 1758251841779, "mdate": 1763686489718, "content": {"title": "Prior-aware and Context-guided Group Sampling for Active Probabilistic Subsampling", "abstract": "Subsampling significantly reduces the number of measurements, thereby streamlining data processing and transfer overhead, and shortening acquisition time across diverse real-world applications. The recently introduced Active Deep Probabilistic Subsampling (A-DPS) approach jointly optimizes both the subsampling pattern and the downstream task model, enabling instance- and subject-specific sampling trajectories and effective adaptation to new data at inference time. However, this approach does not to fully leverage valuable dataset priors and relies on top-1 sampling, which can impedes the optimization process. Herein, we enhance A-DPS by integrating a deterministic (fixed) prior-informed sampling pattern derived from the training dataset, along with group-based sampling via top-k sampling, to achieve more robust optimization—method we call Prior-aware and context-guided Group-based Active DPS (PGA-DPS). We also provide a theoretical analysis supporting improved optimization via group sampling, and validate this with empirical results. We evaluated PGA-DPS on three tasks: classification, image reconstruction, and segmentation, using the MNIST, CIFAR-10, fastMRI knee, and hyperspectral AeroRIT datasets, respectively. In every case, PGA-DPS outperformed A-DPS, DPS, and all other sampling methods.", "tldr": "", "keywords": ["Subsampling", "Active acquisition", "Accelerated MRI", "Hyperspectral imaging", "Top-k sampling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7fb4c8cf07ce714a9ab465811c5e5e654c8e367d.pdf", "supplementary_material": "/attachment/3e6a8a1c03c882e3615353377a82acf5a084e651.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Prior-aware and Context-guided Group-based Active Deep Probabilistic Subsampling (PGA-DPS), which enhances Active Deep Probabilistic Subsampling (A-DPS) by incorporating two key innovations: (1) integrating deterministic prior-informed sampling patterns derived from training data statistics with instance-specific active sampling; (2) introducing DPS-top-k group sampling to replace sequential top-1 sampling. The authors provide theoretical analysis showing that group sampling achieves smaller effective Lipschitz constants, leading to smoother optimization landscapes. Experiments on MNIST classification, fastMRI reconstruction, and hyperspectral segmentation demonstrate that PGA-DPS outperforms existing sampling methods across all evaluation metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The dual sampling strategy combining deterministic priors with adaptive active sampling is well-motivated and novel. While group sampling is not entirely new, its application and theoretical analysis in this context show originality.\n\n2. The method is technically sound with solid implementation. The theoretical analysis (Theorem 1 on Lipschitz constants) provides valuable insights into optimization stability. The experimental design covers three diverse domains, demonstrating method generality.\n\n3. The paper is well-structured with clear method description and reasonable figure presentations."}, "weaknesses": {"value": "1. Questionable practical relevance: The MNIST pixel-level sampling has limited real-world significance as pixel acquisition costs are identical and downsampling doesn't provide substantial practical benefits. For MRI and HSI tasks, while acceleration is mentioned, detailed time cost analysis and end-to-end system evaluation are missing.\n\n2. Incomplete experimental evaluation: All experiments lack 100% data sampling baselines, making it impossible to assess performance degradation at different sampling rates. This significantly undermines the assessment of the method's practical value.\n\n3. Limited dataset scale: MNIST is too simple to validate method scalability. With sufficient computational resources, evaluation on CIFAR-10/100 or larger datasets would strengthen the claims. While fastMRI and AeroRIT are more realistic, their scales remain relatively limited."}, "questions": {"value": "1. Annotation dependency of prior sampling: Prior sampling requires complete training data annotations. How is this handled in annotation-scarce scenarios? How to update prior sampling patterns when new data continuously arrives?\n\n2. Hyperparameter sensitivity: The (Ps, As) ratios vary significantly across tasks. Is there an automatic selection strategy?\n\nOthers can be seen in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RojHDmrKVq", "forum": "GMxQHTyO2T", "replyto": "GMxQHTyO2T", "signatures": ["ICLR.cc/2026/Conference/Submission15488/Reviewer_9EHq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15488/Reviewer_9EHq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761380392192, "cdate": 1761380392192, "tmdate": 1762925778918, "mdate": 1762925778918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main idea of this paper is to augment Deep Probabilisitic Subsampling (DPS) for task adaptive subsampling with Active Deep Probabilistic Subsampling method (A-DPS). The main idea behind the DPS method is to use Gumbel-softmax trick for back-propagation to estimate the gradients. \n\nIn this paper, the proposed method first performs the deterministic subsampling from the training data and then performs the A-DPS method for contextually selecting more samples. Also, another modification proposed is to use DPS-K for sampling instead of DPS-1 in the A-DPS step.\n\nEmpirical results on MNIST and MRI datasets show significant improvements over existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Empirical results show that the usefulness of the proposed method. Significant gains on MNIST and MRI reconstruction dataset.\n\n2. The method is benchmarked against a wide range of alternatives—including greedy algorithms, learned masks (LOUPE), RL methods, and spectral selection approaches proposed for other domains."}, "weaknesses": {"value": "1. The core algorithm is a combination of already published ideas: deterministic mask learning (training-set prior as used in DPS and LOUPE), sequential active sampling (A-DPS), and top-k/group sampling (DPS-top-k, Gumbel-top-k) in a straightforward manner.\n\n2. The tuning of how many samples come from the prior mask versus active sampling (Ps/As) is hand-picked for different tasks, and the contributions are hyperparameter-dependent.\n\n3. The prior sampling part is not clear, and the key details are hidden away in appendix."}, "questions": {"value": "1. How does this method compare against Attention guided methods [1]?\n\n2. Expand on section 3.3.  Also, is Theorem 1 a known result or new contribution?\n\n3. How sensitive is the method to the fixed temperature parameter \n\n4. Can the hyperparameters P_s and A_s (proportions of prior and active sampling) be learned during training instead of fixed?\n\n5. How does the prior-informed deterministic mask compare to purely learned or data-driven static masks in terms of interpretability and robustness?\n\n6. How does the choice of task model (classification vs. reconstruction) affect the optimal proportion of prior vs. active samples?\n\n\nReferences:\n\n[1] Shankaranarayana, Sharath M., et al. \"Deep Attention-guided Adaptive Subsampling.\" arXiv preprint arXiv:2510.12376 (2025).\n\nTypos:\n\nLine 017: impedes -> impede\nLine 280: accrucay -> accuracy\nLine 479: DSP-1 -> DPS-1"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NqidESUz2d", "forum": "GMxQHTyO2T", "replyto": "GMxQHTyO2T", "signatures": ["ICLR.cc/2026/Conference/Submission15488/Reviewer_yF5c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15488/Reviewer_yF5c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885667553, "cdate": 1761885667553, "tmdate": 1762925778506, "mdate": 1762925778506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PGA-DPS, a method that integrates a fixed prior sampling mask with group-based active sampling. The proposed approach builds upon DPS and A-DPS. It is evaluated across three tasks: MNIST, MRI reconstruction, and hyperspectral image segmentation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Extensive experiments are conducted across three tasks.\n\n- The paper is well-written and easy to follow.\n\n- The proposed method is supported by theoretical analysis.\n\n- The approach is an effective variant of the existing DPS-based methods."}, "weaknesses": {"value": "- For all experiments, the number of training epochs is fixed according to the implementation details. It is unclear how the performance would change if the best-performing checkpoint were selected instead of the final epoch model.\n\n- In the MRI reconstruction experiments, one setup crops the slices to 208x208, while another uses 320x320 images. The reason for this inconsistency is not explained.\n\n- The ratios for the fixed prior (Ps) and active sampling (As) are central to the proposed method but are heuristically chosen for each task. While the authors present results across multiple Ps and As settings for the MRI reconstruction task, similar analyses are missing for the classification and segmentation tasks. The segmentation task uses (80%, 20%), which differs from MRI, and the MNIST configuration also varies. The search space for these ratios is large, but no principled optimization strategy is provided."}, "questions": {"value": "- How would the performance change if model selection were based on validation performance (best checkpoint) rather than the final training epoch?\n\n- What is the rationale behind using different image resolutions (208x208 vs. 320x320) across MRI experiments?\n\n- How sensitive is the proposed approach to Ps or As settings in MINST and segmentation tasks?\n\n- Since the choice of Ps and As ratios is crucial, what is the justification for determining these values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ztctPwozbO", "forum": "GMxQHTyO2T", "replyto": "GMxQHTyO2T", "signatures": ["ICLR.cc/2026/Conference/Submission15488/Reviewer_4uGe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15488/Reviewer_4uGe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984184248, "cdate": 1761984184248, "tmdate": 1762925778040, "mdate": 1762925778040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}