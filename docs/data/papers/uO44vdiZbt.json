{"id": "uO44vdiZbt", "number": 17149, "cdate": 1758272771565, "mdate": 1759897193723, "content": {"title": "Neural Super-Resolution for Mesh-Based Simulations under Scarce Supervision", "abstract": "Mesh-based simulations provide high-fidelity solutions to partial differential equations (PDEs), but achieving such accuracy typically requires fine meshes, leading to substantial computational overhead. Super-resolution techniques aim to mitigate this cost by reconstructing high-resolution (HR), high-fidelity solutions from low-cost, low-resolution (LR) counterparts. However, training neural networks for super-resolution often demands large amounts of expensive HR supervision data, posing a major practical limitation. To address this challenge, we propose SuperMeshNet, an HR data-efficient super-resolution framework for mesh-based simulations aided by message passing neural networks (MPNNs). As its core, SuperMeshNet introduces complementary learning that effectively leverages both a small amount of paired LR-HR data and abundant unpaired LR data via two jointly trained, complementary MPNN-based models. Theses models are enriched by task-specific inductive biases that emphasize local variations critical for accurate super-resolution. Extensive experiments demonstrate that SuperMeshNet–an MPNN-based model with inductive biases trained on a dataset with 10% paired LR--HR data and 90% unpaired LR data–achieves an even lower root mean square error (RMSE) than the same MPNN without inductive biases trained on 100% of LR-HR pairs, while in turn requiring 90% less HR data. The source code and datasets are available at https://anonymous.4open.science/r/SuperMeshNet/README.md.", "tldr": "", "keywords": ["Mesh-Based Simulation; Super-Resolution; Semi-Supervised Learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9085802756da6bd669fa0e8680c079adb931de42.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a super-resolution framework, *SuperMeshNet*. The proposed idea consists of two main components, complementary learning method and inductive bias. The complementary learning employs an auxiliary model and predicts pseudo labels for the unlabeled data. Then, they are used to strengthen the main model's performance. Likewise, the main model generates pseudo labels for the unlabeled data and then the auxiliary model learns from them. This pseudo-label ensembling enhances the quality of pseudo labels and thus improves the semi-supervised super-resolution performance. In addition, subtracting mean of node embeddings from the outputs of MPNN layers turn out to further improve the model accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. I like the idea of employing an auxiliary model and ensembling two models' outputs. It has been well incorporated into an appropriate super-resolution tasks for high-resolution PDE solutions.\n\n2. Application to time-dependent PDEs shown in Section 3.7 strengthens the proposed idea. The visualized results clearly show the performance improvements.\n\n3. Appendix provides rich empirical and visualized results. Also, the real-world applications justify the proposed method's efficacy."}, "weaknesses": {"value": "Overall, while the semi-supervised learning mechanism has been wisely incorporated into super-resolution task, I see some critical issues as follows.\n\n1. [**Missing definition of notations**] All notations are defined in appendix, however, it seriously hurts readability. Please bring at least a part of them into the main text to make the manuscript self-contained. \n\n2. [**Novelty issue**] The authors argue that complementary learning is a novel semi-supervised learning method. However, having an auxiliary model and ensembling two models' outputs in a semi-supervised learning framework is not novel. While it is a wise design of semi-supervised learning framework, it is still a combination of employing auxiliary model and pseudo-label ensemblnig. Recently, the word 'novel' is misused in many paper submissions. I would like to point out that 'novel' means that the proposed idea is entirely 'new' in any aspects.\n\n3. [**Inductive bias substantiation issue**] Inductive bias is not well substantiated. In section 2.3, it is said that \"To further improve super-resolution performance, we incorporate two inductive biases into the message passing mechanism of each MPNN layer.\" The authors' logic is that the accuracy is improved when the mean values are subtracted from the node embeddings and thus global mean is uninformative. I believe this has a serious flaw because the conclusion is based on their own limited observations. The higher accuracy does not explain why focusing on deviation improves the MPNN's accuracy. The same flaw in message-level centering. I strongly recommend better understanding why such standarization improves performance and explain the exact reasons. Otherwise, it makes the whole arguments significantly less convincing.\n\n4. [**Comparison to PINNs**] Recently, PINNs have been widely used to solve PDEs, replacing FEM approaches. What if PINNs are used to directly generate HR images from the corresponding parameter values instead of using GNNs to run super-resolution? Although it may not be exactly within the scope of this study since this work narrows down the scope to 'super-resolution', I think this comparison may be a greatly interesting topic for researchers who study PDEs.\n\n5. [**Poor presentation quality***] Too many important information is missing and appear in the Appendix. E.g., notations, pseudocode of the complementary learning, implementation details of two MPNN models, and many other factors. I understand the space is limited and authors may selectively put some information in the appendix, but too much is missing now. I had to go back and forth constantly to read the paper.\n\nDue to the above limitations, I cannot give a positive score for now."}, "questions": {"value": "My questions are included in the above weakness section. Please carefully address them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mOYFT6Ugst", "forum": "uO44vdiZbt", "replyto": "uO44vdiZbt", "signatures": ["ICLR.cc/2026/Conference/Submission17149/Reviewer_3rvZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17149/Reviewer_3rvZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761586637543, "cdate": 1761586637543, "tmdate": 1762927138410, "mdate": 1762927138410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors aim to improve super resolution predictions to datasets generated from PDEs. Doing so is a promising direction for ML as it can significantly speed up expensive simulations to facilitate faster design processes when the error in accuracy is tolerable. The authors aim to tackle this task by combining two different approaches: subtracting the mean from messages and vertex embeddings, and a semi-supervised training mechanism that resist on predicting the difference between two observables in the same resolution."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The authors test this result on a number of datasets, including one with objects. Additionally, although the scope of this work is designed to improve super-resolution with MPNNs, they still compare against other super resolution models and other semi-supervised training methods. In general, I find the number of datasets and the comparisons to other models to be quite good.\n\n- I find the improvements from fully supervised (N=200) to SuperMeshNet to be quite compelling. This comparison shows that SuperMeshNet is able to resolve high resolution images with 10 times less high resolution data, but with 200 low resolution images."}, "weaknesses": {"value": "The comparison between fully supervised (N=200) to SuperMeshNet is attributed to two factors, the inductive bias and the semi-supervised training. Crucially, this comparison only indicates that the combination of the biases AND the semi-supervised learning outperforms the fully supervised training. Nowhere in the paper can I find experiments that disentangle the effects of the bias and the effects of the semi-supervised training.\n\nThe authors do include a comparison between fully supervised training with the same high resolution images (N=20) as SuperMeshNet. I find this comparison moot as SuperMeshNet is trained with the same 20 high resolution images and 200 more low resolution images. It is well known that adding lower quality data to training can improve results. I imagine if one took any of the GNNs and added two different heads for different resolutions, that would also do better than your fully supervised training results. To me, this test does not provide any information specific to SuperMeshNet, only that more data (even if lower quality) can improve the training results.\n\nI consider an ablation between the inductive bias and the semi-supervised training to be a very important comparison. Specifically, training the benchmark architectures with semi-supervised training (Nh=20, N=200) and fully supervised with the bias for Nh=200, and with Nh=20 and N=200. Without these comparisons, it is unclear what the importance of the inductive bias is, and the importance of the semi-supervised training. That is, given your current results, it is possible that the inductive bias applied to all the MPNNs with Nh=20 and N=20 can outperform the fully supervised N=20 model. If that were the case, then the semi-supervised training would be unnecessary.\n\nAlthough I find the fully supervised (N=200) comparison with SuperMeshNet to be quite compelling, I consider the missing ablation between the inductive bias and the semi-supervised training to be a severe oversight, as it is unclear if the semi-supervised training is necessary at all. Without justifying the effects of the bias and the training individually, the authors undermine half of the paper (semi-supervised training) making it quite difficult to accept this paper in its current state. If the authors include this ablation study, I am likely to change my rating to some variation of “accept” as I find the final results overall to be compelling, subject to points brought up by other reviewers."}, "questions": {"value": "- How do the authors think this semi-supervised learning technique would help other architectures? Some experiments applying this learning method to other architectures would be nice to see."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DfKYF1u85W", "forum": "uO44vdiZbt", "replyto": "uO44vdiZbt", "signatures": ["ICLR.cc/2026/Conference/Submission17149/Reviewer_GW1j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17149/Reviewer_GW1j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711940958, "cdate": 1761711940958, "tmdate": 1762927138091, "mdate": 1762927138091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses a graph neural network to construct a mesh-based, semi-supervised super-resolution framework. The authors use a auxiliary model to obtain pseudo-ground truth for low-resolution/high-resolution data pairs in situations where high-resolution data is missing, thereby enabling semi-supervised learning. They also improve the network's super-resolution performance through node-level and message-level centering."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors are the first to discuss a semi-supervised method for mesh super-resolution in considerable detail. This method is beneficial for scenarios where low-resolution data is abundant but paired high-resolution data is scarce, and it has significance for accelerating the training of super-resolution models. \n2. Although the theoretical part of the paper is relatively simple, its presentation is clear. The experimental setup and details are complete and persuasive."}, "weaknesses": {"value": "1.\tLack of sufficiently difficult data for the super-resolution task. A major challenge in super-resolution is that low-resolution data often loses important high-frequency details, which can cause significant qualitative differences in the physical field. For example, in the Kolmogorov flow considered in [1], the results predicted by traditional DNS on a low-resolution mesh show very significant differences from the results on a high-resolution mesh after a certain number of time steps (see Fig. 2 A of [1], time step = 1500). However, I do not see such a discrepancy in the results presented in your appendix (Figure 12). This might suggest that for this type of task, low-resolution simulation is sufficient to capture the qualitative details of the physical field we care about, making high-resolution results less necessary. I think you need to add relevant experiments and, following the example of [1], show the simulation data from low- and high-resolution meshes where large differences exist, as well as your experimental results, to prove that your method can also achieve good performance on such difficult tasks. \n2.\tMissing comparison of inference times. An important application of super-resolution is to perform simulations on low-resolution meshes and obtain high-resolution results via super-resolution, significantly reducing the time cost of high-resolution numerical simulations. I hope you can compare the time cost of high-resolution numerical simulation with the time cost of (low-resolution simulation + super-resolution) to demonstrate that your method can be used to accelerate numerical simulations. \n3.\tExcessive length for content related to a technical trick. Section 2.3 mainly introduces the decentralization of the graph neural network, and the authors emphasize its importance for improving prediction results. While this technique may indeed be important, it is quite empirical and does not deserve so much space in the valuable main text. I hope you can condense this section and shift the paper's focus to the more important presentation of experimental results. You could consider moving some of the figures from the appendix into the main text.\n\nReference:\n[1]: Kochkov, Dmitrii, et al. “Machine Learning–Accelerated Computational Fluid Dynamics.” (2021)"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LmGUFCcA9Y", "forum": "uO44vdiZbt", "replyto": "uO44vdiZbt", "signatures": ["ICLR.cc/2026/Conference/Submission17149/Reviewer_5EFa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17149/Reviewer_5EFa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881623077, "cdate": 1761881623077, "tmdate": 1762927137741, "mdate": 1762927137741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, a neural super-resolution framework is proposed, which supports various message-passing neural networks. A scarce supervision method is designed to relieve the demand for high-resolution data. Node-level centering and message-level centering are applied to enhance the performance and validated by ablation study. Experiments show that the proposed SuperMeshNet outperforms other super-resolution methods and semi-supervised methods while requiring 90% less high-resolution data compared to full supervision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It is in principle reasonable to utilize semi-supervised learning techniques for neural super-resolution tasks, to reduce the demand for expensive high-resolution data.\n- The experiments are very solid and various, including the various benchmarks, comparison with full supervision, comparison with other super-resolution baselines, ablation studies, etc."}, "weaknesses": {"value": "The performance gain is relatively marginal, compared to super-resolution baselines, and semi-supervised regression baselines."}, "questions": {"value": "- What are the computational costs for generating low-resolution and high-resolution data respectively in your experiments?\n- Will the kNN interpolation module bring computational cost issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ou7NSoRkPQ", "forum": "uO44vdiZbt", "replyto": "uO44vdiZbt", "signatures": ["ICLR.cc/2026/Conference/Submission17149/Reviewer_iPBS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17149/Reviewer_iPBS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987666480, "cdate": 1761987666480, "tmdate": 1762927137050, "mdate": 1762927137050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SuperMeshNet, a neural super-resolution framework designed for mesh-based simulations that decreases the reliance on costly high-resolution training data. The approach introduces complementary learning, a semi-supervised method that employs two jointly trained MPNN-based models, along with inductive biases to concentrate on local variations, achieving competitive accuracy with significantly reduced HR supervision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The semi-supervised complementary learning framework proposed in this paper can achieve accuracy comparable to fully supervised methods with only a very small number of high-resolution samples during the training phase.\n2.The proposed framework has proven effective across six different message MPNN types, making it a general solution.\n3.The proposed method outperforms the fully supervised baseline with less training data and is faster to train than other semi-supervised methods."}, "weaknesses": {"value": "1.Complementary learning requires simultaneously optimizing two MPNNs and performing extra kNN interpolations for every unpaired LR sample, so total training time grows substantially compared with fully-supervised training.\n2.Without confidence weighting or external validation in complementary learning, any systematic error in one model is immediately back-propagated into the other, risking synchronized drift that is hard to escape.\n3.The mutual supervision mechanism critically depends on kNN interpolation to bridge mesh mismatches, making the entire framework vulnerable to interpolation errors near geometric singularities or sharp gradients."}, "questions": {"value": "1.The proposed method demonstrates impressive data efficiency by leveraging only a small fraction of paired HR data. Does the performance vary significantly if a different random 10% is chosen, or if a potentially non-representative set is selected? If there are specific HR selection preferences, how should they be chosen?\n2.Both networks share the same encoder although they pursue different output spaces. Could gradient conflict between tasks be reduced by separate encoders or task-specific adapters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MD2nDmIawy", "forum": "uO44vdiZbt", "replyto": "uO44vdiZbt", "signatures": ["ICLR.cc/2026/Conference/Submission17149/Reviewer_EtFo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17149/Reviewer_EtFo"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066173024, "cdate": 1762066173024, "tmdate": 1762927136659, "mdate": 1762927136659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}