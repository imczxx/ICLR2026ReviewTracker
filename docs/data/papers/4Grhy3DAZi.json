{"id": "4Grhy3DAZi", "number": 3094, "cdate": 1757331075360, "mdate": 1759898109420, "content": {"title": "Introspective Adversarial Learning: Autonomous and Continual Preference Learning for LLM Alignment", "abstract": "Large Language Models (LLMs) exhibit impressive capabilities across diverse tasks, yet aligning their outputs with human preferences remains a significant and costly challenge. Traditional alignment methods like Reinforcement Learning from Human Feedback (RLHF) depend heavily on extensive human-annotated preference data, which is difficult to scale. We propose Introspective Adversarial Learning (IAL), a novel alignment framework that enables LLMs to autonomously refine their own outputs through iterative self-improvement, without requiring additional human supervision. IAL introduces a Player-Advisor mechanism where the Player generates candidate responses and the Advisor provides constructive refinement strategies. The refined responses are evaluated by a reward model, and the contrast between initial and improved outputs drives a Preference Transductive Learning process. This reflective cycle allows the model to generate high-quality preference data internally and progressively enhance alignment. Experiments on the zephyr-7b-sft-full model, evaluated via the HuggingFace Open LLM Leaderboard and MT-Bench, show that IAL consistently improves alignment performance while preserving strong general task capabilities. Compared to state-of-the-art methods such as SPIN, SPA, and DPO, IAL achieves superior results without relying on costly human preference annotations, offering a scalable and efficient pathway toward better-aligned LLMs.", "tldr": "IAL enables self-improving LLM alignment without human labels, surpassing existing methods.", "keywords": ["LLM alignment", "Self-Play"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/45815badb72b71d5d45f27ce306d871c2b3b86a8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Introspective Adversarial Learning (IAL), a self-improvement alignment framework where a Player LLM generates an initial response, an Advisor LLM provides targeted suggestions, and the Player regenerates an improved response; a reward model then ranks the pair to form preference data for offline preference optimization via a SPAC-style objective with a positive-preserving modification (SPACP). Experiments on zephyr‑7B‑sft‑full with UltraChat/UltraFeedback report gains over DPO, SPA, and SPIN on the Open LLM Leaderboard (v1) and MT‑Bench, with an ablation on the penalty hyperparameter and a brief appendix result on Qwen"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Clear decomposition into self-refinement data generation and preference optimization, which mirrors practical workflows and is easy to reproduce.\n\nUses an explicit “positive-preserving” modification to address BT-style relative-only issues in DPO/SPAC, with an ablation on γ.\n\nIterative pipeline and reporting across multiple iterations help illustrate convergence dynamics rather than single-shot gains."}, "weaknesses": {"value": "Backbone choice and recency: Using zephyr‑7B‑sft‑full as the main backbone feels dated and weakens external validity; this model was popular circa 2023. At minimum, please include strong, current 7B baselines such as Qwen‑2.5‑Instruct‑7B and Llama‑3.1/3.2‑Instruct‑8B with matched training setups. The appendix reports “Qwen‑2.5‑3B,” but it’s unclear whether this is the base, instruct, or SFT variant—this should be clarified and expanded to 7B.\n\nLeaderboard versioning and setup: The Open LLM Leaderboard evaluation appears to follow the older v1 style and a SPIN-like setup; more convincing evidence would use the updated v2 protocols and clearly document eval agents, few-shot regimes, and any normalization or debiasing steps.\n\nMT‑Bench limitations: MT‑Bench is known to be noisy and less discriminative today; Arena‑Hard (v2) and AlpacaEval 2.0 (length-controlled) are more informative for chat/instruction-following, and math verification should include recent suites such as Math500 and AIME’24/’25-style evaluations.\n\nMarginal gains vs. complexity: The overall framework adds multiple moving parts (Player/Advisor prompting, PairRM curation, SPACP) yet the reported improvements over SPIN/DPO are modest; please quantify cost/benefit (GPU-hours, RM queries, data generation volume) and show statistically significant margins across multiple seeds and models.\n\nNovelty positioning: The contribution reads as a composition of existing ideas (self-refinement/self-play, adversarial critic, positive-preserving objectives) applied on older models/benchmarks. Please sharpen what is new at the algorithmic level beyond SPIN/SPAC/DPO variants, provide theoretical intuition or guarantees for SPACP, and demonstrate unique empirical advantages on modern backbones and benchmarks.\n\nReward model dependence and circularity: Reliance on PairRM as judge risks overfitting to a particular evaluator. Include cross-judge robustness (e.g., different RMs/LLM-as-judge, human evaluation, blinded pairwise) and measure label quality drift across iterations to rule out circular artifacts.\n\nClarity on data flow: UltraChat/UltraFeedback usage, data mixing across iterations, and de-duplication policies need clearer documentation to ensure no leakage or inadvertent contamination, especially when comparing across methods."}, "questions": {"value": "See Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AoerPCpeG9", "forum": "4Grhy3DAZi", "replyto": "4Grhy3DAZi", "signatures": ["ICLR.cc/2026/Conference/Submission3094/Reviewer_Ddec"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3094/Reviewer_Ddec"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760576623052, "cdate": 1760576623052, "tmdate": 1762916548375, "mdate": 1762916548375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper set up a Player-Advisor loop: the Player answers, the Advisor suggests edits, the Player rewrites, and a reward model (PairRM) selects the better of the two; those pairs train the policy with a SPAC-like objective plus a new positive term (SPACP). Trained on UltraChat-derived prompts with zephyr-7b-sft-full, they iterate this process and report consistent gains on the Open LLM Leaderboard and MT-Bench vs. DPO/SPIN/SPA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Clear, reproducible pipeline with concrete steps and illustration.\n+ Results look uniformly better than SFT&DPO and broadly competitive with PPO on the HF leaderboard and MT-Bench.\n+ The SPACP tweak directly targets a known BT/DPO issue (only pushing down the “bad”) and is well-justified."}, "weaknesses": {"value": "- The performance of the method seems heavily dependent on the reward model (PairRM). There is no guarantee of the RM's performance on a wider range of tasks or domain-specific tasks. Figure 4 (left) also confirms this concern.\n- The advisor's contribution is not isolated. The paper shows many cases where the second response is identical under greedy decoding, yet training still helps: this suggests gains may come from the objective, not the advice. An “advisor off / random second rollout” ablation is missing.\n- DPO baseline mismatch. The paper compares to DPO trained on UltraFeedback, not DPO trained on the generated PairRM-labeled pairs. It is hard to tell whether the win is from objective vs. data.\n- Claimed gains are modest and somewhat mixed across tasks. PPO is close on averages in places."}, "questions": {"value": "- How sensitive is IAL to the choice of reward model? How robust are the results when swapping PairRM for a different RM? Does RM adaptation materially help, or does IAL reduce the need for RM specialization?\n- Advisor ablation: Compare (a) advised second generation, and (b) random/independent second rollout. If the preference is determined by external RM anyway, what is the actual delta the advisor introduced?\n- What is the quantitative inference overhead introduced by the player-advisor model. How is it justified by its isolated gain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WpFAQoS9V8", "forum": "4Grhy3DAZi", "replyto": "4Grhy3DAZi", "signatures": ["ICLR.cc/2026/Conference/Submission3094/Reviewer_pPV8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3094/Reviewer_pPV8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986693791, "cdate": 1761986693791, "tmdate": 1762916548142, "mdate": 1762916548142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new preference learning framework, Introspective Adversarial Learning (IAL), where the model alternates between Player and Advisor roles to generate self-refined preference data via the external reward model PairRM, thereby achieving alignment without human feedback."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Novel self-play formulation:The Player–Advisor mechanism is conceptually elegant and extends traditional self-play alignment with an introspective adversarial loop.\n\n2.Practical efficiency:Achieves alignment improvement without costly human preference annotation while maintaining strong general task performance.\n\n3.Comprehensive evaluation:Consistent superiority over DPO, SPA, and SPIN on MT-Bench and the Open LLM Leaderboard."}, "weaknesses": {"value": "1.Dependence on external models:The reliance on PairRM for preference judgment weakens the claim of full autonomy.\n\n2.Limited cost analysis:While human supervision is removed, the paper does not adequately discuss the computational and time overhead introduced by iterative generation and ranking, nor compare it with DPO or other baselines.\n\n3.Lack of introspective analysis – The paper lacks ablation or theoretical analysis on the stability, convergence, and potential failure modes of the introspective adversarial mechanism."}, "questions": {"value": "1.Since PairRM is used to determine preference data, the accuracy and bias of this reward model will directly affect the alignment quality. Have you analyzed its sensitivity?\n\n2.The method introduces substantial computational overhead for modest gains over DPO. It would be valuable to compare with GRPO or other recent reinforcement-style alignment methods to justify efficiency.\n\n3.The PPO comparison appears unfair:PPO uses a separately trained reward model on UltraFeedback, while IAL leverages PairRM. The comparison should ideally use the same reward model for consistency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ONU5XakeRT", "forum": "4Grhy3DAZi", "replyto": "4Grhy3DAZi", "signatures": ["ICLR.cc/2026/Conference/Submission3094/Reviewer_muBz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3094/Reviewer_muBz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991918468, "cdate": 1761991918468, "tmdate": 1762916547131, "mdate": 1762916547131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Player-Advisor mechanism to collect preference pairs, where the first response comes directly from the policy model and the second one is the refined response based on feedback from the model itself. The authors further propose a new learning algorithm, Self-Play with Adversarial Critic-Positive (SPACP), which includes a penalty to encourage the model to retain high probability on the human annotated responses. Experiments show better performance than other baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper adopts refined responses based on feedback from the model itself to get better preference pairs. The self-improvement paradigm could be interesting to the community."}, "weaknesses": {"value": "* Ablations are not convincing enough to justify the proposed method.\n* The equations are not always clear.\n* The use of human response demands human annotation and makes the learning off-policy, which may hurt performance."}, "questions": {"value": "* The formulation is confusing, which reduces the readability. In Equation 8, where is the log \\sigma in the second term from? In Equation 6 and 8, \\lambda is used for the penalty term, but in Equation 9 and 10, it changed to the preference learning term.\n* In lines 283-284, it seems that the purpose of SPACP is to retain the log-likelihood of y^+, but it’s actually used y^t. Why use the human annotation here, which clearly demands more human involvement? Also, the placement of the penalty term related to y^t lacks empirical justification? what if just using y^+ instead (note y^+ is from the model, while y^t is totally off-policy)?\n* Would the off-policy learning caused by y^t hurt model performance? Please add a discussion.\n* There is no clear comparison between SPAC and SPACP on the final model performance. It’s unclear how serious the motivation of SPACP is in practice.\n* The paper claims “reducing alignment costs”, but it’s not always clear how the proposed method compares to the others on the cost, particularly the human efforts. Please add a table detailing it for different training stages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3jRGZBjSsM", "forum": "4Grhy3DAZi", "replyto": "4Grhy3DAZi", "signatures": ["ICLR.cc/2026/Conference/Submission3094/Reviewer_yEmh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3094/Reviewer_yEmh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3094/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098605784, "cdate": 1762098605784, "tmdate": 1762916546967, "mdate": 1762916546967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}