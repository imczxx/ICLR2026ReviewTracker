{"id": "yqTajvdkjv", "number": 9724, "cdate": 1758136684350, "mdate": 1759897702201, "content": {"title": "Selective Underfitting in Diffusion Models", "abstract": "Diffusion models have emerged as the principal paradigm for generative modeling across various domains. During training, they learn the score function, which in turn is used to generate samples at inference. They raise a basic yet unsolved question: *which* score do they actually learn? In principle, a diffusion model that matches the empirical score in the entire data space would simply reproduce the training data, failing to generate novel samples. Recent work addresses this question by arguing that diffusion models *underfit* the empirical score due to training-time inductive biases. In this work, we refine this perspective, introducing the notion of *selective underfitting*: instead of underfitting the score everywhere, better diffusion models more accurately approximate the score in certain regions of input space, while underfitting it in others. We characterize these regions and design empirical interventions to validate our perspective. Our results establish that selective underfitting is essential for understanding diffusion models, yielding new, testable insights into their generalization and generative performance.", "tldr": "", "keywords": ["Diffusion Model", "Generative Model", "Generalization", "Scaling Law"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ea232f8242fc450907ded5daa87de950a4eb464.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates how well a trained diffusion model approximates the empirical score. In particular, the paper challenges the common perspective (Perspective A) that a perfectly trained diffusion model should approximate well the empirical score \\textit{everywhere}, i.e., over the entire data space. A key observation made by the authors is that the supervision signal, at a certain noising step $t$, coming from the DSM objective, only covers a relatively small shell around each noised training data point, leaving a large part of the space unsupervised. This results in the model overfitting the empirical score well in these limited regions (Supervised regions) and underfitting it outside them (Extrapolated regions). As a consequence, at generation time, the model will most likely explore regions for which very little supervision signal has been received during training, hence requiring it to extrapolate. Interestingly, the experiments show that too much supervision signal leads the model to memorise more the training data. On the other hand, giving the model more freedom to extrapolate significantly reduces this effect. Furthermore, if the training objective is modified to shape the model's behaviour in the extrapolation regions (through inductive biases such as convolutions) can lead the model to more favourable performance during generation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and well structured. The exposition follows a natural flow and claims are substantiated with adequate evidence.\n- The observation that the empirical score cannot be precisely approximated everywhere represents -- from the best of the reviewer's knowledge -- a simple yet relatively novel perspective and has important implications on the generation quality and on memorisation. \n- I find experiment 4.3 interesting: counterintuitively adding more supervision data actually increases the effect of memorisation, pointing to the fact that the extrapolation behaviour of the model plays a major role in generalisation."}, "weaknesses": {"value": "- The main weakness of the paper is that it does not discuss in depth the behaviour of diffusion models in the extrapolation region and which factors mostly influence it. This seems a natural (and crucial) question given that in high-dimensions the supervised region is very small and the behaviour of the model outside it is undefined a priori. \n- While the perspective taken by the paper is somewhat novel, there is a large overlap with prior work (see for instance [1,2,3] and reference therein), mainly on memorisation in diffusion models, that is not discussed.\n\n[1] Sclocchi, Antonio, Alessandro Favero, and Matthieu Wyart. \"A phase transition in diffusion models reveals the hierarchical nature of data.\" Proceedings of the National Academy of Sciences 122.1 (2025): e2408799121.\n\n[2] Biroli, G., Bonnaire, T., De Bortoli, V., & Mézard, M. (2024). Dynamical regimes of diffusion models. Nature Communications, 15(1), 9957.\n\n[3] Achilli, Beatrice, et al. \"Memorization and generalization in generative diffusion under the manifold hypothesis.\" Journal of Statistical Mechanics: Theory and Experiment 2025.7 (2025): 073401."}, "questions": {"value": "- On the links with prior work: Previous works have studied the emergence of memorisation in backward diffusion (see for instance [1,2,3] among others). Some of these works have also investigated the onset of memorisation as a function of denoising time.  How does the present work link to these findings? \n- On the role of inductive biases: In the introduction the authors state \"We advance a mechanism for how diffusion models generate\nsamples beyond their training data that is agnostic to architectural inductive biases, unlike previous theories\". I would appreciate if the authors could clarify what they mean in this statement. In particular, from my understanding of the paper, inductive biases are shown to ultimately play a major in shaping the behaviour of the model in the extrapolation region, where little supervision is provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZibQ9UhM51", "forum": "yqTajvdkjv", "replyto": "yqTajvdkjv", "signatures": ["ICLR.cc/2026/Conference/Submission9724/Reviewer_X3ZP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9724/Reviewer_X3ZP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815547764, "cdate": 1761815547764, "tmdate": 1762921222420, "mdate": 1762921222420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies underfitting and generalization in diffusion models. The central claim is that during training, noisy samples $x_t = \\alpha_t x_0 + \\sigma_t \\varepsilon$ concentrate on thin shells around training examples (“supervised regions”), leading the model to approximate the empirical score $s_\\star(\\cdot, t)$ there. Outside these regions (“extrapolation regions”), the learned score $s_\\theta(\\cdot, t)$ deviates more from $s_\\star$, i.e., **selective underfitting**.\n\nEmpirically, the authors (i) investigate supervised regions on ImageNet and show they remain largely non-overlapping until relatively large noise levels ($t \\approx 0.8$); (ii) quantify selective underfitting via score-error gaps; and (iii) use noise inversion and a distillation experiment to argue that trajectories entering supervised regions tend to yield training examples. They further propose practical implications (e.g., predicting final FID; interpreting RePA/architectural effects) via their underfitting metric."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The observation that supervised regions are largely disjoint up to $t\\approx 0.8$ is interesting and consistent with prior reports that learned vs. empirical scores diverge at similar timesteps.\n2. Figures (e.g., Figs. 1 and 5) are intuitive, and the paper is well formatted."}, "weaknesses": {"value": "1. “Selective underfitting” is essentially evaluating the **training/empirical denoising loss** on test data. It is well known that a generalizing denoiser deviates from an empirical denoiser in such regions and can denoise a test image rather than overfit. Rather than being a key to generalization, selective underfitting reads more like a *necessary* or descriptive condition (“where the model is not fitting the empirical score”) than a causal/mechanistic explanation of *how* extrapolation succeeds.\nMoreover, there are more directional, theory-grounded probes of generalization than the proposed gap metric—for example, test denoising loss in [1].\n2. The selective underfitting framing also struggles to reconcile several relevant findings:\n   1. Reproducibility that co-arises with generalization in diffusion models [1].\n   2. The memorization→generalization transition as the number of training samples increases [2].\n   3. Evidence that the learned score deviates from the empirical score even on supervised regions, and that such deviation can *enable* generalization (see Fig. 2 in [3]).\n3. **Issues with experiments.**\n   1. Strongly overfitting (memorizing) models can still learn imperfect empirical scores, but still generate training data. Additional study and comparison on the selective underfitting of such memorized models would strengthen the claims.\n   2. Mismatch in Fig. 2: Comparing score error on training-time forward noisy samples (blue) vs. inference-time backward states (red) is not fully fair. A better test is to complete sampling to obtain a novel image, then add noise at the same $t$ and re-measure score error on that generated sample.\n   3. Results in Fig. 3: The difference between supervised (≈$1.8\\times10^{-1}$) and extrapolated (≈$5.4\\times10^{-1}$) errors is reported, but without a standardized baseline it is hard to judge (i) how much the model actually *overfits* in supervised regions and (ii) whether the extrapolation error is “large” in a practically meaningful sense.\n   4. Results in Fig. 4:The noise-inversion experiment does not directly establish memorization or generalization; it shows a property of trajectories relative to supervised regions.\n   5. Section 5: Several conjectures are offered with minimal evidence (two teasers with toy examples) and feel disconnected from the core underfitting analysis."}, "questions": {"value": "1. **Definition of supervised regions.** Are they strictly the thin spherical shell (as in Prop. 3.2), or the full ball around each sample? In my experiments, the network behaves as a strong “blind” denoiser and can denoise with *mismatched* noise levels [4], which would enlarge the effective supervised region.\n2. **Distillation setup (Fig. 5).** Fixing $D_{\\text{score}}$ and enlarging $D_{\\text{region}}$ may force the model to project many samples toward a few anchor images in $D_{\\text{score}}$, just like effective training-data duplication, which naturally promotes memorization. Why not directly minimize MSE on a *thickened* shell (by perturbing the noise level) rather than distilling $s_\\star(\\cdot)$ over a larger range of data (i.e., addinging more supervised shells)?\n3. These works on interpolation and local overfitting may be useful to connect with [5, 6].\n\n*My current rating is a provisional assessment and can be updated after author responses and discussion.*\n\n**References**\n\n [1] Kadkhodaie et al., *Generalization in Diffusion Models Arises from Geometry-Adaptive Harmonic Representations*, ICLR 2024.\n [2] Zhang et al., *The Emergence of Reproducibility and Consistency in Diffusion Models*, ICML 2024.\n [3] Niedoba et al., *Towards a Mechanistic Explanation of Diffusion Model Generalization*, ICML 2024.\n [4] Sun et al., *Is Noise Conditioning Necessary for Denoising Generative Models?*, ICML 2025.\n [5] Zeno et al., *How Do Minimum-Norm Shallow Denoisers Look in Function Space?*, NeurIPS 2023.\n [6] Zeno et al., *When Diffusion Models Memorize: Inductive Biases in Probability Flow of Minimum-Norm Shallow Neural Nets*, arXiv."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7phJg0CsDB", "forum": "yqTajvdkjv", "replyto": "yqTajvdkjv", "signatures": ["ICLR.cc/2026/Conference/Submission9724/Reviewer_4Qh5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9724/Reviewer_4Qh5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839724603, "cdate": 1761839724603, "tmdate": 1762921221241, "mdate": 1762921221241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies where diffusion models underfit the empirical score, introducing the concept of **selective underfitting** to describe spatially non-uniform score approximation. The authors observe that while training samples lie in narrow Gaussian shells around data points (the denoising targets), inference trajectories often move beyond these shells into regions rarely supervised during training. This leads to a mismatch between training and inference distributions. The paper (1) empirically demonstrates selective underfitting in two theoretically grounded regions of data space, (2) proposes a mechanism explaining how diffusion models can still generate novel samples beyond training data independent of architectural inductive biases, and (3) introduces a scaling law quantifying the efficiency of training procedures, offering a unified perspective on diffusion model optimization and generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**1. Novel conceptual framework:** The idea of selective underfitting provides a compelling and fine-grained understanding of diffusion model training and generalization, refining prior uniform-underfitting views.\n\n**2. Well-written and clearly structured:** The paper is clearly written, with strong motivation, intuitive figures, and well-organized arguments that make a complex topic accessible. The exposition effectively connects conceptual insights with empirical evidence, enhancing readability and comprehension."}, "weaknesses": {"value": "Here are some main concerns about this paper.\n\n**1. The paper does not fully engage with the essence of underfitting in diffusion models.** While the notion of selective underfitting provides a spatially structured view of how diffusion models approximate the empirical score, it does not fundamentally revisit why underfitting occurs in the first place. Recent works have provided deeper theoretical explanations of this phenomenon. Specifically, *Kadkhodaie et al. (2023)* argue that generalization in diffusion models arises from geometry-adaptive harmonic representations, where the learned score function captures the intrinsic geometry of the data manifold rather than merely fitting local noise perturbations. Similarly, *Bonnaire et al. (2025)* show that diffusion models avoid memorization due to implicit dynamical regularization in the training dynamics, which naturally biases the model toward smoother score fields aligned with the true data distribution.\n\nIn light of these perspectives, the present paper’s “selective underfitting” can be viewed more as a manifestation or empirical measure of generalization rather than a fundamentally new explanatory mechanism. The authors discuss where underfitting occurs but not why diffusion models underfit in a principled manner. As a result, the proposed framework risks being descriptive rather than mechanistic—it highlights the spatial distribution of score errors but leaves the underlying regularization or representation principles unexplored.\n\n1. Kadkhodaie, Zahra, et al. \"Generalization in diffusion models arises from geometry-adaptive harmonic representations.\" arXiv preprint arXiv:2310.02557 (2023).\n2. Bonnaire, Tony, et al. \"Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training.\" arXiv preprint arXiv:2505.17638 (2025).\n\n**2. Heavy reliance on conjecture and empirical observation, with limited principled understanding.** The paper relies heavily on conjecture and empirical observation, offering limited theoretical explanation for why selective underfitting emerges. While the experiments are suggestive, the arguments remain largely descriptive rather than derived from score-matching principles or diffusion dynamics. As a result, the work provides intuition but not a principled understanding of the underlying mechanisms."}, "questions": {"value": "The paper links selective underfitting to the generative (sampling) behavior of diffusion models, but it is unclear why underfitting during score learning should directly affect the reverse sampling process. Since overfitting or underfitting is determined primarily by the training dynamics of the score network, not by the stochastic sampling procedure, could the observed sampling behavior simply reflect how the model extrapolates from its learned score field rather than underfitting per se? How do the authors conceptually or empirically separate the effects of training-time underfitting from those of the reverse diffusion dynamics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kkb92xq7dM", "forum": "yqTajvdkjv", "replyto": "yqTajvdkjv", "signatures": ["ICLR.cc/2026/Conference/Submission9724/Reviewer_eMqt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9724/Reviewer_eMqt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762093962364, "cdate": 1762093962364, "tmdate": 1762921220890, "mdate": 1762921220890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}