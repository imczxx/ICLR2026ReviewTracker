{"id": "CkxjXuI7LL", "number": 24217, "cdate": 1758354231485, "mdate": 1759896776200, "content": {"title": "OSS-Bench: Evaluating LLMs via the Realistic Open-Source Software Development Pipeline", "abstract": "In light of the rapid adoption of AI coding assistants, LLM-assisted development has become increasingly prevalent, creating an urgent need for robust evaluation of generated code quality. Existing benchmarks often require extensive manual effort to create static datasets, rely on indirect or insufficiently challenging tasks, depend on non-scalable ground truth, or neglect critical low-level security evaluations, particularly memory-safety issues. In this work, we introduce OSS-Bench, a benchmark generator that automatically constructs large-scale, live evaluation tasks from real-world Open-Source Software (OSS). OSS-Bench replaces functions with LLM-generated code and evaluates them using three novel metrics: compilability, functional correctness, and memory safety, leveraging robust signals like compilation failures, test-suite violations, and sanitizer alerts as ground truth. In our evaluation, the benchmark, instantiated as OSS-Bench(php) and OSS-Bench(sql), profiles 17 diverse LLMs at million-scale tasks, revealing insights such as intra-family behavioral patterns and inconsistencies between model size and performance. Our results demonstrate that OSS-Bench mitigates overfitting by leveraging the evolving complexity of OSS and highlight LLMs' limited understanding of low-level code security via extended fuzzing experiments. Overall, OSS-Bench offers a practical and scalable framework for benchmarking the real-world coding capabilities of LLMs.", "tldr": "A novel benchmark approach to evaluate coding LLMs using practical metrics in the everyday open-source software development pipeline", "keywords": ["Benchmark; Large Language Model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/01ef1f4d94e697e4fde09a910fdf5f2e336aff02.pdf", "supplementary_material": "/attachment/8eb5c4891e16579121c353493198bc5f9eaa1404.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents OSS-BENCH, an automated benchmarking framework designed to evaluate LLMs’ code editing capabilities across three key dimensions: compilability, functional correctness, and memory safety. It aims to overcome the limitations of existing benchmarks, such as Manual effort, less challenging tasks and limited focus on code security."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a comprehensive evaluation of 17 LLM models on code-editing tasks, offering valuable insights into their comparative performance."}, "weaknesses": {"value": "For the OSS-Bench, lack deep analysis and comparison with previous benchmark to clarify the dataset address limitations like less challenging, limited focus on code security. \n\nThe benchmark construction process appears overly straightforward: function extraction from projects, test cases execution, and applying sanitizer tools for memory safety evaluation. While simplicity is not inherently a weakness, the paper should better articulate the novelty and practical value of OSS-BENCH beyond serving as a basic automated pipeline. From this perspective, it is difficult to consider OSS-BENCH a true benchmark rather than a simple workflow.\n\nMoreover, since nearly all functions are included in the evaluation, it remains unclear how this design advances future research or benefits LLMs-based improvement. The current setup relies on only two projects (PHP and SQLite3) with more than 10,000 functions each. Such a narrow yet uniform treatment of all functions makes the approach unrealistic to scale or generalize to more projects in practice."}, "questions": {"value": "Why were only two projects—the PHP interpreter and SQLite3 database engine—selected for evaluation? Since only two projects are included, how do the authors address potential dataset bias and ensure the generalizability and representativeness of the evaluation results?\n\nWhat advantages do these two projects offer compared with those used in previous studies? Can the authors provide quantitative evidence, such as function sampling or statistical comparisons, to support their selection? Only say “they are complex seems subjective”.\n\nThe function extraction process appears to include almost all functions for evaluation. Since the paper’s contribution focuses on the benchmark, are all these functions needed to assess LLMs’ code-editing capabilities, particularly in improving memory safety and efficiency? Given that the best results for Compilability, Functional Test, and Memory Safety exceed 90%, 90%, and 70% respectively, does this imply that current LLMs already demonstrate strong code-editing ability? If so, the paper might consider releasing or analyzing a smaller, representative subset to facilitate further research and fair comparison among LLMs. \n\nIt would be valuable to analyze why LLMs fail on certain functions—what types of code or security aspects remain challenging? Providing insights into these failure cases could substantially strengthen the contribution and deepen our understanding of current LLM limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uojWO08Cgh", "forum": "CkxjXuI7LL", "replyto": "CkxjXuI7LL", "signatures": ["ICLR.cc/2026/Conference/Submission24217/Reviewer_kpz1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24217/Reviewer_kpz1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761106152957, "cdate": 1761106152957, "tmdate": 1762943002039, "mdate": 1762943002039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the rapid rise of AI coding assistants like Copilot and Cursor, highlighting the need for robust evaluation of LLM-generated code quality. Existing benchmarks suffer from issues such as heavy manual effort for static datasets, indirect or unchallenging tasks, non-scalable ground truth, and neglect of low-level security (e.g., memory safety). The authors introduce OSS-BENCH, an automated benchmark generator that creates large-scale, dynamic evaluation tasks from real-world Open-Source Software (OSS). It replaces functions with LLM-generated code and evaluates using three novel metrics: compilability (success rate via compilation failures), functional correctness (test suite pass rate degradation), and memory safety (sanitizer alerts for issues like buffer overflows)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- No manual labeling or LLM-based ground truth; leverages OSS's natural signals (compilation/tests/sanitizers) for dynamic updates as repositories evolve, reducing overfitting to static data.\n\n- Draws from mature OSS (e.g., PHP, SQLite3) with complex low-level code (e.g., garbage collectors, query optimizers), mirroring actual development pipelines in compiled languages—more demanding than simple algorithms.\n\n- Innovates with memory safety (sanitizers + fuzzing), addressing a gap in prior benchmarks; chained scoring accounts for metric dependencies, and Delta encourages meaningful optimizations.\n\n- Million-scale evaluation uncovers patterns like intra-family behaviors (Qwen progress), size-performance inconsistencies, and test suites' inadequacy for security."}, "weaknesses": {"value": "- The paper claims that OSS-BENCH is “the first fully automated OSS pipeline without human or LLM intervention,” yet similar efforts from 2023–2025 diminish this uniqueness. SWE-bench pioneered automated evaluation of LLM-based bug fixing using GitHub OSS issues, covering compilation and testing at a million-task scale to mitigate overfitting. SecRepoBench focuses on secure code generation in real repositories, employing sanitizers to assess memory vulnerabilities—overlapping substantially with OSS-BENCH’s three metrics. BUILD-BENCH  targets OSS compilation agents with dynamic function replacement and superior resource efficiency. SEC-bench and SecureAgentBench extend to agent-level security tasks, including fuzzing. Even earlier work like HumanEval+ leveraged unit-test ground truth. These precedents make OSS-BENCH’s “novel metrics” and “live updates” appear incremental rather than revolutionary. \n\n- The paper introduces memory safety innovations (sanitizer + fuzzing, inspired by OSS-Fuzz), but these are limited to low-level issues such as buffer overflows and use-after-free, while neglecting higher-level vulnerabilities like SQL injection in OSS-BENCHsql, concurrency deadlocks, and side-channel attacks. Although the authors acknowledge insufficient testing (bugs exposed post-fuzzing), they fail to quantify fuzzing costs or integrate static analysis tools (e.g., Coverity). The degradation-focused evaluation overlooks positive improvements such as efficiency benchmarks or runtime profiling, and the weighting mechanism remains subjective and unvalidated. Dynamic updates rely on OSS commits, yet upstream changes like security patches may break consistency due to the absence of version pinning. Compared to SEC-bench, which integrates PoC generation and multi-dimensional security tasks, OSS-BENCH’s “three metrics,” while simple, offer shallow coverage and fail to capture LLM weaknesses in real-world attack chains.\n\n- The use of pass@k=1 (k=1) simplifies large-scale evaluation but overlooks generation diversity—top-tier models like Claude-3.7-Sonnet exhibit variability of 5–10% across multiple samples. Open-source models are uniformly quantized to FP16 (via Ollama), while closed-source models face no such constraints; as Table 1 shows, the leading positions are all closed-source, potentially exaggerating performance gaps (e.g., GPT-O3-Mini vs. large-scale open models). Seeds are fixed at 0 (except Claude), improving reproducibility, yet no ablation explores prompt variations: Figure 2 emphasizes “memory safety + efficiency,” but results appear sensitive to engineering choices (e.g., bug repair tasks). The model set is skewed toward mainstream 2025 releases (17 models, such as Qwen3), ignoring niche or emerging models and non-English/multilingual prompts. Regarding fairness, the chained scoring scheme (c1=s1, c2=s2×c1/100, etc.) accounts for dependency but applies default weights (w1=w2=w3=1/3 + wd=0.1) without sensitivity analysis; by contrast, CASTLE employs weighted ensembles for greater robustness, whereas OSS-BENCH risks dominance by a single metric.\n\n- The framework is strictly limited to compiled languages (e.g., C/C++), leveraging compilation and sanitizer-based ground truth, but excludes scripting languages such as Python and JavaScript, which account for over 50% of GitHub projects according to the 2025 Stack Overflow survey. Only PHP and SQLite3 benchmarks are instantiated, and while the paper claims extensibility to projects like Nginx, Docker, or TensorFlow C-API, this requires manual adapter development (e.g., build scripts, log parsers), introducing human intervention and contradicting the core claim of “no manual or LLM involvement.” Function extraction via libclang is restricted to 10–256 token segments, filtering out ~10% of code and ignoring long functions, macros, or templates. Furthermore, multi-language hybrid projects (e.g., C++ with Python bindings) are not supported, biasing the benchmark toward infrastructure software and failing to represent mainstream domains such as web, app, and ML development. Comparable work like BUILD-BENCH has already extended automated compilation evaluation to multiple OSS projects, highlighting OSS-BENCH’s limited flexibility.\n\n- OSS-BENCH operates at an extremely large evaluation scale—for example, OSS-BENCHphp requires each model to handle roughly 38 million tasks (N=10,534 functions recompiled + 2,000 × M ≈ 19,000 test/security checks), while OSS-BENCHsql involves about 2.41 million tasks. This demands high-end hardware (e.g., AMD EPYC 9184X CPU, NVIDIA H100 GPU, 512GB RAM) and specific environments (Ubuntu 22.04/24.04). Although the paper frames this as “intentional heavy-load standardization,” it creates a significant barrier for academia and small teams: open-source models must run via Ollama with fp16 quantization, further amplifying GPU requirements, whereas closed-source models (e.g., GPT-O1) face fewer constraints. Overall, the evaluation cycle spans weeks to months, reducing reproducibility. In contrast, benchmarks like SWE-bench rely on lightweight GitHub API calls, consuming 10–100× fewer resources. OSS-BENCH’s “million-scale tasks,” while impressive, risk being perceived as over-engineered, hindering community validation and extensibility."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xN753pyV6d", "forum": "CkxjXuI7LL", "replyto": "CkxjXuI7LL", "signatures": ["ICLR.cc/2026/Conference/Submission24217/Reviewer_iSmM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24217/Reviewer_iSmM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761181374460, "cdate": 1761181374460, "tmdate": 1762943001596, "mdate": 1762943001596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OSS-Bench, an automated benchmark for evaluating the code generation performance of LLMs on real open-source software projects. OSS-Bench extracts functions fromOSS repositories, asks LLMs to rewrite them and then tests whether the code compiles, whether it passes functional tests, and whether it avoids memory-safety issues detected by fuzzing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Measuring the quality of LLM-generate code is important. The benchmark seems to achieve it's stated goals."}, "weaknesses": {"value": "The novelty is very weak and issues around memorization are mentioned but not well mitigated. Currently only a few projects are supported and all are in C or C++ limiting the generality of OSS-BENCH (though there's no reason why it couldn't be extended to other compiled languages). Currently, the given correctness properties, especially memory safety, seem only applicable to C/C++ where there is weak memory safety. The evaluation is not comprehensive and only covers particular hyper-parameters, there is no sweep over different operating points for the fuzzing part for example."}, "questions": {"value": "Why is the prompting strategy blind to the code sample being tested. I.e. prompting an LLM to improve memory safety in code that does not handle memory can result in unnecessary memory operations and potential segmentation faults. It was surprising to me that there were that many segv's from the top LLMs -- more explanation and investigation into this issue would be appreciated.\n\nPlease give more details on your fuzzer setup, how did you get/select seed inputs? The Fuzzer is PHP specific as well, how did you (or did you) test the other paplications in OSS BENCH? Why not use a generic fuzzer like AFL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dXRUAE74fX", "forum": "CkxjXuI7LL", "replyto": "CkxjXuI7LL", "signatures": ["ICLR.cc/2026/Conference/Submission24217/Reviewer_fhyK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24217/Reviewer_fhyK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849920832, "cdate": 1761849920832, "tmdate": 1762943001250, "mdate": 1762943001250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OSS-Bench, a benchmark that evaluates coding capabilities of LLMs by editing existing functions in large-scale, real-world open-source projects (PHP, SQLite), and then running a full pipeline including compilation, software testing, and memory-safety checks via sanitizers. The authors define 3 metrics and compute the scores in a chained scheme. Results on 17 models show substantial gaps between model size and real-world robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Building and testing against mature OSS projects reduces human effort, avoids static datasets, and sidesteps LLM-as-judge issues.\n2. This work emphasizes security by integrating systematic sanitizer checks and fuzzing extension.\n3. This framework is extensible to other OSS projects.\n4. The evaluation gives some insightful takeaways, such as model size does not guarantee memory safety in real codebases."}, "weaknesses": {"value": "1. This paper claims to benchmark the \"coding capabilities\" of LLMs. However, the proposed framework primarily measures function-level editing/optimization of existing code, not code generation from natural language/specifications. This somewhat weakens the claim of \"Realistic Open-Source Software Development Pipeline\" since real-world software development workflows involve more code generation than editing/optimization.\n2. The metrics do not capture readability, complexity, style consistency, or long-term maintainability. The Delta metric only encourages more change, not better code quality.\n3. Supplying only the target function with little module/call-site context can incentivize generic defensive guards (e.g., extra NULL pointer checks and range checks) that are redundant in the wider codebase. Current metrics neither detect nor penalize such redundancy.\n4. Although OSS-Bench tries to optimize the code, the evaluation lacks runtime measurements, so slower but safer edits can get more scores."}, "questions": {"value": "1. Do you plan to add runtime performance measurements so that \"optimization\" is evaluated beyond compilation/tests/sanitizers? (See weakness 4)\n2. Have you tried providing context when prompting the model to reduce redundant local checks? (See weakness 3)\n3. How would you design and validate an \"ergonomics\" score to ensure edited code remains readable and maintainable over time? (See weakness 2)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PFPUKDTeUU", "forum": "CkxjXuI7LL", "replyto": "CkxjXuI7LL", "signatures": ["ICLR.cc/2026/Conference/Submission24217/Reviewer_so3N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24217/Reviewer_so3N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921332576, "cdate": 1761921332576, "tmdate": 1762943001002, "mdate": 1762943001002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}