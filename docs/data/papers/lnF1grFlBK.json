{"id": "lnF1grFlBK", "number": 12289, "cdate": 1758206876749, "mdate": 1759897519851, "content": {"title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models", "abstract": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.", "tldr": "In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint.", "keywords": ["Mixture-of-Ternary-Experts", "Memory-efficient Models", "Large Multimodal Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e4b07825d4903944b7d30ccd29b9b317856c1b54.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes MoTE, a Mixture-of-Experts (MoE) architecture where individual experts are quantized to ternary precision during sparse up-cycling to improve memory efficiency for edge deployment. To offset the accuracy loss from ternary quantization, a frozen full-precision shared expert is introduced and reused across routing paths. Experiments on multimodal benchmarks are reported to demonstrate the method’s effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper addresses a practically important issue， reducing memory and compute costs for large multimodal models for edge applications.\n\n2. The idea of introducing a frozen shared expert to stabilize training and compensate for low-precision experts is conceptually simple and intuitive, and the method is easy to implement."}, "weaknesses": {"value": "1. Questionable scalability and necessity: Table 1 shows that MoTE performs poorly on the 0.5B model but achieves better results on 1.5B and 3B models. The authors attribute this to the known trend that larger models are easier to quantize. While this explanation is plausible, it also weakens the claimed necessity of MoTE: if quantization robustness naturally improves with scale, it remains unclear whether MoTE itself contributes meaningfully to the observed gains, or if the improvement is largely driven by model size. Moreover, MoTE’s failure on smaller models calls into question its relevance and practical utility for edge applications.\n\n2. Incomplete and non-equivalent PTQ comparison (Sec. 4.3): The PTQ evaluation compares MoTE + PTQ to MoE-LLaVA + PTQ, but this does not isolate MoTE’s robustness to post-training quantization.\n-- A fairer comparison would be MoTE + PTQ vs. MoTE (no PTQ, corresponding to Table 2), to directly show whether MoTE’s performance is stable under PTQ.\n-- Moreover, the setup is not directly comparable: MoTE applies higher-precision quantization (e.g., INT4) only to its shared expert, while all MoE-LLaVA experts are quantized to lower precision (e.g., 3-bit integers). In addition, PTQ in MoTE affects only the shared expert (the other experts are already ternary), whereas PTQ in MoE-LLaVA applies to the entire expert set. These differences make the comparison not directly meaningful.\n\n3. Uncontrolled confounding factors (Sec. 4.4).: The comparisons mix changes in dataset, token count, and training recipes (e.g., number of stages), making it unclear whether improvements stem from MoTE itself or from other variables. A controlled ablation or at least a discussion on these confounders is needed."}, "questions": {"value": "1. In the three-stage training setup, have you examined whether MoE-LLaVA suffers from catastrophic forgetting in the later stages, and whether freezing the shared expert in MoTE helps mitigate this effect?\n2. What is the relative contribution of the frozen shared expert versus the ternary experts to final performance? Where do the performance gains primarily originate?\n3. Since you argue that more low-precision experts outperform fewer high-precision ones, have you performed an ablation varying the number of experts to validate this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KPf5tq3biS", "forum": "lnF1grFlBK", "replyto": "lnF1grFlBK", "signatures": ["ICLR.cc/2026/Conference/Submission12289/Reviewer_WdCd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12289/Reviewer_WdCd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728490126, "cdate": 1761728490126, "tmdate": 1762923222399, "mdate": 1762923222399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed MoTE, which is designed to break through the severe memory bottlenecks faced by large multimodal Mixture-of-Experts (LMMs) during deployment. The core contribution of this research lies in its ingenious combination of the sparse activation inherent to the MoE architecture with the extremely efficient Ternary quantization technique, applied innovatively to the expert weights. This fusion successfully achieves greatly reduced model size and storage requirements while preserving the excellent performance potential of LMMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work presents a noteworthy improvement in memory utilization, achieving substantial efficiency gains relative to conventional approaches.\n2. The study effectively combines MoE architecture with ternary quantization, showcasing a creative and technically sophisticated integration of two complex methodologies.\n3. The paper offers valuable empirical guidance on maintaining training stability when applying aggressive quantization to large, sparse models—providing useful reference points for future research in scalable, low-precision model development."}, "weaknesses": {"value": "1. The paper’s exclusive focus on ternary (3-bit) quantization lacks adequate theoretical grounding and empirical validation. The rationale for this particular quantization level is not convincingly articulated, leaving the impression that the choice may stem from empirical convenience rather than a principled design objective. A clearer theoretical motivation is necessary to establish why the ternary configuration represents an optimal or essential component of the proposed architecture rather than one of many possible parameterizations. Specifically, the authors should analyze how the Performance–Memory trade-off curve would shift if the number of experts were reduced by half while employing a more conventional 4-bit quantization scheme—a well-recognized balance point between efficiency and fidelity in hardware-aware model design. The authors must provide quantitative and conceptual evidence demonstrating that their MoTE (ternary) formulation yields a fundamental and irreplaceable advantage compared to a MoE (INT4) configuration. Without a comprehensive trade-off analysis across quantization settings (e.g., 3-bit vs. 4-bit), the methodological choice appears ad hoc, weakening the scientific justification and limiting the generalizability of the findings.\n\n2. The experimental evaluation presently exhibits an incomplete coverage of relevant baselines, particularly those representing the strongest compression-oriented alternatives. This omission raises concerns regarding the conclusiveness of the reported improvements. To substantiate claims of efficiency and architectural merit, the evaluation should include a direct, controlled comparison between the proposed MoTE approach and a non-MoE large multimodal model (LMM) employing INT4 quantization under comparable or tighter memory constraints—for example, a LLaVA-1.5 variant enhanced through advanced sparsification or distillation techniques. Such an analysis is essential to confirm that MoTE achieves superior Memory–Performance efficiency relative to state-of-the-art quantization and sparsification methods. Absent such evidence, the claimed structural advantages of MoTE remain unverified. The authors are encouraged to prioritize this core comparison over ancillary baselines, as it directly tests the central premise of the proposed contribution."}, "questions": {"value": "1. Does MoTE deliver tangible advantages over simpler and more mature INT4 compression schemes in practical applications?\n2. Could you clarify the theoretical reasoning that led to selecting ternary (3-bit) quantization as the basis of MoTE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mOSGCUWzOm", "forum": "lnF1grFlBK", "replyto": "lnF1grFlBK", "signatures": ["ICLR.cc/2026/Conference/Submission12289/Reviewer_Pw5W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12289/Reviewer_Pw5W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918036339, "cdate": 1761918036339, "tmdate": 1762923222073, "mdate": 1762923222073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel method named MoTE (Mixture-of-Ternary-Experts) for creating memory-efficient Large Multimodal Models (LMMs) from pre-trained dense checkpoints. The authors observe that standard Mixture-of-Experts (MoE) models, which use full-precision experts, suffer from a large memory footprint, making them difficult to deploy on edge devices. To address this, MoTE proposes a new up-cycling architecture. Instead of replacing the pre-trained Feed-Forward Network (FFN), it retains the full-precision FFN as a shared expert to preserve foundational knowledge. It then adds new ternary experts (with parameters in {−1,0,1}) that are trained during the up-cycling phase. Experimental results show that MoTE achieves comparable performance to the full-precision baseline, MoE-LLaVA, at scales of 1.5B and 3B parameters , while significantly reducing the expert memory footprint (e.g., 6.8GB for MoTE vs. 18.1GB for MoE-LLaVA at the 3B scale). The authors further demonstrate MoTE's compatibility with post-training quantization (PTQ) on its shared expert, showing it outperforms the baseline by 4.3% in average accuracy under an equivalent memory budget of 3.4GB"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper presents a novel and practical architecture for memory-efficient MoE up-cycling. The core insight to retain the pre-trained FFN as a frozen, high-precision shared expert while only training new, low-precision experts is a well-motivated approach to balancing knowledge retention and memory efficiency.\n- The proposed MoTE framework achieves a compelling trade-off between performance and memory. It demonstrates performance comparable to a full-precision MoE baseline (MoE-LLaVA) at scale while drastically reducing the expert memory footprint (e.g., to just 38% of the baseline's at the 3B model size).\n- MoTE is shown to be compatible with standard post-training quantization methods. This compatibility allows its memory-saving advantages to be \"further amplified\", which is a significant practical benefit for deployment on memory-constrained devices.\n- The authors provide a strong set of ablation studies that validate their key design choices. These include demonstrating the critical importance of keeping the shared expert in full BF16 precision (vs. ternary) , the benefit of using ternary (1.58-bit) over binary (1-bit) experts, and the performance gain from initializing routed experts from the FFN."}, "weaknesses": {"value": "- The paper does not state whether the experimental results (e.g., in Table 2 and Table 3) are from a single training run or averaged over multiple runs with different random seeds. This makes it difficult to assess the statistical reliability and robustness of the reported performance gains.\n- The paper compares MoTE primarily against the full-precision MoE-LLaVA baseline and that baseline with PTQ. It does not include comparisons to other potential memory-saving techniques, such as applying parameter-efficient fine-tuning (e.g., LoRA) to the experts or alternative quantization-aware fine-tuning methods.\n- The MoTE method is predicated on having access to the full-precision, pre-trained dense checkpoint to serve as the shared expert. This means the approach would not be applicable for creating efficient MoE models from closed-source, API-only models.\n- The paper notes that the Stage III training time for MoTE is \"similar\" to the baseline (43.3 hours vs. 41.8 hours) , and attributes this to using torch.compile. Quantization-aware training (QAT) is often more computationally intensive than full-precision training. A clearer breakdown of the training compute (e.g., FLOPs) and the impact of this specific compiler optimization is missing.\n- The entire MoTE framework assumes the starting point is a full-precision dense model. It is unclear how the MoTE up-cycling approach would perform, or if it would be compatible, with a dense model that has already been quantized (e.g., an 8-bit or 4-bit base model)."}, "questions": {"value": "- Could the authors clarify whether the main results in Table 2 and the PTQ results in Table 3 are based on single runs or are averaged over multiple repetitions? This would help confirm the reliability of the findings.\n- In addition to comparing against MoE-LLaVA , have the authors considered benchmarking MoTE against other memory-saving MoE fine-tuning methods, such as applying LoRA to the full-precision experts?\n- Regarding the \"similar\" training time reported for Stage III, could the authors provide a more detailed comparison of the computational overhead (e.g., training FLOPs or time without compiler optimizations) for MoTE's QAT process versus the baseline's full-precision training?\n- Could the authors comment on the feasibility or expected performance of the MoTE up-cycling strategy if the initial dense checkpoint was not a full-precision model, but an already-quantized model (e.g., a 4-bit or 8-bit model)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E7P3MB4Ppa", "forum": "lnF1grFlBK", "replyto": "lnF1grFlBK", "signatures": ["ICLR.cc/2026/Conference/Submission12289/Reviewer_xX1m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12289/Reviewer_xX1m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979249114, "cdate": 1761979249114, "tmdate": 1762923221808, "mdate": 1762923221808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}