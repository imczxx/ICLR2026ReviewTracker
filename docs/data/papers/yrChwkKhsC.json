{"id": "yrChwkKhsC", "number": 21382, "cdate": 1758316941050, "mdate": 1759896924964, "content": {"title": "Evaluating the Promise and Pitfalls of Using LLMs in Hiring Decisions", "abstract": "Large Language Models (LLMs) hold promise for automating candidate screening in hiring, but their deployment raises serious concerns about predictive accuracy and algorithmic bias. In this work, we benchmark several state-of-the-art foundational LLMs including models from OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with a domain-specific hiring model (Match Score) for job candidate matching. We evaluate each model’s predictive accuracy (ROC AUC, Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis across declared gender, race, and intersectional subgroups). Our experiments on a dataset of roughly 10,000 real-world recent candidate-job pairs show that Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs 0.77) and achieves significantly more equitable outcomes across demographic groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957 (near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the intersectionals, respectively). We trace this gap to biases in LLM pretraining: even advanced LLMs can propagate societal biases from their training data if not adequately aligned. In contrast, the Match Score model’s task-specific training and bias-mitigation design help it avoid such pitfalls. Furthermore, we show with empirical evidence that there shouldn’t be a dichotomy between choosing accuracy and fairness in hiring: a well-designed algorithm can achieve both accuracy in hiring and fairness in outcomes. These findings highlight the importance of domain-adapted models and rigorous bias auditing for responsible AI deployment in hiring.", "tldr": "On ~10k real resume–job pairs, our domain-specific Match Score beats SOTA LLMs and sustains near-parity fairness (min IR ≈0.95 vs ≈0.80), cautioning against using off-the-shelf LLMs in hiring.", "keywords": ["algorithmic fairness", "LLMs", "hiring", "resume screening", "bias audits", "bias"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72ea110ba357f45127a81551da759d9281d419cb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper shows that a proprietary job applicant evaluation system is a Pareto improvement on various LLM's' zero-shot performance for resume screening, in terms of accuracy and fairness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The authors compare to a relatively exhaustive suite of frontier LLMs.\n2. The subject matter is clearly important, as accurate and fair evaluation of job applicants is an economically and societally valuable problem."}, "weaknesses": {"value": "1. Few details about the proprietary Match Score are provided, limiting the scientific value of the contribution.\n2. Although text is vague on this point, it seems like the Match Score was trained using a split of the same Brown (2025) dataset that was used for evaluation. The authors emphasize that the train and test data are temporally split to ensure that no test data was used for training (which is good). However, this means that the train and test distributions are likely very similar, raising the question of how Match Score would perform on data from another distribution. So the results would be more convincing if also evaluated on a different dataset of labeled applicant resumes. \n3. Similarly, there should be more justification provided for the claim that temporally splitting the Brown (2025) dataset means the test set is sufficiently held-out; for example, is there a risk that a candidate applied to multiple jobs at different times with the same resume?\n4. It is not very surprising that task-specific fine-tuning (especially on a distribution very similar to the test distribution) outperforms zero-shot transfer from general LLMs. But given that fine-tuning many of the LLMs studied is easy (either through the OpenAI APi or the numerous fine-tuning APIs for open models), it would be valuable to compare to those fine-tuned models. (Of course, for all we know, that is exactly what Match Score is.) (I note that the authors do acknowledge this limitation.)\n5. The discussion describes their results as challenging the notion that there is a trade-off between skill-based hiring and fair hiring. But this is not a valid conclusion: Their results only show it is empirically possible to Pareto improve on prompted LLMs. in fact, various other parts of the paper (e.g. the paragraph on \"Fairness-by-design constraints\") suggest that there were trade-offs between accuracy and fairness made during the construction of Match Score."}, "questions": {"value": "Aside from the questions above, can you clarify whether data from Brown (2025) were used in the training of Match Score?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cqmQKDDX4G", "forum": "yrChwkKhsC", "replyto": "yrChwkKhsC", "signatures": ["ICLR.cc/2026/Conference/Submission21382/Reviewer_RRQw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21382/Reviewer_RRQw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761514837931, "cdate": 1761514837931, "tmdate": 1762941734806, "mdate": 1762941734806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper benchmarks a proprietary, task-specific hiring model (“Match Score”) against a set of general-purpose LLMs on resume–job relevance using ~10k recent candidate–job pairs. Inputs are PII-masked; accuracy is reported via ROC-AUC, PR-AUC, and F1 (at the median threshold), and fairness via selection-rate Impact Ratios (IR) across gender, race, and intersectionals with confidence intervals. The headline result is that Match Score outperforms all evaluated LLMs on accuracy and maintains substantially higher worst-case IRs, suggesting that a domain-adapted, fairness-aware model can achieve both accuracy and equitable outcomes."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The topic is societally important. The input standardization and temporal split are reasonable. Reporting CIs for both AUCs and IRs is good practice. The intersectional reporting goes beyond many prior audits and the governance framing (periodic audits, drift monitoring, decision support) is sensible."}, "weaknesses": {"value": "The technical depth is limited: there is no new algorithm or principled fairness method; the paper is largely an applied benchmark with a proprietary winner. Fairness analysis is anchored to selection-rate IR at the median threshold, which is fragile and does not establish robustness across realistic operating points (fixed precision/recall, top-k%) or under rank-based measures. Standard criteria like equalized odds, TPR/FPR gaps, predictive parity, and groupwise calibration are absent.\nLabel bias is unaddressed: the outcome labels (onsite/offer/hire) likely encode historical discrimination; without outcome tests, reweighting/DR adjustments, or counterfactual checks, it is unclear whether the reported “fairness” reflects equitable modeling or replication of biased historical decisions.\nThe proprietary baseline is under-specified: claims about removing “obvious proxies” are not validated empirically (e.g., adversarial subgroup prediction, mutual information, subgroup SHAP). LLM evaluation is narrow (single zero-shot prompt, minimal sensitivity to prompting/temperature/few-shot/JSON schema), inviting concerns that the LLM side is under-optimized.\nReproducibility is weak: data are private, the winning system is proprietary, and transparent baselines trained on the same masked inputs (e.g., LR/GBM/transformer encoders) are missing, making it difficult to attribute gains to domain adaptation rather than pipeline choices."}, "questions": {"value": "1. How did you assess label bias in historical outcomes (e.g., outcome tests, doubly robust adjustments, controls for observable qualifications)?\n2. What features remain post-masking, and how did you test for proxy leakage (adversarial prediction of protected attributes, MI tests, subgroup SHAP)?\n3. Do results hold across multiple operating points (fixed precision/recall, top-k%) and under equalized-odds and TPR/FPR parity? Please add rank-based metrics (e.g., RAS, rank-IR) and permutation tests.\n4. Provide calibration curves and Brier scores overall and by subgroup; is Match Score calibrated equitably?\n5. Report per-group sample sizes (including intersectionals) and identify any excluded cells due to small n.\n6. Add transparent non-proprietary baselines on the same masked inputs to disentangle domain adaptation from proprietary engineering.\n7. Stratify by industry/seniority/role family (fixed effects) to localize disparities and compare with prior findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MnI3oEVFMQ", "forum": "yrChwkKhsC", "replyto": "yrChwkKhsC", "signatures": ["ICLR.cc/2026/Conference/Submission21382/Reviewer_aw9B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21382/Reviewer_aw9B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905991277, "cdate": 1761905991277, "tmdate": 1762941734073, "mdate": 1762941734073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The study examines whether general-purpose LLMs can handle recruitment screening, testing multiple proprietary and open-source models against a specialized system called Match Score using ~10k actual applicant-position matches. Working with anonymized CVs and position requirements, models generate relevance ratings that get split at each system's midpoint to calculate performance and equity measures. Match Score outperforms all LLMs with ROC AUC 0.85 versus 0.77 for the strongest LLM, plus superior demographic parity ratios. The research suggests specialized training with built-in equity constraints yields superior performance and demographic balance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Precise impact ratio formulation with comprehensive demographic and intersectional breakdowns plus statistical bounds.\n- Systematic anonymization workflow ensuring uniform treatment across evaluated systems.\n- Detailed performance matrices displaying both accuracy and selection percentages across demographics."}, "weaknesses": {"value": "- The overall presentation should be a lot improved. For example, usage of bullet points in model definition and metrics doesn't look like a proper research paper. I would move ethical statement and reproducibility statement after conclusions.\n- Model-specific midpoint cutoffs advantage systems with particular score distributions—threshold-agnostic curves, disparity-threshold relationships, and performance-matched comparisons would strengthen analysis.\n- Match Score's demographic-aware optimization directly targets the evaluation criterion, necessitating ablation studies without equity constraints and LLM calibration at comparable operating conditions.\n- Absent demographic cell counts and masking effectiveness audits limit interpretability of statistical bounds and proxy elimination claims.\n- Acknowledged instruction sensitivity lacks empirical quantification through variation studies with error bars."}, "questions": {"value": "- Could you generate threshold-disparity curves and performance-matched analyses for each protected class, enabling model comparison at equivalent utility rather than arbitrary midpoints, including opportunity and odds parity metrics?\n- How does Match Score perform without demographic constraints while maintaining other training conditions-- isolating deliberate optimization from inherent architecture advantages?\n- Please include population counts for Table 2's demographic categories and intersections, plus minimum samples underlying IR calculations.\n- Demonstrate stability across three instruction variations and two generation temperatures, presenting averages and variance for accuracy and IR, including structured output handling and non-compliance protocols.\n- Would basic adaptations like demonstration prompting or minimal fine-tuning narrow observed disparities, even preliminarily testing whether generic LLMs truly resist recruitment applications?\n- Verify anonymization effectiveness empirically-- perhaps training demographic predictors on masked CVs and reporting classification performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "533Ck3rNxw", "forum": "yrChwkKhsC", "replyto": "yrChwkKhsC", "signatures": ["ICLR.cc/2026/Conference/Submission21382/Reviewer_5ynH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21382/Reviewer_5ynH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21382/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997807962, "cdate": 1761997807962, "tmdate": 1762941733641, "mdate": 1762941733641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}