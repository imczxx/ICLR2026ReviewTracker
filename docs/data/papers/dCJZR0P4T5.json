{"id": "dCJZR0P4T5", "number": 12078, "cdate": 1758205558684, "mdate": 1763287119758, "content": {"title": "Stop Tracking Me! Proactive Defense Against Attribute Inference Attack in LLMs", "abstract": "Recent studies have shown that large language models (LLMs) can infer private user attributes (e.g., age, location, gender) from user-generated text shared online, enabling rapid and large-scale privacy breaches. Existing anonymization-based defenses are coarse-grained, lacking word-level precision in anonymizing privacy-leaking elements. Moreover, they are inherently limited as altering user text to hide sensitive cues still allows attribute inference to occur through models' reasoning capabilities.\nTo address these limitations, we propose a unified defense framework that combines fine-grained anonymization (TRACE) with inference-preventing optimization (RPS). TRACE leverages attention mechanisms and inference chain generation to identify and anonymize privacy-leaking textual elements, while RPS employs a lightweight two-stage optimization strategy to induce model rejection behaviors, thereby preventing attribute inference. \nEvaluations across diverse LLMs show that TRACE-RPS reduces attribute inference accuracy from around 50\\% to below 5\\% on open-source models. In addition, our approach offers strong cross-model generalization, prompt-variation robustness, and utility-privacy tradeoffs.", "tldr": "We propose TRACE-RPS, a proactive, unified defense framework that combines fine-grained text anonymization and optimized perturbation to effectively prevent attribute inference attacks in large language models.", "keywords": ["Privacy leakage;Inference-preventing optimization;Text anonymization;Attribute inference attack;Large language models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b8afb2b0a08e6b4b17394813f0fdf18f80a679cb.pdf", "supplementary_material": "/attachment/dbcfa13983beec422793217bd636c2df9143ca9d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces an anonymization method against private attribute inference by LLMs consisting of two main componenst: i) TRACE, an iterative rewriting method based on a proxy attribute inference model and inference attention scores from a white box model; and ii) RPS/MPS a suffix optimization component aimed at either misleading the attacker or making it reject the inference request. The method is evaluated on synthetic datasets taken from prior work and (in the appendix) on real world examples labeled by the authors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Using attention scores, as confirmed also by the evaluation, is an effective addition to the adversarial anonymization loop of Staab et al. 2025.\n- Important problem.\n- Making the attacker LLM reject inference queries is a promising novel direction for anonymization against LLM inference.\n\n**References**\n\nR Staab, et al. Language Models are Advanced Anonymizers. ICLR 2025."}, "weaknesses": {"value": "**Main**\n\nThe paper combines two methods that do not fit well together.\n\nThe adversarial rewriting is adequate to provide users with a way to still use their text in natural contexts while sanitizing them, somewhat irreversibly (to be precise, up to the final accuracy of the best inference). Here, the introduction of attention-score-based privacy sensitive words is a nice contribution over Staab et al.'s framework and is validated by the improvements in results, albeit leading to a slightly lower utility score. However, this alone, is, in my view not a sufficient contribution for acceptance.\n\nAt the same time, the suffix optimization component is entirely complementary and does not serve the same use-case in my view. The suffix can be removed, perhaps easily, although it is hard to assess as the paper does not give many details or examples about what these suffixes look like. At the same time, such suffixes would appear strange in the real world in real use-cases, decreasing the usability and utility of the text in natural conversational contexts, such as Reddit. To this end, it is also surprising that the LLM utility measurement is missing for the suffix optimization case. Additionally, again highlighting that the suffix optimization is a different setting from simply iterative rewriting, it would be important here to evaluate if now any model under any task is more likely to refuse if the suffix is present. This evaluation is important for two reasons: i) from an outcome perspective, ideally, the suffix should only protect against attribute inference, and still permit benign use-cases that might be necessary/useful on the text, e.g., summarization; and ii) from a technical perspective, it is rather trivial to construct a suffix that leads to high rates of refusal indiscriminately of the context (e.g., an explicit instruction for something that goes against the alignment of the model).\n\nOverall, in its current state, the paper lacks the technical novelty and insights sufficient for publication.\n\n**Minor**\n\nThe ASR metric is slightly misleading, as the non-zero score obtained by random-guessing serves 0 additional information to the attacker, and therefore, I suggest to drop this metric. Instead, what is more informative, would be to include a random guessing baseline into the evaluation presentations. This would show the actual information gain of the attacker they have from the inference (the difference of the attribute inference accuracy and the random guessing accuracy).\n\nAs described in the appendix, the models are evaluated at 0 temperature for inference. This might not lead to the best performance, and thus would underestimate the inference risk. I suggest choosing a higher temperature and sampling several responses to have a more robust estimate of the (near worst-case?) inference risk."}, "questions": {"value": "- What exactly are \"Highly Instruction-Following Models\"?\n- Why are the inference scores on, e.g., SynthPAI different from the original paper (compare for instance Llama 2 7B & 13B between table 1 here and figure 4)?\n- Can you give some examples of the refusal suffixes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mO2b00hCIj", "forum": "dCJZR0P4T5", "replyto": "dCJZR0P4T5", "signatures": ["ICLR.cc/2026/Conference/Submission12078/Reviewer_nGu9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12078/Reviewer_nGu9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761063623903, "cdate": 1761063623903, "tmdate": 1762923050361, "mdate": 1762923050361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an anonymization method against private attribute inference by LLMs consisting of two main componenst: i) TRACE, an iterative rewriting method based on a proxy attribute inference model and inference attention scores from a white box model; and ii) RPS/MPS a suffix optimization component aimed at either misleading the attacker or making it reject the inference request. The method is evaluated on synthetic datasets taken from prior work and (in the appendix) on real world examples labeled by the authors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Using attention scores, as confirmed also by the evaluation, is an effective addition to the adversarial anonymization loop of Staab et al. 2025.\n- Important problem.\n- Making the attacker LLM reject inference queries is a promising novel direction for anonymization against LLM inference.\n\n**References**\n\nR Staab, et al. Language Models are Advanced Anonymizers. ICLR 2025."}, "weaknesses": {"value": "**Main**\n\nThe paper combines two methods that do not fit well together.\n\nThe adversarial rewriting is adequate to provide users with a way to still use their text in natural contexts while sanitizing them, somewhat irreversibly (to be precise, up to the final accuracy of the best inference). Here, the introduction of attention-score-based privacy sensitive words is a nice contribution over Staab et al.'s framework and is validated by the improvements in results, albeit leading to a slightly lower utility score. However, this alone, is, in my view not a sufficient contribution for acceptance.\n\nAt the same time, the suffix optimization component is entirely complementary and does not serve the same use-case in my view. The suffix can be removed, perhaps easily, although it is hard to assess as the paper does not give many details or examples about what these suffixes look like. At the same time, such suffixes would appear strange in the real world in real use-cases, decreasing the usability and utility of the text in natural conversational contexts, such as Reddit. To this end, it is also surprising that the LLM utility measurement is missing for the suffix optimization case. Additionally, again highlighting that the suffix optimization is a different setting from simply iterative rewriting, it would be important here to evaluate if now any model under any task is more likely to refuse if the suffix is present. This evaluation is important for two reasons: i) from an outcome perspective, ideally, the suffix should only protect against attribute inference, and still permit benign use-cases that might be necessary/useful on the text, e.g., summarization; and ii) from a technical perspective, it is rather trivial to construct a suffix that leads to high rates of refusal indiscriminately of the context (e.g., an explicit instruction for something that goes against the alignment of the model).\n\nOverall, in its current state, the paper lacks the technical novelty and insights sufficient for publication. -- Post-rebuttal edit, with the rebuttal experiments incorporated, the paper stands on a solid-enough empirical ground for publication.\n\n**Minor**\n\nThe ASR metric is slightly misleading, as the non-zero score obtained by random-guessing serves 0 additional information to the attacker, and therefore, I suggest to drop this metric. Instead, what is more informative, would be to include a random guessing baseline into the evaluation presentations. This would show the actual information gain of the attacker they have from the inference (the difference of the attribute inference accuracy and the random guessing accuracy).\n\nAs described in the appendix, the models are evaluated at 0 temperature for inference. This might not lead to the best performance, and thus would underestimate the inference risk. I suggest choosing a higher temperature and sampling several responses to have a more robust estimate of the (near worst-case?) inference risk."}, "questions": {"value": "- What exactly are \"Highly Instruction-Following Models\"?\n- Why are the inference scores on, e.g., SynthPAI different from the original paper (compare for instance Llama 2 7B & 13B between table 1 here and figure 4)?\n- Can you give some examples of the refusal suffixes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mO2b00hCIj", "forum": "dCJZR0P4T5", "replyto": "dCJZR0P4T5", "signatures": ["ICLR.cc/2026/Conference/Submission12078/Reviewer_nGu9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12078/Reviewer_nGu9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761063623903, "cdate": 1761063623903, "tmdate": 1763470460574, "mdate": 1763470460574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a defense against attribute inference attacks from LLMs. The authors propose TRACE (Textual Revision via Attention and Chain-based Editing), which uses fine-grained privacy vocabulary extraction and inference chain generation to anonymize sensitive text, as well as RPS (Rejection-Oriented Perturbation Search), which uses a two-stage optimization to introduce suffix-based perturbations to induce model rejection, and prevent attribute inference. They also propose an alternative strategy MPS (Misattribute Oriented Perturbation Search), which redirects attribute predictions toward incorrect attributes, for highly instruction-following models. The authors run experiments across a range of LLMs and demonstrate that TRACE-RPS significantly outperforms existing defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors ran evaluations with a diverse range of LLMs and found significant improvements using TRACE-RPS compared to the baselines.\n- The method allows users to proactively protect their privacy before sharing their text."}, "weaknesses": {"value": "- The authors mention evaluations on real-world Reddit data in Appendix C. However, they provided very little information. It would be helpful to describe how the data was collected, and the size of the data. Moreover, it was only evaluated with a subset of the models used in the main text.\n- TRACE shows significantly less improvement as compared to TRACE-RPS, and in some cases performs worse than the baseline. Hence the improvements are more significant for open source models and more work might be needed for closed source models.\n- No strong theoretical basis for why TRACE works. For instance, why should the tokens with the highest attention scores be the ones that should be anonymized?"}, "questions": {"value": "- Why are some results unavailable in Table 1?\n- Could more details be provided about the evaluation on real-world Reddit data?\n- What is the computational cost and latency of running the new framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mYurwBr8DJ", "forum": "dCJZR0P4T5", "replyto": "dCJZR0P4T5", "signatures": ["ICLR.cc/2026/Conference/Submission12078/Reviewer_YMQc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12078/Reviewer_YMQc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761418359565, "cdate": 1761418359565, "tmdate": 1762923049180, "mdate": 1762923049180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work presents multiple techniques (and combinations thereof) to improve natural text anonymization against LLM-based attribute inferences. For this, the authors introduce TRACE and RPS/MPS. TRACE builds on prior work in LLM-based adversarial text anonymization, extending it to account for individual attention scores of a given LLM during the inference process, thus specifically targeting certain tokens that appear particularly impactful. RPS/MPS is orthogonal and directly appends a random-search optimized suffix to a given string that causes a baseline LLM inference to either refuse (RPS) or mispredict (MPS). The evaluation, conducted on two synthetic datasets and complemented by a real-world study in the appendix, shows that TRACE alone improves on existing methods such as FgAA, albeit with a slightly higher utility impact. Furthermore, RPS/MPS is consistently effective at preventing adversarial inferences through either refusal or misprediction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Better, practically applicable text anonymization is a relevant topic, and the paper proposes two novel contributions in this regard.\n- TRACE improves upon the baseline FgAA w.r.t. adversarial inference accuracy consistently (although in cases at the cost of utility).\n- RPS/MPS introduces a new approach to mitigating LLM-based inferences, which has not been specifically explored for this issue before.\n- The paper includes extensive experiments on both synthetic and real-world data.\n- Further it also provides comprehensive ablations, covering different inference prompts (particularly relevant for RPS), TRACE vs. FgAA, positional robustness of RPS, and general settings."}, "weaknesses": {"value": "- TRACE (and TRACE-RPS) shows some loss in utility compared to FgAA. This is not an unexpected trade-off, as it achieves slightly stronger anonymization. At the same time, the RPS utility comparison is somewhat unclear in this setting, since the modification introduced by RPS (based on the given description) is more localized, i.e., noticeable to human readers or simpler adversarial detection methods such as perplexity filters, which is not captured by the utility metric used. To the reviewer, the practical utility impact of RPS/MPS samples remains unclear after reading this work, and potential downsides or pitfalls appear underexplored, particularly the ease of filtering and the change in human perception. Overall the main contribution of the paper feels more like TRACE while RPS/MPS is more of an application of adversarial suffixes (or prefixes) that feel not really realistic given the setting\n- It would be relevant to include more scaling experiments on recent frontier models to verify whether TRACE maintains a consistent advantage even when the baseline FgAA already performs well. Using Gemini 2.5 Pro is a good step, but both GPT 3.5 and 4o are somewhat outdated in this respect. \n- On page 16, you mention a confidence threshold that does not appear elsewhere in the paper. Could you provide additional results on this, including how confident the final evaluation model is in the predictions reported in the main table?\n- The final evaluation model (the adversary) appears to be GPT-4 (Table 1 header). While the reviewer acknowledges rapid progress in model development is acknowledged, it would be valuable to include results using a stronger adversary—for example, in a rebuttal—to demonstrate the stability of the presented results under changing adversaries."}, "questions": {"value": "Besides the points raised above, I have the following questions:\n\n- Could you provide more details about the human-generated dataset?\n- In Appendix E, is the suffix simply moved there, or is it optimized there? Could you give a few practical examples of this suffix?\n- The K.4 example seems particularly interesting, as the inference itself appears to lean more toward a male prediction, which is then followed by a female prediction. Did you observe similar behaviors frequently, and could this be something an adversary might exploit?\n- Can you generally provide full examples of the suffix and multi-step FgAA and Trace results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pQkhySExw6", "forum": "dCJZR0P4T5", "replyto": "dCJZR0P4T5", "signatures": ["ICLR.cc/2026/Conference/Submission12078/Reviewer_yTGc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12078/Reviewer_yTGc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840537105, "cdate": 1761840537105, "tmdate": 1762923048780, "mdate": 1762923048780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel defense method against attribute inference attacks conducted by powerful LLMs, named TRACE-RPS. TRACE employs an iterative adversarial framework to progressively remove private information contained in user-generated content. The anonymization process simultaneously leverages privacy vocabulary knowledge and privacy inference chains to achieve more effective anonymization. Furthermore, the proposed RPS module searches for specific suffixes that guide the LLM to reject privacy inference queries. Extensive experimental results demonstrate the effectiveness of the proposed method in achieving user privacy anonymization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe idea to utilized information from both the privacy vocabulary and adversarial privacy inference chain can offer more fine-grained anonymization compared to previous baselines.\n2.\tBefore releasing the user-generated texts, optimizing the rejected incentive suffix and appending it to the user query to induce the LLM to reject privacy inference attacks is interesting. Utilizing such simple suffixes can significantly enhance the safety of user queries by preventing strong LLMs from performing privacy inference.\n3.\tThe extensive experimental results demonstrate the effectiveness and generalization capability of the proposed method in protecting user privacy contained in user-generated texts."}, "weaknesses": {"value": "The assumed adversarial attacker appears to be somewhat idealized. The proposed method searches for perturbed suffixes to the user query to guide the LLM in rejecting privacy-inference attempts. However, a more advanced attacker might randomly discard these suffixes to mitigate their influence. Although the authors conduct additional experiments by applying perturbations to the prefix and infix positions, advanced attackers could still identify and remove such perturbations before prompting the LLM to infer private information. Considering more powerful and adaptive attackers would provide a more realistic assessment of the proposed method’s actual privacy protection capability.\n\tThe essential cost analysis is missing. Although the authors claim that only one or two tokens are decoded during the RPS and MPS modules, the computational cost of the TRACE module is not discussed. As shown in Table 1, the authors use closed-source LLMs such as GPT-3.5-turbo and GPT-4o for inference-chain generation and anonymization. Repeatedly invoking these models via API calls could incur considerable additional cost. \n\tSome implementation details are missing, and certain descriptions are confusing. As shown in Equation (7), the authors first extract a common privacy vocabulary for all potential private attributes A. However, in Equation (8), the extraction function takes two inputs, (t,a), which seems to imply that each attribute ahas its own privacy vocabulary V. Moreover, the meanings of T_\"def\" ^vand T_\"def\" ^care unclear, as well as why they take tand V(or C) as inputs. More detailed explanations would help clarify these points"}, "questions": {"value": "1.\tWhy does the search for perturbed suffixes involve randomly selecting tokens and replacing them with random alternatives? This strategy seems potentially inefficient for the overall search process. Incorporating heuristic guidance might improve the efficiency and effectiveness of the perturbation search.\n2.\tHow is the Utility metric in Table 4 computed? As stated in Lines 456–457, the authors calculate two scores: the LLM-judge score and the semantic similarity score. Is the Utility metric the average of these two scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mbJbqvS6QR", "forum": "dCJZR0P4T5", "replyto": "dCJZR0P4T5", "signatures": ["ICLR.cc/2026/Conference/Submission12078/Reviewer_mEFa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12078/Reviewer_mEFa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892523801, "cdate": 1761892523801, "tmdate": 1762923048360, "mdate": 1762923048360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}