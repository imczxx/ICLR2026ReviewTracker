{"id": "UBhY1c4r2W", "number": 21325, "cdate": 1758316260446, "mdate": 1759896928433, "content": {"title": "PoSh: Using Scene Graphs to Guide LLMs-as-a-Judge for Detailed Image Descriptions", "abstract": "While vision-language models (VLMs) have advanced into detailed image description, evaluation remains a challenge. Standard metrics (e.g. CIDEr, SPICE) were designed for short texts and tuned to recognize errors that are now uncommon, such as object misidentification. In contrast, long texts require sensitivity to attribute and relation attachments and scores that localize errors to particular text spans. In this work, we introduce PoSh, a metric for detailed image description that uses scene graphs as structured rubrics to guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained errors (e.g. mistakes in compositional understanding). PoSh is replicable, interpretable and a better proxy for human raters than existing metrics (including GPT4o-as-a-Judge). To validate PoSh, we introduce a new dataset, DOCENT. This novel benchmark contains artwork, paired with expert-written references, and model-generated descriptions, augmented with granular and coarse judgments of their quality from art history students. Thus, DOCENT enables evaluating both detailed image description metrics and detailed image description itself in a challenging new domain. We show that PoSh achieves stronger correlations (+0.05 Spearman ρ) with the human judgments in DOCENT than the best open-weight alternatives, is robust to image type (using CapArena, an existing dataset of web imagery) and is a capable reward function, outperforming standard supervised fine-tuning. Then, using PoSh, we characterize the performance of open and closed models in describing the paintings, sketches and statues in DOCENT and find that foundation models struggle to achieve full, error-free coverage of images with rich scene dynamics, establishing a demanding new task to gauge VLM progress. Through both PoSh and DOCENT, we hope to enable advances in important areas such as assistive text generation.", "tldr": "", "keywords": ["detailed image description", "metric", "benchmark", "art", "VLMs"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/caf8e7b1f2eba6535b788c1c32800c2f51a0ab99.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes PoSh, a new metric for evaluating fine-grained image descriptions. PoSh extracts scene graphs from both reference and generated descriptions, preserving the object–attribute–relation structure, and uses these graphs as a structured scoring criterion to guide open-source LLMs in performing fine-grained judgments. This enables the identification of mistakes and omissions in generated text.\n\nThe authors construct a new benchmark, DOCENT, comprising 1,750 artworks from the National Gallery of Art, expert-written descriptions, outputs from VLMs, 300 fine-grained annotations, and 600 coarse-grained pairwise ratings provided by art history students. Experiments show that PoSh outperforms existing metrics on both DOCENT and CapArena. It can also serve effectively as a reward function in reinforcement learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. PoSh addresses a clear gap in evaluating detailed image descriptions. By using scene graphs as structured rubrics, PoSh enables error localization and produces human-aligned, interpretable scores.\n2. Built entirely on open-weight models and public tools, PoSh avoids reliance on proprietary APIs (e.g., GPT-4o), making it accessible and deployable for researchers with limited resources.\n3. The paper introduces DOCENT—a novel dataset of expert-written art descriptions paired with granular and coarse human judgments from domain-knowledgeable annotators. This enables evaluation for complex visual domains."}, "weaknesses": {"value": "1. **Computational overhead may hinder scalability**: The pipeline, comprising scene graph extraction, multi-pass identifier generation, and LLM-based QA, is considerably more complex than standard metrics. The paper does not include ablation studies or timing analyses of individual components. Without such efficiency profiling, it remains unclear whether PoSh offers a favorable trade-off between evaluation quality and computational cost in large-scale settings.\n2. **Lack of validation as a data curation tool for MLLM training**: While the paper demonstrates PoSh’s effectiveness as a reinforcement learning reward signal, it does not explore its utility in filtering or ranking training data for MLLM fine-tuning. Comparing models trained on PoSh-filtered data versus those trained with other metrics would better establish PoSh’s practical value in the full model development lifecycle."}, "questions": {"value": "Do captioning models that incorporate structural priors (e.g., spatial or scene graph inputs) benefit disproportionately under PoSh?\nSome recent methods explicitly inject visual relational priors—such as object positions or scene graph structures—into the captioning process. \n1. Since PoSh itself uses scene graphs as evaluation rubrics, such models might receive inflated scores due to structural alignment between generation and evaluation, rather than superior semantic fidelity. Have the authors evaluated PoSh on outputs from graph-aware captioning systems (e.g., SG-LLaVA [1])?\n2. I also wonder whether the authors think that incorporating structural priors during generation might be more important than adding them during verification.\n\n[1] Jingyi Wang, Jianzhong Ju, Jian Luan, and Zhidong Deng. LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SQb53V358c", "forum": "UBhY1c4r2W", "replyto": "UBhY1c4r2W", "signatures": ["ICLR.cc/2026/Conference/Submission21325/Reviewer_XyHZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21325/Reviewer_XyHZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764833457, "cdate": 1761764833457, "tmdate": 1762941695969, "mdate": 1762941695969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents POSH, a new metric for detailed image description. It computes scores grounded in fine-grained errors by adopting scene graphs to guide LLMs-as-Judge. POSH is interpretable and aligns better with human validation. Additionally, they propose DOCENT, a new dataset of artwork, references, and descriptions. They validate POSH on DOCENT and find it has better correlation with human judgments. By benchmarking open and closed-source models on DOCENT, they identify strengths and weaknesses of VLMs in understanding images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- They focus on an important aspect of VLM understandings. They focus on the detailed description, and the metric is interpretable. \n- They propose a benchmark with expert-written descriptions and 900 granular & coarse judgments from raters. The manual effort is massive.\n- They open-sourced the benchmark and metric, which will benefit the community. \n- They evaluate multiple open-source and closed-source models."}, "weaknesses": {"value": "- The writing could be better. For example, in the table, they use POSH to denote the finetuned Qwen model with POSH reward, while POSH is a metric in the meantime. This is a bit confusing.\n- For the findings of POSH as a reward function, they only experiment with the Qwen2.5-VL-7B model. The findings may not be model-agnostic. \n- It is concerning that POSH works better on their proposed DOCENT benchmark but is adequate on other benchmarks like CapArena."}, "questions": {"value": "- Could you provide results with other VLMs other than Qwen2.5-VL?\n- What is the text and image distribution difference between DOCENT and CapArena? Is there any other caption benchmarks you could use to validate the effectiveness of POSH?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jvCIPdZDRD", "forum": "UBhY1c4r2W", "replyto": "UBhY1c4r2W", "signatures": ["ICLR.cc/2026/Conference/Submission21325/Reviewer_cNnP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21325/Reviewer_cNnP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959789019, "cdate": 1761959789019, "tmdate": 1762941695304, "mdate": 1762941695304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose PoSh, a reference-based metric for long-form image captioning. PoSh works by first constructing scene graphs for both the reference and generated captions. The metric then constructs templated questions for both the reference and generated captions' scene graphs and answers these questions with an LLM judge to determine the recall and precision, respectively, of the generated caption. As the metric generates numerous questions for both scene graphs, PoSh can yield both granular and coarse assessments of caption quality.\n\nThe authors validate alignment with human judgements on CapArena, a pre-existing benchmark, and DOCENT, a dataset containing artwork with expert-written captions that they introduce. PoSh outperforms baseline metrics for granular identification of mistakes and omissions and is either competitive with or outperforms existing metrics for coarser evaluations. The authors also validate their metric as a reward function for RL training, finding improvement over simple SFT."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper has numerous strengths:\n- Firstly, the task of long-form image captioning evaluation is an important one as models continuously improve in capabilities. PoSh acts as an important contribution within this space by proposing a straightforward reference-based metric that converts the references and generations into scene graphs and using these to assess precision and recall. Particularly, the use of questions to assess both precision and recall lends the metric interpretability and granularity.\n- The method outperforms prior baselines for granular evaluations and is competitive with or better than other metrics for coarse evaluations.\n- The authors propose a new benchmark, DOCENT, with the novel domain of visual art. DOCENT is coupled with expert-written reference captions and human judgements for a range of vision-language models. This not only helps the evaluation of the metric but could also be used as a testbed for future metrics or for the generation quality of other vision-language models.\n- Additionally evaluating PoSh's performance as a reward model makes the evaluation of the metric very complete. I can imagine the granular nature of PoSh being also used to generate targeted natural language feedback for models."}, "weaknesses": {"value": "The main weaknesses I can see are:\n- PoSh is going to be sensitive to the accuracy of the extracted scene graphs, where there could be errors either during the dependency parsing process or during coreference resolution. Figure 3 marking \"painting\" as a mistake acts as one example of this. \n- While I think PoSh could act as a strong reward model, it does presume access to detailed reference captions, which is expensive to curate on a large scale. PoSh being reference-based similarly restricts its use for evaluation to dedicated datasets for this purpose."}, "questions": {"value": "The metric's performance on the various evaluations already gives good signal regarding its quality. The paper would nonetheless be improved through an evaluation of the intermediate components of the metric itself. For instance, how accurate is the scene graph extraction (as measured via precision and recall against reference human-annotated scene graphs)? Alternatively, as this might be more feasible during the rebuttal period, how accurate are the LLM judge's answers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sZfXrgpfcy", "forum": "UBhY1c4r2W", "replyto": "UBhY1c4r2W", "signatures": ["ICLR.cc/2026/Conference/Submission21325/Reviewer_hCTY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21325/Reviewer_hCTY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969819097, "cdate": 1761969819097, "tmdate": 1762941694349, "mdate": 1762941694349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new metric for evaluating detailed image descriptions, termed POSH. POSH utilizes scene graphs as structured guidelines to direct LLMs in assessing fine-grained errors in image descriptions, such as compositional correctness. POSH offers a replicable and interpretable evaluation experience, outperforming existing metrics, including GPT4o. Additionally, the authors introduce a new dataset, DOCENT, which contains artwork paired with expert-written descriptions and model-generated descriptions. The dataset also includes annotations from experts, providing a challenging benchmark for evaluating the detailed description of images. Furthermore, the authors proof that POSH can be used as a reward function to achieve better performance than standard sft."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well-written and easy to follow.\n* Evaluating the image description is indeed a non-trivial task, and the proposed new metric for evaluating detailed descriptions is important for the field of image captioning.\n* I agree that a good metric for image-description should be grounded on fine-grained cues, localized on text spans.\n* The paper introduced the DOCENT dataset, which includes expert-written descriptions and annotations, and the quality is well controlled."}, "weaknesses": {"value": "* POSH is reliance on a model to generate the scene graph introduces inaccuracies and errors, which could be a potential bottleneck for its effectiveness.\n* The use of scene graphs to evaluate image-text alignment has been discussed in previous papers like [1]; the authors need to clarify the uniqueness of POSH.\n* The proposed dataset covers artworks, but in practical applications, images of natural scenes are more common.\n\n[1] Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation"}, "questions": {"value": "* Why choose artworks as a benchmark? Instead of other more common or more representative domains\n* How to ensure repeatability, as there are probabilistic models used (qwen3)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uP9ufmIwO8", "forum": "UBhY1c4r2W", "replyto": "UBhY1c4r2W", "signatures": ["ICLR.cc/2026/Conference/Submission21325/Reviewer_6TR7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21325/Reviewer_6TR7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997927204, "cdate": 1761997927204, "tmdate": 1762941693620, "mdate": 1762941693620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}