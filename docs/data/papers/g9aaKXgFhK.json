{"id": "g9aaKXgFhK", "number": 22280, "cdate": 1758328958151, "mdate": 1759896875169, "content": {"title": "Contrastive Gradient Guidance for Test-time Preference Alignment of Diffusion Models", "abstract": "Pre-trained diffusion models demonstrate remarkable performance in text-to-image generation, with current research efforts directed toward aligning them with human preferences across diverse application scenarios. Existing approaches often rely on costly pipelines that require collecting preference data, training reward models, and fine-tuning. A promising alternative is test-time alignment, which steers diffusion models during sampling without retraining. However, current test-time alignment methods typically depend on explicit reward models to provide a guidance signal for modifying a sampling path. These involve decoding a noisy image and estimating its rewards, which adds extra steps with computational overhead and might limit flexibility across diverse scenarios. We propose Contrastive Gradient Guidance (CGG), a conceptually straightforward and practical framework for test-time alignment that avoids explicit reward models by design. CGG derives its guidance signal from the contrastive difference between two diffusion models, parameterized through the gradient of the log-likelihood ratio of the favored and the unfavored distributions. The guidance signal steers a pre-trained diffusion model along its sampling path while implicitly aligning generation with human preferences. Experiments demonstrate that CGG consistently improves preference alignment in text-to-image generation and flexibly adapts to safety-critical and multi-preference scenarios. Moreover, CGG can be combined with prevailing test-time alignment techniques to yield additional gains. These results establish CGG as a principled framework for advancing test-time alignment of diffusion models.", "tldr": "", "keywords": ["Diffusion models", "Text-to-image generation", "Test-time alignment", "Preference alignment"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b82f303443678e690fe2a4ca77288d7398918299.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Contrastive Gradient Guidance (CGG), a method that use fine-tuned policy models to guide the sampling process at test time. The authors conducted experiments on image quality, safety, and multiple preference setup, demonstrating the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is highly flexible and can be plugged in many different base models, finetuned models (DPO, KTO), and even training-free steering (FK-steering).\n2. The proposed method work for a variety of application scenarios such as general preference (PickScore), safety, and combining multiple rewards without training new models."}, "weaknesses": {"value": "1. The paper has poor presentation. In particular, many tables are hard to read.  For example, Table 7 does not have any midrules or booktabs. It is unclear what \"*\" means in Table 1, as it shows up on all CGG numbers. Table 4 and Table 3 has inconsistent header format despite showing results of similar structure. I strongly encourage the author to revise the presentation of results for the final version.\n2. The paper is not well-motivated. I understand the need to get rid of \"explicit reward models\" for test-time alignment methods, however this paper proposes instead we need to have a set of finetuned policy models (e.g. DPO,KTO), which seems to be more difficult to obtain. The author should clearly discuss on what kind of usecases is this assumption justified. For example, I can imagine if we have a newly released foundational model, methods that rely on reward models can still work, but CGG cannot work without having someone else releasing a fine-tuned model first. It is unclear why the setup of CGG is more preferable than works using explicit reward models."}, "questions": {"value": "In Table 1, KTO and CGG has exactly the same number 21.072, can the author confirm this? I understand when $\\gamma$ is set to 1, KTO and CGG should have exactly the same results. However the author used $\\gamma=3.5$, why are the number exactly the same?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cHtQ2q2Nx5", "forum": "g9aaKXgFhK", "replyto": "g9aaKXgFhK", "signatures": ["ICLR.cc/2026/Conference/Submission22280/Reviewer_cSmV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22280/Reviewer_cSmV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798928370, "cdate": 1761798928370, "tmdate": 1762942149096, "mdate": 1762942149096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Contrastive Gradient Guidance (CGG), a test-time framework for aligning diffusion model outputs with human preferences without relying on explicit reward models. CGG leverages the gradient of a log-likelihood ratio between favored and unfavored diffusion model distributions to produce an implicit reward guidance signal. This signal adjusts the model sampling path in real time to produce more preference-consistent outputs. The authors present a theoretical connection between CGG and reward-guided diffusion methods, as well as the established classifier-free guidance (CFG) mechanism. Experiments on Pick-a-Pic v2 and related datasets show that CGG improves preference alignment metrics such as PickScore and ImageReward, demonstrating flexibility across safety-critical and multi-preference scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a novel and conceptually elegant approach to test-time alignment of diffusion models. Its most significant contribution lies in eliminating the reliance on explicit reward models, which are costly to train and often difficult to adapt to new domains or user preferences. By framing alignment as a contrastive gradient problem, the authors provide a computationally efficient solution that maintains flexibility and broad applicability."}, "weaknesses": {"value": "1) While the proposed idea is promising, the empirical evidence supporting CGG’s effectiveness should be strengthened.\n\n2) The performance gains reported in Tables 1 and 2 are relatively modest, and in some cases, CGG does not achieve state-of-the-art scores. The authors should provide deeper justification or analysis to explain the observed improvements and limitations.\n\n3) Furthermore, the evaluation relies solely on automated preference metrics, which may not fully capture human judgment. A user study or human evaluation comparing CGG-generated results with baseline methods may provide better results and would greatly strength the paper’s claim.\n\n4) Finally, the empirical validation currently focuses only on Stable Diffusion 1.5 and SDXL; extending the experiments to additional models would improve the generality and credibility of the findings."}, "questions": {"value": "Please provide more experimental results and justification in the rebuttal period."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HIZZQX08lh", "forum": "g9aaKXgFhK", "replyto": "g9aaKXgFhK", "signatures": ["ICLR.cc/2026/Conference/Submission22280/Reviewer_i9dk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22280/Reviewer_i9dk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802673692, "cdate": 1761802673692, "tmdate": 1762942148878, "mdate": 1762942148878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Contrastive Gradient Guidance (CGG), a test-time alignment method for diffusion models that avoids explicit reward modeling.\nCGG derives a guidance signal from the contrastive difference between favored and unfavored distributions, expressed as the gradient of their log-likelihood ratio.\nBy steering a pretrained diffusion model using this signal, CGG enables preference-aligned generation without additional training.\nExperiments on Pick-a-Pic v2 demonstrate improved PickScore over Diffusion-DPO/KTO baselines, while the framework also adapts to safety-critical and multi-preference settings and can be combined with explicit reward-guided methods such as FK-Steering."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a simple yet practical inference-time preference alignment method that draws an analogy to classifier guidance (CG).\n2. It provides a practical approach that can reuse existing pretrained weights and preference-tuned weights.\n3. In addition to standard preference alignment, the method also improves performance across multiple tasks such as NSFW rate reduction and multi-preference alignment."}, "weaknesses": {"value": "1.The novelty of the proposed method is uncertain. The definitions of ρ (line 181) and κ (line 183) are unclear, and the paper does not provide any numerical or theoretical analysis on how closely the formulation in Equation (7) approximates an actual reward function.\n\n2.The guidance scale γ must be manually tuned, and as shown in Figure 1, the performance for SDXL even degrades as γ increases, making its effectiveness unclear.\n\n3.The evaluation metrics are somewhat limited. It would be important to also include results on Aesthetics, CLIP, and HPS v2 to provide a more comprehensive evaluation."}, "questions": {"value": "1. Could you provide a clearer explanation of the paragraph from lines 180 to 185? \n   In addition, if you have any numerical or theoretical analysis on how closely the proposed formulation approximates the reward function, please share it.\n\n2. Could you also share the evaluation results with Aesthetic, CLIP, and HPS v2, as well as the γ-sensitivity plot similar to Figure 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h5KwpybR7k", "forum": "g9aaKXgFhK", "replyto": "g9aaKXgFhK", "signatures": ["ICLR.cc/2026/Conference/Submission22280/Reviewer_Dq7k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22280/Reviewer_Dq7k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896135150, "cdate": 1761896135150, "tmdate": 1762942148649, "mdate": 1762942148649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Pre-trained diffusion models achieve impressive text-to-image generation quality, but aligning them with human preferences remains challenging due to the high cost of collecting preference data, training reward models, and fine-tuning. The authors proposed Contrastive Gradient Guidance (CGG) as a test-time alignment framework that eliminates the need for explicit reward models by deriving guidance signals from the contrastive difference between two diffusion models. By leveraging the gradient of the log-likelihood ratio between favored and unfavored distributions, CGG steers the diffusion process toward preference-aligned outputs during sampling. Experiments show that CGG improves preference alignment across diverse scenarios, including safety-critical and multi-preference settings, and can be combined with existing test-time alignment methods for further gains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, with clear motivation and logical flow throughout.\n2. It presents a simple approach for performing test-time optimization across multiple preferences."}, "weaknesses": {"value": "1. One major concern is the lack of clear justification for why the proposed method should outperform the PO-based diffusion models used during inference. Moreover, the experimental results reinforce this concern, as the observed improvements over the PO diffusion models appear marginal and not convincingly significant.\n\n2. The connection to using multiple diffusion models for handling multiple preferences is unclear. While the idea seems motivated by the prior study (Liu et al., 2022), the paper lacks a solid theoretical explanation or justification for this linkage.\n\n3. In practice, the method relies heavily on the use of PO-based diffusion models, introducing a strong dependency that limits its general applicability and practicality."}, "questions": {"value": "1. If the inference process effectively performs a weighted average between the PO diffusion models and the base diffusion model, it is unclear what underlying rationale supports the expectation that this method would outperform the PO diffusion models themselves. Could you clarify this?\n\n2. Could this method be compared with other relevant DPO-based approaches, such as Diffusion Model Alignment Using Direct Preference Optimization (Wallace et al., CVPR 2024) [1], to provide a more complete and fair experimental evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aCdU9QsuG7", "forum": "g9aaKXgFhK", "replyto": "g9aaKXgFhK", "signatures": ["ICLR.cc/2026/Conference/Submission22280/Reviewer_vy1u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22280/Reviewer_vy1u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962512031, "cdate": 1761962512031, "tmdate": 1762942148381, "mdate": 1762942148381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}