{"id": "vizM7B7vuW", "number": 16795, "cdate": 1758268739981, "mdate": 1759897218888, "content": {"title": "Image is All You Need: Towards Efficient and Effective Large Language Model-Based Recommender Systems", "abstract": "Large Language Models (LLMs) have recently emerged as a powerful backbone for recommender systems. Existing LLM-based recommender systems take two different approaches for representing items in natural language, i.e., Attribute-based Representation and Description-based Representation. In this work, we aim to address the trade-off between efficiency and effectiveness that these two approaches encounter, when representing items consumed by users. Based on our observation that there is a significant information overlap between images and descriptions associated with items, we propose a novel method, **I**mage is all you need for **LLM**-based **Rec**ommender system (I-LLMRec). Our main idea is to leverage images as an alternative to lengthy textual descriptions for representing items, aiming at reducing token usage while preserving the rich semantic information of item descriptions.\nThrough extensive experiments on real-world Amazon datasets, we demonstrate that I-LLMRec outperforms existing methods that leverage textual descriptions for representing items in both efficiency and effectiveness by leveraging images. Moreover, a further appeal of I-LLMRec is its ability to reduce sensitivity to noise in descriptions, leading to more robust recommendations.", "tldr": "By using images of items interacted with by users for LLMs, we aim to enhance the efficiency and effectiveness of LLM-based recommender systems.", "keywords": ["Recommender Systems", "Large Language Models", "Sequence Modeling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d631214d2b54160dda5c002ecbeef0a7483401a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies how to represent items for LLM-based sequential recommendation and argues there’s an inherent efficiency–effectiveness trade-off between attribute-only text and description-rich text. It proposes I-LLMRec, which replaces long descriptions with images, passing a single visual token per item into a frozen LLM via a learned adaptor (RISA) to align visual features to the language space, and a retrieval objective (RERI) that directly retrieve items from the item pool by leveraging the item images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The efficiency-effectiveness problem is well-motivated, with supporting analysis and the observation of image-description semantic overlap.\n- Extensive experiments on four Amazon dataset demonstrating the effectiveness of the proposed method, with ablation study proving the contribution of each module.\n- The method is less sensitive to noisy descriptions and handles 50% missing images by backing off to text features, still outperforming text-only baselines."}, "weaknesses": {"value": "- The real-world deployment trade-offs need to be reconsidered, and how about the latency of visual encoder cost, adaptor and LLMs.\n- The paper analyzes item coldness, but an explicit study of short user histories (cold-start users) is unexplored.\n- How about the performance of baselines if we add image information?"}, "questions": {"value": "- What is the average total token count per item instance in practice (title + delimiters + [VISUAL]) across datasets?\n- Could you explore how TRSR/TALLRec perform title-generation with beam search (their native setting) and compare to your retrieval at equal candidate sizes?\n- How about the scalability to large e-commerce datasets? The authors should discuss the complexity of this method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qsIdAgLRw6", "forum": "vizM7B7vuW", "replyto": "vizM7B7vuW", "signatures": ["ICLR.cc/2026/Conference/Submission16795/Reviewer_11jm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16795/Reviewer_11jm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841490415, "cdate": 1761841490415, "tmdate": 1762926831727, "mdate": 1762926831727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an LLM-based recommender framework that replaces lengthy product descriptions with product images as the primary carrier of semantic information to reduce token usage and inference overhead while preserving recommendation quality. The method aligns visual features to the language model’s embedding space through a lightweight adapter trained with a recommendation-oriented prompting scheme, and then replaces text generation with a retrieval-style ranking head that scores real candidate items."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Insightful reframing of the input modality under tight token budgets.** The paper clearly identifies the practical bottleneck of long textual inputs for LLM recommenders and argues—backed by empirical analyses of image–text redundancy—that images can serve as compact proxies for much of the descriptive semantics; this reframing is valuable because it opens a path to maintaining recommendation quality while substantially reducing context length, especially in settings where text is verbose, noisy, or unevenly curated.\n    \n2. **Well-scoped, efficiency-oriented architecture that avoids over-generation.** By aligning image features to the LLM and using a retrieval head rather than free-form generation, the approach ensures that outputs correspond to real catalog items, simplifies deployment, and naturally supports score-level fusion with collaborative-filtering and optional textual features; this design choice balances effectiveness with operational efficiency and makes the method easier to integrate into existing candidate-generation and re-ranking stacks.\n    \n3. **Comprehensive within-setting analyses that illuminate robustness and design trade-offs.** The experiments, while on public datasets, go beyond headline metrics to examine sensitivity to context window size, the impact of missing or degraded images, the benefits in cold-start regimes, and ablations that separate the roles of the alignment adapter and retrieval module; these studies help practitioners understand where the method is most reliable and how to combine it with other modalities without assuming that discarding text is universally optimal."}, "weaknesses": {"value": "1. **Limited evidence for scalability to industrial-scale deployments.** Experiments rely on small, public datasets and do not convincingly demonstrate behavior under production constraints such as large candidate pools, high user/item cardinalities, tight latency/service-level targets, and strict token budgets; to strengthen external validity, the paper should report system-level metrics (e.g., throughput, memory footprint, time-to-rank, cost per request) alongside $Hit@K$/$NDCG@K$ under increasing catalog sizes and candidate set growth, include stress tests with realistic traffic patterns and cold-start skew, and, if proprietary data are unavailable, simulate scale (e.g., catalog expansion, multi-tenant load) to approximate industrial conditions.\n    \n2. **Relying on images while discarding available text is counterintuitive and under-justified.** Although the paper argues that product images capture much of the semantic content of descriptions, in practice text often provides complementary signals (attributes, specs, usage constraints, safety notes) and is essential in low-visual domains; a more convincing case would compare early/late fusion and gating strategies, include confidence-aware fallbacks when image quality is poor or missing, analyze error cases where text helps, and broaden coverage to text-centric categories (e.g., books, apps) to verify whether image-only inputs remain competitive when visual cues are intrinsically weak.\n    \n3. **Comparisons lack strong compressed-text or lightweight language baselines.** It is premature to conclude that long descriptions should be replaced rather than compressed without benchmarking against competitive text pathways that control for compute and token budget; recommended baselines include (i) learned summarization to fixed length with budget-aware prompts, (ii) small/distilled LMs or sentence encoders with adapter/prefix tuning, (iii) classical text features (TF–IDF/BM25) with a learned re-ranker, and (iv) category/attribute tokenization with compact encoders; for fairness, the paper should equalize total trainable parameters, training wall-clock, and inference budget across modalities and report $NDCG@K$/$Hit@K$ under the same token caps to verify whether images truly dominate when text is efficiently compressed.\n    \n4. **Methodological fairness is unclear given image-side tuning and a frozen LLM.** The current setup tunes an image adapter while leaving the LLM unfine-tuned, which may disadvantage purely textual baselines and conflate modality choice with where adaptation capacity is allocated; to ensure apples-to-apples comparisons, the authors should either (a) freeze both pathways symmetrically or (b) allow comparable adaptation on the text side (e.g., LoRA, prefix-tuning, lightweight adapters) under a matched parameter/cost budget, and further ablate the contribution of the visual adapter versus the retrieval head to isolate where gains originate, ideally reporting robustness under domain shift and varying rates of missing or low-quality images."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RCPmiJP5ol", "forum": "vizM7B7vuW", "replyto": "vizM7B7vuW", "signatures": ["ICLR.cc/2026/Conference/Submission16795/Reviewer_1MNz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16795/Reviewer_1MNz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915258045, "cdate": 1761915258045, "tmdate": 1762926831175, "mdate": 1762926831175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes I-LLMRec, an LLM-based recommender that replaces lengthy item descriptions with item images to represent a user’s interaction history. It introduces (1) RISA (Recommendation-oriented Image–LLM Semantic Alignment), which learns an adaptor to map visual features into the LLM space using prompt-based supervision on next-item properties (brand/category/title/description), and (2) RERI (Retrieval-based Recommendation via Image features), which retrieves items from the corpus with user representations derived from an LLM token [REC], projected to a shared space and compared via dot products. Experiments across four Amazon categories (Sports, Grocery, Art, and Phone) demonstrate that image-based representation significantly outperforms attribute- and description-based LLM baselines, achieving about 2.93× faster inference while delivering roughly 22% performance gains over attribute-based representation. Additionally, this approach shows enhanced robustness under tight context budgets and is more resilient to noisy descriptions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Significant and Practical Problem:** The paper addresses a critical and practical problem in LLM-based recommender systems: the trade-off between efficiency (low token usage) and effectiveness (rich semantic representation). It clearly diagnoses why attribute-based methods are efficient but semantically poor, while description-based methods are rich but computationally expensive.\n\n2. **Excellent Efficiency and Scalability:** The core engineering contribution is demonstrating that using images as a prompt representation is highly efficient. Figure 3 provides strong evidence that the inference time of `I-LLMRec (I)` (red line) remains low and stable as the user sequence length (`|S_u|`) increases, whereas text-based methods like `TRSR (D)` (black line) and `I-LLMRec+D` (gray line) scale poorly due to their reliance on lengthy descriptions.\n\n3. **Robustness to Context Window Limits:** The analysis in Figure 4 is a key strength. It shows that `I-LLMRec`'s performance is stable even with a severely constrained context window (e.g., 256 tokens). In contrast, description-based models suffer a sharp performance decline as the context window shrinks, highlighting `I-LLMRec`'s practical value in token-limited environments.\n\n4. **Novel Alignment Methodology:** The proposed Recommendation-oriented Image–LLM Semantic Alignment (RISA) module is a clever contribution. Instead of relying on generic image–caption alignment, it trains the adaptor (M) to align visual features with recommendation-specific properties (e.g., brand, category, title, description). The ablation study (Table 2, row (a) vs. (b)) confirms that this task-specific alignment is crucial, leading to significant performance gains.\n\n5. **Clear Analysis of Text Robustness:** Figure 5 provides a clear empirical argument for the robustness of images over text. It demonstrates that text-based approaches are vulnerable to either information loss from summarization (Fig. 5a) or performance degradation from noise in full descriptions (Fig. 5b)."}, "weaknesses": {"value": "1. **Title and Premise are Overclaimed:** The paper's title, \"Image is All You Need\", and its central premise are fundamentally contradicted by its own best-performing model. The SOTA results reported in Table 1 for \"I-LLMRec (I)\" are, according to the ablation study (Table 2, row (e)), achieved by a model that fuses Image + CF + Text features. This model explicitly requires textual features ($r_{u,i}^{lext}$) in its RERI retrieval module to achieve top performance. The paper does not replace text; it moves text from the (inefficient) LLM prompt to the (efficient) retrieval head. This is a major overclaim.\n2. **Incomplete Efficiency Costing:** The efficiency gains reported in Figure 3 are potentially misleading. The paper does not clarify whether the inference time includes the (potentially significant) computational cost of extracting visual features using the SigLIP encoder. If these features are assumed to be pre-computed, the reported wall-clock times do not reflect the full cost of a real-time recommendation.\n3. **Weak Justification:** The entire motivation rests on the \"significant information overlap\" between images and text 。The primary evidence is a CLIP cosine similarity of ~0.31 (Figure 1c) 。This value is not self-evidently \"significant\" or \"surprisingly high.\" It is only marginally higher than COCO positive pairs (0.26)  and is very far from a strong correlation. This justification is weak and feels like an overstatement to fit the narrative.\n4. **Flawed Argument on Textual Information:** The paper claims in Section 4.2 that text-specific information \"has surprisingly little impact on recommendations\" 。The evidence cited is that I-LLMRec+D (adding text descriptions to the prompt) did not improve performance。This is a flawed argument. This experiment only proves that adding text to the prompt is redundant (which Figure 3 confirms is also inefficient)，not that text information itself is useless. In fact, the paper's own ablation (Table 2, row (d) vs. (e)) directly disproves this claim, showing that adding \"Text\" features to \"Image + CF\" features improves performance (e.g., Sport Hit@5 from 0.4491 to 0.4570; Art Hit@5 from 0.5795 to 0.5883) 。This confirms that text is impactful and necessary for the model's best performance.\n5. **Limited External Validity:** All datasets used are from the retail and fashion domains. These are visually-driven domains where this method is predisposed to succeed. The paper provides no evidence of generalization to visually-weak domains (e.g., books, news, services)."}, "questions": {"value": "1.  Your SOTA results in Table 1 (e.g., 0.4570 Hit@5 on Sport) appear to match row (e) of your ablation study in Table 2, which is the **\"Image + CF + Text**\" variant. How do you justify the title \"Image is All You Need\" when your best-performing model explicitly requires **text features** in its retrieval module? Given that your actual contribution seems to be moving text features from the (inefficient) LLM prompt to the (efficient) retrieval head, would a more accurate title be \"Images as an Efficient Prompt Representation for Multimodal Recommender Systems\"?\n\n2. In Section 4.2, you claim text-specific information has \"little impact,\" using the `I-LLMRec+D` experiment as evidence. However, your own ablation study (Table 2, row (d) vs (e)) shows that adding \"Text\" features to the retrieval module *improves* performance (e.g., Sport Hit@5 from 0.4491 to 0.4570) . How do you reconcile this direct contradiction?\n\n3.  Does the `I-LLMRec+D` experiment (text in prompt) not simply prove that text in the *prompt* is redundant *if* text is already being used in the *retrieval head*, rather than proving that text information *itself* is useless?\n\n4.  Do the inference times reported in Figure 3 include the wall-clock time for **visual feature extraction** (i.e., the SigLIP encoder pass)?\n\n5.  If the visual features were pre-computed, what is the actual latency of this feature extraction step per item, and how does this cost compare to the LLM processing time (which you optimized)? In a real-time system, wouldn't this encoder cost be a significant bottleneck?\n\n6.  You justify your entire premise on a CLIP cosine similarity of ~0.31, calling it \"significant overlap\". Given that a 0.31 correlation is generally considered weak, why do you believe this is strong enough evidence to claim that images can semantically *replace* detailed text descriptions?\n\n7.  All your experiments were conducted on retail and fashion datasets (Sports, Art, Clothing, etc.) where items are inherently visual. How do you expect I-LLMRec to perform in **visually-weak domains**, such as recommending books (where the cover art has low semantic value), news articles, or financial services?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SvO5myoZqK", "forum": "vizM7B7vuW", "replyto": "vizM7B7vuW", "signatures": ["ICLR.cc/2026/Conference/Submission16795/Reviewer_3dZW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16795/Reviewer_3dZW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915611947, "cdate": 1761915611947, "tmdate": 1762926830475, "mdate": 1762926830475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the trade-off between efficiency and effectiveness in LLM-based recommendation. Existing methods rely on either attribute-based or description-based item representations, which are limited by low semantics or high token cost. The authors observe strong information overlap between images and descriptions and propose I-LLMRec, which uses images as compact item representations. The model includes two modules: RISA for image–language alignment and RERI for retrieval-based recommendation. Experiments on multiple real-world datasets show that I-LLMRec improves both efficiency and accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe motivation is clear, and the trade-off is well analyzed.\n2.\tThe proposed method is overall reasonable and effective.\n3.\tExperiments are extensive and convincing.\n4.\tThe paper is well organized and easy to follow."}, "weaknesses": {"value": "1.\tThe paper lacks discussion or comparison with other generative textual item identifier such as IDGenRec [1] and semantic ID methods such as LC-Rec [2] or SETRec [3].\n2.\tThe experiments are limited to Amazon datasets. The generalization to other domains such as micro-video recommendation (e.g., MicroLens) is necessary to strengthen the paper.\n3.\tLack necessary ablation study on the image data, which is a key component of the proposed method. \n\n[1] Tan et al., IDGenRec: LLM-RecSys Alignment with Textual ID Learning. SIGIR’24  \n[2] Zheng et al., Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation. ICDE’24.  \n[3] Lin et al., Order-agnostic Identifier for Large Language Model-based Generative Recommendation. SIGIR’25."}, "questions": {"value": "1.\tHow does I-LLMRec perform on non-e-commerce datasets, such as MicroLens?\n2.\tCan the authors compare token efficiency with semantic ID approaches such as LC-Rec or SETRec?\n3.\tHow significant is the contribution of the image modality? The ablation study should include the model using CF and text only, without image features."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OoaLWWDPyR", "forum": "vizM7B7vuW", "replyto": "vizM7B7vuW", "signatures": ["ICLR.cc/2026/Conference/Submission16795/Reviewer_xUwd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16795/Reviewer_xUwd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046894035, "cdate": 1762046894035, "tmdate": 1762926829996, "mdate": 1762926829996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}