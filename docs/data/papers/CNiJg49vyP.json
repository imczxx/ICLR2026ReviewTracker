{"id": "CNiJg49vyP", "number": 7162, "cdate": 1758010069093, "mdate": 1759897869439, "content": {"title": "H+: An Efficient Similarity-Aware Aggregation for Byzantine Resilient Federated Learning", "abstract": "Federated Learning (FL) enables decentralized model training without sharing raw data. However, it remains vulnerable to Byzantine attacks, which can compromise the aggregation of locally updated parameters at the central server. \nSimilarity-aware aggregation has emerged as an effective strategy to mitigate such attacks by identifying and filtering out malicious clients based on similarity between client model parameters and those derived from clean data, i.e., data that is uncorrupted and trustworthy.\nHowever, existing methods adopt this strategy only in FL systems with clean data, making them inapplicable to settings where such data is unavailable.\nIn this paper, we propose H+, a novel similarity-aware aggregation approach that not only outperforms existing methods in scenarios with clean data, but also extends applicability to FL systems without any clean data.\nSpecifically, H+ randomly selects $r$-dimensional segments from the $p$-dimensional parameter vectors uploaded to the server and applies a similarity check function $H$ to compare each segment against a reference vector, preserving the most similar client vectors for aggregation. The reference vector is derived either from existing robust algorithms when clean data is unavailable or directly from clean data. Repeating this process $K$ times enables effective identification of honest clients. Moreover, H+ maintains low computational complexity, with an analytical time complexity of $\\mathcal{O}(KMr)$, where $M$ is the number of clients and $Kr \\ll p$.\nComprehensive experiments validate H+ as a state-of-the-art (SOTA) method, demonstrating substantial robustness improvements over existing approaches under varying Byzantine attack ratios and multiple types of traditional Byzantine attacks, across all evaluated scenarios and benchmark datasets.", "tldr": "", "keywords": ["Federated lerning", "Byzantine attack."], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d5bf6e041038a82bb01429a9ea1655cc6c128b78.pdf", "supplementary_material": "/attachment/68a6945a3a8c48dd968623a9f72c914bfe50edc6.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes H+, a similarity-aware aggregation method aimed at improving the robustness of federated learning (FL) against Byzantine attacks. The approach enhances existing robust aggregation algorithms by modeling client similarity both with and without access to clean reference data. When clean data is available, H+ can further identify and prioritize honest clients, improving resilience to adversarial behavior."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is easy to follow.\n2. Extensive experiments were conducted."}, "weaknesses": {"value": "1. The idea of using ‚Äúrepeated slicing‚Äù to mitigate the curse of dimensionality is not new and has already been proposed in DnC (NDSS 2021). The paper does not clearly explain how its approach differs conceptually or technically from prior work, which raises concerns about the novelty of this contribution.\n\n2. The paper introduces a new similarity check function H, but does not explain its underlying intuition or provide insight into why it performs better than standard similarity metrics such as cosine similarity or Euclidean distance. Without a clear theoretical or empirical justification, it is difficult to assess the significance of this design choice.\n\n3. All evaluated attacks are from 2020 or earlier, which limits the relevance of the experimental results. Considering the submission aims for ICLR 2026, the absence of more recent and adaptive attack baselines is surprising and weakens the claimed effectiveness of the proposed method.\n\n4. The paper provides no theoretical discussion or analysis regarding the effectiveness of the similarity function H or the potential impact of the proposed defense on model convergence. Such analysis would be critical to support the soundness and stability of the approach.\n\n5. It remains unclear whether the proposed method generalizes to modern model architectures such as Vision Transformers (ViT). Given the growing dominance of ViTs in vision-related FL tasks, this omission limits the scope and practical relevance of the work.\n\n6. As highlighted by Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on Production Federated Learning (IEEE S&P 2022), the proportion of malicious clients in real-world federated deployments is typically below 1%. Under such conditions, Byzantine attacks pose little practical threat. Thus, assuming more than 50% of clients are malicious is unrealistic and substantially reduces the real-world value of the proposed evaluation."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lKzqcTZstz", "forum": "CNiJg49vyP", "replyto": "CNiJg49vyP", "signatures": ["ICLR.cc/2026/Conference/Submission7162/Reviewer_frR2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7162/Reviewer_frR2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760805177789, "cdate": 1760805177789, "tmdate": 1762919326230, "mdate": 1762919326230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a new similarity-aware aggregation method to enhance the security of federated learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a novel method aimed at enhancing the security of federated learning systems.\n\n2. Experimental results validate the proposed method‚Äôs effectiveness."}, "weaknesses": {"value": "1. If the base method fails, the proposed approach also fails.\n\n2. The concept of enhancing the robustness of existing defense methods has already been explored in previous studies.\n\n3. The attacks evaluated in the paper are relatively weak and do not represent strong or adaptive adversaries.\n\n4. The paper does not clearly describe the details of clean data used in the experiments.\n\n5. The work lacks formal theoretical guarantees."}, "questions": {"value": "1. In cases where clean data is unavailable, which is common in real-world federated learning since obtaining clean data is often difficult, the proposed H+ method cannot extend the robustness boundary of the underlying aggregation rule. It only enhances performance when the base method (for example, Krum, GM, or Median) already provides some level of resilience. However, when the base method fails under strong attacks, H+ also fails. This greatly limits its effectiveness. For instance, as shown in [a], the Krum aggregation rule is inherently vulnerable to the Krum attack, meaning that Krum provides no robustness under such attack. In this situation, the H+ method will also fail.\n\n2. When clean data is unavailable, H+ can only improve performance if the base method already offers partial robustness. This idea has already been explored in paper [b], which also focuses on improving the robustness of existing aggregation methods such as GM and Median. The authors should clearly explain how their method differs from [b] and include a direct experimental comparison.\n\n3. The attacks considered in this paper are relatively weak. The authors should evaluate their method against state-of-the-art poisoning attacks in federated learning, such as those proposed in [a] and [c].\n\n4. When clean data is available, the paper does not specify its size or characteristics. It remains unclear whether the clean data can be arbitrary or must be sampled from the same distribution as the clients‚Äô training data.\n\n5. The paper only presents empirical results to demonstrate robustness and does not provide any theoretical guarantees or convergence analysis under adversarial conditions.\n\n\n\n[a] Local Model Poisoning Attacks to Byzantine-Robust Federated Learning. In USENIX Security Symposium 2020.\n\n[b] Do We Really Need to Design New Byzantine-robust Aggregation Rules. In NDSS 2025.\n\n[c] A Little Is Enough Circumventing Defenses For Distributed Learning. In NeurIPS 2019."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "97M3569oaI", "forum": "CNiJg49vyP", "replyto": "CNiJg49vyP", "signatures": ["ICLR.cc/2026/Conference/Submission7162/Reviewer_pF77"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7162/Reviewer_pF77"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875484489, "cdate": 1761875484489, "tmdate": 1762919325614, "mdate": 1762919325614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces H+, a defense against poisoning attacks in federated learning settings. The method is capable of working in two different scenarios: 1) when the aggregator has its own trusted dataset (or alternatively, there is a number of trusted clients with clean datasets); 2) there is no trusted dataset or a trusted set of clients. The defense relies on a similarity metric that is computed on random subsets of the parameters of the model, allowing to speed up the computation and to reduce the effect of the curse of dimensionality. The experimental evaluation includes different computer vision benchmarks, models, and defenses, both in scenarios with and without clean data. In these settings, H+ shows competitive results and outperforms other defenses in the related work."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The defense work in scenarios where the aggregator has a clean/trusted dataset available and in scenarios with no trusted data, which is uncommon for defenses against poisoning attacks in federated learning, who typically rely on one or the other assumption. \n+ The proposed method is also modular and allows to combine it with other aggregation methods, including robust aggregation with KRUM, MCA, median, etc. \n+ The authors also prioritized computational efficiency, reducing the number of parameters used for the computation of the similarity metric."}, "weaknesses": {"value": "+ The main limitation of the proposed defense is that it requires to provide the expected number of benign participants and, throughout most experiments the authors assume that this number is known in advance, which is unrealistic for practical scenarios, where the number of attackers is unknown and, also, can vary throughout the training of the algorithm. The ablation study only explores scenarios with ¬±10% deviations in the number of benign clients, which barely tests robustness to practical uncertainty. \n\n+ Following the previous point, the paper lacks evaluation in benign conditions, i.e., when there are no attackers. This is important as we cannot assess whether H+ introduces any degradation, noise, or training instability when all clients are honest and we only select a subset of N clients at each training iteration. In this sense, a robust aggregation method should aim to keep the baseline model‚Äôs accuracy in the absence of attacks. \n\n+ The similarity metric in equation (10) is not well justified and discussed. Why is this metric appropriate compared to other existing similarity metrics, like, for example, the cosine similarity. Is there any advantage in using a non-symmetric similarity metric like the one in (10), compared to symmetric similarity metrics? I think this point would make the paper sounder. \n\n + There is a relevant dependence with respect to the different hyperparameters of the method. In this sense, the method uses fixed hyperparameters with minimal justification, so that, generalization across datasets remain unclear, and the ablation studies do not provide a clear view of the sensitivity to these hyperparameters. \n\n+ In the experiments the authors just considered scenarios with 50 clients, limiting the scope of the analysis to, for example, scenarios with more clients, where scalability starts to be a more important aspect to consider."}, "questions": {"value": "+ Based on the comments in the previous sections, it would be interesting to observe how H+ performs when there is no attack and to have a deeper analysis of the effect of N (the number of benign clients) in scenarios where the defender knows little about the potential number of attackers. \n\n+ How do the authors justify the use of (10) as a metric to compute the similarities compared to other metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lEkItbv8mt", "forum": "CNiJg49vyP", "replyto": "CNiJg49vyP", "signatures": ["ICLR.cc/2026/Conference/Submission7162/Reviewer_8Kvh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7162/Reviewer_8Kvh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934109162, "cdate": 1761934109162, "tmdate": 1762919324956, "mdate": 1762919324956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces H+, a similarity-aware aggregation method designed for byzantine-resilient federated learning (FL). The method leverages a novel similarity check function, H, which measures the alignment of client-uploaded model parameter segments with a reference vector, intended to distinguish honest clients from byzantine attackers. H+ is adaptable: in systems with clean data, the reference is derived from the clean set; when such data is unavailable, outputs from robust aggregators are used as references. The paper claims improved computational efficiency ($\\mathcal{O}(KMr)$) compared to existing similarity-based approaches and empirically demonstrates H+ outperforms or enhances baselines across several attack types, Byzantine ratios, and datasets (Tiny-ImageNet, CIFAR-100, CIFAR-10), with results detailed in extensive tables and visualized in figures."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Methodological Generality: The H+ framework provides a unified approach to similarity-aware aggregation in federated learning, accommodating both scenarios‚Äîwith and without access to clean reference data. This flexibility extends its practical relevance beyond the limitations of earlier methods that operate only in clean-data settings.\n2. Computational Efficiency: By employing random sampling of low-dimensional vector segments (where ùëü ‚â™ ùëù) for similarity computation, H+ significantly reduces the computational cost compared to traditional cosine-similarity-based aggregation, making it well-suited for large-scale models.\n3. Empirical Robustness: The experimental results demonstrate that H+ consistently enhances test accuracy and resilience against a variety of Byzantine attack types including Gaussian, Sign-flip, LIE, and FoE across a broad range of attack ratios. It often achieves substantial performance gains over existing robust aggregators such as Median, Krum, GM, MCA, CClip, FLTrust, and Zeno++.\n4. Ablation and Sensitivity Analysis: The inclusion of comprehensive ablations and sensitivity studies effectively isolates the contributions of key components (e.g., the H function and the ùëÅ hyperparameter), reinforcing the credibility and robustness of the empirical findings.\n5. Clarity and Analytical Rigor: The methodology is clearly presented through both descriptive explanations and pseudocode, with accompanying analytical discussions that justify the time complexity and design choices, ensuring transparency and reproducibility."}, "weaknesses": {"value": "1. Some more recent works working on mitigating Byzantine attacks should be surveyed and cited.\nXu, J., Zhang, Z., Hu, R., Achieving Byzantine-Resilient Federated Learning via Layer-Adaptive Sparsified Model Aggregation (2024) ; etc\n2. Complexity in Evaluation Setup and Limited Dataset Diversity: The experimental evaluation primarily focuses on image classification tasks using well-established, moderately scaled datasets such as Tiny-ImageNet and CIFAR-100/10, with up to 50 clients. While the results are promising, the study relies on conventional model architectures (MobileNetV3, VGG16, ResNet18) and does not provide evidence of scalability to more complex domains such as NLP, time series, or federated tabular data."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "alYjsjRv4n", "forum": "CNiJg49vyP", "replyto": "CNiJg49vyP", "signatures": ["ICLR.cc/2026/Conference/Submission7162/Reviewer_aGBA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7162/Reviewer_aGBA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970101353, "cdate": 1761970101353, "tmdate": 1762919324278, "mdate": 1762919324278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}