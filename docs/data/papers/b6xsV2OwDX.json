{"id": "b6xsV2OwDX", "number": 6368, "cdate": 1757974302977, "mdate": 1759897919387, "content": {"title": "The Challenge of Reliable Vision–Language Model Responses in Driving", "abstract": "Reliable decision-making relies on both prediction and reasoning. In this work, we investigate whether Vision-Language Models (VLMs), when applied as driving assistants, can genuinely understand how present observations shape future outcomes, or whether their outputs merely reflect patterns memorized during training without grounded temporal reasoning. While recent efforts have integrated VLMs into autonomous driving, prior studies typically emphasize scene understanding and instruction generation, implicitly assuming that strong visual interpretation naturally enables future reasoning and thus ensures reliable decision-making—a claim we critically examine.\nWe identify two major challenges limiting VLM reliability in this setting: response inconsistency—where minor input perturbations yield different answers or, in some cases, responses degenerate toward near-random guessing—and limited temporal reasoning, in which models fail to reason and align sequential events from current observations, often resulting in incorrect or even contradictory responses. Moreover, we find that models with strong visual understanding do not necessarily perform best on tasks requiring temporal reasoning, indicating a tendency to over-rely on pretrained patterns rather than modeling temporal dynamics.\nTo address these issues, we adopt existing evaluation methods and introduce FutureVQA, a human-annotated benchmark dataset specifically designed to assess future scene reasoning. In addition, we propose a simple yet effective self-supervised tuning approach that improves both consistency and temporal reasoning without requiring temporal labels.", "tldr": "", "keywords": ["Autonomous Driving", "Temporal Reasoning", "Reliable Driving Assistant"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2bd377bfc193ffa526abff42a301aa7f218cfe59.pdf", "supplementary_material": "/attachment/0f5f6b2898d8ad79393ffe89fd05abb056583c62.pdf"}, "replies": [{"content": {"summary": {"value": "The paper probes whether VLMs used as driving assistants truly perform temporally grounded reasoning, finding two reliability failures—response inconsistency under semantics-preserving perturbations and weak temporal reasoning.\n\nIn addition, It introduces FutureVQA and an evaluation protocol (self-aligned future descriptions + multi-trial consistency) to test future-scene reasoning over 1–12-second horizons."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper formalizes reliable temporal reasoning with explicit alignment between past-only and future-conditioned predictions and gives concrete measures under semantics-preserving perturbations.\n* The paper propose a well-constructed dataset that targets future reasonin. Human-annotated FutureVQA focus on the time-specific prediction with diverse, naturally phrased questions and a multi-trial protocol."}, "weaknesses": {"value": "* The main concern is the scale and context limitations. The benchmark contains 2.7k human-annotated QA and each input provides only a 5-second history while evaluating up to 12 s, which may underrepresent longer-horizon dynamics and diverse real-world conditions. \n* Evaluation may be judge-biased. Future caption quality is partly scored by a single model-based judge (GPT-4o), and text similarity metrics (BLEU/ROUGE/CIDEr) are used—both may poorly capture safety-critical temporal reasoning and can introduce evaluator bias."}, "questions": {"value": "The primary concern is the limitation in scale and context. Providing additional clarification here would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dXCnY8PPJV", "forum": "b6xsV2OwDX", "replyto": "b6xsV2OwDX", "signatures": ["ICLR.cc/2026/Conference/Submission6368/Reviewer_HYS6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6368/Reviewer_HYS6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491222378, "cdate": 1761491222378, "tmdate": 1762918658764, "mdate": 1762918658764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates VLM reliability in driving, finding models struggle with response inconsistency and temporal reasoning. It introduces FutureVQA, a human-annotated benchmark for future prediction. It also proposes FutureAgent, a self-supervised method that trains the model to predict pseudo-descriptions of future frames, improving temporal consistency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper shifts focus from simple accuracy to the critical issues of reliability and temporal reasoning in driving. The analysis of response inconsistency using option shuffling is a simple and effective diagnostic. The introduction of FutureVQA provides a valuable, human-annotated resource for the field. The paper is clearly written, and the proposed FutureAgent method is an intuitive self-supervised approach that demonstrates improved performance. This work provides a useful framework for evaluating VLM foresight."}, "weaknesses": {"value": "A key limitation lies in the problem's formulation. The FutureAgent task trains the model to predict the single, recorded future from the dataset. This setup treats the future as a passive, deterministic event. However, for a reliable driving assistant, the future is conditional on the ego-vehicle's own actions (e.g., braking vs. accelerating). The current method trains for passive prediction of what did happen, not for the action-conditional foresight of what might happen given different choices. This overlooks the agent's own influence on the environment."}, "questions": {"value": "1. The FutureAgent task trains the model to passively predict a single, recorded future. Do the authors agree this is a limitation? How might the proposed method be extended to learn action-conditional future reasoning (e.g., \"What will happen if I brake now?\" vs. \"...if I continue at this speed?\")?\n2. Table 1 shows that FutureAgent reduces the accuracy drop (the \"S-M\" column) compared to its baseline. Could you elaborate on why you believe this specific self-supervised task improves this measure of consistency?\n3. The exponential decay weighting prioritizes short-term predictions. Have you experimented with other weighting functions, such as one that gives more weight to challenging long-term predictions?\n4. How do you interpret the performance of FutureAgent compared to models explicitly trained on video dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CCRj4Ff2l5", "forum": "b6xsV2OwDX", "replyto": "b6xsV2OwDX", "signatures": ["ICLR.cc/2026/Conference/Submission6368/Reviewer_Z1qg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6368/Reviewer_Z1qg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702201460, "cdate": 1761702201460, "tmdate": 1762918658254, "mdate": 1762918658254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the reliability of VLMs when applied as driving assistants, focusing on their ability to perform temporal reasoning and generate consistent responses. It identifies two critical limitations of current VLMs: response inconsistency and limited temporal reasoning. To address these issues, this paper introduce FutureVQA, a human-annotated benchmark dataset designed to evaluate future scene reasoning capabilities of VLMs, and propose a self-supervised tuning approach (FutureAgent) that enhances temporal consistency and reasoning without requiring explicit temporal labels. Experiments on multiple open-source and commercial VLMs demonstrate that the proposed method effectively improves response consistency and future scene prediction performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper analyzes and highlights key reliability issues (response inconsistency and poor temporal reasoning) of VLMs in safety-critical driving scenarios.\n\nFutureVQA provides a valuable human-annotated dataset tailored for evaluating future scene reasoning in driving, addressing the gap in existing benchmarks that lack focus on temporal dynamics."}, "weaknesses": {"value": "Inference speed and suitability for real-time driving applications are not discussed\n\nThe cite format is incorrect, it seems the authors used 'cite' rather than 'citep' required in the template.\n\nFutureVQA focuses on basic future scene questions; it does not fully cover complex driving scenarios (e.g., emergency situations, multi-agent interactions), raising concerns about the benchmark’s ecological validity.\n\nThis paper identifies response inconsistency but does not deeply analyze its underlying causes (e.g., model architecture, training data biases, or prompt sensitivity mechanisms), limiting targeted improvements.\n\nWhether there are other specialized temporal reasoning models, it is hard to assess the technical contribution and relative advantage of this paper in the driving domain."}, "questions": {"value": "Have you explored why VLMs exhibit response inconsistency (e.g., internal randomness, prompt phrasing sensitivity, or knowledge gaps)? How can these specific causes be mitigated beyond the proposed self-supervised tuning?\n\nHow does the inference speed of FutureAgent? Can this method be applied for real-time deployment in autonomous driving systems?\n\nCan FutureVQA include more complex driving scenarios (e.g., adverse weather, traffic accidents) and diverse question types (e.g., causal reasoning about collisions)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aeS6UU3XWM", "forum": "b6xsV2OwDX", "replyto": "b6xsV2OwDX", "signatures": ["ICLR.cc/2026/Conference/Submission6368/Reviewer_fBK2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6368/Reviewer_fBK2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732573237, "cdate": 1761732573237, "tmdate": 1762918657852, "mdate": 1762918657852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically evaluates the temporal reasoning and future scene prediction capabilities of VLMs in the context of autonomous driving. The authors introduce the FutureVQA benchmark, a challenging human-annotated dataset specifically designed for future scene understanding. They further propose a self-supervised fine-tuning approach that improves models’ temporal consistency and reasoning ability without requiring explicit temporal annotations. Experimental results demonstrate the limitations of existing VLMs and show that the proposed method provides significant gains in both accuracy and temporal alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and underexplored problem of reliable temporal reasoning for VLMs in safety-critical driving scenarios.\n2. The introduction of the FutureVQA benchmark fills a gap in the evaluation of future scene understanding, featuring diverse, human-annotated, and time-specific questions.\n3. The proposed self-supervised fine-tuning method is practical, annotation-efficient, and yields clear improvements without requiring additional temporal data labels."}, "weaknesses": {"value": "1. The current experiments are conducted on general-purpose VLMs and do not include domain-specific models pre-trained for autonomous driving. Since the proposed self-supervised fine-tuning method relies on the quality of pseudo-labels generated by the baseline model, it would be interesting to see whether using models with driving-specific knowledge would lead to different performance improvements.\n\n2. The evaluations mainly focus on quantitative metrics and lack more intuitive case studies. What are the concrete improvements before and after applying the self-supervised fine-tuning approach? It would be helpful to include representative qualitative examples to support the quantitative results, which could provide clearer evidence of the method’s effectiveness."}, "questions": {"value": "1. The current experiments are conducted on general-purpose VLMs and do not include domain-specific models pre-trained for autonomous driving. Since the proposed self-supervised fine-tuning method relies on the quality of pseudo-labels generated by the baseline model, it would be interesting to see whether using models with driving-specific knowledge would lead to different performance improvements.\n\n2. The evaluations mainly focus on quantitative metrics and lack more intuitive case studies. What are the concrete improvements before and after applying the self-supervised fine-tuning approach? It would be helpful to include representative qualitative examples to support the quantitative results, which could provide clearer evidence of the method’s effectiveness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vOztxKDvxn", "forum": "b6xsV2OwDX", "replyto": "b6xsV2OwDX", "signatures": ["ICLR.cc/2026/Conference/Submission6368/Reviewer_xA9B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6368/Reviewer_xA9B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987360506, "cdate": 1761987360506, "tmdate": 1762918657470, "mdate": 1762918657470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}