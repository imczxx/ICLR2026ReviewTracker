{"id": "zgNQGoqKS4", "number": 4212, "cdate": 1757638987835, "mdate": 1759898047117, "content": {"title": "Structuring Hidden Features via Clustering of Unit-Level Activation Patterns", "abstract": "We propose a self-supervised learning framework that organizes hidden feature representations across layers, thereby enhancing interpretability. The framework first discovers unit-level structures by comparing activation patterns across data samples. Building on these structures, we introduce a structure-aware regularization objective that (i) promotes feature reuse across layers via identity mappings and (ii) encourages the emergence of representative units that serve as anchors for related features. This regularization yields clearer and more structured feature pathways, enhancing the interpretability of the learned representations. Experiments demonstrate that our method induces structured feature pathways on synthetic data, improves interpretability on CIFAR-10 as measured by Grad-CAM++ metrics, and maintains competitive performance with slightly improved mean accuracy on both CIFAR-10 and ImageNet-1K.", "tldr": "", "keywords": ["feature learning", "representation learning", "interpretability"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0aa0c0986f7d66be58514f80e9bc9d848796ebb5.pdf", "supplementary_material": "/attachment/86eac2386e6fcc6cd9eaaea661b3aa50e8f2f6c1.zip"}, "replies": [{"content": {"summary": {"value": "Deep neural networks develop complex and unstructured internal representations, often creating redundant features that are difficult to interpret. This paper introduces a self-supervised regularization method to better organize hidden features, enabling their reuse across layers and increasing feature diversity within layers. This approach improves interpretability, makes better use of network resources, and may enhance generalization performance.\n\nThe method has two main components: First, it identifies redundant features through cross-layer clustering. Second, it implements a structure-aware regularization that encourages the reuse of one unit per cluster through residual connections while allowing other units to learn complementary features.\n\nThe authors tested their approach on three datasets: a synthetic task, CIFAR-10, and ImageNet, using variants of the ViT architecture. They developed new metrics to measure feature reuse, diversity, while utilizing previously proposed metrics for interpretability, and performance. Compared to standard training methods, their results showed better feature organization and interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents its ideas clearly and comprehensively, with excellent organization and complete details. \n- The approach is novel, introducing efficient methods to reduce computational costs without compromising effectiveness. The use of group ranked transformation for clustering helps reduce sensitivity to magnitude differences. The evaluation framework and analysis metrics are well-designed. \n- The concept of enabling precise unit-level feature reuse across layers while utilizing residual layers is particularly novel."}, "weaknesses": {"value": "- The paper's primary weakness lies in its limited experimental scope. While the presented results are promising, a broader evaluation across diverse datasets, model architectures, and network layers would better demonstrate the method's generalizability and practical impact. Enhanced visualizations of feature organization across multiple layers would also strengthen the paper's empirical validation.\n\n- The introduction of multiple hyper-parameters without detailed ablation studies makes it challenging to determine optimal settings for future applications."}, "questions": {"value": "- Is the structure loss calculated at the sample level?\n- When clustering flattened representations across token positions and layers, multiple units from different token positions can end up in the same cluster, but only one anchor unit is selected per cluster. Would selecting multiple anchor units per cluster for each unique token position improve results?\n- How does the method handle cases where the same unit position from different layers appears in different clusters but has the lowest index in each? This could create multiple loss computations for the same residual stream position. \n- How does this method perform with text inputs, where token counts vary and token positions can have significantly different representations? A discussion is required on the applicability across domains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "avVI8usG5O", "forum": "zgNQGoqKS4", "replyto": "zgNQGoqKS4", "signatures": ["ICLR.cc/2026/Conference/Submission4212/Reviewer_ZjDG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4212/Reviewer_ZjDG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878660207, "cdate": 1761878660207, "tmdate": 1762917232136, "mdate": 1762917232136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a self-supervised learning framework aimed at improving the interpretability of deep neural networks by structuring hidden feature representations. The method operates at the hidden-unit level, clustering activation patterns across data samples and imposing a structure-aware regularization that encourages cross-layer feature reuse and the emergence of representative anchor units."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Structured feature representation is an interesting topic. The paper does introduce a hidden-unit-level approach to organize features though it may remain complex.\n2. The combination of clustering hidden units and enforcing structure via a regularization objective is conceptually interesting and aligns with efforts to improve interpretability through learned representations."}, "weaknesses": {"value": "1. The evaluation relies heavily on Grad-CAM++ metrics. Gradient-based attribution methods are known to have limitations (especially in deep networks) and can produce misleading explanations. This raises concerns about whether the reported scores in interpretability are meaningful.\n\n2. The paper does not adequately position itself relative to prior explanation methods. Many traditional explanation methods are missing, such as TCAV (Kim et al.) and DINO. While these methods are not specifically relevant to \"structure\", they are helpful for understanding features.\n\n3. The paper does not convincingly demonstrate that structured representations are helpful for downstream tasks. If not, the advantage of structured features over existing feature characterization methods can be the key. But this part is missing in the current scope.\n\n4. I am curious about the impact of the structure-aware regularization on feature dynamics and learning efficiency."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rRFrLyeueO", "forum": "zgNQGoqKS4", "replyto": "zgNQGoqKS4", "signatures": ["ICLR.cc/2026/Conference/Submission4212/Reviewer_cEKB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4212/Reviewer_cEKB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900723695, "cdate": 1761900723695, "tmdate": 1762917231879, "mdate": 1762917231879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a novel way to align the feature across many different layers of a neural network. The author propose to collect a latent embedding buffer during training, which contains the embedding across different sample, position, and layers, then cluster these embedding to create \"feature anchor\" points. Then the author define an auxiliary loss to constraint the latent of the neural network to match these feature anchor accordingly. The author claims the this auxiliary loss makes the model more explainable in two way: 1. features across different layers are more aligned. 2. model trained auxiliary loss when applied with grad-cam produces better unsupervised segmentation map."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The author present a novel way to make neural network more interpretable. \nMethod presented by the author is not post-hoc, unlike many other interpretability works.\nThe author shows their method aligned with class-level segmentation map better, when applied grad-CAM."}, "weaknesses": {"value": "The author only provide the baseline against VIT trained with standard classification loss, but did not compare their method with other method that improves model's interpretability.\nThe model does \nThe author does not provide standard evaluation (classification accuracy) between standard VIT training and model trained with their auxilary loss. \nThere are many moving part of the design, ranking as preprocessing, and group rank, the author did not provide enough ablation study to show these design are necessary."}, "questions": {"value": "Can author read my summary to see if my understanding is correct? If not, please tell me and also explain to me how the model actually works. \nI would guess the training with auxiliary will improve model's interpretability but hurt the model's performance, if so, how much?\nThe ranking part is confusing to me. Why do the author use \"rank\" to preprocess the latent embedding, then something like normalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8f1XTlkAOn", "forum": "zgNQGoqKS4", "replyto": "zgNQGoqKS4", "signatures": ["ICLR.cc/2026/Conference/Submission4212/Reviewer_uggD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4212/Reviewer_uggD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762041471557, "cdate": 1762041471557, "tmdate": 1762917231578, "mdate": 1762917231578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}