{"id": "z8uo4lTxkq", "number": 11685, "cdate": 1758203067230, "mdate": 1759897561244, "content": {"title": "POET: Partially Observed Earth Transformer with High-Dimensional Position Embedding", "abstract": "The Earth system is integral to every aspect of human life, and accurately forecasting the system states is vital in many domains. Current sensing technology can only obtain partial observations of the Earth, such as meteorological factors collected by multiple weather stations or flood monitoring in different river locations. In this paper, we focus on forecasting physical quantities into the future based on partial observations of scattered stations, recorded as high-dimensional time series. While Transformers are well-suited for processing 1D natural language or 2D vision data, their attention mechanism may struggle to learn higher-dimensional dependencies in Earth data. To advance data-driven Earth modeling, we present Partially Observed Earth Transformer, short as POET, which captures the 3D dependencies underlying the Earth system observations alternately from the temporal, spatial, and variate views. To tackle the position-insensitivity of the attention mechanism, we apply attention with a novel High-dimensional Position Embedding (HiPE) strategy that meticulously encodes the geographical bias of each Earth observation. HiPE not only effectively integrates the off-the-shelf prior knowledge into attention but also automatically discovers the latent relation in the high-dimensional system. In a set of empirical studies, POET achieves consistent state-of-the-art forecasting skills in weather, flood and air quality, across both global and regional Earth systems.", "tldr": "", "keywords": ["deep learning", "machine learning", "earth system"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40f27099ea390bc82d13c8f56bd0bf90c0009e58.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "SUMMARY: The authors tackle the problem of encoding spatio-temporal Earth system data (think climate, atmospheric data etc.), focusing specifically on the task of dense reconstruction from sparse / partial observations. For example, this could be interpolating (and potentially extrapolating in time) a dense spatio-temporal map of the world from observations at a set of sensor stations. This is a well established methodological problem and relevant to a myriad of applications in the geosciences and beyond. The methodological contribution of the paper is twofold: (1) the authors propose a new positional encoding method HiPE to encode complex spatio-temporal interdependencies; (2) on top of that, they propose POET, a learning framework with an attention mechanism, for modeling spatio-temporal data. They test their proposed method on real-world Earth systems data including weather and air quality data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "STRENGTH:\n\n- The motivation for the paper and the problem it tackles is extremely relevant, has many concrete applications and direct pathways for impacting real-world modeling and decision making. It's always great to see more work on geospatial data at the major ML conferences!\n\n- Building better neural net architectures for spatio-temporal data is a very important problem and e.g. explicitly mentiond in [1]; this is especially true for neural nets that take geographic coordinates and/or timestamps as inputs. The proposed neural net architecture is explained well.\n\n- I enjoyed the ablation studies in the paper, especially the analysis of cross-correlations between different \"variates\" / input channels.\n\n- Overall, the paper is written quite well and somewhat easy to follow (except the details on the positional encoding)"}, "weaknesses": {"value": "SHORTCOMINGS:\n\nMajor:\n\n- My main problem with this study is the following: In their discussion of related work, the authors state that \"To date, the encoding of positional information in the context of complex Earth system modeling remains an underexplored\narea, largely due to the high dimensionality and unclear spatiotemporal dependency.\" This statement is not correct in its  gravity and the related work section lacks discussion of several existing positional embedding approaches for Earth data. For example, work on positional encodings for geographic coordinates (lon, lat) includes work on sinusoidal transforms ([2,3]) and spherical harmonics transforms ([4]). Work on jointly encoding spatio-temporal coordinates includes [5]. And work on positional encodings for different \"variates\" (i.e., channels or spectral bands) includes [6, 7]. The lack of engagement with any of this work and the subsequent lack of testing and comparing against these different approaches makes it difficult for me to assess how useful the HiPE contribution actually is. \n\n- Following from this, I am not fully sure how the spatio-temporal coordinates are actually encoded. If I understand Eq 3 correctly, they are just left raw? That is, raw lon/lat coordinates and a raw timestamp, with 2 and 1 learnable parameters respectively? If that is indeed the case, the learning of spatio-temporal dynamics might be severely limited. Or am I missing something here? I would appreciate a clarification.\n\nMinor:\n\n- The results tables are partially quite hard to read, especially Tab 2 would benefit from reformatting."}, "questions": {"value": "Overall, this paper tackles an important problem and is presented well. What holds this back is the lack of discussion and comparison to previous work on space-time coordinate encoding. I would appreciate some clarifications from the authors especially on the points raised in \"Weaknesses\" above.\n\nReferences for papers mentioned in the above boxes:\n\n[1] Rolf, Esther, et al. \"Mission Critical--Satellite Data is a Distinct Modality in Machine Learning.\" arXiv preprint arXiv:2402.01444 (2024).\n\n[2] Mai, Gengchen, et al. \"Sphere2Vec: multi-scale representation learning over a spherical surface for geospatial predictions.\" arXiv preprint arXiv:2201.10489 (2022).\n\n[3] Mai, Gengchen, et al. \"Multi-scale representation learning for spatial feature distributions using grid cells.\" arXiv preprint arXiv:2003.00824 (2020).\n\n[4] Rußwurm, Marc, et al. \"Geographic location encoding with spherical harmonics and sinusoidal representation networks.\" arXiv preprint arXiv:2310.06743 (2023).\n\n[5] Chen, Weibin, et al. \"Deep random features for scalable interpolation of spatiotemporal data.\" The Thirteenth International Conference on Learning Representations. 2024.\n\n[6] Cong, Yezhen, et al. \"Satmae: Pre-training transformers for temporal and multi-spectral satellite imagery.\" Advances in Neural Information Processing Systems 35 (2022): 197-211.\n\n[7] Ahmad, Muhammad, et al. \"Spatial spectral transformer with conditional position encoding for hyperspectral image classification.\" IEEE Geoscience and Remote Sensing Letters (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NvnleJ1bbY", "forum": "z8uo4lTxkq", "replyto": "z8uo4lTxkq", "signatures": ["ICLR.cc/2026/Conference/Submission11685/Reviewer_AXCp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11685/Reviewer_AXCp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761241189299, "cdate": 1761241189299, "tmdate": 1762922735331, "mdate": 1762922735331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a general mulit-variate spatio-temporal machine learning model called POET to forecast Earth Systems.\nThe model proposes to express earth models as a three dimensional setup, where one dimension is the spatial position (lat, lon), the second one time, and the third one the variate of interest.\nThe model is comprised of three sequential transformer blocks, with each block receiving the positional encoding of one of the three dimension, effectively attending to each dimension separately.\nThese layers are called \"Temporal-Attention\", \"Spatial-Attention\" and \"Variate-Attention\".\nAt each layer, they additionally use a extended version of RoPE that allow for a better Rotary position embedding, which translates the absolute positional embeddings into a relative one, and allows the model to learn from the relation between grid positions.\nTested on three earth system tasks, the model outperforms a series of of models and shows versatility and adaptability."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The article is well written and the figures of high quality, it is easy to follow. It is easy to see how the proposed model could fit a wide range of earth system applications. The model can generalize well to a wide range of applications and is showcased on earth system tasks that are quite different from each other.\nIntegrating the positional encoding in a layers is smart, and I'm surprised this has not been done before. Intuitively, it makes sense to apply RoPE in this context. In addition, utilizing RoPE instead of a relative positional encoding seems quite promising."}, "weaknesses": {"value": "Although the paper is interesting, there are, in my opinion, a few key weaknesses:\n- It is not clear how the data was split for the downstream tasks. The model obviously has to be trained from scratch for each downstream task, so there should be a train, validation and test dataset. Given that the model is a forecasting task, there should be a holdout set that has never been seen by the model. From the result, it is not clear if that is the case, and we can't make sure there is no data leakage.\n- The model is clearly a fixed location forecast with spatio-temporal dynamics. But the authors only compare to state of the art models of time series forecasting, not spatio-temporal forecasts. I do think this is a weakness as spatio-temporal models for fixed location forecasts have consistently shown better performance than time series forecasts for applications like weather forecasting (c.f. Yang et al. 2025 (https://arxiv.org/abs/2410.12938), Allen et al. 2025 (https://arxiv.org/abs/2404.00411)). Integrating spatio-temporal models as baseline comparison would be welcome.\n- It is not clear what the authors mean by \"Knowledge driven position\". The authors mention \"leverages prior information about the observation. This prior knowledge is derived from domain expertise, structured metadata, or predefined rules, which provide static and interpretable position information\", but there is no mention in the remaining paper of the use metadata, rules etc. This is quite confusing to me.\n- Although I do think that the proposed positional encoding is very interesting, it isn't shown in the paper that this is actually performing better than the alternatives. Does the model perform worse if all positional encodings were introduce at the same time? Is RoPE performing better than a learned positional encoding or a relative positional encoding?\n\n\nMinor weakness:\n- Although the figures are of very high quality, they are a bit misleading. The use of the globe in figures 1 and 2 make it seem like the model is either 1) global or 2) allows inference at arbitrary locations. But after reading the paper it is clear that the model is a fixed location forecasting model. I am not sure how, but I would try to make this more clear.\n- Why only show MAE for certain results but both RMSE and MAE for others?\n- In table 2, Germany's MAE is better for TimeXer, iTransformer, PatchTST. Wrongly bolded for POET.\n- Please keep the formatting consistent between tables"}, "questions": {"value": "What is the prior knowledge?\nWhat is the data split for use cases?\nCan you compare to performance if positional encoding was done all at once?\nWhat about using a learned positional encoding, or a relative instead of absolute?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "abpoSBjmLd", "forum": "z8uo4lTxkq", "replyto": "z8uo4lTxkq", "signatures": ["ICLR.cc/2026/Conference/Submission11685/Reviewer_pGZZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11685/Reviewer_pGZZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761602379485, "cdate": 1761602379485, "tmdate": 1762922734677, "mdate": 1762922734677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors present POET, for Partially Observed Earth Transformer, an architecture for spatio-temporal Earth modelling when available data is not evenly distributed across the globe and consistently available across sensors. The architecture consists of an encoder with three self attention layers to capture  temporal, spatial, and variate dependencies. The authors introduce a new high dimensional positional encoding strategy that encodes the geographical bias of Earth positions. \nThe positional encoding consists of an absolute position (rules-based/expert derived) and a learnable position. The authors demonstrate their method on meteorological, air quality  and river discharge forecasting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clear and well-written. The paper tackles a challenge  that is common in many Earth modelling tasks, the fact that data is not available on a regular grid on the Earth and that there are spatial biases. \nThe experiments are sound and the authors compare their method thoroughly to other baselines on 3 different tasks. The model demonstrates improved performance on all 3 datasets, and the authors also conduct thorough ablation studies on the components of their model."}, "weaknesses": {"value": "I think the name \"Partially-observed earth transformer\" is a bit misleading. For example, it is very common for certain meteorological data to only be available at meteorological stations, which obviously are not evenly distributed across the globe. \nWith respect to encoding space and time, the authors do not mention any work in the location embedding space (cf. geopriors, location embeddings such as SatCLIP, deep random features...). \nI think the main thing is: what is the problem that POET tackles that work in location embeddings is not really considering? \n\"Partially observed\" might be more relevant if it referred to missing data points at certain times/locations (which seemed to be the case that was considered when looking at Fig.1. However when looking at the design of the model and the evaluation, it seems that all variate values are available at a given time and location."}, "questions": {"value": "Related to the point raised in the weaknesses, how do you deal with missing data of certain variables at certain time/lovations? Have the authors tried experiments where information is not available at all times across all sites of a considered dataset? If not, can you explain how your model could be used in that context of if not, state explicitly this limitation? \n\nHave you tried space time encoding, such as the ones used in torchspatial ?\nHave you tried swapping T,S,V order in the encoder?\n\nThere seems to be a discrepancy in POET AVG results between tables 2 and 3. Which ones are the results to consider? \nTable 3 results: do the authors have some idea why the use of HiPE (or partial components of HiPE) do not bring a big improvement compared to not using it on the Flood, Germany and Bavaria CausalRIvers predictions, when HiPE seems more useful for the Wind and Temp predictions of the GTWSF dataset?  \n\nAnalysis of spatial proximity: how are the spatial embeddings reprojected into lat lon on the map? I don’t really understand what the point of mapping them back on the map is? Is that more to compare the distance between the expert-derived embeddings vs distance between the hiPE embeddings, and in that case why not represent them in the embedding space?\n\nRelatedly, I have a comment about Fig. 5 If I understand correctly, the results that are compared are from the POET model with HiPE and without HiPE. Regarding this claim, “Therefore, the relative distance between the spatial positions learned by POET is significantly greater than their prior geographical position” I’m not sure it is very visible from the map that the distance is significantly greater between the red triangles than the blue dots. \n\nI may have missed this but I could not find how the knowledge-driven position is defined in the different experiments. \n\nOverall, I think the paper has potential, the experiments are sound and there was clearly a lot of work put into this. But so far it is not quite clear how well motivated proposing this architecture is, and there are some missing comparisons with simple space time encoding schemes. There are also some missing details about the implementation (what is the knowledge-driven expertise). I am very open to revising my score, I am sorry if I missed something here, and I would appreciate if the authors would clarify these points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jUxuTFQ3Wn", "forum": "z8uo4lTxkq", "replyto": "z8uo4lTxkq", "signatures": ["ICLR.cc/2026/Conference/Submission11685/Reviewer_UAmW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11685/Reviewer_UAmW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605679497, "cdate": 1761605679497, "tmdate": 1762922734181, "mdate": 1762922734181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose POET, a Transformer-based model that alternately captures temporal, spatial, and cross-variable dependencies while introducing a novel High-dimensional Position Embedding (HiPE) to encode geographic biases and reveal latent relationships. Empirical results demonstrate that POET achieves consistent state-of-the-art performance across weather, flood, and air-quality forecasting tasks at both global and regional scales."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written with a well-structured presentation of the methodology and experimental findings.\n2. The proposed HiPE effectively integrating prior knowledge and data-driven positional information.\n3. The three-level attention mechanism allows POET to model intricate dependencies across different dimensions.\n4. Extensive experiments across diverse Earth system forecasting tasks provide strong evidence of the model’s effectiveness."}, "weaknesses": {"value": "* For the positional embedding design, the paper should compare POET’s HiPE with other widely used baselines, such as learnable node-specific embeddings [1] or fixed geographic encodings (e.g., DIRECT, CARTESIAN3D, and WRAP as discussed in the appendix of [2]). Such comparisons would strengthen the justification for HiPE and more clearly demonstrate its advantages over existing positional encoding methods in Earth system forecasting.\n[1] Taming Local Effects in Graph-based Spatiotemporal Forecasting\n[2] Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Networks\n\n* The attention mechanism across three dimensions is computationally expensive for large numbers of stations. Currently, POET is feasiably evaluated on datasets with 3,850 stations. For global weather forecasting, the station numbers are generally more than 10,000, how to accelerate model inference?\n\n* POET disentangles spatial and temporal modeling. Whether it is possible to model joint spatial-temporal attention, which is demonstrated effectiveness in other tasks (e.g., ViViT and Sora)?\n\n* From Table 3, the performance improvement for regional dataset (e.g., Flood, Germany) is relatively small."}, "questions": {"value": "* Additional positional embedding baselines.\n* Computational efficiency discussion.\n* Additional insights on model designs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gmIVXPg0dD", "forum": "z8uo4lTxkq", "replyto": "z8uo4lTxkq", "signatures": ["ICLR.cc/2026/Conference/Submission11685/Reviewer_2Zqh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11685/Reviewer_2Zqh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752756341, "cdate": 1761752756341, "tmdate": 1762922733854, "mdate": 1762922733854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}