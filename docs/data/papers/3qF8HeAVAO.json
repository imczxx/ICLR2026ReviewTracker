{"id": "3qF8HeAVAO", "number": 1808, "cdate": 1756938064776, "mdate": 1759898185304, "content": {"title": "CLAP: Unsupervised 3D Representation Learning for Fusion 3D Perception via Curvature Sampling and Prototype Learning", "abstract": "Unsupervised 3D representation learning reduces the burden of labeling multimodal 3D data for fusion perception tasks. Among different pre-training paradigms, differentiable-rendering-based methods have shown most promise. However, existing works separately conduct pre-training for each modalities due to computational costs of processing large point clouds with images. As such, mutual benefit of high-level semantics (from image) and 3D structure (from point cloud) has not been exploited. To address this gap, we propose a joint unsupervised differentiable-rendering-based pre-training method for images and point clouds, termed CLAP, short for Curvature sampLing and leArnable Prototype. Specifically, our method overcomes the computational hurdle by Curvature Sampling to select the more informative points/pixels for pre-training. To uncover the performance benefits brought by their complementarity, we propose to use learnable prototypes to represent parts of the 3D scenes in a common feature space and an Expectation-Maximization training scheme to associate embeddings of each modality to prototypes. We further propose a swapping prediction loss that explores their interplay through prototypes along with a Gram Matrix Regularization term to maintain training stability. Experiments on NuScenes and Waymo datasets show that CLAP achieves up to 100% more performance gain as compared to previous SOTA pre-training methods. Codes and models will be released.", "tldr": "Joint unsupervised 3D representation learning for fusion perception.", "keywords": ["Unsupervised 3D Representation Learning; Fusion Perception; Autonomous Driving"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4a6310b3acb997e13ab4a6608f362cdd264c3a46.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a joint unsupervised differentiable rendering pre-training method for images and point clouds, called CLAP, curvature sampling and learnable prototype. Specifically, CLAP overcomes computational barriers through curvature sampling, thus selecting more informative points/pixels for pre-training. To reveal the complementary performance advantages of both, it is proposed to use learnable prototypes to represent the various parts of the 3D scene in the common feature space and associate the embedding of each modality with the prototype using an expectation-maximizing training scheme. This paper also proposes an exchange prediction loss that explores their interactions through the prototype and uses a Gram matrix regularization term to maintain training stability. Experiments on the NuScenes and Waymo datasets show performance gains of up to 100% for CLAP compared to previous SOTA pre-training methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of this paper are as follows:\n1. The research direction of this paper has obvious value. The joint multi-modal unsupervised representation learning has a wide range of application scenarios for real-world applications in 3D vision.\n2. The research content of this paper is quite challenging, and there is a shortage of work that can effectively solve the limitations in this field.\n3. From the perspective of methods and results, this paper is highly complete and the methods are sufficiently original. For example, the curvature sampling, expectation maximization training, and swap prediction loss are proposed for the first time.\n4. The performance quality of this paper is excellent. Although there are not many illustrations and result figures in the text due to content limitations, it is easy for readers to understand and recognize."}, "weaknesses": {"value": "The weaknesses of this paper are as follows:\n1. Some elements in Figure 1 should be distinguished, including but not limited to: the sub-images corresponding to the point cloud and rendering loss are consistent in content, which seems to be unable to reflect the role of rendering loss. It is recommended to modify the sub-images corresponding to the rendering loss to have information with object supervision; (a) (b) The element repetition is high. It is recommended to reflect the transformation of the point cloud and rendering loss after using Swapping Prototype Assignment and Curvature Sampling.\n2. As the pipeline of the method, Figure 2 shows the depth integration and RGB integration, however, this is not explained in the subtitles or the text. In addition, Nuscenes and waymo do not have depth maps. How is the depth integration achieved in this paper?\n3. The rendering loss is mentioned many times in the paper, which should refer to the loss of rendering the point cloud as an image. However, the method introduces the projection of points to pixels through a given projection matrix. The rendering process seems redundant?\n4. In Formula 1, the author says that $\\hat{\\mathbf{P}} \\in \\mathbb{R}^{\\hat{D} \\times \\hat{H} \\times \\hat{W} \\times \\hat{d}_{\\mathrm{p}}}$ is the generated embedding feature, which is obviously not per-point features. For each superscript, the author should give a clear explanation, and it is recommended to explain it in text and in Figure 2.\n5. Formula 4 needs more introduction. How are $L_{rend}$ and $L_{proto}$ implemented, and what type of loss is used?\n6. The example of curvature sampling in Figure 3 is unconvincing to me. Because there is no curvature in a two-dimensional image, it is recommended that the author redraw the drawing as described in the text. Operate on LiDAR point cloud, reflect the signed distance function, surface normal, etc.\n7. The experiment section of this paper shows that a pre-trained model is used for fine-tuning, but it is not accurate to call this an unsupervised method.\n8. Figure 4 obviously needs to be improved. It is recommended that the author add sample sub-images in a row to fill the blanks. In addition, why are there fewer points with high sampling weights and concentrated at the bottom of the car object? According to theoretical analysis, shouldn't the area with large curvature transformation be concentrated above or around the car? The author must conduct sufficient experimental analysis, as I guess this may be a defect caused by the implicit signal function used.\n9. The method of this paper is abbreviated as CLAP (**C**urvature samp**L**ing and le**A**rnable **P**rototype). Is it to be as similar as possible to the classic work CLIP? I think it is easier to understand if CSLP appears in the title. Of course, it depends entirely on the author's ideas.\n10. The formulas and mathematical symbols in this paper seem to have flaws. It is recommended to give a complete explanation or separate them by pronunciation, for example, $f^{enc}$ can be changed to $f^{encoder}$ or $f^{en}$. Especially in formulas 13 and 14, there is only one dot above the mathematical symbols. It is recommended to change to a more obvious expression, such as a line or superscript."}, "questions": {"value": "As a reviewer, I reviewed this work on an almost per-word-by-word basis. Because of my obvious interest in it, however, given the current presentation quality and experimental results (see **Weaknesses**) of this paper leave much to be desired, I can only give it a neutral rating.\n\nI very much hope that the authors can provide enough evidence to convince me in the limited time available. If the evidence is sufficient, I will not begrudge my score and may highlight it to AC, conversely, I may feel that it needs enough revisions to reject it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No Ethics Concerns"}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Y0Zixf9WN4", "forum": "3qF8HeAVAO", "replyto": "3qF8HeAVAO", "signatures": ["ICLR.cc/2026/Conference/Submission1808/Reviewer_tkBv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1808/Reviewer_tkBv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760665623918, "cdate": 1760665623918, "tmdate": 1762915896258, "mdate": 1762915896258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CLAP, an unsupervised 3D representation learning framework that enables joint pre-training of LiDAR and camera encoders via differentiable rendering. It builds upon the UniPAD framework but introduces two key innovations: Curvature Sampling – a memory-efficient sampling strategy that prioritizes points with higher geometric curvature to retain informative structures during rendering. Prototype Learning – a cross-modal feature alignment mechanism using learnable prototypes optimized via Expectation-Maximization (EM), swapping prediction loss, and Gram Matrix Regularization to bridge image and point cloud modalities. Experiments on NuScenes and Waymo demonstrate good improvements over previous SOTA methods, particularly under few-shot fine-tuning settings. Ablation studies further show the effectiveness of each proposed component."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation\n\n- Curvature Sampling is intuitive; it prioritizes complex regions (e.g., vehicles, edges) while maintaining low computational overhead in theory.\n\n- Prototype Learning sounds crucial to align and interact between LiDAR and image modalities, improving cross-modal understanding.\n\n- Experiments include both NuScenes (5%) and Waymo (1%), with detailed scaling analyses (0.5–5%) showing gains.\n\n- Ablation results show contributions of each component."}, "weaknesses": {"value": "- The authors still focus mainly on few-shot downstream training (NuScenes 5%, Waymo 1%), which, while useful for showing sample efficiency, is non-standard in representation learning. Full-data fine-tuning results would better demonstrate scalability and practical performance benefits.\n\n- Although Curvature Sampling is motivated as memory-efficient, the paper does not explicitly quantify memory or runtime savings compared to UniPAD’s “Memory-friendly Ray Sampling.”\n\n- The prototype mechanism is interesting, but it’s unclear how well these prototypes correspond to meaningful physical parts or semantic regions. From figure 5, only the prototype assigned to the ground and the car can be figured out correctly.\n\n- The ablation table confirms effectiveness but doesn’t directly compare against UniPAD’s sampling strategies"}, "questions": {"value": "Please check the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hQCDtBs2Ym", "forum": "3qF8HeAVAO", "replyto": "3qF8HeAVAO", "signatures": ["ICLR.cc/2026/Conference/Submission1808/Reviewer_brLm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1808/Reviewer_brLm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760822872661, "cdate": 1760822872661, "tmdate": 1762915896068, "mdate": 1762915896068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CLAP is an unsupervised pre-training method for fusion 3D perception in autonomous driving, with the core idea of achieving joint unsupervised pre-training of images and LiDAR point clouds through **curvature sampling** and **learnable prototype learning**. \n\nFirstly, CLAP uses a curvature sampling strategy to select informative points and pixels, solving the computational cost problem of multi-modal data processing. Secondly, it randomly initializes learnable prototypes and combines the Expectation-Maximization (EM) algorithm to construct a cross-modal common feature space, associating image semantics with point cloud geometric information. Meanwhile, it designs a swapping prediction loss to explore inter-modal interaction and introduces Gram matrix regularization to avoid prototype collapse. \n\nExperiments on NuScenes and Waymo datasets show that CLAP performs excellently in downstream tasks such as 3D object detection (e.g., BEVFusion, CenterPoint) and semantic segmentation (e.g., Cylinder3D), with significant performance improvements compared to random initialization and existing SOTA methods (e.g., UniPAD). It also has outstanding scalability in few-shot fine-tuning scenarios and can effectively improve model performance during cross-dataset transfer."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.Strong scalability of the method: In few-shot fine-tuning scenarios, CLAP’s performance improvement increases as the amount of training data decreases, demonstrating strong scalability potential.\n\n2.Practical engineering significance: CLAP reduces the computational cost of joint unsupervised pre-training for image and point cloud modalities, facilitating information interaction between different modalities.\n\n3.Experimental validation of effectiveness: On benchmarks (e.g., NuScenes, Waymo), CLAP outperforms existing baseline methods. Cross-task (3D object detection, semantic segmentation) transfer experiments further validate its effectiveness."}, "weaknesses": {"value": "1.Insufficient analysis of results:Further analysis is needed to determine whether CLAP’s advantages lie in handling complex scenarios or achieving broad accuracy gains. Notably, its inferior performance on specific classes (vs. UniPAD/PPKT in Table 1) investigating the underlying causes.\n\n2.Memory efficiency requires further proof:UniPAD separately pre-train the image and point cloud encoders due to GPU memory constraints. Although curvature sampling is introduced to mitigate this issue, the absence of rigorous empirical validation (e.g., memory reduction metrics or training efficiency analysis) limits its persuasiveness. Future work should include quantitative evaluations to demonstrate its benefits."}, "questions": {"value": "Given that learnable prototypes are employed to represent 3D scene segments, with similarity estimated via the Gram matrix $G$ to prevent collapse, have you conducted ablation studies on $N_{K}$? Does there exist an optimal $N_{K}$? beyond which the additional prototypes become noise sources rather than meaningful representations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8huwOmqlcV", "forum": "3qF8HeAVAO", "replyto": "3qF8HeAVAO", "signatures": ["ICLR.cc/2026/Conference/Submission1808/Reviewer_4dye"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1808/Reviewer_4dye"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761405293308, "cdate": 1761405293308, "tmdate": 1762915895815, "mdate": 1762915895815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CLAP, a framework for unsupervised fused joint pre-training of LiDAR and camera data through differentiable rendering. CLAP incorporates two key components: a Curvature Sampling strategy to select geometrically informative regions and a set of learnable prototypes to represent distinct parts of a 3D scene, which are optimized using an Expectation-Maximization (EM) procedure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The manuscript is well-written, and the motivation for the work is clearly established.\n\nThe authors design a curvature sampling strategy to identify informative points and pixels for sampling.\n\nLearnable prototypes are utilized to establish a common feature space, and an Expectation-Maximization approach is employed to optimize these prototypes, enabling them to represent distinct parts of the 3D scene."}, "weaknesses": {"value": "The coverage of related work and compared methods lacks comprehensiveness. Notably, several recent methods from the \"3DTrans\" GitHub repository, which reportedly achieve strong performance, are neither discussed nor included in the experimental comparisons.\n\nWhen compared to other outdoor self-supervised learning (SSL) methods that also employ differentiable rendering, the primary novelty of the proposed CLAP framework appears to be the Curvature Sampling strategy.\n\nThe use of prototype learning is a well-established technique in the SSL community. Consequently, positioning this as a major contribution is not fully persuasive, as its incremental advancement over existing approaches is not sufficiently demonstrated."}, "questions": {"value": "I recommend expanding the comparative analysis to include several recent methods from the '3DTrans' benchmark. A fair and thorough comparison is essential to validate the claim that CLAP represents a meaningful advancement over existing alternatives. The current evaluation feels selective and does not fully establish the method's state-of-the-art status.\n\nI would like the authors to report the mean and variance of five replicates in all experiments, as reproducibility has always been a problem in this field.\n\nI'd like to know how the prototype learning proposed in this paper differs from other existing prototype learning methods. If it's merely being borrowed from this field, then its value wouldn't be that great.\n\nHow much time does online differentiable rendering consume? As far as I know, it slows down the pre-training process. Please explain this in detail."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oKlaApywtC", "forum": "3qF8HeAVAO", "replyto": "3qF8HeAVAO", "signatures": ["ICLR.cc/2026/Conference/Submission1808/Reviewer_JtoM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1808/Reviewer_JtoM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883787529, "cdate": 1761883787529, "tmdate": 1762915895674, "mdate": 1762915895674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}