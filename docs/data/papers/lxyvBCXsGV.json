{"id": "lxyvBCXsGV", "number": 10197, "cdate": 1758163591584, "mdate": 1763140563928, "content": {"title": "Concept-Based Steering of LLMs for Conditional Molecular Generation", "abstract": "Generating valid, unique, and high-fidelity molecules while precisely controlling for multiple properties simultaneously remains challenging. While prior works with LLMs have achieved success by fine-tuning language models on novel molecular corpora, they remain limited in scope. Real-world applications require generating molecules from unseen property distributions, a task that remains challenging for fine-tuned models. To this end, we present Concept-based Activation STeering (CAST), the first approach to apply activation steering to directly edit a model's internal representation for conditional molecular generation. CAST offers a lightweight, flexible alternative to fine-tuning by computing property-conditioned steering vectors via a concept network that does not require retraining the LLM. Through extensive experiments on datasets such as Therapeutics Data Commons, we show that CAST consistently outperforms existing methods on both in-distribution and out-of-distribution conditional generation tasks. We also conduct comprehensive ablation studies to highlight the extent of control our concept-guided steering provides on the molecules generated by the LLM.", "tldr": "We create a new method that used activation engineering which is an efficient, flexible and scalable approach for improved conditional molecular generation.", "keywords": ["activation engineering", "large language models", "concept bottleneck", "conditional molecular generation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e122de90a59973d2a57e7ddaa8eb9e6f59eecef7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce concept-based activation steering (CAST) for conditional molecular generation with LLMs. \nCAST enables precise control over multiple molecular properties without requiring model fine-tuning. \nExtensive experiments on in-distribution and out-of-distribution datasets showcase CAST's superior property alignment and generative efficiency compared to existing methods. \nResults demonstrate that CAST consistently outperforms base LLMs and even competitive fine-tuned models in most scenarios. \nThe method maintains molecular diversity and quality while achieving strong property alignment. \nQualitative assessments reveal that CAST generates molecules with higher novelty and synthesizability, improving the quality of generated molecules. \nThe method's robustness and efficiency are evident across various datasets and out-of-distribution scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper transfers the idea of activation steering from general NLP tasks to conditional molecular generation, employing a Concept Bottleneck Model (CBM) to learn interpretable property directions (QED and LogP) for dynamic activation modulation. The paper clearly points out the limitations of traditional methods that rely on handcrafted contrastive prompts and fixed steering strengths, whereas CAST introduces a CBM-based, automated, and continuous way to generate property directions, with a logically coherent and well-structured design.\n\n- The experiments cover multiple scenarios within the TDC datasets (ZINC, MOSES, and ChEMBL) and OOD settings, comparing CAST with Baseline, ActAdd, LoRA, and full SFT models, and reporting metrics such as MAE, Validity, and GenEff. CAST outperforms the baselines in most tables while using far fewer parameters than LoRA or full SFT, though in some specific settings SFT or LoRA achieve better results."}, "weaknesses": {"value": "- Although the paper introduces a CBM to automatically generate steering vectors, the overall framework essentially remains a standard activation addition paradigm. Compared with existing approaches such as Activation Addition, Contrastive Steering, and Representation Engineering, the contribution is mainly at the application level, lacking new theoretical insights or substantial algorithmic innovation.\n\n- While the CAST framework indeed extracts hidden states from intermediate layers and passes them through the CBM to generate concept vectors that are re-injected into the residual stream, the choice of layer position is an empirical design decision. The authors justify it only by citing prior work claiming that “intermediate layers retain rich semantic information,” without providing mechanistic explanations or conducting layer-wise ablation to validate this choice or its uniqueness.\n\n- The paper does not justify the selection of QED and LogP as target properties, nor explain their chemical or pharmacological relevance to generative controllability.”\n\n- The work lacks a discussion on the causes of OOD performance degradation, and the OOD task design itself is relatively simplistic."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lIIZwI0sDq", "forum": "lxyvBCXsGV", "replyto": "lxyvBCXsGV", "signatures": ["ICLR.cc/2026/Conference/Submission10197/Reviewer_guuH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10197/Reviewer_guuH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540273243, "cdate": 1761540273243, "tmdate": 1762921561770, "mdate": 1762921561770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CAST, a concept-based activation steering approach for molecular LLMs that aims to control continuous properties without fine-tuning the base LLM. The authors train a small concept bottleneck module on top of a frozen decoder-only LLM to map its hidden states to property-specific directions, which are later injected at an intermediate layer during decoding. On ZINC/MOSES/ChEMBL and OOD targets (incl. Conjugated-xTB), CAST reduces MAE versus frozen baselines, where the authors also argue that CAST also enables high validity, novelty, and synthesizability."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors present a method that introduces a simple auxiliary network to learn appropriate hidden features for steering, without any direct training or modification of the base LLM.\n\n- They conduct experiments across diverse application scenarios and, in addition to standard baselines, include both full fine-tuning and LoRA-based fine-tuning, enabling a thorough performance comparison."}, "weaknesses": {"value": "- The overall performance of CAST still lacks adequate justification: the CBM module’s outputs are trained solely with a separate MLP-based property regression loss, and the training pipeline does not include any end-to-end objective that evaluates the effect of injecting these features into the LLM’s hidden states during generation. Consequently, there is no principled guarantee that adding these features will improve property control or overall generation quality.\n\n- Similarly, the expectation that validity, novelty, and synthesizability would improve is insufficiently supported; indeed, the tendencies reported in Tables 5–6 do not consistently substantiate these claims and, in some cases, appear neutral or mixed with respect to such improvements."}, "questions": {"value": "- Considering that the target properties can be correlated (e.g., QED partially depends on LogP), how essential is the orthogonality loss? Could the authors quantify the performance sensitivity to including versus removing this term, and report whether any observed differences are significant?\n\n- Is there a justification(e.g., an ablation or comparative study) for the choice of the intermediate layer at which CBM features are injected? How does performance vary across candidate layers and depths, and is there a principled criterion guiding this selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VixZDg00cY", "forum": "lxyvBCXsGV", "replyto": "lxyvBCXsGV", "signatures": ["ICLR.cc/2026/Conference/Submission10197/Reviewer_8nFv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10197/Reviewer_8nFv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722551854, "cdate": 1761722551854, "tmdate": 1762921561125, "mdate": 1762921561125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Concept-based Activation STeering (CAST), a lightweight and flexible method for controlling large language models (LLMs) to generate molecules with specific properties, such as drug-likeliness (QED) and solubility (LogP). Unlike traditional methods that require costly retraining or fine-tuning of the LLM , CAST keeps the base LLM frozen. It works by training a small Concept Bottleneck Model (CBM) that learns to map the LLM's internal activations to property-specific steering vectors. During inference, these vectors are added back into the LLM's hidden states to \"steer\" the generation process toward the desired property values. Through extensive experiments, CAST is shown to consistently outperform baseline models and other activation steering methods, achieving strong performance on both in-distribution and out-of-distribution (OOD) generation tasks and demonstrating superior generative efficiency compared to even fine-tuned models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Efficiency and Flexibility**: The primary strength of CAST is that it does not require retraining or fine-tuning the base LLM, whose parameters remain frozen. This makes it a highly efficient and flexible alternative to SFT and LoRA, which require costly retraining for new property distributions.\n\n\n2. **Novel Steering Mechanism**: The paper successfully improves upon traditional activation steering. By using a CBM to automatically compute property-conditioned steering vectors , CAST avoids the major limitations of prior work, such as the reliance on manually crafted contrastive prompts and fixed steering magnitudes.\n\n\n\n3. **Strong OOD Performance**: The method demonstrates remarkable robustness on out-of-distribution (OOD) tasks. In settings where target properties are outside the training distribution, CAST consistently outperforms baseline models and remains highly competitive with fully fine-tuned models."}, "weaknesses": {"value": "1. **Limited Property Control**: The method was only evaluated on two continuous molecular properties, QED and LogP. Though the authors note that QED and LogP are \"relatively straightforward\" benchmarks, the method's effectiveness on more complex or challenging properties, such as HOMO-LUMO gaps or excitation energies, remains untested.\n\n2. **Lack of Qualitative Examples**: While the paper presents extensive quantitative results that are convincing, they can also be confusing: how this simple steering technique intuitively influences such a complex molecular generation process? To better elucidate the underlying mechanism, could the authors show how a specific molecule's SMILES string or 2D graph varies as they continuously change the steering strength for a single property?"}, "questions": {"value": "1. As I am new to the concept of activation steering, I would appreciate a more detailed illustration of the inference-time procedure. Specifically, regarding the concept values $c_i$, are they predicted by the model during inference, with their strength then tuned by the users (i.e., scaled up or down) to find an optimal value? Or, alternatively, are the $c_i$ values entirely user-specified at inference, with the Concept Bottleneck Model (CBM) being completely dropped? My current understanding favors the first interpretation. If this is correct, does it imply that generating a specific molecule requires careful manual tuning of the steering strength to achieve an optimal result? This can be a weakness.\n\n\n2. See W2.\n\n3. If my understanding in Q1 is correct, I have a follow-up regarding the claim in Experiment 4.4 #1: \"Take, for example, QED... the max MAE usually is close to 0.5 or 0.6 across all settings, implying that through the steering strength variable, CAST provides enough flexibility...\" Could the authors please elaborate on the use of the term \"usually\" in this context? I am asking for clarification because, upon reviewing Table 7, some of the reported max MAE values appear to be substantially larger than 0.6. How is the claim reconciled with these larger values in Table 7?\n\n\nI will consider raising my score if my questions or misunderstandings of this paper are properly addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rO67SXvh6n", "forum": "lxyvBCXsGV", "replyto": "lxyvBCXsGV", "signatures": ["ICLR.cc/2026/Conference/Submission10197/Reviewer_U2g5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10197/Reviewer_U2g5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973360659, "cdate": 1761973360659, "tmdate": 1762921560342, "mdate": 1762921560342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}