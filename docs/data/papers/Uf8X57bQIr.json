{"id": "Uf8X57bQIr", "number": 1799, "cdate": 1756935049242, "mdate": 1759898185962, "content": {"title": "GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra", "abstract": "Monocular 3D reconstruction methods and vision-language models (VLMs) demonstrate impressive results on standard benchmarks, yet their true understanding of geometric properties remains unclear. We introduce GIQ, a comprehensive benchmark specifically designed to evaluate the geometric reasoning capabilities of vision and vision-language foundation models. GIQ comprises synthetic and real-world images and corresponding 3D meshes of diverse polyhedra—including Platonic, Archimedean, Johnson, and Catalan solids, as well as stellations and compound shapes—covering varying levels of complexity and symmetry. Through systematic experiments involving monocular 3D reconstruction, 3D symmetry detection, mental rotation tests, and zero-shot shape classification tasks, we reveal significant shortcomings in current models. State-of-the-art reconstruction algorithms trained on extensive 3D datasets struggle to reconstruct even basic geometric forms accurately. While foundation models effectively detect specific 3D symmetry elements via linear probing, they falter significantly in tasks requiring detailed geometric differentiation, such as mental rotation.\nMoreover, advanced vision-language assistants exhibit remarkably low accuracy on complex polyhedra, systematically misinterpreting basic properties like face geometry, convexity, and compound structures. GIQ will be publicly available\n, providing a structured platform to highlight and address critical gaps in geometric intelligence, facilitating future progress in robust, geometry-aware representation learning.", "tldr": "We introduce a new geometric benchmark (GIQ) that reveals state-of-the-art vision and vision-language models fundamentally misunderstand 3D geometry, failing at basic reconstruction, classification, and reasoning tasks.", "keywords": ["Geometric Reasoning", "Benchmarking", "Foundation Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d80a3983c5c4bd2a78adb827114cd3d159f61f2.pdf", "supplementary_material": "/attachment/9207fd32795a06c05ff4bd1d94a468f1599d74fb.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposed a new benchmark, the Geometric IQ Test (GIQ), with simulated and physical polyhedra to evaluate the geometric reasoning capabilities of vision and vision-language models on four tasks. In addition, this work identifies limitations of existing models through an empirical study on the novel benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The GIQ dataset consists of synthetic and real-world images with corresponding 3D meshes that cover 224 unique polyhedra (with various levels of complexity and shapes). It is designed to evaluate visual reasoning with respect to 3D object geometry properties. The dataset of polyhedra provides unambiguous ground truth for evaluation.\n- This work provides extensive empirical evidence on existing models, demonstrating the current limitations of both vision and vision-language models. The experiment covers four tasks: (1) monocular 3D reconstruction, 3D symmetry detection, mental rotation tests, and zero-shot polyhedron classification.\n- The new polyhedra-based benchmark enables the research communities to systematically evaluate vision model performance on fundamental and fine-grained geometric reasoning (e.g., symmetry, complexity, and geometric properties)."}, "weaknesses": {"value": "- The test split exclusively contains views from 26 unique polyhedral shapes not present in the training set. How are these 26 shapes selected?  Would it be fairer to conduct n-fold validation (with different test set on n runs) to avoid potential biased findings due to the dataset split?\n- It is not entirely clear to me how the new benchmark finding informs the model's performance for arbitrary 3D shapes for real-world applications. I understand that using a polyhedral shape provides highly controllable samples and reliable ground truth. It would be great if the author could provide a discussion on how the findings translate to real-world scenarios for arbitrary objects that do not satisfy the polyhedral shapes' properties. \n- The inclusion of real-world images taken with paper models is a good initiative. However, from the provided images, the surface is based on non-reflective material. Hence, it seems to be similar to synthetic data but with some variance in shadow conditions. In addition, wild images were preprocessed via centre cropping and background removal. In such a scenario, it is uncertain how \"difficult\" the wild image is compared to synthetic data. It would be useful to analyse the results with the synthetic and wild images separately. In a more challenging and ideal case, I wish the benchmark could have real-world images taken with polyhedral models built with reflective material."}, "questions": {"value": "- For the monocular 3D reconstruction task, each of the models was trained on millions of 3D assets. However, I believe this model is not aware of polyhedral shapes nor trained with polyhedral shapes. Hence, these models might be \"overthinking\" about the underlying 3D shape of the provided image, and not taking advantage of polyhedral space properties in 3D model generation. Instead of just describing the observation from the generated results, I would like the author to discuss the reasons why these models are performing badly, and what insights to inform future research. In addition, what is the data split for the monocular 3D reconstruction task?\n- Figure 2 and Table 1 (appendix) show a total of 238 samples from 8 groups. This number is more than the 224 unique polyhedra shapes stated in the paper. Please clarify. \n- It would be helpful to provide a background or reference for 4-fold rotational symmetry and 5-fold rotational symmetry. This will help the reader to understand the task better."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v5lmY87LIT", "forum": "Uf8X57bQIr", "replyto": "Uf8X57bQIr", "signatures": ["ICLR.cc/2026/Conference/Submission1799/Reviewer_MgEn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1799/Reviewer_MgEn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760681711140, "cdate": 1760681711140, "tmdate": 1762915893743, "mdate": 1762915893743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the GIQ (Geometric IQ Test) benchmark. This tool specifically evaluates the 3D geometric reasoning abilities of modern vision foundation models (VLMs). The GIQ dataset contains images and 3D meshes for 224 unique polyhedra. These shapes range systematically in complexity and symmetry. They cover simple Platonic solids up to complex structures like stellations. Both synthetic renderings and real-world photographs are included. The authors conducted four key experiments on state-of-the-art models. The results showed a significant gap in performance. Models struggle with explicit, robust geometric tasks despite high scores on common benchmarks. The paper concludes that GIQ is a crucial diagnostic tool. It is designed to guide the development of future, more geometry-aware foundation models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It provides the critical insight that a model's implicit ability to encode a feature (demonstrated by successful linear probing for symmetry) does not translate into explicit, robust geometric reasoning in other tasks. The results are quite interesting.\n2. For GIQ, it uses polyhedra with well-defined properties (symmetry groups, face types) to provide precise, unambiguous ground truth for fine-grained geometric evaluation, which is lacking in large, existing 3D datasets.The constructed dataset is diverse, including both controlled synthetic renderings (Mitsuba PBR) and \"wild images\" of physical paper models captured in diverse real-world conditions. I appreciate the contribution of this proposed benchmark and its comprehensive dataset."}, "weaknesses": {"value": "1. One concern is the potential VLM prompt ambiguity. The exact zero-shot prompt methodology for testing VLMs is not detailed in the provided text. The reported low accuracy could potentially be influenced by sub-optimal or ambiguous prompting rather than a pure geometric failure of the models.\n2. While polyhedra are rigorous, their highly regular and stylized nature may not fully capture the complexity and irregularity of general, arbitrary objects found in the real world. One potential concern is how to better represent the 3D objects in any format, such as liquid. This may remain as the future work."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DvU4vebwHE", "forum": "Uf8X57bQIr", "replyto": "Uf8X57bQIr", "signatures": ["ICLR.cc/2026/Conference/Submission1799/Reviewer_GJKD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1799/Reviewer_GJKD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888775968, "cdate": 1761888775968, "tmdate": 1762915893624, "mdate": 1762915893624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new benchmark for 3D geometric reasoning in modern vision and vision-language models. This benchmark includes a key dataset of 224 polyhedra spanning Platonic, Archimedean, Johnson, Catalan, Kepler-Poinsot, stellated, and compound solids. They are represented in both synthetic renderings (via Mitsuba) and real-world photographs (of physical paper models). Many tasks are included in a comprehensive evaluation, inc., monocular 3D reconstruction, 3D symmetry detection, mental rotation and zero-shot classification.\n\nThe most valuable and interesting part of this paper is the conclusion. Results reveal that while pretrained encoders sometimes implicitly capture symmetry, most models fail catastrophically on real 3D reasoning tasks, especially reconstruction, rotation, and geometric classification, even on simple solids."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "I believe this is an interesting and significantly meaningful paper.\nIt sets a geometrical IQ (G-IQ) test for modern vision models and vlms in 3D reconstruction. GIQ fills a clear gap: existing 3D datasets (Objaverse, OmniObject3D, GSO) test recognition and reconstruction, but not reasoning about geometry or symmetry. Polyhedra are a brilliant choice since they offer mathematically clean ground truth, structured complexity, and interpretability, and the dataset design (Mitsuba renders + paper models) ensures good control and realism, and they find that DINOv2 captures symmetry implicitly while failing in higher-level reasoning, which is interesting and insightful.\n\nThe experiments are comprehensive and rigorous. The paper presentation is clear and easy to follow."}, "weaknesses": {"value": "Since the paper’s premise involves geometric reasoning, a simple human baseline (even small-scale) on the same tasks would help contextualise the human-level intelligence in G-IQ test, which will help to understand how far the modern vision models are away from human-level performance.\n\nFor the zero-shot classification, it’s unclear how prompts and outputs were standardised. Did all models get the same prompt verbatim? Were answers normalised (e.g., “cube” vs. “hexahedron”)?"}, "questions": {"value": "Could GIQ be used for fine-tuning or instruction-tuning VLMs to improve their geometric intelligence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mBQILjJYZd", "forum": "Uf8X57bQIr", "replyto": "Uf8X57bQIr", "signatures": ["ICLR.cc/2026/Conference/Submission1799/Reviewer_pamt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1799/Reviewer_pamt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914402107, "cdate": 1761914402107, "tmdate": 1762915893417, "mdate": 1762915893417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a geometric IQ (GIQ) benchmark designed to evaluate the 3D geometric reasoning ability of vision and vision-language foundation models. The dataset includes synthetic and real-world images of 224 unique polyhedra, covering convex, nonconvex, stellated, and compound classes, each captured from multiple viewpoints.\nThe benchmark defines four diagnostic tasks – monocular 3D reconstruction, symmetry detection, mental rotation, and zero-shot shape classification – to probe different aspects of geometric understanding. A linear probing framework is used to assess how well pretrained visual representations encode geometric properties such as symmetry and shape equivalence.\nExperimental results show that while models like DINOv2, CLIP, and GPT-4o exhibit some pattern sensitivity, none demonstrate consistent 3D awareness or symmetry invariance, especially when generalizing from synthetic to real conditions. Overall, the benchmark highlights a significant gap between current foundation models and true geometry-aware visual reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper is well-motivated and addresses an important gap. Foundation models based on language and/or 2D images are unlikely to develop 3D understanding. The proposed benchmark demonstrates these limitations through controlled experiments.\n2.The combination of synthetic and real images provide a well-controlled domain-shift test for evaluating geometric invariance versus appearance sensitivity."}, "weaknesses": {"value": "1. The benchmark primarily evaluates 2D-pretrained encoders (e.g., CLIP, DINOv2, MAE, ConvNeXt) and 2D VLMs (e.g., GPT-4o, Claude, Gemini, Llava), which were never designed to represent 3D structure—so their poor performance is somewhat expected. The study highlights a limitation of 2D pretraining rather than establishing a hierarchy of geometry-aware capabilities.\n2.Including geometry-native or 3D-aware models -- such as multi-view pretrained networks (VGGT, DUSt3R, MASt3R, etc.) or 3D-VLMs (PointLLM, SceneLLM, LLaVA-3D) -- would strengthen the conclusions and demonstrate that the benchmark can distinguish true geometric understanding from 2D appearance bias.\n3. While the dataset includes both synthetic and real images, evaluations are limited to single-view inputs, which can be ambiguous for many polyhedra. Additionally, there is no exploration of prompted geometric reasoning (e.g., symmetry-aware or chain-of-thought prompts) for VLMs, which might reveal latent reasoning capacity.\n4. Linear probing only captures linearly separable features and may not fully reflect a model’s potential for nonlinear geometric reasoning or spatial representation. Complementary analyses (e.g., fine-tuning or nonlinear probes) would make the conclusions stronger."}, "questions": {"value": "1. How do 3D-aware VLMs perform on the benchmark?\n2. Does prompting vision-language models to explicitly reason about geometry (e.g., symmetry, rotation, convexity, or step-by-step reasoning) change their performance?\n3. How do the models perform with multiview images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FUgFcVgI6p", "forum": "Uf8X57bQIr", "replyto": "Uf8X57bQIr", "signatures": ["ICLR.cc/2026/Conference/Submission1799/Reviewer_15Vf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1799/Reviewer_15Vf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762122909493, "cdate": 1762122909493, "tmdate": 1762915893267, "mdate": 1762915893267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}