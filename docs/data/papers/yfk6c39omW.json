{"id": "yfk6c39omW", "number": 21024, "cdate": 1758312939514, "mdate": 1763699763035, "content": {"title": "Escaping Model Collapse via Synthetic Data Verification:  Near-term Improvements and Long-term Convergence", "abstract": "Synthetic data has been increasingly used to train frontier generative models. However, recent study raises key concerns that iteratively retraining a generative model on its self-generated synthetic data may keep deteriorating model performance, a phenomenon often coined model collapse. In this paper, we investigate ways to modify the synthetic retraining process to avoid model collapse, and even possibly help reverse the trend from collapse to improvement. Our key finding is that by injecting information through an external synthetic data verifier, whether a human or a better model, synthetic retraining will not cause model collapse. Specifically, we situate our theoretical analysis in the fundamental linear regression problem, showing that verifier-guided retraining yields early improvements when the verifier is accurate, and in the long run, the parameter estimate converges to the verifier’s knowledge center. Our theory predicts that the performance of synthetic retraining will have early gains but eventually plateaus or even reverses, unless the verifier is perfectly reliable. Indeed, our experiments on both linear regression as well as  Conditional Variational Autoencoder (CVAE) trained on MNIST data also confirm these theoretical insights.", "tldr": "", "keywords": ["Model Collapse", "Synthetic Data", "Verifier-guided retraining"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/174d493c4b6009178fbe03ac976ca7ac17c84e08.pdf", "supplementary_material": "/attachment/2ee40e6521750e2dac3b7f0136b81724f23819d8.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies verifier-guided synthetic retraining: a loop that (i) generates synthetic data from the current model, (ii) filters it with an external verifier via a binary accept/reject rule, and (iii) retrains only on the verified subset. In a linear-regression setting, the authors give a one-step MSE bound that separates synthetic variance reduction from verification bias/variance. They then analyze the multi-round process and prove it behaves as a noisy contraction that converges to the verifier’s knowledge center. Experiments in linear regression and a CVAE on MNIST show early, clear gains with verification; long-run plateau depended on verifier quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work formalizes a widely used practice, i.e., filtering synthetic data with a verifier and shows how it changes collapse dynamics.\n \n- A clear theoretical framework and detailed analysis followed by empirical evidence in simple setup."}, "weaknesses": {"value": "- Core analysis is linear regression with a spherical verifier and a special synthetic design; extensions to non-linear models are discussed but not derived. This limits direct transfer of the rates/conditions to modern LMs/vision models. \n\n- MNIST/CVAE is a clean demo but dated; no language or large-scale vision results. Also, while FID trends are solid, downstream task metrics or human evals would strengthen claims about generative quality. \n\n- In the MNIST setup, the useful verifiers are trained with much more training data than the initial 500 data points for the VAE. In the reality, for images, we will normally use all data possible to train both the generative model and the verifier, therefore, another more practical setup is to fix the training set for both the original VAE and the verifier, and iterate from that."}, "questions": {"value": "- How sensitive are the results to the shape of the verifier region? \n\n- A probabilistic perspective might also be interesting since these are generative models. The retraining process can be thought of as to move the original training data distribution towards the distribution defined by the verifier, which actually defines a generative model itself. When the retraining iteration goes to infinite, we are distilling the generative model defined by the verifier into a parametric generative model e.g., a VAE."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "442ygfa8iS", "forum": "yfk6c39omW", "replyto": "yfk6c39omW", "signatures": ["ICLR.cc/2026/Conference/Submission21024/Reviewer_ENpW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21024/Reviewer_ENpW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827055636, "cdate": 1761827055636, "tmdate": 1762940606368, "mdate": 1762940606368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the effect of verifier quality on model collapse from training on self-generated synthetic data.  The authors structure their analysis from linear regression with a verifier that has controllable bias.  It is discovered that a mildly biased verifier may provide initial improvements, but will ultimately lead to performance plateaus or collapse over many iterations.  The authors evaluate their work on linear regression as well as preliminarily on a CVAE implementation on MNIST."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is quite clearly presented, and the math is approachable to verify, being based on linear regression."}, "weaknesses": {"value": "The paper is quite toy from the perspective of model (linear regression), data, and verifier.  This may be fine for theoretical proofs, but it does limit the scope of takeaway when considering the applicability to frontier generative models.  It would be interesting to see actual experiments on language modeling and large-scale pretrained models beyond such toy settings.  Particularly as there are a variety of reinforcement learning from verifiable rewards works on frontier models at the moment, it should not be difficult to apply it for the verifier-filtered experiments proposed here.\n\nEven MNIST is a bit toy compared to available datasets of natural images.\n\nThe takeaways also make intuitive sense, rendering the findings a bit unsurprising - one does expect that a mildly biased verifier should help initially but performance should not be able to break free from the biases of the verifier when the scale of seen data is predominantly that filtered by the verifier."}, "questions": {"value": "Why is verifier-based filtering in particular important to study thoroughly - even for frontier generative models?  Whereas naively retraining the generative model on self-generated synthetic data may indeed lead to model collapse, the field has seen success in avoiding this through verifiable rewards (Reinforcement Learning from Verifiable Rewards, as used in DeepSeek [1], Tulu3 [2]).  Why do filtered retraining from a verifier rather than use all data but labeled with a reward from the verifier (such as labelling unsuccessful generations with a 0)?  The assumption for the components is the same in both approaches, but RLVR seems to work better than verifier-based filtered training.\n\n[1] DeepSeek-AI et al., DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, 2025.\n\n[2] Lambert et al., Tulu 3: Pushing Frontiers in Open Language Model Post-Training, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sD6PEiyBny", "forum": "yfk6c39omW", "replyto": "yfk6c39omW", "signatures": ["ICLR.cc/2026/Conference/Submission21024/Reviewer_TG7U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21024/Reviewer_TG7U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868638295, "cdate": 1761868638295, "tmdate": 1762940606024, "mdate": 1762940606024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper mainly studies the role of verification in preventing model collapse using a linear regression framework. Their analysis finds that, in the short term, the proper use of verification can enhance performance by reducing variance. However, in the long term, it may cause the model parameters to converge toward those used to construct the verifier. The results of this paper are supported by rigorous theoretical analysis and experimental evidence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written and easy to follow.\n\nThe claims in the paper are supported by rigorous theoretical analysis and comprehensive experiments.\n\nThe results are quite intuitive and make a lot of sense."}, "weaknesses": {"value": "I think this paper presents a solid theoretical work. Below, I will list some drawbacks; however, they do not constitute reasons for rejection, as we all understand that in theoretical analysis, it is often challenging to address more complex but practical models.\n\n1. During iterations, it is assumed that the synthetic covariates can only be the copies of a fixed orthonormal set. This assumption might be a little bit strong, can this be further relaxed?\n\n2. It seems that the overparametrized regime has not been considered. Is it possible to generalize the results to the overparametrized regime?\n\n3. In practice, the verifier may evolve over time. Could the authors consider whether a similar analysis can be conducted under this setting?"}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h0VQvOPm7n", "forum": "yfk6c39omW", "replyto": "yfk6c39omW", "signatures": ["ICLR.cc/2026/Conference/Submission21024/Reviewer_j2Ts"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21024/Reviewer_j2Ts"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868975148, "cdate": 1761868975148, "tmdate": 1762940605654, "mdate": 1762940605654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the role of verifier-based filtering in retraining on generated data to avoid model collapse. It shows short-term variance reduction and long-term convergence toward the verifier’s knowledge center."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and easy to follow.\n- Verification plays an important role in preventing collapse in self-consuming models. It’s a simple and intuitive idea."}, "weaknesses": {"value": "- I am mainly concerned about where the incremental theoretical contribution lies. The connection to prior verification in self-consuming works is not discussed in depth (see Question 1).\n- The paper considers pure synthetic replacement. In practice, retraining typically mixes in real data or accumulates data.\n- Experiments are small-scale (MNIST/CVAE only) and lack validation on larger datasets or modern models."}, "questions": {"value": "1. The paper briefly cites [1] but does not clearly explain how it differs from that line of work. Could the authors clarify what the theoretical novelty is relative to [1][2], and how this work connects to recent studies [3][4]?\n2. I have some concerns about experimental setup:\n    - Does initializing on only 500 real MNIST images limit the baseline and exaggerate the apparent early gains?\n    - The paper states that \"the verifier is trained on varying amounts of real data together with an equal number of synthetic samples.\" Which round’s synthetic samples are used? Is the verifier retrained each iteration (i.e., changing over time), or fixed?\n    - The paper mentions keeping the Top-$10\\%$ of generated samples. If the verifier is binary, why is there a \"top\" ranking rather than simply using all passing samples (and reporting number)? If it is deterministic top-scoring selection, does this still match the theory's assumption?\n\n[1] Damien Ferbach, Quentin Bertrand, Joey Bose, and Gauthier Gidel. Self-consuming generative models with curated data provably optimize human preferences. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.\n\n[2] Xiukun Wei and Xueru Zhang. Self-consuming generative models with adversarially curated data. In Forty-second International Conference on Machine Learning, 2025.\n\n[3] Kareem Amin, Sara Babakniya, Alex Bie, Weiwei Kong, Umar Syed, and Sergei Vassilvitskii. Escaping collapse: The strength of weak data for large language model training. In Scaling Self-Improving Foundation Models without Human Supervision, 2025.\n\n[4] Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, and Bowen Zhou. How to synthesize text data without model collapse? In Forty-second International Conference on Machine Learning, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BYDeTYQm3B", "forum": "yfk6c39omW", "replyto": "yfk6c39omW", "signatures": ["ICLR.cc/2026/Conference/Submission21024/Reviewer_SkJA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21024/Reviewer_SkJA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977816235, "cdate": 1761977816235, "tmdate": 1762940605269, "mdate": 1762940605269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors analyze the learning dynamic of generative models iteratively retrained on their own data. The novelty comes from the fact that each step, (potentially bad) data are discarded from the retraining loop, using an external verifier.\nAuthors validate their theory on synthetic experiments and MNIST."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The topic is very timely: quantifying how much can be leverage from synthetic data is a central question for generative modelling."}, "weaknesses": {"value": "- Disconnection between the analysis and motivation. As [1, 2, 3], authors analyze a linear regression setting setting, that does significantly differ from the generative model setting (as started by the authors in the conclusion). Did author try to make the analysis in the true generative model setting? Like [4]? Can authors provide justification of why to study this setting? and it is relevant to generative modelling?\n- In particular, the proposed setting looks like well-studied semi-supervised learning results [4, 5]?\n- Experiments: for Figure 3, authors mentioned they used the FID metric. I di not find any additional comment, so I assume authors use standard vanilla FID. If so, I think this is a mistake for multiple reasons:\n    - FID rely on the Inception embedding, that is a standard for natural images: to the best of my knowledge, MNIST images are not considered natural images.\n    - In addition, the point of using a embedding is to lower the dimensionality of the data, in order to have a better approximation of the empirical Wasserstein distance\nTo my knowledge, the best for MNSIT, is to train a classifier from scratch, and take the last layer for the embedding.\n- Figure 3b, what is the \"verifier training size\" in the caption? Is this a way to vary the quality of the verifier?\n\n[1] Elvis Dohmatob, Yunzhen Feng, and Julia Kempe. Model collapse demystified: The case of regression\n\n[2] Elvis Dohmatob, Yunzhen Feng, Arjun Subramonian, and Julia Kempe. Strong model collapse\n\n[3] Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, and Julia Kempe. A tale of tails: Model collapse as a change of scaling laws\n\n[4] Damien Ferbach, Quentin Bertrand, Avishek Joey Bose, and Gauthier Gidel. Self-consuming generative models with curated data provably optimize human preferences\n\n[4] Ben-David, Shai, Tyler Lu, and Dávid Pál. \"Does Unlabeled Data Provably Help? Worst-case Analysis of the Sample Complexity of Semi-Supervised Learning.\" COLT. 2008.\n\n[5] Zhou, Z.-H. (2018). A brief introduction to weakly supervised learn­\ning. National Science Review"}, "questions": {"value": "- \"Suppose each eigenvalue of the design matrix X 0 is ω( n0 )\", what is w? do you assume that the eigenvalues are strictly a function w of n0?\n- I do not understand the takeaway of Theorem 3.1, especially I do not understand the discussion around Equation 9.\n- \"This contribution also clarifies a common misconception: even with a perfect verifier (θc = θ ⋆ ) and infinitely many synthetic samples in one iteration, convergence cannot occur in a single step. As\nshown in Theorem 3.1, while infinite samples remove the synthetic variance term, the verification bias+variance term persists.\" I do not understand this comment: if the verifier is perfect, why would the verification bias persist?\n- Could authors comment on the conclusion line 299, with the \"3 phases\" depending on the verifier, isn't it obvious that if the verifier is unbiased, it will help, and it is not a well-suited verifier, then the filtering procedure will not be helpful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pvz62EC0BZ", "forum": "yfk6c39omW", "replyto": "yfk6c39omW", "signatures": ["ICLR.cc/2026/Conference/Submission21024/Reviewer_Lw2s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21024/Reviewer_Lw2s"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762084115442, "cdate": 1762084115442, "tmdate": 1762940604237, "mdate": 1762940604237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}