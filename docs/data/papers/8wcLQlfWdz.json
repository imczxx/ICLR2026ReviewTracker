{"id": "8wcLQlfWdz", "number": 1484, "cdate": 1756886746620, "mdate": 1759898206448, "content": {"title": "UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning", "abstract": "Building generalist robot policies that can handle diverse tasks in open-ended environments is a central challenge in robotics. To leverage knowledge from large-scale pretraining, prior work has typically built generalist policies either on top of vision-language understanding models (VLMs) or generative models. However, both semantic understanding from vision-language pretraining and visual dynamics modeling from visual-generation pretraining are crucial for embodied robots.\nRecent unified models of generation and understanding have demonstrated strong capabilities in both comprehension and generation through large-scale pretraining. We posit that robotic policy learning can likewise benefit from the combined strengths of understanding, planning and continuous future representation learning. Building on this insight, we introduce UniCoD, which acquires the ability to dynamically model high-dimensional visual features through pretraining on over 1M internet-scale instructional manipulation videos. Subsequently, UniCoD is fine-tuned on data collected from the robot embodiment, enabling the learning of mappings from predictive representations to action tokens. Extensive experiments show our approach consistently outperforms baseline methods in terms of 9\\% and 12\\% across both simulation environments and real-world out-of-distribution tasks. Demos and code can be found at \\href{https://sites.google.com/view/uni-cod}{our anonymous website}.", "tldr": "We propose a unified Vision-Language-Action (VLA) framework that integrates language-conditioned planning with continuous visual feature prediction, demonstrating strong generalization capabilities across both simulation and real-world experiments.", "keywords": ["VLA", "Robot Learning", "Manipulation", "Generalist Policy"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/576a546dd9ef5cfa89794a2f6a8a09b4ecbca74e.pdf", "supplementary_material": "/attachment/f16cd1c62771550544d033161cd5ee82d2e7a542.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a VLA called UniCoD. The model is based on the mixture of transformers paradigm trained on both continuous and discrete losses. The model is trained in two stages: the first stage only contains vision-language data and trains the model on CE next-token prediction and  MSE loss. The second stage introduces action data and trains a flow-matching expert, as well as the MSE loss for the generation expert. The authors present good results on SimplerEnv-WindowsX [table 1], calvin [table 2] and on a real-world robot [figures 4,5]."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors provide gains on multiple benchmarks and real-world evaluations.\n- The ablations in Table 4 clearly show that the MSE loss helps."}, "weaknesses": {"value": "1. The novelty is limited.\n2.  The used baselines are not consistent among tables (e.g, table 2 doesn't have octo.) and standard baselines like Groot models are missing.\n3. The writing is not very clear about what the contribution is."}, "questions": {"value": "1. Why are the baselines not consistent among tables? E.g., table 2 doesn't have octo. Can you add all baselines to all tables? \n2. Table 4 seems to show that pretraining barely helps, it only gives 2 points. Is that correct?\n3. Can you clarify what the novelty is?\n4. Could you add error bars to your results?\n5. Could you ablate the use of discrete predictions too in table 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A2wAP0Kub8", "forum": "8wcLQlfWdz", "replyto": "8wcLQlfWdz", "signatures": ["ICLR.cc/2026/Conference/Submission1484/Reviewer_TXuu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1484/Reviewer_TXuu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761259225379, "cdate": 1761259225379, "tmdate": 1762915782302, "mdate": 1762915782302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UniCoD, a novel framework for training generalist robot policies by unifying discrete and continuous representation learning. The core challenge it addresses is that existing methods typically rely on either vision-language models (for understanding) or generative models (for dynamics), while both are crucial for robotics. UniCoD learns to simultaneously understand tasks via discrete language representations and model world dynamics by predicting continuous future visual features. Experiments conducted in simulation (SimplerEnv, Calvin) and on two real-world platforms (a 7-DOF arm and a 12-DOF dexterous hand) show that UniCoD achieves state-of-the-art performance. It significantly outperforms baseline methods, demonstrating superior generalization to novel objects and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method involves a two-stage training process:\n1. Pre-training: The model is first pre-trained on over 1 million internet-scale instructional videos and embodied VQA data to learn these joint representations. It's a heavy work. \n2. Fine-tuning: An action expert is then added, and the model is fine-tuned on robot-specific data, learning to map its predictive representations to action tokens."}, "weaknesses": {"value": "1. This framework is followed the understanding&generation framework. Please discuss the difference between CoT-VLA, Up-VLA and HybridVLA. \n2. The author need to give more information about the efficiency of the model control, especially when the paper utilize such heavy framework to perform the robot control.\n3. I wonder whether the ``pretraining on over 1M internet-scale instructional manipulation videos'' is beneficial for all downstream manipulation tasks. Particularly, please show that the generation quality of the images after the downstream manipulation finetuning.\n4. I don't know the meaning of utilizing the such kind of heavy pipeline to perform VLA. The performance improvement is completely unable to offset the overall computational overhead, like compared to CogACT in SimplerEnv-Google Robot Benchmarks."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UPaRvm5ckY", "forum": "8wcLQlfWdz", "replyto": "8wcLQlfWdz", "signatures": ["ICLR.cc/2026/Conference/Submission1484/Reviewer_Cpwz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1484/Reviewer_Cpwz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570089411, "cdate": 1761570089411, "tmdate": 1762915782144, "mdate": 1762915782144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "UniCoD is a unified multimodal framework that uses a MoT architecture to integrate text understanding, visual prediction, and action execution for robotic manipulation.  \n\nIn the first stage, it learns joint visionâ€“language embeddings by aligning textual instructions with visual observations and predicting future visual states in a continuous feature space using a frozen visual encoder. Then, UniCoD fine-tunes this model with embodiment data by introducing an action expert that learns continuous action distributions via flow matching, enabling coherent mapping from multimodal inputs to robot actions.\n\nResults show that UniCod achieves state-of-the-art results across both simulated and real-world environments."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "By jointly training on text, vision, and continuous future prediction, UniCoD builds deeply aligned multimodal embeddings that are resilient to noise or missing information in any single modality.\n\nThe modular MoT design enables selective fine-tuning (e.g., only the action expert), reducing computational cost and preventing catastrophic forgetting of general skills.\n\nThe dual-objective training (cross-entropy for language, MSE for vision, flow matching for actions) helps maintain stable convergence and robust multimodal coordination."}, "weaknesses": {"value": "The model design with multiple expert modules (for language, vision, generation, and action) requires substantial computational resources and careful coordination. This can make training costly, difficult to reproduce, and potentially unstable without large-scale infrastructure. Are there analysis on the cost for fine-tuning on new tasks and new envs?\n\nHow about the interpretability of the model? For example, are there any safety or hazard-preventing modules or self-correcting modules of the model?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "roXblpNBpM", "forum": "8wcLQlfWdz", "replyto": "8wcLQlfWdz", "signatures": ["ICLR.cc/2026/Conference/Submission1484/Reviewer_pt64"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1484/Reviewer_pt64"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931669336, "cdate": 1761931669336, "tmdate": 1762915782044, "mdate": 1762915782044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "UniCoD is a VLA policy that couples discrete language/understanding tokens with continuous predictions of future visual features in a Mixture-of-Transformers. The model is pretrained on ~1M instructional/manipulation videos to predict future visual embeddings and VQA/planning tokens, then fine-tuned with an action expert via flow matching while retaining the future-prediction head. The premise is that policy learning improves when semantic reasoning, planning, and explicit future representations are learned jointly. Reported experiments show state-of-the-art results on SimplerEnv and CALVIN, plus strong performance on two real-robot setups. Ablations (Table 4) suggest that future-feature prediction accounts for a substantial share of the gains."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed procedure is intuitive and well-motivated, combining discrete reasoning with continuous visual forecasting.\n\n2. The authors did conduct extensive experimental analyses, including on two real-world robots, with a clearly detailed protocol. The results show that UniCoD achieves the state-of-the-art performance in multiple benchmarks.\n\n3. The paper is clearly written, and the ablation studies performed by the authors are thorough and convincing."}, "weaknesses": {"value": "1. While the experimental results are detailed, they are only point-estimates with no std or confidence-intervals. The authors should add these, since they did multiple trials. This is a significant weakness."}, "questions": {"value": "1. Please add statistical analyses of the results for multiple trials.\n2. The dataset is very large. Is it guaranteed that unseen objects are not in the training dataset ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KeWXO76yIH", "forum": "8wcLQlfWdz", "replyto": "8wcLQlfWdz", "signatures": ["ICLR.cc/2026/Conference/Submission1484/Reviewer_Nbom"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1484/Reviewer_Nbom"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131686850, "cdate": 1762131686850, "tmdate": 1762915781931, "mdate": 1762915781931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}