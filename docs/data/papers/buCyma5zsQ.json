{"id": "buCyma5zsQ", "number": 8721, "cdate": 1758096115581, "mdate": 1759897767720, "content": {"title": "ParaRater: Enhancing Cross-Lingual Transfer in Large Langauge Models with Meta-Learning", "abstract": "Multilingual LLMs are rapidly emerging, accompanied by claims of supporting an ever-increasing number of languages. However, significant gaps remain between their performance in English and in other languages. Due to the limited quantity and quality of low-resource language data, independently improving these languages is a tough route. A natural alternative is to transfer the capabilities learned in English to low-resource languages. Parallel corpora play a key role in such transfer, and some prior works have conducted empirical studies. Yet, which types of parallel corpora contribute most effectively to cross-lingual transfer has not been systematically explored.\nTo address this, we propose ParaRater, a corpus selection method designed to identify the most valuable English data to be translated into target languages, thereby constructing high-quality parallel corpora that efficiently boost performance in those languages. ParaRater leverages meta-learning to directly align corpus selection with model performance on native target-language data. It further employs a two-stage filtering process to pinpoint data that is only effective when both language versions appear in training—i.e., truly impactful parallel corpora.\nWe demonstrate the effectiveness of this approach across multiple languages and provide detailed qualitative analyses, offering new insights into cross-lingual transfer in large language models. Our rater, datasets, and code are all released open-source.", "tldr": "", "keywords": ["LLM", "Multilinguistic"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b3c643ae5a245f88ccd64f1fe190280859c2624.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The central question of the paper is:\nGiven a pair of parallel documents (e.g. English original and Chinese translation),\nhow much does it contribute to the multilingual performance of a language model, when included into the pretraining data?\nMore specifically, the authors care about cases in which the *combination* of the two documents drives performance, not either language version on its own.\n\nTo address this question, they introduce a meta-learning approach/model called *ParaRater*.\nIn a first stage, ParaRater is trained to rate each document independently (i.e., ignoring parallelism).\nPairs of documents that get a high score in both languages are retained.\nIn a second stage, a new instance of ParaRater is trained on a subset of the corpus:\nsource-language documents that got a high score, plus *mismatched* target-language documents that also got a high score, plus some low-scoring source-language documents.\nThe authors retain only those source-language documents (and corresponding target-language versions) whose ranking dropped in the second stage,\narguing that these documents only contribute because of parallelism."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Originality:**\nThe paper introduces a new approach to rate documents in parallel corpora for their contribution to quality of multilingual language models.\nIn particular, it disentangles truly parallel document contributions from those in which only the source-language or only the target-language version contributes to model performance.\n\n**Quality:**\nThe approach appears methodologically sound.\n\n**Clarity:**\nFair enough, but could be improved (see below).\n\n**Significance:**\nThe contribution could lead to a new way of assessing the importance of parallel documents for multilingual LM pretraining.\nThe authors mention one potential use case that seems particularly exciting:\nassessing the importance of source-language (usually English) documents *before* they are translated,\nthus guiding translation efforts."}, "weaknesses": {"value": "**\"Low-resource\":**\nYou should clarify what you mean by \"low-resource languages\".\nThere are several definitions in the literature (e.g. https://aclanthology.org/2024.emnlp-main.983.pdf , https://aclanthology.org/2020.acl-main.560.pdf).\nSome of the target languages you are investigating are actually very high-resource according to most definitions (Chinese, French, German);\nat best they are lower-resource compared to English.\nThis is a real limitation:\nYou are implicitly assuming that machine translation into the target languages is possible (if expensive).\nFor truly low-resource languages, a human translation effort would be required, which is of course even more expensive.\nThis limitation is worsened by the fact that you train a separate ParaRater for each language.\nSo if you want to advertise your work with the \"low-resource\" keyword, you should at least acknowledge this limitation.\n\nSome of the open-ended questions in my next answer may lead towards addressing this limitation.\n\n**Clarity:**\nEspecially section 4 is hard to follow:\nIt is difficult to understand the structure of the section at a glance.\nRelatedly, figure 1 is also hard to follow.\nTo improve clarity, you could in particular:\n\n* announce the structure at the beginning of the section, and in particular describe each of the two stages in ~1 sentence;\n* provide a more intuitive description of stage 2, in addition to the formal description you already give;\n* state more prominently that the rater scores source and target language documents separately;\n* state more prominently that you are training a separate ParaRater for each target language;\n* boldface *ParaTool* and *ParaCore* on lines 332-224, since your introduction mentions them as separate contributions;\n* answer the clarification questions below.\n\n**Grammatical errors:**\n* l. 192: \"a high-value pre-determined size of subset\" -> \"a high-value subset of pre-determined size\"\n* l. 307 \"other data select method\" -> \"other data selection methods\""}, "questions": {"value": "Clarification questions:\n\n* l. 290-292: \"*Unlike some previous work, we do not use concatenated parallel pairs in order to eliminate potential bias introduced by the data format; instead, all parallel\ndocuments are distributed independently within the training data.*\"\nWhich previous work? And why do you do this?\n* algorithm 1 (l. 717-738): Is $\\theta^{(0)}$ the same at every step of the outer loop, or is it the old $\\theta^{(t)}$? At present this is unclear to me.\n\nMore open-ended / future-work questions:\n* l. 273-283 (source-only selection): Is this just a hypothetical scenario or have you actually tried something of the sort? For source-only selection, would you reuse a ParaRater you trained on a parallel corpus, or retrain a new one? If you train a new one, how would you adapt the training recipe?\nYou should say more about this, since this is possibly the most important use case.\n* Does a ParaRater trained on one LM architecture/size transfer to another LM architecture/size?\n* Do ParaRaters trained for different target languages select different source language (English) documents? What are the differences, if any?\n* Depending on the answer to the previous question: Have you considered training just one ParaRater for a set of languages (instead of a separate one for each language)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yDhgNt5o3r", "forum": "buCyma5zsQ", "replyto": "buCyma5zsQ", "signatures": ["ICLR.cc/2026/Conference/Submission8721/Reviewer_4mvy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8721/Reviewer_4mvy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761308491723, "cdate": 1761308491723, "tmdate": 1762920521937, "mdate": 1762920521937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on selecting parallel data used for pre-training of LLMs for facilitating their cross-lingual transfer capabilities.\nGiven a set of corpora, a two-stage filtering strategy is applied.\nFirst,"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is simple, and the experiments are extensive, including those on pre-training.\n- The case study examines intuitive examples, providing useful insights for future research on cross-lingual transfer.\n- To the best of my knowledge, there has been little work focusing on data selection for pre-training large language models, particularly with regard to enhancing cross-lingual transfer capabilities."}, "weaknesses": {"value": "- I believe the overall presentation could be improved. For instance, the notations surrounding Equation (5) are somewhat complex and could be simplified for clarity. Figure 1 may also be enhanced to more effectively illustrate the concept—particularly the Stage 2 component, which is difficult to interpret in its current form. Moreover, the paper lacks sufficient details on how the selected data are actually utilized. If the data are used merely for language modeling, the role of parallel data remains unclear. Since this work focuses on parallel data, it would be worthwhile to explore alternative ways of leveraging such data beyond conventional language modeling, which can also be performed using monolingual data. The unique merits of parallel data can—and should—be investigated further, rather than using them simply as another component of standard autoregressive pre-training.\n- The performance of the proposed method is not sufficiently convincing. Although the baselines employed are rather general methods for filtering pre-training data and do not explicitly address the research question of enhancing cross-lingual transfer, they generally outperform the proposed approach. It would strengthen the paper to include ablation studies that analyze key algorithmic choices. For example, why was Qwen3-Embedding-0.6B selected as the initial rater? Providing justification and comparative evidence for such decisions would improve the paper’s reliability.\n- I also have concerns regarding the practical feasibility of the proposed method. The work centers on selecting parallel data, yet the experiments rely on machine-translated data due to the scarcity of high-quality parallel corpora. While parallel data can be beneficial, in practice, obtaining such data at scale and with sufficient quality is challenging. Given this limitation, it may be more practical to explore how to improve cross-lingual transfer using non-aligned, monolingual data instead."}, "questions": {"value": "- Typo (L139): curpus -> corpus"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l3oCQsOREE", "forum": "buCyma5zsQ", "replyto": "buCyma5zsQ", "signatures": ["ICLR.cc/2026/Conference/Submission8721/Reviewer_W5xU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8721/Reviewer_W5xU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528932626, "cdate": 1761528932626, "tmdate": 1762920521600, "mdate": 1762920521600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The study is motivated by the multilingual performance gap in LLMs. Investigate which types of parallel corpora contribute most to cross-lingual transfer performance by introducing ParaRater which is aimed at supporting corpus selection by identifying the most relevant English text to translate. This selection is adaptive based on the target language by leveraging meta learning. The authors apply their framework to create the ParaCore training dataset"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- While meta-learning for data selection is well established, the authors introduce novel approach specific for task-specific fine-tuning for multilingual models.\n- The idea of selecting corpora to build task or language specific is relevant and interesting.\n- The study provides good illustrative examples on how this framework can be used."}, "weaknesses": {"value": "- Mismatch between the framework explained in the introduction and the framework presented in the abstract and discussion. Filtering source and target language corpora requires access to parallel data, which conflicts with the proposed benefit of selecting source language data to translate. \n- Missing baseline: The proposed data selection approach should be compared to a random data selection from the same corpus (FineWeb 2).\n- The corpus selection is applied to the highly curated fineweb 2 corpus. To assess the effectiveness of ParaRater, the method should also be applied to more noisy, non-curated web corpora.\n- Parts of the description of the proposed ParaRater method are unclear, introducing multiple adaptations and changes to the objectives after initial descriptions\n  - assumes D_{test} is available in the source language\n- The only realistic experimental setup is described in lines 279-283. As the authors correctly note, all previous methodologies dependent on test data (or machine translated test data), which is unrealistic.\n- COMET based filtering makes the experimental setup more unrealistic, as the results are biased towards easy to translate texts"}, "questions": {"value": "- Please clarify how you envision the framework applied in a practical setting without access to abundant parallel corpora. Please also specify which results presented in the manuscript correspond to this setting. \n\n- typo line 139"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bg4WrfHryd", "forum": "buCyma5zsQ", "replyto": "buCyma5zsQ", "signatures": ["ICLR.cc/2026/Conference/Submission8721/Reviewer_fq6e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8721/Reviewer_fq6e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911983210, "cdate": 1761911983210, "tmdate": 1762920521224, "mdate": 1762920521224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the lack of systematic understanding regarding which types of parallel corpora most effectively support cross-lingual transfer from English to other languages. The authors propose **ParaRater**, a meta-learning–based data selection method that identifies the most valuable English sentences to translate, by directly aligning corpus selection with model performance on native target-language data. Using a two-stage filtering mechanism, ParaRater isolates data instances that only improve performance when both their English and translated versions are included in training—capturing truly impactful parallel examples. Experiments across multiple languages demonstrate the method’s effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors identify a highly valuable yet underexplored issue in multilingual that not all parallel data equally contribute to cross-lingual transfer. \n\n2. Beyond merely selecting “more useful” sentence pairs, the work provides a deeper diagnostic by distinguishing whether performance gains stem from the **joint presence** of source and target sentences in training, or from English **independent contributions**. This insight helps isolate truly synergistic bilingual examples that drive effective cross-lingual knowledge transfer."}, "weaknesses": {"value": "1. **Limited Methodological Novelty**:  \n   The distinction between the proposed method (ParaRater) and prior work such as **DataRater** is not clearly articulated. The approach appears to primarily adapt an existing framework to a new setting—cross-lingual transfer—by modifying the optimization objective: instead of merely selecting parallel examples with low loss, it favors those whose non-parallel counterparts yield significantly higher loss. While this shift is meaningful, the core technique lacks substantial innovation beyond the objective redesign.\n\n2. **Missing Baseline Comparison**:  \n   The paper omits a critical baseline: **random selection of English sentences for translation**. Without this, it is difficult to assess whether the gains truly stem from intelligent data selection or simply from the inclusion of any additional parallel data.\n\n3. **Scalability and Transferability Concerns**:  \n   It remains unclear whether the ParaRater selector must be retrained for different model architectures or scales (e.g., 7B vs. 70B LLMs). If a dedicated selector is required per model, the computational cost of training the selector may outweigh the marginal gains—especially when compared against the simpler random baseline.\n\n4. **Evaluation Scope Limited to Pretraining**:  \n   The authors argue that parallel data is most effective during pretraining. However, given the prevalence of strong multilingual foundation models today, the community increasingly focuses on **post-training** (e.g., instruction tuning, alignment) for cross-lingual enhancement. The absence of experiments in post-training settings raises concerns about whether the observed pretraining benefits generalize to more practical, downstream adaptation scenarios.\n\n5. **Lack of Scaling Analysis**:  \n   While the method is validated on a 1.2B-parameter model, its effectiveness on larger-scale LLMs (e.g., 7B, 13B, or beyond) is unexamined. As model capacity increases, the sensitivity to data quality may change—potentially diminishing the relative impact of curated parallel data. A scaling study would be essential to justify the method’s relevance in modern LLM development pipelines."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rjHLz4LSVh", "forum": "buCyma5zsQ", "replyto": "buCyma5zsQ", "signatures": ["ICLR.cc/2026/Conference/Submission8721/Reviewer_NFEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8721/Reviewer_NFEf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981316892, "cdate": 1761981316892, "tmdate": 1762920520905, "mdate": 1762920520905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}