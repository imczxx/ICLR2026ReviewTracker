{"id": "rlq9aKsY7T", "number": 17128, "cdate": 1758272518916, "mdate": 1759897194855, "content": {"title": "Gradient Inversion Transcript: Leveraging Robust Generated Priors to Reconstruct Training Data from Gradient Leakage", "abstract": "We propose Gradient Inversion Transcript (GIT), a novel model-based approach for reconstructing training data from leaked gradients. GIT employs a data reconstruction model, whose architecture is tailored to align with the inversion of the federated learning (FL) model's back-propagation process. Once trained offline, GIT can be deployed efficiently and only relies on the leaked gradients to reconstruct the input data, rendering it applicable under various distributed learning environments.\nWhen used as a prior for other iterative optimization-based methods, GIT not only accelerates convergence but also enhances the overall reconstruction quality. GIT consistently outperforms existing methods across multiple datasets and demonstrates strong robustness under challenging conditions, including inaccurate gradients, data distribution shifts and discrepancies in model parameters.", "tldr": "", "keywords": ["gradient inversion", "training data reconstruction", "image prior"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/171fbc6b688d8e0ad0756c07cee0d44042219d53.pdf", "supplementary_material": "/attachment/e481fbb4324eda64b1877ec67c19fc7e25596767.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Gradient Inversion Transcript (GIT), a novel model-based method for reconstructing training data from leaked gradients in federated learning. The core idea is to construct a reconstruction model whose architecture is adaptively tailored to approximate the inverse of the target model's back-propagation process. The authors propose two variants: a theoretically-grounded \"Exact-GIT\" that directly implements the derived recursive inversion formula, and a more practical \"Coarse-GIT\" that uses MLPs to approximate this process. The paper claims that GIT is more efficient and robust than existing methods, showing strong performance across various datasets and under challenging conditions like gradient perturbations and distribution shifts. Furthermore, the output of GIT is proposed as a prior to accelerate and improve optimization-based attacks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of constructing an inversion process by modelling the recursive relationship of layer input gradients through layer-by-layer insertion of the corresponding gradients is creative and is a potentially valuable improvement over existing work. The mathematics is also technically sound.\n\n2. Evaluating GIT not only as a standalone attack, but in combination with existing optimization methods is a good contribution towards refining different frameworks.\n\n3. Under the evaluated settings, it seems that GIT is substantially more effective compared to existing attacks."}, "weaknesses": {"value": "1. **Mathematical writing and details**: The details in Section 3 are unclear and contain factually wrong mathematics:\n   - For the most common applications in this field, the gradient operator on a matrix is defined as the element-wise operator on each gradient element. In their definition, the authors instead seem to use the transpose, causing slight confusion in the manuscript. They should clarify this to avoid inconsistencies between different readers' background knowledge. \n   - Tensor multiplication is usually a different operation to what is defined in the paper (i.e. in the standard tensor multiplication definition, a $A\\times B$ matrix multiplied by a $C\\times D$ matrix will result in a $A\\times B \\times C\\times D$ tensor). I believe what the authors denote is a simple matrix multiplication, and should clarify this to avoid further confusion.\n   - The authors should either present proofs of their statements in Appendix, or refer to existing proofs in the main text, as the results are non-trivial. In the current state the writing level is below par to what is expected in an ICLR submission.\n\n2. **Writing and Clarity**:\n   - I elaborate on clarity issues in the Introduction and Related Work sections in **W3** and **W4**.\n   - The technical details are overall too vague across the entire paper. A rewrite with separate sections/paragraphs in the main text or Appendix are warranted for clarifying the attacker assumptions, the mathematical derivations, algorithmic description, and the experimental settings. It must be clarified where the derivations in Section 3 are used as inspiration, and where they are directly incorporated as part of the inversion MLP.\n   - It is unclear how much of the Section 3 derivations are equivalent or inspired by R-GAP [5], who perform a similar recursive approach explicitly on a layer-by-layer basis.\n   - (Minor) I would discourage the authors from referencing prior work only through hyperlinks in Table 1, without explicit references in the main text (as they did for iLRG and SPEAR), and acknowledging the difference from other related work.\n   - (Nitpick) In L430 the authors refer to themselves as \"I\".\n\n3. **Setting specification**: The authors seem to present a generally unorthodox problem setting. First, in their setting, the authors describe:\n    - \"Our method does not need the parameters of the leaked model or the labels of the training data, as GIT trains the auxiliary reconstruction model on publicly available data with known labels.\" -> Given that in order to obtain gradients for the training data, one needs to assume knowledge of the model weights, this statement's assumptions are inherently stronger than acknowledged.\n    - \" As illustrated in Figure 1, we adopt a similar premise to DLG Zhu et al. (2019): the attacker hacks the channel to inject data to one client, which shares the gradient with the server and other clients\" -> This statement is incorrect and confusing for multiple reasons: 1) To the extent of my knowledge, no data is injected in any gradient leakage setting, and the authors describe no such event in their paper as well, 2) the correlation between data injection and gradient sharing is unclear and seemingly unrelated, 3) \"Hacking\" a single client does not seem to relate to the other clients, who most often share their gradients with the server, 4) In the most usual case (including DLG) of gradient inversion, no hacking is involved, but a server, that can be either malicious, or honest-but-curious, monitors the shared gradients and provides the model updates to the clients. In some distributed settings, some of these interactions are slightly different, but the core assumptions remain the same.\n    - Figure 1 introduces the notion of a hacked channel, which can both both somehow inject data and receive gradient information. This seems more unrealistic than the honest-but-curious server setting, as it assumes that the client is is completely unaware of the data they are using, as well as the gradients they are sending. This proposal seems like an unnecessary man-in-the-middle-like attack, which also does not interact with other clients, despite the claims of the authors that the attack will reconstruct both the hacked user (who should be using the injected data?), and the remaining users' data.\n    - \"The attack aims to reconstruct the data from both the hacked client and other clients by shared gradients\" -> This statement introduces the notion of a \"hacked client\", without emphasis on what the hacking entails or what actions can be read/written.\n    - \"unlike iterative optimization-based methods, which are not applicable for scenario (2) since the attacker has no access to inject data to unhacked clients and is therefore unable to reconstruct training data from them.\" -> The injection angle is again unclear, and the authors must clarify their assumptions regarding this again.\n\n The paper fails to articulate a coherent or realistic threat model. The 'hacked channel' and 'data injection' premises are contrived and deviate from established literature without justification, while the claim of reconstructing data from non-hacked clients remains entirely unsubstantiated.\n\n4. **Related work**: I remain unconvinced of the familiarity of the authors with related work, leading to the confusing and questionable position of GIT in the gradient leakage field. I write some comments and corrections below:\n    - **RTNN**, while also dealing with privacy, is a **model-inversion** technique, which explores a significantly different challenge to gradient leakage in Federated Learning.\n    - Most of the settings, described in Table 1 are wrong. For one, essentially all gradient leakage attacks **assume knowledge of the model parameters and architectures**. This is because plenty of them rely on optimising the proxy gradients to be as close as possible to the observed ones, for which you would need to emulate both the backward and forward pass. This is the fundamental assumption of **DLG [1], Geiping et al. [2], iDLG [3], etc.**. This is what I suppose the authors refer to as \"the gradient query\" pipeline, but having access to that and having the access to the model are essentially equivalent.\n    - As some model-based methods also incorporate such a loss, that also holds for that section as well.\n    - The column \"Pretrained GAN\" restricts the architecture of the generative model without no particular purpose, i.e. DGGI [4] uses a diffusion model.\n    - SPEAR [5] 's position in the field is different than what the authors have described. The attack requires the existence of a ReLU-activated linear layer, but not much beyond that. In particular, the authors specifically highlight in the paper that their method does not require access to the data labels, or whatever the authors refer to as the \"output logit\", which should be further clarified.\n    - LIT, and by extension, GIT, also require the computation of gradients for their training set. This, by extension, assumes knowledge of the model weights and architecture.\n\n This misrepresentation of prior work invalidates the paper's comparative analysis and suggests the authors do not have a firm grasp of the literature. The current state of the introduction and related work sections warrants a full rewrite of the transcript, without which this paper will remain scientifically invalid.\n\n5. **Exact-GIT**: The authors present 2 methods, with **Exact-GIT** supposed to more closely emulate Equations (3) and (5)'s explicit formulae. However, the only results the authors present is an analysis into the closeness of the trained parameters with the actual model weights. Given that a substantial part of the paper is dedicated to the recurrent relationship methodology, it is unclear whether this contribution is valid without any reconstruction results. Further, given the lack of baseline or explanation of the values, it is further unclear what the trainable parameters starting to approach the model weights implies. Lastly, the authors describe in L244 that they treat the \"unknown weights\" as additional parameters (which is in relevant settings unnecessary, as described in **W3**), making the system underdetermined, with their being the same number of weight parameters, as gradient constraints, on top of which there are also input free variables.\n\n6. **Evaluation**:\n    - All evaluation settings omit specifying the batch size and image size, which limits the understanding of the difficulty of the setting.\n    - The provided reconstruction images in G.7 seem quite blurry, while the prior work the authors compare to usually include images of significantly better quality. This discrepancy puts the results and practicality of the attack into question.\n    - The \"ViT\" setting is not elaborated upon, particularly how the architecture relates to the transformer one and Equation 5. The authors must be more precise in their explanations.\n    - The \"Innacurate Gradients\" setting is usually supported by reporting the accuracy on the main task, so as to show that the reported noise is not too disruptive to the training pipeline, which the auhtors hav enot provided.\n    - The \"Distribution shift\" experiments 1) use overlapping classes in basically all settings, and 2) still contain data from the same dataset, which may have a particular type of prior distribution behind them. This weakens the claim that GIT works under significant distribution shifts."}, "questions": {"value": "1. What is the setting, in which GIT operates? The authors should clarify all assumptions for the attacker and system explicitly and clearly.\n\n2. The authors must provide further experimental details.\n    - What batch size was used for each experiment? Assuming a batch size of 1, the authors only evaluate against outdated attacks, while more modern attacks have been shown to handle batch sizes significantly larger than 1.\n    - How does the attack perform without having a method to analytically compute the output logits, especially for higher batch sizes? \n    - If, as claimed in the intro, the attack \"reconstructs the data from both the hacked client and other clients\", what is the difference between the settings, and what is the performance difference?\n    - The authors claim in 5.2.1 that \"The results confirm the vulnerability of optimization-based methods against gradient perturbations\". However, outside of IG, the other attacks remain seemingly unaffected by the perturbations, suggesting either that the authors' claim is incorrect, or there exists a flaw in the evaluation. Can the authors explain this discrepancy?\n\n3. What are the reconstruction results for Exact-GIT, and how is the weight convergence relevant to them?\n\n4. The supplementary material provides an implementation for the MLP and ResNet variation of what seems like Coarse-GIT, there seem to be no details on the setup for Exact-GIT. Given that most of theoretical development is unused in Coarse-GIT, not having the option to investigate and replicate the Exact-GIT implementation is unfortunate. Can the authors provide additional clarification, and add the implementation to the supplementary material?\n\n5. The authors report lower PSNR for Geiping et al. [2] for CIFAR10 with the LeNet network than the original paper, even compared to batch sizes of 1, posing the evaluation methodology into question. Why is that the case?\n\n6. As training goes on, the model weights change, and hence the gradients of any input will change in comparison to the initial model state. How does a GIT, trained on the initial model state, perform over the FL training period?\n\n7. Can the authors compare their proposed methodology to that of R-GAP? Given that both techniques propose methods based on similar recursive formulae, the authors should make this clear.\n\n8. Given that Equation 3.2 does not depend on $f_i^{out}$, can the authors discuss the implications of this finding? This seems to imply that depending on how one picks $f^{in}$ and $f^{out}$, it might result in a system of varying difficulty.\n\n9. The authors should address all remaining concerns from the **Weaknesses** section.\n\n## Current recommendation\n\nI am assinging this paper a recommendation of **2: Reject**. Despite the creative central concept of formally inverting the back-propagation process, this paper suffers from fundamental flaws that render it unsuitable for publication in its current state. The problem setting is unorthodox and poorly justified, with confusing and unrealistic assumptions about a \"hacked client\" and \"data injection\" that deviate from standard gradient leakage scenarios, as well as the paper's own application. Critically, the mathematical derivations contain significant errors, most notably in Equation 5, and lack the necessary rigor, invalidating the conclusions that follow. The related work section and comparison table demonstrate a profound misunderstanding of prior literature, misrepresenting the core assumptions. Combined with a lack of empirical results for the \"Exact-GIT\" method and insufficient detail in the experimental setup, the paper's claims are not adequately supported. A complete overhaul of the problem formulation, mathematical foundations, and literature review is required. I encourage the authors to address the weaknesses and questions during the discussion phase, so they can clarify some of my concerns and improve their transcript.\n\n### References\n\n[1] Zhu, Ligeng, Zhijian Liu, and Song Han. \"Deep leakage from gradients.\" Advances in neural information processing systems 32 (2019).\n\n[2] Geiping, Jonas, et al. \"Inverting gradients-how easy is it to break privacy in federated learning?.\" Advances in neural information processing systems 33 (2020): 16937-16947.\n\n[3] Zhao, Bo, Konda Reddy Mopuri, and Hakan Bilen. \"idlg: Improved deep leakage from gradients.\" arXiv preprint arXiv:2001.02610 (2020).\n\n[4] Dimitrov, Dimitar I., et al. \"Spear: Exact gradient inversion of batches in federated learning.\" Advances in Neural Information Processing Systems 37 (2024): 106768-106799.\n\n[5] Zhu, Junyi, and Matthew Blaschko. \"R-gap: Recursive gradient attack on privacy.\" arXiv preprint arXiv:2010.07733 (2020)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hOqpinsggs", "forum": "rlq9aKsY7T", "replyto": "rlq9aKsY7T", "signatures": ["ICLR.cc/2026/Conference/Submission17128/Reviewer_xnwp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17128/Reviewer_xnwp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761299065302, "cdate": 1761299065302, "tmdate": 1762927124095, "mdate": 1762927124095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission introduces Gradient Inversion Transcript (GIT), a model-based attack to reconstruct training data from leaked gradients in federated learning. The main idea is to construct an auxiliary reconstruction model whose architecture is adaptively designed to approximate the inverse of the target model's back-propagation process. The authors present two variants: \"Exact-GIT,\" a theoretically-grounded but computationally intensive version, and \"Coarse-GIT,\" a more practical and efficient version that uses shallow MLPs to approximate the inversion at each layer or module. The experimental results demonstrates that GIT can be used for direct and efficient data reconstruction or as a powerful generator of priors to significantly boost the performance of traditional optimization-based attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Direct but Effective idea: Mirroring the inverse of the target model's gradient flow is interesting. The theoretical derivation in Section 3, which forms the basis of the method, is sound and clearly explained.\n- Strong Performance: The empirical evaluation is comprehensive and demonstrates the effectiveness of GIT. 1) Direct Inference: As shown in Table 2, GIT consistently outperforms both optimization-based methods (DLG, IG) and other model-based methods (LTI) in direct reconstruction across two datasets (CIFAR-10, ImageNet), multiple metrics (MSE, PSNR, SSIM), and different architectures (LeNet, ResNet, ViT); 2) As a Prior: When combined with an optimization method (GIT+IG), the approach achieves state-of-the-art results, significantly outperforming other hybrid methods like GIAS+IG. This shows that GIT generates higher-quality initial estimates."}, "weaknesses": {"value": "- Maybe outdated baseline: The latest baseline mentioned in this submission was presented in 2023. Is there any new state-of-the-art methods?\n- Practical Complexity: The reconstruction model's architecture must be custom-built for each target model. Is the architecture design of the Coarse-GIT affect the performance?\n- Computational Cost: The training efficiency of GIT is worse than DLG and IG."}, "questions": {"value": "- The GIT+IG significantly outperforms GIAS+IG. GIAS relies on a pre-trained generative model, whereas GIT does not. What are the reasons that GIT provides better priors? Is it primarily because its architecture is tailored to the inversion task, whereas a standard GAN architecture is too generic?\n- The effectiveness of gradient attacks is affected by the batch size, since gradients are averaged over more samples. Your experiments seem to focus on small batch sizes. Could you provide empirical results or discuss how the performance of GIT degrades as the batch size increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WNCSfTXolD", "forum": "rlq9aKsY7T", "replyto": "rlq9aKsY7T", "signatures": ["ICLR.cc/2026/Conference/Submission17128/Reviewer_4vGz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17128/Reviewer_4vGz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753812425, "cdate": 1761753812425, "tmdate": 1762927123499, "mdate": 1762927123499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Gradient Inversion Transcript (GIT), a model-based method for reconstructing private training data from leaked gradients. GIT builds a reconstruction model that mirrors the inverse of back-propagation and adapts its architecture to the attacked model. Trained offline on public input–gradient pairs, GIT reconstructs data efficiently without requiring model parameters or labels. Two variants are introduced—Exact-GIT (theoretical inversion) and Coarse-GIT (approximate, efficient). Experiments on CIFAR-10, ImageNet, and facial datasets with LeNet, ResNet, and ViT show that GIT achieves superior performance and strong robustness to noisy gradients and distribution shifts. Moreover, using GIT outputs as priors for iterative optimization further improves accuracy and convergence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-The proposed solution is conceptually sound and appears novel.\n\n-The evaluation setting is extensive. The authors conduct experiments across multiple architectures and cover diverse datasets. The paper also goes beyond standard settings by testing under challenging conditions.\n\n-The proposed method shows consistently superior results across datasets and architectures, outperforming both optimization- and model-based baselines."}, "weaknesses": {"value": "-The comparison baselines are relatively outdated, mostly limited to earlier methods such as DLG, IG, and LTI. The paper lacks comparisons with more recent or stronger baselines, making it difficult to assess whether the proposed method remains competitive.\n\n-The paper assumes the attacker can both inject data into a client and obtain structured per-layer gradients, which requires high-level access and control. This dual assumption may be overly strong or unrealistic in many real-world FL deployments."}, "questions": {"value": "-The problem formulation is not entirely clear. Does the proposed attack allow the adversary to reconstruct data from all clients in the federated system, or only from the compromised one? If it can access data from other clients, please clarify the underlying assumption or mechanism that enables this cross-client reconstruction.\n\n-The paper claims that GIT is adaptive to the leaked model’s architecture, but this seems to apply only to Exact-GIT. Is Coarse-GIT also adaptive in any way, or does it merely use a fixed MLP structure? If the latter, it appears to lose the key advantage of architectural adaptivity. \n\n-The experimental section seems to mix results from Exact-GIT and Coarse-GIT, which makes it unclear which variant contributes to the reported performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HwI49bEZ8B", "forum": "rlq9aKsY7T", "replyto": "rlq9aKsY7T", "signatures": ["ICLR.cc/2026/Conference/Submission17128/Reviewer_jgr4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17128/Reviewer_jgr4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819311387, "cdate": 1761819311387, "tmdate": 1762927123122, "mdate": 1762927123122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper \"Gradient Inversion Transcript (GIT)\" presents a new framework for reconstructing training data from leaked gradients in distributed or federated learning settings. GIT models the mathematical process of back-propagation itself and learns a neural mapping that approximates its inverse, this mathematical derivation reveals that gradients encode sufficient information to estimate layer inputs through tensor operations and pseudo-inverses. GIT can reconstruct private input data directly from leaked gradients in one forward pass, achieving real-time inversion, outperforming prior state-of-the-art inversion attacks in reconstruction accuracy, robustness to noise, and computational efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Theoretically Grounded and Interpretable. GIT is derived from the back propagation equations, not designed heuristically. This gives it a strong theoretical foundation and interpretability \nArchitecture-Adaptive Modular Design. GIT decomposes the target model into multi-input multi-output (MIMO) modules. Each module learns to invert its corresponding back-propagation block, allowing GIT to adapt naturally to different model architectures\nCross-Client attack Capability. GIT can reconstruct inputs of other clients using shared or aggregated gradients by leveraging its learned understanding of how gradients encode input activations, achieving \"one2many\" inversion capability"}, "weaknesses": {"value": "Pretraining depends on public data. GIT requires public data to train the inversion model offline. If the public data are very different from the target domain, he reconstruction quality may degrade substantially.\nWhether it is effective in the actual scene? The attack assumes knowledge of the model architecture and access to shared gradients. This setting may not adapt to fully secured or asynchrony."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0UDAPHtY77", "forum": "rlq9aKsY7T", "replyto": "rlq9aKsY7T", "signatures": ["ICLR.cc/2026/Conference/Submission17128/Reviewer_zriV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17128/Reviewer_zriV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762067045329, "cdate": 1762067045329, "tmdate": 1762927122647, "mdate": 1762927122647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the gradient leakage problem in the federated learning setting. The problem is challenging to reconstruct training images from leaked gradients. This is achieved by an auxiliary model, that GIT generator is trained with image/gradient pairs.  In inference, this GIT generator is applied to the leaked gradients produced from clients' data. Finally, reconstructed images can be obtained. To be honest, we did not check the correctness of all the mathematical formulations in this paper, but the experiments show the better attack performance. In the appendix, visualization results are also shown. In order to show the effectiveness of this method, CIFAR-10 and ImageNet are applied with different network architecture including LeNet, ResNet, and ViT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall idea is clear and interesting. An auxiliary model GIT generator is trained to reconstruct images. The results show that the proposed method can at least recover some shape and color information from leaked gradients, even though many details are missing.\n\n2. This paper is well organized and provides mathematical formulations. Also, reconstruction under different scenarios is discussed, including inaccurate gradients, distribution shift.\n\n3. Dynamically selection of architecture of the threat model is interesting and important for achieving better performance."}, "weaknesses": {"value": "1. The discussion on the reasons of focusing on input-gradient mapping-based methods is not sufficient. Why does this paper focus on input-gradient mapping-based approach? The reconstructed images from GIT can be applied by diffusion models to produce reasonable image details. From the visualization results in appendices, the reconstructed images lack of enough details.\n\n2. The motivation is to estimate clients' training data in federated learning settings. The discussion in federated learning settings is not enough. It seems the proposed method can be also used to attach centralized training."}, "questions": {"value": "1. The distribution of gradients is affected by many factors, including the stage of model training, the dataset distribution, and network architectures. How to choose a GIT generator for different stages of federated learning, for example early training with large learning rate or finetuning with smaller learning rate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rT9rp9PCQH", "forum": "rlq9aKsY7T", "replyto": "rlq9aKsY7T", "signatures": ["ICLR.cc/2026/Conference/Submission17128/Reviewer_QwL7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17128/Reviewer_QwL7"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159552062, "cdate": 1762159552062, "tmdate": 1762927122199, "mdate": 1762927122199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}