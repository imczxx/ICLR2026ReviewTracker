{"id": "cLvzWgk8Lt", "number": 18264, "cdate": 1758285791528, "mdate": 1759897115585, "content": {"title": "Automated Capability Discovery via Foundation Model Self-Exploration", "abstract": "Foundation models have become general-purpose assistants, exhibiting diverse capabilities across numerous domains through training on web-scale data. It remains challenging to precisely characterize even a fraction of the full spectrum of these abilities and potential risks in any new model. Existing evaluation approaches often require significant human effort, and it is taking increasing effort to design ever harder challenges for more capable models. We introduce Automated Capability Discovery (ACD), a framework that designates one foundation model as a scientist to systematically propose open-ended tasks probing the abilities of a subject model (potentially itself). By combining frontier models with ideas from the field of open-endedness, ACD automatically and systematically uncovers a diverse spectrum of surprising capabilities and failures in the subject model. We demonstrate ACD across a range of foundation models (including the GPT, Claude, and Llama series), showing that it automatically generates thousands of distinct tasks, which are then clustered to reveal dozens of broader capability areas and failure modes, that would be challenging for any single team to uncover. We further validate our method's automated scoring with extensive human surveys, observing high agreement between model-generated and human evaluations. By leveraging foundation models' ability to both create tasks and self-evaluate, ACD is a significant step toward scalable, automated evaluation of novel AI systems. \nAll code and evaluation logs are open-sourced at https://anonymous.4open.science/r/ACD-D13E.", "tldr": "", "keywords": ["large language models", "foundation models", "automated evaluation", "model self-exploration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d9fc7d6a1ae8d9057fedaf4510fb3858b6607ed4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper develops an Automated Capability Discovery System whereby an AI (LLM) scientist model probes an AI subject model. Their systems can discover a wide range of unexpected capabilities and failures. ACD also generates a comprehensive report, describing the AI scientist's study of the AI subject model."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This was an exceptionally well-written paper. It was a very smooth and cohesive narrative. \nI think systems like this will be increasingly built in the future, and excited to see steps in this direction."}, "weaknesses": {"value": "The text in Figure 4 is extraordinarily small (even when zooming in). \n\nIt would be nice to compare the evaluations done by ACD to the evaluations done by existing scientists on existing models. Does ACD rediscover many of the capabilities or vulnerabilities that scientists identified in existing models? More discussion of how the weaknesses or capabilities discovered by ACD differ from human evaluators, for instance, they are more varied/diverse or less diverse. Even anecdotal evidence would suffice, but getting human experts to judge the quality of inputs (rather than general human participants) would be significant. \n\nA clearer discussion of automated red teaming and how this differs from previous automated systems would be helpful.\n\nMore discussion of the resource usage, ie, api prices, GPU usage, time, etc in the main body would be a significant improvement. Is this system cheap and cost-effective, or does it require resources that only frontier or well-resourced labs would have?"}, "questions": {"value": "Does ACD rediscover many of the capabilities or vulnerabilities that scientists identified in existing models?\n\nWhat is the computational intensity and price to run this system? \n\nCan such a system be used to benchmark rank models, and if so, what is the ranking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zJG3n5brZZ", "forum": "cLvzWgk8Lt", "replyto": "cLvzWgk8Lt", "signatures": ["ICLR.cc/2026/Conference/Submission18264/Reviewer_ALgU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18264/Reviewer_ALgU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983719520, "cdate": 1761983719520, "tmdate": 1762927987854, "mdate": 1762927987854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Automated Capability Discovery, a framework where a “scientist” foundation model autonomously generates and evaluates tasks to probe the abilities and failure modes of a \"subject\" model. They claim that ACD surfaces diverse task families and reveals both strengths and \"surprising\" weaknesses across GPT-4o, Claude, and Llama models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "- The authors work on a timely challenge in scaling adaptive evaluation for foundation models.\n- The approach demonstrates the ability to generate a wide variety of task types\n- The method has potential practical value because it can produce reusable task repositories for evaluating new models."}, "weaknesses": {"value": "- The paper does not sufficiently define or analyze the notion of “interestingness” and novelty used to filter task proposals; in particular, given that this is a key element in their approach, I'd appreciate a more detailed explanation of how these were evaluated (or supposed to be evaluated by the LLM). I saw the appendix but this still seems very high-level – was either of these concepts at least validated somehow during the task generation process?\n- The authors do not discuss the risk of systematic bias (a) if the scientist and subject models are the same and (b) if the same model (or similar models) both propose tasks and evaluate correctness (see this paper for more problem context: https://arxiv.org/abs/2404.13076).\n- The paper repeatedly claims that ACD reveals failure modes that \"traditional\" benchmarks miss but it doesn't provide any evidence for these claims, so their statements feel very anecdotal.\n- The criteria for declaring a task “consistently solved” or “consistently failed” are not clearly specified or justified.\n- The robustness of the human evaluation is uncertain because the reviewers’ domain knowledge across highly diverse task types is not established.\n- In general, the authors claim that the tasks are valid but don't provide sufficient evidence that this is true (see this paper for a more in-depth explanation of how to establish validity in evals: https://arxiv.org/abs/2505.10573)\n- Nit: abbreviation “FM” was introduced but is inconsistently used\n- The related work & background section don't sufficiently detail how the authors' approach is novel compared to previous attempts (neither 2.1 nor the first paragraph of Section 3 establish this). I'm willing to update my score if this gets clarified."}, "questions": {"value": "- In Section 5.3, was Claude 3.5 also the evaluator throughout the whole discovery process?\n- What does “sufficiently” mean in line 225? What is the stop criteria?\n- Does “consistently solved” mean that all n shot evals need to be correct?\n- How is the validity of each of translated task code ensured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qgzCKG6VWo", "forum": "cLvzWgk8Lt", "replyto": "cLvzWgk8Lt", "signatures": ["ICLR.cc/2026/Conference/Submission18264/Reviewer_foyz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18264/Reviewer_foyz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042075798, "cdate": 1762042075798, "tmdate": 1762927986919, "mdate": 1762927986919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper prompts a large language model to generate questions to study the capabilities of a LLM (either the same or different). They claim that the resulting benchmark is highly diverse, full of novel tasks, and interesting."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written."}, "weaknesses": {"value": "At it's core, this paper fails to provide any real evidence that their methodology is useful. This paper is full of phrases like \"novel task families\" and \"interestingly new,\" seeking to highlight the allegedly highly diverse and presumably useful questions. However at no point do they present any empirical analysis - qualitative or quantitative - of these aspects. While they do do a human study of the questions developed using their methodology, they only ask the humans if the tasks are \"valid and coherent\" and not if they are diverse, novel, interesting, new, etc.\n\nAdditionally, at no point do they compare the evaluation questions developed via their methodology to those using any other LLM-powered methodology or to static benchmarks. We have no way to know if this methodology is superior to the dozens of other similar papers, nor even to believe that it's significantly better than static benchmarks. They also fail to provide ablations on any aspect of their pipeline, leaving the door open for significant improvements via minor tweaks.\n\nFinally they seem to scope their literature review in such a fashion as to render no other methodology a worthwhile comparison point, despite the fact that papers like \"Discovering Language Model Behaviors with Model-Written Evaluations\" and \"AutoBench-V: Can Large Vision-Language Models Benchmark Themselves?\" would make perfectly good points of comparison (neither is cited). If this is deliberate, it is academic misconduct."}, "questions": {"value": "What evidence do you have that the tasks identified by this methodology are interesting or worthwhile, either in abstract or compared to other methods for producing benchmarks?\n\nDid you know about the paper \"Discovering Language Model Behaviors with Model-Written Evaluations\"? If so, why did you not cite or compare to it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NZArX6G4z0", "forum": "cLvzWgk8Lt", "replyto": "cLvzWgk8Lt", "signatures": ["ICLR.cc/2026/Conference/Submission18264/Reviewer_w2xK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18264/Reviewer_w2xK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106228857, "cdate": 1762106228857, "tmdate": 1762927986533, "mdate": 1762927986533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}