{"id": "UF0uK2oGeW", "number": 11609, "cdate": 1758202455665, "mdate": 1759897564869, "content": {"title": "OmniQuality-R: Advancing Reward Models through All-Encompassing Quality Assessment", "abstract": "Current visual evaluation approaches are typically constrained to a single task — focusing either on technical quality for low-level distortions, aesthetic quality for subjective visual appeal, or text-image alignment for semantic consistency.} With the growing role of reward models in guiding generative systems, there is a need to extend into an all-encompassing quality assessment form that integrates multiple tasks. To address this, we propose OmniQuality-R, a unified reward modeling framework that transforms multi-task quality reasoning into continuous and interpretable reward signals for policy optimization.\n\\tct{Inspired by subjective experiments, where participants are given task-specific instructions outlining distinct assessment principles prior to evaluation, we propose OmniQuality-R, a structured reward modeling framework that transforms multi-dimensional reasoning into continuous and interpretable reward signals.\nTo enable this, we construct a reasoning-enhanced reward modeling dataset by sampling informative plan-reason trajectories via rejection sampling, forming a reliable chain-of-thought (CoT) dataset for supervised fine-tuning (SFT). Building on this, we apply Group Relative Policy Optimization (GRPO) for post-training, using a Gaussian-based reward to support continuous score prediction. To further stabilize the training and improve downstream generalization, we incorporate standard deviation (STD) filtering and entropy gating mechanisms during reinforcement learning. These techniques suppress unstable updates and reduce variance in policy optimization. We evaluate OmniQuality-R on three key IQA tasks: aesthetic quality assessment, technical quality evaluation, and text-image alignment. Experiments show OmniQuality-R improves robustness, explainability, and generalization, and can guide text-to-image generation models at test time without retraining by serving as an interpretable reward function.", "tldr": "", "keywords": ["MLLM", "IQA"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a2e4c0ca81b811b1a901cff48af2f5929158d08b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a multi-task reward modeling framework OmniQuality-R for image quality assessment. It constructs a reliable chain-of-thought dataset to pre-train MLLM for reward modeling and then proposes to use continuous reward, standard deviation filtering, and entropy gating mechanisms to train the model with GRPO. It demonstrates OmniQuality-R can produce state-of-the-art performance on image quality assessment tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The reward modeling pipeline is robust and the quantitative results demonstrate the effectiveness of the pipeline. This can be of interest to the community who work on reward modeling and generative models. \n- The sufficient detail provided in the text enables reproduction of the paper feasible. \n- The paper is written clearly and easy to follow."}, "weaknesses": {"value": "- While the proposed pipeline is clear and robust, the main question arises from the technical contribution of the proposed method. \nThe main pipeline is quite similar to the one proposed in \"Q-Ponder: A Unified Training Pipeline for\nReasoning-based Visual Quality Assessment\" (Can et al. 2025), including the way it treats the reward with a continuous function and creating CoT dataset that target multiple aspects for cold-start initialization, even though the exact target task is different. \n- The related work section could highlight the difference between the current work and related works. \n\nDue to the above point, the reviewer currently feels the work is incremental"}, "questions": {"value": "As the reviewer agrees with the technical robustness of the pipeline, the main point that could be addressed in the rebuttal is regarding the weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EGGVy2l5xP", "forum": "UF0uK2oGeW", "replyto": "UF0uK2oGeW", "signatures": ["ICLR.cc/2026/Conference/Submission11609/Reviewer_3eej"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11609/Reviewer_3eej"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549633850, "cdate": 1761549633850, "tmdate": 1762922685899, "mdate": 1762922685899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose OmniQuality-R, a unified framework for reward modeling in image quality assessment. It try to combine multiple aspects like technical quality, aesthetic appeal and text-image alignment into one model. Inspired by how human evaluate images, it use structured reasoning with cot and then apply rl with GRPO. They introduce Gaussian reward instead of binary, and add STD filtering and entropy gating to make training more stable. The model is evaluate on three tasks and claim to improve robustness, explainability and can even guide T2I generation at test time without retraining."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of all-encompassing quality assessment is timely, with rise of AIGC and need for better reward models in generative AI. Current methods are limited to single task, this one integrate them nicely.\n\n- Two-stage training process seem innovative: first cold-start SFT with rejective sampling to build good CoT data, then RL with Gaussian reward for continuous scoring. This could help in other regression tasks beyond IQA.\n\n- Mechanisms like STD-guided filtering and entropy gating address common issue in RL like vanishing advantage and low entropy, which make training unstable. Good to see ablation or discussion on this.\n\n- Evaluation cover diverse tasks: aesthetic (AVA), technical (KonIQ?), alignment. And it show application in guiding T2I models, which is practical.\n\n- Figure 1 and 2 are clear and help understand the framework."}, "weaknesses": {"value": "- Dataset construction rely on rejective sampling, but how to ensure the \"hard\" and \"easy\" sample filtering is fair? Might bias the model.\n\n- It would be better if include more ablation on component like without Gaussian reward or without entropy gating. Also, cross-domain generalization not fully address, e.g., on real-world UGC vs synthetic distortion.\n\n- Writing have some repetition, like \"all-encompassing\" use too much, and some sentence long and hard to follow.\nNot discuss computational cost – MLLM fine-tuning with RL is expensive, how scalable it is?"}, "questions": {"value": "- In Gaussian reward, how to choose sigma? Is it tune per task or fixed and how?\n\n- For test-time guidance in T2I, can you give example of how reward is use – like in sampling or optimization loop?\n\n- Why focus only on three tasks? Could it extend to video quality or 3D?\n\n- In rejection sampling, what MLLM is use to generate plan and reason – is it same base model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NIkimUhfyC", "forum": "UF0uK2oGeW", "replyto": "UF0uK2oGeW", "signatures": ["ICLR.cc/2026/Conference/Submission11609/Reviewer_csT7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11609/Reviewer_csT7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724847438, "cdate": 1761724847438, "tmdate": 1762922685564, "mdate": 1762922685564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniQuality-R, a unified reward model for image quality assessment covering technical quality, aesthetics, and text-image alignment. It uses a structured plan-and-reason approach with chain-of-thought data and a two-stage training process combining supervised fine-tuning and reinforcement learning with a Gaussian reward. Stabilized by entropy gating and standard deviation filtering, OmniQuality-R achieves strong generalization, interpretability, and effectively guides text-to-image generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The advantages of this paper are as follows:\n1.OmniQuality-R separates the planning and reasoning stages, enabling structured and interpretable evaluations across multiple image quality dimensions. \n2.The incorporation of standard deviation (STD) guided filtering and entropy gating effectively improves the Group Relative Policy Optimization (GRPO) by reducing variance and enhancing generalization. \n3.Unlike previous works that use binary rewards, this paper adopts a continuous Gaussian reward, which is practical and effective for score regression tasks. \n4.The proposed method achieves state-of-the-art performance on multiple benchmarks and demonstrates strong out-of-domain generalization capabilities."}, "weaknesses": {"value": "1.The overall training pipeline is largely similar to the previous UnifiedReward-Think approach, and the interpretability and stability of step-by-step Chain-of-Thought (CoT) rewards have already been discussed in UnifiedReward-Think. Moreover, the claim of being \"all-encompassing\" is not well substantiated in this paper. It is recommended that the authors clarify why they chose these particular dimensions and how these choices contribute to Image Quality Assessment\n2.Regarding Test-Time Text-Image (T2I) Optimization, the SANA-1.0-1.6B model used is relatively small and not mainstream. The evaluation focuses only on semantic improvements, and the visualization is limited to positional optimization.  Efficiency in aesthetic and image quality optimization need be explored\n3.The STD filtering and Entropy Gating discard some rollout samples. Poor hyperparameter choices might lead to the need for more rollouts. It is suggested that the authors provide experimental comparisons of GRPO with different numbers of function evaluations (NFE) to illustrate this effect."}, "questions": {"value": "Please refer to the Weaknesses section above for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R02tthHGu6", "forum": "UF0uK2oGeW", "replyto": "UF0uK2oGeW", "signatures": ["ICLR.cc/2026/Conference/Submission11609/Reviewer_PjR5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11609/Reviewer_PjR5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904189345, "cdate": 1761904189345, "tmdate": 1762922684587, "mdate": 1762922684587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniQuality-R, a unified reward model designed for \"all-encompassing\" IQA. The authors identify that current IQA models are typically specialized for a single task (e.g., technical quality, aesthetics, or text-image alignment), lack interpretability, and generalize poorly."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Pros:\n1. The paper has an ambitious goal of unifying three distinct IQA tasks into a single, cohesive framework. This \"all-encompassing\" perspective is a step beyond narrow, single-task models and aligns better with the multifaceted nature of human quality perception.\n2. The \"plan-then-reason\" structure and use of CoT data directly tackle the black-box nature of many IQA models. The model doesn't just output a score, it provides a step-by-step rationale.\n3. The authors evaluate their model on an impressive number of datasets covering all three tasks, including both in-domain and out-of-domain scenarios. The consistent top-tier performance across these benchmarks strongly validates their approach."}, "weaknesses": {"value": "Cons:\n1. The quality of the entire framework hinges on the initial CoT dataset generated by the Qwen2.5-VL-7B \"teacher\" model. The paper does not discuss the potential limitations or biases inherited from this teacher. A flawed teacher could propagate systematic errors or a particular \"style\" of reasoning into OmniQuality-R.\n2. The technical contribution is not very clear since the training components have already existed. The multi-stage pipeline (CoT generation, rejective sampling, SFT, and a customized GRPO training) is highly complex but is not quite novel.\n3. The paper excels at showcasing positive results but would be strengthened by an analysis of its failure modes. Which types of images or prompts does it struggle with? When does its reasoning break down? This would provide a more balanced view of the model's capabilities and limitations.\n4. Aesthetic assessment is inherently subjective and culturally dependent. The paper could benefit from a discussion on this limitation and how the framework might be adapted to handle diverse or personalized aesthetic preferences.\n5. More Dataset Generation details are desired. The \"Plan-then-Reason\" dataset construction is a critical first step. Was there any human verification of the auto-generated CoT trajectories, or was the process fully automated? How sensitive is the final performance to the choice and quality of this initial teacher MLLM?\n6. Could you elaborate on the specific criteria used to classify examples as \"easy\" or \"hard\" for the rejective sampling finetuning? Is this based on prediction error, response length, or another heuristic?\n7. In Table 7, the best-performing \"OmniQuality-R select\" method combines both aesthetic and technical scores. How were these two scores combined? Was it a simple sum, a weighted average, or a more complex function? How was this combination strategy determined?\n8. The analysis plan is generated in Stage 1 but removed for SFT to encourage the model to \"internalize\" planning. Have you explored an alternative where the model explicitly generates a plan first during inference? Could this potentially improve robustness on out-of-distribution or particularly complex assessment tasks?"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gQd1p2XWzg", "forum": "UF0uK2oGeW", "replyto": "UF0uK2oGeW", "signatures": ["ICLR.cc/2026/Conference/Submission11609/Reviewer_FyQ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11609/Reviewer_FyQ2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939952483, "cdate": 1761939952483, "tmdate": 1762922684052, "mdate": 1762922684052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}