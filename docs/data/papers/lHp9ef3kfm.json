{"id": "lHp9ef3kfm", "number": 10028, "cdate": 1758156936479, "mdate": 1763610626718, "content": {"title": "Connecting Independently Trained Modes via Layer-Wise Connectivity", "abstract": "Empirical and theoretical studies have shown that continuous low-loss paths can be constructed between independently trained neural network models. This phenomenon, known as mode connectivity, refers to the existence of such paths between distinct modes‚Äîi.e., well-trained solutions in parameter space. However, existing empirical methods are primarily effective for older and relatively simple architectures such as basic CNNs, VGG, and ResNet, raising concerns about their applicability to modern and structurally diverse models. In this work, we propose a new empirical algorithm for connecting independently trained modes that generalizes beyond traditional architectures and supports a broader range of networks, including MobileNet, ShuffleNet, EfficientNet, RegNet, Deep Layer Aggregation (DLA), and Compact Convolutional Transformers (CCT). In addition to broader applicability, the proposed method yields more consistent connectivity paths across independently trained mode pairs and supports connecting modes obtained with different training hyperparameters.", "tldr": "We propose LLPF algorithm, which leverages the layer-wise mode connectivity to reliably construct mode connectivity across a broader range of model families.", "keywords": ["Neural Network", "Mode Connectivity"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c0ce2f56efa3a2614beff8c560561f82c0121d5.pdf", "supplementary_material": "/attachment/c13d4cee04a5153f2c7af270c49ae07ce719b977.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an algorithm for finding low-loss paths between independently trained models across a wide variety of architectures. The proposed method is based on the empirical observation that the variance of trained model parameters (i.e., the sum of squared parameter values) tends to remain approximately constant. Using this approach, the authors demonstrate that continuous paths with small training losses can be found in various architectures trained on CIFAR-10. They also show that low-loss paths can be identified even between models trained with different hyperparameters, which result in different parameter variances."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper proposes an algorithm that enables verification of mode connectivity across a variety of architectures.  \n* It demonstrates that low-loss paths can be discovered even between models trained with different hyperparameters."}, "weaknesses": {"value": "* It is unclear whether Algorithm 2 can always successfully discover a path between models with different parameter variances.  \n* The experiments rely almost entirely on CIFAR-10.  \n* The method does not necessarily yield paths with low test loss.  \n* Although the paper claims that paths can be found between models trained with different hyperparameters, the actual experiments are restricted to a very limited setting."}, "questions": {"value": "## The mathematical notation could be clarified to avoid confusion.\n\nFor example, $l_x$ is used to denote the $x$-th layer. However, $x$ is also used to represent a hyperparameter, and this dual use may cause confusion. In addition, $x$ commonly represents the input, so using it as a hyperparameter symbol is unconventional.  \n\nFurthermore, the variance of the $l_x$-th layer is written as $\\mathrm{Var}(\\theta_{l_x})$. However, this expression does not refer to the variance of a distribution in the usual statistical sense but rather to the squared L2-norm of the layer parameters. Using the term ‚Äúvariance‚Äù might imply that $\\theta_{l_x}$ is a random variable drawn from a distribution determined by the training algorithm. Since the intent seems to be to compute the variance across the dimensions of $\\theta_{l_x}$ itself, it would be clearer to simply write $\\Vert \\theta_{l_x} \\Vert^2$.\n\n## It is unclear whether Algorithm 2 consistently works as intended.\n\nAs I understand it, Algorithm 2 searches for a path from a model with larger parameter variance to another with smaller variance. It is used to align the variances of two pretrained models so that Algorithm 1 can then find a low-loss path between them. In other words, if Algorithm 2 fails to produce an intermediate model whose variance matches that of the destination model, Algorithm 1 may not work properly.  \n\nAt line 402, the authors write:  \n> Unlike the results in Figure 2, the L_2 distance here is not expected to converge to very small values, because the destination point in Algorithm 2 is the origin, which is not itself a low-loss mode.\n\nThis suggests that Algorithm 2 does not always produce models with small L2 norms. This might be acceptable if we could guarantee that models with small parameter norms necessarily yield large losses. However, that is not always the case. For example, when a convolutional layer is followed by layer normalization, scaling the convolution weights does not change the layer output. Consequently, the variance of such weights could be arbitrarily small without degrading performance. Therefore, if one of the pretrained models happens to have a much smaller parameter norm, the proposed method might fail. Unless Algorithm 2 can **always** find the lowest-variance model with performance comparable to the original, this remains a potential limitation.\n\n## The experiments are biased toward CIFAR-10.\n\nAlthough various architectures are tested, the dataset choice is limited to CIFAR-10. It would be more convincing to include experiments on larger-scale datasets such as CIFAR-100 or ImageNet.\n\n## There is no guarantee that models with low test loss will be found.\n\nAs acknowledged by the authors, the proposed algorithm ensures low training loss but not low test loss. If the purpose of this study is to provide new insight into the generalization properties of SGD, this limitation should be considered a notable weakness.\n\n## It would be helpful to include experiments where hyperparameters other than weight decay are changed.\n\nThe paper claims that low-loss paths can be found between models trained with different hyperparameters. However, in practice, the only hyperparameter varied is the weight decay, which mainly affects the scale of the parameters. It is well known that the sharpness of the loss landscape is also influenced by other hyperparameters such as the learning rate. At line 314, the authors mention:  \n> Empirically, we find that starting from sharp minima can cause our algorithm to fail to find feasible paths.\n\nHowever, the paper does not provide experiments that explicitly examine the effect of sharpness. It would strengthen the work to include experiments where other hyperparameters, such as the learning rate, are varied to demonstrate the broader applicability of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mF15UNghKf", "forum": "lHp9ef3kfm", "replyto": "lHp9ef3kfm", "signatures": ["ICLR.cc/2026/Conference/Submission10028/Reviewer_suKz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10028/Reviewer_suKz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761025470329, "cdate": 1761025470329, "tmdate": 1762921436859, "mdate": 1762921436859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Thank you for the time spent reviewing our paper. We appreciate the reviewers‚Äô comments that would help us further improve the presentation of our work.\n\nWe will address all reviewers' concerns in the revised version of our work. The major changes of our manuscript include: \n\n- Motivation: We have rewritten Paragraphs 1, 2 and 3 in the Introduction to clarify the motivation behind our work.\n\n- Dataset scale: Our evaluation is conducted on CIFAR-scale datasets because connecting modes on ImageNet is computationally prohibitive. The original submission only includes DLA and ResNet results on CIFAR-100; we will additionally conduct experiments for MobileNet, ShuffleNet, RegNet, and EfficientNet on CIFAR-100 in the revision.\n\n- Architecture selection: The model architectures were selected based on their popularity within the PyTorch community. For transformer-based models, we use CCT as a practical alternative to ViT, which does not support CIFAR-scale inputs. We have added an explanation of this selection procedure in the Appendix.\n\n- Results and Evaluation: we will perform all experiments requested by the reviewers and include them in the revised version. For details please check the detailed change list.\n\n- We identified an error in our earlier statement in the ‚ÄúPrerequisite‚Äù section claiming that ‚Äúinput models should lie in flat low-loss regions.‚Äù After conducting additional experiments, we found that Algorithm 1 also works for sharp minima. These new results are included in the Appendix under ‚ÄúConnecting modes located in sharper minima.‚Äù We would like to express our special thanks to Reviewer WDuq and Reviewer suKz for encouraging us to investigate and clarify this point\n\nWe also have some general clarification of related works:\n\n- Linear mode connectivity: we would like to address that ***mode connectivity across independently trained modes*** is different from ***linear mode connectivity (LMC)***. Please check our \"Mode Connectivity in Spawning and Permutation\" Section (Page 4).\n\n- AutoNEB, FGE, and SPRO: We will compare our runtime against AutoNEB in Table 7. We do not include the runtimes of FGE (Fast Geometric Ensembling) or its follow-up method SPRO, because these approaches produce only an ensembled model, i.e., a single point on the mode connection, rather than constructing a full continuous path. We clarify this point in Table 1.\n\n\nThe changes of our manuscript are listed below.\n\nChange list:\n\n(1) Added citation [Juneja 2023] in the section ‚ÄúMode Connectivity in Spawning and Permutation‚Äù as an example of linear mode connectivity in the transfer-learning setting. [Reviewer WGfT]\n\n(2) Add the runtime of AutoNEB to Table 7. [Reviewer WGfT]. \n\nThe runtime of FGE and SPRO is not included because these methods output only a single point, as they are model-ensembling approaches rather than full path-finding algorithms. As stated in the FGE paper [1], bottom of page 6: \"While inspired by mode connectivity, FGE does not rely on explicitly finding a connecting curve, and thus does not require pre-trained endpoints, and so can be trained in the time required to train a single network.\" SPRO builds upon FGE to reason about mode-connecting volumes and accelerate ensembling, and therefore inherits this property.\nWe have added a sentence in Table 1 to make this clear to the reader.\n\n(3) Revised the paragraph 1,2 and 3 in the Introduction to better explain the motivations behind our work. [Reviewer WGfT, Reviewer 9MCz]\n\n(4) Added a clarifying sentence to the Figure 2 caption explaining the patterns observed in both the training-loss and L2-distance panels. [Reviewer WGfT]\n\n(5) Added a paragraph in Appendix A.9 detailing how we select model architectures. [Reviewer WGfT]\n\n(6) Added a sentence in the ‚ÄúPrerequisite‚Äù section (Line 309) providing theoretical support for our claim that minima with near-zero loss are typically flatter. [Reviewer WDuq]\n\n(7) Added a new Appendix section \"Connecting modes located in sharper minima\", evaluating whether Algorithm 1 succeeds on sharp minima with near-zero loss. (in progress) [Reviewer WDuq, Reviewer suKz]\n\n(8) Will conduct additional experiments on MobileNet, ShuffleNet, EfficientNet, and RegNet using CIFAR-100. (in progress) [Reviewer suKz]\n\n(9) The symbol for training hyperparameters are replaced with Greek letter ùúâ. [Reviewer suKz]\n\n(10) Conduct experiments of \"Connecting Modes on Different Variance Sphere\" with different learning rate and weight decay. [Reviewer suKz]\n\n(11) Add a section in Appendix \"Empirical Investigation of the variance sphere range supported by Algorithm 2\" to show that Algorithm 2 could cover most of the modes obtained by SGD. [Reviewer suKz]\n\n\n[1] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, & Andrew Gordon Wilson. (2018). Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs."}}, "id": "nRUgXqxMbJ", "forum": "lHp9ef3kfm", "replyto": "lHp9ef3kfm", "signatures": ["ICLR.cc/2026/Conference/Submission10028/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10028/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10028/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763643244542, "cdate": 1763643244542, "tmdate": 1763644870305, "mdate": 1763644870305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Thank you for the time spent reviewing our paper. We appreciate the reviewers‚Äô comments that would help us further improve the presentation of our work.\n\nWe will address all reviewers' concerns in the revised version of our work. The major changes of our manuscript include: \n\n- Motivation: We have rewritten Paragraphs 1, 2 and 3 in the Introduction to clarify the motivation behind our work.\n\n- Dataset scale: Our evaluation is conducted on CIFAR-scale datasets because connecting modes on ImageNet is computationally prohibitive. The original submission only includes DLA and ResNet results on CIFAR-100; we will additionally conduct experiments for MobileNet, ShuffleNet, RegNet, and EfficientNet on CIFAR-100 in the revision.\n\n- Architecture selection: The model architectures were selected based on their popularity within the PyTorch community. For transformer-based models, we use CCT as a practical alternative to ViT, which does not support CIFAR-scale inputs. We have added an explanation of this selection procedure in the Appendix.\n\n- Results and Evaluation: we will perform all experiments requested by the reviewers and include them in the revised version. For details please check the detailed change list.\n\n- We identified an error in our earlier statement in the ‚ÄúPrerequisite‚Äù section claiming that ‚Äúinput models should lie in flat low-loss regions.‚Äù After conducting additional experiments, we found that Algorithm 1 also works for sharp minima. These new results are included in the Appendix under ‚ÄúConnecting modes located in sharper minima.‚Äù and we have revised the ‚ÄúPrerequisite‚Äù section to remove the flat-loss requirement. We would like to express our special thanks to Reviewer WDuq and Reviewer suKz for encouraging us to investigate and clarify this point.\n\nWe also have some general clarification of related works:\n\n- Linear mode connectivity: we would like to address that ***mode connectivity across independently trained modes*** is different from ***linear mode connectivity (LMC)***. Please check our \"Mode Connectivity in Spawning and Permutation\" Section (Page 4).\n\n- AutoNEB, FGE, and SPRO: We will compare our runtime against AutoNEB in Table 7. We do not include the runtimes of FGE (Fast Geometric Ensembling) or its follow-up method SPRO, because these approaches produce only an ensembled model, i.e., a single point on the mode connection, rather than constructing a full continuous path. We clarify this point in Table 1.\n\n\nThe changes of our manuscript are listed below.\n\nChange list:\n\n(1) Added citation [Juneja 2023] in the section ‚ÄúMode Connectivity in Spawning and Permutation‚Äù as an example of linear mode connectivity in the transfer-learning setting. [Reviewer WGfT]\n\n(2) Add the runtime of AutoNEB to Table 7. [Reviewer WGfT]. \n\nThe runtime of FGE and SPRO is not included because these methods output only a single point, as they are model-ensembling approaches rather than full path-finding algorithms. As stated in the FGE paper [1], bottom of page 6: \"While inspired by mode connectivity, FGE does not rely on explicitly finding a connecting curve, and thus does not require pre-trained endpoints, and so can be trained in the time required to train a single network.\" SPRO builds upon FGE to reason about mode-connecting volumes and accelerate ensembling, and therefore inherits this property.\nWe have added a sentence in Table 1 to make this clear to the reader.\n\n(3) Revised the paragraph 1,2 and 3 in the Introduction to better explain the motivations behind our work. [Reviewer WGfT, Reviewer 9MCz]\n\n(4) Added a clarifying sentence to the Figure 2 caption explaining the patterns observed in both the training-loss and L2-distance panels. [Reviewer WGfT]\n\n(5) Added a paragraph in Appendix A.9 detailing how we select model architectures. [Reviewer WGfT]\n\n(6) Added a sentence in the ‚ÄúPrerequisite‚Äù section (Line 309) providing theoretical support for our claim that minima with near-zero loss are typically flatter. [Reviewer WDuq]\n\n(7) Added a new Appendix section \"Connecting modes located in sharper minima\", evaluating whether Algorithm 1 succeeds on sharp minima with near-zero loss. (in progress, will implement ***relative flatness measure*** soon) [Reviewer WDuq, Reviewer suKz]\n\n(8) Will conduct additional experiments on MobileNet, ShuffleNet, EfficientNet, and RegNet using CIFAR-100. (in progress) [Reviewer suKz]\n\n(9) The symbol for training hyperparameters are replaced with Greek letter ùúâ. [Reviewer suKz]\n\n(10) Conduct experiments of \"Connecting Modes on Different Variance Sphere\" with different learning rate and weight decay. [Reviewer suKz]\n\n(11) Add a section in Appendix \"Empirical Investigation of the variance sphere range supported by Algorithm 2\" to show that Algorithm 2 could cover most of the modes obtained by SGD. [Reviewer suKz]\n\n\n[1] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, & Andrew Gordon Wilson. (2018). Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs."}}, "id": "nRUgXqxMbJ", "forum": "lHp9ef3kfm", "replyto": "lHp9ef3kfm", "signatures": ["ICLR.cc/2026/Conference/Submission10028/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10028/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10028/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763643244542, "cdate": 1763643244542, "tmdate": 1763657246484, "mdate": 1763657246484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LLPF, a new algorithm that implements mode connectivity, i.e. finding a low-loss path between two modes obtained by independent training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The presentation is clear and the paper is easy to follow.\n- The experiment results verify the effectiveness of the proposed algorithm."}, "weaknesses": {"value": "- I don't see why this problem is important. In my understanding, mode connectivity is more like a phenomenon that helps us better understand the loss landspace, instead of a challenge that needs to be solved by an algortihm. Perhpas the authors can describe more of the practical usefulness of their algorithm?\n- The algorithm looks trivial, and is not explained at all. For example, why do you need to project back to the variance sphere at each step? How do you ensure there is no loss barrier between the point and the projected point (e.g. M1 and M2, in the context of Figure 1)? how do you guarantee M3 is not too far away from M2?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vsjo5vgeHh", "forum": "lHp9ef3kfm", "replyto": "lHp9ef3kfm", "signatures": ["ICLR.cc/2026/Conference/Submission10028/Reviewer_9MCz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10028/Reviewer_9MCz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608294779, "cdate": 1761608294779, "tmdate": 1762921436538, "mdate": 1762921436538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel method to find connecting low-loss paths between models by a layer-wise search. For each layer, the method explores the sphere of layer-parameters with similar variance. The method is sensitive to the order in which layers are traversed, arguing that the order should follow the data-flow. The method is applicable to modern architectures where standard linear or Bezier interpolation approaches fail."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method to find low-loss paths is novel and interesting.\n- The proposed method is tested for recent model architectures.\n- The notion of a _variance sphere_ and the layer-wise variance correction step are intuitive and sound.\n- The empirical evaluation shows the method works for a wide variety of model architectures and results are consistent across seeds.\n- Algorithms are clearly presented and linked to geometric reasoning.\n- Consistent paths across seeds and architectures suggest a shared geometric structure in modern networks, potentially revealing deeper properties of the loss landscape."}, "weaknesses": {"value": "- The experiments are _somewhat_ limited to visualizations of loss/accuracy trajectories; no quantitative comparison of path quality (e.g., path length, interpolation efficiency, energy landscape visualization).\n- The effect of layer order, variance correction, and training steps is not systematically analyzed in an ablation study.\n- The approach is iterative and layer-wise. The paper acknowledges high compute requirements but gives no runtime analysis."}, "questions": {"value": "- You state that ‚Äúminima with near-zero training loss are generally flat.‚Äù Could you quantify this or connect it to measurable flatness metrics (e.g., relative flatness [3], Fisher-Rao Norm [2])?\n- The observation that low training loss implies flatness can be explained for the CE loss: Walter et al. [4] have shown that flatness relates to model confidence and model confidence increase wit ha decreasing CE loss.\n- Have you checked whether sharp minima with near-zero loss actually fail your algorithm? That would support your empirical claim.\n- Since the paper‚Äôs method depends on starting from flat minima, and relative flatness [3] formalizes layer-wise curvature-weight norms, there seems to be a conceptual overlap. Could the variance-sphere assumption be seen as implicitly fixing relative flatness per layer?\n- Adilova et al. [1] found that certain layers, or layer clusters, have a stronger impact on the loss barrier. How would ordering the layers according to their _importance_, i.e., influence on the loss barrier, impact your method? \n- Adilova et al. [1] also found that different directions in parameter space have different impacts on the loss value. It would be interesting to see whether the layer-wise updates of the proposed methods correspond to the space perpendicular to the training space and averaging direction.\n\nReferences: \n\n[1] Adilova, Linara, et al. \"FAM: Relative Flatness Aware Minimization.\" Topological, Algebraic and Geometric Learning Workshops 2023. PMLR, 2023.\n\n[2] Liang, Tengyuan, et al. \"Fisher-rao metric, geometry, and complexity of neural networks.\" The 22nd international conference on artificial intelligence and statistics. PMLR, 2019.\n\n[3] Petzka, Henning, et al. \"Relative flatness and generalization.\" Advances in neural information processing systems 34 (2021): 18420-18432.\n\n[4] Walter, Nils Philipp, et al. \"When Flatness Does (Not) Guarantee Adversarial Robustness.\" arXiv preprint arXiv:2510.14231 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OkyPr6rni9", "forum": "lHp9ef3kfm", "replyto": "lHp9ef3kfm", "signatures": ["ICLR.cc/2026/Conference/Submission10028/Reviewer_WDuq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10028/Reviewer_WDuq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907970316, "cdate": 1761907970316, "tmdate": 1762921436177, "mdate": 1762921436177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new algorithm (Low Loss Path Finding) to mode connect two independently (from different initializations) trained (with different data ordering and augmentations) networks. The paper tests the algorithm on architectures that are not common in this line of work (MobileNet, ShuffleNet, etc), claiming that they are more recent. The authors claim that their method is more reliable‚Äîproducing consistent paths for networks trained with different hyper-parameters---though they don‚Äôt actually cite or confirm if the prior work fail on these settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The authors revisit non-linear mode connectivity to a broader set of architectures beyond the commonly studied ResNet/VGG models.\n- I believe the discussion of the models trained with different hyper-parameters and the focus on different variances is an important point that is not addressed in the prior literature. The algorithm and the discussion of data flow order is sound."}, "weaknesses": {"value": "- The paper does not provide clear motivation for why demonstrating mode connectivity in MobileNet, ShuffleNet, or the other selected architectures is important or beneficial. What practical or theoretical insights would we gain from showing mode connectivity in these specific models? The choice of architectures appears arbitrary and is not justified. Moreover, the proposed architectures (MobileNet, ShuffleNet, EfficientNet, RegNet) are roughly contemporary with the mode connectivity literature itself, contradicting the claim of addressing \"modern\" architectures. Addressing different model families like transformers, diffusion models could have been a fresher take on this topic.\n- The architectures in the (linear) mode connectivity literature are indeed outdated. The paper should cite Juneja 2022, and Altƒ±nta≈ü 2025 for discussion of transformer-based models and NLP domain.\n- Even though they claim to be more effective, the paper doesn‚Äôt compare the proposed method against the baseline methods (NEB, FGE, and SRPO) on the architectures that are evaluated in the prior work and the architectures that the paper expands in.\n- The experimental setup appears to be inherited from prior work without significant extension or deepening. Even all three baselines (Garipov, Draxler, Benton) conducted experiments at the CIFAR-100 scale. The paper does not appear to push beyond the established experimental boundaries in terms of dataset scale, complexity, or diversity of evaluation metrics\n- The paper claims the method \"generalizes beyond traditional architectures\" and supports a \"broader range of networks,\" but the selected architectures are not particularly modern and the experimental validation does not convincingly demonstrate this generalization beyond what prior methods might achieve.\n- The benefits of the algorithm is not validated. According to Table 6, algorithm 1 takes 21 hours on an H100 GPU for a ResNet18 on Cifar-10. If I am not misreading this table, the cost of the algorithm is quite high. Training a ResNet18 or 20 on Cifar-10 shouldn‚Äôt take more than 10 minutes."}, "questions": {"value": "- Could the authors explain the different phases in L2 distance in Figure 2? Is there any pattern emerging? I am also curious if the authors have an insight about behavior in the top subfigure and its implications for the local geometry?\n- Why were these specific architectures chosen? What are the practical implications of achieving mode connectivity for these specific architectures?\n- Can the authors provide experimental results comparing their method to NEB, FGE, and SRPO on the same set of architectures? How does the proposed method's computational cost compare to these baselines? Table 6 could incorporate the runtime for these baseline methods. \n- The authors could draw connection between the muP regime and their algorithm to provide insights. The paper could also benefit from discussing scale invariances."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XM52NXZCdW", "forum": "lHp9ef3kfm", "replyto": "lHp9ef3kfm", "signatures": ["ICLR.cc/2026/Conference/Submission10028/Reviewer_WGfT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10028/Reviewer_WGfT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956997567, "cdate": 1761956997567, "tmdate": 1762921435784, "mdate": 1762921435784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}