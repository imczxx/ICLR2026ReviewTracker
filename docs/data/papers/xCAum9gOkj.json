{"id": "xCAum9gOkj", "number": 1648, "cdate": 1756900895674, "mdate": 1762941702120, "content": {"title": "MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training", "abstract": "Vision Language Action (VLA) models derive their generalization capability from diverse training data, yet collecting embodied robot interaction data remains prohibitively expensive. In contrast, human demonstration videos are far more scalable and cost-efficient to collect, and recent studies confirm their effectiveness in training VLA models. However, a significant domain gap persists between human videos and robot-executed videos, including unstable camera viewpoints, visual discrepancies between human hands and robotic arms, and differences in motion dynamics. To bridge this gap, we propose MimicDreamer, a framework that turns fast, low-cost human demonstrations into robot-usable supervision by jointly aligning vision, viewpoint, and actions to directly support policy training. For visual alignment, we propose H2R Aligner, a video diffusion model that generates high-fidelity robot demonstration videos by transferring motion from human manipulation footage. For viewpoint stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos via homography and inpaints occlusions and distortions caused by warping. For action alignment, we map human hand trajectories to the robot frame and apply a constrained inverse kinematics solver to produce feasible, low-jitter joint commands with accurate pose tracking. Empirically, VLA models trained purely on our synthesized human-to-robot videos achieve few-shot execution on real robots. Moreover, scaling training with human data significantly boosts performance compared to models trained solely on real robot data; our approach improves the average success rate by 14.7\\% across six representative manipulation tasks.", "tldr": "", "keywords": ["Robotics; Embodied AI; Vision-Language-Action Models"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/de11ceea59ac7d006e767ff330f8ab6d12bea124.pdf", "supplementary_material": "/attachment/a8fc329eaeb92b2a76dd2b5bc09e04168228feba.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of translating human demonstration videos into robot training data. The approach involves (1) using retargeting to map human hand poses to robot joint configurations via inverse kinematics (IK), and (2) training a diffusion transformer to synthesize robot observations by conditioning on real background images after removing the human hand and robot appearance rendered using simulation. The resulting policy is co-trained on a mixed dataset comprising both synthetic and real robot data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of converting human demonstration videos into robot training data is intriguing and has the potential to mitigate the high cost and effort associated with collecting in-context robot data. \n\n\nThe proposed method is also evaluated on real robot hardware, and the reported performance appears strong based on the presented results."}, "weaknesses": {"value": "Given the existence of prior works that have already explored retargeting and inpainting for similar objectives [1, 2], the technical contribution of this paper appears relatively incremental. \n\nWhile one of the main contributions is claimed to lie in the visual generation component, the experiments provided do not adequately evaluate this aspect. \n\nMoreover, the comparative analysis is limited --- more relevant baselines should be included to assess both the quality of the generated visual content and the resulting downstream policy performance. \n\n\n[1] Phantom: Training Robots Without Robots Using Only Human Videos, CoRL’25; \n\n[2] Masquerade: Learning from In-the-wild Human Videos using Data-Editing, arXiv’25;"}, "questions": {"value": "Have the authors performed quantitative evaluations of image realism, such as computing distances in feature space? Furthermore, is there any observed correlation between the realism of the generated images and the downstream policy performance? \n\nHave the authors evaluated the method’s ability to generalize across domains by performing zero-shot transfer, i.e., training the policy entirely with synthetic data and testing it directly in real-world environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kb7ynkVKzB", "forum": "xCAum9gOkj", "replyto": "xCAum9gOkj", "signatures": ["ICLR.cc/2026/Conference/Submission1648/Reviewer_mmNg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1648/Reviewer_mmNg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945058939, "cdate": 1761945058939, "tmdate": 1762915839680, "mdate": 1762915839680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Nlx2oqrgcO", "forum": "xCAum9gOkj", "replyto": "xCAum9gOkj", "signatures": ["ICLR.cc/2026/Conference/Submission1648/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1648/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762940936067, "cdate": 1762940936067, "tmdate": 1762940936067, "mdate": 1762940936067, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MimicDreamer, a pipeline to turn cheap human egocentric demos into robot-usable training signals for VLA models by aligning (i) viewpoint (i.e., the EGOSTABILIZER module), (ii) actions (IK-based method to transform the human wrist poses to robot EE poses), and (iii) vision (H2R-ALIGNER diffusion model guided by sim foreground + real background scene videos). On six manipulation tasks, models trained with synthesized human-to-robot data match or outperform robot-only training.."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The strengths of this paper is:\n\n- Clear writing & coherent system design. The paper is well-structured with a logical flow from problem setup to method and experiments. Each module’s purpose is motivated and the overall pipeline is easy to follow. Figures meaningfully support the text. \n\n- Real-world experiments are convincing. The evaluation includes on-robot tasks with clear metrics, showing consistent gains over baselines."}, "weaknesses": {"value": "Main concern: limited methodological novelty\n\nWhile the system is thoughtfully engineered, most contributions appear to be system integration of known components rather than new algorithms. Below I outline where prior art already covers similar ideas and where clarifications/ablations are needed.\n\n- Viewpoint Stabilization\n\n  - The egocentric video stabilization pipeline relies on standard video-processing techniques; the value is primarily engineering and integration.\n\n  - The inpainting step depends on high-quality binary masks. Please discuss scalability across diverse scenes/objects and report failure cases (e.g., thin tools, fast motion) and the annotation cost or automation rate of mask generation.\n\n- Action Alignment\n\n    - Using MANO-driven retargeting for arm/hand control is established; see [1-4]:\n\n    - The paper doesn’t situate its retargeting in this literature; please add a related-work comparison and quantify what is new (e.g., constraints, smoothing, calibration, or robustness claims).\n\n    - Most existing work note residual action gaps after retargeting, which is a reason they introduce robot data for co-training [4], or online/offline RL finetuing[2, 3]. Though this work focuses on VLA fine-tuning, how severe such action gap is?\n\n[1]. Qin, Yuzhe, et al. \"Anyteleop: A general vision-based dexterous robot arm-hand teleoperation system.\" arXiv preprint arXiv:2307.04577 (2023).\n\n[2]. Shaw, Kenneth, Shikhar Bahl, and Deepak Pathak. \"Videodex: Learning dexterity from internet videos.\" Conference on Robot Learning. PMLR, 2023.\n\n[3]. Chen, Zoey Qiuyu, et al. \"Dextransfer: Real world multi-fingered dexterous grasping with minimal human demonstrations.\" arXiv preprint arXiv:2209.14284 (2022).\n\n[4]. Liu, Yangcen, et al. \"Immimic: Cross-domain imitation from human videos via mapping and interpolation.\" arXiv preprint arXiv:2509.10952 (2025).\n\n- Visual Alignment: key questions and missing comparisons:\n   - Why a diffusion generator? Prior work shows that rendering robot embodiments via direct MANO→robot pose estimation + image inpainting can suffice for single and bimanual tasks; see [5, 6]. Other simple strategies—e.g., inpainting guidance lines or masking both human/robot hands—also reduce visual mismatch [7, 8]. Please include baselines against these simpler, cheaper methods to justify the added complexity.\n\n   - Why condition on sim renders? If the model already sees the real manipulation video (rich scene cues) and the retargeted robot actions, what additional signal do sim-rendered frames provide—geometry? lighting? pose disambiguation? An ablation with/without sim-conditioning (and with IK-replay only) would clarify its contribution. If the goal is merely to convey the robot’s appearance, then straightforward compositing/inpainting—as in [5,6]—should suffice.\n\n[5]. Lepert, Marion, Jiaying Fang, and Jeannette Bohg. \"Phantom: Training robots without robots using only human videos.\" arXiv preprint arXiv:2503.00779 (2025).\n\n[6]. Lepert, Marion, Jiaying Fang, and Jeannette Bohg. \"Masquerade: Learning from in-the-wild human videos using data-editing.\" arXiv preprint arXiv:2508.09976 (2025).\n\n[7]. Kareer, Simar, et al. \"Egomimic: Scaling imitation learning via egocentric video.\" 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2025.\n\n[8]. Bahl, Shikhar, Abhinav Gupta, and Deepak Pathak. \"Human-to-robot imitation in the wild.\" arXiv preprint arXiv:2207.09450 (2022).\n\nTaking these points together, the work seems better suited to a robotics venue than a learning-focused conference like ICLR."}, "questions": {"value": "- The main question is about the framework novelty instead of integration, see the Weaknesses,\n\n- The minor question: Could lightweight test-time prompting or adapters on off-the-shelf VLA models yield similar gains (e.g., style/camera prompts, action-space remapping), as in [1]?\n\n[1]. Zheng, Ruijie, et al. \"Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies.\" arXiv preprint arXiv:2412.10345 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jeztCIixh9", "forum": "xCAum9gOkj", "replyto": "xCAum9gOkj", "signatures": ["ICLR.cc/2026/Conference/Submission1648/Reviewer_gZL6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1648/Reviewer_gZL6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963563992, "cdate": 1761963563992, "tmdate": 1762915839333, "mdate": 1762915839333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework to directly convert human/egocentric manipulation videos to robot demonstration data. The task is disentangled into two problems. First, for action alignment, the hand trajectory is converted to robot actions with IK solver. Then, this action is utilized to simulate embodiment-only sequences and is sent to a video diffusion model, as well as ego videos processed by stabilization, to produce robot manipulation videos aligning with the egocentric video's environment. The generated videos are used to perform VLA training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-organized and presented.  \n2. The problem that the paper is trying to solve is meaningful for scaling up robot data. \n3. Distengling action and visual alignment makes sense."}, "weaknesses": {"value": "1. The major concern lies in the robustness/accuracy of the proposed method, and many experiments are missing regarding this issue:  \n(1) First, the sim robot video is the main input condition of the H2R aligner, so how does the quality of the sim robot video impact the final generation result? What if it is not well-aligned with stable ego videos?  \n(2) Furthermore, there are many conditions to decide the quality of the sim robot videos, including camera in/ex-trinsics, 3D hand trajectory, and accuracy of IK solver. So what is the actual performance of the hand trajectory extraction, as well as the accuracy of the IK?  More importantly, how does the performance of this impact the accuracy/quality of the derived robot action and its corresponding sim-rendered robot-only videos?  What is the upper bound, for example, to what extent of this factor is wrongly provided, and the system will fail?   \n(3) What if the real-world egocentric videos have a large gap (camera view, manipulation/grasping points) with the training data?   \nTo summarize, although the reviewer understands that the derived action does not need to be perfect/precise, it is still unknown about the robustness of the system, which is very important for future work. \n2. Some related works [1, 2, 3] should be mentioned. \n\n*Minor*:   \nIn the proposed pipeline, the action alignment and ego stabilization are performed first, then they are sent to the H2R aligner. However, the abstract does not clearly show this sequence, which is recommended to be revised.\n\n**Refs:**        \n[1] Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation. CORL 2025.  \n[2] TASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation. CVPR 2025.  \n[3] Towards Generalizable Zero-Shot Manipulation via Translating Human Interaction Plans. ICRA 2024."}, "questions": {"value": "If the first question of the weaknesses is well-discussed, the reviewer may consider raising the score; otherwise, it will be decreased."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VOyrI9ZpzU", "forum": "xCAum9gOkj", "replyto": "xCAum9gOkj", "signatures": ["ICLR.cc/2026/Conference/Submission1648/Reviewer_PTdV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1648/Reviewer_PTdV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981474864, "cdate": 1761981474864, "tmdate": 1762915839191, "mdate": 1762915839191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}