{"id": "76OZBE4Rb6", "number": 1684, "cdate": 1756905166557, "mdate": 1763752973600, "content": {"title": "Sample Lottery: Unsupervised Discovery of Critical Instances for LLM Reasoning", "abstract": "Reinforcement learning has equipped large language models (LLMs) with the capability of reasoning over complicated logical problems through policy optimization. However, conventional methods require complete annotation of the entire dataset and allocate computation uniformly over all samples. We articulate the \\textit{lottery sample hypothesis} in policy optimization of LLMs: a large training set contains a small subset that, when trained alone, yields performance comparable to that of the full dataset. This paper therefore explores the following question: How can we identify these lottery-winning samples from the original dataset without access to answers? Unlike prior efforts that analyze the effect of different samples in the training set with complete annotation, this paper focuses on the unsupervised discovery of critical instances for LLM reasoning and proposes a novel framework termed Complementary Conformal Selection (CONST). Specifically, CONST evaluates the importance of samples by considering two complementary components: procedural volatility and outcome volatility. Procedural volatility measures the potential variations during the LLM’s reasoning process, while outcome volatility captures inconsistencies in the final answer. Subsequently, conformal prediction is used to obtain a prediction set whose cardinality serves as the criterion for selecting the lottery-winning samples for annotation. We also provide a theoretical analysis, showing that CONST can effectively approximate the optimal policy. Extensive experiments on various LLMs across different datasets demonstrate the effectiveness of CONST.", "tldr": "", "keywords": ["Large Language Model", "Reinforcement Learning with Verifiable Reward"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e283800117fa5b3944c98df6b6c17dd79bb12358.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on identifying data subsets that achieve performance comparable to using the full dataset. The proposed method, CONST, leverages procedural and outcome volatility to construct conformal prediction sets, enabling unsupervised discovery of critical instances. Experimental results demonstrate that CONST is both efficient and effective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem addressed is important and compelling. Identifying significant samples within datasets can reduce computation, conserve resources, and clarify how data-collection choices influence training. \n\n- Notably, the method requires minimal human annotation, which enhances its generalizability and practical applicability."}, "weaknesses": {"value": "The paper would benefit from clarifying its scope earlier. My understanding is that the method is developed and evaluated specifically for the RLVR setting. To help readers set the right expectations, I suggest explicitly stating this focus in the title and/or abstract. Doing so would also avoid any impression that the approach covers broader RL or non-RL problems.\n\nI also have several additional questions/suggestions, which I listed in the Question part. I did not examine the theoretical part in depth and just provisionally assume its correctness; I’m happy to discuss if other reviewers raise concerns."}, "questions": {"value": "How should  n_p be selected in practice? For each truncated CoT, how many final answers are sampled? Only one or multiple ones?\n\nDoes outcome volatility functionally overlap with procedural volatility? In other words, is outcome volatility a special case of procedural volatility? \n\nWhat are instances? Is that data samples? Like 4 instances means 4 questions in the math dataset.\n\n\nCould you also report the experimental results where you (i) generate multiple answers per test question, (ii) filter them by the scoring function and the threshold (from Conformal Prediction) to form a candidate subset, and (iii) measure the pass rate of the final answer set? A comparison against self-consistency and entropy-only baselines would be helpful for understanding the motivation for using the calibration set.\n\nIn addition, using a calibration set incurs a cost as well: you use m = 1024 instances (vs. 4 or 8 for training). Are these 1024 instances annotated? Finally, the choice of  m likely matters—why fix m=1024? A sensitivity analysis over m would be informative.\n\nAssumptions are central to the validity of your claims; please state them explicitly and tie each result to the assumption it requires: permutation tests and exchangeability. Also, please discuss the practical applicability. Why can we use the MMLU subset as a calibration set for the BM set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RKhSZeWgXf", "forum": "76OZBE4Rb6", "replyto": "76OZBE4Rb6", "signatures": ["ICLR.cc/2026/Conference/Submission1684/Reviewer_fBZw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1684/Reviewer_fBZw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910919823, "cdate": 1761910919823, "tmdate": 1762915855782, "mdate": 1762915855782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether a very small subset of training problems can drive reinforcement-learning-with-verifiable-reward (RLVR) fine-tuning of large language models to the same accuracy obtained from the fully-annotated corpus. It fuses procedural volatility (how unstable an answer is to reasoning truncation) and outcome volatility (how inconsistent full answers are across rollouts) into a single sample-utility score using conformal prediction. This mechanism is intuitive, computationally light compared to full uncertainty modeling, and agnostic to the base model or reward function—making it broadly applicable."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. If validated at larger scale, CONST could substantially reduce reward annotation costs in reasoning RL pipelines, which is an increasingly important direction for sustainable LLM alignment.\n\n2. Novel formulation: Using conformal prediction to combine procedural and outcome uncertainty for sample scoring is an elegant, model-agnostic idea.\n\n3. The figures and algorithms are cleanly presented; the method’s intuition is easy to follow even for non-specialists."}, "weaknesses": {"value": "1. Similar ideas have appeared under names like self-consistency filtering, or uncertainty-guided selection. CONST’s originality mainly comes from framing these within conformal prediction and the RLVR objective.\n\n2. The “ε-approximate lottery-sample” assumption is interesting but unverifiable. The authors don’t measure ε or show gradient proximity, so the bound doesn’t really illuminate why CONST works.\n\n3. Procedural and outcome volatility both depend on stochastic decoding variance. Without separating linguistic noise from reasoning uncertainty, the method could mis-rank samples for harder domains.\n\n4. Compared mainly to classification-style active learning. RL- or reasoning-specific selection strategies (e.g., self-consistency filtering, entropy-weighted sampling, or value-driven selection) are absent."}, "questions": {"value": "1. Why not incorporate log P(Y | X) from π₀ into fπ₀(X,Y)? Did likelihood-based scoring perform worse?\n\n2. Which embedding and K were used? How sensitive is CONST to these settings? Provide ablations with identical budgets but varying clustering to isolate diversity effects.\n\n3. Could CONST operate in an active loop where the fine-tuned policy re-selects new samples? Any preliminary results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GLEQC0gPVe", "forum": "76OZBE4Rb6", "replyto": "76OZBE4Rb6", "signatures": ["ICLR.cc/2026/Conference/Submission1684/Reviewer_d26V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1684/Reviewer_d26V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977635954, "cdate": 1761977635954, "tmdate": 1762915855554, "mdate": 1762915855554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CONST, a framework for identifying lottery-winning samples that are most critical for RLVR in LLMs. Instead of requiring full annotations for all training data, CONST selects a very small subset of informative instances without access to ground-truth answers. It combines two complementary measures: procedural volatility (instability of reasoning paths) and outcome volatility (variability in final answers), and leverages conformal prediction to quantify uncertainty through the size of prediction sets. Samples with higher uncertainty are selected for annotation and used for RLVR optimization. Theoretical analysis under the lottery sample hypothesis shows CONST can approximate the optimal policy, and experiments on several mathematical reasoning benchmarks demonstrate that CONST achieves near full-dataset performance with less than 0.5% of annotated samples, outperforming existing active learning baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The method is valuable because it helps us understand which samples truly improve model performance and which do not, providing insight into data efficiency for RL-based reasoning.\n\n2. The experiments are comprehensive and convincing, covering multiple models, datasets, and ablation settings to clearly show the method’s effectiveness.\n\n3. The paper is easy to follow and well organized, with a clear narrative from motivation to theory and experiments, making complex ideas accessible."}, "weaknesses": {"value": "1. I want to know whether CONST is suitable for logic reasoning datasets with discrete or small answer spaces such as multiple-choice tasks with only four options, where outcome volatility may be artificially low and conformal prediction less informative.\n\n2. Theoretical results justify that an optimal subset can approximate full-data training, but the analysis stops short of proving that CONST reliably finds such a subset, the connection between the proposed selection criterion and the theoretical gradient proximity assumption remains heuristic.\n\n3. Is the method sensitive to the number of instances in the calibration dataset?\n\n4. The ablation (V2) skips the clustering step in Algorithm 1, but the paper never explains how clustering is done—what features or metrics are used and how the number of clusters is chosen."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ePji5E9wuJ", "forum": "76OZBE4Rb6", "replyto": "76OZBE4Rb6", "signatures": ["ICLR.cc/2026/Conference/Submission1684/Reviewer_H6dA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1684/Reviewer_H6dA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988706750, "cdate": 1761988706750, "tmdate": 1762915855333, "mdate": 1762915855333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a new data subset selection method called CONST. CONST identifies the critical instances from training data by measuring their procedural volatility (variations in the reasoning chain) and outcome volatility (inconsistencies in the final answer), combining these metrics using conformal prediction to determine sample importance. Experimental results demonstrate that training on the small subset selected by CONST achieves comparable performance to full-dataset training, showcasing its effectiveness for data-efficient policy optimization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The problem is important and interesting. It is good that the proposed method requires minimal annotation from experts and also does not use any LLM annotations.\n\n- The authors analyse their algorithm theoretically.\n\n- The authors show that using a small number of examples gives almost as good a performance as the whole dataset."}, "weaknesses": {"value": "- The main drawback is that the authors do not discuss the whole of data subset selection and valuation literature. Example papers include:\n\nPaul, Mansheej, Surya Ganguli, and Gintare Karolina Dziugaite. \"Deep learning on a data diet: Finding important examples early in training.\" Advances in neural information processing systems 34 (2021): 20596-20607.\n\nGuo, Chengcheng, Bo Zhao, and Yanbing Bai. \"Deepcore: A comprehensive library for coreset selection in deep learning.\" In International Conference on Database and Expert Systems Applications, pp. 181-195. Cham: Springer International Publishing, 2022.\n\nDas, Soumi, Manasvi Sagarkar, Suparna Bhattacharya, and Sourangshu Bhattacharya. \"CheckSelect: Online Checkpoint Selection for Flexible, Accurate, Robust, and Efficient Data Valuation.\" IEEE Transactions on Artificial Intelligence (2024).\n\n- I am not sure about the validity of assumption 3. The authors cite another paper, calling it a standard assumption. However, in my opinion, this assumption is very strong and is often invalid. Also, I am not sure if the proof is novel under this assumption.\n\n- The authors did not show results on bigger reasoning models."}, "questions": {"value": "Why are the authors not reporting results on qwen 2.5 7 b or qwen 3 8b ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pzcTGAZ4k2", "forum": "76OZBE4Rb6", "replyto": "76OZBE4Rb6", "signatures": ["ICLR.cc/2026/Conference/Submission1684/Reviewer_rXws"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1684/Reviewer_rXws"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762490142533, "cdate": 1762490142533, "tmdate": 1762915855091, "mdate": 1762915855091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}