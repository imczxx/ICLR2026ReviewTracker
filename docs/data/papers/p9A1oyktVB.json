{"id": "p9A1oyktVB", "number": 16154, "cdate": 1758260753256, "mdate": 1763548396242, "content": {"title": "Pseudo-Non-Linear Data Augmentation: A Constrained Energy Minimization Viewpoint", "abstract": "We propose a simple yet novel data augmentation method for general data modalities based on energy-based modeling and principles from information geometry. Unlike most existing learning-based data augmentation methods, which rely on learning latent representations with generative models, our proposed framework enables an intuitive construction of a geometrically aware latent space that represents the structure of the data itself, supporting efficient and explicit encoding and decoding procedures. We then present and discuss how to design latent spaces that will subsequently control the augmentation with the proposed algorithm. Empirical results demonstrate that our data augmentation method achieves competitive performance in downstream tasks compared to other baselines, while offering fine-grained controllability that is lacking in the existing literature.", "tldr": "We propose a simple, information-geometric approach to data augmentation that is learning-free, efficient, controllable, and broadly applicable to structured data.", "keywords": ["data augmentation", "information geometry", "energy-based model"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83d7f7f57f1a9b7e10bd539b6ff8ade8c1d5883e.pdf", "supplementary_material": "/attachment/16a3e29cf1e54a369ff3b3bb86d7c23c7215f1be.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a learning-free, geometry-aware framework for data augmentation based on energy-based modeling and information geometry. Unlike deep generative models that rely on trained latent representations, the proposed method constructs a statistical manifold via log-linear models on posets and performs data augmentation through forward and backward projections on low-dimensional submanifolds. The approach, termed Pseudo-Nonlinear Data Augmentation (PNL), combines the linearity of projections with nonlinear geometric curvature, offering both computational efficiency and controllability. Empirical results on images (MNIST, CIFAR-10), speech, and tabular datasets show that PNL performs comparably or better than autoencoder-based and standard augmentation methods, while being faster and interpretable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a geometric viewpoint on data augmentation grounded in energy minimization and information geometry, distinct from the dominant deep learning paradigms. The formulation via dually-flat manifolds and log-linear models on posets is both elegant and principled.\n\n2. The method requires no model training, avoiding the heavy computational cost and data dependence of generative augmentation. Projections are formulated as convex programs solvable with first-order methods.\n\n3. Augmentation behavior can be explicitly controlled by the choice of submanifold and the poset structure, enabling fine-grained manipulation of data geometry.\n\n4. The method demonstrates applicability to image, speech, and tabular data without domain-specific modifications.\nShows robustness and consistency, especially on small datasets with high variance."}, "weaknesses": {"value": "1. The baselines are somewhat minimal (autoencoder and standard augmentation).\nComparison to modern generative augmentations (e.g., diffusion-based, mixup, manifold mixup) would strengthen the claims.\n\n2. The method requires defining a partial order (poset) over features, which might be nontrivial or restrictive for unstructured or permutation-invariant data.\n\n3. While the method is theoretically interpretable, more concrete visualization or case studies of controllable augmentation (beyond MNIST/CIFAR) would enhance clarity."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iZMNK6XKel", "forum": "p9A1oyktVB", "replyto": "p9A1oyktVB", "signatures": ["ICLR.cc/2026/Conference/Submission16154/Reviewer_gGYH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16154/Reviewer_gGYH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761374791250, "cdate": 1761374791250, "tmdate": 1762926322453, "mdate": 1762926322453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new learning-free data augmentation method that is based on energy-based modeling. The authors first assume a poset structure of the data. Using this poset structure, they map the data to a log linear model on the poset. They then associate dually-flat coordinates with the log linear model. This constitutes the embedding of the data which is then used to generate data augmentations as follows. Given a sub-manifold, the authors project the embedding and then backward project using some heuristic assumptions on the structure of the data. Overall, this results in a task-independent data augmentation method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is presented clearly. The authors manage to put interesting and novel ideas together. This is an interesting addition to the landscape of data augmentation. The fact that the method is learning-free is very appealing."}, "weaknesses": {"value": "1. The poset structure does not fit many data domains well. This is acknowledged by the authors. I don't understand why they suggest that images have an inherent partial order. Directed graphs and time series might conform to such partial orderings.\n\n2. It is unclear what the log linear model captures about the data. The authors mention energy-based geometric modeling but more discussion is needed. It seems that there is first a normalization is done (Example 4.1.) and then this is interpreted as a probability distribution. It is unclear what the meaning of this probability distribution is for many data domains.\n\n3. Apart from the total energy in the features, $\\phi(z_i)$ preserves all the information about the data. Similarly, $\\theta$ coordinates should then do the same. It is unclear why the geometry here is better for data augmentation than the direct data space."}, "questions": {"value": "1. Can $z_i$ in principle contain data labeling? As far as I understand, $z_i$ is just data without labels. This makes the data augmentation procedure task-agnostic. This results in a method that cannot incorporate any biases that are beneficial for the downstream task except the choice of poset structure. Can you explain how the poset structure introduces such biases in the case of images?\n\n2. Why do you think the probability distribution that comes after applying $\\phi$ is a good representation of the data? This should be dependent on the properties of data. For example, could you explain why it is a good representation for images? What is added by the dually flat coordinates? Why would the data be in a linear subspace of this manifold?\n\n3. You cite generative-model-based data augmentation suffers from two challenges: efficiency and controllability. Is this the case for MNIST and CIFAR-10? I believe the challenges apply to harder datasets and I don't see why the proposed method scales to high dimensions.\n\nThe following paper is very relevant related work as it has an encoding/decoding scheme over a manifold that is learned.\n\n[1] Yüksel, Oguz Kaan, et al. \"Semantic perturbations with normalizing flows for improved generalization.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HWB1EGsdqq", "forum": "p9A1oyktVB", "replyto": "p9A1oyktVB", "signatures": ["ICLR.cc/2026/Conference/Submission16154/Reviewer_ZQPF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16154/Reviewer_ZQPF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491529504, "cdate": 1761491529504, "tmdate": 1762926322094, "mdate": 1762926322094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a projection-based data augmentation framework that maps data onto a statistical manifold and performs controlled perturbations through forward and backward projection. The main novelty focuses on achieving learning-free, efficient, and controllable augmentation with semantics preserved, by leveraging the geometric structure induced by posets. Experiments on image and speech datasets validate that the method can maintain core structural features while introducing meaningful variation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is learning-free and inverse-consistent. In addition, it is controllable with explicit selection of structural attributes."}, "weaknesses": {"value": "1. Performance gains over simple AE-based augmentation are modest, especially in CIFAR, where improvements are marginal.\n2. Efficiency is claimed relative to diffusion models, but no direct comparison or runtime analysis is provided.\n3. As discussed in the paper, the method requires data to admit a meaningful poset structure, which cannot directly handle permutation-invariant data such as point clouds or general graphs.\n4. The experiments are primarily on images, as argued in line 88, with more different modalities, like graph, video, might be more interesting."}, "questions": {"value": "The backward projection relies on nearest neighbors in the latent space to reconstruct data, which means the augmented samples often stay very close to existing examples. As the dataset grows, will this approach become less scalable and may fail to produce diverse new samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zEd1m3l9Da", "forum": "p9A1oyktVB", "replyto": "p9A1oyktVB", "signatures": ["ICLR.cc/2026/Conference/Submission16154/Reviewer_W952"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16154/Reviewer_W952"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882016721, "cdate": 1761882016721, "tmdate": 1762926321572, "mdate": 1762926321572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a projection-based data augmentation framework that maps data onto a statistical manifold and performs controlled perturbations through forward and backward projection. The main novelty focuses on achieving learning-free, efficient, and controllable augmentation with semantics preserved, by leveraging the geometric structure induced by posets. Experiments on image and speech datasets validate that the method can maintain core structural features while introducing meaningful variation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method is learning-free and inverse-consistent. In addition, it is controllable with explicit selection of structural attributes."}, "weaknesses": {"value": "1. Performance gains over simple AE-based augmentation are modest, especially in CIFAR, where improvements are marginal.\n2. Efficiency is claimed relative to diffusion models, but no direct comparison or runtime analysis is provided.\n3. As discussed in the paper, the method requires data to admit a meaningful poset structure, which cannot directly handle permutation-invariant data such as point clouds or general graphs.\n4. The experiments are primarily on images, as argued in line 88, with more different modalities, like graph, video, might be more interesting."}, "questions": {"value": "The backward projection relies on nearest neighbors in the latent space to reconstruct data, which means the augmented samples often stay very close to existing examples. As the dataset grows, will this approach become less scalable and may fail to produce diverse new samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zEd1m3l9Da", "forum": "p9A1oyktVB", "replyto": "p9A1oyktVB", "signatures": ["ICLR.cc/2026/Conference/Submission16154/Reviewer_W952"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16154/Reviewer_W952"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882016721, "cdate": 1761882016721, "tmdate": 1763738724095, "mdate": 1763738724095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a learning-free and geometry-aware data augmentation framework termed Pseudo-Non-Linear Data Augmentation (PNL-DA). The key idea is to view data augmentation as an energy minimization problem on a latent manifold derived from the intrinsic structure of the dataset.\n\nInstead of using neural generators or search-based augmentation (e.g., AutoAugment), the authors construct an information-geometric latent space and perform constrained energy descent to generate perturbed samples that respect the data manifold while introducing non-linear diversity. This results in augmented data that remain semantically valid yet expand the effective data support.\n\nThe paper claims that PNL-DA:\n1. requires no additional network training,\n2. offers fine-grained controllability over augmentation strength via explicit constraints, and\n3. achieves competitive or superior downstream performance across standard image classification benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper reframes data augmentation as an energy minimization process under latent-space constraints. This is a refreshing deviation from purely heuristic or search-based augmentations. The geometric viewpoint is conceptually elegant and could inspire follow-up work linking augmentation and manifold regularization.\n\n2. In contrast to most augmentation strategies that involve training auxiliary networks or policy search, this method is deterministic and learning-free once the latent geometry is built. This makes it lightweight and potentially appealing for resource-constrained environments.\n\n3. The energy constraint allows explicit control over perturbation magnitude, which is a desirable property compared to black-box stochastic augmentation policies. The paper demonstrates that tuning this constraint can balance sample diversity and fidelity.\n\n4. Experiments on several vision datasets (CIFAR-10/100, Tiny-ImageNet, SVHN, and others) show that PNL-DA matches or slightly surpasses traditional augmentations and performs competitively against AutoAugment and RandAugment, while using less computational cost.\n\n5. The exposition of the method—particularly the geometric intuition and constrained optimization formulation—is well organized and mathematically sound. The visualizations of latent-space perturbations help the reader grasp the intuition."}, "weaknesses": {"value": "1. The energy constraint is intuitive, but there is no formal proof that minimizing it preserves label semantics or improves generalization bounds. The paper could benefit from a more rigorous analysis linking the energy formulation to risk minimization.\n\n2. The method introduces a constraint coefficient controlling perturbation scale. The results indicate non-trivial sensitivity, but the paper lacks systematic tuning guidelines or ablation to quantify robustness."}, "questions": {"value": "1.The paper frames data augmentation as energy minimization on a latent manifold. Could you clarify the exact connection between your “energy” function and the traditional risk minimization objective? Is the energy equivalent to a potential function, or is it empirically constructed?\n\n2. You define “pseudo-non-linear” transformations. In what precise sense are they pseudo non-linear? Are these transformations non-linear in the data space but linear in latent space, or vice versa?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tJaDxZbmk6", "forum": "p9A1oyktVB", "replyto": "p9A1oyktVB", "signatures": ["ICLR.cc/2026/Conference/Submission16154/Reviewer_dmdm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16154/Reviewer_dmdm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967999668, "cdate": 1761967999668, "tmdate": 1762926320630, "mdate": 1762926320630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}