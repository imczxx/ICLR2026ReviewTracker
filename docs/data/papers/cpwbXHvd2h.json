{"id": "cpwbXHvd2h", "number": 13814, "cdate": 1758223107559, "mdate": 1759897411012, "content": {"title": "FACET: A Fragment-Aware Conformer Ensemble Transformer", "abstract": "Accurately predicting molecular properties requires effective integration of structural information from both 2D molecular graphs and their corresponding equilibrium conformer ensembles. In this work, we propose a scalable Structure-Aware Graph Transformer that efficiently aggregates features from multiple 3D conformers while incorporating fragment-level information from 2D graphs. Unlike prior methods that depend on static geometric solvers or rigid fusion strategies, our approach employs a trainable attention-based mechanism within a graph transformer to dynamically fuse 2D and 3D representations. We further enhance this mechanism by injecting fragment-specific structural biases into the attention layers, enabling the model to capture fine-grained molecular details. Our method scales to large datasets, handling up to 75,000 molecules and hundreds of thousands of conformers, and achieves state-of-the-art results in molecular property prediction and reaction-level modeling. It is particularly effective on chemically diverse compounds, including organocatalysts and transition-metal complexes.", "tldr": "", "keywords": ["molecular properties prediction", "3D conformers", "graph transformer", "2D-3D fusion", "fragment aware module", "Fused Gromov-Wasserstein distance"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fb1502c5376b3b9a6ed1c033f8ba67c74c048bc2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work focuses on molecular property prediction by integrating 2D graph information and 3D conformer ensemble information. The main contributions include the introduction of fragment-level structural priors, supported by comprehensive ablation studies, and a fused Gromov-Wasserstein (FGW) alignment pre-training stage that employs a Graph Transformer with an FGW distance loss. This replaces the computationally expensive FGW alignment process and leads to substantial efficiency improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written, and the proposed method is described in a detailed and logically consistent manner. The experimental setup is well documented, enhancing reproducibility.\n2. The overall pipeline design is reasonable. The fragmentation strategy for generating fragmented graphs, along with the subsequent fusion of atom node features and fragmentation features, is well-motivated. The use of 3D-MPNNs (VisNet/SchNet) for extracting 3D conformer features ensures E(3)-invariance, making the overall framework theoretically consistent.\n3. To address the high computational cost of FGW alignment, the work proposes replacing it with a Graph Transformer trained using a supervised FGW distance L2 loss. Section 4 provides theoretical analysis supporting this design, and experimental results (Fig. 2) show positive correlations. This approach significantly improves efficiency in both training and inference (Figs. 7 and 3).\n4. The method achieves strong performance on multiple regression benchmarks, and the ablation study provides clear evidence for the effectiveness of the fragment-level structural priors."}, "weaknesses": {"value": "1. Since the pipeline consists of three training stages, the pipeline illustration (Fig. 1) could be reorganized to clearly delineate these stages and indicate the corresponding losses for each stage. This would make the process easier to follow.\n2. There is no comparison of model parameters and inference time across all baselines. While it may not be necessary to include every method, most recent approaches should be considered, rather than comparing only with CONAN-FGW.\n3. There is no ablation or discussion on whether Training Stage 2 (i.e., training the Graph Transformer to approximate the FGW distance) is necessary. Specifically, what if Stage 1 is retained, but the Graph Transformer is randomly initialized and trained directly without Stage 2 pre-training (i.e., without the approximate FGW distance loss $\\mathcal{L}_{enc}$)? Would this configuration still achieve good results, and how would it compare with the proposed approach? If the performance difference is small, it could indicate that Stage 2—and the overall FGW-based motivation—may not be critical to the method’s success."}, "questions": {"value": "1. Regarding Weakness [3], if a relevant discussion already exists, please indicate where it appears. Otherwise, additional discussion and experimental results on this point would be valuable.\n2. In the implementation, is the graph fragmentation step handled during data pre-processing (i.e., fragmented and stored in advance) or performed dynamically during each forward pass (i.e., fragmented in real time)? If it is the latter, what proportion of the total forward-pass time (from raw data input to loss computation) does it account for, and does it introduce noticeable computational overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZwY0TxnL1F", "forum": "cpwbXHvd2h", "replyto": "cpwbXHvd2h", "signatures": ["ICLR.cc/2026/Conference/Submission13814/Reviewer_Jbmu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13814/Reviewer_Jbmu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761296580967, "cdate": 1761296580967, "tmdate": 1762924342072, "mdate": 1762924342072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel architecture for molecular property prediction that integrates fragment-level chemical structure with 3D conformer ensembles. FACET introduces a Fragment-Conformer Attention Module (FCAM) that hierarchically aggregates information within fragments, across conformers, and at the whole-molecule level, ensuring rotational, translational, and permutational invariance. This fragment-aware design captures both local and global geometric interactions more effectively than atom-level models. FACET achieves state-of-the-art performance on multiple 3D molecular benchmarks while using fewer conformers and offering improved interpretability and computational efficiency"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. FACET introduces a novel multi-scale attention mechanism combining fragment, conformer, and molecule-level reasoning - an elegant fusion of chemistry-driven inductive bias with deep transformer architectures. It extends prior conformer-based GNNs by explicitly modeling fragment ensembles, bridging a gap between quantum-chemical interpretability and machine-learned representation power.\n2. The paper's architectural design is technically sound and well-motivated, supported by ablation studies showing the benefit of fragment-aware aggregation.\n3. The writing is clear and well-organized, moving systematically from intuition to formalism and experiments.\n4. FACET represents a meaningful advance for 3D molecular ML, offering a scalable, interpretable, and chemically grounded alternative to purely atom-level GNNs."}, "weaknesses": {"value": "1. **Dependence on Predefined Fragmentation:** The model’s reliance on fixed fragment decompositions could limit adaptability to novel or unusual chemistries.\n2. **Theoretical Limitations:** The invariance proof (Theorem 1) assumes perfect fragment-conformer alignment and ignores potential numerical instabilities in coordinate normalization. The theorem's validity would benefit from a more rigorous group-theoretic formalization akin to SE(3)-equivariant models.\n3. **Ablation on Conformer Ensemble Size:** While the authors claim reduced conformer dependence, quantitative experiments varying ensemble size (1, 5, 10, 20) are limited. Such results would strengthen the argument for FACET's data efficiency.\n4. **Computational Overhead and Resource Reporting:** FACET adds multiple attention layers per fragment and conformer, but runtime and memory usage are not reported."}, "questions": {"value": "1. **Fragment Definition and Generalization:** How are fragments defined and standardized across molecules? Is the fragmentation algorithm differentiable or fixed? Could learned or adaptive fragmentations improve performance on datasets with different chemical domains?\n2. **Conformer Ensemble Sampling Strategy:** The method relies on conformer ensembles from datasets such as GEOM. How sensitive is FACET to the number of conformers used during training and inference? Could the model degrade when applied to datasets lacking reliable conformer ensembles (e.g., generated on-the-fly via RDKit)?\n3. **Theoretical Guarantees on Invariance:** The paper asserts rotational, translational, and permutational invariance of FACET (Theorem 1), but the proof sketch in Appendix C assumes strict alignment across conformers. How does FACET ensure invariance when conformers differ significantly in spatial orientation or fragment positioning?\n4. **Fragment-Level vs. Atom-Level Trade-offs:** Have the authors evaluated the granularity trade-off - i.e., how predictive accuracy changes when using atom-level embeddings versus different fragment granularity levels?\n5. **Conformer Diversity Representation:** Does FACET perform any diversity regularization or weighting among conformers? Could the model collapse to a single representative conformer if conformer embeddings are highly correlated?\n6. **Data Efficiency and Scalability:** While FACET achieves good performance, training time and parameter counts are not discussed in detail. How does FACET scale with molecular size and number of conformers compared to GNN baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D7PiVCeNQu", "forum": "cpwbXHvd2h", "replyto": "cpwbXHvd2h", "signatures": ["ICLR.cc/2026/Conference/Submission13814/Reviewer_Csta"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13814/Reviewer_Csta"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761454026139, "cdate": 1761454026139, "tmdate": 1762924341622, "mdate": 1762924341622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FACET, a hybrid model integrating a Message Passing Neural Network (MPNN) with a Graph Transformer (GT) to accurately predict molecular properties from 2D molecular graphs and 3D conformations. Existing methods for aligning graph and conformational features often suffer from high computational costs. FACET addresses this challenge through two key innovations: (1) It leverages a GT to approximate the computation of the Fused Gromov-Wasserstein (FGW) distance, enabling efficient feature alignment with a provable theoretical error bound; (2) It embeds fragment-level structural priors from the molecular graph and sampled conformational features into the MPNN and GT, effectively capturing multi-scale relationships between the two molecular representations.\nExperimental results demonstrate that FACET outperforms several baseline models, including CONAN-FGW, on the MoleculeNet and MARCEL datasets. Furthermore, FACET exhibits significantly faster training and inference speeds, validating its computational efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The use of a GT to approximate the FGW distance is a notable innovation, combining theoretical rigor with algorithmic efficiency for feature alignment.\n2. The model design is sound, integrating fragment-level structural priors and conformational features through a multi-scale message-passing architecture.\n3. The model achieves a favorable balance between predictive performance and computational speed, underscoring its practical utility.\n4. Comprehensive evaluations on multiple benchmark datasets provide strong, empirical support for the model's effectiveness and efficiency."}, "weaknesses": {"value": "1. The abstract inadequately highlights the core contributions, particularly the novel use of GT for efficient FGW approximation.\n2. The model diagram (Figure 1) lacks clarity. The MPNN for processing 3D conformations is represented only by the symbol \"Φ\", and the GT is depicted generically as an \"attention mechanism,\" which hinders a straightforward understanding of the model's architecture.\n3. The conclusion is underdeveloped, merely restating the main contributions and results without critical analysis or forward-looking perspectives.\n4. Table 1 contains errors. The column header \"Model\" is incorrect and should be \"Dataset\" based on the content. Furthermore, the sum of the values under the \"Train,\" \"Valid,\" and \"Test\" rows for the \"Lipo\" dataset does not match the value in the \"Total\" row."}, "questions": {"value": "1. Revise the abstract to more prominently and explicitly state the core methodological innovations.\n2. Redesign Figure 1 to provide a clearer, more detailed visual representation of the MPNN and GT components.\n3. Integrate the key points from the appendix's \"Limitations of FACET\" section into the main \"Conclusion\" section to offer a more critical summary and a concrete outlook for future work.\n4. Conduct a thorough review of all tables, especially Table 1, to ensure data consistency and accuracy across all entries."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mPA91E0xPd", "forum": "cpwbXHvd2h", "replyto": "cpwbXHvd2h", "signatures": ["ICLR.cc/2026/Conference/Submission13814/Reviewer_1ftv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13814/Reviewer_1ftv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881842204, "cdate": 1761881842204, "tmdate": 1762924341239, "mdate": 1762924341239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a well-executed study on a scalable framework for integrating conformational information into molecular property prediction. The authors propose a learned embedding of the Fused Gromov-Wasserstein (FGW) distance to efficiently aggregate 3D conformers, enabling the model to scale to large datasets. Additionally, fragment-level structural priors are incorporated to enhance representation learning. The proposed method achieves state-of-the-art performance across multiple benchmarks, demonstrating its effectiveness."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Scalability**: The use of learned FGW embeddings enables efficient conformer aggregation, making the method suitable for large-scale datasets.\n- **Structural Integration**: The combination of 3D conformers with 2D fragment-level information provides a rich, multi-scale representation of molecular structure.\n- **Empirical Performance**: The method consistently outperforms strong baselines across diverse tasks, offering compelling evidence of its advantage.\n- **Architectural Flexibility**: The framework is compatible with multiple backbone architectures and tasks, highlighting its versatility."}, "weaknesses": {"value": "- **Split Strategy**: All experiments rely on random splits. Evaluating performance under scaffold splits would better assess the model’s ability to generalize across distinct chemical scaffolds—a key criterion for real-world applicability.\n- **Role of 2D Fragments**: The necessity of 2D graphs and fragment-level features is not fully justified, given that 3D conformers already encode topological and spatial information.\n- **Limited Ablation Scope**: Ablation studies are restricted to MoleculeNet tasks. Extending these analyses to other datasets (e.g., MARCEL) would strengthen the claims and demonstrate robustness across domains."}, "questions": {"value": "Given that the proposed method approximates real FGW calculations using a learned embedding, is the performance theoretically upper-bounded by the true FGW metric? If so, under what conditions would the approximation match or exceed FGW-based methods in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tmEbTTMXnO", "forum": "cpwbXHvd2h", "replyto": "cpwbXHvd2h", "signatures": ["ICLR.cc/2026/Conference/Submission13814/Reviewer_NGyk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13814/Reviewer_NGyk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972819100, "cdate": 1761972819100, "tmdate": 1762924340843, "mdate": 1762924340843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}