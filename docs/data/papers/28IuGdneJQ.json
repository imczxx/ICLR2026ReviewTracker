{"id": "28IuGdneJQ", "number": 3163, "cdate": 1757346359764, "mdate": 1759898104866, "content": {"title": "SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples", "abstract": "In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be formulated as a one-sided entropic optimal transport problem and solved via an alternative minimization algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Empirical studies validate our theory and demonstrate that SFBD-OMNI substantially improves recovery in the challenging regime of non-identifiable corruption processes.", "tldr": "", "keywords": ["ambient diffusion", "diffusion models", "generative modeling", "density deconvolution"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5cf2c079ade41e405ff60da2df9c0a72c315c2b4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SFBD-OMNI, a new framework for generating from a clean data distribution while training from corrupted samples and a limited number of clean samples. The proposed method generalizes SFBD to arbitrary corruption models, providing both theoretical guarantees and practical algorithms, including an online variant that supports end-to-end training. The authors handle non-identifiable corruptions by introducing a regularization that acts as a \"projection\" (in the KL sense), enforcing the solution to be close to a given distribution constructed from a small set of clean samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Overall, the proposed approach is clean and elegant, relying on well-established principles.\n2. The paper is well-written and easy to follow. The paper is structured in a clear and logical manner, and the authors provide several intuitive examples throughout to help the reader understand the concepts presented.\n3. The experimental results seem good -- the proposed approach improves upon previous methods, and it is able to handle non-identifiable degradations such as de-colorization."}, "weaknesses": {"value": "1. Only two datasets (CIFAR-10 and CelebA) are evaluated. Testing on more complex real-world domains (e.g., MRI, satellite data) could have been more appealing from a practical standpoint. In my opinion, this is a major weakness.\n2. While clean-sample weighting is explored (Figure 2), the effect of other hyperparameters such as $\\gamma$ (update ratio) remains unclear.\n3. The authors perturb the endpoints $y$ to \"prevent degeneration\", since flow matching is a deterministic map. But an explanation about the effects of such perturbations (both in terms of theory and practice) is missing.\n4. The proposed approach still uses a small number of clean samples to train the generative model, which is a limitation. In real-world scenarios and scientific applications (e.g., astrophysics), we often only have corrupted samples."}, "questions": {"value": "Why was flow matching selected as the primary instantiation? Would other stochastic bridges (e.g., Schrodinger) behave differently? Perhaps using another bridge could alleviate the need to perturb the endpoint samples? It would be interesting to assess several types of flows, especially since, from the beginning of the paper, the authors formulate the problem in the stochastic sense (using Brownian motion, eq. 8), but then implement it with flow matching (which does not involve Brownian motion)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3lZ0srObNl", "forum": "28IuGdneJQ", "replyto": "28IuGdneJQ", "signatures": ["ICLR.cc/2026/Conference/Submission3163/Reviewer_Sh1d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3163/Reviewer_Sh1d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761515447583, "cdate": 1761515447583, "tmdate": 1762916580354, "mdate": 1762916580354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of restoring clean signals from corrupted measurements in a setting with little to no access to clean samples and a black-box access to the corruption model. Taking inspiration from the Stochastic Forward-Backward Deconvolution (SFBD) algorithm for Gaussian noise corruption, the authors introduce SFBD-OMNI as its extension to arbitrary corruption processes. At the core of the method lies a reformulation of the problem using the Donsker-Varadhan variational principle, which leads to an explicit objective for the (augmented) Kullback-Leibler Ambient Projection (KLAP) problem. In addition to the resulting formulation of SFBD-OMNI, the authors also propose its online (flow) variant that performs only partial updates to the data buffer used during training. Both methods are empirically compared with a proper set of baselines on both identifiable and non-identifiable processes. In addition, an ablation regarding the influence of the clean sample weighting is presented."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper is written extremely well considering that the topic is not the easiest to describe formally. The flow of the text is great, with additional intuitive explanations following the introduction of more difficult concepts (like Figure 1).\n\nS2. The authors propose a novel reformulation of the considered objective using the Donsker-Varadhan variational principle, showing a connection to the entropic optimal transport objective. The theoretical results are interesting and relevant to the considered problem. Importantly, the convergence analysis applies to both the standard SFBD-OMNI and the online version.\n\nS3. The proposed methods outperform the considered baselines and the empirical results align with the theoretical considerations."}, "weaknesses": {"value": "W1. My main concern relates to the accuracy of one of the claims. The authors suggest that, under suitable identifiability conditions, the ground-truth clean data distribution can be recovered without access to clean samples (as lines 065-069 seem to suggest). Proposition 4 guarantees convergence under the considered update rules, even with an arbitrary initialization and full replacement in the training buffer $\\Large \\varepsilon$. Does this imply that SFBD works when no clean samples are used at all, i.e., even when the initial pretraining phase is skipped? Is this scenario empirically verified in the paper, or do the results only cover the case where clean samples are used for pretraining but omitted from the updates in line 4 of Algorithm 1/2? If this is not verified, I would ask the authors to provide results for this case.\n\nW2. I understand that some methods in Table 1 are only applicable to specific corruption processes. Are there any instances where a baseline is theoretically suitable for one of the tested problems, but its results are not included in the table? If so, the paper would greatly benefit from evaluating these baselines in such cases, as this would provide a more complete comparison and support future work."}, "questions": {"value": "I am generally very sympathetic to this work and consider the approach, together with its theoretical derivations, very elegant. I was leaning toward a score of 8. However, I am very interested in the authors' answer to W1 above and consider it an important clarification before raising my score from 6 to 8. Please also refer to W2. Below are some additional minor questions.\n\nQ1. lines 046-048: Is this phrased correctly? Shouldn't it be *With only a limited number of corrupted samples but an abundance of clean ones (...)*?\n\nQ2. line 160: The authors probably meant $g=0$ for Eq. 6.\n\nQ3. I-projection (line 216) should be expanded to Information-projection for clarity.\n\nQ4. line 253: What if $r(\\mathbf{y}|\\mathbf{x})=0$ under the $\\log$?\n\nQ5. lines 289-298: The subscripts and superscripts for $k$ are probably mixed up."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V6XBmqCgAK", "forum": "28IuGdneJQ", "replyto": "28IuGdneJQ", "signatures": ["ICLR.cc/2026/Conference/Submission3163/Reviewer_6bRN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3163/Reviewer_6bRN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829975520, "cdate": 1761829975520, "tmdate": 1762916580165, "mdate": 1762916580165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose SFBD-OMNI, a bridge model–based framework that maps corrupted sample distributions to the underlying clean data distribution for image restoration and generation recovery. The method extends the prior SFBD framework to handle arbitrary measurement models beyond the Gaussian corruption setting, providing a more general and theoretically grounded approach to learning from imperfect or partially observed data"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Theoretical generalization capability for any unbound noise. The paper extends prior diffusion-based deconvolution frameworks SFBD to non-identifiable measurement processes, providing a principled formulation via one-sided entropic optimal transport. This generalization makes the approach applicable to a broader range of real-world degradations \n- Data-efficient restoration without extensive ground-truth supervision. The proposed framework learns to recover clean image distributions using only a handful of clean samples, making image restoration more practical and label-efficient compared to traditional fully supervised diffusion approach"}, "weaknesses": {"value": "- Limited novelty via SFBD framework extension. While the paper provides a principled generalization of SFBD to non-identifiable measurement models via one-sided entropic OT, the overall formulation and optimization pipeline remain close to the original SFBD framework. The methodological increment feels more like an extension or refinement rather than a fundamentally new paradigm.\n\n- Insufficient experimental evidence to substantiate the theoretical analysis. Despite a well-developed theoretical formulation, the experimental validation is narrow and mostly relies on simple synthetic corruption processes. The work lacks evaluation on real-world restoration datasets (e.g., **RainDrop (Qian et al., CVPR 2018)**, **AllWeather (Valanarasu et al., 2022)** ) that could better demonstrate generalization under uncontrolled degradations. Moreover, the notion of “limited clean data” (only 50 samples) is fixed and not systematically analyzed against traditional fully-supervised baselines, leaving unclear how much clean supervision is truly needed.\n\n- Sensitivity to hyperparameter settings and practical complexity:\nThe algorithm involves several critical hyperparameters, such as the clean-sample weight λ and the online update ratio γ, which the authors fix empirically (λ / (1 + λ) = 0.2, γ = 0.002). The paper does not provide an ablation or sensitivity study, and training remains complex due to alternating optimization of all hyperparameters. This raises concerns about reproducibility and the need for manual tuning to achieve stable performance."}, "questions": {"value": "1.\tHow does the method scale with different amounts of clean data?\nThe paper fixes the number of clean samples to 50 (~0.1%), but it would be informative to see how performance changes when this ratio varies, or how it compares to a small supervised diffusion baseline trained on the same subset?\n\n2.\tDo the authors plan to evaluate SFBD-OMNI on real-world restoration datasets (e.g., RainDrop, AllWeather) to demonstrate its applicability beyond synthetic corruptions, and to compare its performance against recent fully supervised diffusion-based restoration methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z6HVWVY3Rv", "forum": "28IuGdneJQ", "replyto": "28IuGdneJQ", "signatures": ["ICLR.cc/2026/Conference/Submission3163/Reviewer_GiSu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3163/Reviewer_GiSu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950524009, "cdate": 1761950524009, "tmdate": 1762916579979, "mdate": 1762916579979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SFBD-OMNI, a bridge-model framework to learn the clean data distribution when you have many corrupted samples and few clean ones, assuming black-box access to the corruption process. They cast “recover clean from corrupted” as a KL Ambient Projection (KLAP) problem and show two key variational views: (i) the classic AmbientGAN min–max formulation, and (ii) a new formulation via the Donsker–Varadhan principle that turns KLAP into a one-sided entropic optimal transport objective with a tractable alternating minimization (posterior update + prior update). \n\nThis yields closed-form updates and naturally plugs into diffusion/bridge models to learn the posterior \n $u_\\theta(x∣y)$. To address non-identifiable corruption (e.g., grayscale or blur), they augment the objective with a KL regularizer toward a small set of clean samples $h$ and analyze the limit as the regularization weight vanishes, recovering the I-projection within the feasible set. They present SFBD-OMNI and an online variant (partial refresh of reconstructed samples) with convergence guarantees. Experiments on CIFAR-10 and CelebA show improved FID over baselines (including in non-identifiable cases) when a small number of clean samples is mixed in training."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Unifying view + derivation: Recasting KLAP with Donsker–Varadhan to obtain a one-sided entropic OT objective is elegant and principled; it clarifies when/why alternating updates should work and connects to entropic OT literature (contrast with AmbientGAN’s min–max). \n- Identifiability analysis + “few clean” fix: Clear conditions for identifiability and a regularized KL that provably picks the best element in the feasible set, matching practice where a handful of clean samples are available.\n- Convergence guarantees: Both batch and online SFBD-OMNI have convergence results (including a rate bound when $\\lambda \\to 0$). That’s stronger theory than many EM-like or Ambient-style methods.\n- Empirical coverage: Results span identifiable (masking, Gaussian noise) and non-identifiable (grayscale, blur) regimes; OMNI competes with or beats specialized methods and EM-Diffusion. \n- Practical modeling choice: Using bridge/flow-matching to learn posteriors is a strong fit; the paper also addresses degeneracy of deterministic paths with noise injection."}, "weaknesses": {"value": "- Assumption on black-box sampling from r(y|x): Many real sensors provide only forward passes on x, not easy sampling of y|x across conditions; measuring cost or calibration drift may complicate the assumed operator access. The paper could discuss robustness to operator mismatch (misspecified r). \n- Posterior learning scalability/details: Training $u_\\theta(x|y)$ as a conditional bridge can be heavy; stability hinges on path choices and endpoint noise. More ablations on bridge choices, noise level, and compute vs. FID trade-offs would help (training time is non-trivial).\n- Evaluation breadth: Only CIFAR-10 ($32^2$) and CelebA ($64^2$) with limited corruptions. Stronger evidence would include higher-res datasets, complex forward models (e.g., compressive sensing, Poisson noise), and domain tasks where distributional recovery helps downstream reconstructions.\n- FIDs are fine but you really do need to show empirical samples for the learned model.\n- Tuning $\\lambda, \\gamma$: While theory supports limits, practice shows a “sweet spot” for clean-weight; guidance for automatic selection (e.g., validation via corrupted likelihood) is missing.\n- Identifiability test practicality: The paper discusses conditions and a criterion qualitatively; a procedural test practitioners can run on a new corruption (and with finite data) would increase usability.\n- Minor nitpick, but Section 4.2 needs some cleaning up bc there's too much content in a small space and it's also a key contribution. Maybe reduce some content in Section 4.1 to give more space to 4.2? Also a small section with notation may be helpful, I had to search for the definition of $h$ which was last defined in Section 3.2 (maybe just say $p_data$?). Also, between Eqn (14) and (15), can you add a small phrase saying \"taking \\mathbb{E} over $y\\sim q$ and subtracting E[ log q(y) ], we get...\" It was a little confusing at first since there are many functions and it wasn't clear what's a function of y, x, etc."}, "questions": {"value": "- Operator misspecification: How sensitive is SFBD-OMNI to errors in $r(y|x)$? Say you're doing pixel dropout and you have the wrong estimate of dropout probabilities?\n- Automatic hyper-selection: How exactly do you select the $\\lambda$ (clean-weight) and $\\gamma$ (online update ratio) parameters?\n- For my own understanding, what's the right way to think of $\\Phi(p)$ in Proposition 3? Is it a penalty for how different the pushforwards in y are for $p^*$ and $p$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zWnhZkT1qG", "forum": "28IuGdneJQ", "replyto": "28IuGdneJQ", "signatures": ["ICLR.cc/2026/Conference/Submission3163/Reviewer_b1RA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3163/Reviewer_b1RA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762241406009, "cdate": 1762241406009, "tmdate": 1762916579744, "mdate": 1762916579744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}