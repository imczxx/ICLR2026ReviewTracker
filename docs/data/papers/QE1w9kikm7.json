{"id": "QE1w9kikm7", "number": 10896, "cdate": 1758184372001, "mdate": 1763604966352, "content": {"title": "Theoretical Analysis of Relative Errors in Gradient Computations for Adversarial Attacks with CE Loss", "abstract": "Gradient-based adversarial attacks using the Cross-Entropy (CE) loss often overestimate robustness due to relative errors in gradient computation induced by floating-point arithmetic. Empirical methods like MIFPE mitigate this by scaling logits with a factor $ c = T / \\Delta_{\\text{detach}} $ where $ T = 1 $, significantly improving evaluation accuracy. However, a theoretical understanding of these errors remains limited.\nTo bridge this gap, we pioneer the first rigorous theoretical analysis of floating-point errors in CE-based gradient attacks, systematically dissecting relative errors across four distinct scenarios: (i) unsuccessful untargeted attacks, (ii) successful untargeted attacks, (iii) unsuccessful targeted attacks, and (iv) successful targeted attacks. This foundational study uncovers novel patterns in numerical instability and derives the optimal scaling factor $T = t^\\* $ that minimizes error impact in each scenario. Notably, our analysis reveals that $ t^\\* $ closely approximates 1 in unsuccessful untargeted attacks, providing a theoretical justification for MIFPE's empirical choice and addressing prior optimality gaps.\nTo validate the correctness of our theoretical derivations, we refine MIFPE by incorporating $ T = t^\\* $ into the Theoretical MIFPE (T-MIFPE) loss function, which further reduces floating-point-induced errors. Comprehensive experiments validate our theory.", "tldr": "", "keywords": ["adversarial attacks", "floating-point errors", "robustness evaluation", "optimal scaling factor", "theoretical analysis"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0360b04f94d08696e7c3ccc3c89de0d317516d9.pdf", "supplementary_material": "/attachment/84d79e552c30c736358a940861380853c71991aa.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on floating-point errors in CE-based gradient attacks, which dissects relative errors across four distinct scenarios: (i) unsuccessful untargeted attacks, (ii) successful untargeted attacks, (iii) unsuccessful targeted attacks, and (iv) successful targeted attacks. To this end, this paper uncovers patterns in numerical instability and derives the optimal scaling factor that minimizes error impact in each scenario."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The topic this paper focused on,  the floating-point errors in CE-based gradient attacks, is very novel and interesting."}, "weaknesses": {"value": "1. The template of this paper should be ICLR 2026, rather than ICLR 2025.\n\n2. The motivation is not clearly explained, which make this paper difficult to understand. For example, authors do not explain how the model robustness is overestimated, without any experimental evidence. Besides, the floating-point error is also not well-defined. Authors should mathematically present its definition.\n\n3. A lot of symbols are not defined. For example, what is the definition of $z_{pi_1}$ and $z_{pi_2}$? What is the definition of $c$ in Eq. (5)? What is the motivation or intuition of using CE(cz,y)?\n\n4. The assumption for $\\partial_{\\hat x}(z_{pi_1}-z_{pi_2})$ is not experimentally verified. It may not hold in all model architectures or training regimes. Authors should experimental verify the correctness of this assuption before using it.\n\n5. Could $t*$ be chosen in a simplified manner for practical usage?\n\n6. It is unclear whether defenses could adapt to T-MIFPE or whether the observed improvements hold under adaptive attack strategies. Please clarify it.\n\n7. While T-MIFPE consistently improves over MIFPE, the gains are small (e.g., 0.01–0.34% in robust accuracy). However, this gain is so small, which may limit the practical use of this theory."}, "questions": {"value": "Please refer to weakness. Authors should improve their writing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wyWHn5TjdR", "forum": "QE1w9kikm7", "replyto": "QE1w9kikm7", "signatures": ["ICLR.cc/2026/Conference/Submission10896/Reviewer_Crpg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10896/Reviewer_Crpg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760926172406, "cdate": 1760926172406, "tmdate": 1762922103892, "mdate": 1762922103892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the floating-point arithmetic issue that causes the over-estimate robustness in gradient-based adversarial attacks using CE loss. Previous works find that scaling logits by a factor $c=T/\\Delta_{\\texttt{detach}}$ where $T=1$ can improve robustness evaluation, but lack theoretical justification. Therefore, this paper provides the first formal analysis of this aspect across four distinct scenarios, and refines MIFPE to further validate the theorem. Experiments confirm the analytical correctness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation is clear, and the paper provides the first theoretical analysis of the floating-point issue and why scaling logits by a factor can improve the estimation.\n- The analysis covers four typical attack settings (target/untarget and successful/unsuccessful).\n- Experiments further validate the correctness of the theory."}, "weaknesses": {"value": "- Lack of some details.\n- The notations and equations need some explanations for better clarity and readability."}, "questions": {"value": "- Is $t^\\ast$ computed averaged per batch?\n- Would other loss functions, such as CW, also present similar issues? Could the framework extend to these cases?\n- What precision mode is used for experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lf69J6h4FH", "forum": "QE1w9kikm7", "replyto": "QE1w9kikm7", "signatures": ["ICLR.cc/2026/Conference/Submission10896/Reviewer_cggz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10896/Reviewer_cggz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740405549, "cdate": 1761740405549, "tmdate": 1762922103554, "mdate": 1762922103554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A previous work, MIFPE, presented a technique to improve gradient-based adversarial attacks with CE loss by mitigating numerical underflow errors. This is achieved by rescaling the logits with a factor that depends on a parameter T, whose empirically estimated value is 1.\nThis paper extends MIFPE with a theoretical analysis aimed at obtaining an optimal value for the T parameter. Based on their findings, the authors propose a method to dynamically adjust T at each iteration of the attack. Experiments show that this strategy consistently improves attack performance compared to using a fixed T=1, although the improvement is very small, as the optimal computed value for T is often close to 1."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow\n- The addressed topic is relevant, as reliably estimating robustness against adversarial examples is still an open problem\n- The experimental findings confirm the theoretical basis, and the proposed strategy in some settings even improves AutoAttack (which is considered a state-of-the-art method)"}, "weaknesses": {"value": "- The paper contribution, although theoretically sound and empirically proven, is mainly limited to estimating an optimal value for an already existing method. The main issue (numerical underflow in CE loss) and the solution (MIFPE) were presented in that previous work, where additionally their authors already tried to provide a basic theoretical justification and an empirical estimate of T (which aligns with the findings provided in this paper).\n- The absolute runtime overhead of the additional T parameter estimation at each attack iteration is reported. However, this is not informative for assessing the overall impact on the attack performance. You should relate this value to the normal attack runtime, for instance, by reporting the relative runtime increase for each iteration and for the entire attack process.\n\nMinor issues:\n- In both the Introduction and Related Work sections, some symbols ($z, \\pi, \\Delta_{detached}$) appear without explaining their meaning, which is then described in the Theory Analysis section. You should either explicitly state their meaning as they appear or postpone them.\n- In Sect. 2.1, you formalize the input vector as an image with C, W, and H dimensions. I believe that this can be generalized to consider any input dimension, as the approach should be applied to other application domains than images.\n- In the Related Work section, the statement \"extensive empirical evidence has revealed their significant limitation in overestimating model robustness\" should report a supporting reference (for instance, [a] or even the already cited [b]).\n- In Fig. 2, the last caption words mention \"gray vertical dashed lines\", but I guess you meant red.\n\n[a] Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D., Goodfellow, I.J., Ma̧dry, A., & Kurakin, A. (2019). On Evaluating Adversarial Robustness. ArXiv, abs/1902.06705.\n\n[b] Carlini, N., & Wagner, D.A. (2016). Towards Evaluating the Robustness of Neural Networks. 2017 IEEE Symposium on Security and Privacy (SP), 39-57."}, "questions": {"value": "- I think it would be very interesting to individually report the APGD-CE performance from the AutoAttack results, to analyze the improvements related to fixing the underflow errors in CE loss. Could you please show these results?\n- I am also interested in understanding whether using the T-MIFPE approach inside the APGD-CE loss can further improve the attack performance by combining the automatic restarts and step size improvements of APGD with the mitigation of CE numerical errors. I suppose it is sufficient to modify a few lines in the autoattack implementation. Could you please provide that?\n- In the appendix, last page, you state that \"Each experimental run [...] was executed under $\\ell _\\infty$  - or $\\ell _2$ -bounded threat models\". However, in the paper, I only see results for the former. Is it a refusal, or did you actually run experiments for the latter as well? If so, what are your findings for this setting? I think it should be relevant to at least discuss them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FXQxu2MkI5", "forum": "QE1w9kikm7", "replyto": "QE1w9kikm7", "signatures": ["ICLR.cc/2026/Conference/Submission10896/Reviewer_LF5W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10896/Reviewer_LF5W"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909859061, "cdate": 1761909859061, "tmdate": 1762922103225, "mdate": 1762922103225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first systematic theoretical analysis of floating-point–induced relative errors in gradient computations for cross-entropy–based adversarial attacks. The authors classify attacks into four cases: successful and unsuccessful, targeted and untargeted. They derive the optimal scaling factor that minimizes these errors and propose a theoretically grounded loss function named T-MIFPE. Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate consistent yet modest improvements over MIFPE, confirming the validity of the theoretical analysis."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Provides a theoretical treatment of floating-point–induced gradient errors across multiple attack scenarios.\n2. Experiments across multiple datasets and models consistently support theoretical findings.\n3. Offers a generalizable framework for analyzing numerical stability in adversarial attacks."}, "weaknesses": {"value": "1.Experimental improvements are minimal (mostly ~0.1%), which may limit practical impact despite theoretical justification.\n2. Some theoretical assumptions (e.g., independence between gradient terms and scaling factor) are not thoroughly discussed.\n3. The analysis is restricted to CE-based attacks; more complex losses or adaptive attacks remain unexplored.\n4. The paper is difficult to follow."}, "questions": {"value": "1. Does the theoretical framework consider potential dependence between the scaling factor and gradient terms?\n2. Could the approach generalize to other loss functions such as C&W or DLR?\n3. Since experiments use a fixed random seed, have you tested result stability across multiple initializations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6Q1SRgLVT9", "forum": "QE1w9kikm7", "replyto": "QE1w9kikm7", "signatures": ["ICLR.cc/2026/Conference/Submission10896/Reviewer_5exQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10896/Reviewer_5exQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058613084, "cdate": 1762058613084, "tmdate": 1762922102898, "mdate": 1762922102898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}