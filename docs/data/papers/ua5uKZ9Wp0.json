{"id": "ua5uKZ9Wp0", "number": 5135, "cdate": 1757853256631, "mdate": 1759897992525, "content": {"title": "Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum", "abstract": "The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval.", "tldr": "A state-of-the-art video embedding model for universal video retrieval across diverse tasks and domains via multimodal curriculum learning on large-scale synthesized data .", "keywords": ["Video retrieval", "multimodal large language models", "video embedding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5271f74159ff210652f3cc0d8a9c3c8c4d8252e5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of universal video retrieval by proposing a co-designed framework jointly targeting evaluation, data, and training aspects. The authors introduce the Universal Video Retrieval Benchmark (UVRB), design a scalable data synthesis pipeline (V-SynFlow) yielding over 1.55 million high-quality, multimodal pairs (UVRD), and propose a Modality Pyramid curriculum to train a General Video Embedder (GVE) model for robust generalization. Extensive experiments demonstrate state-of-the-art zero-shot performance across challenging retrieval scenarios and provide new insights into the strengths and weaknesses of current paradigms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper simultaneously introduces a novel dataset and a new video retrieval method, presenting a substantial and comprehensive contribution.\n\n2.The paper conducts extensive experiments to demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1.The authors claim that a major contribution of this work is the introduction of a new benchmark; however, the related work section lacks a thorough discussion of existing benchmarks, and the paper fails to provide comparative analyses with them.\n\n2.The authors claim to have introduced a new benchmark; however, as evidenced in Section 3.1, this benchmark appears to merely aggregate existing datasets with only basic categorization. Moreover, the authors do not perform any substantial data cleaning or in-depth analysis, raising concerns about the comprehensiveness and validity of the proposed benchmark.\n\n3.The experiments in the paper are conducted exclusively on the authors’ newly proposed benchmark, which undermines the persuasiveness of the claimed effectiveness of the video retrieval method, as its generalizability and robustness remain unverified on established or diverse datasets.\n\n4.The paper contains citation formatting errors: in numerous places where the citep command should have been used, the authors have failed to apply it correctly.\n\n5.The GVE method appears to involve only minor modifications to the training strategy built upon the Qwen-VL model. To more rigorously validate the effectiveness of GVE, the authors should also evaluate the model without this method under otherwise identical conditions. Ideally, such an analysis would be included in an ablation study. However, the notation used in the current ablation study—specifically “GVE-s” and “GVE-i”—is not clearly defined, making it difficult to interpret what components or variants these labels refer to."}, "questions": {"value": "1.Is UVRB merely an aggregation of existing data? If so, beyond simply combining datasets, what additional efforts did the authors undertake to ensure the benchmark offers distinct advantages? If not, how was the data in the benchmark generated?\n\n2.What do GVE-i and GVE-s respectively represent in the ablation study?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oNiOINIPpb", "forum": "ua5uKZ9Wp0", "replyto": "ua5uKZ9Wp0", "signatures": ["ICLR.cc/2026/Conference/Submission5135/Reviewer_NrXo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5135/Reviewer_NrXo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761464855357, "cdate": 1761464855357, "tmdate": 1762917903538, "mdate": 1762917903538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a holistic video retrieval research framework built on the co-design of evaluation, data, and modeling. The authors introduce the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets to diagnose model capabilities across diverse tasks (e.g., textual, composed, visual) and domains (e.g., coarse-grained, fine-grained, long-context). Guided by diagnostics from this benchmark, they present V-SynFlow, a scalable workflow to synthesize a high-quality, multi-task dataset of 1.55 million video-text pairs (UVRD). Finally, they propose the Modality Pyramid, a curriculum learning strategy to train their General Video Embedder (GVE), an MLLM-based model. Experiments show that GVE achieves state-of-the-art zero-shot performance on UVRB, and the analysis reveals new insights, such as the finding that performance on partially relevant retrieval tasks is a better predictor of universal capability than traditional benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Holistic Framework: The main strength is the ambitious and well-executed \"evaluation-data-training\" co-design. This approach moves beyond incremental model improvements to address a systemic issue in the field.\n\nComprehensive Benchmark (UVRB): The creation of UVRB is a major contribution that allows for a much more nuanced and diagnostic evaluation of video retrieval models than was previously possible.\n\nStrong Empirical Results: The GVE model demonstrates superior zero-shot performance across nearly all tasks and domains, validating the effectiveness of the proposed data and training curriculum. The fact that a smaller 3B parameter GVE outperforms larger 7B baselines is particularly compelling."}, "weaknesses": {"value": "Reliance on Synthetic Data: While the synthesis pipeline is sophisticated, it relies on an MLLM captioner. This introduces a potential for model-inherent biases or systematic errors in the training data that may not reflect real-world human annotations. The authors were clearly aware of the \"garbage in, garbage out\" problem. Their V-SynFlow pipeline includes a \"Multi-granular Quality Control\" stage as a first line of defense. This pre-filtering aims to ensure the MLLM captioner starts with a clean, coherent set of videos, reducing the chance of generating nonsensical descriptions. However, this is still automated, not human, validation."}, "questions": {"value": "Regarding the Modality Pyramid curriculum: How sensitive is the task scheduling to the choice of the initial \"prober\" model ($\\Psi_{1}$)? Would starting with a weaker or architecturally different prober (e.g., a CLIP-based model instead of GME-7B) significantly alter the training trajectory?Your finding that partially relevant (PR) retrieval is the best proxy for universal capability is fascinating. Do you have a hypothesis as to why this is the case? Does it require a more robust understanding of semantics to distinguish subtle relevance from complete irrelevance?In your V-SynFlow pipeline, what measures were taken to audit for and mitigate potential factual inaccuracies or hallucinations from the MLLM captioner? Could these artifacts inadvertently penalize models that are better grounded during evaluation?The performance degradation when scaling spatial tokens beyond 400 (Figure 13) is an interesting result. Does this suggest that the vision encoder or the projection layer is not effectively summarizing high-resolution features, or is it more of an attentional issue within the LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PIRcYlM7wL", "forum": "ua5uKZ9Wp0", "replyto": "ua5uKZ9Wp0", "signatures": ["ICLR.cc/2026/Conference/Submission5135/Reviewer_ReuT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5135/Reviewer_ReuT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761506605592, "cdate": 1761506605592, "tmdate": 1762917903125, "mdate": 1762917903125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a universal video retrieval framework along with a new benchmark that encompasses multiple tasks—including textual, composed, and visual retrieval—across various domains such as coarse-grained, fine-grained, and long-context scenarios. It further presents the Universal Video Retrieval Dataset (UVRD) and the General Video Embedder (GVE), which leverages synthetic data for training. The effectiveness of GVE is demonstrated through evaluations on the UVRD benchmark, showing performance improvements over baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper investigates a new Universal Video Retrieval (UVR) task and evaluates the proposed method across a diverse set of benchmarks, demonstrating strong performance relative to existing baseline approaches.\n- The proposed method and architecture are relatively simple in design, yet they prove to be effective across a wide range of video retrieval tasks."}, "weaknesses": {"value": "- While the paper argues that UVRB is a new benchmark, it seems like the benchmark is just a combination of prior works. \n- The distinction between the proposed approach and prior work, such as UNITE, is not clearly articulated, making it difficult to assess the novelty of the contribution.\n- The data generation pipeline should be compared with existing baselines; however, such comparisons are either missing or insufficiently discussed, limiting the understanding of its advantages or uniqueness.\n- The motivation and corresponding evaluation appear somewhat weak. For instance, the paper claims that mastering perceptual primitives first is beneficial; however, this claim is only supported by improvements in final performance. A more carefully designed experimental setup is needed to explicitly validate this hypothesis."}, "questions": {"value": "- Could the authors elaborate on the key differences or advancements introduced in this work?\n- How does this approach differ in design or effectiveness from existing data generation methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7qIS7rmaSz", "forum": "ua5uKZ9Wp0", "replyto": "ua5uKZ9Wp0", "signatures": ["ICLR.cc/2026/Conference/Submission5135/Reviewer_LNxe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5135/Reviewer_LNxe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894690734, "cdate": 1761894690734, "tmdate": 1762917902855, "mdate": 1762917902855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of narrow video retrieval paradigms by proposing a co-designed framework for universal video retrieval. Core contributions include: (1) the UVRB, a suite of 16 datasets covering multi-task and multi-domain scenarios for diagnostic evaluation; (2) V-SynFlow, a scalable data synthesis pipeline generating 1.55M high-quality multi-task training pairs (UVRD); (3) the Modality Pyramid curriculum, which leverages task hierarchies to train the GVE based on Qwen2.5-VL; and (4) extensive experiments showing GVE achieves state-of-the-art zero-shot generalization on UVRB."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Holistic Framework: The central strength is the novel co-design of evaluation, data, and modeling. This holistic approach breaks the cycle of narrow benchmarks leading to specialized models and provides a scalable path forward.\n- Comprehensive Benchmark: The creation of a large-scale, diagnostic benchmark is a significant and lasting contribution that will benefit the entire research community.\n- SOTA Performance: The proposed GVE model demonstrates impressive state-of-the-art performance in a strictly zero-shot setting, validating the effectiveness of the entire framework.\n- Insightful Analysis: The paper goes beyond reporting metrics and provides a deep dive into the dimensional capabilities of different models. The findings on the importance of partially relevant retrieval and the performance divergence between CLIP and MLLM-based architectures are particularly insightful."}, "weaknesses": {"value": "- Narrow Domain Coverage: UVRB does not include specialized domains (e.g., medical, industrial, surveillance), where visual semantics and query intent differ significantly. Extending the benchmark to these domains would enhance generalizability claims."}, "questions": {"value": "How does the Modality Pyramid’s temperature scheduling (σ(t)) affect training dynamics? Are there scenarios where alternative scheduling strategies (e.g., task-specific temperatures) yield better results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R1gCoZZmv2", "forum": "ua5uKZ9Wp0", "replyto": "ua5uKZ9Wp0", "signatures": ["ICLR.cc/2026/Conference/Submission5135/Reviewer_sBTQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5135/Reviewer_sBTQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966947423, "cdate": 1761966947423, "tmdate": 1762917902588, "mdate": 1762917902588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}