{"id": "SBAe5nzF0E", "number": 23280, "cdate": 1758341602873, "mdate": 1759896823030, "content": {"title": "Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs", "abstract": "Solving partial differential equations (PDEs) with machine learning has recently attracted great attention, as PDEs are fundamental tools for modeling real-world systems that range from fundamental physical science to advanced engineering disciplines. Most real-world physical systems across various disciplines are actually involved in multiple coupled physical fields rather than a single field. For example, in 3D integrated circuits (ICs), electrical current injection or electromagnetic wave propagation can induce localized heating, which in turn alters the electromagnetic properties of the embedded components. However, previous machine learning studies mainly focused on solving single-field problems, but overlooked the importance and characteristics of multiphysics problems in real world. Multiphysics PDEs typically entail multiple strongly coupled field quantities, thereby introducing additional complexity and challenges, such as inter-field coupling. Nevertheless, benchmark testing for the application of machine learning in solving multiphysics problems remain largely unexamined. To identify and address the emerging challenges in multiphysics problems, we mainly made three contributions in this work. First, we collect the first general multiphysics dataset, the Multiphysics Bench, which focuses on multiphysics PDE solving with machine learning. Multiphysics Bench is also the most comprehensive multiphysics PDE dataset to date, featuring the broadest range of coupling types, the greatest diversity of multiphysics PDE formulations, and the largest scale of coupled physics data. Second, we conduct the first systematic investigation on multiple representative learning-based PDE solvers, such as Physics-Informed Neural Networks (PINNs), Fourier Neural Operators (FNO), Deep Operator Networks (DeepONet), DiffusionPDE and M2PDE solvers, on multiphysics problems. Unfortunately, naively applying these existing solvers usually shows very poor performance for solving multiphysics. Third, through extensive experiments and discussions, we report multiple insights and a bag of useful tricks for solving multiphysics with machine learning, motivating future directions in the study and simulation of complex, coupled physical systems. Notably, our multiphysics data enables PDE solvers to incorporate more comprehensive physical laws, leading to more accurate solutions to real-world problems.", "tldr": "We present the Multiphysics Bench, the first comprehensive benchmark for evaluating machine learning solvers for multiphysics PDEs, revealing key limitations of existing models while providing actionable insights and tools for future advances.", "keywords": ["Multiphysics", "PDEs", "Benchmark", "Scientific Machine Learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d2badd07cc724bc21fa536bdd42d5db1d9b33f76.pdf", "supplementary_material": "/attachment/83203405efd32d1c00730a421b62e89dc253db4e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the first general multiphysics dataset, the Multiphysics Bench, for PDE solving. It includes six canonical coupled systems (e.g., electro-thermal, thermo-fluid, magneto-hydrodynamic, etc.) generated via finite element simulations. The authors evaluate four representative neural PDE solvers, PINNs, FNO, DeepONet, and DiffusionPDE, on the Multiphysics Bench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, clearly motivated, and easy to follow.\n2. Multiphysics problems are largely underexplored in SciML benchmarks. While many benchmarks exist for single-physics problems, multiphysics systems are far more representative of real-world challenges but have lacked a standardized dataset for evaluation.\n3. The experiments are systematic and provide valuable insights. The failure of general-purpose PDE solvers like FNO stresses the importance of investigating into new PDE solvers for multi-physics PDEs."}, "weaknesses": {"value": "1. Since the Multiphysics Bench's focus is on coupled PDEs, the evaluation would be significantly stronger if it included at least one baseline specifically designed for such coupled systems. One possible solver might be [1].\n2. The paper's primary contribution appears to be the introduction of new datasets. The work would be substantially strengthened by the inclusion of a novel PDE solver tailored to the specific challenges these datasets present.\n3. The citation style is incorrect for ICLR guidelines (\\citep should be used for parenthetical citations). \n\n[1] Xiao, X., Cao, D., Yang, R., Gupta, G., Liu, G., Yin, C., ... & Bogdan, P. (2023). Coupled multiwavelet neural operator learning for coupled partial differential equations. arXiv preprint arXiv:2303.02304."}, "questions": {"value": "1. What do you think is the difference between coupled PDE in a single-physics domain compared to multi-physics one?\n2. The authors conduct experiments on the 'Complete vs. Incomplete Physical Priors' section to prove the significance of understanding coupling PDEs instead of focusing on the sliced PDEs. However, the distinction between complete and incomplete prior isn't that obvious. Could authors provide stronger proof about the importance of physcial priors here? And also, could the authors provide an explanation on why DeepONet works better with incompete physical prior."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DHYeWFo8Mb", "forum": "SBAe5nzF0E", "replyto": "SBAe5nzF0E", "signatures": ["ICLR.cc/2026/Conference/Submission23280/Reviewer_rneb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23280/Reviewer_rneb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761532401578, "cdate": 1761532401578, "tmdate": 1762942588043, "mdate": 1762942588043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a multi-physics benchmark dataset. \n\nThe paper describes $6$ paired tasks and evaluates the performance with $4$ models. \n\nThe paper also demonstrates that the current models' performance may suffer when naively applying the method to this dataset and proposes two strategies to alleviate the problem. \n\nThe paper provides data and the script to generate the data, and the baseline code (at least planned)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "A new problem that has practical implications for modeling multi-physics systems."}, "weaknesses": {"value": "It may not be of general interest."}, "questions": {"value": "A fundamental question is if this class of problems can be reduced to a single PDE and the conditions under which it is possible to do this. \n\nAnother interesting question is that if we had a unique PDE, what would make a problem more difficult to solve?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sjFZWFUayv", "forum": "SBAe5nzF0E", "replyto": "SBAe5nzF0E", "signatures": ["ICLR.cc/2026/Conference/Submission23280/Reviewer_6qbz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23280/Reviewer_6qbz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925383442, "cdate": 1761925383442, "tmdate": 1762942587667, "mdate": 1762942587667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel benchmark (Multiphysics Bench) for evaluating Scientific Machine Learning methods applied to coupled multiphysics Partial Differential Equations (PDEs), consisting of six canonical coupling problems that cover diverse scenarios like bidirectional/unidirectional coupling and both steady-state/frequency-domain and transient dynamics. A robust baseline is established by rigorously testing four machine learning approaches: Physics-Informed Neural Networks (PINNs), Fourier Neural Operators (FNO), DeepONet, and DiffusionPDE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper systematically focuses on the critical and complex domain of coupled multiphysics PDEs. The inclusion of diverse and challenging coupling types (bidirectional/unidirectional, equation/parameter-level) demonstrates an original and comprehensive problem formulation that accurately reflects real-world engineering and science.\n2. The authors use four leading learning-based PDE solvers (PINN, FNO, DeepONet, and DiffusionPDE) on all six problems\n3. The problems are deliberately chosen to expose specific technical weaknesses of current models, such as gradient inconsistencies in PINNs and mode collapse in FNOs when applied to coupled systems.\n4. The generation of the dataset, particularly its size ($10^4$ training samples) and the complexity of the output fields (e.g., 12 output channels for Acoustic-Structure coupling), represents a significant benchmark for training modern learning-based PDE solvers."}, "weaknesses": {"value": "1. The paper needs to explain why the relative $L_2$ error performance of baseline models remains unchanged (or nearly unchanged) despite a significant increase in the data scale (number of training samples) in Table 4.\n2. The paper correctly identifies various failure modes (e.g., FNO mode collapse, PINN gradient inconsistency), but the justification is often descriptive rather than quantitatively analytical."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8jIEcHBdKf", "forum": "SBAe5nzF0E", "replyto": "SBAe5nzF0E", "signatures": ["ICLR.cc/2026/Conference/Submission23280/Reviewer_izj7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23280/Reviewer_izj7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950035640, "cdate": 1761950035640, "tmdate": 1762942587408, "mdate": 1762942587408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Multiphysics Bench, a dataset and benchmark suite for evaluating SciML PDE solvers on multiphysics problems. It consists FEM-generated data for a collection of six multiphysics problem (electro-thermal, thermo-fluid, electro-fluid, magneto-hydrodynamic, acoustic–structure, mass-transport–fluid). The paper also evaluates four classes of SciML solvers (PINNs, FNO, DeepONet, DiffusionPDE), reports empirical findings, and suggests a couple of \"tricks\" to help improve performance.\n\nWhile the paper presents a solid engineering effort, it offers limited scientific insights or conceptual contribution. It finds that SciML tools behave the same way on larger, coupled PDEs, which is interesting to note, but it neither advances our understanding of the unique challenges presented by multiphysics over single-physics problems for SciML methods, nor offer any deeper insights into the generalization of SciML methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Multi physics problems are widely present in the real-world. \n\nhighly Creating a standardized, multi-scenario multiphysics benchmark is valuable for the SciML community; the chosen scenarios are relevant to real applications (electronics, fluid/thermal systems, acoustics, porous-media transport).\n- Many different PDEs are included in the benchmark with different initial and boundary conditions, although there are a few omissions.\n- Evaluation of four major solver families (PINNs, DeepONet, FNO, DiffusionPDE), although there are a few omissions. The paper also reports many metrics (RMSE, relative L2, MaxError, etc.)\n- The paper alludes to practical issues such as imbalance of residual magnitudes and degradation in transient tasks and suggests remedies like quantile normalization and auto-balanced weighting."}, "weaknesses": {"value": "Major Weakness: The scientific contribution of the paper is unclear.  \n\n- The multiphysics benchmark dataset does not focus on what sets multiphysics problems apart from single-physics problems (cross-field interactions, different spatio/temporal scales across fields, etc.). Simply measuring global reconstruction error does not inform us why SciML methods might fail on multiphysics problems or how to fix them? \n\n- The empirical takeaway is essentially that existing SciML models for PDEs generalize to multiphysics problems about as well as they do for single physics problems. This is neither surprising not theoretically illuminating. In fact, it suggests that the multiphysics coupling is weak or moderate. So, the benchmark does not isolate multiphysics difficulty.\n\n- The benchmark currently emphasizes steady-state (frequency-domain) systems, and has only one transient case. So, the temporal diversity is limited and excludes a class of time-dependent or dynamically unstable multiphysics systems that are highly relevant in practice. So, the benchmark does not fully capture the challenges of transient coupling, time-scale stiffness, or nonlinear feedback dynamics, which are common in many real-world multiphysics problems.\n\nOther Weaknesses:\n\n- SciML methods are known to be very brittle and sensitive to hyperparameters (see [1]). The empirical results in the paper may not be reliable. There are no confidence intervals in the majority of the results (except Figure 9).\n\n- The abstract and the introduction point to insights and \"bag of tricks\". These insights do not appear to be different from single physics scenarios, and the \"bag of tricks\" are not substantial enough to be included in the main paper.\n\n- The benchmarks do not capture real-world operational challenges, where measurements are noisy, PDE parameters are not known exactly, PDEs are approximations of the underlying physical phenomenon. The utility of evaluating on a benchmark that does not incorporate the such real-world challenges is unclear.\n\n[1] McGreivy, Nick, and Ammar Hakim. \"Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations.\" Nature Machine Intelligence 6, no. 10 (2024): 1256-1269."}, "questions": {"value": "- How sensitive are the SciML methods to hyperparameter tuning? Would deeper FNOs, or PINNs with different loss weights change the conclusions?\n\n- Do any of the SciML methods violate conservation laws in multiphysics problems? If yes, can the violation be quantified?\n\n- Do PINN residuals overfit to one physics and underfit to another?\n\n- How would foundation models for PDEs like Poseidon [2] perform?\n\n- There is a claim in the introduction: \"By aligning computational solvers more closely with the governing physical laws, MultiphysicsBench provides a robust foundation for developing models that are more accurate, resilient, and generalizable—paving the way for future advances in simulating complex, coupled physical systems.\" This is not apparent to the reviewer. Can you elaborate how the contributions of the paper aligns with this claim?\n\n[2] Poseidon: Efficient Foundation Models for PDEs, NeurIPS 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vcKnt4wGah", "forum": "SBAe5nzF0E", "replyto": "SBAe5nzF0E", "signatures": ["ICLR.cc/2026/Conference/Submission23280/Reviewer_fhMJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23280/Reviewer_fhMJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112885559, "cdate": 1762112885559, "tmdate": 1762942587127, "mdate": 1762942587127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Multiphysics Bench, a dataset and benchmark suite for evaluating SciML PDE solvers on multiphysics problems. It consists FEM-generated data for a collection of six multiphysics problem (electro-thermal, thermo-fluid, electro-fluid, magneto-hydrodynamic, acoustic–structure, mass-transport–fluid). The paper also evaluates four classes of SciML solvers (PINNs, FNO, DeepONet, DiffusionPDE), reports empirical findings, and suggests a couple of \"tricks\" to help improve performance.\n\nWhile the paper presents a solid engineering effort, it offers limited scientific insights or conceptual contribution. It finds that SciML tools behave the same way on larger, coupled PDEs, which is interesting to note, but it neither advances our understanding of the unique challenges presented by multiphysics over single-physics problems for SciML methods, nor offer any deeper insights into the generalization of SciML methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Multi physics problems are widely present in the real-world. \n- Creating a standardized, multi-scenario multiphysics benchmark is valuable for the SciML community; the chosen scenarios are relevant to real applications (electronics, fluid/thermal systems, acoustics, porous-media transport).\n- Many different PDEs are included in the benchmark with different initial and boundary conditions, although there are a few omissions.\n- Evaluation of four major solver families (PINNs, DeepONet, FNO, DiffusionPDE), although there are a few omissions. The paper also reports many metrics (RMSE, relative L2, MaxError, etc.)\n- The paper alludes to practical issues such as imbalance of residual magnitudes and degradation in transient tasks and suggests remedies like quantile normalization and auto-balanced weighting."}, "weaknesses": {"value": "Major Weakness: The scientific contribution of the paper is unclear.  \n\n- The multiphysics benchmark dataset does not focus on what sets multiphysics problems apart from single-physics problems (cross-field interactions, different spatio/temporal scales across fields, etc.). Simply measuring global reconstruction error does not inform us why SciML methods might fail on multiphysics problems or how to fix them? \n\n- The empirical takeaway is essentially that existing SciML models for PDEs generalize to multiphysics problems about as well as they do for single physics problems. This is neither surprising not theoretically illuminating. In fact, it suggests that the multiphysics coupling is weak or moderate. So, the benchmark does not isolate multiphysics difficulty.\n\n- The benchmark currently emphasizes steady-state (frequency-domain) systems, and has only one transient case. So, the temporal diversity is limited and excludes a class of time-dependent or dynamically unstable multiphysics systems that are highly relevant in practice. So, the benchmark does not fully capture the challenges of transient coupling, time-scale stiffness, or nonlinear feedback dynamics, which are common in many real-world multiphysics problems.\n\nOther Weaknesses:\n\n- SciML methods are known to be very brittle and sensitive to hyperparameters (see [1]). The empirical results in the paper may not be reliable. There are no confidence intervals in the majority of the results (except Figure 9).\n\n- The abstract and the introduction point to insights and \"bag of tricks\". These insights do not appear to be different from single physics scenarios, and the \"bag of tricks\" are not substantial enough to be included in the main paper.\n\n- The benchmarks do not capture real-world operational challenges, where measurements are noisy, PDE parameters are not known exactly, PDEs are approximations of the underlying physical phenomenon. The utility of evaluating on a benchmark that does not incorporate the such real-world challenges is unclear.\n\n[1] McGreivy, Nick, and Ammar Hakim. \"Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations.\" Nature Machine Intelligence 6, no. 10 (2024): 1256-1269."}, "questions": {"value": "- How sensitive are the SciML methods to hyperparameter tuning? Would deeper FNOs, or PINNs with different loss weights change the conclusions?\n\n- Do any of the SciML methods violate conservation laws in multiphysics problems? If yes, can the violation be quantified?\n\n- Do PINN residuals overfit to one physics and underfit to another?\n\n- How would foundation models for PDEs like Poseidon [2] perform?\n\n- There is a claim in the introduction: \"By aligning computational solvers more closely with the governing physical laws, MultiphysicsBench provides a robust foundation for developing models that are more accurate, resilient, and generalizable—paving the way for future advances in simulating complex, coupled physical systems.\" This is not apparent to the reviewer. Can you elaborate how the contributions of the paper aligns with this claim?\n\n[2] Poseidon: Efficient Foundation Models for PDEs, NeurIPS 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vcKnt4wGah", "forum": "SBAe5nzF0E", "replyto": "SBAe5nzF0E", "signatures": ["ICLR.cc/2026/Conference/Submission23280/Reviewer_fhMJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23280/Reviewer_fhMJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112885559, "cdate": 1762112885559, "tmdate": 1763102619607, "mdate": 1763102619607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}