{"id": "qlS7iZ6pXk", "number": 10608, "cdate": 1758177285980, "mdate": 1759897640900, "content": {"title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents", "abstract": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward agentic applications, where web browsing capabilities are fundamental for retrieving information from diverse online sources. However, existing open-source web agents either demonstrate limited information-seeking abilities on complex tasks or lack transparent implementations. In this work, we identify that the key challenge lies in the scarcity of challenging data for information seeking. To address this limitation, we introduce WebExplorer: a systematic data generation approach using model-based exploration and iterative, long-to-short query evolution. This method creates challenging query-answer pairs that require multi-step reasoning and complex web navigation. By leveraging our curated high-quality dataset, we successfully develop advanced web agent WebExplorer-8B through supervised fine-tuning followed by reinforcement learning. Our model supports 128K context length and up to 100 tool calling turns, enabling long-horizon problem solving. Across diverse information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able to effectively search over an average of 16 turns after RL training, achieving higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best performance among models up to 100B parameters on WebWalkerQA and FRAMES. Beyond these information-seeking tasks, our model also achieves strong generalization on the HLE benchmark even though it is only trained on knowledge-intensive QA data. These results highlight our approach as a practical path toward long-horizon web agents.", "tldr": "We propose WebExplorer, a new approach for synthesizing high-quality training data to develop advanced long-horizon web agents.", "keywords": ["web agents", "reasoning", "reinforcement learning", "large language model", "search", "evolution"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35daf2c2596dd676f3f7cf213e9cdc59d188958e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "WebExplorer proposes a prompting-plus-RL framework to train web-browsing agents capable of multi-hop reasoning and long-horizon search. The key idea is to synthesize challenging web-based QA trajectories by (1) exploring the web from seed entities and (2) evolving the resulting query via a long-to-short obfuscation process that increases reasoning depth and ambiguity. These evolved QA pairs form the training datasets, which is then used for SFT and reinforcement learning of the 8B agent."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces a two-stage prompting pipeline (explore + evolve) to generate synthetic, high-difficulty web QA data without manual labeling, which I believe could be suitable for large-scale web agent training for different domains.\n2. Produces long-horizon training dataset for the community beyond the benchmarks for evaluation. In particular, the resulting QA pairs represent real-world web reasoning challenges.\n3. The motivations and empirical results are aligned. The drafting of this paper is clear."}, "weaknesses": {"value": "1. The paper focuses primarily on QA-style web reasoning tasks (e.g., BrowseComp, WebWalkerQA, HLE), omitting broader benchmarks like WebArena [1], which test action execution and task planning. This makes it unclear how well WebExplorer generalizes beyond question answering tasks.\n2. There is no contribution towards the training part. RL phase (GRPO) is described briefly, without details on reward formulation, training stability.\n\n[1] Zhou, Shuyan, et al. \"WebArena: A Realistic Web Environment for Building Autonomous Agents.\" The Twelfth International Conference on Learning Representations. 2024"}, "questions": {"value": "1. How are the Wikipedia and BrowseComp seed entities selected? Are they balanced across domains (e.g., science, culture, current events)?\n\n2. Was there any automatic or human validation to ensure factual correctness and optimal solution of the generated agent web search process?\n\n3. The distribution of tool-call usage across Initial QA, Evolved QA, and BrowseComp closely resembles the visualization style and rationale presented in Figure 3 of [2]. Given this similarity, it is not clear why SailorFog-QA is not included for comparison within this figure.\n\n[2] Li, Kuan, et al. \"WebSailor: Navigating Super-human Reasoning for Web Agent.\" arXiv preprint arXiv:2507.02592 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BzBb4c5Z6n", "forum": "qlS7iZ6pXk", "replyto": "qlS7iZ6pXk", "signatures": ["ICLR.cc/2026/Conference/Submission10608/Reviewer_qJ6U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10608/Reviewer_qJ6U"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761526294675, "cdate": 1761526294675, "tmdate": 1762921872373, "mdate": 1762921872373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposes WebExporer, a prompting-based framework to synthesize hard and long-horizon web QA data for training browsing agents. \nThe proposed method first performs model-based exploration: start from a seed entity, let an LLM search and browse iteratively, and collect an information. Then progressively remove salient clues and obfuscate specifics (dates, names, locations) so the final query has a unique answer but no obvious search entry point.\nUsing ~40K such evolved QA pairs, they SFT and then RL-train an 8B model (on top of Qwen3-8B) with 128K context and up to 100 tool calls, which beats or matches much larger open-source web agents.\nThe authors argue that better synthetic data is more important than model size for long-horizon browsing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is easy to follow and the presentation is clear\n- The idea is straightforward and is well motivated to address the issues in prior works\n- The constructed data and trained model will be valuable for the community to build upon"}, "weaknesses": {"value": "- One limitation is that the claimed “answer remains unique after evolution” is only justified in a limited setting. The method deliberately removes or obfuscates the anchors (names, dates, teams, offices) that guarantee disambiguation, so correctness ultimately depends on a specific crawl, retrieval ranking, and the LLM judge. The real web is broader and changes over time, a later or different crawl could surface multiple plausible answers and render the claim invalid.\n- A second limitation is the lack of isolating ablations. Table 1 and Fig 3 show that the evolution step makes the queries harder, the paper jointly changes several factors including exploration, the long to short rewriting procedure, large-context SFT, RL/GRPO, and a fixed two-tool browsing scaffold. With all these we see the final gains in Table 2, where the best WebExplorer-8B outperforms larger models. Without disentangling these components, the current evidence supports the effectiveness of the overall pipeline, but not the evolution step itself as the decisive contributor.\n- Analysis on the data generation framework is also a bit lacking (e.g., effect on tuning the number of exploration steps)\n\nOverall, the idea is clear and seems promising, but the ablation and analysis seems to be more lacking to fully support the claims."}, "questions": {"value": "- How often does an evolved question has more than one plausible answer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mxAV3F5RX0", "forum": "qlS7iZ6pXk", "replyto": "qlS7iZ6pXk", "signatures": ["ICLR.cc/2026/Conference/Submission10608/Reviewer_C5QJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10608/Reviewer_C5QJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969118214, "cdate": 1761969118214, "tmdate": 1762921871969, "mdate": 1762921871969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes WebExplorer, a framework for generating large-scale, long-horizon, information-seeking datasets and training web agents. The key idea is to perform model-based exploration and long-to-short query evolution, where complex queries are iteratively simplified while preserving their reasoning dependencies. The synthesized data is then used to train WebExplorer-8B, an open model fine-tuned with supervised data and optimized with GRPO-based reinforcement learning. Experiments on multiple benchmarks (BrowseComp, WebWalkerQA, FRAMES) show that WebExplorer-8B outperforms larger open baselines such as WebSailor-72B and achieves strong results in both English and Chinese settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Scalable data synthesis framework. The long-to-short query evolution process provides an efficient way to generate reasoning-intensive, multi-hop search tasks without manual annotation. It is practical and broadly applicable to information-seeking agents.\n\n- Solid empirical performance. WebExplorer-8B achieves competitive or state-of-the-art results among similarly sized models on several benchmarks. The ablations confirm the contribution of both query evolution and RL fine-tuning."}, "weaknesses": {"value": "- Narrow agent capability relative to “web agent” framing. The proposed system primarily performs search-based QA or URL-specific retrieval, lacking genuine web interaction capabilities such as clicking, typing, or navigation through multi-step actions. The title and framing overstate its scope as a “web agent” system but it only focuses on the information-seeking subtype of a web agent. As such, it aligns more with Search-R1-style search agents than interactive web agents as those proposed in previous works (e.g., MiniWoB, WebShop, WebArena, WebVoyager, Mind2Web, WebRL). \n\n- Missing discussion of major related works. The paper overlooks a line of important prior efforts on interactive web agents, as well as recent data generation pipelines for search agent (e.g., ZeroSearch). Without contextualizing these works, the contribution appears less distinctive.\n\n- Limited novelty and unclear generalization. The pipeline (prompt-based data synthesis + SFT + RL via GRPO) follows an established pattern in reasoning and web-agent research. Moreover, most benchmarks resemble the data construction process, raising concerns about distribution overlap. Broader evaluations on truly unseen or interactive environments would better support generalization claims."}, "questions": {"value": "- Could the authors clarify the action boundaries of WebExplorer—does it support any interactive web behavior (e.g., navigating the web) beyond search/RAG-like QA?\n\n- Would extending the framework to multi-step interaction tasks (e.g., form filling or navigation) be feasible under the same data synthesis paradigm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hdBFT5QQ5g", "forum": "qlS7iZ6pXk", "replyto": "qlS7iZ6pXk", "signatures": ["ICLR.cc/2026/Conference/Submission10608/Reviewer_Aau4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10608/Reviewer_Aau4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977445877, "cdate": 1761977445877, "tmdate": 1762921871565, "mdate": 1762921871565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a method for training deep research agents. A key component of the proposed method is the data synthesis. Authors improve upon previous methods for data synthesis by leveraging an LLM agent with search and browse tools to generate initial queries, rather than relying on complex graph-based logic or prompts as with previous work. When combined with the standard recipe of SFT + GRPO, the resulting data achieves an impressive performance for a 8B model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Quality:\nThe empirical results are strong. WebExplorer-8B significantly outperforms comparable 7B/8B models and even rivals larger 32–72B models on BrowseComp-en/zh, GAIA, FRAMES, and HLE. The ablation on the evolution step (Table 1) shows that the obfuscation process effectively increases task difficulty.\n\nClarity:\nThe synthesis prompts and tool schemas are well documented, offering a transparent view of the pipeline. However, the paper omits which base LLM was used to perform the synthesis, which is a key detail. Assuming this is clarified in the final version, the work provides a useful recipe for building web-based deep research datasets, which will allow subsequent research to build upon.\n\nSignificance and Originality:\nThe idea of using web agent for query synthesis is a novel idea, although there are concurrent work such as https://openreview.net/forum?id=ExBGKdOBo8 which explore the similar idea. Note that while authors claim the obfuscation step as their own contribution in line 087, Gao et al (2025) paper authors cite (ASearcher) also includes the Fuzz step which does similar obfuscations."}, "weaknesses": {"value": "While the system performs impressively, the conceptual novelty is modest. The core ideas - ReAct-style trajectories, GRPO training, and query obfuscation - are largely adopted or lightly adapted from existing frameworks such as ASearcher (Gao et al., 2025). The technical depth is therefore limited. Although developing a highly performant model requires many judicious design choices, it is somewhat unclear whether this work will lead the community to rethink the underlying problem."}, "questions": {"value": "In line 294, authors discuss that BrowseComp-en data are used as exemplars for data synthesis. Although only three QA pairs are used, wouldn't this introduce some contamination to the benchmark data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xXMxsLz7Ew", "forum": "qlS7iZ6pXk", "replyto": "qlS7iZ6pXk", "signatures": ["ICLR.cc/2026/Conference/Submission10608/Reviewer_tZ9e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10608/Reviewer_tZ9e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980514067, "cdate": 1761980514067, "tmdate": 1762921871152, "mdate": 1762921871152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}