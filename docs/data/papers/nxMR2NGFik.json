{"id": "nxMR2NGFik", "number": 10343, "cdate": 1758167697554, "mdate": 1759897657226, "content": {"title": "LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics", "abstract": "Machine unlearning for large language models (LLMs) aims to remove undesired data, knowledge, and behaviors (e.g., for safety, privacy, or copyright) while preserving useful model capabilities. Despite rapid progress over the past two years, research in LLM unlearning remains fragmented, with limited clarity on what constitutes effective unlearning and how it should be rigorously evaluated. In this work, we present a principled taxonomy of twelve recent stateful unlearning methods, grouped into three methodological families: divergence-driven optimization, representation misalignment, and rejection-based targeted unlearning. Building on this taxonomy, we revisit the evaluation of unlearning effectiveness (UE), utility retention (UT), and robustness (Rob), focusing on the WMDP benchmark. Our analysis shows that current evaluations, dominated by multiple-choice question (MCQ) accuracy, offer only a narrow perspective, often overstating success while overlooking the model’s actual generation behavior. To address this gap, we introduce open question-answering (Open-QA) metrics that better capture generative performance and reveal the inherent UE–UT tradeoff across method families. Furthermore, we demonstrate that robustness requires finer-grained analysis: For example, vulnerabilities differ substantially between in-domain relearning and out-of-domain fine-tuning, even though both fall under model-level attacks. Through this study, we hope to deliver a full-stack revisit of LLM unlearning and actionable guidance for designing and evaluating future methods.", "tldr": "", "keywords": ["Machine Unlearning", "Evaluation", "Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d0ed52f6013d07be9670761dda649c957fc97ed2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a comprehensive study of unlearning in large language models. The authors first propose a taxonomy that categorizes existing unlearning strategies into three main families: divergence-driven optimization, representation misalignment, and rejection-based targeted unlearning. They then show that common multiple-choice (MCQ) evaluations fail to reflect true model forgetting and utility retention, and introduce Open-QA–style metrics (such as entailment scoring) to capture generative behavior more faithfully. Finally, the paper conducts thorough robustness assessments under various post-unlearning attacks, including in-domain relearning, out-of-domain fine-tuning, quantization, and jailbreak prompts. The study concludes with insights on trade-offs between effectiveness, utility, and robustness, offering guidance for future unlearning designs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Strength 1**: The paper proposes a structured taxonomy of twelve recent LLM unlearning approaches, neatly grouping them into three methodological families. This taxonomy is well motivated and useful for organizing a fast-moving area with fragmented prior work.\n\n**Strength 2**: The paper provides a structured and clear examination of robustness across multiple threat models, including model-level attacks (relearning, out-of-domain fine-tuning, quantization) and input-level jailbreak prompts. The visualizations in Figures 2–4 effectively highlight cross-method differences and make robustness trade-offs and vulnerabilities easy to interpret."}, "weaknesses": {"value": "**Weakness 1**: The paper claims to provide a “full-stack” taxonomy and evaluation perspective on LLM unlearning. However, several directly relevant surveys and evaluation studies are missing in the related work and positioning, such as Wang et al.(2024) and Blanco-Justicia et al. (2025). Since the contribution of this work heavily relies on its claim to offer a uniquely systematic overview, the absence of these discussions weakens the novelty.\n\n**Weakness 2**: Given the paper’s emphasis on distinguishing true unlearning from model degradation, the absence of standard behavioral diagnostics such as perplexity, KL divergence, or exact memorization weakens the evidence for its UE–UT trade-off claims. Incorporating at least one of these metrics would help confirm that observed performance changes reflect targeted forgetting rather than general model collapse.\n\n**Weakness 3**: The main empirical claims are derived almost entirely from one benchmark (WMDP-Bio) and one model architecture (Llama-3-8B). Without broader validation across other standard unlearning settings (e.g., TOFU, RWKU) or larger-scale models, it remains unclear whether the reported method-family tradeoffs and robustness patterns generalize.\n\n[1] Wang, Q., Han, B., Yang, P., Zhu, J., Liu, T., & Sugiyama, M. (2024). Towards effective evaluations and comparisons for llm unlearning methods. arXiv preprint arXiv:2406.09179.\n[2] Blanco-Justicia, A., Jebreel, N., Manzanares-Salor, B., Sánchez, D., Domingo-Ferrer, J., Collell, G., & Eeik Tan, K. (2025). Digital forgetting in large language models: A survey of unlearning methods. Artificial Intelligence Review, 58(3), 90."}, "questions": {"value": "Question 1: Are there plans or recommendations for extending the evaluation to other datasets, or to different LLM architectures? Any preliminary results would increase confidence in generality.\n\nQuestion 2: Can the authors clarify the sampling strategy for selecting (x, y) pairs in NPO/SimNPO and RMU training? Specifically, are forget samples drawn uniformly or in a class-balanced manner, and are rare forget types specially treated? Clearer description would improve reproducibility and confidence in the fairness of comparisons.\n\nQuestion 3: To better understand the reliability of the Open-QA evaluation, could the authors clarify how they verify the correctness of reference answers used in the entailment-score calculation, and whether this automatic metric shows reasonable agreement with human judgments, including any known cases where it may misjudge outputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KWS2fZkcoP", "forum": "nxMR2NGFik", "replyto": "nxMR2NGFik", "signatures": ["ICLR.cc/2026/Conference/Submission10343/Reviewer_VkyJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10343/Reviewer_VkyJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539704633, "cdate": 1761539704633, "tmdate": 1762921676616, "mdate": 1762921676616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive analysis of llm unlearning, offering a full-stack perspective that integrates methodological categorization, evaluation, and robustness. The authors identify the fragmentation and inconsistency in prior unlearning research and propose a taxonomy of twelve stateful unlearning methods.\n\nThe paper further reevaluates unlearning effectiveness (UE) and utility retention (UT), highlighting that popular benchmarks, such as WMDP, rely heavily on multiple-choice question (MCQ) format—an approach that overlooks the generative capabilities of LLMs. To address this, the authors advocate introducing open question answering (Open-QA) as a complementary evaluation lens. Open-QA captures the model’s actual generation ability and exposes hidden trade-offs, such as over-forgetting in divergence-driven methods that degrade utility beyond MCQ assessments.\n\nThe work also examines robustness against both model-level attacks (e.g., in-domain relearning, out-of-domain fine-tuning, quantization) and input-level attacks (e.g., prompt jailbreaking). Results reveal nuanced vulnerabilities: divergence-driven optimization methods resist relearning attacks better, whereas representation misalignment methods handle out-of-domain fine-tuning more robustly. The paper further shows that combining unlearning with robust optimization frameworks (e.g., SAM, IRM) improves resilience across threat types."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The authors are obviously the experts in the field. The draft is well structured, insightful, and easy to read. The paper is a concrete and precise summary of the current progress.\n\nSome statements are interesting, such as divergence-driven optimization methods are typically more resilient to in-domain relearning, while representation misalignment methods better withstand out-of-domain fine-tuning."}, "weaknesses": {"value": "As the first method to address unlearning and retention trade-off, I think GRU [1] need to be mentioned in Sec 3.  \n\n[1] GRU: Mitigating the Trade-off between Unlearning and Retention for Large Language Models\n\nIt somehow likes a surveying paper, maybe it is useful to discuss its difference and uniqueness with previous works, like [2].\n\n[2] Rethinking Machine Unlearning for Large Language Models\n\nMetrics are proposed beyond MCQ and Open-QA, such as those test the likelihood in generating the original responses. Forgive me if I made some mistakes, [3] suggests many metrics of this kind and states ES (same name as in this paper but actually different formulation) as a robust and reliable metric. I do not know if it should be mentioned for the overall concreteness. Also, Open-QA seems not new, but it seems that the authors  do not further sufficient references. \n\n\n[3] OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics\n\nA general problem I want to discuss with the authors is that, under the current common unlearning setup, do you think WMDP is strictly reliable? It seems that we will typically implicitly assume that the knowledge has been parameterized into the model, so, can we ensure that the knowledge in WMDP will exist in the considered models? \n\nI cannot guarantee it is correct, but some researchers in this field told me RMU only works well for WMDP while being hard to tuning in achieving proper performance for other benchmarks. Therefore, the claim “representation misalignment generally outperforms rejection-based targeted unlearning“ is too strong and the conclusion is only constrained in the considered experimental setups. \n\nSome paper discussed the drawbacks of divergence-based optimization methods in wrong reweighting [4], maybe it can be mentioned as a simple explanation about why over-forgetting. [4] also mentioned the drawback of rejection based method, stating something like mapping to new targets does not mean old knowledge is overwrite. Not sure if they should be mentioned for concreteness. \n\n[4] Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go Beyond\n\nPersonally, I think cross-language attack is also interesting, as it can reflect if the underlying knowledge has been removed or we just make the model to refuse outputting particular form of responses. Do the authors think it should be mentioned?\n\n\nMaybe some attentions are required to explain why this paper is useful in more directly advancing the field. As a very bias and personal understanding about the tastes of the current community, open challenges as well as some potential solutions (and preliminary verifications) are required for acceptance. It is just a minor suggestion, I do not expect that the authors will come up with some new methodologies during rebuttal, but at least a summary of the current open question is required. Overall, this is a good paper that summarizes the current progress, I will definitely change my scores when the authors address my concerns."}, "questions": {"value": "Kindly please see the drawbacks above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0UgcTfZG2Y", "forum": "nxMR2NGFik", "replyto": "nxMR2NGFik", "signatures": ["ICLR.cc/2026/Conference/Submission10343/Reviewer_dRnP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10343/Reviewer_dRnP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761590164168, "cdate": 1761590164168, "tmdate": 1762921675603, "mdate": 1762921675603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a “full-stack” re-examination of LLM unlearning. It organizes twelve representative methods into three families—(i) divergence-driven optimization (e.g., NPO, SimNPO, robustified with SAM/IRM), (ii) representation misalignment (e.g., RMU, RR, TAR/LAT), and (iii) rejection-based targeted unlearning (e.g., ELM, DPO, IDK+AP)—and proposes a unified evaluation protocol spanning multiple-choice (MCQ) and open-ended generation (OpenQA). Unlearning effectiveness $\\mathrm{UE}$ and utility retention $\\mathrm{UT}$ are defined for both regimes; for OpenQA, outputs are judged via an NLI-based entailment score (ES) against a reference answer. Aggregate views $\\mathrm{UE}\\_{\\mathrm{Avg}}$ and $\\mathrm{UT}\\_{\\mathrm{Avg}}$ summarize overall behavior.\n\nOn WMDP (notably WMDP-Bio) with Llama-3-8B-Instruct as the base model, the study highlights a systematic gap: divergence-driven methods can match representation-misalignment methods on $\\mathrm{UE}\\_{\\mathrm{MCQ}}$ yet suffer marked drops in $\\mathrm{UT}\\_{\\mathrm{OpenQA}}$, revealing over-forgetting that MCQ alone obscures. Robustness is examined under in-domain relearning, out-of-domain fine-tuning, post-unlearning quantization and jailbreaks; a practical recipe shows a DPO warm-start mitigating the utility collapse of IDK+AP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work tightly couples method taxonomy, metric design, and robustness analysis in one coherent framework. Pairing MCQ and OpenQA via $\\mathrm{UE}$ and $\\mathrm{UT}$ surfaces failure modes that MCQ alone cannot detect; the NLI-based ES moves the evaluation closer to realistic generative behavior. \n\n\nThe WMDP findings are crisply interpreted: divergence-driven methods may excel on $\\mathrm{UE}\\_{\\mathrm{MCQ}}$ yet degrade $\\mathrm{UT}\\_{\\mathrm{OpenQA}}$, and the logits inspection provides a concrete mechanism rather than post-hoc speculation. Robustness is thoughtfully scoped—separating in-domain relearning from out-of-domain fine-tuning, diagnosing quantization-induced capacity loss, and linking jailbreak susceptibility to training choices. The DPO warm-start for IDK+AP is practical and immediately adoptable without architectural changes.\n\nI believe the most significant advantage of this paper lies in integrating the robustness of the model layer (intra-domain relearning, out-domain fine-tuning, quantization) with the input layer (jailbreaking) into a unified evaluation framework."}, "weaknesses": {"value": "Headline trends are shown on a single base model (Llama-3-8B-Instruct). Whether the family-level narrative (e.g., representation-misalignment methods being more resilient to out-of-domain fine-tuning; divergence-driven methods being more vulnerable in OpenQA) holds across architectures (Gemma/Qwen) and scales (2B/14B) remains uncertain. Some studies have pointed out that different model sizes have certain influences and patterns on unlearning.\n\nThe OpenQA ES hinges on two choices under-specified in the main text: the reference answer policy (dataset ground truth vs. the original pre-unlearned model) and the NLI judge. Either can shift $\\mathrm{UE}\\_{\\mathrm{OpenQA}}$ and $\\mathrm{UT}\\_{\\mathrm{OpenQA}}$ and potentially flip method rankings; scorer variance/calibration and human agreement are not reported.\n\nRobustness coverage, though broad, uses a narrow jailbreak family and a limited set of quantization bit-widths/schemes. I think that having a unified \"budget\" that coordinates computational cost, number of steps, and hyperparameter search range across different methods would help improve fairness and reproducibility across algorithm families."}, "questions": {"value": "1) Can the core trends—MCQ/OpenQA divergence in the $\\mathrm{UE}$–$\\mathrm{UT}$ trade-off, family-specific robustness profiles, and the NPO logits-flattening effect—be reproduced on at least two **additional architectures** (e.g., Gemma/Qwen) and **different scale** (e.g., 2B and 13B)? A compact architecture × size matrix would establish external validity.\n\n\n2) What puzzles me is that in Appendix A (Experimental Setup), the authors explicitly state that ES uses NLI to determine the implication relationship between the model output and the ground truth (benchmark annotation) (premise = model output, hypothesis = benchmark answer) and scores accordingly. However, in Section 4 of the main text (Open-QA as an important perspective for UE/UT evaluation), it states that ES measures the consistency between the model output and the original (unforgotten) model response (i.e., the \"correct answer\"). This is because if the original model output is considered the \"correct answer,\" then when the original model itself is flawed, ES will incorrectly classify the output that \"corrects towards the truth\" as wrong; using the benchmark annotation as a reference will not. Since one of the most important arguments in this paper is that \"Divergence-driven approaches are more prone to over-forgetting in Open-QA and representation misalignment is more stable,\" and **these conclusions rely on the ES metric of Open-QA**, if switching the reference source leads to a reversal in UE/UT rankings, the robustness of the main conclusions will be compromised. How should this conflict be handled? Could you report the consistency of conclusions reached under the two references? If possible, use multiple NLI judges (≥2) and a human-labeled subset to estimate ES variance/calibration and provide CIs for $\\mathrm{UE}\\_{\\mathrm{OpenQA}}$ and $\\mathrm{UT}\\_{\\mathrm{OpenQA}}$.\n\n\n3) Extend bit-widths and include multiple schemes (GPTQ/AWQ/RTN). Plot full UE–UT curves to disentangle genuine robustness from capacity collapse, and define an explicit “unusable” region (e.g., $\\mathrm{UT}\\_{\\mathrm{OpenQA}}$ below a percentile threshold) to prevent misleading gains in $\\mathrm{UE}$ from capacity loss.\n\nIf you can clarify the above issues and supplement the experimental evidence to demonstrate the generalizability of the conclusions, I would be happy to raise score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ugMmu8eMPV", "forum": "nxMR2NGFik", "replyto": "nxMR2NGFik", "signatures": ["ICLR.cc/2026/Conference/Submission10343/Reviewer_8HQQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10343/Reviewer_8HQQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179941892, "cdate": 1762179941892, "tmdate": 1762921674963, "mdate": 1762921674963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work is trying to organize the fragmented literature in llm unlearning by\n1. developing a taxonomy of twelve unlearning methods into three methodological families: divergence-driven optimization, representation misalignment, and rejection-based targeted unlearning\n2. rethinking unlearning evaluation through open-question answering (Open-QA) metrics that complement standard multiple-choice (MCQ) measures\n3. providing a multi-axis robustness study spanning in-domain relearning, out-of-domain fine-tuning, quantization, and jailbreak attacks.\n\nAn important contribution of the paper is that MCQ-only evaluation yields a myopic view of forgetting behavior, and that robustness properties differ across algorithmic families."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. I like the framing of methods around the three-family taxonomy. It helps unify results from recent lines of work (NPO, RMU, IDK, TAR, etc.) that previously appeared disjoint.\n2. Introducing Open-QA metrics exposes phenomena such as over-forgetting and under-forgetting invisible to MCQ scores. Though the use of ROUGE score based evaluation has been a common trend in evaluating unlearning right from the earliest benchmarks such as TOFU. I would frame it as an analysis of the metric, rather than an introduction of it.\n3. The decomposition of robustness into in-domain vs out-of-domain fine-tuning, quantization, and jailbreak is valuable and empirically well-motivated.\n4. I liked the Loss landscape discussion and how different unlearning methods influence the smoothness, or locality of the change. This could be echoed more!"}, "weaknesses": {"value": "The paper’s contribution overlaps conceptually with Wang et al. (ICLR 2025) (Towards Effective Evaluations and Comparisons for LLM Unlearning Methods) and Dorna et al. (NeurIPS 2025) (OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics), both of which explicitly define faithfulness and robustness as desiderata for unlearning evaluation.\nYet the present work neither cites nor situates itself relative to those frameworks. This is problematic, and a careful comparison along those axes is important.\n\n### Faithfulness and Robustness\nWhile the paper provides valuable extension to evaluations beyond MCQ to Open-QA, its claims of improved evaluation quality should be examined through the lens of faithfulness and robustness as formalized in prior works such as Wang et al. (2024) and Dorna et al. (2025).\nIn particular, it remains unclear whether the proposed Open-QA metrics exhibit higher faithfulness to true unlearning outcomes (e.g., measured via calibration or counterfactual red-team tests) and whether they are robust under metric-perturbation protocols. Evaluating this framework within the framework of Wang et. al., or the OpenUnlearning benchmark suite would provide quantitative evidence for its added merit.\n\n### Past contributions\nMany of the contributions in this paper, such as use of jailbreak prompts, or quantization have been done in Wang et al (not cited), and the results around extractive and open ended metrics being more robust and faithful also exists in these works. I am having a hard time situating the contributions of this paper, which in isolation is a fantastic collection and systemization of knowledge, to be clear."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pYCGOTd7Dn", "forum": "nxMR2NGFik", "replyto": "nxMR2NGFik", "signatures": ["ICLR.cc/2026/Conference/Submission10343/Reviewer_kNrs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10343/Reviewer_kNrs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762640283438, "cdate": 1762640283438, "tmdate": 1762921674415, "mdate": 1762921674415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}