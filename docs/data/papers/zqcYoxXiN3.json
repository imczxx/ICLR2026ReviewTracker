{"id": "zqcYoxXiN3", "number": 3259, "cdate": 1757389102982, "mdate": 1763631060573, "content": {"title": "Aegis: Automated Error Generation and Identification for Multi-Agent Systems", "abstract": "Large language model based multi-agent systems (MAS) have unlocked significant advancements in tackling complex problems, but their increasing capability introduces a structural fragility that makes them difficult to debug. A key obstacle to improving their reliability is the severe scarcity of large-scale, diverse datasets for error attribution, as existing resources rely on costly and unscalable manual annotation. To address this bottleneck, we introduce *Aegis*, a novel framework for **A**utomated **e**rror **g**eneration and attr**i**bution for multi-agent **s**ystems. *Aegis* constructs a large dataset of **9,533** trajectories with annotated faulty agents and error modes, covering diverse MAS architectures and task domains. This is achieved using a LLM-based manipulator that can adaptively inject context-aware errors into successful execution trajectories. Leveraging fine-grained labels and the structured arrangement of positive-negative sample pairs, *Aegis* supports three different learning paradigms: Supervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning.  We develop learning methods for each paradigm.  Comprehensive experiments show that trained models consistently achieve substantial improvements in error attribution. Notably, several of our fine-tuned LLMs demonstrate performance competitive with or superior to proprietary models an order of magnitude larger, validating our automated data generation framework as a crucial resource for developing more robust and interpretable multi-agent systems.", "tldr": "", "keywords": ["Multi-Agent Systems; Failure attribution; Automated data generation; Learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7fb2d68008246b8dbeee818d26a7c987bfcd6f49.pdf", "supplementary_material": "/attachment/05b6c28958da5ec2a23c1ea50a108f2d27fa0d3f.zip"}, "replies": [{"content": {"summary": {"value": "This work features two major contributions for failure attribution of multi-agent system. The first is a large-scale dataset of annotated failure trajectories (failed agents and error modes), obtained by manipulating successful trajectories. The second is unique training methods (reinforcement learning and contrastive learning) which are made possible thanks to the characteristics of the constructed dataset. Experiments on the test set of the proposed dataset and the OOD Who&When dataset demonstrate the effectiveness of the training data and methods, with SFT being the most effective on average. There are many extra analyses and observations presented, which are of potential value for future works."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Manipulating successful trajectories is a reasonable and scalable way to create data for failure attribution. This is evidenced by that the resulting dataset has 9000+ samples, which is of significantly larger size than e.g. Who&When dataset.\n2. The proposed method of data generation is also controlled enough that it can provide fine-grained information for training methods beyond SFT.\n3. Having results on OOD/unseen dataset (Who&When) makes the evaluation more comprehensive and convincing."}, "weaknesses": {"value": "I don't see major weaknesses in the submission."}, "questions": {"value": "1. When validating generated trajectories (i.e., seeing whether the intervention really induces a failure), is there a concrete like rule-based function for that, or llm-as-a-judge is employed? If it's the latter case, how accurate the ground-truth labels are (how much noise is there)?\n2. Why GRPO consistently performs worse than SFT (e.g., Qwen3-8B-Thinking + GRPO is worse than Qwen3-8B-Non-Thinking + SFT)? Any theory or thoughts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dqCcuKn1MH", "forum": "zqcYoxXiN3", "replyto": "zqcYoxXiN3", "signatures": ["ICLR.cc/2026/Conference/Submission3259/Reviewer_qv7V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3259/Reviewer_qv7V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779572765, "cdate": 1761779572765, "tmdate": 1762916633693, "mdate": 1762916633693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce Aegis, a frame to automatically generate erroneous trajectory for multi-agent system. It's used to construct a large dataset of trajectory with error annotation and modes. The author use the generated dataset to fine-tune LLM to do error attribution with three different learning paradigms. Experiment results show the finetuned model improves on the task of error attribution of multi-agent systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The data generation pipeline seems effective and the constructed dataset could be useful for the community\n2. The results show that the constructed data can significantly improve open-source models capability of error attribution\n3. The proposed DCL is interesting and looks promising even though it's still lag behind LLMs"}, "weaknesses": {"value": "1. The design of the error mode taxonomy is critical, but the authors didn't discuss and reveal how it is done in the main body of the paper\n2. The data generation pipeline will result in real positive trajectories and synthetic negative trajectories, this may not align with real world cases where errors are real and diverse"}, "questions": {"value": "In addition to the two strategies, using an underperforming LLM for certain steps may be a good way to generate realistic errors but it's not guaranteed to produce error \n\nAlso, how does the model performance on aegis-bench correlated with who&when? It would be better to do a correlation analysis to see if aegis-bench is positively correlated with who&when\n\nFor DCL, it may be better to use better text encoders, eg, qwen3-embedding for better performance; also it would be good to list the number of parameter of DCL to show it's much smaller compared to LLM"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b74YWmqa1G", "forum": "zqcYoxXiN3", "replyto": "zqcYoxXiN3", "signatures": ["ICLR.cc/2026/Conference/Submission3259/Reviewer_HRkG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3259/Reviewer_HRkG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890587789, "cdate": 1761890587789, "tmdate": 1762916633169, "mdate": 1762916633169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Responses to All Reviewers"}, "comment": {"value": "We thank all reviewers for the time and effort they dedicated to providing this thorough and constructive feedback. To best address the most critical and recurring questions, we provide this global response on **the alignment of our synthetic Aegis data with real-world failure modes**.\n\nWe have uploaded a revised version of our paper. All significant additions and modifications to the revised manuscript have been marked **in blue** for easy identification. We believe these additions, supported by new analyses detailed below, will strengthen the paper and address the reviewers' insightful comments.\n\n\nWe are grateful for the opportunity to provide new validating analyses that address this point from four perspectives: **(1) Conceptual Principle, (2) Methodological Grounding, (3) Empirical Data Validation, and (4) Practical Results.**\n\n1. First, on a conceptual principle, our framework is grounded in established academic practice for complex systems diagnosis.  As noted in comprehensive surveys [1], acquiring large-scale labeled data for every new, complex case is \"impractical\".  This is why the field relies on two key, validated principles that Aegis adopts: (1) error mode transferability, and (2) high-fidelity synthetic data generation.  Critically, the survey also validates our specific generation methodology.  It highlights using \"a simulation process...  to explore potential anomalous conditions\" and techniques like \"adversarial perturbation\" as standard, necessary methods to **\"mitigate the problem of insufficient samples\"**.  Our \"correct-to-incorrect\" pipeline is a direct implementation of this principle: we use successful trajectories as high-fidelity simulations and apply targeted, context-aware perturbations to programmatically generate the very error data that is otherwise impossible to acquire at scale.\n\n2. Second, our methodology is grounded in real-world data.  Aegis does not invent arbitrary errors.  Our framework programmatically reproduces the 14 known, real-world error modes from the **MAST taxonomy** [2].  This taxonomy was itself empirically derived from a rigorous analysis of over 150 genuine, \"naturally erroneous\" failure traces.  To validate that our injection of these modes is accurate, we conduct a new human IAA study.  We ask three human annotators (blind to our labels) to classify the root-cause error mode for 100 sampled trajectories.  We find our human-human agreement is $\\boldsymbol{\\kappa=0.85}$.  Our programmatic Aegis label achieves a program-human agreement of $\\boldsymbol{\\kappa=0.81}$.  These scores are comparable to the high benchmarks reported in the MAST paper itself ( $\\kappa=0.88$ human-human, and $\\kappa=0.77$ for their LLM-annotator-human agreement).\n\n3. Third, we perform a three-way semantic comparison against MAST-Data and Who&When. We first extract the semantic content of each trajectory (removing timestamps, metadata, and formatting artifacts) and encode all texts using the *all-MiniLM-L6-v2* model with normalized embeddings. The centroid distances show that Aegis is no farther from a real benchmark than the real benchmarks are from each other:\n\n| Pair | Distance | Type |\n| :--- | :--- | :--- |\n| Aegis ↔ MAST | 0.521 | Synthetic vs. Real |\n| MAST ↔ WhoWhen | 0.490 | Real vs. Real |\n| Aegis ↔ WhoWhen | 0.398 | Synthetic vs. Real |\n\nBeyond distances, the clustering metrics (**silhouette = 0.11**) indicate no meaningful separation among the three datasets.  Additionally, the pairwise Mann–Whitney U tests show that the difference between Aegis–MAST (mean **p = 0.097**) is almost identical to that between MAST–Who&When (mean **p = 0.089**), confirming that Aegis differs from real data no more than the real data differ from each other. The corresponding UMAP and t-SNE visualizations are included in **Appendix B.3 (Figure 7)**, and we respectfully invite reviewers to take a look.\n\n4. Finally, the practical results provide definitive proof of generalization. As shown in Table 1, models trained only on our synthetic Aegis data achieve strong OOD performance on the human-annotated Who&When benchmark. To quantify this, we run a new correlation analysis across all models. We find a statistically significant positive correlation between performance on Aegis-Bench and Who&When for the core attribution skills: Agent $\\mu$F1 (**r=0.714, p<0.001**) and Error MF1 (**r=0.744, p<0.001**). This proves that training on Aegis successfully teaches models the generalizable skills needed to diagnose real-world failures. The details of the relevant analysis are placed in **Appendix D.2 and Table 11**.\n\n[1] Yan, Peng, et al. \"A comprehensive survey of deep transfer learning for anomaly detection in industrial time series: Methods, applications, and directions.\"\n\n[2] Cemri, Mert, et al. \"Why do multi-agent llm systems fail?\""}}, "id": "VuYD8CAiAk", "forum": "zqcYoxXiN3", "replyto": "zqcYoxXiN3", "signatures": ["ICLR.cc/2026/Conference/Submission3259/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3259/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3259/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763631419816, "cdate": 1763631419816, "tmdate": 1763631419816, "mdate": 1763631419816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AEGIS, an automated framework for generating large-scale error attribution data in multi-agent systems built from LLMs. By injecting controlled, context-aware faults into correct trajectories, AEGIS creates over 9,500 annotated failure cases. These are used to train models to improve attribution performance across both synthetic and real-world datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work presents a scalable and principled pipeline for generating faulty MAS trajectories with programmatic ground-truth labels. This is a significant contribution, as data scarcity has long been a limiting factor for MAS error analysis.\n2. AEGIS supports supervised, RL-based, and contrastive learning methods. The design shows thoughtful alignment between the structure of synthetic data and training objectives, which is rarely done this thoroughly in MAS.\n3. The authors benchmark a wide array of models across multiple axes and test both in-domain and OOD generalization. Fine-tuned models on AEGIS surpass many proprietary LLMs in accuracy, showing the power of programmatically generated data for enabling high-fidelity diagnostics. This makes the empirical evaluation strong and convincing."}, "weaknesses": {"value": "1. While the paper proposes a novel Disentangled Contrastive Learning approach, the details of how training pairs are generated remain underspecified. It is unclear how a single correct trajectory is transformed into contrastive pairs and how these are selected. A walkthrough or algorithmic example showing how one trajectory is transformed into training instances for DCL would greatly enhance clarity and reproducibility.\n2. AEGIS generates incorrect trajectories by injecting errors into correct ones, but many real-world MAS failures, such as those in Who&When, originate in naturally erroneous decision paths. This creates a potential misalignment between how the data is constructed and how errors manifest in the wild. While the authors demonstrate strong generalization, the conceptual gap between error injection (correct to incorrect) and naturally flawed trajectories (originally incorrect) is not addressed. This weakens the connection to real-world deployment contexts and could affect performance on systems where errors are emergent rather than injected.\n3. While the results are strong, the paper would benefit from qualitative examples of common failure modes that the best models still struggle with."}, "questions": {"value": "1. The paper evaluates on the Who&When benchmark, where trajectories can exceed 50 steps. However the model has a maximum input length of 8192 tokens during training. The paper does not clarify how such lengthy interactions are handled. Are steps truncated, summarized, or skipped? \n2. Are the injected errors randomly selected, or is there a balancing mechanism to ensure coverage across error modes? Could the model become biased toward more frequently injected errors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ACMwVs7pwS", "forum": "zqcYoxXiN3", "replyto": "zqcYoxXiN3", "signatures": ["ICLR.cc/2026/Conference/Submission3259/Reviewer_nsVH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3259/Reviewer_nsVH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934942407, "cdate": 1761934942407, "tmdate": 1762916632809, "mdate": 1762916632809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to address the bottleneck of data scarcity in training error-attribution systems for LLM-based multi-agent systems' execution. The authors introduce Aegis, a framework to programatically generate large-scale dataset of annotated error trajectories. Aegies starts with collecting successful, error-free baseline trajectories from a pre-selected set of MAS, then applies targeted LLM-based interventions to the baseline trajectories (by prompt injection and response corruption), and then validates that the system continuing to execute from the corrupted state indeed fails to complete the task, filtering out the ones that still do not lead to a failure. The authors create 9533 annotated error trajectories across 6 MAS with Aegis, and train models using SFT, RL and CL."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Addresses an important problem of data scarcity in training models for MAS error identification.\n* Provides a simple intervention to utilize previous successful trajectories.\n* Presents a dataset of 9,500+ annotated trajectories, and validate its usefulness by showing OOD generalization in training with this data."}, "weaknesses": {"value": "- Since the technique starts with successful trajectories, it is unclear how this will be applied to MAS built for domains with very high failure rates.\n- Due to the dataset being built by corrupting successful trajectories, the dataset will miss out entirely on representing real-world failure modes of the MAS being represented in the dataset.\n- Even though an intervention is made in the MAS trajectory to corrupt it, the downstream failure reason could be different from the intervention."}, "questions": {"value": "1. Can the authors comment on how it could be ensured that an error introduced under taxonomy error category X, actually leads to observed failure mode X, and not a different failure mode?\n2. Can the authors discuss how Aegis handles representing real-world failure modes of MAS, since it starts from successful trajectories?\n3. The studies performed in the paper show improvements in the ability to detect failure modes in a MAS trajectory (through 3 different learning modes), however, it is unclear how this can be utilized to improve the performance of the MAS itself. Could the authors comment on that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fYl2r8fDWK", "forum": "zqcYoxXiN3", "replyto": "zqcYoxXiN3", "signatures": ["ICLR.cc/2026/Conference/Submission3259/Reviewer_Dj9H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3259/Reviewer_Dj9H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762048824912, "cdate": 1762048824912, "tmdate": 1762916632110, "mdate": 1762916632110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}