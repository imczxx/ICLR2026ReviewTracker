{"id": "iok8ebFKPQ", "number": 20767, "cdate": 1758309889680, "mdate": 1763682168621, "content": {"title": "Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration", "abstract": "Applications of machine learning in chemistry are often limited by the scarcity and expense of labeled data, restricting traditional supervised methods. In this work, we introduce a framework for molecular reasoning using general-purpose Large Language Models (LLMs) that operates without requiring labeled training data. Our method anchors chain-of-thought reasoning to the molecular structure by using unique atomic identifiers. First, the LLM performs a zero-shot task to identify relevant fragments and their associated chemical labels or transformation classes. In an optional second step, this position-aware information is used in a few-shot task with provided class examples to predict the chemical transformation.\nWe apply our framework to single-step retrosynthesis, a task where LLMs have previously underperformed. Across academic benchmarks and expert-validated drug discovery molecules, our work enables LLMs to achieve high success rates in identifying chemically plausible reaction sites ($\\geq$90%), named reaction classes ($\\geq40$%), and final reactants ($\\geq$74\\%). \nUltimately, our work establishes a general blueprint for applying LLMs to challenges where molecular reasoning and molecular transformations are key, positioning atom-anchored LLMs as a powerful solution for data-scarce chemistry domains.", "tldr": "A novel framework that uses LLMs to reason over molecular structures, enabling zero-shot, chemically plausible retrosynthesis without requiring task-specific fine-tuning or labeled data.", "keywords": ["Large Language Models", "Molecular Reasoning", "Retrosynthesis", "Cheminformatics"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0aa856b8672eb3c344f7bbe996511e7b2f126296.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a novel, label-free framework for molecular reasoning using general-purpose LLMs that operates without requiring labeled training data. LLM's chain-of-thought is applied to the molecular structure by using unique atomic identifiers, which enables the one-shot identification of relevant chemical fragments and transformation classes for the retrosynthesis task. This approach is highly significant as it bypasses the need for large, labeled datasets. The algorithm achieves high success rates test on both academic benchmarks and expert-validated drug discovery molecules."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written, and the figures are helpful."}, "weaknesses": {"value": "See questions"}, "questions": {"value": "Can this idea be generalized to more complicated tasks, e.g., molecules?\n\nMy understanding is that the LLM is not trained on chemical data but only on language data. What is the intuition that LLM transfers its knowledge to such a different area, without finetuning?\n\nWhat is the inference cost of this LLM approach compared to existing, traditional approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LOUxsIGbyL", "forum": "iok8ebFKPQ", "replyto": "iok8ebFKPQ", "signatures": ["ICLR.cc/2026/Conference/Submission20767/Reviewer_GCce"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20767/Reviewer_GCce"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940507969, "cdate": 1761940507969, "tmdate": 1762934195108, "mdate": 1762934195108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an atom-anchored reasoning framework that enables general large language models to perform chemically interpretable retrosynthesis without any task-specific training data. By introducing atom-level mappings and a two-stage chain-of-thought process, the model achieves strong performance in identifying reaction centers, classifying reaction types, and predicting reactants, demonstrating its capability to reason over molecular structures in a transparent and generalizable way."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The framework requires no labeled data, making it highly adaptable to data-scarce chemical domains.\n\n* The atom-level anchoring enhances interpretability and aligns model reasoning with real chemical logic.\n\n* The approach generalizes across benchmarks and real drug molecules, showing practical applicability."}, "weaknesses": {"value": "* The model's performance relies heavily on well-crafted prompts and example selection.\n\n* It struggles with edge cases involving unusual reaction mechanisms or rare functional groups."}, "questions": {"value": "* Can the method generalize to multi-step synthesis and stereoselective reactions?\n\n* Would fine-tuning on limited chemistry-specific reasoning traces further improve accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eYfHVztVR7", "forum": "iok8ebFKPQ", "replyto": "iok8ebFKPQ", "signatures": ["ICLR.cc/2026/Conference/Submission20767/Reviewer_UZNg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20767/Reviewer_UZNg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009401515, "cdate": 1762009401515, "tmdate": 1762934194003, "mdate": 1762934194003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for leveraging LLMs for tasks in molecular analysis and specifically single-step retrosynthesis. The method relies on atom-mapping syntax for SMILES strings, and uses LLMs as evaluators at the level of atoms and bonds. The framework then uses these fine-grained evaluations to judge the potential for disconnection, and then to predict the reaction that could be used to materialize the disconnection.\nInteresting idea overall."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors provide a new method to improve the analytic capabilities of LLMs, and show that this improves dramatically their applicability on complex chemical analysis tasks.\nthe evaluation is thorough and uses public benchmarks and datasets, evaluates different LLMs both closed, open and across multiple scales. the prompts are released as well which contributes to reproducibility."}, "weaknesses": {"value": "Looking at the sizes of the datasets used for evaluation, it would be important to do a discussion on costs and latencies of the proposed methodology. For instance, how many LLM calls are required per problem? how does it scale? e.g. it seems like the method scales with the number of atoms in the molecule. Please discuss this further.\n\nThe authors describe the advantages of the method more as a way of collecting synthetic data given how scarce real data is in specific fields. It would be a very interesting contribution, and would really make a better case for this claim, if some generated dataset was released for use of the community. Or if you trained another model, or analyzed the data, etc. Something related to creating a dataset and demonstrate its usefulness. I believe a lot of data was already created during the creation of the results presented here, so shouldn't be very problematic."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "swkmuq7LV8", "forum": "iok8ebFKPQ", "replyto": "iok8ebFKPQ", "signatures": ["ICLR.cc/2026/Conference/Submission20767/Reviewer_thjJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20767/Reviewer_thjJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762115077578, "cdate": 1762115077578, "tmdate": 1762934192582, "mdate": 1762934192582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}