{"id": "qEkzamffGT", "number": 121, "cdate": 1756728981880, "mdate": 1763551993279, "content": {"title": "Data-Centric Unlearning: Optimizing Labels and Retain Data via Learning Dynamics", "abstract": "Machine unlearning mitigates adverse effects from erroneous, outdated, or private training data. Although unlearning algorithms have advanced for classifiers and LLMs, the critical role of unlearning training data quality remains largely unexplored. This work addresses this fundamental gap by systematically investigating how to construct effective unlearning training sets, focusing on optimal label assignment for samples and strategic selection for the retain set. We leverage learning dynamics theory to analyze the impact of training data on unlearning performance. Precisely, we derive: (1) an optimal label assignment scheme for both unlearning and retain samples, and (2) the principle that neighborhood and boundary samples are most beneficial for inclusion in the retain set. We translate these theoretical insights into data optimization algorithms tailored for both classifiers and LLMs unlearning. Extensive experiments across classifier and LLMs unlearning tasks demonstrate that our data optimization strategies significantly enhance the performance of existing SOTA unlearning algorithms. Our work establishes data optimization as a crucial pillar for effective machine unlearning.", "tldr": "Our work optimizes key training data (labels, retain data) in machine unlearning via learning dynamics to boost SOTA for classifiers/LLMs.", "keywords": ["Machine unlearning; learning dynamics; data optimization; data selection"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1f9851ed41d04157050906dd04913049a6c6b0a0.pdf", "supplementary_material": "/attachment/b69b8082246e05a0b90d1393f86050013ba5aba2.zip"}, "replies": [{"content": {"summary": {"value": "The paper is well-motivated, turns learning-dynamics (LD/RLD) analysis into concrete, data-layer rules for unlearning, and shows consistent gains across multiple baselines and modalities (classifiers and an LLM setting). The idea is sound and practically usable as a drop-in data optimisation front end for existing unlearning methods. Main reservations are the reliance on one-step LD assumptions, heuristic aspects and engineering overhead in the LLM pipeline, and limited discussion of operational costs and failure modes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper offers a set of data-centric unlearning rules (e.g., retain-label smoothing, neighbor/boundary mining) derived from learning dynamics (RLD), with clear theoretical backing and an intuitive explanation of each mechanism.\n\n- The proposed framework wraps around a variety of existing unlearning baselines and introduces a tunable knob to adjust the trade-off between forgetting and retaining.\n\n- Experiments demonstrate consistent improvements across multiple baselines and datasets, enhancing both privacy/locality metrics and overall utility, with minimal performance degradation.\n\n- The method is designed to be minimally invasive, requiring no changes to core model training pipelines, making it easily adoptable by practitioners and scalable to larger models."}, "weaknesses": {"value": "- The paper builds directly on Ren & Sutherland (2025) and similar recent LLM dynamics literature. While the \"rule-guided\" aspect is new in framing, the practical recipe is largely a reweighting and scheduling heuristic over existing unlearning baselines. The novelty is seems modest.\n\n- The main concern for me is that the analysis rests on single-sample, small-step SGD with a ‚Äústable relative influence‚Äù assumption, then drops O(Œ∑^2) terms. Modern training (mini-batches, Adam/AdamW, momentum, weight decay, label smoothing) alters the effective dynamics; the paper neither extends the theory to these settings nor shows that conclusions still hold.\n\n- The two-stage ‚Äúgenerate candidates then score‚Äù approach lacks the clearer optimality story present for classifiers and appears sensitive to prompt/scorer choices. \n\n\nMinor:\n-You seems mix ùëì_ùúÉ and ùúã_ùúÉ. Pick one to ensure the consistency\n-The dimensions of A,K,G  are not defined....\n- The last term of (2) O(Œ∑2...) appears without assumptions (smoothness/Lipschitz of the Jacobian).\n\nI am open to raising my score if the authors can convincingly address the concerns"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3OIeksFSen", "forum": "qEkzamffGT", "replyto": "qEkzamffGT", "signatures": ["ICLR.cc/2026/Conference/Submission121/Reviewer_XLa5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission121/Reviewer_XLa5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761044448181, "cdate": 1761044448181, "tmdate": 1762915456030, "mdate": 1762915456030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We would like to sincerely thank all the reviews for the time and care theydevoted to reviewing our paper. After carefully consideration on all the  comments and suggestions, we have decided to withdraw our paper at this stage in order to further improve the work. The reviews are very valuable to us, and we will draw on it closely as we revise and extend this research."}}, "id": "TgmTgjUJkj", "forum": "qEkzamffGT", "replyto": "qEkzamffGT", "signatures": ["ICLR.cc/2026/Conference/Submission121/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission121/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763551992500, "cdate": 1763551992500, "tmdate": 1763551992500, "mdate": 1763551992500, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a data-centric unlearning pipeline that jointly optimizes labels for forget/retain sets and curates retain data (neighbor/boundary/adversarial) guided by learning-dynamics heuristics. It reports modest improvements over several baselines on image classifiers and a heuristic extension to LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a coherent, data-centric perspective that jointly optimizes forget/retain labels and retain-set composition. The rules are simple and practical to implement.\n- The empirical evaluation is comprehensive, including multiple datasets/baselines."}, "weaknesses": {"value": "- The paper lacks a clear unlearning formulation aligned with standard goals (e.g., retrain-from-scratch indistinguishability). The proposed objective is heuristic and not shown to target the canonical criterion.\n\n- The results in Table 1 are not strong. The gains are modest and no confidence intervals or multi-seed reporting, so it‚Äôs unclear if improvements are statistically meaningful.\n\n- Theoretical scope and assumptions are not stress-tested. For example, boundary claims use linear/softmax analyses and extend to deep nets qualitatively. It would be helpful to add experiments that directly measure boundary proximity (e.g., margin estimates or confidence entropy) vs observed ‚Äúretain-set protection‚Äù to substantiate the link.\n\n- Some important details are under-specified or missing (e.g., the MIA protocol), which makes comparisons hard to interpret."}, "questions": {"value": "1. Please justify Eq. (4) and explain how your optimization objective connects to the canonical unlearning target. \n\n2. Which MIA did you use? Please describe the exact protocol to enable replication and fair comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O0jktpBoC0", "forum": "qEkzamffGT", "replyto": "qEkzamffGT", "signatures": ["ICLR.cc/2026/Conference/Submission121/Reviewer_oaia"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission121/Reviewer_oaia"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782048773, "cdate": 1761782048773, "tmdate": 1762915455789, "mdate": 1762915455789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical gap in machine unlearning‚Äîoverlooking the quality of training data‚Äîby proposing a data-centric framework (DataOpt) grounded in learning dynamics theory. The proposed framework systematically optimizes two key components: label assignment for both forget and retain sets, and strategic selection of retain samples. DataOpt can be applied to both classifier and LLM unlearning scenarios. The experiments conducted on CIFAR - 10, Tiny - ImageNet, and the RWKU benchmark demonstrate that DataOpt outpereforms existing unlearning methods. Sensitivity analyses also confirm the efficacy and controllability of the framework."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a theoretical justification for label and retain sample optimization based on learning dynamics, leading to closed-form solutions for optimal label assignments\n2. The experimental section is wide-ranging, with experiments across standard vision benchmarks (CIFAR-10, Tiny-ImageNet) and challenging LLM benchmarks \n3. The paper provides detailed appendices for mathematical proofs, algorithmic steps, experimental details, and implementation settings."}, "weaknesses": {"value": "1. The design of New Response Generation and adversarial sample generation for LLMs lacks sufficient rationality.  The method of relying on a one-time generation via LLMs followed by scoring and ranking does not ensure that the generated content aligns with the intended optimization goals. There is no mechanism to enforce diversity among candidates or verify the thoroughness of sensitive information removal. This may lead to suboptimal or unsafe responses that retain hidden sensitive knowledge. \n2. The writing of the Method section is disorganized, with several critical issues impairing clarity. Label assignment for forget set and retain set are conflated, key parameters (e.g., PGD for classification, response generation for LLMs) lack justification in the main text and are only mentioned in appendices. The writing makes it hard to follow how components integrate into the DataOpt framework.\n3. For LLM unlearning tasks, the comparison against baseline algorithms is overly basic and omits recent state-of-the-art methods, such as [1-3], which prevents a comprehensive evaluation of whether the proposed method outperforms the latest alternatives. This gap limits the ability to validate the method‚Äôs superiority in real-world LLM unlearning scenarios.\n\nRefs:\n\n[1] A closer look at machine unlearning for large language models. In The Thirteenth International Conference on Learning Representations, 2025.\n\n[2] LLM unlearning via loss adjustment with only forget data. In The Thirteenth International Conference on Learning Representations, 2025\n\n[3]  The wmdp benchmark: measuring and reducing malicious use with unlearning. In Proceedings of the 41st International Conference on Machine Learning"}, "questions": {"value": "Please refer to Weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7OaHGSHwYP", "forum": "qEkzamffGT", "replyto": "qEkzamffGT", "signatures": ["ICLR.cc/2026/Conference/Submission121/Reviewer_GT5c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission121/Reviewer_GT5c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963974864, "cdate": 1761963974864, "tmdate": 1762915455547, "mdate": 1762915455547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DataOpt reframes machine unlearning as a data-centric optimization: it first assigns degree-controlled target labels to forget samples via an ‚Äúunlearning degree‚Äù k and then selects retain data that are near the forget samples and decision boundaries (including adversarial examples), which most strongly regulate unintended shifts. This yields controllable forgetting‚Äîhigher k smoothly increases privacy (lower forget accuracy/MIA) with only minor utility loss‚Äîwhile consistently boosting existing unlearning methods in experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a pivotal but underexplored facet of machine unlearning: how the training data for unlearning is constructed. This is crucial for both privacy-driven deletions and surgical updates to deployed models. By elevating data selection and labelling to first-class design variables, the work addresses a clear gap with tangible practical value. Notably, its retain-set strategy is boundary-focused and derived from learning-dynamics analysis rather than heuristics."}, "weaknesses": {"value": "My primary concern is the ‚Äúone-step influence stability‚Äù assumption. if the model moves only a tiny amount, first-order terms dominate and the analysis holds. However, in realistic training: multiple epochs, learning-rate schedules, adaptive optimisers (e.g., Adam), and data-order effects, the influence of a single update can shift substantially. What is true after one small step may not persist after thousands.\nFor LLMs, the two-stage approximate optimisation  feels heuristic relative to the cleaner classification treatment. Using the same (or a closely related) LLM to score fluency/relevance risks circularity and leakage, and the formal guarantees do not carry over to sequence models.\nOn the empirical side, the main results lack error bars or statistical tests; multiple seeds and explicit variance reporting would materially strengthen the claims.\nClarifying point: In Table 3, the rows for DataOpt (Retain set only) and DataOpt (Boundary sample only) appear identical line-for-line."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uQ5OPSsoLv", "forum": "qEkzamffGT", "replyto": "qEkzamffGT", "signatures": ["ICLR.cc/2026/Conference/Submission121/Reviewer_JZnz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission121/Reviewer_JZnz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996094612, "cdate": 1761996094612, "tmdate": 1762915455378, "mdate": 1762915455378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}