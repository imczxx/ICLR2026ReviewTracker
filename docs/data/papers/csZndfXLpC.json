{"id": "csZndfXLpC", "number": 23737, "cdate": 1758347778124, "mdate": 1759896799562, "content": {"title": "SILA: Enhancing Long-Context Retrieval Capability of Linear Attention via Selective Ignoring", "abstract": "Linear attention models have recently emerged as computationally efficient alternatives to Transformers.\nDespite competitive performance on general commonsense tasks, they still struggle to match Transformers on long-context retrieval tasks.\nIn this work, we re-examine linear attention models from the perspective of memory writing.\nWe propose that enabling linear attention models to learn selective ignoring provides a promising approach to addressing long-context retrieval tasks under fixed memory capacity.\nGuided by this principle, we demonstrate how to interpret and intervene in the behavior of linear attention models, thereby revealing the true retrieval capabilities of popular models.\nInformed by these observations, we introduce Selective Ignoring Linear Attention (SILA), which incorporates a redesigned memory architecture and a weighted loss training strategy to encourage selective memory writing.\nSILA exhibits remarkable long-context retrieval capabilities, achieving 20$\\times$ context length extrapolation on the Passkey Retrieval task, and demonstrating superior memory utilization efficiency on the Needle-in-a-Haystack benchmark.", "tldr": "", "keywords": ["language model", "linear attention", "long-context retrieval"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/113c96cbebb867c96220d52692c7d7a95bb757f9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SILA, which enhances linear attention retrieval through improvements in both recurrence modeling and training objectives. Its core idea is to allow the model to selectively disregard irrelevant tokens.\n\nThe authors begin with an analysis of the NIAH task and establish a more reliable evaluation setting. Motivated by the need for selective memory writing, SILA incorporates several architectural innovations: it decouples the “read” and “write” operations (i.e., read-before-write) and introduces state-dependent gates. In addition, the authors employ a progressive selective weighted loss, based on a pretrained transformer, to improve the efficiency of long-context training.\n\nExperimental results under the Transformer-to-RNN paradigm demonstrate that SILA improves the retrieval capability of linear attention models while maintaining competitive performance on general short-context modeling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation and proposed approach of this work are well-reasoned and coherent. The paper identifies a key deficiency of linear attention models under NIAH-style tasks and demonstrates that this issue can be alleviated through selective ignoring.\n\n2. The analysis of the NIAH benchmark is logical and convincing, ensuring the reliability of the experimental evaluation.\n\n3. The improvements in both modeling and training are essential and directly address the need for selective ignoring."}, "weaknesses": {"value": "1. Some of the conclusions are not thoroughly or fairly validated. For example, the passkey retrieval extrapolation results of Qwen3 and GatedDelta are not shown; since the transferred Qwen3 checkpoints already support 32k context without ntk scaling, further clarification is needed here. In addition, in Table 5, only SILA is a transferred model, making it difficult to fairly compare the effectiveness of the proposed method with other selective writing approaches such as LongMamba.\n\n2. The paper lacks ablation studies and detailed analyses for the proposed improvements. It remains unclear how each component—such as the training strategy and the two architectural modifications—specifically contributes to retrieval performance or NIAH results.\n\n3. The experimental setting is overly constrained. The model scale (0.6B), training length (1k/4k), and evaluation scenario (Transformer-to-RNN transfer) are all highly specialized. Under such a narrow setup, it is difficult to disentangle how much of the observed gain arises from the proposed method itself rather than from others like parameter increases or tuning. Moreover, there are concerns regarding the scalability of the approach."}, "questions": {"value": "1. In Table 1, the in-context retrieval capability appears correlated with training data scale. Since Qwen3 was trained on 36T high-quality tokens while other linear models were significantly undertrained, would a Transformer trained on a comparable corpus (e.g., hundreds of billions of tokens) still demonstrate similarly robust NIAH-Word performance?\n\n2. Compared to linear attention baselines such as GatedDeltaNet, how do SILA’s computational and parameter costs change? What are the shapes of $W_{\\gamma}$ and $W_{\\beta}$? \n\n3. What is the rationale for the chosen functional form of the forget gate $\\phi(x)$ on the negative axis in Eq.10, which seems relatively uncommon?\n\n4. As the memory-dependent gates introduce nonlinearity in the recurrence via the hidden state, does this hurt parallelism or training efficiency, particularly for long-context sequences? From an implementation standpoint, is it necessary to materialize all intermediate states across time steps?\n\n5. Regarding the weighted loss, how were the threshold and scaling factor determined? Were these hyperparameters tuned through systematic sweeps?\n\n6. What are the NIAH results for SILA (standard loss)? Only commonsense reasoning results are reported.\n\n7. To support the claim that the improvement stems from selective ignoring, it would be helpful to visualize and analyze the gate values (${\\gamma}_t$ and ${\\beta}_t$) and compare them against those of GatedDeltaNet."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PC5XtPSJBw", "forum": "csZndfXLpC", "replyto": "csZndfXLpC", "signatures": ["ICLR.cc/2026/Conference/Submission23737/Reviewer_PcX2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23737/Reviewer_PcX2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746586390, "cdate": 1761746586390, "tmdate": 1762942785819, "mdate": 1762942785819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper re-frames long‑context retrieval in linear attention models as a memory‑writing problem and argues that strong performance often comes from specialized digit‑token shortcuts rather than general associative recall. The authors show instability in standard NIAH evaluation and recommend sample‑level haystack shuffling and also propose SILA, a linear‑attention variant that decouples recall from writing and introduces memory‑dependent gates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper convincingly shows that linear models often “win” on NIAH by preferentially writing digits rather than learning general retrieval. This is an important finding. \n2. Two concrete fixes—sample‑level shuffling and NIAH‑Word—expose the over‑reliance on digits. This yields an evaluation setup other works can adopt immediately.   \n3. SILA’s read‑before‑write decoupling  allows using the current token locally without committing it to memory; memory‑dependent gates use retrieved state to decide writing/forgetting."}, "weaknesses": {"value": "1. Teacher‑dependence & compute overhead not quantified. The weighted‑loss pipeline requires per‑token attention from a reference Transformer. The paper does not report added training FLOPs/throughput or wall‑clock vs. a standard linear‑attention pretrain.  \n2. SILA performs an extra memory read for gating/recall each step. Please report inference speed and memory footprint vs. comparable linear baselines. \n3. Unfair comparisons: SILA‑0.6B is initialized from Qwen3‑0.6B and trained on 15B FineWeb‑Edu tokens with bespoke weighting, whereas many baselines are off‑the‑shelf with different data/state sizes."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dJNcVpnBPR", "forum": "csZndfXLpC", "replyto": "csZndfXLpC", "signatures": ["ICLR.cc/2026/Conference/Submission23737/Reviewer_Roi4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23737/Reviewer_Roi4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893517417, "cdate": 1761893517417, "tmdate": 1762942785407, "mdate": 1762942785407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SILA (Selective Ignoring Linear Attention), an architecture designed to enhance the long-context retrieval ability of linear attention models. SILA introduces (1) a memory-dependent gating mechanism to selectively write information into memory and (2) a weighted-loss scheme to emphasize important tokens during training. Experiments show strong gains in synthetic and benchmark long-context retrieval tasks, demonstrating better generalization to 10–100× longer contexts than training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-motivated and relevant problem: The paper tackles a key limitation of efficient attention models—poor long-range retrieval—highly relevant to long-context LLM research.\n\n- Clear and technically novel mechanism: The selective ignoring gate and weighted-loss supervision are simple yet effective extensions that yield interpretable and consistent improvements.\n\n- Comprehensive empirical validation: The paper provides clear component-wise ablations and visual analyses showing how the gating improves selective memory use."}, "weaknesses": {"value": "- Limited scalability to current LLMs: SILA requires replacing the attention mechanism and retraining from scratch, making it impractical for integration into existing large pretrained models (e.g., GPT-series).\n\n- Benchmark and scale limitations: Evaluations are restricted to mid-sized (0.6B) models and synthetic retrieval tasks, lacking validation on realistic large-scale or multi-domain benchmarks.\n\n- Incomplete comparison: The paper does not benchmark against other recent selective-memory or efficient-attention architectures."}, "questions": {"value": "see weeknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nwVv9W6C9s", "forum": "csZndfXLpC", "replyto": "csZndfXLpC", "signatures": ["ICLR.cc/2026/Conference/Submission23737/Reviewer_5kAD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23737/Reviewer_5kAD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972448179, "cdate": 1761972448179, "tmdate": 1762942785091, "mdate": 1762942785091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Motivated by the limitations of existing linear attention methods that fail to perform robust long-context retrieval due to the fixed memory capacity, the paper proposes a novel linear attention method that learns to selectively ignore irrelevant tokens for long-context retrieval tasks and attend to important instruction tokens. The proposed method, Selective Ignoring Linear Attention (SILA), shows improvement on needle-in-the-haystack tasks with better memory utilization efficiency compared with prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is motivated by the limitations of existing linear attention methods on robust long-context retrieval tasks, with well-designed controlled experiments, and the proposed method is designed to solve the identified issues of prior methods.\n\n2. The paper conducts extensive experiments and compares the proposed methods over several strong baselines from the literature. Given similar training token sizes and model sizes, models trained with the proposed method outperform strong baselines, such as RWKV7 and Gated DeltaNet, by a large margin, especially on NIAH-word, demonstrating better robustness. \n\n3. The paper further demonstrates the efficiency of the proposed method compared with other linear attention methods and also shows the general reasoning capabilities of the method, adding empirical strengths to the method."}, "weaknesses": {"value": "1. The evaluation of long-context retrieval is limited to NIAH and its variants. The proposed method is only compared with baselines on NIAH-1, NIAH-2, and NIAH-Word in Table 2. On the passkey retrieval task, SILA is not compared with any other baseline; on the in-context recall task from MAD-Lab benchmark, SILA is only compared with baselines in the setting using 2-layer shallow models instead of the 0.6 B model. The empirical strength of SILA on long-context retrieval needs to be further validated by comparisons with other methods on more benchmarks, such as MAD, Multi-query associative calls (MQAR), and RegBench. \n\n2. There lacks analysis of different components of SILA. There is no ablation of the loss design or gate design of SILA, and it is unclear which part of the proposed method contributes most to the empirical improvements on NIAH. \n\n3. Particularly, there is no analysis of how SILA addresses the memorization issues of other linear attention methods on NIAH: Is there evidence for SILA models not biasing towards digit tokens and performing general retrieval tasks, beyond the results on NIAH-Word? Is there evidence for SILA models correctly attending to the instruction tokens for the strong instruction variants of NIAH?\n\n\nThat being said, I'm willing to adjust the scores if the authors can provide more analysis results, especially the ones mentioned above."}, "questions": {"value": "1. In Table 2, are the NIAH-1 and NIAH-2 datasets with or without sample-level shuffling, following findings from Figure 1(b)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4CBBr9F2f8", "forum": "csZndfXLpC", "replyto": "csZndfXLpC", "signatures": ["ICLR.cc/2026/Conference/Submission23737/Reviewer_iNnf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23737/Reviewer_iNnf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762194112208, "cdate": 1762194112208, "tmdate": 1762942784689, "mdate": 1762942784689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}