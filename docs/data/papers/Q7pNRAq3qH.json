{"id": "Q7pNRAq3qH", "number": 11259, "cdate": 1758194560411, "mdate": 1763729873239, "content": {"title": "A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models", "abstract": "The foundational premise of generative AI for images is the assumption that images are inherently low-dimensional objects embedded within a high-dimensional space. Additionally, it is often implicitly assumed that thematic image datasets form smooth or piecewise smooth manifolds. \nCommon approaches overlook the geometric structure and focus solely on probabilistic methods, approximating the probability distribution through universal approximation techniques such as the kernel method.\nIn some generative models the low dimensional nature of the data manifest itself by the introduction of a lower dimensional latent space. Yet, the probability distribution in the latent or the manifold's coordinate space is considered uninteresting and is predefined or considered uniform.\nThis study unifies the geometric and probabilistic perspectives by providing a geometric framework and a kernel-based probabilistic method simultaneously. The resulting framework demystifies diffusion models by interpreting them as a projection mechanism onto the manifold of ''good images''. This interpretation leads to the construction of a new deterministic model, the Manifold-Probabilistic Projection Model (MPPM), which operates in both the representation (pixel) space and the latent space.\nWe demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion Model (LDM) across various datasets, achieving superior results in terms of image restoration and generation.", "tldr": "This study bridges geometric and probabilistic approaches in generative AI.", "keywords": ["generative AI", "image manifolds", "diffusion models", "kernel methods", "probabilistic modeling", "latent space", "manifold learning", "MPPM", "LMPPM", "image generation", "universal approximation", "geometric methods"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c097e9d5d3fbf64707ee5309410c5855225f3e89.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The article presents an integrated perspective that unifies geometric and probabilistic views by introducing a geometric framework and a kernel-based probabilistic method. Within this framework, the diffusion model is interpreted as a projection mechanism on the manifold of “high-quality images,” providing new insight into its underlying nature. Building on this interpretation, the authors propose a deterministic model—the Manifold Probability Projection Model (MPPM)—which operates coherently in both the representation (pixel) and latent spaces. Experimental results indicate that the Latent Space MPPM (LMPPM) surpasses the latent diffusion model (LDM) across multiple datasets, demonstrating superior performance in image restoration task."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The perspective of this article is very interesting. It is of great significance to unify the understanding of geometry and probability. This is of great significance for modeling more complex data manifold distributions.\n\n2.The theory in this article is very solid. As a work of great theoretical significance, it deserves attention.\n\n3.The paper is well-written and the motivation is very convincing."}, "weaknesses": {"value": "1.I have some concerns about the theoretical assumptions.  The article assumes the existence of Gaussian noise perturbations between points on the clean image manifold and the real images.  However, if the task is not image restoration, or if the data are already sufficiently clean, this assumption and consequently the proposed theory may not hold effectively. \n\n2.The experimental section lacks comparisons with several relevant baselines in image restoration and inverse problem research [1–3]. In addition, manifold-preserving approaches [4–6] should also be considered for a more comprehensive evaluation. It seems insufficient that the author only compares with DAE and LDM.\n\n[1] A Unified Conditional Framework for Diffusion-based Image Restoration\n\n[2] DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior\n\n[3] Refusion: Enabling Large-Size Realistic Image Restoration\n\n[4] Manifold Preserving Guided Diffusion\n\n[5] CFG++: Manifold-Constrained Classifier-Free Guidance for Diffusion Models\n\n[6] Improving Diffusion Models for Inverse Problems using Manifold Constraints\n\n3.Why do other methods work well on SSIM but worse on FID? Could this be because they only learned the distribution with noise added instead of the clean data distribution? The author lacks a more profound analysis."}, "questions": {"value": "1.Would the proposed method still be effective under the assumption of clean data? Or whether it can be directly used for generating images rather than for image restoration tasks?\n\n2.Refer to Weakness 2, how about the performance of other related models?\n\n3.In Figure 5, I noticed that the image generated by LMPPM does not contain white teeth. Could this be caused by the limitations of some manifold probability distributions? Or is it because of some probability assumptions that the model ignores the special manifold of the tooth part?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "85qqNPRwyI", "forum": "Q7pNRAq3qH", "replyto": "Q7pNRAq3qH", "signatures": ["ICLR.cc/2026/Conference/Submission11259/Reviewer_Vt93"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11259/Reviewer_Vt93"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760514908286, "cdate": 1760514908286, "tmdate": 1762922418590, "mdate": 1762922418590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new model, the Manifold Probabilistic Projection Model (MPPM) and its latent version, which interprets diffusion models as geometric projections that iteratively move corrupted inputs toward the clean image manifold. Based on the manifold assumption that image data resides on a low-dimensional smooth manifold, the paper integrates a learned distance function to the probability vector fields to guide image reconstruction and generation. The method shows superior performance compared to the latent diffusion model on image restoration and generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a new view of the diffusion model as a projection onto the manifold. \n\n- Using a distance-based geometric approach and a kernel-based probabilistic model, the paper tries to make an interpretable link between them and attempts to make a unified framework.\n\n- Detailed definitions on loss, architectures, and training settings have been provided in the Appendix."}, "weaknesses": {"value": "- While the idea of viewing the diffusion model as a projection onto the manifold is interesting, I cannot find a theoretical explanation or demonstration that the iterative process approximates a projection. Also, Equations 11 and 12 are heuristic updates without any guarantee of convergence.\n\n- The formulation is overly complex without clear benefit. Distance function, kernels, and autoencoders introduce considerable complexity, but I do not see why it should be explicitly better than exisiting diffusion models. Empirical results cannot be the justification as the datasets are too small and baselines are too weak.\n\n- The experiments are limited to simple datasets: MNIST and SCUT-FBP5500 datasets. It does not show general applicability or scalability. Experiments on datasets with the scale of CIFAR-10 or LSUN would be recommended. Also, the compared baselines are too weak and naive to say the complex formulation of the method should be used.\n\n- As the model introduces additional networks and iterative updates, it should require a comparison of computational complexity to diffusion models. Does learning distance functions and using it cost significantly?\n\n- Ablation analysis on the main components, like the distance function or kernels, would make the claim of the paper stronger."}, "questions": {"value": "Please address the questions raised in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "i9WS99cI2C", "forum": "Q7pNRAq3qH", "replyto": "Q7pNRAq3qH", "signatures": ["ICLR.cc/2026/Conference/Submission11259/Reviewer_UwHW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11259/Reviewer_UwHW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577661429, "cdate": 1761577661429, "tmdate": 1762922417931, "mdate": 1762922417931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduce a geometric picture framing VAE, GAN diffusion with respect to the data manifold, and they interpreted diffusion models as iterative manifold projection. Then they derive a model / objective (LMPPM) based on this idea, and and show some improved performance regarding clearing image degradation on some datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "### Strength\n\n- The conceptual discussion about the manifold geometry in diffusion, VAE and GAN is interesting, and think about diffusion model in this geometric way is laudable."}, "weaknesses": {"value": "### Weakness\n\n- The new methods in Sec.4 is not very convincing and/or not super well framed in the literature. Specifically, I feel there are so many connection to existing diffusion models, energy based model etc., just by re-interpreting the entities.\n    - e.g. the first term of the loss in eq. 13 learn the distance or energy instead of the score vector itself.\n    - the 5th term is very similar to the denoising score matching objective if we parametrize score by denoisers [^3,^4] since $z_i^{shift}$ is basically the denoiser. which is enforcing the gradient of distance to be nicely aligned to score\n    - In this regard, it seems the main innovation is that we have a distance function without explicit conditioning of noise scale. But seems [^5,^6] also discussed / discovered that the time / noise scale conditioning is not necessary.\n\n[^5] Sun, Q., Jiang, Z., Zhao, H., & He, K. (2025). Is Noise Conditioning Necessary for Denoising Generative Models?.\n\n[^6] Kadkhodaie, Z., Guth, F., Simoncelli, E. P., & Mallat, S. (2024). Generalization in diffusion models arises from geometry-adaptive harmonic representations. ICLR\n\n- From the algorithm or the method itself, I cannot see a clear reason why the proposed MPPM or LMPPM method is better than LDM. is it the case than LDM needs a certain noise / time conditioning, thus if you input the wrong noise / time, it will not correctly denoise the image? but for LMPPM, you have no time conditioning, so you are more robust in that regard?\n    - Currently the FID in Table 1 is very high for LDM and DAE, which is a bit concerning. I feel something is wrong in the implmentation of these baselines…\n    - Elucidating why LMPPM is better via ablation / control experiment can largely improve the paper, and increase my evaluation of the paper."}, "questions": {"value": "- in abstract why do the authors say “*The foundational premise of generative AI for images is the assumption that images are inherently low-dimensional objects embedded within a high dimensional space*”? Seems generative AI can still work if images are not low dimensional objects…. I agree with the assumption, but do not think it’s a foundational premise of generative AI.\n- I feel the geometric view of diffusion models (Sec 3, Fig. 2) is definitely correct and worth noting, but it’s also not entirely new. Authors could mention very similar figures as in Fig1 [^1] Fig4 [^2]. e.g. the quantity noted in eq. 8 has name in many papers, i.e. ideal denoiser [^3,^2], and the relation between score and denoiser has been known as tweedie’s formula [^4]. $\\hat{x}_{\\text{MMSE}} = \\mathbb{E}[u \\mid x] = x + \\sigma^2 \\nabla_x \\log P(x)$\n\n[^1] Chen, D., Zhou, Z., Wang, C., Shen, C., & Lyu, S. (2024). On the trajectory regularity of ode-based diffusion sampling. ICML https://arxiv.org/abs/2405.11326 \n\n[^2] Wang, & Vastola, (2024). The unreasonable effectiveness of gaussian score approximation for diffusion models and its applications. TMLR https://arxiv.org/abs/2412.09726\n\n[^3] Karras, T., Aittala, M., Aila, T., & Laine, S. (2022). Elucidating the design space of diffusion-based generative models. NeurIPS \n\n[^4] Efron, B. (2011). Tweedie’s formula and selection bias. Journal of the American Statistical Association\n\n- Eq.14 is also known as ideal denoiser with delta mixture distribution / empirical distribution [^2,^3].\n\n- As the authors pointed out the Riemannian geometry of data manifold through the generator of GAN, VAE have been studied for a while, some reference could be added for this tradition [^7,^8,^9].\n\n[^7] Shao, H., Kumar, A., & Fletcher, P. T. (2017). The riemannian geometry of deep generative models. *CVPR Workshops* \n\n[^8] Wang, B., & Ponce, C. R. (2021). The geometry of deep generative image models and its applications. ICLR\n\n[^9] Chadebec, C., & Allassonnière, S. (2022). A geometric perspective on variational autoencoders. NeurIPS"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "toQlrhigJb", "forum": "Q7pNRAq3qH", "replyto": "Q7pNRAq3qH", "signatures": ["ICLR.cc/2026/Conference/Submission11259/Reviewer_oVYE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11259/Reviewer_oVYE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799433486, "cdate": 1761799433486, "tmdate": 1762922417617, "mdate": 1762922417617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank the reviewers for their time and insightful comments. We are encouraged by the positive feedback on the theoretical rigor (**Reviewers uPVp, Vt93**) and the geometric intuition (**Reviewer OVYE**).\n\nBased on your feedback, we have updated the manuscript significantly. The major changes include:\n\n- **New Experiments on CelebA**: To address concerns regarding dataset scale (Reviewers uPVp, UwHW), we added restoration results on CelebA (Appendix D, Figs. 18-19).\n    \n- **Toy Example Analysis**: We added a 1D manifold embedded in 3D (Figs. 10-11) to demonstrate the geometric superiority of MPPM in low-density regions (addressing Reviewers OVYE, Vt93).\n    \n- **Extended Related works**:  throughout the theoretical part, we added relations and differences with respect to the existing literature (EBMs, noise conditioning and more). \n    \n- **Theoretical Proofs**: We added a reference and the relation to the Tweedie’s formula and a proof that the distance to the manifold decreases along the flow.\n    \n- **Ablation study**:  We studied the effect of our new distance function via an ablation study.\n\n * **FID**:  We added several experiments to verify that our implementations of DAE and DM/LDM are correct."}}, "id": "WWISA0uPwB", "forum": "Q7pNRAq3qH", "replyto": "Q7pNRAq3qH", "signatures": ["ICLR.cc/2026/Conference/Submission11259/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11259/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11259/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728913870, "cdate": 1763728913870, "tmdate": 1763728913870, "mdate": 1763728913870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Manifold-Probabilistic Projection Model (MPPM) and its latent variant (LMPPM), which unify geometric and probabilistic interpretations of generative modeling. The method interprets diffusion models as iterative projections onto the manifold of “good” images, defined through a distance function and an associated kernel-based probability density. The authors derive this formulation rigorously from geometric principles, introduce both ambient-space and latent-space implementations, and connect the model to classical autoencoder architectures.\nThe paper is well-written, mathematically detailed, and offers a clear conceptual framework that bridges geometry and probability in generative modeling."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Excellent clarity and presentation of the theoretical derivation.\n* Well-organized narrative: the geometric intuition, probabilistic extension, and algorithmic details are all coherent and rigorous.\n* The proposed framework provides an elegant deterministic alternative to diffusion sampling, supported by sound intuition.\n* Clear and readable mathematical notation throughout."}, "weaknesses": {"value": "**Experimental scope:**\n* Despite the theoretical strength, experiments are limited to MNIST and SCUT-FBP5500, which are small and relatively trivial datasets. After such a solid theoretical development, this weak experimental section feels like a missed opportunity.\n* Evaluations rely mainly on the Latent MPPM variant, and mostly in a reconstruction setting rather than true generation.\n* While reconstruction is a valid demonstration, it is not the most relevant metric for generative models. The paper would be much stronger if generation quality were assessed on more challenging datasets such as ImageNet 64×64, CelebA-HQ, or CIFAR-10. Even a small-scale generation study (e.g., 32×32), if focused, would make the contribution more complete.\n\n**Focus dilution:**\nThe inclusion of reconstruction experiments makes the paper feel slightly misaligned with its main message. A more focused evaluation of generation performance would better highlight the model’s strengths.\n\n**Minor technical comments:**\n* Missing citations for the Eikonal equation (line 145) and kernel density estimation (line 185).\n* Line 216: the term “normalized gradient” seems redundant since $|| D_M (x) || = 1$ by the Eikonal equation.\n* Equation (10): unclear why $G(z)$ appears outside the exponential.\n* Line 55: the authors do not **propose** the manifold assumption but rather **assume** it."}, "questions": {"value": "I have no questions other than asking the authors to perform more focused experiments, as indicated in the \"Weakness\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4GIbjlOkM6", "forum": "Q7pNRAq3qH", "replyto": "Q7pNRAq3qH", "signatures": ["ICLR.cc/2026/Conference/Submission11259/Reviewer_uPVp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11259/Reviewer_uPVp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846416376, "cdate": 1761846416376, "tmdate": 1762922417252, "mdate": 1762922417252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}