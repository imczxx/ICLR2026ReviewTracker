{"id": "gQPD83DrGp", "number": 21885, "cdate": 1758323124270, "mdate": 1763579563124, "content": {"title": "Measuring the Intrinsic Dimension of Earth Representations", "abstract": "Within the context of representation learning for Earth observation, geographic Implicit Neural Representations (INRs) embed low-dimensional location inputs (longitude, latitude) into high-dimensional embeddings, through models trained on geo-referenced satellite, image or text data. Despite the common aim of geographic INRs to distill Earth's data into compact, learning-friendly representations, we lack an understanding of how much information is contained in these Earth representations, and where that information is concentrated. The intrinsic dimension of a dataset measures the number of degrees of freedom required to capture its local variability, regardless of the ambient high-dimensional space in which it is embedded. This work provides the first study of the intrinsic dimensionality of geographic INRs. Analyzing INRs with ambient dimension between 256 and 512, we find that their intrinsic dimensions fall roughly between 2 and 10 and are sensitive to changing spatial resolution and input modalities during INR pre-training. Furthermore, we show that the intrinsic dimension of a geographic INR correlates with downstream task performance and can capture spatial artifacts, facilitating model evaluation and diagnostics. More broadly, our work offers an architecture-agnostic, label-free metric of information content that can enable unsupervised evaluation, model selection, and pre-training design across INRs.", "tldr": "We show that the intrinsic dimension of a geographic Implicit Neural Representation (INR) informs how (i) representative, (ii) task-aligned, and (iii) spatially-biased the INR is.", "keywords": ["Intrinsic Dimension", "Implicit Neural Representations", "Location Encoding", "Positional Encoding", "Earth Observation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e390d64e2346568cdd081d951e99bcc8acf1e6df.pdf", "supplementary_material": "/attachment/d880926b13a73d6158feef925a4f1705f158d1d7.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents techniques to measure intrinsic dimension and degree of task-alignment for various geospatial location and image encoders. The paper compares various global and local measures of intrinsic dimension. The paper conducts various experiments comparing the ID of location and image encoder against downstream task performance. This could prove valuable to the community for analyzing the informativeness of geospatial embeddings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well motivated and makes a novel contribution by analyzing the intrinsic dimension of various geospatial location and image encoders and comparing it with downstream task performance.\n2. Geospatial embeddings are increasingly being used to address various geospatial tasks. The paper presents a methodology to evaluate the \"goodness\" of these embeddings generated using existing geospatial encoders. This methodology can be very useful for the community in future."}, "weaknesses": {"value": "1. It is difficult to comprehend clearly the methods used for computing global and local IDs. Theory is included in the supplementary material which is very useful but I highly suggest moving some important mathematical theory into the main paper.\n2. Although measuring IDs of geospatial models an interesting direction, the paper falls short of several experiments that could help strengthen the paper:\n* How do IDs relate to PCA/ICA and geospatial embedding visualization? It might be interesting to discuss how IDs can be used for possibly better visualization of geospatial embeddings than maybe using PCA/ICA which are not spatially explicit.\n* One important experiment is missing that compares ID with the embedding dimension for each model architectures. The paper should discuss how the intrinsic dimension is related to the overall dimensionality of the embeddings (for the same model architecture and training regime). For example, if GeoCLIP is retrained with an embedding dimension of 768 or 1024, how would the intrinsic dimension change? Would it increase, decrease or remain the same. The same goes for image encoders. How would the intrinsic dimension vary when ScaleMAE is trained with different embedding dimensions.\n* For measuring the IDs of image encoders, the authors should consider comparing the IDs across different spatial resolution of satellite images. **Comparing the IDs of image encoders using low resolution Sentinel-2 imagery is not enough to conclude the global ID of GeoCLIP approaches to that of various image encoders**. The authors should also experiment with high resolution imagery as that will possibly increase the IDs of image encoders.\n* Concrete measures for task-alignment need to be shown. The authors could compute a mutual information estimate such as CLUB [1] between the geospatial embeddings and downstream task variables and compare it against the ID values to truly understand if IDs are correlated with downstream task performance.\n* Can ID measure be made more task-specific since tasks vary in spatial frequency. Currently, ID seems to be independent of task and only dependent on the embedding model.\n* **How do IDs relate to other downstream tasks that go beyond memorization** especially where the location encoders are used as a geo-prior such as satellite image generation or fine-grained image classification? **I believe the results demonstrated in the paper can easily be gamed by overfitting on the task, the authors need to show some analysis on tasks that are more challenging such as generation.**\n* It should be interesting to see IDs for non-geospatial image encoders such as Jepa or Dinov2. Recently, webscale version of Dinov3 has shown to outperform the satellite-only Dinov3 model on various geospatial downstream tasks.\n* Limited location and image encoders are compared. There are recent task-aligned location encoder models such as SINR, TaxaBind and Climplicit. There are multimodal image encoders such as AnySat and Galileo and Range. It will be interesting to see the ID analysis on such models.\n\n**Suggestion**\n\nI appreciate the authors for exploring the IDs of various geospatial models. The idea and motivation behind the paper is novel and will surely be useful for the community. However, the paper requires a major revision as the **paper makes strong claims without a comprehensive and sound experimental setup**. I would suggest the authors to include the above suggested experiments to improve the paper. The paper should clearly state concrete applications of computing the IDs of geospatial models. The paper also has typos which need to be fixed. The paper could be structured better with more theory in the main paper and better explanations for technical terms. I am definitely willing to increase my score once the authors have addressed my concerns."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OCYAmqpgYN", "forum": "gQPD83DrGp", "replyto": "gQPD83DrGp", "signatures": ["ICLR.cc/2026/Conference/Submission21885/Reviewer_Ayws"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21885/Reviewer_Ayws"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761513525337, "cdate": 1761513525337, "tmdate": 1762941969304, "mdate": 1762941969304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a study on the intrinsic dimensionality of geographic implicit neural representations (INRs), quantifying how much real information these learned location embeddings actually capture. The authors present some interesting findings including that global ID estimates of geographic INRs are competitive with those of large-scale image encoders, and that that global ID correlates positively with downstream performance when measured on frozen, pre-trained models (representativeness), but correlates negatively when measured in the activation space of supervised models (task-alignment). \n\nOverall, I find this work quite interesting and valuable, as their methodology offers a principled way to measure information content in location embedding techniques beyond traditional downstream performance metrics."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "[S1] The paper is the first to examine the intrinsic dimension of location embeddings, providing valuable insights into the representational limits and strengths of popular approaches. The study is especially useful given the large and growing number of works on location embeddings in GeoAI.\n\n[S2] The paper is well written and easy to follow. The methodology for estimating intrinsic dimension is clearly explained, and the results give interesting insights, including how intrinsic dimension varies with model architecture, input modalities, and spatial resolution.\n\n[S3] The work is clearly written, well-organized, and thoroughly evaluated across a diverse range of models and experimental cases."}, "weaknesses": {"value": "[W1] The TwoNN estimator is used to measure task alignment, with low ID values observed for task-specific representations. Can the authors elaborate on whether this could be an artifact of estimator bias due to non-uniform data, rather than actual compression?\n\n[W2] We observe notable differences in Global ID estimates across different estimators, especially for SatCLIP (e.g., SatCLIP-L40: 8.08 for FisherS vs. 2–2.5 for MLE, TLE, and TwoNN). The authors primarily focus on FisherS, which consistently gives much higher values, but the reasons for this discrepancy are not fully discussed. Is such a large gap expected in practice? Can the authors provide further empirical or theoretical justification for prioritizing FisherS over other estimators for Global ID?"}, "questions": {"value": "[1] Please see W1 & W2. \n[2] I am wondering if the study can generally extended to study encoders of geospatial objects such as [a] [b] [c]\n\n[a] \"Towards general-purpose representation learning of polygonal geometries.\" GeoInformatica 2023.\n\n[b] \"Poly2Vec: Polymorphic Fourier-Based Encoding of Geospatial Objects for GeoAI Applications.\" ICML 2025.\n\n[c] \"Geo2Vec: Shape-and Distance-Aware Neural Representation of Geospatial Entities.\" arXiv 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iXF6QuQHDH", "forum": "gQPD83DrGp", "replyto": "gQPD83DrGp", "signatures": ["ICLR.cc/2026/Conference/Submission21885/Reviewer_1jrM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21885/Reviewer_1jrM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851062880, "cdate": 1761851062880, "tmdate": 1762941968983, "mdate": 1762941968983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel research on the intrinsic dimension (ID) of geographic Implicit Neural Representations (INRs). While the measures used to quantify ID already exist in literature, it is important to bring the concept to the community of spatial representation learning. The authors also conducted comprehensive analysis on how ID affects the downstream performance of geographic INRs, revealing that ID can be a good index of representation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It is important to introduce the concept of ID to the community of spatial representation learning. Unlike image or text embedding where there is **compression** of information, spatial representation such as location encoding usually embeds very low dimensional data (e.g. two-dimensional latitudes and longitudes) into high dimensional spaces, which is not a compression but an expansion. The quality of the expansion, i.e., whether the embedding manifold maintains useful topological information, is a critical concern -- by the curse of dimensionality, higher dimensions tend to make embeddings less distinguishable and less informative.\n\n2. Local ID and global ID allow fine-grained analysis of the embedding manifold.\n\n3. The correlation analysis between ID and downstream task performance is very useful. The positive/negative correlations can be used to guide spatial representation learning or to select appropriate embedding methods for specific downstream tasks."}, "weaknesses": {"value": "1. The linear correlations fitted in Figure 3 and Figure 4 can be misleading. From the authors' perspective, they only need to show that the ID and model performance are positively/negatively correlated; the correlation does not need to be linear. For example, the subplots in Figure 3a look more quadratic/logarithmic than linear.\n\n2. Table 1 can not be used to fairly compare the ID of baseline models. The models evaluated are trained on very different datasets and downstream tasks. For example, GeoCLIP having very high ID, from my experience, owes a lot to the spatial bias of the MP16 dataset it was trained on. This is also shown in Figure 2. In order to make the comparison between numbers meaningful, it is better to train different models on the same dataset(s)."}, "questions": {"value": "1. The non-linear correlations in Figure 3 and Figure 4 may have a theoretical implication. In the 4th chapter of https://escholarship.org/uc/item/5bs589v5, it is mathematically proven that the average information content in the samples drawn within a given geographical region decreases logarithmically when there is spatial dependency.\n\n2. Can you train some baseline models on a spatially balanced dataset (e.g. OSV5M) and compare their ID? This is more useful to prove that ID can be used to quantify the quality of an embedding method.\n\n3. Do you have any idea why in Figure 5, the Space2Vec method has such large increase in ID compared to Sphere2Vec variants?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "frdYQtIpLv", "forum": "gQPD83DrGp", "replyto": "gQPD83DrGp", "signatures": ["ICLR.cc/2026/Conference/Submission21885/Reviewer_zZ97"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21885/Reviewer_zZ97"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953294282, "cdate": 1761953294282, "tmdate": 1762941968762, "mdate": 1762941968762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for quantifying the intrinsic dimension of Earth representations and studying the correlation between ID with the model performance on downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The  author proposed to use Intrinsic Dimension (ID) as a new metric to quantify the representativeness and task-lignment of the Earth representation. It gives us a way to interpret the location encoders and how the design can impact the model performance\n2. A thorough analysis has been carried out to analysis the relation between ID and the task performance in the context of Earth presentation learning."}, "weaknesses": {"value": "Although I enjoy reading this paper, there are some weaknesses I need to point out:\n1. In the main results table (Table 1), the authors compared different pretrained location encoders with the global ID metric. However, different location encoders have different designs, different pretraining objectives, and different pretraining datasets and modalities. It is very hard to see any pattern here. A controlled experiment is needed in which the type of location encoders, the pretraining objectives, and the pretrained datasets need to be ablated to see how different parts impact the ID and performance of the location representations.\n2. Figure 6 has a similar problem: 3 models use different data modalities, different architectures, and different pretraining strategies. A controlled experiment is needed here."}, "questions": {"value": "1. \"Global IDs of geographic INRs are similar to that of embeddings derived from image encoders\". However, compared with image encoders, location encoders have much less learnable parameters. Does this result indicate that the ID depends more on the dataset and task characteristics (e.g., spatial distribution of the class labels) instead of the model?\n2. Figrue 4 shows that \"Lower global ID in activation space of supervised models corresponds to higher task performance\". Why the results from location embeddings and activation features show a reverse pattern? \"past work, that found lower ID indicates more concentrated, linearly separable structure and thus better generalization\" Can you explain this in details?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "X5XvSbUsk3", "forum": "gQPD83DrGp", "replyto": "gQPD83DrGp", "signatures": ["ICLR.cc/2026/Conference/Submission21885/Reviewer_F7dp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21885/Reviewer_F7dp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962401764, "cdate": 1761962401764, "tmdate": 1762941968541, "mdate": 1762941968541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response by the Authors to all Reviewers"}, "comment": {"value": "We thank all reviewers for their insightful questions and feedback on our work.\n\nWe are particularly encouraged by the consensus that our work is “novel” (Reviewers zZ97, Ayws, 1jrM), “well-motivated” (Reviewer Ayws), “useful” (Reviewers Ayws, zZ97, 1jrM), and the positive feedback that our work is “well-written, easy to follow, and well-organized” (Reviewer 1jRM) with our experimental setup on measuring the ID of geographic implicit neural representations described as “thorough” (Reviewer F7dp) and “comprehensive” (Reviewers zZ97, 1jrM). While the concept of intrinsic dimension has existed in classical information theory since the early 60s, and in deep learning for close to two decades, we are grateful for the reviewers for noting that this the first attempt to introduce this principled evaluation metric to the spatial representation learning community and location embeddings (Reviewers F7dp, zZ97, 1jrM) especially amidst their growing popularity and increased use (Reviewers Ayws, 1jrM). \n\nWe have responded to individual reviewers’ comments, and are confident that we can address the remaining questions and critiques during the remainder of the discussion period. Motivated by the feedback, we have made changes to our now revised manuscript with changes marked in blue font. We are available to assist if any questions remain."}}, "id": "oAsWuSr9gl", "forum": "gQPD83DrGp", "replyto": "gQPD83DrGp", "signatures": ["ICLR.cc/2026/Conference/Submission21885/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21885/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission21885/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763613812313, "cdate": 1763613812313, "tmdate": 1763613812313, "mdate": 1763613812313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}