{"id": "zEJd3JXVxb", "number": 14264, "cdate": 1758231489204, "mdate": 1763512252561, "content": {"title": "DD-Ranking: Rethinking the Evaluation of Dataset Distillation", "abstract": "In recent years, dataset distillation has provided a reliable solution for data compression, where models trained on the resulting smaller synthetic datasets achieve performance comparable to those trained on the original datasets. To further improve the performance of synthetic datasets, various training pipelines and optimization objectives have been proposed, greatly advancing the field of dataset distillation. Recent decoupled dataset distillation methods introduce soft labels and stronger data augmentation during the post-evaluation phase and scale dataset distillation up to larger datasets (e.g., ImageNet-1K). However, this raises a question: Is accuracy still a reliable metric to fairly evaluate dataset distillation methods? Our empirical findings suggest that the performance improvements of these methods often stem from additional techniques rather than the inherent quality of the images themselves, with even randomly sampled images achieving superior results. Such misaligned evaluation settings severely hinder the development of DD. Therefore, we propose DD-Ranking, a unified evaluation framework, along with new general evaluation metrics to uncover the true performance improvements achieved by different methods. By refocusing on the actual information enhancement of distilled datasets, DD-Ranking provides a more comprehensive and fair evaluation standard for future research advancements.", "tldr": "", "keywords": ["dataset distillation", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/236fab02ef934897396c205610574aeedacfee60.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper first observes that existing dataset distillation approaches adopt inconsistent evaluation protocols, differing in their use of label types (e.g., fixed hard/soft labels or per-image soft labels), data augmentation strategies, etc. To enable a fair comparison across methods, the authors propose a unified benchmark along with two new evaluation metrics, LRS and ARS, designed to be robust against variations in the use of label types and augmentation techniques. Experimental results reveal that recent approaches relying heavily on soft labels (i.e., decoupled methods) are ineffective, often performing worse than a random selection baseline. In contrast, more conventional methods employing a single hard or soft label per image continue to demonstrate superior performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The introduction of a unified benchmark for dataset distillation methods is both timely and significant for the community. Establishing a standardized evaluation protocol is crucial for ensuring fair comparisons and clear assessments across approaches. The paper convincingly shows that recent methods relying on per-epoch soft labels have been overrated.  \n- The paper proposes novel evaluation metrics, LRS and ARS. These metrics provide a more robust and flexible framework for evaluating methods that adopt different recipes to train networks on distilled datasets.\n- The paper provides extensive experimental results."}, "weaknesses": {"value": "- The main finding that improvements in decoupled methods come largely from knowledge distillation rather than the distilled dataset itself is interesting, but this point has already been raised in [1]. It would be helpful to acknowledge and connect to that prior work.  \n- Some experimental results appear to be missing. For example, lines 149–161 mention results on random noise with soft labels, but I could not find the corresponding figure.  \n- While comparing against randomly selected samples is meaningful, I am less convinced by the comparisons using hard labels or without augmentation. Many methods are explicitly designed with particular label formats and augmentation strategies (e.g., DATM, EDF, IDC, FYI, etc.), so the evaluation feels less fair in those settings.\n- The evaluation metrics introduce tunable parameters ($\\lambda$ in Eq. (3) and $\\gamma$ in Eq. (4)). This flexibility may unintentionally influence the results and deserves some discussion.\n- A few methods evaluated in Sec. 4.2 are not included in Sec. 4.3, which leaves the comparison incomplete.\n\n[1]: Qin, Tian, Zhiwei Deng, and David Alvarez-Melis. \"A label is worth a thousand images in dataset distillation.\" Advances in Neural Information Processing Systems 37 (2024): 131946-131971."}, "questions": {"value": "- How were the soft labels generated for random images when comparing with DATM or EDF? In the original methods, labels are jointly optimized with the images. Clarification on this process would be helpful.\n- Why are learning rates tuned specifically for randomly selected images (lines 267–269) instead of applying the same learning rate used with synthetic images? Please explain the rationale behind this choice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LW18sMTtFX", "forum": "zEJd3JXVxb", "replyto": "zEJd3JXVxb", "signatures": ["ICLR.cc/2026/Conference/Submission14264/Reviewer_6gJb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14264/Reviewer_6gJb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761205226725, "cdate": 1761205226725, "tmdate": 1762924716191, "mdate": 1762924716191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new evaluation benchmark for dataset distillation (DD) in image classification, aiming to assess the effectiveness of distilled datasets compared to random selection. The benchmark focuses on two proposed metrics: label-robust score and augmentation-robust score.\nAs summarized in Table 1, existing DD methods differ significantly in their use of labels (hard vs. soft labels, and whether soft labels come from a fully-trained teacher or are jointly optimized during distillation) and augmentations (e.g., resize-crop, patch-shuffle, cutmix). These differences make direct comparison of DD methods difficult. The paper argues that prior evaluations, which each use their own label and augmentation setups, are unfair and inconsistent.\nTo address this, the authors propose:\n* Label-robust score: compares the accuracy of distilled data versus random selection under the same label setting (e.g., both using hard labels or the same soft labels). \n* Augmentation-robust score: compares distilled data versus random selection under the same augmentation setting (e.g., same augmentation type or no augmentation). \n\nThe proposed benchmark aims to standardize evaluation conditions and reveal the true contribution of the distilled images themselves, separate from the effects of labels or augmentations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper provides a meaningful attempt to standardize the evaluation of dataset distillation methods, enabling a more controlled comparison against random selection under matched label and augmentation setups. \n* The results highlight interesting findings: under hard-label usage, matching-based DD methods remain stronger than recent soft-label–based approaches, suggesting that much of the improvement in newer methods (e.g., SRe2L) may stem from knowledge distillation rather than from the intrinsic quality of the synthetic images."}, "weaknesses": {"value": "* Limited applicability of the metric: Although the proposed metrics allow comparisons under matched label/augmentation setups, they do not measure the ultimate achievable performance of each DD method under its best hyperparameter and setup choices. Since DD performance also depends on factors like architecture, optimizer, and training configuration, comparing distilled datasets only under uniform conditions offers limited insight into each method’s full potential. \n* Ambiguous interpretability of the two measures: The two metrics—label-robust and augmentation-robust scores—merely quantify relative test accuracies rather than any intrinsic quality of the synthetic datasets. It is unclear how these two scores should be used jointly or whether they could be unified into a single, more interpretable evaluation measure. \n* Limited scope beyond image classification: The paper focuses solely on image classification. Modern distillation applications extend to vision-language and language model distillation, where data efficiency is more critical. It is unclear how the proposed robustness metrics could generalize to multimodal or text-based distillation tasks, limiting the broader applicability and fundamental impact of the proposed benchmark."}, "questions": {"value": "1. On metric applicability:\n    * How do the proposed label-robust and augmentation-robust scores reflect the best achievable performance of each DD method? \n    * Could the benchmark be extended to allow comparisons when each method is evaluated under its own optimal settings (e.g., best label/augmentation choices)? \n2. On metric design and coherence:\n    * How should users interpret the two robustness scores jointly? \n    * Is there a principled way to combine the label-robust and augmentation-robust scores into a single unified measure that better reflects dataset quality? \n3. On generalization beyond image classification:\n    * Can the proposed evaluation framework be adapted for multimodal or language model distillation tasks, where label and augmentation definitions are more complex? \n    * If not, how might the authors envision extending these metrics to broader domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fGXbYpybXB", "forum": "zEJd3JXVxb", "replyto": "zEJd3JXVxb", "signatures": ["ICLR.cc/2026/Conference/Submission14264/Reviewer_2Cdm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14264/Reviewer_2Cdm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705417027, "cdate": 1761705417027, "tmdate": 1762924715782, "mdate": 1762924715782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies significant unfairness in current dataset distillation evaluation practices, mainly caused by inconsistent training configurations. In particular, the use of soft labels often leads to performance gains that originate from knowledge distillation rather than the actual quality of the synthetic data, while improvements from data augmentation do not necessarily indicate better dataset informativeness. To address these issues, the authors introduce three new evaluation metrics, namely Hard Label Recovery (HLR), Improvement Over Random (IOR), and Label Robust Score (LRS), which aim to disentangle the effects of knowledge distillation and data augmentation from the intrinsic performance of distilled datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly demonstrates that performance improvements in existing dataset distillation methods often result from knowledge distillation or data augmentation rather than from the informativeness of the synthetic images.\n2. The proposed evaluation metrics for comparing the performance of different models are clearly defined.\n3. The authors conduct experiments with LRS and ARS across different model architectures, teacher models, and hyperparameter settings to verify the robustness of the proposed method."}, "weaknesses": {"value": "1. The paper spends excessive space analyzing the limitations of existing methods. This part is repetitive and should be condensed into a shorter empirical motivation section.\n2. Although LRS and ARS are intuitively motivated, their theoretical foundation is weak and lacks conceptual depth.\n3. In Section 3, the definition of DD RANKING is unclear. It is not specified whether it refers to LRS, ARS, or a combination of both.\n4. In line 240, the description of the normalization method for computing ARS is vague and should be explicitly defined."}, "questions": {"value": "In line 210, does the random subset refer to real images from the original dataset or to synthetic noise samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "heMXYdNWG7", "forum": "zEJd3JXVxb", "replyto": "zEJd3JXVxb", "signatures": ["ICLR.cc/2026/Conference/Submission14264/Reviewer_KAsP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14264/Reviewer_KAsP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770253312, "cdate": 1761770253312, "tmdate": 1762924715070, "mdate": 1762924715070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that prior work on dataset distillation has been evaluated on an unfair playing field, and that simple accuracy comparisons do not guarantee fair or consistent assessment. To address this, the authors introduce DD-Ranking, which proposes a unified evaluation framework comprising four components to compare diverse DD methods under equitable criteria. A key strength is that DD-Ranking aims to deliver consistent evaluation irrespective of model architecture, the presence or absence of soft-label optimization, and the specific data-augmentation settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-written with a clear, thorough, and concise introduction that effectively summarizes key points\n* The authors demonstrated through extensive experiments that the proposed evaluation metrics are meaningful and effective."}, "weaknesses": {"value": "* Discussion of limitations is lacking\n* Theoretical background would be needed"}, "questions": {"value": "## Discussion of limitations is lacking\n* This paper evaluates performance only on datasets and models designed for classification tasks. I am curious how the authors envision establishing fair evaluation protocols in the context of multi-modal dataset distillation (MDD), where tasks and modalities may differ substantially.\n\n## Theoretical background would be needed\n* The paper aims to mitigate the unfairness introduced by data augmentation through the Augmentation-Robust Score (ARS). However, γ is fixed to 0.5, equally weighting the accuracy gaps obtained under augmentation and non-augmentation settings.\nConceptually, if the goal is to isolate and remove the influence of augmentation, measuring only the non-augmented accuracy gap (i.e., $acc_{syn-naug} − acc_{rdm-naug}$) might be sufficient and more principled. Why is the augmented gap ($acc_{syn-aug} − acc_{rdm-aug}$) necessary to include in the computation?\n* Moreover, have the authors examined which of the two gaps (augmented or non-augmented) plays a more significant role in determining distillation quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "55qzgPE0dX", "forum": "zEJd3JXVxb", "replyto": "zEJd3JXVxb", "signatures": ["ICLR.cc/2026/Conference/Submission14264/Reviewer_XoEQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14264/Reviewer_XoEQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806179380, "cdate": 1761806179380, "tmdate": 1762924714028, "mdate": 1762924714028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We thank all reviewers for constructive feedback. Here, we provide the response to some commonly asked questions.\n\n---\n\n## Theoretical Foundation\n\nWe provide a theoretical analysis of why LRS is fair for evaluating two methods A and B with different settings. ARS can be derived similarly. \n\n**Definition of Fair Comparison:** A comparison between methods A and B is fair if and only if the evaluation metric isolates the **data quality** contribution while neutralizing **setting-dependent** performance gains. Formally, we let:\n\n- $ q_A, q_B $: **Intrinsic data quality** of methods A and B\n- $ \\beta(\\pi) $: performance boost from the evaluation setting $\\pi $ (independent of data quality)\n\nConsider two DD methods:\n\n- Method A: distills $ \\mathcal{D}_A $ with evaluation protocol $ \\pi_A $ (e.g., hard labels, batch size 256, step learning rate scheduler)\n- Method B: distills $ \\mathcal{D}_B $ with evaluation protocol $ \\pi_B $ (e.g., soft labels, batch size 1024, cosine learning rate scheduler)\n\nWe can decompose the performance (accuracy in the image classification task) as follows:\n\n- $ Acc(D_A,\\pi_A)= q_A+ \\beta(π_A)+ \\epsilon_A $\n- $ Acc(D_B,\\pi_B)= q_B + \\beta(π_B)+ \\epsilon_B $\n\nwhere $\\beta(\\pi)$ represents the performance gain brought by the evaluation setting $\\pi$ and $\\epsilon \\sim N(0,1)$ is the residual term.\n\nFrom this, we can easily see that comparing accuracy directly is unfair since we have no clue about the relationship between $\\beta(\\pi_A)$ and $\\beta(\\pi_B)$. \n\nNow, we consider the random selection baseline: $Acc(D_{rand}, \\pi_{rand}) = q_{rand} + \\beta(\\pi_{rand}) + \\epsilon_{rand}$. We only show why IOR is fair and HLR can be derived similarly.\n\n- $IOR_{A} = Acc(D_A, \\pi_A) - Acc(D_{rand_A}, \\pi_{rand_A}) = q_A - q_{rand_A} + \\beta(\\pi_A) - \\beta(\\pi_{rand_A}) + \\epsilon_A - \\epsilon_{rand_A} = q_A - q_{rand_A} + \\epsilon_A - \\epsilon_{rand_A}$\n- $IOR_{B} = Acc(D_B, \\pi_B) - Acc(D_{rand_B}, \\pi_{rand_B}) = q_B - q_{rand_B} + \\beta(\\pi_B) - \\beta(\\pi_{rand_B}) + \\epsilon_B - \\epsilon_{rand_B} = q_B - q_{rand_B} + \\epsilon_B - \\epsilon_{rand_B}$\n\nWe can cancel the $\\beta$ terms because $\\pi_A = \\pi_{rand_A}$ and $\\pi_B = \\pi_{rand_B} $. Taking the expectation of the difference we have\n$$\n\\mathbb{E}[IOR_A - IOR_B] = \\mathbb{E}[q_A - q_B] - \\mathbb{E}[q_{rand_A} - q_{rand_B}] + \\mathbb{E}[\\epsilon_A - \\epsilon_B] - \\mathbb{E}[\\epsilon_{rand_A} - \\epsilon_{rand_B}]\n$$\nNow, for the same original dataset $D$, the expected intrinsic quality of random selection should be a constant, meaning $\\mathbb{E}[q_{rand_A} - q_{rand_B}] = 0$. Thus, we have $\\mathbb{E}[IOR_A - IOR_B] = \\mathbb{E}[q_A - q_B]$. **This shows that the comparison of $IOR$ is a direct comparison of the intrinsic synthetic data quality without any data-irrelevant factors.**\n\n---\n\n## Missing Results for Random Noise with Soft Labels\n\nWe are very sorry for the missing results. Here are the experiment results corresponding to Section 2.2.\n\n**Experiment Settings**\n\n- IPC: 50\n- Model: ResNet-18\n- Soft Label Temperature: 20.0 (same as SRe2L, RDED, etc.)\n- Data Augmentation: RandomResizedCrop (same as SRe2L)\n- Learning Rate: 1e-3\n- \\#Epochs: 300\n- Optimizer & Scheduler: AdamW & CosineAnnealing\n\n**Experiment Results**\n\n| dataset    | random guessing | batch size=128 | batch size=1024 |\n| :--------- | :-------------- | :------------- | :-------------- |\n| ImageNet1K | 0.1%            | 0.5%           | 0.3%            |\n\nAs shown in the table, under different batch sizes, the pure noise outperforms random guessing. These results provide strong evidence for our claim that soft labels (used at the evaluation time of DD) don't contribute to data quality, but only improve the test accuracy.\n\n---\n\n## Revision Update\n\nWe have update the revision following all reviewers' suggestion (text in red), including\n- Table 2a: Missed results for random noise\n- Section 2: Trimmed version\n- Line 163-164: Discussion and acknowledgement of related work\n- Section 3.4: The theoretical analysis, same as above\n- Line 508-511: Discussion of multimodal dataset distillation\n- Appendix A: Discussion of different $ \\lambda $ values."}}, "id": "rJth1mEa5q", "forum": "zEJd3JXVxb", "replyto": "zEJd3JXVxb", "signatures": ["ICLR.cc/2026/Conference/Submission14264/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14264/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission14264/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763511435569, "cdate": 1763511435569, "tmdate": 1763511931422, "mdate": 1763511931422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}