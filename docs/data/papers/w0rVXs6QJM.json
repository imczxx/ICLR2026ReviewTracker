{"id": "w0rVXs6QJM", "number": 25372, "cdate": 1758367274263, "mdate": 1759896723156, "content": {"title": "EucliFold: Probing 3D Euclidean Prior in VLMs via Cognitively-Stratified Folding Tasks", "abstract": "Humans leverage robust 3D spatial priors to align perception with the physical world, enabling flexible and intelligent behavior. While Vision-Language Models (VLMs) exhibit impressive zero-shot performance, it remains unclear whether they possess genuine spatial reasoning capabilities, as standard evaluations are confounded by dataset bias and spurious correlations. To address this, we introduce **EucliFold**, a synthetic visual question-answering benchmark focused on cube net folding in Euclidean space—a domain that enables precise analysis while requiring genuine spatial understanding. We propose a **cognitively-stratified evaluation framework** that decomposes spatial reasoning into three hierarchical levels: **Perception** (grounding sensory input to spatial representations), **Operation** (manipulating representations according to instructions), and **Imagination** (autonomous spatial problem-solving under geometric constraints). This decomposition isolates genuine spatial reasoning from superficial pattern matching. To mitigate evaluation biases, we employ **Winograd-style accuracy** using minimal-pair contrastive samples. Our evaluation reveals that state-of-the-art VLMs demonstrate reasonable perceptual capabilities but fail significantly at operational and imagination-level spatial reasoning, suggesting reliance on statistical patterns rather than genuine geometric understanding. Ablation studies confirm the effectiveness of our cognitively-stratified decomposition and bias-resistant evaluation methodology.  EucliFold provides a rigorous testbed for probing emergent spatial priors in future models and demonstrates how systematic cognitive decomposition can reveal nuanced capability gaps in VLMs.", "tldr": "", "keywords": ["vision language model", "synthetic dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b834136df5c9f61b6e5976859831dd3fcf904e9.pdf", "supplementary_material": "/attachment/353df013a9588e5616023b26dcc22016dddd0a9c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a new benchmark called Euclifold to test Vision Language Models (VLMs) spatial understanding ability. The authors consider the tasks of both interpreting and manipulating synthetically rendered 3D cubes with colored arrows rendered on each face to generate diverse stimuli and associated queries that test physical reasoning. \n\nThree different tasks of increasing complexity are introduced. The first task, Perception involves making predictions about the spatial structure of the cube, i.e. directions and colors of the arrows printed on each face of the cube. The second task, Operation involves understanding what the cube will look like when acted upon (i.e rotated by a certain angle). And finally, the third task, Imagination tests whether models can understand whether an unfolded cube (i.e net) can be folded back into it’s original configuration, or whether two unfolded cubes can be folded into the same cube.\n\nThe conclusion is that existing models are fine for perception, but fail on operation and imagination."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It’s an open question whether VLMs truly have 3D spatial understanding capabilities. There have been some benchmarks that test for this on natural images such as [1], but the full extent of the limitations is not understood yet. This benchmark is a good addition to the existing tests.\n2. Introducing a hierarchy of tasks of increasing complexity is nice because it helps us make judgements about where exactly do models struggle (i.e perception or reasoning). \n3. I appreciate that the authors tested scaling behavior. It’s important to test a wide range of model architectures before making any confident conclusions. It was interesting to see that for the folding task, we need to go to 80B to get above chance results, and even that is not really solving the problem. I agree with the authors that this challenges the assumption that scaling data and parameters alone will bridge the human-AI gap in spatial reasoning, especially given that the stimuli are simple and tasks are some of the easier 3D tasks in the range of 3D spatial reasoning capabilities that humans have."}, "weaknesses": {"value": "1. I think the benchmark mainly functions as an adversarial test. It’s clear that models can’t solve this task, but does the benchmark help generate results which suggest a way forward?  The paper would benefit from a discussion of possible solutions. Otherwise, I’m not sure what’s to be gained from simply knowing that “model’s can’t do X”\n2. The stimuli are also purely synthetic, and I’m not sure what would be the real world analog of this task. I get that it’s something that humans can do which models can’t and it’s worth understanding why, but what really is the practical relevance? Why should we care about cube folding? I think some arguments connecting these abstract reasoning tests to real-world spatial cognition, robotics, or embodied perception, or maybe even including some real world stimuli might help.\n3. The paper evaluates only a few VLMs. It would be a good idea to consider evaluating a wider range of models to know for sure that all models struggle on these tasks.\n4. There’s some prior work ( Unfolding Spatial Cognition [2] ) that also explored the cube folding task. It would strengthen the paper to clarify how Euclifold differs or improves on those benchmarks.\n\nWriting Improvements:\n\nL440: “achieving only X% and Y%” seems to be a typo.\n\nThe term “net” is abruptly introduced. It might be a good idea to define it clearly when first mentioned by adding a short visual or textual explanation of what a cube net is. It wasn’t obvious to me what cube nets are when I first read the paper. \n\nSection 4.1.2 is titled “Imagination Task” but doesn't seem to have results for matching? \n\nReferences \n[1] Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas\n[2] Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations"}, "questions": {"value": "1. What mechanisms might help VLMs move beyond pattern-matching toward true spatial reasoning? Does the benchmark help provide any insights on that? \n2. Could this framework generalize to other shapes or real 3D object data?\n3. How does Euclifold substantively differ from the Unfolding Spatial Cognition paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tff2RNHaeu", "forum": "w0rVXs6QJM", "replyto": "w0rVXs6QJM", "signatures": ["ICLR.cc/2026/Conference/Submission25372/Reviewer_tg8N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25372/Reviewer_tg8N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940131832, "cdate": 1761940131832, "tmdate": 1762943417597, "mdate": 1762943417597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EucliFold, a synthetic benchmark for evaluating spatial reasoning in Vision-Language Models (VLMs) through cube-net folding tasks. The authors claim to decompose spatial reasoning into three “cognitive” levels (Perception, Operation, Imagination) and propose a “Winograd-style” evaluation metric to mitigate response bias. Experiments show that current VLMs perform well at perception-level tasks but fail at higher reasoning levels."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The dataset is well-structured and technically sound in its generation pipeline.\n\n2. The introduction of Winograd-Style Accuracy is the most elegant part of the paper. By comparing model behavior across minimal-pair samples, it effectively neutralizes response bias and probes genuine understanding rather than superficial correlation. \n\n3. The paper shows commendable experimental rigor through its ablations on prompt style, negative-sample design, and chain-of-thought prompting, which effectively expose bias and reasoning shortcuts in current VLMs."}, "weaknesses": {"value": "**1. Lack of conceptual novelty :**  \nThe entire setup—cube folding, synthetic evaluation of 3D reasoning, multi-level breakdown of spatial ability—has been explored extensively in prior benchmarks such as [1][2][3][4][5]. The paper’s “cognitive stratification” is essentially a re-labeling of well-known task complexity tiers (Mental rotation→Operation; Mental simulation → Imagination) without offering new theoretical insight or analysis into why current models fail.\n\n**2. Superficial empirical contribution :**  \n The experimental findings (VLMs are good at perception, bad at reasoning) replicate established results rather than extend them. There is no mechanistic or representational analysis explaining how or why geometric priors fail to emerge. The work therefore lacks diagnostic depth.\n\n**3. Limited scope of evaluation:**  \n Restricting all tasks to cube folding confines the conclusions to a very narrow spatial domain. The benchmark tests geometric pattern recognition more than true 3D spatial generalization.\n \n[1] *11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis*  \n[2] *SpatialViz-Bench: An MLLM Benchmark for Spatial Visualization*  \n[3] *DOES SPATIAL COGNITION EMERGE IN FRONTIER MODELS?*  \n[4] *Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models*  \n[5] *Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics*"}, "questions": {"value": "**1. Overstated framing :** \nThe paper heavily relies on cognitive-science terminology (e.g., “autonomous imagination”) without actual modeling or measurement of cognitive processes. This creates the impression of interdisciplinary depth without substantive connection to cognitive theory.\n\n**2. Missing key quantitative details :** \n Several sections (e.g., final accuracies in Table/Figure 1) leave placeholders (“X%”, “Y%”), making it difficult to judge significance. Statistical testing and ablation rigor are below the expected standard for ICLR.\n\n**3. No fundamental insight or methodological innovation:** \n Beyond dataset generation and metric adaptation, the paper does not propose any new algorithmic or representational approach to probing Euclidean priors. It therefore reads more as an engineering replication than a scientific advancement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZgDuhFchYp", "forum": "w0rVXs6QJM", "replyto": "w0rVXs6QJM", "signatures": ["ICLR.cc/2026/Conference/Submission25372/Reviewer_7MGa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25372/Reviewer_7MGa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990853039, "cdate": 1761990853039, "tmdate": 1762943417262, "mdate": 1762943417262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EucliFold, a benchmark designed to probe 3D Euclidean spatial reasoning in Vision-Language Models (VLMs). It constructs synthetic cube-folding tasks and proposes a cognitively-stratified evaluation decomposing spatial reasoning into three hierarchical levels:\n1. Perception – grounding visual inputs into spatial representations,\n2. Operation – manipulating these representations per instructions,\n3. Imagination – autonomous problem solving under spatial constraints.\n\nUsing Winograd-style minimal-pair contrastive evaluation, the benchmark aims to reduce bias and disentangle genuine geometric reasoning from statistical pattern matching. Experiments across major VLMs (GPT-4o, Gemini, Claude, Qwen, InternVL) show strong perception-level accuracy but near-chance performance on operation and imagination levels, suggesting current VLMs lack robust internal 3D spatial priors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Unique Task Design: 3D Euclidean Folding under Controlled Symmetry\n- The use of cube-net folding in Euclidean space is mathematically precise, cognitively interpretable, and generalizable.\n- The dataset generation pipeline (24 rotational groups × 11 nets × mutation filtering) ensures exhaustive coverage of geometric configurations while minimizing bias — a notable methodological innovation.\n\nBias-Resistant Evaluation Methodology\n- The adaptation of Winograd-style contrastive accuracy to visual–spatial reasoning is new and impactful.\n- It isolates genuine geometric understanding from spurious response tendencies or dataset artifacts, addressing a major weakness in prior spatial-reasoning evaluations.\n\nHigh Relevance and Clarity\n- Addresses a timely and important question: Do VLMs possess emergent 3D Euclidean priors?\n- Writing and figures are clear, well-structured, and easy to reproduce, strengthening its potential as a community benchmark."}, "weaknesses": {"value": "Limited Model-Level Diagnosis\n- The paper identifies deficits but doesn’t probe why models fail\n\nLack of Cross-Domain Validation\n- It remains unclear whether EucliFold performance correlates with or transfers to other spatial benchmarks (LEGO-Puzzles, 3DSRBench)"}, "questions": {"value": "1. Have you tested whether models fine-tuned on EucliFold exhibit better generalization to real-world or naturalistic 3D reasoning tasks?\n\n2. Did you analyze intermediate visual or multimodal representations (e.g., via attention maps or feature embeddings) to understand where VLMs fail — at perception, transformation, or integration stages?\n\n3. Do you envision EucliFold as a diagnostic benchmark only, or also as a training dataset for spatially grounded VLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "U0ILl8t2px", "forum": "w0rVXs6QJM", "replyto": "w0rVXs6QJM", "signatures": ["ICLR.cc/2026/Conference/Submission25372/Reviewer_U2rQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25372/Reviewer_U2rQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994152622, "cdate": 1761994152622, "tmdate": 1762943417063, "mdate": 1762943417063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EucliFold, a new visual QA benchmark designed to probe the 3D spatial reasoning capabilities of VLMs. EucliFold is built on synthetic cube net folding tasks. \nThe paper's primary contributions are:\n - A \"cognitively-stratified\" evaluation framework that decomposes spatial reasoning into three hierarchical levels: Perception, Operation, and Imagination\n - A new synthetic dataset for these tasks, generated with a controlled pipeline to create 2D nets and 3D cube views.\n - A \"Winograd-style accuracy\" metric that uses minimal-pair contrastive samples to mitigate VLM response biases and isolate genuine reasoning from statistical pattern matching.\n\nUsing this framework, the authors evaluate a suite of VLMs. The key finding is that while models show reasonable performance on Perception tasks, their abilities degrade significantly on Operation tasks and fail almost completely on Imagination tasks. This suggests that current VLMs rely on superficial 2D pattern matching rather than possessing robust, generalizable 3D geometric priors."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Cognitive Framework: The cognitively-stratified decomposition into Perception, Operation, and Imagination is a major strength. This framework is well-motivated by cognitive science  and allows for a much more nuanced analysis than a single \"spatial reasoning\" score. It helps pinpoint where and why models are failing, moving from \"can they see?\" to \"can they manipulate?\" to \"can they solve?\"\n\n2. Evaluation: The authors show a deep awareness of the common pitfalls in VLM evaluation. The use of minimal-pair contrastive samples and the \"Winograd-Style Accuracy\" metric  is an excellent choice. This bias-resistant approach helps ensure that the results reflect genuine reasoning gaps rather than just dataset artifacts or model response biases , which is a common weakness in other benchmarks. The ablation studies (e.g., on \"easy negative samples\" ) further validate their methodology."}, "weaknesses": {"value": "1. Insufficient Comparison to Related Work: The paper’s novelty is limited as it fails to adequately differentiate itself from or compare against several existing spatial reasoning benchmarks (e.g., SPACE, OmniSpatial, SpatialViz-Bench, STARE). A direct comparison is needed to justify this benchmark's unique contribution.\n\n2. Missing Evaluation of Latest SOTA Models: The evaluation is incomplete. To make a strong claim about the limitations of current VLMs, the paper must include results from the latest SOTA models, such as GPT-5, Gemini 2.5 Pro, the Claude 4 series, and InternVL 3.5.\n\n3. Lack of Detailed Results Tables: The paper relies almost exclusively on figures to present results. This makes it difficult to ascertain precise numerical scores. The paper would be much stronger if it included dedicated results tables, especially for the Operation and Imagination tasks, rather than only referring to Figure 1.\n\n4. Superficial Error Analysis: The analysis lacks depth. While the paper identifies that models fail at Operation and Imagination, it does not investigate why. A deeper qualitative analysis of the failure modes is missing.\n\n5. Mismatch between \"Euclidean\" Claims and Task: The \"EucliFold\" title is a misnomer. The task is limited to cubes, which are highly constrained, discrete objects, and may not adequately probe an understanding of general 3D Euclidean space. Furthermore, the main paper omits details on how prompts were transformed into \"Euclidean terms\" for the \"Euc\" ablation study, making this claim hard to verify."}, "questions": {"value": "1. Following on from Weakness 2, what is the performance of the latest SOTA models (e.g., Gemini 2.5 Pro, Claude 4-series, Qwen3-VL) on your benchmark?\n\n2. The data generation uses 50 distinct cubes, each with its 24 possible 3D representations. Since the dataset is synthetic, what is the reasoning for this design (low cube variety, high rotational variety) versus using a much larger set of unique cube patterns?\n\n3. Paper mentions \"near-perfect human performance\" and details the human study in Appendix E. What were the precise, aggregated human accuracy scores for each of the three cognitive levels?\n\n4. The scaling plots (e.g., Figure 4) mix different model families (InternVL, Qwen) on the same x-axis (\"Model Size\"). This is not a valid scaling analysis, as it confounds scale with architecture. Can you provide scaling plots using a single, consistent model family?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2SB52GXUqn", "forum": "w0rVXs6QJM", "replyto": "w0rVXs6QJM", "signatures": ["ICLR.cc/2026/Conference/Submission25372/Reviewer_r6Nd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25372/Reviewer_r6Nd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038554029, "cdate": 1762038554029, "tmdate": 1762943416800, "mdate": 1762943416800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}