{"id": "5txkAwKd12", "number": 9871, "cdate": 1758145285415, "mdate": 1759897690506, "content": {"title": "Causal Representation Learning on Degraded Multi‑Sensor Streams", "abstract": "Many systems require real-time fusion of multi-sensor streams to produce causal estimates that drive online decisions. These systems must distill information across sensors while contending with missing and degraded measurements. As the number of sensors grows, both observable dropouts and latent degradation become more likely, making multi-sensor, multi-task processing brittle for conventional sequential models.  We propose two plug-in modules that attach to any unidirectional backbone (e.g., LSTM or causal Transformer): (i) Subchannel Hierarchical Input Embedding (SHIE) forms channel-level embeddings from fine-grained subchannels so that degraded values perturb only a local slice of the representation; (ii) Repetitive Cross-Modal Fusion Transformer (RCFT) performs iterative sensor-wise (cross-modal) attention at each time step, fusing concurrent measurements across sensors. Both modules support many-to-many estimation and are domain-agnostic with respect to loss functions and input/output shapes. We augment vanilla LSTM and Transformer backbones with SHIE and RCFT and evaluate on four multi-sensor datasets: electric grid state estimation, physical activity monitoring, room occupancy prediction, and cognitive load estimation. Across datasets, the augmented models outperform their baselines and remain accurate as missing-data rates rise far beyond those seen in training. Ablations isolate the contribution of each module, and the combined approach improves robustness without relying on separate imputation.", "tldr": "", "keywords": ["Causal", "Multimodal", "Robust Time Series Estimation", "Sensor"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7761984375a766cb16dfaebe58add4e04e845371.pdf", "supplementary_material": "/attachment/46fe0773e6b73b037bd049d560edc3ec3472c95b.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces a causal multimodal fusion framework for time-series inference under missing or corrupted sensor data. It proposes two key modules. 1- SHIE, which structures each sensor’s subchannels to localize the effect of missingness, and 2- RCFT, which alternates causal temporal modeling and cross-modal attention to iteratively refine representations across modalities. These components form a end to end model capable of real time, streaming inference while keeping causal constraints. The approach is tested on diverse datasets including physical activity, smart grid estimation, and cognitive monitoring showing improved robustness and accuracy compared to prior multimodal baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces two simple and effective plug and play components that are easily integrated into a wide range of existing causal or sequential models. This modular design enhances flexibility and show the potential for applying different BBs.\n\n- The paper is easy to follow with clearly making notation, and motivation of each component. The logical flow from problem formulation to architectural design makes the ideas easier to access and justify.\n\n- The paper provides the code with an easy-to-follow instruction that makes the following up process much easier. I liked that.\nThis level of transparency is valuable for the research community.\n\n- They provide relatively complete ablation study with covering different ratios of data corruption and missingness. This thorough analysis helps validate the robustness and general effectiveness of their model.\n\n-  The appendix is useful and provides additional structural details along with the additional results, data format, and noise insertion setting. These supplementary materials add clarity and reproducibility to the overall work which is valuable."}, "weaknesses": {"value": "- The causal modeling is motivated for real-time or streaming applications, and comparing under this constraint is reasonable.\nBut in some settings we may have access to the full observation sequence (e.g., offline analysis or retrospective inference). In that case, how to use future context to refine your latents? kinda smoothing or bidirectional inference variant would be feasible and likely beneficial in this case.\nI suggest either including a full-sequence inference experiment or, at minimum, clarifying how the current causal architecture could be extended to support non-causal or bidirectional inference. This can make your work applicable to broader apps.\n\n- The paper (in their intro) argues that two-stage smoothing or imputation pipelines can not be trained end-to-end and propagate errors to the predictor. This is true for classical modular pipelines, but recent differentiable smoothing frameworks (e.g., variational approaches, diffusion-based imputers, or RNN-based bidirectional models) allow joint, end-to-end optimization. Please clarifying this in draft.\n\n- The model assigns separate SHIE and RCFT modules to each modality, which preserves modality-specific representations but can make scalability issues. The computational and parameter complexity grow with the number of modalities, making the approach potentially inefficient for systems with many sensors or high modality diversity. For this concern, the paper does not include an explicit complexity analysis.\n\n- The reported results lack error bounds or standard deviations. Although in appendix (line 1232) running trials with different random seeds are stated, only single-point error and accuracy values are presented in the tables. Without reporting variability across runs, it is difficult to assess the robustness of the method to initialization or its statistical reliability.\n\n- Although I found detailed hyperparameter values in the released code, the paper does not clearly describe how these values were determined (e.g., via grid search, random search, or heuristic tuning). Providing this information would improve reproducibility and ensure a more balanced comparison across methods.\n\n- Both UniTime [1] and PowerPM [2] share conceptual overlap with this work and can strengthen the benchmark evaluation (currently the benchamrks are four: TSMixer, iTrans, CroSSL and MBT). \nUniTime addresses cross-domain time-series modeling with heterogeneous inputs, similar to the this paper goal of multimodal inference across varying feature dimensions. PowerPM targets large-scale electricity time-series with hierarchical and temporal dependencies, again similar with this paper structure, causal modeling of smart-grid data. Including these two models as additional benchmarks (or at least arguing how they are related to this work and how they are expected to perform in your exps) would provide a more diverse and representative comparison.\n\n- All modalities used in the experiments seems to continuous valued signals, such as physical or physiological sensor readings. It remains unclear how the proposed framework would handle categorical or discrete modalities. If such an extension is possible, it would be useful to explain how categorical inputs are embedded and fused. Otherwise, this limitation should be explicitly acknowledged in the paper.\n\n[1] - Liu, Xu, et al. \"Unitime: A language-empowered unified model for cross-domain time series forecasting.\" Proceedings of the ACM Web Conference 2024. 2024.\n\n[2] - Tu, Shihao, et al. \"Powerpm: Foundation model for power systems.\" Advances in Neural Information Processing Systems 37 (2024): 115233-115260."}, "questions": {"value": "Thank you for your work and all the efforts. I have some questions about the paper:\n\n- How can the proposed model be extended to full (non-causal) inference, where future observations are also used to refine latents or predictions? If such an extension is feasible, please consider including an experiment. If no, clearly state in the draft what structural and objective modifications would be required to achieve this, and mention a reasonable training objective or procedure for that setting.\n \n- Originally you make notation that modalities are shown with $n$ (line 113). Later you switch to $m$ to note modality (e.g. line 697). Could you make the notation consistent? Although the modality branch is representing each modality that finally is gonna build the output, but you named it 'task': $m \\in {1,...,M}$ before.\n\n- In line 714, why there is _m index for MHA? MHA is a single softmax operator over your Q,V,K without weight parameters, shared for all of your queries, right? Then, this notation can cause confusion that you have M separate MHA blocks rather than a shared one.\n\n- Your model assigns independent SHIE and RCFT modules to each modality, which preserves modality-specific semantics but also increases parameter count and computational cost linearly (or worse) with the number of modalities. increase of accuracy is intuitive in this case, but it raises questions about scalability. Could you include a complexity analysis, either empirical (e.g., runtime scaling with modality count) or theoretical (e.g., asymptotic time and memory complexity in $\\mathcal{O}(.)$ format )? it clarifies how the model’s cost grows with modality size.\n\n- In your results, the S-R (T) variant substantially outperforms the TSMixer baseline in terms of error (e.g., your error of 1.191 is up to eight times lower than TSMixer in the Physical Activity Monitoring task). However, the corresponding accuracy improvement seems marginal (onnly a few percentage points). Could you provide an explanation for this?\n\n- Why there is not standard deviation or error bound reported in tables? Could you add it with $\\pm$ sign?\n\n- Could you add a detailed explanation of the hyper parameter optimization? How did you come up with the final set of HPs? Same for the baselines, did you use the original HPs listed in their original drafts? Or did you follow a HPs optimization recipe for the baselines as well?\n\n- Could you also explain how your structure can be extended to both categorical-continuous modalities inputs? It helps the reader to understand the scope of your work better."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jHuE63CLcw", "forum": "5txkAwKd12", "replyto": "5txkAwKd12", "signatures": ["ICLR.cc/2026/Conference/Submission9871/Reviewer_JMYZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9871/Reviewer_JMYZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559748855, "cdate": 1761559748855, "tmdate": 1762921341803, "mdate": 1762921341803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets causal, real-time fusion of multi-sensor streams where inputs can be missing or degraded. The authors propose two plug-in modules that can be attached to unidirectional backbones (LSTM or causal Transformer): (i) Subchannel Hierarchical Input Embedding (SHIE): embeds each sub-sensor (e.g., accelerometer axes) independently and then aggregates at the channel level, so that missing/corrupted values perturb only a local slice of the representation. (ii) Repetitive Cross-Modal Fusion Transformer (RCFT): maintains one branch per modality and alternates a causal temporal update with modality-wise attention at every layer to iteratively exchange information across streams while respecting causality. They evaluate on four tasks—grid voltage estimation (84- and 4583-node systems), physical activity monitoring (PAMAP2, wrist-only), room occupancy estimation, and cognitive load classification—injecting degradations under an MCAR policy and sweeping missingness up to 95%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper is explicitly about causal, streaming inference with missing and degraded inputs, which is an important and practical setting often sidestepped by imputation pipelines that use future context.\n\n+ SHIE and RCFT are architecture-agnostic modules for LSTMs or causal Transformers; the design keeps per-modality identity, enabling iterative fusion without early entanglement.\n\n+ SHIE’s locality preservation and RCFT’s message-passing style, with causal masking and per-layer cross-modal attention, are motivated and contrasted with early/late fusion and window forecasters."}, "weaknesses": {"value": "- Training and eval use MCAR, with temporally clustered bursts, but many deployments exhibit MAR/MNAR patterns tied to environment or device dynamics. Results may overstate robustness when missingness is not feature- or context-dependent;\n\n- Although tasks span domains, they are primarily tabular sensor streams. Vision/audio or high-bandwidth modalities are not tested, limiting claims of domain-agnosticism in rich-media settings;"}, "questions": {"value": "1. Have you tested RCFT/SHIE under MAR (feature- or state-dependent missingness) or MNAR? If not, can you share preliminary results or hypothesize failure modes vs. MCAR?\n\n2. Beyond architecture locality, have you tried auxiliary objectives (e.g., self-supervised outlier scoring per subchannel) to explicitly teach the model to down-weight corrupted readings during RCFT fusion?\n\n3. How sensitive is performance to how often cross-modal attention is performed (e.g., every L layers vs. every layer)? Any diminishing returns from fusion density at fixed depth?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R9xNNrtsqS", "forum": "5txkAwKd12", "replyto": "5txkAwKd12", "signatures": ["ICLR.cc/2026/Conference/Submission9871/Reviewer_CjVi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9871/Reviewer_CjVi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672831373, "cdate": 1761672831373, "tmdate": 1762921341448, "mdate": 1762921341448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two \"plug-in\" modules, SHIE and RCFT, intended to improve the robustness of causal sequence models (like LSTMs or Transformers) for multi-sensor time-series tasks. The stated goal is to handle degraded and missing data in real-time.\n\n   1. SHIE (Subchannel Hierarchical Input Embedding) processes sensor inputs by first embedding fine-grained \"subchannels\" (e.g., X/Y/Z axes of an accelerometer) before aggregating them, with the goal of isolating local sensor noise.\n\n   2. RCFT (Repetitive Cross-Modal Fusion Transformer) maintains a separate branch of computation for each sensor modality and performs iterative, layer-by-layer fusion using cross-attention at each time step.\n\nThe authors evaluate these modules by adding them to LSTM and Causal Transformer backbones and testing them on four multi-sensor datasets. The core of the evaluation is a robustness test where models trained with 5% missing data are evaluated on test sets with up to 95% missing data. The paper claims this approach significantly outperforms both the vanilla backbones and recent SOTA models like iTransformer, TSMixer, and CroSSL."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The ideas behind isolating local noise (SHIE) and iterative refinement (RCFT) over one-shot fusion are correct and well-motivated \n2. The paper does successfully demonstrate that a vanilla Causal Transformer or LSTM is insufficient for this task and that the RCFT module provides a significant performance boost (Table 2).\n3. MR-AUC Metric: The proposed MR-AUC metric is a sensible and useful way to summarize model robustness across a range of degradation levels"}, "weaknesses": {"value": "This paper suffers from two major, disqualifying flaws: (1) a misleading title and framing that misrepresents the paper's contribution, and (2) a fundamentally unfair experimental comparison that invalidates its primary claims of state-of-the-art performance.\n\n1.  **Misleading Title and Unfair Experimental Framing:**\n    * **The title is a misnomer.** The paper is titled \"**Causal Representation Learning** on Degraded Multi-Sensor Streams.\" This is a significant misrepresentation. The field of \"Causal Representation Learning\" (CRL) is a specific, well-defined area of research focused on discovering latent representations that capture the underlying causal graph or structural causal model (SCM) of a system. This field, (e.g., Schölkopf, 2019; Arjovsky et al., 2019; Locatello et al, 2021), aims to learn features that are invariant to interventions or domain shifts, often for counterfactual reasoning. This paper does **not** engage with this field in any way. It builds a *causal temporal model* one that respects the arrow of time (i.e., no lookahead). This is a standard constraint for any real-time system and is **not** \"Causal Representation Learning.\" This misleading title seems intended to borrow prestige from a popular but unrelated field.\n    * **This misrepresentation extends to the SOTA comparison.** The paper's headline claim of outperforming iTransformer, TSMixer, and MBT is invalid. The authors' method is a **causal, streaming, per-step** (sequence-to-one) model. The baselines it compares against are **window-to-window**, non-recurrent forecasters. As the authors admit (Appendix A.3), these models are not designed for this task. \"Adapting\" them by appending a linear layer (Section 5.2) hobbles them and guarantees their failure, especially as they have no recurrent state to fall back on when inputs are missing. The \"collapse\" of SOTA models (e.g., Fig. 7) does not demonstrate the superiority of SHIE/RCFT; it demonstrates that the wrong tool was used for the job. This is a classic \"apples-to-oranges\" comparison and is not valid.\n\n2.  **Incremental Novelty and Unclear Contributions:**\n    * **SHIE:** The idea of embedding sub-components (subchannels) before aggregation is a standard hierarchical processing technique. It is a sensible engineering choice (akin to patching) but is not a novel contribution.\n    * **RCFT:** Iterative, layer-by-layer fusion using cross-attention is a well-known pattern in multi-modal literature.\n    * **The Ablation (Table 2) is Damning:** The paper's *only* valid comparison is the internal ablation. This shows that RCFT does almost all the work. For the 84-node Transformer (A-MAE*), the vanilla model scores 2.345. Adding SHIE-only brings this to 0.405 (a good improvement). But adding RCFT-only brings it to 0.224. The final combined model (S-R) scores 0.209. This demonstrates that RCFT is the primary driver, and the contribution of SHIE is marginal at best."}, "questions": {"value": "1. Why is the paper titled *\"Causal Representation Learning\"* when it does not appear to engage with the main CRL literature (for example, learning invariant structural causal models) and instead focuses on a standard causal temporal model?\n\n2. How can the reported SOTA comparisons (for example, against iTransformer) be considered valid when the baseline models are non-causal, window-based architectures that seem to have been adapted in ways that inherently disadvantage them in a streaming setting with high missingness?\n\n3. Why is the most critical ablation missing, namely, the performance of a simple backbone combined with RCFT only? From Table 2, this component appears to account for nearly all the reported gains, which would make SHIE a minor contribution.\n\n4. Following the previous question, why were more appropriate baselines — such as other robust, streaming, stateful models (for example, state-space models or recurrent architectures), not included for comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sH1NogCx21", "forum": "5txkAwKd12", "replyto": "5txkAwKd12", "signatures": ["ICLR.cc/2026/Conference/Submission9871/Reviewer_pN1V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9871/Reviewer_pN1V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993310875, "cdate": 1761993310875, "tmdate": 1762921341096, "mdate": 1762921341096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes two plug-in modules, Subchannel Hierarchical Input Embedding (SHIE) and Repetitive Cross-Modal Fusion Transformer (RCFT), to improve robustness in causal multi-sensor time-series modeling under missing or degraded inputs. SHIE builds pre-subchannel embeddings that localize noise effects and RCFT alternates causal temporal updates and modality-wise attention to enable iterative cross-sensor fusion. \n\n\nBoth modules can attach to any causal backbone (LSTM or causal Transformer). The authors evaluate on four datasets (power grid state estimation, PAMAP2 activity, room occupancy, and cognitive load) and compare performance against TSMixer, iTransformer, CroSSL, and MBT and introduce a new robustness metric MR-AUC (Missingness Robustness AUC)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. Clear motivation and framing. The problem of real-time degraded multi-sensor fusion is practical and under-explored. \n\nS2. Methodologically sounds. The causal masking and modular plug-in design are coherent and reproducible. \n\nS3. Broad evaluation. Four datasets, multiple backbones, and ablation studies are all included. \n\nS4.Presentation quality. Writing, figures, and appendices are clear and detailed. Also included an anonymized OSF release for reproducibility."}, "weaknesses": {"value": "exactly what is missing, etc. \n\nW1.Limited novelty. RCFT is essentially a causal Transformer with modality-wise cross-attention, conceptually similar to iTransformer or standard multimodal Transformer. SHIE is a hierarchical embedding variant. Both are incremental architectural refinements rather than fundamentally new principles of causal modeling.  \n\nW2. Inconsistent empirical gains. Table 2 shows that SHIE and RCFT do not consistently improve performance. \n\n- On PAMAP2, all variants perform identically (Accuracy ≈ 0.86, AU-ROC ≈ 0.66) \n\n- On Cognitive Load, the plugins perform identically for LSTM. \n\n- On Room Occupancy, the prediction accuracy improves slightly (0.3% for LSTM and 0.5% for Transformer) \n\n- Only the grid estimation task shows strong numerical drops in MAE. \n\nTherefore, the claim that both modules “consistently” enhance robustness is overstated. In addition, the results are reported as medians over 30 runs but without variance or significance testing. Small deltas could easily fall within random variation \n\nW3. Weak justification of robustness setting. All experiments uses MCAR noise. Real-world degradation is typically MAR or MNAR correlated with context or sensor type. Thus, robustness to realistic failures remains unverified. \n\nW4. No efficiency or latency evaluation. The paper emphasizes “lightweight plug-ins” and “streaming operation” but no FLOPs, parameter counts, or inference-time comparisons. It’s unclear whether the benefits justify the added architectural complexity."}, "questions": {"value": "Q1: How sensitive is RCFT to the number of modalities and attention heads? \n\nQ2: When do SHIE and RCFT fail to help (e.g., PAMAP2)? \n\nQ3: What are the computational costs of adding both modules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z8UVLQAhL7", "forum": "5txkAwKd12", "replyto": "5txkAwKd12", "signatures": ["ICLR.cc/2026/Conference/Submission9871/Reviewer_yjjw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9871/Reviewer_yjjw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004349307, "cdate": 1762004349307, "tmdate": 1762921340631, "mdate": 1762921340631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes two plug-in modules, Subchannel Hierarchical Input Embedding (SHIE) and Repetitive Cross-Modal Fusion Transformer (RCFT), to improve robustness in causal multi-sensor time-series modeling under missing or degraded inputs. SHIE builds pre-subchannel embeddings that localize noise effects and RCFT alternates causal temporal updates and modality-wise attention to enable iterative cross-sensor fusion. \n\n\nBoth modules can attach to any causal backbone (LSTM or causal Transformer). The authors evaluate on four datasets (power grid state estimation, PAMAP2 activity, room occupancy, and cognitive load) and compare performance against TSMixer, iTransformer, CroSSL, and MBT and introduce a new robustness metric MR-AUC (Missingness Robustness AUC)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. Clear motivation and framing. The problem of real-time degraded multi-sensor fusion is practical and under-explored. \n\nS2. Methodologically sounds. The causal masking and modular plug-in design are coherent and reproducible. \n\nS3. Broad evaluation. Four datasets, multiple backbones, and ablation studies are all included. \n\nS4.Presentation quality. Writing, figures, and appendices are clear and detailed. Also included an anonymized OSF release for reproducibility."}, "weaknesses": {"value": "W1.Limited novelty. RCFT is essentially a causal Transformer with modality-wise cross-attention, conceptually similar to iTransformer or standard multimodal Transformer. SHIE is a hierarchical embedding variant. Both are incremental architectural refinements rather than fundamentally new principles of causal modeling.  \n\nW2. Inconsistent empirical gains. Table 2 shows that SHIE and RCFT do not consistently improve performance. \n\n- On PAMAP2, all variants perform identically (Accuracy ≈ 0.86, AU-ROC ≈ 0.66) \n\n- On Cognitive Load, the plugins perform identically for LSTM. \n\n- On Room Occupancy, the prediction accuracy improves slightly (0.3% for LSTM and 0.5% for Transformer) \n\n- Only the grid estimation task shows strong numerical drops in MAE. \n\nTherefore, the claim that both modules “consistently” enhance robustness is overstated. In addition, the results are reported as medians over 30 runs but without variance or significance testing. Small deltas could easily fall within random variation \n\nW3. Weak justification of robustness setting. All experiments uses MCAR noise. Real-world degradation is typically MAR or MNAR correlated with context or sensor type. Thus, robustness to realistic failures remains unverified. \n\nW4. No efficiency or latency evaluation. The paper emphasizes “lightweight plug-ins” and “streaming operation” but no FLOPs, parameter counts, or inference-time comparisons. It’s unclear whether the benefits justify the added architectural complexity."}, "questions": {"value": "Q1: How sensitive is RCFT to the number of modalities and attention heads? \n\nQ2: When do SHIE and RCFT fail to help (e.g., PAMAP2)? \n\nQ3: What are the computational costs of adding both modules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z8UVLQAhL7", "forum": "5txkAwKd12", "replyto": "5txkAwKd12", "signatures": ["ICLR.cc/2026/Conference/Submission9871/Reviewer_yjjw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9871/Reviewer_yjjw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004349307, "cdate": 1762004349307, "tmdate": 1763136568539, "mdate": 1763136568539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}