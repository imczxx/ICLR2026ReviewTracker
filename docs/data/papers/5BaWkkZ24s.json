{"id": "5BaWkkZ24s", "number": 17366, "cdate": 1758275111335, "mdate": 1759897179767, "content": {"title": "Supervised Graph Contrastive Learning for Gene Regulatory Networks", "abstract": "Graph Contrastive Learning (GCL) is a powerful self-supervised learning framework that performs data augmentation through graph perturbations, with growing applications in the analysis of biological networks such as Gene Regulatory Networks (GRNs). The artificial perturbations commonly used in GCL, such as node dropping, induce structural changes that can diverge from biological reality. This concern has contributed to a broader trend in graph representation learning toward augmentation-free methods, which view such structural changes as problematic and to be avoided. However, this trend overlooks the fundamental insight that structural changes from biologically meaningful perturbations are not a problem to be avoided but a rich source of information, thereby ignoring the valuable opportunity to leverage data from real biological experiments.\nMotivated by this insight, we propose SupGCL (Supervised Graph Contrastive Learning), a new GCL method for GRNs that directly incorporates biological perturbations from gene knockdown experiments as supervision. SupGCL is a probabilistic formulation that continuously generalizes conventional GCL, linking artificial augmentations with real perturbations measured in knockdown experiments and using the latter as explicit supervisory signals.\nTo assess effectiveness, we train GRN representations with SupGCL and evaluate their performance on downstream tasks. The evaluation includes both node-level tasks, such as gene function classification, and graph-level tasks on patient-specific GRNs, such as patient survival hazard prediction. Across 13 tasks built from GRN datasets derived from patients with three cancer types, SupGCL consistently outperforms state‑of‑the‑art baselines.", "tldr": "We propose SupGCL, a supervised graph contrastive learning framework that uses gene knockdown data to supervise the learning of graph structural changes, achieving state-of-the-art performance on 13 tasks across three cancer datasets.", "keywords": ["Representation Learning", "Graph Contrastive Learning", "Gene Regulatory Network"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb2f494a6621b951102d94a9c76f8bdc95948951.pdf", "supplementary_material": "/attachment/7a4e9f4e1d177f12e23c54baea439e2f323efb2f.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a new method called SupGCL for learning representations of gene regulatory networks (GRNs).\n\n- SupGCL directly incorporates real biological perturbation data (gene knockdown experiments) as supervision in the contrastive learning setup.\n\n- Instead of only using random augmentations, they use measured changes in network structure (after the perturbation) as positive pairs, aligning the representation learning more closely with biologically meaningful changes.\n\n- The paper evaluates SupGCL on multiple different tasks (both node-level and graph-level) derived from patient GRN data across multiple cancer types and shows that SupGCL consistently outperforms state-of-the‐art baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### 1. Technical Soundness\n\n- The paper presents a probabilistic generalization of traditional Graph Contrastive Learning (GCL), introducing biologically supervised augmentations via gene knockdown data. The authors mathematically link supervised and unsupervised contrastive objectives through the augmentation temperature parameter ($\\tau_a$) and establishes that standard GCL is a limiting case when $\\tau_a \\to \\inf$.\n\n- Algorithm 1 provides a clear training loop using importance sampling, softmax-based probability modeling, and AdamW optimization, improving computational tractability. \n\n- The method transforms artificial augmentations (random node/edge dropout) into biologically grounded perturbations (gene knockdowns) that serve as supervisory signals.\n\n- Loss functions are well-defined for both node- and augmentation-level contrastive objectives, with consistent normalization and temperature scaling across levels. The inclusion of a theoretical corollary (linking SupGCL to GRACE) adds credibility.\n\n### 2. Experimental Soundness\n\n- SupGCL is evaluated on multiple downstream tasks across multiple cancer types, covering node-level and graph-level settings. This demonstrates robust empirical coverage.\n\n- The authors compare SupGCL against multiple baselines, ranging from traditional GCL methods, augmentation-free approaches, to supervised GRN inference models.\n\n- The $\\tau_a$ ablation study validates the theoretical claim: as $\\tau_a$ increases (less supervision), performance converges to that of GRACE, supporting the model’s theoretical soundness.\n\n- Embedding visualization and clustering metrics (NMI, ARI) show that SupGCL yields clearer subtype separation than other baselines, confirming representational quality."}, "weaknesses": {"value": "### 1. Technical Limitations\n\n- The framework relies on sampling-based estimation of normalization constants and importance sampling of probabilities. While efficient, it introduces stochastic noise that might affect convergence stability. No convergence analysis or complexity bounds are provided.\n\n- Although knockdown data is used as biological supervision, the mapping from experimental perturbations (teacher GRNs) to graph-level augmentations is only heuristically justified, not theoretically proven to represent equivalent \"positive pairs.\"\n\n- The model assumes directed graphs with homogeneous node types and dense connectivity. However, GRNs are often sparse, hierarchical, and multi-modal. This could challenge generalizability beyond the datasets used.\n\n### 2. Experimental Limitations\n\n- Improvements are generally small (1–3%), and statistical significance is not tested across all metrics. The paper acknowledges that SupGCL \"did not achieve statistically significant superiority in every single task.\"\n\n- Cross-domain generalization is not good. Pre-training on one cancer type does not generalize to others — the model \"fails to improve performance on downstream tasks for other cancer types\"\n\n- Although the algorithm uses sampling for efficiency, the paper lacks runtime comparisons or scalability analysis relative to baseline GCL methods."}, "questions": {"value": "See the weakness section. In addition:\n\n- SupGCL’s dual-graph supervision (G–H pairs for each gene knockdown) may scale poorly to full-genome datasets with thousands of genes.\n\n- The approach requires availability of knockdown experiments; many genes lack this data, limiting applicability.\n\n- Importance sampling introduces stochasticity that may bias gradients during optimization.\n\n- Although biologically motivated, the paper doesn't rigorously formalize why knockdown graphs are \"valid augmentations\" in the contrastive sense.\n\n- The paper does not clarify whether directionality is preserved in the encoder's message passing, which could distort regulatory logic.\n\n- Evaluations are restricted to three cancer types and a single knockdown dataset. Results may not generalize to other tissues, diseases, or GRN reconstruction pipelines.\n\n- Although several baselines are compared, there’s no inclusion of modern biological foundation models or multi-omics graph frameworks (e.g., MUSE-GNN).\n\n- Variance in learned embeddings or predictive uncertainty (important for biological tasks) is not explored."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4cR0FZBQKi", "forum": "5BaWkkZ24s", "replyto": "5BaWkkZ24s", "signatures": ["ICLR.cc/2026/Conference/Submission17366/Reviewer_ceKv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17366/Reviewer_ceKv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760877764764, "cdate": 1760877764764, "tmdate": 1762927279302, "mdate": 1762927279302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a supervised graph contrastive learning framework named SupGCL for cancer drug response prediction based on gene regulatory networks (GRNs). The method attempts to leverage gene knockdown experimental data as supervision signals to guide the graph contrastive learning process, aiming to improve performance on downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors' integration of gene regulatory networks with drug response prediction represents a promising interdisciplinary research direction.\n2. The paper covers both graph-level and node-level tasks, demonstrating extensive experimentation.\n3. The idea of using real gene knockdown data as supervision signals is forward-looking and innovative."}, "weaknesses": {"value": "1. The paper claims to have \"theoretically proven that existing GCL methods are special cases of the proposed SupGCL.\" This statement is logically untenable. SupGCL is a highly domain-specific method whose core relies on GRNs as edge features and biological perturbation data as supervision signals. In contrast, general graph contrastive learning methods (e.g., GraphCL, GRACE) are designed to be universal and do not depend on such specific prior knowledge or external experimental data.\n\n2. The paper lacks comparisons with some recent state-of-the-art unsupervised baselines. Furthermore, is it fair to compare a method using supervision signals against unsupervised GCL? Supervised contrastive learning [1] itself is not a novel concept. The authors are advised to discuss the similarities and differences between their approach and existing supervised contrastive learning frameworks.\n\n3. If the authors believe that artificial data augmentations (e.g., random edge dropping) disrupt the biological realism of GRNs, while knockdown experiments represent genuine biological perturbations, then forcing the model to bring together a \"distorted view\" and a \"true perturbed view\" is logically contradictory. This is equivalent to requiring the model to treat a \"noisy, unrealistic\" state as \"similar\" to a \"real, meaningful\" state. Such a design may lead the model to learn merely a \"denoising\" capability rather than authentic perturbation-response patterns, potentially even corrupting the learned representations. The authors need to provide further clarification on this point.\n\n4. The current contrastive framework of SupGCL only exploits the \"correlation\" in knockdown data, learning the association between \"knocking down Gene X\" and \"gene expression change Y\", and fails to fully leverage the potential of causal data, remaining at the level of \"correlational learning.\"\n\n5. Although the paper analyzes robustness to noise in estimated GRNs, it completely overlooks a discussion on the quality of the knockdown data itself. The performance of SupGCL heavily depends on the knockdown efficiency, off-target effects, and reproducibility of the knockdown experiments. If the knockdown data is of poor quality (e.g., low efficiency, severe off-target effects), the supervision signal itself becomes biased, a problem more fundamental than GRN estimation noise. The authors are recommended to include a discussion on the quality of knockdown data.\n\n[1] Khosla P, Teterwak P, Wang C, et al. Supervised contrastive learning[J]. Advances in neural information processing systems, 2020, 33: 18661-18673."}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gas4bAeDZM", "forum": "5BaWkkZ24s", "replyto": "5BaWkkZ24s", "signatures": ["ICLR.cc/2026/Conference/Submission17366/Reviewer_29KV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17366/Reviewer_29KV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761465240144, "cdate": 1761465240144, "tmdate": 1762927278884, "mdate": 1762927278884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SupGCL, a supervised graph contrastive learning framework tailored for Gene Regulatory Networks (GRNs). Unlike traditional augmentation-based or augmentation-free GCL methods, SupGCL leverages real biological perturbations—specifically, gene knockdown experiments—as supervisory signals to guide both node-level and augmentation-level contrastive learning. The authors present a unified probabilistic framework that generalizes traditional GCL as a special case and validate their approach through extensive experiments across 13 downstream tasks spanning three cancer types. Across all evaluations, SupGCL consistently outperforms existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel Supervision Source: The use of gene knockdown data as a real-world supervisory signal for contrastive learning is both innovative and biologically meaningful.\n\nTheoretical Generalization: The framework extends GCL into a probabilistic supervised model, showing that prior unsupervised GCL methods are special cases. Theoretical proofs and ablation studies reinforce this claim.\n\nExperimental Rigor: The study includes comprehensive experiments across multiple tasks, cancer types, and baselines, providing strong empirical support.\n\nBiological Relevance: By grounding augmentations in actual biological perturbations, SupGCL maintains biological fidelity—crucial for GRN interpretability and biomedical relevance."}, "weaknesses": {"value": "Limited Generalizability: As noted by the authors, SupGCL trained on one cancer type does not transfer effectively to others, limiting its broader biomedical applicability.\n\nDependence on External Data: The framework’s reliance on knockdown data from LINCS constrains its use to settings where such experimental data exist, reducing scalability for rare or novel conditions.\n\nModest Gains in Some Tasks: Although performance improvements are consistent, the magnitude of gains is modest in certain node-level tasks, prompting consideration of the tradeoff between added supervision and practical benefit."}, "questions": {"value": "Cross-Cancer Generalization: Incorporating domain adaptation techniques or pan-cancer pretraining could enhance the model’s transferability across different cancer types.\n\nPerturbation-Free Extension: Developing a fallback mechanism—such as simulated perturbations informed by biological pathways—would make SupGCL applicable when knockdown data are unavailable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1R1e2D8A5v", "forum": "5BaWkkZ24s", "replyto": "5BaWkkZ24s", "signatures": ["ICLR.cc/2026/Conference/Submission17366/Reviewer_tEnf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17366/Reviewer_tEnf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761612882413, "cdate": 1761612882413, "tmdate": 1762927278414, "mdate": 1762927278414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to improve upon standard graph contrastive learning (GCL) paradigm particularly for gene regulatory networks (GRNs).\nThe main motivation for this paper is that, standard GCL which utilize artificial random augmentations (node dropping, edge dropping, feature perturbation) can produce structures that diverge from \"biological reality\" and the representations learned lack any kind of biological supervision /signal during pre-training that might be beneficial to be encoded for down stream tasks.\n\nBased on this insight, the authors propose supervised GCL (SupGCL), particularly for GRNs where they utilize real experimental gene knockdown data as supervision (via \"teacher GRNs\") during the contrastive learning phase. SupGCL minimizes the KL divergence between the similarity distributions of real and simulated gene knockdowns, at both node and graph levels, so the authors claim that the learned graph embeddings respect true biological perturbation structure.\n\nThe authors argue that with their method is superior to augmentation-free GCL approaches as they completely ignore structural augmentations. The authors also show that when the temperature param tends to infinity in SupGCL we recover back the standard node level GCL contrastive objective, allowing one to control the amount of \"biological\" supervision.\n\nEmpirically, the authors utilize LINCS and TCGA data, which each contain ~1K real gene knockdown experiments for pretraining supervision GCL.\nThey consider both node level and graph level downstream classification tasks. Both these task follow the standard protocol of finetuning with MLP heads before 10-fold CV evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1) Clear motivation for the approach of using teacher GRNs via real biological experimental data. The loss formulation is sound, and paper provides good intuition for it. The formulation is novel and an extension to GCL when we have access to such supervision data.\n\n2) Paper is easy to follow and detailed.\n\n3) Empirically shows SupGCL with low temperatures values surpasses GRACE (standard node level GCL) and with high temp values approaches its performance. This experiment / ablation partially supports the authors claims."}, "weaknesses": {"value": "1) While the main motivation is valid, the loss formulation and use of supervision goes against the goal of self-supervised pre-training which standard GCL follows. The proposed formulation assumes availability of real world biological data (e.g., gene knockdown data). The authors don't discuss the ease / cost of obtaining them. The main benefits of standard GCL which is large scale self-supervision is lost with SupGCL.\n\n\n2) Experiments suggest the improvement SupGCL obtains is very marginal across the different node and graph level tasks. The difference particularly with the best baseline is within 1 std deviation for a majority of the tasks. This does point that the supGCL loss formulation is not very optimal and weakens the authors claim that SupGCL consistently outperforms SoTA baselines.\n\n3) The authors do not discuss the results in Table 2 and 3 in detail. This is the main experiment to back the claims the authors have in the paper. What is the intuition for w/o - pretrain for e.g., is primarily (5/9) being the best baseline. Do the chosen downstream tasks not require expensive GCL pre-training ?.\n\n4) There is a lot of existing literature on learning structural augmentations in a self-supervised fashion that have shown superior performance compared to standard GCL. This work does not consider them in experiments nor discuss them. Learned augmentations have the benefit of pre training without costly supervision which is the case for this work. [e.g., 1,2,3]\n\n5) One of the goals of standard GCL formulation is also that it can help in transfer learning. And many of the baseline methods evaluate pre-training representations in this setting. The authors don't discuss if this is even applicable for their formulation.\n\nRefs\n[1] Suresh, S., Li, P., Hao, C., & Neville, J. (2021). Adversarial graph augmentation to improve graph contrastive learning. Advances in Neural Information Processing Systems, 34, 15920-15933.\n\n[2] Yin, Y., Wang, Q., Huang, S., Xiong, H. and Zhang, X., 2022, June. Autogcl: Automated graph contrastive learning via learnable view generators. In Proceedings of the AAAI conference on artificial intelligence (Vol. 36, No. 8, pp. 8892-8900).\n\n[3] Feng, S., Jing, B., Zhu, Y. and Tong, H., 2024. Ariel: Adversarial graph contrastive learning. ACM Transactions on Knowledge Discovery from Data, 18(4), pp.1-22."}, "questions": {"value": "1) GRNs need to be estimated for the \"teacher\" views as well which encode the results of the knockdown experiment. It would good for the authors to comment on how expensive SuPGCL is in end-to-end computational complexity compared to self-supervised approaches such as GRACE / SGRL and even simpler and baselines like w/o - pretrain.\n\n2) Other than zeroing out features for nodes and surrounding edges, were other artificial augmentations tried to simulate knockdown genes. Like for example 1) randomizing the knockdown node features 2) removing a portion of the edges surrounding the knockdown node etc. The question is more about the motivation of the chosen approach for simulation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jlAGfZupH8", "forum": "5BaWkkZ24s", "replyto": "5BaWkkZ24s", "signatures": ["ICLR.cc/2026/Conference/Submission17366/Reviewer_nY9n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17366/Reviewer_nY9n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17366/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950305673, "cdate": 1761950305673, "tmdate": 1762927277573, "mdate": 1762927277573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}