{"id": "g0Vw52zai8", "number": 563, "cdate": 1756746894954, "mdate": 1763593180438, "content": {"title": "GASPACHO: Gaussian Splatting for Controllable Humans and Objects", "abstract": "We present GASPACHO, a method for generating photorealistic, controllable renderings of human–object interactions from multi-view RGB video. Unlike prior work that reconstructs only the human and treats objects as background, GASPACHO simultaneously recovers animatable templates for both the human and the interacting object as distinct sets of Gaussians, thereby allowing for controllable renderings of novel human object interactions in different poses from novel-camera viewpoints. We introduce a novel formulation that learns object Gaussians on an underlying 2D surface manifold rather than in 3D volume, yielding sharper, fine-grained object details for dynamic object reconstruction. We further propose a contact constraint in Gaussian space that regularizes human–object relations and enables natural, physically plausible animation. Across three benchmarks—BEHAVE, NeuralDome, and DNA-Rendering—GASPACHO achieves high-quality reconstructions under heavy occlusion and supports controllable synthesis of novel human–object interactions. We also demonstrate that our method allows for composition of humans and objects in 3D scenes and for the first time showcase that neural rendering can be used for the controllable generation of photoreal humans interacting with dynamic objects in diverse scenes.", "tldr": "We present a method for controllable (human identity+object category+human/object pose+camera pose) generation of photoreal human-object interaction", "keywords": ["gaussian splatting", "human-object interaction", "3D humans"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d289754e4f944382ba0120723420b602ad8e17c9.pdf", "supplementary_material": "/attachment/ba0b7c4d1ca457c1832ba2693f10f8c55b8725a4.zip"}, "replies": [{"content": {"summary": {"value": "This work focuses on modeling human-object interactions from multi-view RGB video with Gaussian Splatting. While previous work often under neglect the dynamic movement of object, this work introduce a coarse-to-fine pipeline for reconstructing dynamic objects. The moded humans and objects can be animated to synthesize novel interactions. It also introduce human-object contact constraints in Gaussian splace to ensure proper concats when 3D Gaussian humans are animated to interact with objects."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. While the baseline methods cannot accurately reconstruct objects, the proposed method demonstrates much better visual performance than baselines.\n\n2. The introduced components are vaild through ablation study.\n\n3. Methods are introduced with mathematical equations."}, "weaknesses": {"value": "1. The process of obtaining position maps from input images should also be included in Figure 2 for completeness.\n\n2. The application of reconstructing human interact with novel objects seems fancy, but it can be achieved with Gaussian editing or segmention baselines. The advancement of using the proposed method is not mentioned.\n\n3. The introduction of baselines in ablation studies is confusing. Meanwhile the paper uses a lot of bold font without a clear pattern. The format of this paper need to be further polished.\n\n4. Figure 3 seems to be redundant, given the information are included in figure 2. \n\nMissing related work:\n[1] Wang, Xiaoyuan, et al. \"HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis.\" arXiv preprint arXiv:2506.19291 (2025).\n\nMinor weakness: Some texts are overlap with figures. Meanwhile some labels of figure images are introduced in the caption, which could be clearer if put them directly under the images, like Figure 7. Meanwhile the space between text looks weird. Hope this can be adjusted in the revision."}, "questions": {"value": "1. Is this method able to model multple humans and objects in one scene?\n\n2. Why traditional 3DGS fails under natrually occurring human-object occlusions, but feature-based planes can manage that? Are other baselines also feature-based? If not, the contribution of feature-based representation seems to be more important."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IsW4ymKdOg", "forum": "g0Vw52zai8", "replyto": "g0Vw52zai8", "signatures": ["ICLR.cc/2026/Conference/Submission563/Reviewer_ttja"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission563/Reviewer_ttja"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761305576420, "cdate": 1761305576420, "tmdate": 1762915547640, "mdate": 1762915547640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GASPACHO, a method for generating photorealistic and controllable renderings of human–object interactions from multi-view RGB video. The approach models both the human and the interacting object as distinct sets of Gaussian primitives. It introduces a novel formulation for objects, learning Gaussians on an underlying 2D surface manifold to capture fine-grained details. The method also proposes a contact constraint in Gaussian space to regularize human-object relations and enable physically plausible animations. Experiments on the BEHAVE, NeuralDome, and DNA-Rendering datasets demonstrate high-quality reconstructions under occlusion and the ability to synthesize novel, controllable interactions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper simultaneously reconstructs humans and interacting objects under occlusion, advancing beyond human-only Gaussian methods.\n- Introducing contact constraints in Gaussian space improves physical plausibility and reduces interpenetration during animation.\n- The framework enables photorealistic and controllable synthesis of human–object interactions across diverse scenes and viewpoints."}, "weaknesses": {"value": "- The quantitative results presented in Table 1 and Table 2  are difficult to parse quickly. The best-performing result in each column is not bolded or otherwise highlighted, forcing the reader to manually scan all numbers to identify the state-of-the-art.\n- Although the paper introduces a contact constraint in Gaussian space, the contact quality remains suboptimal. Hands and other body parts often penetrate the object surfaces, suggesting that the constraint is weak or insufficiently enforced during animation.\n- The method makes a strong assumption that it only models \"one dynamic object\" and that this object's motion \"can be explained using only a rigid transform\". This is a significant limitation, as it excludes a vast category of common human-object interactions involving non-rigid or articulated objects, such as interacting with blankets, clothing, ropes, or laptops."}, "questions": {"value": "- In Figure 2, what does the gray translucent mask represent?\n- The contact constraint (Sec 3.5) is defined by mapping a fixed set of SMPL vertices (\"feet, hips, hands\")  to the nearest Gaussians. How does the method handle or enforce plausible contact for other body parts not in this list?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zZyjxSjZPO", "forum": "g0Vw52zai8", "replyto": "g0Vw52zai8", "signatures": ["ICLR.cc/2026/Conference/Submission563/Reviewer_H4ie"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission563/Reviewer_H4ie"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891441496, "cdate": 1761891441496, "tmdate": 1762915547489, "mdate": 1762915547489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GASPACHO, a 3D Gaussian-based neural rendering framework that jointly reconstructs animatable humans and dynamic objects from multi-view RGB sequences, with the explicit goal of enabling controllable human–object interactions under novel human/object poses and viewpoints. The core ideas are: (i) learn pose-dependent Gaussian maps for humans and pose-independent Gaussian maps for objects, anchored to canonical templates; (ii) use a composition- and occlusion-aware loss during training so that occluded human regions are not penalized improperly; and (iii) introduce a Gaussian-space contact refinement that adjusts a sparse set of “contact Gaussians” to promote physically plausible human–object contact at test time. Compared to prior 3DGS avatar work that typically reconstructs humans in isolation, GASPACHO explicitly separates and animates both entities and demonstrates novel cross-sequence retargeting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses controllable human–object interactions instead of human-only avatars, enabling cross-sequence retargeting and scene composition.\n2. Introduces pose-independent Gaussian maps for rigid objects that stabilize pose optimization and sharpen textures; principled occlusion-aware losses mitigate erroneous supervision; contact refinement improves plausibility.\n3. Demonstrates consistent quantitative gains over strong baselines and compelling qualitative compositions across multiple datasets."}, "weaknesses": {"value": "1. Restricts to one rigid object, and there is no support for non-rigid objects or multiple objects, limiting applicability to richer HOI scenes.\n2. Requires SMPL poses and object masks; sensitivity to pose/mask errors is not thoroughly analyzed. A robustness study would be informative.\n3. While the paper explains why some dynamic 3DGS systems are not controllable, additional qualitative side-by-side or an explicit metric for controllability would strengthen the case. Also, report compute/time/memory for training/inference to contextualize practicality."}, "questions": {"value": "1. What are training and inference times, memory footprints, and Gaussian counts for typical sequences (by dataset)?\n2. How does performance degrade with noisy SMPL, imperfect masks, or fewer cameras? Can the method recover without accurate masks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uop95FnqD2", "forum": "g0Vw52zai8", "replyto": "g0Vw52zai8", "signatures": ["ICLR.cc/2026/Conference/Submission563/Reviewer_zCUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission563/Reviewer_zCUP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972550329, "cdate": 1761972550329, "tmdate": 1762915547365, "mdate": 1762915547365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GASPACHO, a framework that reconstructs animatable humans and objects from multi-view posed videos. It reconstructs humans and objects separately using distinct sets of 3D Gaussians, enabling controllable rendering of novel human–object interactions under different poses and from novel camera viewpoints. For object reconstruction, it employs pose-independent Object Gaussian Maps for efficient reconstruction. Additionally, it introduces a contact constraint in Gaussian space to regularize human–object relationships. Experimental results demonstrate that the proposed method achieves SOTA performance and produces reasonable novel view renderings."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed work addresses the problem of reconstructing both humans and objects from multi-view posed videos and rendering novel human–object interactions with new poses, which is interesting.\n\n- GASPACHO reconstructs objects using pose-independent object maps, enabling efficient and robust object reconstruction.\n\n- The proposed method achieves comparable performance to previous SOTA methods on human reconstruction and animation tasks."}, "weaknesses": {"value": "- The writing quality is poor, making the paper difficult to read and understand. Moreover, many technical details are unclear:\n    - Numerous mathematical notations are introduced without proper explanation, making it difficult to interpret the method section.\n    - In Line 265, the term “ground-truth images” is confusing, as ground truth usually refers to evaluation images; input images would be more appropriate.\n    - In Line 199, since the Gaussians are learned later, how are their locations projected onto the planes?\n     - In Line 202, it is unclear how the frame with minimal occlusion is determined.\n    - The process of obtaining the object template is poorly described; providing pseudocode for this step would improve clarity.\n    - The initialization and training procedure of the StyleUNet network for producing Gaussians are not explained.\n\n- The paper includes only one quantitative comparison with previous methods, which is insufficient to validate the results.\n\n- The paper lacks a discussion of the proposed method’s limitations."}, "questions": {"value": "- The main issue lies in the clarity of the paper’s writing and presentation.\n\n- How do the training and inference times compare with the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tNzFqsTCOc", "forum": "g0Vw52zai8", "replyto": "g0Vw52zai8", "signatures": ["ICLR.cc/2026/Conference/Submission563/Reviewer_Rw2b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission563/Reviewer_Rw2b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975309062, "cdate": 1761975309062, "tmdate": 1762915547272, "mdate": 1762915547272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}