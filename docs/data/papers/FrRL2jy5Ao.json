{"id": "FrRL2jy5Ao", "number": 18573, "cdate": 1758289196166, "mdate": 1763187644429, "content": {"title": "OptSHAP: Explaining Dimensionality Reduction-based Models for Tabular Data via Optimization", "abstract": "Dimensionality Reduction-based Models (DRbMs), which couple a dimensionality reduction technique with a predictive model, are commonly used to mitigate overfitting and reduce computational complexity regarding high-dimensional tabular data. However, their two-stage architecture presents considerable challenges for explainability, as the projection obscures the original feature space, thus making the model output difficult to interpret in terms of the input features. Model-agnostic explanation methods are applicable to DRbMs but typically rely on sampling-based approximations, leading to instability and low-faithfulness explanations. To address these limitations, we introduce OptSHAP, the first optimization-based attribution specifically designed for DRbMs. Our method leverages reduced-space attributions and then redistributes them back to the original feature space through a transformation that satisfies the principle of efficiency. Additionally, we propose a novel evaluation metric, the $k$-Local Stability Score (LSS), which quantifies the stability of feature attribution methods by averaging their distances to local explanations. Extensive empirical evaluations across high-dimensional datasets, various dimensionality reduction techniques, and multiple machine learning models demonstrate that OptSHAP outperforms state-of-the-art attribution methods, achieving up to $24\\times$ stability and $2\\times$ fidelity on key benchmarks.", "tldr": "A novel attribution explanation method for Dimensionality Reduction-based Models", "keywords": ["Explainable Artificial Intelligence", "Dimensionality Reduction", "Tabular Data", "Optimization"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b2991d4ca6b5d50b80c0ca121e9fef81c8836156.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the feature attribution problem in the context of DRbMs for tabular data, a niche area within explainability research. The authors identify a gap between standard feature attribution methods and DR techniques and propose an optimization-based transformation block $\\mathcal{T}$ to bridge this gap. The transformation block strictly follows two fundamental properties and can be used as a plugin for existing explainers to enhance their performance on DRbMs. Finally, the proposed approach is integrated with TreeSHAP and empirically compared with several well-known black-box explainers for validation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- This work is reasonably organized, with a straightforward storyline that facilitates the understanding of the proposed method.\n- The authors list fundamental properties of DRbM explainers with appropriate justifications, providing a theoretical foundation that partly supports the design of the method.\n- The experimental setting includes validation metrics from complementary perspectives."}, "weaknesses": {"value": "- This paper uses a method name that appears somewhat over-claiming. Although titled “OptSHAP” and leveraging results from TreeSHAP, the proposed method itself has no connection to either SHAP or the Shapley value, according to the description in Section 4. Including “SHAP” in the method name therefore appears misleading.\n- The motivation is not convincing:\n    - This paper asserts that model-specific methods are unfit for DRbMs. Yet, given the sole focus on linear DR techniques, it is unclear why existing explanation methods would fail to handle such linear transformations. Particularly, such a linear transformation is analogous to a single dense layer without an activation function, which most approaches can accommodate. That said, I agree there are difficulties in processing information flow through a two-stage heterogeneous architecture, but this seems more like an engineering problem rather than a research one.\n    - Additionally, linear transformations are generally regarded as explainable/interpretable by themselves. The motivation to develop a specialized approach for such transparent mappings is not well justified. The inherent transparency of linear DRs likely explains why only very few studies specifically investigate their explainability, as they can be covered within a more general framework.\n- The method design is not sufficiently motivated. It is promising that the author listed some properties to guide the design; however, it still remains unclear why a $\\tanh(\\cdot)$, which does not reflect the linear nature of the target transformations, was adopted for the redistribution of attributions.\n- The experimental design represents another major weakness. All the other competitors are black-box approaches and relatively old (earlier than 2017). Comparing TreeSHAP, a white-box approach that leverages the advantage in model structural knowledge, with black-box approaches weakens the credibility of the results, making it unclear whether the observed improvements are achieved by the proposed optimization process or from TreeSHAP.\n\nTo conclude, this work proposes a plugin for feature attribution in DRbM settings; its contribution and impact appear limited due to insufficient motivation and an unjustified method design."}, "questions": {"value": "- See weaknesses\n- Could the authors further clarify the specific design of $\\mathcal{T}$? Why is a direct decomposition/redistribution of attributions in the reduced space, according to the proportion of each contribution, insufficient?\n- Could the authors clarify the MoRF and ABPC results? Why do many of them show negative values? I am skeptical about the general experimental setup, as negative MoRFs suggest that removing the most relevant features leads to an increase in prediction confidence, which contradicts the expectation.\n- How is OptSHAP designed to deliver “variance-reduced” explanations? It appears that stability is guaranteed directly by TreeSHAP, and thus should not be considered a contribution of this work if that is the case."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bXx7SNMEkg", "forum": "FrRL2jy5Ao", "replyto": "FrRL2jy5Ao", "signatures": ["ICLR.cc/2026/Conference/Submission18573/Reviewer_2hiz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18573/Reviewer_2hiz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761646603504, "cdate": 1761646603504, "tmdate": 1762928286430, "mdate": 1762928286430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "gi4jcp7xhL", "forum": "FrRL2jy5Ao", "replyto": "FrRL2jy5Ao", "signatures": ["ICLR.cc/2026/Conference/Submission18573/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18573/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763187643317, "cdate": 1763187643317, "tmdate": 1763187643317, "mdate": 1763187643317, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "OptSHAP is an optimization-based reallocation method that turns reduced-space attributions into original-feature attributions for DR pipelines.\n\nIt does so using a tanh-gated transform keyed to the projection’s Jacobian to preserve signs and approximate efficiency. \n\nIt introduces k-Local Stability Score for efficient stability evaluation. \n\nOn three high-dimensional tabular datasets across PCA/ICA/LDA and four tree models, OptSHAP improves faithfulness (MoRF/ABPC) and stability (LSS, INFD) compared to KernelSHAP, EIC, and LIME. \n\nHypothesis tests suggest allocation does not significantly change total importance. \n\nThe paper also argues G-DeepSHAP is unsuitable for DRbMs due to denominator blow-ups in latent space distances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a timely problem. The toy example + formal problem statement well motivate “how to get back to features” with sign and efficiency goals.\n\nThe tanh-gate transform is conceptually simple, bounded, and sign-aware; is lightweight and avoids sampling variance. The sign-preservation theorem and noise-stability bound are helpful sanity checks (even if conservative)"}, "weaknesses": {"value": "The formula is not derived from a theoretical decomposition of SHAP or any formal game-theoretic principle. Instead, it is heuristically designed to satisfy two intuitive desiderata. The paper does not systematically explore alternate formulas or functional forms for the transform T. However, they do justify the use of tanh in the appendix.\n\nOptSHAP do not satisfy the full Shapley axiom set for the original features, nor claim uniqueness of the solution. In such a scenario, I'm not sure if using the term SHAP in the title is a good idea."}, "questions": {"value": "How would tanh compare to Linear mapping, Softmax or ReLU weighting or Normalized loadings?\n\nThe current theorems ensure local sign and stability but not closeness to the true Shapley decomposition. Can the authors derive a bound on the deviation of OptSHAP attributions from Shapley-consistent values?\n\nDo results hold if f is logistic regression or a shallow MLP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6b3oZkFpzd", "forum": "FrRL2jy5Ao", "replyto": "FrRL2jy5Ao", "signatures": ["ICLR.cc/2026/Conference/Submission18573/Reviewer_YAFh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18573/Reviewer_YAFh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886420441, "cdate": 1761886420441, "tmdate": 1762928285934, "mdate": 1762928285934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work aims particularly to pass the attributions from the reduced feature space to the original, interpretable feature space under a setting of DRbMs. Although SHAP appears to be part of the name, this work does not investigate or propose any approach that facilitates the estimation of Shapley values. Instead, it focuses solely on the redistribution of attributions for the linear DR transformation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-The authors address a rather important topic"}, "weaknesses": {"value": "- The references are rather old.\n- The approach is very simple and intuitive, as described in section 4, and illustrated by Figure 1. Yet it is confusing why a particular solution should be developed for explaining linear components, which are already explainable/interpretable by themselves.\n- There is also a fundamental misunderstanding between the MSE and the efficiency property. The authors refer to IG and SIM-Shapley to justify the slight deviation from the efficiency property. At least for these two articles, the original papers are referring to the MSE of the estimates rather than the deviation in terms of efficiency."}, "questions": {"value": "- Section 4 introduces an optimization problem that is subject to the efficiency property. However, it appears unclear at all why the allocation transform is activated by $\\tanh$. Given the linear property of the studied transformations, an attribution redistribution directly following the weights of each feature will lead to a perfect decomposition. For example, $(0.36, 0.3, -0.06)$ and $(0.086, -0.129, 0.343)$, derived from the normalized contribution to each PC, are solutions that strictly align with the linearity of the DRs and adhere to the efficiency property. Why is the proposed method better than this natural solution?\n\n- The experimental results are less convincing, as the allocation method is combined with a much more powerful explainer (TreeSHAP). Shouldn’t the competitors share the same explanation kernel to highlight the benefit of the specific re-allocation design? This is easily doable as all the competitors can be deployed at the reduced feature space and then concatenated by the “Opt”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YT5hkWdgcc", "forum": "FrRL2jy5Ao", "replyto": "FrRL2jy5Ao", "signatures": ["ICLR.cc/2026/Conference/Submission18573/Reviewer_Xksz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18573/Reviewer_Xksz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991937969, "cdate": 1761991937969, "tmdate": 1762928284477, "mdate": 1762928284477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-step approach called OptSHAP to compute feature attributions for dimensionality reduction-based models. First, model-specific attribution methods such as TreeSHAP are used to obtain attribution scores for the features in the reduced space. Second, the attribution scores in the reduced space are allocated to the input features by solving an optimization problem.\n\nTheoretically, it is shown that OptSHAP satisfies sign preservation between the reduced and input space and is stable with respect to gradient perturbation. Empirically, OptSHAP (1) outperforms model-agnostic baselines with respect to the insertion/deletion and stability metrics, and (2) satisfies the efficiency property."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper addresses the under-explored problem of feature attribution in dimensionality reduction-based models, proposing the desirable property of sign preservation when allocating the reduced-space attributions to input feature attributions.\n- The theoretical results are supported by empirical results on simulation or real-world data.\n- OptSHAP outperforms model-agnostic baselines in both the insertion/deletion metrics and stability metrics."}, "weaknesses": {"value": "- The empirical results focus entirely on tree-based models as the predictor. Although the authors motivate this choice in Remark 4.3, the limited scope still raises the question about whether OptSHAP works **empirically** for other predictor classes such as neural networks.\n- Theorem 4.4 is motivated by approximation errors that can happen in gradient computation. It is unclear whether Gaussian nosies are reasonable distributions for modeling such approximation errors. The choice of Gaussian noises in Theorem 4.4 need to be better connected to the problem of approximation errors.\n- Lines 394-396 mention that individual features may contribute positively or negatively to the output, and the implication is that attribution values should be balanced around zero. It is unclear why the implication holds. For example, it's possible that more samples have negative contribution from a given feature. In this case the beeswarm plot in Figure 6 can look skewed (such as the LIME example)."}, "questions": {"value": "- Does OptSHAP outperform model-agnostic baselines when the downstream predictor is not tree-based, such that attribution methods other than TreeSHAP need to be used? Overall, is the empirical success of OptSHAP restricted to tree-based predictors?\n- Why are gradient approximation errors modeled by Gaussian nosies?\n- How is the second expectation in Section 6.4 justified theoretically? Do we expect the second expectation to hold for all datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ETDiY4YMEE", "forum": "FrRL2jy5Ao", "replyto": "FrRL2jy5Ao", "signatures": ["ICLR.cc/2026/Conference/Submission18573/Reviewer_excZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18573/Reviewer_excZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762367127376, "cdate": 1762367127376, "tmdate": 1762928282555, "mdate": 1762928282555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}