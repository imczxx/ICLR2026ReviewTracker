{"id": "fa8iL9O7QV", "number": 25114, "cdate": 1758364320322, "mdate": 1759896733811, "content": {"title": "REAL-TIME RISK EVALUATION FOR LLM DECISION- MAKING VIA AN REGRET BOUND", "abstract": "We study real-time risk certification for large language model (LLM) agents with black-box action selection rules, aiming to upper-bound the per-round regret. We fix a reference policy map $f$ (e.g., a softmax with temperature $T$, whose TV-Lipschitz constant is $C$, though any TV-Lipschitz mapping can be used), which takes a predicted opponent action distribution as input and returns a reference policy. We form the plug-in reference policy $s\\_{\\hat{\\mu}\\_t}=f(\\hat{\\mu}\\_t)$ from the model's predicted opponent distribution $\\hat{\\mu}\\_t$. Our certificate is $r\\_t \\le L(E\\_{pred}+E\\_{pol}+E\\_{mis})$,\nwhere $E\\_{pred}:=\\frac{C}{2}||\\mu\\_t-\\hat\\mu\\_t||\\_1$ (prediction error), $E\\_{pol}:=\\frac{1}{2}||\\pi\\_t^{\\ast}-s\\_{\\mu\\_t}||\\_1$ (policy error), $E\\_{mis}:=\\frac{1}{2}||\\pi\\_t-s\\_{\\hat\\mu\\_t}||\\_1$ (policy mismatch), $L$ is the Lipschitz constant of the instantaneous regret with respect to total variation induced by $Q$ (hence domain-dependent), $C$ is the TV-Lipschitz constant of $f$, $\\pi\\^*\\_t$ denotes the one-hot best response to $\\mu_t$ under $Q\\_t$ (ties broken arbitrarily), and $\\pi_t$ is the agent's policy. We assume access at time $t$ to the realized opponent distribution $\\mu_t$ and the per-round payoffs $Q_t$ (and hence $\\pi^{\\ast}$), so the certificate is fully computable in real time. In this bound, prediction error measures the accuracy of the model's opponent modeling (belief calibration). In contrast, policy error, together with the policy mismatch $\\frac{1}{2}\\|\\pi_t-s_{\\hat{\\mu}_t}\\|_1$, quantifies the precision of the decision side given $\\hat{\\mu}\\_t$. Therefore, this bound enables us to localize the risk of the decision to either prediction or action selection. We applied the certificate to separate, in real time and for black-box policy agents, whether decision risk stems from prediction or from action selection. In the Ultimatum and $2\\times2$ general-sum games, the dominant component is opponent- and game-dependent. This separation does not yield a characterization common to all games and opponents, but under the same game and opponent strategy, it reveals consistent differences between models.", "tldr": "", "keywords": ["LLM", "game theory"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c0a730952e32408e9cdda672363e43bf0ebcb85.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a per-round regret decomposition for LLM-based agents into prediction error, policy error, and policy mismatch, claiming it provides a real-time “risk evaluation” for LLM decisions. The authors test it on simple 2×2 games (Prisoner’s Dilemma, Ultimatum, Win-Win) using GPT-based models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The topic could become interesting if extended to non-toy settings, real online environments, or theoretically justified forms of dynamic regret analysis."}, "weaknesses": {"value": "1. **Technical Depth**\n   * The final result in Eq 6 is a direct restatement of standard Lipschitz and triangle inequalities from classical online learning.\n\n2. **Conceptual and Mathematical Confusion**\n   * The decomposition is just an additive partition of regret into L₁ distances; it does not represent a new bound or performance measure.\n   * Regret is inherently non-decreasing over time, yet all figures show decreasing regret curves—this indicates either a misunderstanding or incorrect implementation. They define per-round regret, I am not sure this is well-defined and also plotted in the graph\n\n3. **Not Clear Experiments**\n   * Figures lack numerical scales, explanations, or statistical comparisons. The discussion simply narrates obvious behavioral differences (“GPT5 defects less”) without analysis.\n   * The claimed “diagnostic value” of the decomposition is never demonstrated; the results provide no actionable insight or correlation with performance.\n   * My main point is that the experiments use only static, toy 2×2 games with hard-coded payoff matrices and no adaptive or learning component. Hence, the results do not provide new insights into LLM performance or regret minimization. A constructive suggestion would be to test the proposed framework on more complex or dynamic environments where prediction and policy mismatch could vary meaningfully over time. Also please add the prompt that authors used. Some of the contents in Appendix should be moved to main paper. \n\n4. **Poor Writing and Presentation Quality**\n   * The paper has many grammatical errors and awkward phrasing (“an regret bound,” “LLMs tends to choose defect”).\n   * Notation is inconsistent and undefined in several places. -- For example, name of the LLM is not exactly same as the official name and many inconsistency between figure and main paper (GPT4o, gpt-4o), inconsistent upper case and lower case.  No definition on Y, A, policy map, etc... \n   * Figures are unlabeled, and captions are uninformative, and not easy to read."}, "questions": {"value": "Written in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OgFxvJKthq", "forum": "fa8iL9O7QV", "replyto": "fa8iL9O7QV", "signatures": ["ICLR.cc/2026/Conference/Submission25114/Reviewer_BKcF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25114/Reviewer_BKcF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760535381572, "cdate": 1760535381572, "tmdate": 1762943329728, "mdate": 1762943329728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how to quantify per-round regrets for LLM decision making. The authors derive upper bounds for two-player LLM games. Then, they conduct experiments in a variety of games with multiple LLMs, and compare the derived upper bound and the real regret bound."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Quantifying LLM decision-making regret is an important topic. The setup and frequently used notation are explained clearly. Experiment results are provided to verify their theoretical founding."}, "weaknesses": {"value": "The contribution of this paper is limited and I believe it does not match the bar of this conference. Although the authors spend much efforts in deriving the upper bounds for regret in the main text, the result is a direct corollary by applying basic triangular inequalities, and therefore, is trivial.\n\nBesides, empirically, it seems the gap between derived upper bound and the real regret is very large (e.g., Fig 1 and 7), which suggests the derived upper bound is not an effective indicator. It is valuable to investigate tighter upper bounds to characterize the real-time risk."}, "questions": {"value": "In experiments, given that the agents are LLM, it is not straightforward for me to see how the upper bound (Eq. 6) is computed. For example, how to compute $\\hat{\\mu}_t$ and $f(\\hat{\\mu}_t)$? I would suggest the authors to elaborate this in main text (or move the discussion from appendix to the main text)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ACaWV38YkO", "forum": "fa8iL9O7QV", "replyto": "fa8iL9O7QV", "signatures": ["ICLR.cc/2026/Conference/Submission25114/Reviewer_oco5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25114/Reviewer_oco5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761038490427, "cdate": 1761038490427, "tmdate": 1762943328706, "mdate": 1762943328706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to provide a decomposition for the per-round regret, which might offer a diagnostic tool for deploying LLM agents. Authors give upper bounds of each part of the decomposition."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Providing a practical diagnostic tool for deploying LLM agents is a timely topic."}, "weaknesses": {"value": "- Authors hope to provide a diagnostic tool for analyzing the per-round regret. However, the final three terms of the decomposition depends on the opponent's policy, which cannot be known in many cases. \n\n- This manuscript seems not to be ready for the submission since the presentation is highly uncelar, which makes readers hard to follow. Moreover, there are many typos across the whole manuscript. The writing of the abstract is distracting. It looks like authors hope to formally define many definitions in the abstract, which makes no sense to me. Even in this case, authors still do not define what is $\\mathbb{Y},\\mathbb{A}$ in the abstract. \n\n- The presentation of the problem setup in Section 3 is chaotic. Authors do not justify the reference rule. Many typos appear in section 3, which makes readers hard to comprehend."}, "questions": {"value": "I would suggest authors carefully revising the manuscript. For example, authors should present main message in the abstract rather than trying to formally write the problem setup there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AvJ506Cvi9", "forum": "fa8iL9O7QV", "replyto": "fa8iL9O7QV", "signatures": ["ICLR.cc/2026/Conference/Submission25114/Reviewer_YBPU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25114/Reviewer_YBPU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761543680203, "cdate": 1761543680203, "tmdate": 1762943328467, "mdate": 1762943328467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for real-time risk certification for Large Language Model (LLM) agents operating in interactive environments, aiming to upper-bound the per-round regret. The core contribution is the derivation of this regret bound, which is decomposed into three specific terms: prediction error, policy error, and policy mismatch. This decomposition yields real-time diagnostics that crucially allow attribution of decision risk, revealing whether failures stem from flawed opponent modeling (belief calibration) or from suboptimal action selection given the agent's belief. Experiments conducted across $2 \\times 2$ general-sum games (like the Prisoner's Dilemma) and Ultimatum games demonstrate that the dominant risk component shifts depending on the specific LLM, game, and opponent strategy, thus establishing per-round online certification as a practical diagnostic tool for safer and more effective LLM agent deployment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper introduces a novel per-round regret certificate to set an instantaneous upper bound on LLM decision risk in interactive environments. \n\n* The framework adapts classical per-round bounds from online learning, ensuring a rigorous theoretical foundation that is fully computable in real time. \n\n* Experiments validate the framework across key strategic settings, including $2 \\times 2$ general-sum games (like Prisoner's Dilemma) and the Ultimatum Game, utilizing multiple state-of-the-art LLMs (GPT4o, GPT5, gemini2.5 flash lite)."}, "weaknesses": {"value": "* The experimental validation utilizes only a **fixed set of payoff parameters** for each specific game structure (e.g., Prisoner's Dilemma, Ultimatum Game). This critically limits the verification of the framework's robustness, as the key calculated components—specifically the regret bound's scaling factor ($L$) and the optimal action choice ($\\pi^*_t$) for the Policy Error term—are fundamentally dependent on those specific payoff values.\n\n* The framework critically relies on real-time access to the opponent's true action distribution ($\\mu_t$) and payoffs ($Q_t$). While standard in theory, this assumption limits its practical utility in real-world black-box settings, as $E_{pred}$ (Prediction Error) and the optimal response ($\\pi^*_t$) cannot be accurately computed without knowing $\\mu_t$."}, "questions": {"value": "* To validate the framework's robustness, are supplementary experiments necessary using diverse payoff matrices that systematically vary the domain-dependent scaling factor ($L$)?\n* If real-world deployment prohibits accessing the opponent's true distribution ($\\mu_t$), how does the framework maintain its diagnostic utility and theoretical guarantees under partial observability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VpGGVqHFRa", "forum": "fa8iL9O7QV", "replyto": "fa8iL9O7QV", "signatures": ["ICLR.cc/2026/Conference/Submission25114/Reviewer_GEfJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25114/Reviewer_GEfJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841340459, "cdate": 1761841340459, "tmdate": 1762943328191, "mdate": 1762943328191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}