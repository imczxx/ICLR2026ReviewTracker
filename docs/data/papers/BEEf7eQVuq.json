{"id": "BEEf7eQVuq", "number": 5098, "cdate": 1757847336034, "mdate": 1759897994574, "content": {"title": "Accelerating Transformer Training: Architectural Symmetry, Positional Encoding, and Teleportation", "abstract": "As neural architectures continue to grow in complexity and scale, the development of advanced optimization techniques has become increasingly important. Teleportation has recently emerged as a principled approach for accelerating the convergence of gradient descent-based algorithms by traversing loss-invariant level sets to identify parameterizations with favorable geometric properties. Although prior teleportation methods have achieved notable success in feedforward and convolutional networks, extending these techniques to Transformer architectures presents unique challenges. In particular, existing approaches typically assume the symmetry structure of vanilla attention, overlooking the critical role of positional encodings, which fundamentally reshape architectural symmetries and render earlier analyses inapplicable. To address this gap, we present a systematic study of teleportation in Transformer-based models. We first characterize how the architectural symmetry of multihead attention is modified under two widely used positional encoding schemes--sinusoidal and rotary--and provide a comprehensive description of the resulting symmetry groups. Guided by these insights, we introduce a teleportation framework tailored to Transformers and evaluate its effectiveness across diverse configurations, datasets, and modalities. Our results demonstrate the versatility of teleportation, elucidate the interplay between positional encoding and architectural symmetry in Transformer optimization, and establish a foundation for the principled development of teleportation algorithms that fully exploit the symmetry structure of Transformer architectures.", "tldr": "This work present a systematic study of teleportation in Transformer-based models", "keywords": ["transformer", "functional equivalence"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bfa1bc1d35ad82abbcd1314d712cd2f87df3fe78.pdf", "supplementary_material": "/attachment/d75d53eeabb520c809c6f008728554d00495b448.zip"}, "replies": [{"content": {"summary": {"value": "Based on a systematic study of multi-head attention and positional embeddings, this paper successfully extends the teleportation technique to Transformer architectures. Moreover, to avoid using expensive Hessian-based methods, the authors propose finding an optimal$g \\in G$ through small perturbations. Experimental results demonstrate its effectiveness on Transformer models for small vision and NLP tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well structured and easy to follow.\n- A detailed analysis of teleportation in Transformer architectures is provided."}, "weaknesses": {"value": "- The experiments are conducted only on very small datasets. Since Transformer architectures are typically applied to large-scale datasets, the current experimental settings are not very convincing.\n- What is the definition of \"speedup\" in Table 1? The “Time/epoch” for teleport-based training is higher than that of the baseline without teleportation, which seems inconsistent with the claimed “Speedup.”\n- Although the authors discuss and claim that the cost of perturbation sampling remains below 3% in their small-scale datasets and model settings, such overhead can be significant in large-scale Transformer pre-training under distributed settings.\n- Why is the speedup for the AdamW optimizer limited compared with SGD? As most large-scale Transformer models are trained with AdamW (or Muon), this limitation reduces the practical value of the proposed method."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PrIWDx0AXW", "forum": "BEEf7eQVuq", "replyto": "BEEf7eQVuq", "signatures": ["ICLR.cc/2026/Conference/Submission5098/Reviewer_GkSi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5098/Reviewer_GkSi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595685632, "cdate": 1761595685632, "tmdate": 1762917872492, "mdate": 1762917872492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The presented paper provides an explicit construction of the \"maximal symmetry group\" of self-attention with ROPE as its positional encoding. It also proposes a new teleportation method that introduces minimal perturbation and is easy to compute."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The explicit construction of the maximal symmetry group of MHA with ROPE seems pretty novel and strong. \n- The presentation is clear, and the paper is easy to follow.\n- I do appreciate the study of the maximal symmetry group, because it characterizes the boundary of symmetry-based methods. I believe this concept and the results proved in this paper can be beneficial to the community."}, "weaknesses": {"value": "Firstly, the whole section 4 looks pretty strange to me, for the following reasons:\n- I don't see a connection between Section 4 and previous sections. It seems like they are discussing totally different topics. In my understanding, the key idea of Section 2 and 3 is to derive a formula of the maximal symmetry group of MHA with ROPE, while in Section 4 the authors suddenly turn to discuss how to perform teleportation effectively. I don't see a clear logical connection between these two topics.\n- Also, I don't understand the idea behind \"minimal perturbation\". The entire concept of symmetry-based teleportation is to explore the weight space without changing the functionality, isn't it? If we keep the perturbation small, the small perturbation itself is enough to keep the functionality not changing too much, then what's the point of using symmetric transformations as teleportation?\n- In line 320, the authors claim: \"each teleportation step is now performed within this ball ...\". However, later in the actual implementation (line 330) \"we sample near the identity by constructing a diagonal matrix as follows...\", but the diagonal matrices do not form a ball in GL(n). Also, if the teleportation is limited to a zero-measure set of the symmetry group (the set of all diagonal matrices), what's the point of exploring the maximal symmetry group? \n\nOther weaknesses & questions:\n- To me the most novel and important result of this paper is perhaps Theorem 3.2. However, the main paper does not mention how to prove this theorem. I hope the authors can at least write a proof sketch and the ideas behind it.\n- There is no need at all to discuss the absolute positional encoding. Why not remove the whole Section 3.1 and save the sapce to write a proof sketch of Theorem 3.2?\n- The title and abstract is somewhat misleading. This paper actually only focuses on the symmetry in the self-attention module (which is reasonable, since the symmetry in FFN is the same as those in MLP and has been extensively studied). The current title and abstract is too broad. It's better to limit the scope to multi-head attention. The current title sounds like a survey paper.\n- Although the experiment results look good, they are limited to small datasets. and the ablation study is not sufficient enough. For example, I think this is the key question the ablation study should answer: to outperform the baseline, which part is the most critical? the \"small perturbation\", the sampling of $g$, or the stability truncation?\n\nMinor issues and questions:\n- What is \"proper real algebraic variety\"?\n- What does $\\nabla \\mathcal L|_{g\\theta}$ mean in eq.(13)? Do you mean the gradient at $g \\theta$?\n- In eq. (13), why minimize the operator norm of the gradient? \n- Definition 2.2 is not needed. Actually, I think the definition of \"maximal\" or \"small exception\" is still unclear to me. Is it possible to rewrite is with the language of, say, measures?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hem6FsVfYI", "forum": "BEEf7eQVuq", "replyto": "BEEf7eQVuq", "signatures": ["ICLR.cc/2026/Conference/Submission5098/Reviewer_PThV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5098/Reviewer_PThV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603246688, "cdate": 1761603246688, "tmdate": 1762917870043, "mdate": 1762917870043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses the symmetry group that occurs in the weight space of the self-attention component, with a focus on attention with rotary-positional encodings. It extends the ideas from previous work [1] by including the positional encodings. Then it proceeds to propose an algorithm for training with so called teleportation where in some steps the parameters are perturbed to values that result in a functionally-equivalent model but with larger gradient norms. Finally, the paper demonstrates positive effects of the proposed methods on the convergence speed of SGD and the generalization of the obtained model.\n\n[1] Hoang V. Tran, Thieu Vo, An Nguyen The, Tho Tran Huu, Minh-Khoi Nguyen-Nhat, Thanh Tran, Duy-Tung Pham, and Tan Minh Nguyen. Equivariant neural functional networks for transformers. In The Thirteenth International Conference on Learning Representations, ICLR, 2025."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The presentation of the paper is very good. The problem and prerequisites are well outlined and the main result clearly presented. I especially appreciate the shortened versions of the proofs in the appendix before stating the whole proof.\n2. For my understanding, finding a symmetry group that takes the embedding into account is novel.\n3. The proposed method seems to improve the convergence speed of Transformer models in the studied settings.\n4. The paper provides an ablation study of the proposed method together with recommendations on how to set the hyperparameters."}, "weaknesses": {"value": "1. The paper does not explain how the algorithm accounts for the symmetry group H from equation 9 in case of models with RoPE embedding. The procedure described in lines 329-335 explains only how to sample from GL(n).\n2. The optimizer of choice while training Transformers is usually Adam(W) and not SGD, yet the proposed method mainly improves the performance of SGD with only a tiny improvement for AdamW. The method applied to AdamW is also not tested on a language task only on MNIST and CIFAR-10 (table 5)."}, "questions": {"value": "1. What exactly is meant by sharpness in Table 4? Is it some Hessian-based measure or something else?\n2. When used on an architecture with RoPE, how does the method sample the set of perturbations for the query and key parameters so that they are part of H from equation 9?\n3. How does the method perform when the model is trained with AdamW on a language task?\n4. Does the teleportation + SGD outperform training with AdamW? I would be curious to see a figure similar to Figure 2 comparing these two settings.\n5. When adding teleportation to an optimizer, is it necessary to retune optimizer hyperparameters? I would ideally like to see some experimental evidence to whether that is the case or not.\n6. In line 429 the paper states that the teleportation can be applied also to the FFN part of the network. What is the symmetry group used in this case? Does it take activations into account?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "I would like to flag this submission as potentially violating the submission policy. Submission 5096 presents the same theoretical result on the characterisation of the symmetry of attention with widely used PE, and I have reasons to believe (see below) that both papers share authors. The papers use the theoretical result in different application contexts (LMC and efficient Transformer training) but they both claim that the theoretical result and its discussion are one of their main contributions.\n\nThe reason I believe that the papers share authors and claim the same contributions are the following parts of the papers:\n* In “Contribution” section of submission 5096 in lines 110-117 we read:\n>>2. In Section 3, we analyze how positional encodings alter the internal structure of attention. We focus primarily on the most widely used encodings, Absolute PE and Relative PE. In particular, we study sinusoidal PE as a representative of APE and rotary PE as a representative of RPE, and show why results from the vanilla case do not extend directly to these settings. \n>> 3. In Section 4, we present the main result of the paper, which characterizes the full symmetry of attention with widely used positional encodings. This characterization underlies the matching algorithm for Multihead Attention described in Section 5.\n\nMeanwhile, in “Contribution” section of submission 5098 in lines 98-102 we read:\n>> 2. In Section 3, we analyze how positional encodings alter the internal structure of attention. We focus primarily on the most widely used encodings, Absolute PE and Relative PE. In particular, we study sinusoidal PE as a representative of APE and rotary PE as a representative of RPE, and show why results from the vanilla case do not extend directly to these settings. We then present our finding that fully characterizes the symmetry of attention with widely used PE.\n* The symmetry group under RoPE both papers introduce is the same, just with slightly different notation, see lines 219-247 in 5096 and lines 249-278 in 5098.\n* The main theoretical result regarding multihead attention with RoPE is the same in both papers with just slightly altered exposition, see lines 308-320 in 5096 (Theorem 4.2) and lines 279-289 in 5098 (Theorem 3.2).\n* Both papers derive their theorem as a consequence of a more general result. Submission 5096 states it lines 289-292 (Theorem 4.1) while submission 5098 in lines 895-903 in the appendix (Theorem B.1). The proofs of both theorems also look strikingly similar (see lines 1499-1527 in submission 5096 and lines 905-933 in submission 5098)."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y0d3LMki9R", "forum": "BEEf7eQVuq", "replyto": "BEEf7eQVuq", "signatures": ["ICLR.cc/2026/Conference/Submission5098/Reviewer_V6vP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5098/Reviewer_V6vP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947258950, "cdate": 1761947258950, "tmdate": 1762917869699, "mdate": 1762917869699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper first analyzes how multihead attention symmetry is affected by sinusoidal and rotary positional encodings. Then this paper introduces a teleportation framework for transformer-based on these insights, and demonstrates its effectiveness across various settings, highlighting the relationship between positional encoding, symmetry, and optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a bridge for the practical application of the teleportation framework in deep learning training.\n2. This paper is well-organized and easy to read."}, "weaknesses": {"value": "1. In Table 1, since CIFAR and MNIST datasets are not large enough to demonstrate the generalization, further experiments on ImageNet1K comparing with other teleportation methods are needed to demonstrate the effectiveness of the proposed framework."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NV0onvMEDT", "forum": "BEEf7eQVuq", "replyto": "BEEf7eQVuq", "signatures": ["ICLR.cc/2026/Conference/Submission5098/Reviewer_BQtQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5098/Reviewer_BQtQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762197144132, "cdate": 1762197144132, "tmdate": 1762917869448, "mdate": 1762917869448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}