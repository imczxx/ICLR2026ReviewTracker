{"id": "7cecAmjinr", "number": 21055, "cdate": 1758313273705, "mdate": 1759896944739, "content": {"title": "Refusal Degrades with Token-Form Drift: Limits of Token-Level Alignment", "abstract": "Safety alignment of large language models (LLMs) is typically learned through supervised fine-tuning and preference optimization on a fixed distribution of token sequences. We show that this process couples refusal behavior to token form, making alignment fragile under token-form drift—semantics-preserving shifts in orthography, delimiters, substitutions, or segmentation. In controlled perturbation studies, we observe a universal rise–plateau–collapse pattern: refusals degrade as distributional divergence increases, harmful compliance peaks, and extreme shifts collapse into incoherence rather than recovered safety. To scale beyond handcrafted substitutions, we develop an LLM-in-the-loop perturbation framework that automatically discovers diverse, readable adversarial forms. Cross-form evaluation reveals a capability–vulnerability tradeoff: larger models resist low-level shifts longer, yet admit more effective perturbations over broader ranges, exposing wider attack surfaces. A patch-then-break study further shows that fine-tuning against one perturbation form does not transfer, as new effective forms re-emerge rapidly. These results demonstrate that current alignment remains token-level and form-sensitive, motivating future defenses that target semantics directly through form-invariant training, normalization, and cross-form robustness evaluation.", "tldr": "", "keywords": ["Token-form drift", "Safety alignment", "Adversarial input perturbations"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a68039dff5897c3f21444ee30473da8233300fb8.pdf", "supplementary_material": "/attachment/553d1be1fcf40621fc840f8836b89393bb15ab09.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates token level vulnerabilities in LLMs, particularly under conditions of token drift. To address this issue, the study introduces two complementary approaches. (1). a manual, character-level perturbation method for inducing token drift, and (2). an automated, iterative feedback based framework. Through these perturbations, the paper demonstrates the vulnerability of LLMs to token level drift and highlights the potential of such drifts to serve as effective jailbreak mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "In terms of models, a wider array of models have been used for the evaluation and the paper presents interesting analysis on the model behavior with the increasing size in language modes. \n\n1. Experiments were conducted on different datasets for the sake of generalization.\n\n2. Personally in my line of work I had observed the vulnerability of LLMs of character level drift and the message that the LLMs are vulnerable towards token drift is agreeable. Though the message on RLHF being the culprit for the vulnerability needs validation. See weakness for details.\n\n3. The interactive automated framework for evaluating the token drift is effective and performs well against jailbreaks. Given that the paper is not presented as jailbreak paper the results in this section are satisfactory. \n\n4. The paper does explore the possibilities of using the automated generation strategy towards model improvement."}, "weaknesses": {"value": "1. Can you motivate the reasoning for choosing the specific character perturbation as the perturbation strategy for manual perturbation. The motivation for the simple strategy though effective is unclear.\n\n2. Can you provide results from non-instruct models to validate the role of instruction tuning as the main culprit behind the token level degradation in language models\n\n3. The paper serves as a findings type of paper on establishing the token level vulnerabilities. While the presented work highlights the vulnerabilities of the existing models, validation on non-instruction tuned models/ isolated experiments with fine tuning is necessary (albeit in small scale) towards the conclusion of RLHF as culprit."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bzgT9DSh6n", "forum": "7cecAmjinr", "replyto": "7cecAmjinr", "signatures": ["ICLR.cc/2026/Conference/Submission21055/Reviewer_WNND"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21055/Reviewer_WNND"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761327173213, "cdate": 1761327173213, "tmdate": 1762940628427, "mdate": 1762940628427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper demonstrates that current LLM safety alignment is fragile to token-form distribution shifts. Through controlled and automated perturbation studies, the authors reveal a consistent rise–plateau–collapse failure pattern where refusals weaken, harmful compliance peaks, and models eventually become incoherent as perturbations increase. Larger models show stronger robustness at low-level shifts but expose larger attack surfaces across diverse perturbation forms, creating a capability–vulnerability tradeoff. The results show that alignment methods predominantly rely on surface correlations rather than semantic understanding, causing rapid re-emergence of jailbreaks despite fine-tuning. The paper motivates the need for form-invariant, semantics-focused defense strategies and highlights key contributions: conceptualizing token-form drift, empirically validating universal degradation dynamics, and emphasizing cross-form robustness evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper identifies and quantifies the alignment generalization gap under token-form drift. The study reveals an important hidden risk in current safety alignment: modern large models possess narrow safety generalization. The authors further design an automated framework to generate semantically consistent perturbations, showing that fragility to token-form drift is a systematic rather than incidental phenomenon."}, "weaknesses": {"value": "This paper identifies and systematizes the phenomenon of the alignment generalization gap under token-form drift; however, it lacks a theoretical analysis of this phenomenon. There are 3 key issues that deserve further attention:\n\n1. Can the source of token-form drift be theoretically modeled? For example, can the authors analyze how tokenization strategies, including segmentation rules and subword composition, lead to significant alignment degradation even when semantic meaning is preserved?\n\n2. Since the model can bypass alignment constraints under structural perturbations, does this imply a different degree of separability between safety alignment signals and pretrained knowledge during learning? Can the authors explain this discrepancy from the perspective of optimization dynamics?\n\n3. How can a more principled training framework be designed to mitigate such superficial alignment? For instance, is continued pretraining a viable solution?"}, "questions": {"value": "In weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5jyADtllZk", "forum": "7cecAmjinr", "replyto": "7cecAmjinr", "signatures": ["ICLR.cc/2026/Conference/Submission21055/Reviewer_frC3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21055/Reviewer_frC3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756445166, "cdate": 1761756445166, "tmdate": 1762940628158, "mdate": 1762940628158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the robustness of safety alignment in Large Language Models (LLMs). It finds that current alignment methods are fragile because they are coupled to specific \"token forms\". The authors introduce \"token-form drift\", which refers to semantics-preserving changes in input form, such as symbol substitution. The core finding is that this drift systematically degrades refusal behavior, even when the model understands the harmful semantic intent. The paper empirically demonstrates a universal \"rise-plateau-collapse\" failure pattern. As perturbation increases, refusal fails (Rise), harmful compliance peaks (Plateau), and extreme drift leads to semantic incoherence rather than recovered safety (Collapse). This reveals a \"capability-vulnerability tradeoff\": more capable models resist simple drift longer, but their ability to understand complex perturbations creates a wider attack surface. The main contribution is showing that current alignment is only \"token-level\" and form-sensitive, motivating future defenses that target \"form-invariant\" semantic alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The originality of this paper lies in its novel problem formulation. It defines \"token-form drift\" to explain alignment fragility. This concept effectively separates the model's semantic understanding from its form-sensitive refusal behavior. This provides a new and useful lens for analyzing why current safety methods fail, moving beyond just finding new attacks.\n\nThe quality of the work is high. The authors experimentally prove their claims through various and extensive experiments. They use controlled, progressive perturbations to systematically test robustness. Furthermore, they develop an LLM-in-the-loop automated framework to show that diverse, effective perturbations can be discovered automatically. This comprehensive empirical validation strongly supports the paper's central hypothesis.\n\nThe paper is written with high clarity. The authors provide detailed interpretations for their experimental results. Complex findings, such as the universal \"rise-plateau-collapse\" pattern and the \"capability-vulnerability tradeoff\", are explained in a clear and understandable manner. This detailed analysis makes the paper's core arguments easy to follow.\n\nThe paper's significance is high. It demonstrates a fundamental limitation of current token-level alignment pipelines. The work provides valuable insight to researchers by suggesting what additional processes are necessary for future LLM alignment. By highlighting the need for \"form-invariant\" alignment, normalization, and cross-form evaluation, it directs the field toward developing more robust safety defenses."}, "weaknesses": {"value": "The paper provides strong experimental proof for its insights. However, the core idea that alignment is sensitive to token-level shifts is an observation that many practitioners involved in training LLMs may already be familiar with. While the empirical validation is thorough, the work positions itself more as an analysis or explanatory paper rather than presenting a new technique. Given that ICLR typically emphasizes technical novelty, this work's contribution might be a better fit for a workshop or a review-style journal that values systematic analysis and problem formulation.\n\nAdditionally, the paper's key finding—that larger models are \"paradoxically more vulnerable\"—could be debated. The claim is partially correct, but the longer plateau of vulnerability in larger models stems from their superior capability to maintain semantic coherence under wider perturbations. In contrast, smaller models collapse faster. This rapid collapse of smaller models does not necessarily make them more robust or safe; it just indicates they are less capable of interpreting the input. Therefore, framing the larger models as \"more vulnerable\" might be an overstatement, as this vulnerability is a direct, if unintended, consequence of their higher capability."}, "questions": {"value": "The paper is very clear, and the Appendix is comprehensive. I had no difficulty understanding the methodology or the results. My only point of discussion relates to the technical novelty, which was mentioned in my main review. The work provides an excellent analysis and formulation of the \"token-form drift\" problem. However, the core finding might feel familiar to practitioners. Could the authors please elaborate on what they consider the primary technical contribution, beyond the valuable problem formulation and experimental analysis? For instance, does the automated framework itself represent a novel technical method that could be generalized, or is the main contribution the analysis it enables? A response on this point could help clarify the paper's positioning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4uxbuU83Fm", "forum": "7cecAmjinr", "replyto": "7cecAmjinr", "signatures": ["ICLR.cc/2026/Conference/Submission21055/Reviewer_LqcY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21055/Reviewer_LqcY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891096842, "cdate": 1761891096842, "tmdate": 1762940627663, "mdate": 1762940627663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies form sensitivity in LLM safety: refusals learned via alignment degrade under semantics-preserving token-form drift, which includes orthography, separators, segmentation etc. It reports that refusals degrade as drift increases. It uses an LLM-in-the-loop search to find readable adversarial forms and a patch-then-break experiment showing SFT on one form doesn’t transfer. This work motivates “form-invariant” defenses and future alignment methods to use cross-form evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper isolates a concrete vulnerability in current alignment of semantics-preserving token-form drift, and shows that refusal behavior degrades from this vulnerability in ways not covered by most standard safety evaluations.\n\n2. The discovery process is automated and operates in a black-box setting, which potentially allows it to generate a broader set of human-interpretable adversarial attacks that could benefit future safety research.\n\n3. The patch-then-break experiment is an operationally useful finding: fine-tuning to eliminate a single successful form does not generalize, and closely related variants re-emerge quickly, signaling limited value when alignment fails to generalize. This also highlights that most issues arise from OOD data, which the proposed method is particularly effective at producing.\n\n4. The observed “rise–plateau–collapse” trend is interesting and shows a Pareto frontier between adversarial strength and semantic preservation. This offers a useful concept for designing future robustness or attack evaluations."}, "weaknesses": {"value": "1. The attack novelty is limited compared to prior LLM-in-the-loop jailbreak and fuzzing methods such as AutoDAN. Although the paper focuses on one particular adversarial form (token-form drift), the overall pipeline is conceptually very similar to existing LLM-in-the-loop, mutation-based prompt optimization methods.\n\n2. The reported “rise–plateau–collapse” curve is not particularly surprising. Prior work on cipher-based jailbreaks and encoding attacks has already shown that moderate perturbation can bypass safety (the rise and plateau phases), while extreme distortion eventually reduces success. The results largely demonstrate this known pattern rather than uncovering a new mechanism.\n\n3. The paper attributes this region to the model’s inability to understand the input but does not provide direct evidence. It would be valuable to investigate this more deeply, for example by analyzing LLM outputs or applying interpretability methods. The model for sure understand part of the input tokens, and it would be particularly interesting to test whether refusals are triggered when all understood tokens remain in-distribution or detects adversarial intent. Gradient-based jailbreaks such as GCG also seem to contradict the assumption that nonsensical inputs necessarily yield low ASR; exploring this relationship would clarify the collapse phenomenon.\n\n4. The perturbation seeds appear to be selected from a manually designed set. It would be helpful to quantify how much of the ASR comes from these initial seeds vs. later evolved forms, and to include ASR statistics for each mutation step to better understand this."}, "questions": {"value": "1. Many of the results assume a consistent notion of “semantic preservation.” Given that the judge is also an LLM, how do you ensure the validator’s understanding of meaning aligns with the model being attacked?\n\n2. Can the authors comment on whether the drift ladder’s behavior depends on tokenization granularity (e.g., BPE vs unigram models)?\n\n3. Have you tested these jailbreaks against more aligned models? Since many alignment vulnerabilities are known to stem from shallow and less generalizable alignment methods like SFT, it would be valuable to evaluate whether token-form drift remains effective against models trained with stronger alignment methods."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "Includes jailbreaks"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Gna5PZyfhC", "forum": "7cecAmjinr", "replyto": "7cecAmjinr", "signatures": ["ICLR.cc/2026/Conference/Submission21055/Reviewer_6DBz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21055/Reviewer_6DBz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982156100, "cdate": 1761982156100, "tmdate": 1762940626952, "mdate": 1762940626952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}