{"id": "gvBASOB0F8", "number": 16752, "cdate": 1758268309820, "mdate": 1759897221367, "content": {"title": "SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations", "abstract": "Recent advances on 6D object-pose estimation have achieved high performance on representative benchmarks such as LM-O, YCB-V, and T-Less. However, these datasets were captured under fixed illumination and camera settings, leaving the impact of real-world variations in illumination, exposure, gain or depth-sensor mode—and the potential of test-time sensor control to mitigate such variations—largely unexplored. To bridge this gap, we introduce SenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures, 9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels. For five common household objects (spray, pringles, tincase, sandwich, and mouse), we acquire 166.4k RGB and 16.7k depth images, which can provide 1,380 unique sensor-lighting permutations per object pose. Experiments with state-of-the-art models on our dataset demonstrate that applying multimodal sensor control at test time yields substantial performance gains, achieving a 19.5 pp improvement on pretrained generalizable models. It also enhances robustness precisely where those models tend to fail. Moreover, even instance-level pose estimators, where train and test set share identical object and background, performance still varies under environmental and sensor change, demonstrating that test-time sensor control remains effective compared to costly expansions in the quantity and diversity of real-world training data, without any additional training. SenseShift6D extends the object pose evaluation paradigm from data-centered to sensor-aware robustness, laying a foundation for adaptive, self-tuning perception systems capable of operating robustly in uncertain real-world environments.", "tldr": "", "keywords": ["6D pose estimation", "RGB-D benchmark", "Adaptive Sensor Control"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6095526fdbd0e53428c0b64f04538019fd6ac7a.pdf", "supplementary_material": "/attachment/a26e75251e6b2147144f583a225742e66a3b13b9.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces SenseShift6D, a new real-world RGB-D benchmarking dataset for 6D pose estimation that emphasizes variation in sensor and environmental settings (exposure, gain, illumination, and depth). The dataset features 5 objects, 166.4k RGB frames, 16.7k depth frames, and a large number of sensor/lighting permutations (reported as 1,380 unique sensor–lighting permutations per object pose) and is evaluated with both pretrained, generalizable pose estimators and instance-level models. The experiments analyze the effect of autoexposure and sensor configurations and show that multimodal (RGB+depth) test-time sensor adaptation can improve performance; the authors demonstrate that an optimal test-time sensor adaptation yields an AUC improvement of at least +15 over baseline auto-exposure settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-\tIntroduction of an extensive 6D pose dataset that incorporates diverse illuminations, exposure, gain and depth levels. This enables benchmarking the robustness pose estimation models in challenging real-world conditions.\n-\tComprehensive evaluation on different sensor configurations with pretrained models as well as instance-level pose estimation models.\n-\tThe AUC with optimal sensor control is at least +15 points higher compared to the baseline auto-exposure sensor configurations, indicating the potential of test-time sensor control (Table 4)."}, "weaknesses": {"value": "-\tThere is no validation split, instead there is only a training and test split, which might lead to potential overfitting of the evaluated models.\n-\tThe motivation is sensor-aware test-time adaptation, but the paper only shows the Oracle upper bound and not a single practical adaptation method.\n-\tLimited number of objects in the dataset (5 objects) may lead to non-generalizable conclusions."}, "questions": {"value": "-\tHow do you concretely obtain the Oracle selection in the evaluation? Wouldn’t there be a validation split necessary for such a procedure?\n-\tIn Table 5, training on Train-Var does not result in a performance gain on Test-Var for some object categories. Do you have any insights on that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XGYtiyf3XF", "forum": "gvBASOB0F8", "replyto": "gvBASOB0F8", "signatures": ["ICLR.cc/2026/Conference/Submission16752/Reviewer_bar8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16752/Reviewer_bar8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748626392, "cdate": 1761748626392, "tmdate": 1762926797376, "mdate": 1762926797376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SenseShift6D, a new benchmark including RGB and Depth images for 6D pose estimation. The benchmark encompasses a range of image capture setups, including RGB exposures, RGB gains, as well as depth capture modes and illumination levels for five household objects. The benchmark expands the existing 6D pose estimation dataset by incorporating additional environmental and sensor-related variations. The evaluation of a popular estimation algorithm on the benchmark shows the potential of the SenseShift6D and identifies the new challenges in current 6D pose estimation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is well written and structured.\n2.\tThe paper is well-motivated and contributes meaningfully to the 6D pose estimation research.\n3.\tThe extensive evaluation of existing algorithms on the benchmark presents many valuable insights."}, "weaknesses": {"value": "1.\tThe collected dataset has the ChArUco board presented in the images, also for the other well-known dataset. The question is whether the ChArUco board could potentially introduce bias to the pose estimation algorithms if the estimation algorithm is trained on a dataset that includes the ChArUco board. See more detailed discussion in [1]. I suggest removing the calibrator after calibration is done, as in the YCB dataset.\n2.\tThe number of objects and the background of the objects are limited. The multi-object with a more complex background and a more cluttered environment is missing, as discussed in the limitations.\n3.\tAlthough the evaluation on the test dataset with sensor configuration adaptation shows the potential of improving the detection performance,  the current manual tuning procedure is not applicable in the real application, as the correct prediction is unknown. From this perspective, it is not entirely clear that adaptive sensor control on demand is a viable practical approach to addressing the challenges of environmental factor shifts.\n\n[1] Uncovering the Background-Induced bias in RGB based 6-DoF Object Pose Estimation, https://arxiv.org/abs/2304.08230"}, "questions": {"value": "Could you please discuss the potential methods of online automated sensor control and the potential problems (limitations) associated with it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a5v9fTrSLg", "forum": "gvBASOB0F8", "replyto": "gvBASOB0F8", "signatures": ["ICLR.cc/2026/Conference/Submission16752/Reviewer_rQYE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16752/Reviewer_rQYE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967723976, "cdate": 1761967723976, "tmdate": 1762926796982, "mdate": 1762926796982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new benchmark dataset for 6D object pose estimation named SenseShift6D. It contains data with various sensor parameters including RGB exposure, gain and depth capture modes, as well as various environmental conditions such as illumination levels. The dataset contains 166.4k RGB and 16.7k depth images of 5 household objects. The authors found that test-time sensor control by dynamically selecting optimal sensor parameters at inference can improves object pose estimation accuracy."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The idea of incorporating several camera parameter variations in object pose dataset collection is nice.\n- This benchmark explores RGBD sensor parameter variation (especially photometric parameters) for 6D pose estimation. This is an interesting and important aspect of the object pose estimation problem.\n- The benchmark captures real camera effect under different parameter variation. This effect is not easily re-produced in synthetic data. This will be valuable for studying the problem.\n- The dataset has a clear and well-defined split: Train-Def, Train-Var, Test-Def, and Test-Var.\n- It provides benchmarking of several recent methods."}, "weaknesses": {"value": "- The dataset is not marker-less. There are a lot of AR tags on the board beneath the object. It is OK since other datasets such as LineMOD also did this. But this dataset would still be far away from \"data in the wild\".\n- This dataset only contains 5 objects which is quite small in number compared to other related datasets.\n- It only contains single-object tabletop scenes without occlusion or multi-objects scenes.\n- Not sure if all RGB and depth sensors can control their modes to allow varying camera parameters. For RGB and depth cameras who parameters are not easily changeable at test time, the contribution of this dataset would be significantly reduced."}, "questions": {"value": "- The RGB and depth images does not have the same number of images. Do RGB images and depth images have one-on-one correspondences?\n- Is the Sandwich object rigid or deformable? If deformable, how to ensure its shape stays the same?\n- How to convince users to use this dataset instead of other related datasets such as HOPE and IPD? They contain more objects anyway."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JcOQ2qpPG7", "forum": "gvBASOB0F8", "replyto": "gvBASOB0F8", "signatures": ["ICLR.cc/2026/Conference/Submission16752/Reviewer_KbWb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16752/Reviewer_KbWb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097164936, "cdate": 1762097164936, "tmdate": 1762926796338, "mdate": 1762926796338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}