{"id": "Sm8yTdWjai", "number": 12258, "cdate": 1758206649986, "mdate": 1759897522181, "content": {"title": "RedDebate: Safer Responses Through Multi-Agent Red Teaming Debates", "abstract": "We introduce RedDebate, a novel multi-agent debate framework that provides the foundation for Large Language Models (LLMs) to identify and mitigate their own unsafe behaviors. Existing AI safety approaches often rely on costly human evaluation or isolated single-model assessment, both constrained by scalability and prone to oversight failures. RedDebate employs collaborative argumentation among multiple LLMs across diverse debate scenarios, enabling them to critically evaluate one another’s reasoning and systematically uncover unsafe failure modes through fully automated red-teaming. We further integrate distinct long-term memory modules that preserve safety-relevant insights from debate interactions and leverage them during subsequent inference, facilitating continuous refinement of model behavior. Empirical evaluation on safety benchmarks across a diverse set of models demonstrates that RedDebate substantially reduces unsafe outputs. While debate alone allows LLMs to refine their behavior, the addition of memory modules yields further significant reductions. To the best of our knowledge, RedDebate is the first fully automated framework to unify multi-agent debate and red-teaming to progressively enhance LLM safety without human intervention.", "tldr": "", "keywords": ["AI Safety", "Multi-Agent Debate", "Red-Teaming", "Large Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/138aa16aa74879a46faccdafb96dd6107bb39285.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces RedDebate, a MAS debate framework designed to identify and mitigate unsafe behaviors. RedDebate employs fully automated red-teaming to uncover unsafe patterns and uses long-term memory to preserve these insights, leveraging them in subsequent inference. Experiments demonstrate the effectiveness of RedDebate."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The concept of a fully automated MAS safety enhancement framework is very interesting, and I believe this is a highly practical research direction.\n\n- The authors have conducted very detailed experiments, designing various debate strategies and memory modules. They also performed ablation studies on various hyperparameters to evaluate the effectiveness of RedDebate."}, "weaknesses": {"value": "- The paper lacks a discussion on the additional time overhead introduced by RedDebate. Although this point is mentioned in the limitations section, I believe it is still necessary to measure and present the time and computational resources consumed, as efficiency and cost are critical in many real-world scenarios.\n\n- The paper lacks comparison with stronger baselines. The authors only compare RedDebate with Self-Critique, while potentially overlooking other work with similar objectives, such as [1] and [2].\n\n[1] arxiv.org/abs/2305.14325\n\n[2] arxiv.org/abs/2305.19118"}, "questions": {"value": "- I am curious whether larger-scale commercial models (e.g., gpt, claude) could benefit from this framework. Or would they simply refuse to answer harmful questions and fail to correct other agents' responses, similar to the behavior of gpt-oss as shown in Appendix B?\n\n- What is the intended attack/defense scenario? Is RedDebate meant to be a pre-processing or training stage for a MAS, where its safety is enhanced through many rounds of debate before being deployed on real-world tasks? Or is the debate itself the end task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iLekZ3PQBd", "forum": "Sm8yTdWjai", "replyto": "Sm8yTdWjai", "signatures": ["ICLR.cc/2026/Conference/Submission12258/Reviewer_TaUG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12258/Reviewer_TaUG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760640945593, "cdate": 1760640945593, "tmdate": 1762923197848, "mdate": 1762923197848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RedDebate, a multi-agent debate framework for LLM behavioral safety. The authors tested 3 main types of debating mechanisms: peer refinement, devil-angel, and Socratic - Socratic worked best among those. They also tested different mechanisms of long-term memory, which was integrated into multi-agent debate, and showed that incorporating LTM substantially improves behavioral safety. Among the LTM mechanisms, the guardrail approach was overall the most effective."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of applying multi-agent debating is interesting and intuitive. The authors also thoroughly explored different mechanisms of long-term memory to augment debating, which is a novel approach. This paper is clearly written. The appendices provide helpful information that supplement the main text, such as the capability evaluation before vs after safety training and human validation of LlamaGuard."}, "weaknesses": {"value": "I have a few major concerns about the evaluation, which prevent me from fully understanding the significance of the contribution. I'm open to raising my score if these concerns can be addressed during rebuttal.\n\n1. The current selection of benchmarks doesn't enable robust evaluation on the effectiveness of RedDebate. While HarmBench is a widely adopted benchmark in the safety literature, it's relatively small (with a few hundred examples) and potentially overfit by recent models. CoSafe doesn't seem to be an informative benchmark since the baseline error rate is already very low (7-8%) with little room for meaningful improvement. Including benchmarks that are bigger, more recent, and more able to distinguish different models/methods will provide substantially more information about how effective RedDebate is. nvidia/Aegis-AI-Content-Safety-Dataset-2.0 might be a good resource for this.\n\n2. The evaluation in Table 2 seems unfair for the Self-Critique baseline. Self-Critique outperforms SReD without LTM on HarmBench by a large margin - only when LTM mechanisms are included does SReD beat Self-Critique. My interpretation of this is that debating may not be more effective than self-reflection for mitigating safety, and self-reflection is potentially cheaper since it doesn't require multiple models, which undermines the contribution of RedDebate."}, "questions": {"value": "1. Line 298: the term \"agreement rate\" is misleading when the metric quantifies the switch rate from unsafe to safe rather than agreement. Switching from unsafe to safe doesn't necessarily indicate agreement, and agreement doesn't always lead to unsafe to safe switches. Consider using a more accurate metric name.\n\n2. Any insights on why Self-Critique consistently outperforms SReD on HarmBench but underperforms on CoSafe?\n\n3. Table 2: Could you equip Self-Critique with LTM? I wonder if that would outperform SReD + LTM, at least on HarmBench\n\n4. How do different evaluation metrics change over early rounds? Table 7 only shows rounds 3-5, but how about rounds 1-2?\n\n5. What is the inference, computational, and time costs of the various debate methods evaluated in the main text? Is this framework realistic to be deployed at inference time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B9Ts7DNQ5O", "forum": "Sm8yTdWjai", "replyto": "Sm8yTdWjai", "signatures": ["ICLR.cc/2026/Conference/Submission12258/Reviewer_LkJB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12258/Reviewer_LkJB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761516022333, "cdate": 1761516022333, "tmdate": 1762923197376, "mdate": 1762923197376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RedDebate, a novel framework designed to enhance LLM safety by automating the red-teaming process. Addressing the scalability limitations of human evaluation and the inherent blind spots of single-agent self-correction, the authors propose a multi-agent system where LLM agents collaboratively debate adversarial or unsafe prompts. This structured argumentation allows agents to critically evaluate one another's reasoning, systematically uncover unsafe failure modes, and iteratively refine their own responses. The paper's primary contributions are the fully automated framework itself, which unifies multi-agent debate with red-teaming; the exploration of different debate strategies (such as Socratic and Devil-Angel) to effectively elicit and correct unsafe behavior; and the integration of distinct long-term memory modules (including textual, parametric, and guardrail-based) that enable agents to learn persistently from previously identified failures. Empirical evaluations on safety benchmarks demonstrate that the RedDebate framework significantly reduces unsafe outputs without human intervention, with the guardrail-based memory approach yielding the most substantial safety improvements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses the well-motivated and critical problem of LLM safety, tackling the clear scalability and reliability limitations of existing human-led or single-agent evaluation methods.\n\n2. The manuscript is well-written, clearly articulating the proposed \"RedDebate\" framework, the experimental setup, and the subsequent analysis of the results.\n\n3. The work provides a great, novel perspective on AI safety by framing it as a learning problem solved through multi-agent interaction and, most notably, by integrating different long-term memory modules (textual, parametric, and guardrail-based) to ensure persistent safety improvements."}, "weaknesses": {"value": "1. The technical novelty of the framework is somewhat limited, as it primarily integrates and applies existing concepts (multi-agent systems, red-teaming, and memory) rather than introducing entirely new techniques.\n\n2. The paper lacks sufficient baseline comparisons. While it includes a \"Self-Critique\" baseline, it would be strengthened by comparisons against other contemporary automated red-teaming or multi-agent debate frameworks.\n\n3. The evaluation is limited in its scope, focusing on a specific set of smaller-scale open-source models and two standard safety datasets. The findings' generalizability to larger, state-of-the-art models remains unclear.\n\n4. While some ablation studies are present (primarily in the appendix), the paper lacks a comprehensive ablation on the different components to clearly isolate their individual impact (e.g., the precise contribution of specific debate strategies versus the long-term memory)."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8gS9Lhy7H2", "forum": "Sm8yTdWjai", "replyto": "Sm8yTdWjai", "signatures": ["ICLR.cc/2026/Conference/Submission12258/Reviewer_bAEh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12258/Reviewer_bAEh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761610158223, "cdate": 1761610158223, "tmdate": 1762923196818, "mdate": 1762923196818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes RedDebate, the first fully automated framework integrating **Multi-Agent Debate** with **Red Teaming** to enhance LLM safety without human intervention. Through collaborative debate among model agents, it systematically exposes and corrects potentially unsafe behaviors, significantly outperforming traditional single-agent self-critique or manual red teaming approaches."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. First to propose and systematically implement the combination of \"multi-agent debate + automated red teaming\" for LLM safety alignment.\n2. Creative introduction of a **long- and short-term memory mechanism** to continuously accumulate safety experience, with experiments conducted on multiple memory variants.\n3. Compares multiple debate strategies and validates the corresponding effectiveness on standard safety benchmarks such as **HarmBench** and **CoSafe**."}, "weaknesses": {"value": "1. Multi-agent debate and memory updates significantly increase inference cost (e.g., debate agents generate 1.3× more tokens per round than Self-Critique). While the authors argue that safety gains justify the cost, the approach may not be applicable in resource-constrained scenarios.\n2. Although the primary focus of this work is on improving safety, the experimental design might allow an overly \"safe\" agent to dominate the debate, potentially leading the entire debate process toward overly cautious conclusions. *Note: This is just a concern, not a confirmed issue.*\n\n**Minor Comments**\nOverall visual presentation (figures, diagrams) could be further improved."}, "questions": {"value": "1. Has the framework been tested with agents of differing safety tendencies (e.g., pairing high-risk models with conservative models) to verify robustness in heterogeneous model settings?\n2. Have you considered possible approaches to reduce computation and deployment costs?\n3. How is safety ensured during the debate process? For example, a high-risk model may reveal harmful details during discussion — how is this controlled or prevented?\n\nI'm willing to increase my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zp7AsgopE0", "forum": "Sm8yTdWjai", "replyto": "Sm8yTdWjai", "signatures": ["ICLR.cc/2026/Conference/Submission12258/Reviewer_vVnY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12258/Reviewer_vVnY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12258/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097976266, "cdate": 1762097976266, "tmdate": 1762923196434, "mdate": 1762923196434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}