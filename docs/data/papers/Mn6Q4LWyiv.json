{"id": "Mn6Q4LWyiv", "number": 1636, "cdate": 1756899855466, "mdate": 1763518687730, "content": {"title": "Human-like Supramodal Concept Learning Boosts Emotion Recognition", "abstract": "Multimodal emotion recognition has shown promise but is often hindered by the complexity of integrating heterogeneous sensory inputs. Intriguingly, the human brain addresses this challenge through abstract, modality-independent emotion schemas, known as supramodal emotion concepts, which are learned gradually from emotional experiences across different sensory modalities. Here, we propose a learning strategy to construct supramodal emotion concepts across vision, text, and audio. Each modality’s data repeatedly passes through a shared emotion encoder and its corresponding modality-specific non-emotion encoder in a decoupling framework, extracting modality-independent emotion representations. Inspired by hippocampal replay in humans, these representations are aggregated from a memory pool during downstream emotion recognition to form supramodal emotion concepts. We demonstrate the effectiveness of this approach in multiple settings:(1) a lightweight image-based model achieves state-of-the-art results on several benchmark datasets with lower complexity than existing unimodal methods; (2) unimodal models using vision, text, or audio from video clips achieve performance comparable to multimodal models; and (3) concept-guided multimodal models further improve performance, surpassing current state-of-the-art.", "tldr": "AI application inspired by neuroscience knowledge", "keywords": ["Supramodal emotion concept; Human behavior; Replay strategy"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1b9034148ecf42e20fb950d7077be5605375e6cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper attempts to address challenges in multimodal emotion recognition by simulating the human brain's mechanisms of supramodal concept learning and hippocampal replay. The authors propose a two-stage training framework to learn modality-independent emotion representations, which are then used to construct so-called supramodal concepts to guide downstream tasks. While the experimental results are impressive, I have several major reservations regarding the paper's core motivation, the soundness of its methodology, and the rigor of its experimental design."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method achieves competitive performance with a lightweight model, demonstrating excellent efficiency.\n\n2. The methodological design relaxes the strict requirement for paired multimodal data, offering good practical flexibility."}, "weaknesses": {"value": "1. The paper grounds its core motivation in human-like learning and hippocampal replay.  However, the implementation of these complex neuroscience concepts appears overly simplistic and superficial.  \nFor instance, hippocampal replay is operationalized as retaining the top 20% correctly predicted samples per class and then averaging them to form a concept. This is, in essence, a method for constructing a selective prototype or centroid.  \nWhile potentially effective, forcibly linking this to the complex biological process of hippocampal replay seems to overstate the novelty of the mechanism.  A reviewer would expect a deeper, more mechanistic simulation rather than a mere terminological borrowing.\n\n2. In the critical first stage of multimodal joint learning, to align image and audio (7 classes) with text (3 classes), the authors merge multiple distinct negative emotions (fear, disgust, sadness, and anger) into a single negative category.  This constitutes a severe loss of information. It means that the foundation for learning \"modality-independent emotion representations\" is built upon a coarse-grained label space that has lost significant nuance. This raises serious doubts about the quality and validity of the fine-grained \"concepts\" that are subsequently learned.  \n\n3. A fairer and more insightful comparison would be to benchmark their unimodal model against other state-of-the-art unimodal emotion recognition methods on the same datasets."}, "questions": {"value": "1. Beyond a surface-level analogy, how does averaging high-confidence features substantively model the complex neural mechanism of hippocampal replay?\n\n2. What is the theoretical motivation for the proposed two-stage training pipeline, which seems overly complex? \n\n3. Why can't alignment and learning be achieved simultaneously via sequential learning to simplify the framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EEAeRtXiN5", "forum": "Mn6Q4LWyiv", "replyto": "Mn6Q4LWyiv", "signatures": ["ICLR.cc/2026/Conference/Submission1636/Reviewer_p1Zm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1636/Reviewer_p1Zm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760455946200, "cdate": 1760455946200, "tmdate": 1762915838114, "mdate": 1762915838114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Dpo3SV4Wn1", "forum": "Mn6Q4LWyiv", "replyto": "Mn6Q4LWyiv", "signatures": ["ICLR.cc/2026/Conference/Submission1636/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1636/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763518686449, "cdate": 1763518686449, "tmdate": 1763518686449, "mdate": 1763518686449, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multimodal emotion recognition method that combines existing techniques, such as modality-specific and modality-shared representations, loss of orthogonality, alignment of equation 3, etc."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper proposes a learning strategy to construct supramodal emotion concepts across vision, text, and audio. \n2. This paper uses the CLIP encoders to extract the multimodal features."}, "weaknesses": {"value": "1. The novelty and contributions of this paper are too limited and marginal. In fact, this work is essentially almost a direct use of the existing methods. For example, modality-specific encoders, modality-shared encoders, and orthogonal loss are commonly used in existing works such as MISA[1], FDMER[2]. In particular, the alignment loss of Eq.3 is directly used of DMD[3]. \n\n[1] Misa: Modality-invariant and-specific representations for multimodal sentiment analysis.\n[2] Disentangled representation learning for multimodal emotion recognition.\n[3] Decoupled Multimodal Distilling for Emotion Recognition.\n\n2. Comparative experiments are unfair. For example, the text feature of this work is from CLIP, but the text feature of other compared methods is from GloVe.Therefore, the experimental results have no credibility at all. The author should maintain the same experimental setup.\n3. The paper heavily relies on neuroscientific terminology (vmPFC, hippocampal replay) to justify its approach, but the connection is tenuous and superficial. The model's \"replay\" mechanism is a gross oversimplification of a complex neural process. There is no evidence that sampling from high-confidence features mimics the selective, consolidative, and generative nature of hippocampal replay. The claim that the model's shared encoder mimics the vmPFC is an unsubstantiated analogy. The work does not provide any analysis (e.g., neural alignment) to support this claim beyond a simple reference to the brain region's function."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VtUvCw9WkB", "forum": "Mn6Q4LWyiv", "replyto": "Mn6Q4LWyiv", "signatures": ["ICLR.cc/2026/Conference/Submission1636/Reviewer_1wiN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1636/Reviewer_1wiN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761366218531, "cdate": 1761366218531, "tmdate": 1762915837939, "mdate": 1762915837939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a human-like supramodal concept learning framework to enhance emotion recognition. Inspired by the human hippocampal replay mechanism and vmPFC supramodal encoding, it uses a decoupling framework of shared emotion encoder and modality-specific non-emotion encoder. Through two-stage training (multimodal joint and sequential cross-modal), it extracts modality-independent representations, constructs supramodal concepts to guide downstream models. Experiments verify its effectiveness on multiple datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. It closely integrates the human brain's supramodal emotion cognitive mechanism, simulates the selectivity and generalization of hippocampal replay to construct concepts, and adopts a Transformer architecture that conforms to the neural response of vmPFC, making the model design more in line with human emotional cognitive logic and enhancing the rationality and innovation of the method.\n\n2. It covers multiple types of datasets (unimodal/multimodal, real/cartoon faces), conducts benchmark comparisons, ablation experiments and visualization verification, comprehensively verifying the framework's advantages in performance, robustness and generalization."}, "weaknesses": {"value": "1. Although the paper proposes a brain-inspired supramodal concept learning framework, the technical novelty of the method itself is limited. The idea of extracting modality-independent emotional features is similar to that of MISA and FEDER, while the multimodal joint learning based on supervised contrastive loss resembles ConFEDE. Moreover, several existing emotion studies also address noisy labels by constructing soft-label constraints for consistency regularization, which is similar to this method.\n\n2. Some references cited in the Introduction seem inaccurate or do not support the described claims. In particular, the references in the third paragraph of the Introduction fail to substantiate the discussion. Additionally, the statement “Transformer-based models outperform CNNs in capturing neural response patterns in mid-to-high-level brain regions, including the vmPFC (Caucheteux et al., 2023)” is questionable and should be carefully verified.\n\n3. In the replay-inspired supramodal concept construction stage, the process of selecting the top 20% correctly predicted samples to build a high-confidence feature pool lacks sufficient explanation. It remains unclear how the confidence is measure, whether it is based on prediction logits, similarity scores, or another metric. And how the k high-confidence features are sampled from each modality pool.\n\n4. Since the text modality uses only three sentiment labels (positive, negative, neutral), it is unclear whether the supramodal concept space is also three-dimensional. If so, the negative concept would merge emotions such as fear, disgust, sadness, and anger, which are semantically distinct. This coarse-grained abstraction may hinder fine-grained emotion recognition. It should clarify how supramodal concept evaluation benefits the final classification performance under this condition.\n\n5. The baseline methods used for comparison are relatively outdated. Incorporating stronger and more recent multimodal or contrastive learning baselines would better demonstrate the competitiveness of the proposed approach.\n\n6. The performance improvements reported are relatively modest, despite the use of large pretrained encoders (e.g., CLIP, CLAP) and multiple datasets for supramodal concept learning. It is thus unclear whether the gains stem from the proposed framework itself or from the advantages of pretrained models and data diversity."}, "questions": {"value": "1. The current figure only illustrates the framework of supramodal concept learning. A figure that presents the entire pipeline, covering both the training and testing stage, would significantly improve clarity and reader comprehension.\n\n2. Tables 3 and 4 lack explanations regarding the datasets used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8Vw6b6TrFK", "forum": "Mn6Q4LWyiv", "replyto": "Mn6Q4LWyiv", "signatures": ["ICLR.cc/2026/Conference/Submission1636/Reviewer_VUe3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1636/Reviewer_VUe3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644544223, "cdate": 1761644544223, "tmdate": 1762915837751, "mdate": 1762915837751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a brain-inspired framework for multimodal emotion recognition, introducing “supramodal emotion concepts” through a replay-based mechanism. The idea of decoupling emotion/non-emotion features and simulating hippocampal replay for concept construction is novel and intuitively appealing. The proposed model is technically sound, and extensive experiments demonstrate consistent performance gains over SOTA methods on multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "+ The paper is well-constructed.\n+ Clear motivation and well-designed two-stage training pipeline.\n+ Lightweight yet high-performing architecture with LoRA fine-tuning."}, "weaknesses": {"value": "- The replay idea works well in experiments, but the paper doesn’t clearly explain why it works. The method is mainly inspired by the hippocampal replay process in the brain, yet there’s no formal or mathematical framework behind it. For example, no optimization objective or information-theoretic view. Right now, the approach feels more like an intuitive, heuristic trick rather than something derived from solid principles. It would be much stronger if the authors could provide a clearer theoretical explanation for how replaying high-confidence samples improves cross-modal transfer.\n\n- The phrase “supramodal emotion concept” is a key idea in the paper, but it’s never really defined in a precise way. It’s hard to tell how this concept differs from the usual “modality-invariant features” learned by contrastive or disentanglement-based models. Without a clear mathematical or algorithmic definition, the contribution feels a bit fuzzy, and it’s difficult to evaluate how general or novel the idea actually is."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DeijIfvJKv", "forum": "Mn6Q4LWyiv", "replyto": "Mn6Q4LWyiv", "signatures": ["ICLR.cc/2026/Conference/Submission1636/Reviewer_d6sL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1636/Reviewer_d6sL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997481542, "cdate": 1761997481542, "tmdate": 1762915837570, "mdate": 1762915837570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}