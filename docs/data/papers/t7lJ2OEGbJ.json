{"id": "t7lJ2OEGbJ", "number": 20161, "cdate": 1758303185563, "mdate": 1759896997851, "content": {"title": "Learning to Evict from Key-Value Cache", "abstract": "The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve only as indirect proxies for a token’s future utility and introduce computational overhead. We reframe KV cache eviction as a reinforcement learning (RL) problem: learning to rank tokens by their predicted usefulness for future decoding. To this end, we introduce KV Policy (KVP), a framework of lightweight per-head RL agents trained on pre-computed generation traces using only key and value vectors. Each agent learns a specialized eviction policy guided by a holistic reward, derived from future utility, that evaluates the quality of the ranking across all cache budgets, requiring no modifications to the underlying LLM or additional inference. Evaluated on the long-context benchmark RULER and the multi-turn dialogue benchmark OASST2-4k, KVP significantly outperforms baselines. Furthermore, zero-shot tests on standard downstream tasks indicate that KVP generalizes well beyond its training distribution. These results demonstrate that learning to predict future token utility is a powerful and scalable paradigm for adaptive KV cache management.", "tldr": "We reframe KV cache management from a heuristic-based problem to a learnable one, aiming to rank cache entries by their predicted future utility", "keywords": ["Large Language Model", "Key-Value Cache Compression", "Natural Language Processing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d70dabcc2fab85d4e385a374420b684ab3eb7b72.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces KV Policy, a reinforcement learning framework for KV cache eviction in LLMs. Instead of using heuristic policies (recency, attention score, etc.), KVP learns to rank tokens by predicted future utility. Each attention head in the LLM is paired with a lightweight RL agent trained offline on pre-computed KV traces using only keys, values, and positional embeddings. The reward function evaluates the ranking quality across all cache budgets without additional inference. Experiments on long-context benchmarks (RULER, OASST2) and zero-shot downstream tasks (BoolQ, ARC-Challenge, MMLU, HellaSwag) show that KVP achieves consistently better accuracy and lower perplexity than heuristic and attention-based baselines such as SnapKV, TOVA, and StreamingLLM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The use of per-head lightweight RL agents is well-motivated and efficiently parallelizable.\n\n2. The authors include careful ablations isolating the contribution of the RL objective and the reward design."}, "weaknesses": {"value": "1. While the paper claims that KVP introduces minimal overhead, there are no quantitative measurements of inference latency or throughput. Reporting actual wall-clock runtimes or speedups relative to heuristic and attention-based baselines would strengthen the empirical evaluation.\n\n2. The paper claims offline efficiency, but training 112 separate agents still requires substantial GPU resources. A clearer cost-benefit analysis would help.\n\n3. Because KVP requires a dedicated offline training phase, comparing it only with training-free heuristic baselines is not entirely fair. It would strengthen the evaluation to include comparisons with other training-based or learned compression methods, such as Gisting Token (https://arxiv.org/abs/2509.15763) or Activation Beacon (https://arxiv.org/abs/2401.03462).\n\n4. Experiments are limited to one base model (Qwen2.5-7B-Chat). It would strengthen the paper to demonstrate that the learned policies generalize to other architectures (e.g., Llama, Mistral).\n\n5. Since the core motivation is to reduce memory and latency, experiments on truly long-context settings (e.g., 10k–100k tokens or more) are necessary. The current benchmarks (RULER and OASST2-4k) do not fully test KVP’s scalability under extreme context lengths, limiting the conclusions about its real-world applicability.\n\n6. The generalizability of KVP remains uncertain. It would be important to assess how sensitive the learned policies are to the choice of offline training data. For example, would a policy trained on conversational datasets transfer effectively to domains such as code generation or complex reasoning tasks?\n\n7. KVP operates offline and applies a fixed learned policy per head. This limits adaptivity during runtime when generation dynamics may differ from training data distributions."}, "questions": {"value": "See the limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RmQluHE1UE", "forum": "t7lJ2OEGbJ", "replyto": "t7lJ2OEGbJ", "signatures": ["ICLR.cc/2026/Conference/Submission20161/Reviewer_DJnn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20161/Reviewer_DJnn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761344202189, "cdate": 1761344202189, "tmdate": 1762933678209, "mdate": 1762933678209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission investigates the problem of KV cache management in LLMs and proposes a method called KVP which adopts Reinforcement Learning to learn a strategy minimizing the future value of tokens to be evicted from the cache. While this is an interesting, novel approach, the submission does not make a compelling case that KVP improves the performance of SOTA methods in a significant way."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The formulation of KV cache eviction as a learning problem is original. The authors prove that, under two reasonable assumptions, the subset selection problem can be reduced to a ranking problem, which can then be formulated as an RL problem.\n\nIt is interesting that the policy requires only the keys, values, and their positions as input and no attention information. It is a strength of KVP that it can be pretained and does not incur any overhead at inference time.\n\nI appreciate the ablation study demonstrating that supervised learning does not work and RL is necessary.\n\nThe paper is written clearly."}, "weaknesses": {"value": "The experiments show that KVP achieves the best accuracy or perplexity on RULER and OASST2 for most cache sizes (fig 2) and competitive accuracy on the downstream tasks BOOLQ and ARC CHALLENGE (fig 3). However, there is typically a tradeoff between accuracy and latency and storage space. Therefore, the authors should also report the latency and storage space of the various tested methods.\n\nThe authors have only performed experiments with a version of Owen, and I would like to see whether their results generalize to another LLM, such as a Llama model.\n\nWhile the learning approach of KVP is interesting and well-described, the main benefit of KVP remains unclear: is it better accuracy, reduced latency, reduced storage space, etc.?"}, "questions": {"value": "What is the latency and storage space of the various tested methods?\n\nAre the perplexity gains reported in figure 2 statistically significant?\n\nWhy do you use different performance metrics for RULER (accuracy) and for OASST2 (perplexity)?\n\nHow does KVP work for another LLM, such as a Llama model?\n\nWhat is the main benefit of KVP: better accuracy, reduced latency, reduced storage space, etc.?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GuLFhZwkKY", "forum": "t7lJ2OEGbJ", "replyto": "t7lJ2OEGbJ", "signatures": ["ICLR.cc/2026/Conference/Submission20161/Reviewer_xyfd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20161/Reviewer_xyfd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448551578, "cdate": 1761448551578, "tmdate": 1762933677486, "mdate": 1762933677486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper recasts KV-cache eviction as a reinforcement-learning (RL) ranking problem and proposes KV Policy (KVP). This is a lightweight policy that has per-head agents that learn to rank tokens by predicted future usefulness using only cache features, improving both efficiency and downstream accuracy. Under some low uniqueness and nestedness assumptions, eviction reduces to learning a single budget-agnostic ranking. KVP instantiates a Plackett–Luce stochastic ranking policy and leverages Gumbel-Sort for parallel, one-shot permutation sampling.  Each head’s agent is a small MLP scoring (key, value, position) and is trained offline on precomputed traces with a global reward equal to the negative cumulative future attention of evicted tokens summed over all budgets; the reward is normalized and optimized with REINFORCE using an RLOO baseline.  At inference time, the learned per-head rankings evict the lowest-ranked entries for any budget using only K/V/position (no queries or attention), requiring no extra LLM calls. The paper shows that KVP outperforms strong heuristics and attention-based baselines on RULER accuracy and OASST2-4k perplexity across most cache sizes. Additionaly, it generalizes competitively in zero-shot tests on BoolQ and ARC."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a principled, budget-agnostic formulation of KV eviction and a practical, lightweight per-head policy that uses only cache-local features. Due to this, the technique is fast, query-free, and easy to deploy. Empirically, it outperforms strong heuristics and attention-aware baselines across cache sizes and tasks, showing robust generalization."}, "weaknesses": {"value": "Training optimizes a proxy based on future attention computed from offline Q/K/V traces. This can misalign with downstream utility and requires precomputing and storing full-sequence Q/K/V (attention matrices omitted only due to size)."}, "questions": {"value": "1. Your reduction to a single budget-agnostic ranking hinges on uniqueness and nestedness. Do you have empirical evidence that generation traces satisfy these, and how sensitive is performance when nesting is violated (for instance, due to head/layer complementarity)?\n\n2. Since KVP is attention-free and trained on offline \"future-attention\" signals, can you quantify alignment with downstream metrics vs. attention-aware policies that exploit query-specific information at prefil?\n\n3. Could you add direct comparisons (same prefill-then-compress protocol and absolute-budget axes) to KeyFormer and MorphKV (Dialogue without Limits), and discuss where their key-centric selection/compression differs from your per-head RL ranking? Also, would their ideas change your conclusion about using a uniform per-head/layer budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oZkwse7wDS", "forum": "t7lJ2OEGbJ", "replyto": "t7lJ2OEGbJ", "signatures": ["ICLR.cc/2026/Conference/Submission20161/Reviewer_6mwy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20161/Reviewer_6mwy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993773983, "cdate": 1761993773983, "tmdate": 1762933675785, "mdate": 1762933675785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose KV Policy (KVP), a reinforcement learning (RL) framework that reframes cache eviction as a learning-to-rank problem. Each attention head in the transformer is assigned a lightweight RL agent trained on precomputed generation traces to predict each token's future utility using only key, value, and positional embeddings. Empirical results on RULER and OASST2-4k show that KVP outperforms other attention-based (e.g., SnapKV, TOVA) and attention-free (e.g., StreamingLLM, KeyDiff) baselines in accuracy and perplexity under the same budgets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. This paper tackles an important problem of KV cache compression by eviction.\n\nS2. This paper proposes to learn to evict the KV states, which is less explore in the area.\n\nS3. The paper is well written and structured."}, "weaknesses": {"value": "W1. Comparisons with recent sparse kv cache retrieval approaches, e.g., IceCache, ArkVale, MagicPig, InfiniGen, should also be included in addition to the kv cache eviction approaches.\n\nW2. More backbone LLMs should be included. Qwen2.5 should be upgraded to Qwen3-8B. At least Llama3.1-8B should be included for the different variety of the models. One of a medium size LLMs, i.e., ~32B, should also be included to demonstrate the scalability of the proposed approach.\n\nW3. More long-context benchmarks, e.g., longbench, should be included. Moreover, long-generation benchmarks, e.g., longgenbench, should also be included in experiments."}, "questions": {"value": "Q1. What is the training cost, i.e., training data size, training time, etc., of the RL-based approach?\n\nQ2. Would the RL-based approach be able to be combined with the heuristics-based approaches?\n\nQ3. Can you show some failure case studies, such that we can better understand the pros and cons of the RL-based approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AVTU9mpeJl", "forum": "t7lJ2OEGbJ", "replyto": "t7lJ2OEGbJ", "signatures": ["ICLR.cc/2026/Conference/Submission20161/Reviewer_TTjf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20161/Reviewer_TTjf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052261374, "cdate": 1762052261374, "tmdate": 1762933670056, "mdate": 1762933670056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}