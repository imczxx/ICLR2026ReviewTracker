{"id": "Pg7V0VGS3p", "number": 8187, "cdate": 1758072933337, "mdate": 1763382600264, "content": {"title": "Target-Driven Policy Optimization for Sequential Counterfactual Outcome Control", "abstract": "Identifying optimal treatment sequences from offline data to guide temporal systems toward target outcomes is a critical challenge with profound implications for fields like personalized medicine. While existing methods are mostly evaluated in offline settings, practical applications demand online, adaptive strategies that can respond in real-time. To address this, we propose \\textbf{G}oal-aware \\textbf{I}ntervention via \\textbf{F}actual-\\textbf{T}arget Training (\\textbf{GIFT}), a novel framework for learning sequential treatment policies from observational data. GIFT learns a goal-conditioned policy by applying variance-controlled importance weights, which serves as our reward-rescaling mechanism, to guide patient trajectories towards a desired target. Theoretically, our algorithm is guaranteed to converge, and we characterize its induced approximation bias by bounding the gap between our solution and the learned policy's true value. Experiments show GIFT significantly outperforms existing methods in creating goal-oriented policies for online deployment.", "tldr": "", "keywords": ["Causal Inference", "Counterfactual Target Achievement", "Sequential Decision Making"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9cf8ec9a025fcc2d4a397230179adff969b2e131.pdf", "supplementary_material": "/attachment/17b0ca600d70f5b2589d7cf601e959d8b38d7976.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of learning sequential intervention policies from offline observational data to guide temporal systems toward achieving desired outcomes, which is particularly relevant in fields such as personalized medicine. The authors introduce GIFT (Goal-conditioned Intervention via Factual-Targeted Training), a framework that frames the problem as a goal-conditioned Markov Decision Process. It combines Hindsight Experience Replay (HER) with a clipped importance-weighted reward rescaling mechanism to tackle issues related to sparse rewards and distributional shifts in offline reinforcement learning. The paper presents a theoretical analysis demonstrating the convergence of a modified Bellman operator and provides empirical results from synthetic and semi-synthetic datasets, indicating that GIFT outperforms several baseline methods in terms of accuracy, generalization, and inference efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Overall, the paper is well organized and addresses a meaningful application. The problem setting presents practical challenges, and integrating HER with offline RL is a complex task. The experiments are thorough and support the effectiveness of the proposed GIFT method."}, "weaknesses": {"value": "-  The core methodological contributions appear incremental. The use of HER for goal relabeling and clipped importance sampling for stabilizing off-policy learning are well-established techniques in the RL literature; the paper largely recombines these ideas without introducing a fundamentally new algorithmic principle. The theoretical results, though correctly derived, follow standard analyses of contraction mappings and bias-variance trade-offs in approximate dynamic programming, offering limited conceptual advancement. Moreover, the claimed novelty of framing counterfactual target achievement as a goal-conditioned MDP is more a reframing of existing dynamic treatment regimes or model-predictive control formulations than a paradigm shift.\n\n- The experiments do not convincingly demonstrate that the observed gains stem from a novel insight. The ablation studies confirm the utility of HER and reward rescaling, but do not isolate a unique contribution beyond their combined effect. Additionally, the paper does not sufficiently differentiate itself from prior work in offline goal-conditioned RL or conservative policy learning (e.g., CQL), and the comparison with CQL is treated as an ablation rather than a serious alternative"}, "questions": {"value": "- How does this work differ from prior work in offline goal-conditioned RL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s2NuVJOcYF", "forum": "Pg7V0VGS3p", "replyto": "Pg7V0VGS3p", "signatures": ["ICLR.cc/2026/Conference/Submission8187/Reviewer_equA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8187/Reviewer_equA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880391292, "cdate": 1761880391292, "tmdate": 1762920146192, "mdate": 1762920146192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GIFT, a goal-conditioned offline reinforcement learning framework for learning adaptive intervention policies from observational data. The task consists in selecting sequential interventions to drive a temporal system toward a desired target outcome, using only offline observational data. The method combines HER to label trajectories with new goals and reward rescaling using clipped importance weights. The method is shown to theoretically converge with bounded bias. Experiments on synthetic tumor growth and semi-synthetic MIMIC datasets are presented."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- the paper is well written and nice to read. It is overall clear and concepts are well explained.\n- the problem setting of sequential counterfactual target achievement is a relevant task, that has also been explored in different fields like control, RL and causality\n- the paper addresses two aspects met in sequential decision making of sparsity of success signals and distributional shift with HER and clipped importance weighting (similarly to offline RL regularization)\n- the paper combines both theoretical results and experimental analysis, showing better performance in generalization and efficiency compared to the selected baselines"}, "weaknesses": {"value": "- since the motivation and potential impact of this work are strongly tied to real-world applications, the paper should more clearly discuss the practical validity of its assumptions. Causal-based assumptions are only briefly mentioned in the main paper but they are very strong assumptions that will not actually be met in most real-world scenarios. \n- the task of target-driven policy optimization is a well known and studied problem in a variety of fields, ranging from treatment planning model-free/model-based RL, and model-predictive control. The main difference I see with this approach is that here the focus is more on the reconstruction of an underlying causal model and then using the supposedly 'true' counterfactual trajectories modeling for finding optimal policy. However, many other approaches focus on similar tasks by optimizing for performance of optimal policy reconstruction. Even though some of the theoretical results might be less strong since no causal model is assumed, the assumptions are also weaker and in real-world settings these will be very competitive baselines. Most of the baselines used in the paper do not learn reactive (closed-loop) policies but are counterfactual-based methods with similar underlying assumptions. Other standard offline RL algorithms beyond CQL, such as vanilla Q-learning, SAC, or DQN trained with the same reward formulation, etc. and different model-based control baselines (e.g., PILCO, PETS, or other MPC-style methods) that explicitly learn system dynamics for similar tasks are not considered.\n- the importance-weighted reward scaling introduces bias that depends on the clipping thresholds: in theorem 4.2 the fixed-point solution deviates from the true value by $\\sup|r(1-\\bar\\rho)|/(1-\\gamma)$. Tuning these clipping bounds $\\epsilon_1,\\epsilon_2$ is seems very critical since too narrow clipping injects bias and too wide risks high variance\n\n\nMinor:\n- \"an novel\" in line 065\n- \"Offline Date Augmentation\" instead of Offline Data Augmentation\n- section Experimentx"}, "questions": {"value": "- can the authors elaborate on the causal assumptions for observational data they use in the paper and when these are realistic?\n- how can you guarantee that the assumption of a bounded function class for the Q approximator holds in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sPs7gWY8Wb", "forum": "Pg7V0VGS3p", "replyto": "Pg7V0VGS3p", "signatures": ["ICLR.cc/2026/Conference/Submission8187/Reviewer_v1xV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8187/Reviewer_v1xV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926530339, "cdate": 1761926530339, "tmdate": 1762920145609, "mdate": 1762920145609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors study how to guide temporal systems toward target outcomes. In particular, the goal-conditional adaptive strategy has been proposed via variance-controlled importance weights. Both theoretical and empirical studies have been performed to showcase the potential of the developed algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow\n2. In this work, both theoretical studies and empirical studies have been provided."}, "weaknesses": {"value": "1. This work is based on the offline RL settings, and the authors also mentioned the challenges in distributional shift. However, how the developed algorithm addresses the problem of distributional shift needs to be justified from a theoretical standpoint. \n2. The regret bound is not provided for evaluating the effectiveness of the learned policy. \n3. The motivation for using the state-encoder and LSTM is not clear. \n4. As the authors are using domain data for experiments, could the authors highlight the benefits of the algorithms in the domain problem?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pBIzw7Vd1s", "forum": "Pg7V0VGS3p", "replyto": "Pg7V0VGS3p", "signatures": ["ICLR.cc/2026/Conference/Submission8187/Reviewer_37Nt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8187/Reviewer_37Nt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936345724, "cdate": 1761936345724, "tmdate": 1762920145095, "mdate": 1762920145095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a goal-conditioned reinforcement learning approach to develop counterfactual intervention policies in clinical settings. This work proposes GIFT, a framework within which intervention paradigms can move into more proactive, online settings rather than traditional counterfactual intervention strategies that plan offline. This proposed approach is shown to be more accurate than existing offline planning approaches in two clinically based synthetic domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly formulates the MDP definition of the problem setting. The literature review of prior offline planning approaches to sequential counterfactual policy development is exhaustive, to my knowledge. This helps outline the presumed importance of the GIFT framework. I found that the methodology presented in Section 4 was reasonable but unfortunately lacked specificity and completeness (more in the next section). I especially appreciated the use of HER and the reward rescaling components. I feel that these help provide clear components by which the proposed GIFT framework rests on. Obviously, there is more to to GIFT than just these but I found that conceptually, they are what helped solidify the overall translation of standard offline planning to a more proactive policy, this is borne out in the ablation analysis provided in Section 5."}, "weaknesses": {"value": "Despite my overall positive impressions of the proposed GIFT framework, I do not feel that the writing of the paper is appropriately detailed or scoped. I'm not entirely sure that the method is properly evaluated against relevant baselines given its construction and training. The paper, while originating in the counterfactual policy optimization line of literature, is more adequately viewed as a purely goal-conditioned RL work. There is not sufficient literature analysis in offline goal-conditioned RL (there has been major work coming out of Ben Eysenbach's lab along these lines the last two years) nor sufficient benchmarking. Granted the evaluations (accuracy of interventional outcome vs. policy return) of these two communities differ immensely but I feel that it's not appropriate to ignore one of them entirely. Because of this major omission, I cannot view the introduction of GIFT as wholly original or significant. \n\nWhat could have changed my perspective of GIFT was a very detailed procedural overview of GIFT. There are a lot of details that are not provided about the step-by-step flow of information provided in a high-level fashion in Figure 2. I felt that the most important details missing were those about how the encoded state and goals came together, how they were used in the policy, etc and then finally what the intended output of the neural network was. I was surprised that the latter set of details were not outlined clearly. My guess is that the network provides both an estimate of the next observation as well as the clinical target measure $y_t$? Most egregious of these omitted details is an information about what the intended clinical measures that were used as targets. For the tumor task, it is pretty self-evident that this would be tumor size. But for the MIMIC task there were no details about what the intended clinical task was. After digging into the data details for the semi-synthetic use case of MIMIC data, my concerns deepened greatly. The patient trajectories are stated to be sampled randomly from the database. This is irresponsible and non-standard practice as there is a large spread of heterogeneous patient presentations with various clinical conditions they are being treated for within the MIMIC database. Even then, there is still zero indication what the designed target variables are. From this, I am very wary of the experimental design and have a hard time trusting the results.\n\nThere are several additional elements of the work that are very low in quality. The usage of $\\tau$ as a temporal horizon (I'm guessing) is never articulated. This makes understanding Figures 3, 4, 6, and corresponding tables difficult to understand. A simple explanation about how predictions over extended horizons increase the difficulty and then clearly identify the notation used to represent this would go a long way. The same concerns arise in the use of $\\gamma$ to identify different levels of confounding in the tumor dataset. First, the use of $\\gamma$ is confusing since this is common used as a discount factor in the RL literature. So this introduces some confusion at the outset. Secondarily, the effect of confounding through the parameter is never described in order to delineate the importance of separate investigations in Figure 3 and on. \n\nThere are a lot of missing details about the datasets used to support the experimental analysis in Section 5. Yes, many of these are included in the appendix but there needs to be some detail to provide appropriate context for the main body of the paper. It's not clear what the \"target trajectory\" corresponds to in these offline settings. The goal relabeling HER procedure is also not sufficiently detailed. Was the goal set to just be the terminal state of the various trajectories? Was there any point where the training data was segmented such that a single trajectory could be bootstrapped using multiple goals at various temporal horizons? \n\nIn short, I think that the paper could be greatly improved with a far more careful re-write, where technical details about the proposed method are elevated and made central to the paper. Doing so while also working to more clearly differentiate the proposed methods from already established GCRL methods would help to strengthen the work. The clinical+interventional use case is well motivated and of high impact."}, "questions": {"value": "Beyond those questions outlined in the above \"Weaknesses\" section, I have the following questions:\n\n- In Section 4.3, the reward rescaling approach is not well motivated or justified. What is the purpose for the clipping mechanism? Why wasn't the interval for clipping parametrically studied? What is the IS ratio used for? Is it to counteract intrinsic biases that arise in the relabeling of the goals? More details about this approach would be appreciated. Also, in the IS ratio, is the behavior policy just the observed dynamics from the offline data? \n\n- What is the RMSE calculated between? Is it averaged over the entirety of the trajectory? Or is it accumulated in other ways? \n\n- Table 1's caption raises some confusion. Is the RMSE reported (and the results through Section 5) computed over both the training and test sets? \n\n- At the end of Section 5.2 a separate experiment using the Tumor dataset is outlined where different intervention policies are used to generate target outcomes differing from the training data. Where are these policies derived from? What does this entail?\n\n- What is the dotted purple line in Figure 4? It is missing from the legend."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fGu29fkdY2", "forum": "Pg7V0VGS3p", "replyto": "Pg7V0VGS3p", "signatures": ["ICLR.cc/2026/Conference/Submission8187/Reviewer_Jjqv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8187/Reviewer_Jjqv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762067145291, "cdate": 1762067145291, "tmdate": 1762920144606, "mdate": 1762920144606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}