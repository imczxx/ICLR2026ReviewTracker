{"id": "T1V8f4SUjw", "number": 13953, "cdate": 1758225779823, "mdate": 1759897400566, "content": {"title": "Neighborhood Learning in Weighted Beeping Networks", "abstract": "Neighborhood Learning (NL) is a fundamental tool in Multiagent Systems (MAS). The task is for each autonomous agent to learn, in parallel, some information (e.g., agent identifier, message, etc.) from every neighboring agent, according to some notion of vicinity. NL thus requires communication among neighboring agents, which is particularly challenging if agents are tiny devices with very limited capabilities (for instance, biological systems) and may interrupt each other.\nIn this work, we study how the speed of learning depends on the system topology. We model the communication environment as a Weighted Beeping Network (WBN). In a WBN, network nodes (one for each agent) communicate by deciding whether to beep or stay silent -- all the beeps are then scaled by weights on the corresponding links, and a threshold function is applied at each idle node to check if they heard a beep or not.\nWe introduce a novel characteristic of a WBN topology, called Maximum Average Influence (MaxAveInf), and we prove almost tight upper and lower bounds on the running time to accomplish NL task by a Multiagent System, as a linear function of that characteristic. Although MaxAveInf is a global characteristic and it could be as large as the number of all agents in some networks, even with small neighborhoods,\nfor networks with small value of MaxAveInf we succeeded to give a provably-efficient nearly-optimal algorithmic solution.", "tldr": "", "keywords": ["Neighborhood Learning", "Multiagent Systems", "Weighted Beeping Networks"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d56157c18f2f915ae6552eea78ba331e314aac39.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present the problem of Neighbourhood Learning on weighted beeping graphs, where a network of agents must learn some information from their neighbours over a weighted graph. In this setting agents can only beep or listen. They define a metric (MaxAveInf) which is an implicit quantifier of the complexity of the learning task, and provide theoretical results on how the complexity of the NL problem is bounded by this metric."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- To the best of my knowledge, in its context, the paper seems like a strong theoretical exercise and is relatively clearly written.\n- The theoretical results seem correct."}, "weaknesses": {"value": "- The paper is missing a lot of definitions, formal concepts and contextualisation of the work. It makes it really hard to judge the contribution of the work.\n- I am not convinced of the relevance of the so called NL problem. The papers cited in lines 34-36 are MARL works where there is no explicit mention of the need for learning some neighbourhood structures (but instead learning policies or strategies that maximise some reward, that is influenced by the policies of other agents).\n- It may be due to a misconception, but I feel like this work would be better framed in the context of a networked systems/distributed control (TCNS, TAC...) venue. I am not convinced of its relevance to the broader learning/game theory/multi-agent systems community (of which I'm part of)."}, "questions": {"value": "- Line 63, authors start their contribution with \"we study Neighbourhood Learning\" but at this point its not clear what NL is, and is also not defined in the following lines. The only attempt at defining it happens in the very first paragraph: \"To solve NL, each agent must, in parallel, receive an information from every neighboring agent, according to some notion of vicinity\", but this does not help to specify the problem. I don't see why message passing over networks is necessarily related to learning, or what is there to learn in this case.\n- Line 68, authors say that an NL problem \"does not have a concise formula for the exact number of rounds\". At this point, a round is not defined, and also the phrase is confusing. What is a concise formula for an exact number of rounds? \n- Line 71: Again, NL is not properly, formally defined, and a solution to an NL problem is not defined either, much less what it means for a solution to be short. What is formally an NL problem, what constitutes a solution to an NL problem, and what does it mean for a solution to be short?\n- Definition 1 sounds very hand-wavy. What does it mean for a node to learn a message and an ID? what is a message? (a sequence of bits?) Is learning = storing a sequence of bits? Also, what does it mean to analyse a sequence? Does this mean that each node is able to decode some information encoded as a binary sequence?\n- Line 215: Is this an algorithm? Also, isn't the running time O(1)? (since it seems like the nodes just sample from a Bernoulli distribution at each time-step?\n- Theorem 1: What does it mean to solve a one beep local broadcast? Wasn't one-beep local broadcast an algorithm that implies sampling a beep with a certain probability? What does a solution mean in this case?\n\nI would have further, related comments and questions, but I think these convey the message. I do not doubt that the paper, polished and presented with adequate formal context would be of value to networked systems communities. But even in a hypothetical optimal form I would find the fit to ICLR to be very tangential."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zaOTzLckrm", "forum": "T1V8f4SUjw", "replyto": "T1V8f4SUjw", "signatures": ["ICLR.cc/2026/Conference/Submission13953/Reviewer_ZVY4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13953/Reviewer_ZVY4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762283811, "cdate": 1761762283811, "tmdate": 1762924458875, "mdate": 1762924458875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and formalizes the Neighborhood Learning (NL) problem within a novel Weighted Beeping Network (WBN) model. The WBN extends the classic Beeping Model by incorporating link weights, which model the varying strength of influence between nodes, and a threshold-based delivery condition that captures the physical reality of signal-to-interference. The paper's primary contribution is a nearly tight characterization of the complexity of NL in this new model. The authors identify a new graph parameter, the Maximum Average Influence (MaxAveInf or W), and prove that it is the fundamental parameter governing the problem's complexity. They provide an algorithm that solves NL in O(W log n · (log n + M)) rounds and show a nearly matching lower bound of Ω(W) for even a simplified one-beep version of the problem. An experimental evaluation on real-world and synthetic datasets is included to support the theoretical findings."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The introduction of the WBN model is a significant and timely contribution. It moves the field beyond the simplistic binary communication of the Beeping Model towards a more realistic one that accounts for weighted interference, bridging a gap to practical wireless models like SINR while retaining the simplicity needed for foundational distributed computing theory.\n\nThe discovery and analysis of the MaxAveInf (W) parameter is the paper's core intellectual contribution. The authors compellingly argue that W, not local parameters like degree, is the correct complexity measure for NL in this setting. Proving both upper and lower bounds linear in W is a strong and elegant result.\n\nThe proposed algorithm is well-structured. The progression from the simple One-beep Local Broadcast (OBLB) to the full NL algorithm is logical. The technique of using a 2log n-round encoding for IDs that implicitly provides collision detection is a key insight that makes the solution work.\n\nThe proofs for both the upper and lower bounds appear sound and rigorous. The lower bound construction, while specific, is non-trivial and effectively establishes the fundamental limit.\n\nThe extensive experiments on diverse datasets demonstrate that the theoretical analysis is not just a worst-case artifact; the algorithm performs well in practice, and the dependence on W is observable."}, "weaknesses": {"value": "The paper's single greatest weakness is its exposition. It fails to articulate a clear, motivating story upfront. The core research questions are buried in technical details. A reader must work hard to understand why the WBN model is needed and what fundamental problem is being solved. A stronger introduction that contrasts with the limitations of existing models and sharply states the paper's contributions is needed.\n\nKnowledge of W: The algorithm's efficiency critically depends on knowing or estimating W. The proposed \"doubling\" method is mentioned but not analyzed, and the experimental estimation is heuristic. A more rigorous treatment of this practical necessity is required.\n\nStatic Weights & Synchrony: The model assumes relatively static weights and synchronous rounds. While extensions are briefly discussed, the core analysis does not address the significant challenges of dynamic environments or full asynchrony, limiting the immediate practical applicability.\n\nEfficiency for Short Messages: The O(log n) overhead for transmitting IDs is significant when the message M is very short (e.g., a single bit). The algorithm may be inefficient for such applications.\n\nGenerality of Lower Bound: The lower bound relies on a specific, adversarial bipartite graph topology. While this is standard for worst-case analysis, the paper does not discuss whether this bound holds or can be improved for more common, well-structured networks (e.g., unit disk graphs, grids), which would be of great interest."}, "questions": {"value": "Could the introduction be reframed to more clearly state the limitations of the standard Beeping Model for modeling interference and then pose the research questions (modeling, algorithmic, complexity) that this paper answers?\n\nThe algorithm requires a parameter p ~ 1/W. In a fully decentralized setting, how can a node realistically estimate or adapt to the global parameter W without central coordination? Is there a local, adaptive strategy?\n\nThe 2log n encoding is elegant but costly for short messages. Have the authors considered alternative methods or is there a lower bound showing this overhead is unavoidable?\n\nThe lower bound uses a specific topological construction. Do the authors believe the Ω(W) bound is tight for common network classes like planar graphs or random geometric graphs, or could a better algorithm exist for these topologies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q3yXualnEi", "forum": "T1V8f4SUjw", "replyto": "T1V8f4SUjw", "signatures": ["ICLR.cc/2026/Conference/Submission13953/Reviewer_VRTD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13953/Reviewer_VRTD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929499635, "cdate": 1761929499635, "tmdate": 1762924458447, "mdate": 1762924458447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper’s connection to the ICLR community is weak. Although it frames the work as concerning Multiagent Systems (MAS), the actual formulation and analysis revolve entirely around a self-defined Weighted Beeping Network (WBN) abstraction. The study focuses on communication dynamics in this artificial model rather than addressing learning or coordination challenges typically investigated in AI or machine learning for multi-agent settings (e.g., reinforcement learning, policy optimization, distributed inference, or emergent communication).\n\nThe defined Neighborhood Learning (NL) problem appears disconnected from established multi-agent learning paradigms. The motivation for introducing the beeping network abstraction and its relation to learning or optimization processes is unclear, while the presented results focus solely on theoretical communication bounds. Furthermore, the references to AI and multi-agent learning literature are superficial, and the work remains primarily a network-theoretic study with limited relevance to the ICLR scope.\n\nIn summary, while the theoretical analysis may be sound, the paper lacks meaningful engagement with the AI or multi-agent learning community, and its self-defined problem does not connect to recognized research directions within ICLR."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The theoretical analysis may be sound;"}, "weaknesses": {"value": "W1. The relevance is low;\n\nW2. The beeping network (self-defined version) is not explained how it works in multi-agent learning systems;"}, "questions": {"value": "See the summary"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ax2PrUVIHW", "forum": "T1V8f4SUjw", "replyto": "T1V8f4SUjw", "signatures": ["ICLR.cc/2026/Conference/Submission13953/Reviewer_Hvtk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13953/Reviewer_Hvtk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958629272, "cdate": 1761958629272, "tmdate": 1762924457451, "mdate": 1762924457451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and formally analyzes the Neighborhood Learning (NL) problem within a novel Weighted Beeping Network (WBN) model. The NL task requires each agent in a multi-agent system to learn the identity and a message from all its \"influential\" neighbors, where influence is determined by a link weight exceeding a threshold. The authors propose a new graph characteristic, Maximum Average Influence, and provide an elegant randomized algorithm that solves NL. They complement this with an almost tight lower bound, proving that their algorithmic solution is nearly optimal. The theoretical analysis is supported by an extensive experimental evaluation on both real-world (social networks, gene regulatory) and synthetic (scale-free, 3D-grid) datasets, demonstrating the algorithm's practical efficiency and superiority over a naive baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper does an excellent job of introducing a fundamental and non-trivial problem (Neighborhood Learning) that is a cornerstone for more complex multi-agent coordination and learning. The motivation, linking it to biological systems (insect colonies, bacteria) and resource-constrained devices, is compelling.\n2. The technical core of the paper is exceptionally strong. The introduction of the MaxAveInf characteristic is insightful, and the subsequent analysis is rigorous. Providing almost matching upper and lower bounds is a significant theoretical contribution that clearly characterizes the problem's complexity.\n3. The WBN model is a meaningful and non-trivial extension of the standard Beeping Model. Incorporating weights and a delivery threshold makes it more realistic for capturing signal strength, interference, and noisy environments, potentially bridging the gap between simple theoretical models and practical wireless or biological communication."}, "weaknesses": {"value": "1. The baseline is a simple periodic algorithm. Are there any other distributed coordination algorithms, even from different models (e.g., standard message-passing), that could be adapted to the WBN setting for a more competitive comparison?\n2. The connection to AI and learning is currently implicit (NL as a primitive for MAS) rather than explicit. The paper would be significantly strengthened for an AI audience by concretely outlining how solving NL enables specific AI tasks."}, "questions": {"value": "1. How can NL be used as a subroutine for decentralized training or inference in Graph Neural Networks deployed on a WBN?\n2. Can this protocol enable efficient collaborative learning or federated learning in a beeping network?\n3. How does it facilitate coordination in Multi-Agent Reinforcement Learning with restricted communication?\n4. In the real-world networks you tested, what was the typical relationship between W and n? Are there common network structures in AI applications (e.g., hierarchical, small-world) where W is provably small, guaranteeing fast performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zj1W7uQ16A", "forum": "T1V8f4SUjw", "replyto": "T1V8f4SUjw", "signatures": ["ICLR.cc/2026/Conference/Submission13953/Reviewer_BgzR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13953/Reviewer_BgzR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995158951, "cdate": 1761995158951, "tmdate": 1762924455830, "mdate": 1762924455830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}