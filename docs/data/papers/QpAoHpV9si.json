{"id": "QpAoHpV9si", "number": 20607, "cdate": 1758308189520, "mdate": 1763677737678, "content": {"title": "Adaptive Generation of Bias-Eliciting Questions for LLMs", "abstract": "Large language models (LLMs) are now widely deployed in user-facing applications, reaching hundreds of millions worldwide. As they become integrated into everyday tasks, growing reliance on their outputs raises significant concerns. In particular, users may unknowingly be exposed to model-inherent biases that systematically disadvantage or stereotype certain groups. However, existing bias benchmarks continue to rely on templated prompts or restrictive multiple-choice questions that are suggestive, simplistic, and fail to capture the complexity of real-world user interactions. In this work, we address this gap by introducing a counterfactual bias evaluation framework that automatically generates realistic, open-ended questions over sensitive attributes such as sex, race, or religion. By iteratively mutating and selecting bias-inducing questions, our approach systematically explores areas where models are most susceptible to biased behavior. Beyond detecting harmful biases, we also capture distinct response dimensions that are increasingly relevant in user interactions, such as asymmetric refusals and explicit acknowledgment of bias.Leveraging our framework, we construct CAB, a human-verified benchmark spanning diverse topics, designed to enable cross-model comparisons. Using \\bench, we analyze a range of LLMs across multiple bias dimensions, revealing nuanced insights into how different models manifest bias. For instance, while GPT-5 outperforms other models, it nonetheless exhibits persistent biases in specific scenarios. These findings underscore the need for continual improvements to ensure fair model behavior.", "tldr": "We introduce a counterfactual bias evaluation framework and a human-verified dataset to more realistically measure and compare how LLMs exhibit biases across sensitive attributes in open-ended interactions.", "keywords": ["LLM", "Bias", "Counterfactual", "Evaluation", "Dataset"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/459f18e2e4fd81f2518478ae7ab04df1e5104d30.pdf", "supplementary_material": "/attachment/ae08d59fa44bb583f5672df0085a8e0703c99538.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework for evaluating bias in LLMs through adaptive generation of counterfactual questions. The core approach employs a genetic algorithm-inspired optimization process where questions are iteratively mutated and selected based on their ability to elicit biased responses from target models. The authors introduce a multi-dimensional evaluation scheme. The authors also construct CAB, a human-verified benchmark. They evaluate various state-of-the-art models on this benchmark, finding that most of the sex-related questions elicit biased responses from at least one model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper attempts to address limitations in existing bias evaluation benchmarks, particularly the reliance on templated prompts and multiple-choice formats that poorly reflect real-world interactions. The multi-dimensional evaluation framework is thoughtful, especially in distinguishing between models that exhibit bias versus those that acknowledge it, which is indeed a common conflation in prior work. The human verification step adds credibility to the final benchmark. The comprehensive evaluation across various frontier models with detailed categorization of bias types provides practical value for understanding current model behaviors."}, "weaknesses": {"value": "My primary concern with this work is that the counterfactual evaluation paradigm is far from novel and has been extensively employed in existing bias evaluation literature (WinoQueer, CrowS-Pairs, WinoBias, BBQ, etc.). The authors fail to adequately justify how their application of counterfactuals constitutes a meaningful technical contribution. Simply generating questions with placeholder attributes like {man/woman} and comparing responses is standard practice in this field. The claim that their approach is innovative because it generates \"realistic, open-ended questions\" rather than templates is undermined by the fact that their questions are still built around the same counterfactual template structure, just with more elaborate contexts around them.\n\nThe genetic algorithm optimization aspect, while interesting, raises significant questions about what is actually being learned. The framework essentially tunes questions to maximize bias in specific target models, but it's unclear whether this produces questions that genuinely reflect real-world bias scenarios or merely adversarial examples that exploit model-specific quirks. The fact that questions are optimized on five models but then evaluated on nine different ones partially addresses this, but the paper doesn't adequately explore whether the learned questions transfer well or simply represent a form of overfitting to the target models' failure modes.\n\nThe reliance on LLM judges for both fitness evaluation during generation and final bias assessment is problematic. While the authors acknowledge this limitation briefly, they don't sufficiently address how errors in the judge's assessment propagate through the entire framework. If the judge misidentifies bias during the optimization process, the genetic algorithm will evolve questions based on incorrect fitness signals. The paper lacks any systematic validation of judge reliability or inter-annotator agreement beyond the final human filtering step, which only verifies that questions are well-formed rather than that the bias scores are accurate.\n\nThe benchmark construction methodology also has concerning aspects. Using only 405 questions total across three attributes seems quite limited for making broad claims about model bias. The human filtering process removed roughly 35% of generated questions, but the paper provides minimal detail about what made these questions unsuitable beyond brief categories. Were they syntactically malformed, or did they fail to actually elicit bias? This distinction matters significantly for understanding whether the generation process works as intended. Furthermore, the heavy filtering suggests the generation process produces substantial amounts of unusable content, raising questions about the practical utility of the approach for scaling to larger benchmarks.\n\nThe experimental design choices are not well justified. Why were these specific five target models chosen for generation rather than others? Why is k=3 samples adequate for determining fitness? The paper sets various hyperparameters (gamma=0.5, specific mutation probabilities) without ablation studies or sensitivity analysis. The implicit question results showing a 40% drop in bias are presented as validation, but could equally suggest the framework is overfitted to explicit phrasings and doesn't generalize to more realistic interaction patterns."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CEToAPFPHQ", "forum": "QpAoHpV9si", "replyto": "QpAoHpV9si", "signatures": ["ICLR.cc/2026/Conference/Submission20607/Reviewer_L7Et"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20607/Reviewer_L7Et"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760608106838, "cdate": 1760608106838, "tmdate": 1762934013194, "mdate": 1762934013194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response [1/2]"}, "comment": {"value": "We thank all reviewers for their constructive feedback. Below, we address common questions labeled GQ1-GQ3, to which we will link in the individual replies for each reviewer. Alongside this rebuttal, we have pushed an updated manuscript with new additions and changes highlighted in blue.\n\n**GQ1 Why did you use an LLM-as-Judge, and to what extent does this create self-referential bias? Are the bias scores consistent across models/humans?**\n\nWe thank the reviewers for raising these questions. Our use of an LLM-as-a-Judge is primarily motivated by the nature of CAB’s evaluation: the judge does not assess a single answer in isolation, but focuses on differences across counterfactual pairs. This comparative evaluation substantially limits the impact of any inherent biases the judge model itself may possess, since it is anchored in relative contrast rather than absolute judgments. GPT-5 Mini was chosen as our primary judge because it offers a strong accuracy-cost tradeoff, which is essential given the large number of intermediate judgments required in the CAB pipeline. Using multiple judges throughout would increase computational cost almost linearly and is not feasible at scale.\n\nTo assess judge robustness, we re-evaluated all judgments on the final CAB dataset using a completely independent frontier model (Claude 4 Sonnet). As shown in Appendix E, the two judges exhibit strong alignment, indicating that CAB’s evaluation is not dependent on any specific model’s internal values. Given the overall scale of CAB, a full human study was here outside the scope of our financial constraints.\n\nInstead, we aimed to ensure that CAB’s final dataset is subject to human verification. Importantly, while our pipeline dynamically explores a wide range of topics to optimize bias-inducing questions for a target model, the current CAB dataset is a *fixed derived artifact consisting only of the prompts*. Notably, these questions consistently elicit measurable biased behavior in current frontier models (Appendix G), a fact that not only justifies their selection but also strongly indicates that our prior evolutionary generation is, in fact, able to consistently find such transferable bias-inducing samples (see GQ2).\n\n**GQ2 Do the questions discovered by CAB overfit to specific model families? How well do questions transfer between model families?**\n\nWe thank the reviewers for highlighting this point. Prompted by this concern, we explicitly tested whether models are disproportionately sensitive to prompts generated by their own family. **We find no such effect.** Across all evaluated frontier models, the fitness-score variance between different generator models is very small (0.02 on average across all models and attributes; sex = 0.025, race = 0.04, religion = 0.013). Further, we also observe that the number of “high bias” answers for a given generator is similar across all evaluated models and not particularly spiked for models in the same family.\n\n\nThis indicates that CAB’s questions do not overfit to generator-model idiosyncrasies. This observation is further supported by models such as Grok-4, Kimi-K2, and DeepSeek V3.1, whose families were never used for generation, actually showing higher bias scores on CAB than counterparts like Gemini-2.5 Pro, or Claude Sonnet 4.\n\nOur choice of generator models was guided by the need to cover a diverse set of smaller but sufficiently capable models to produce realistic, nuanced conversational prompts. Since CAB evaluates all major frontier model families, some overlap between generator families and evaluation families is structurally unavoidable, and excluding entire families would instead mostly reduce overall diversity.\n\nOverall, the transferability results confirm that CAB questions generalize well across model families, consistent with the high question quality observed throughout the benchmark. We have added the full transferability statistics in the newly added Appendix I."}}, "id": "rCsAWgOwhU", "forum": "QpAoHpV9si", "replyto": "QpAoHpV9si", "signatures": ["ICLR.cc/2026/Conference/Submission20607/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20607/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20607/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763676464926, "cdate": 1763676464926, "tmdate": 1763676464926, "mdate": 1763676464926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an automated counterfactual question generation framework to discover realistic prompts that elicit biased responses from LLMs and then uses that framework to build a human-verified benchmark of bias-inducing questions across three sensitive attributes (sex, race, religion). The method uses LLMs in three roles (1) proposes or mutates candidate questions, (2) queries target models multiple times per attribute value, and (3) has an LLM judge score responses on four dimensions (Bias, Relevance, Acknowledgment, Refusal). The paper analyzes many state-of-the-art models on CAB and categorizes the kinds of biased behavior found.\n\nI think that the main contribution of this paper is its strong execution and resulting dataset that can be used by subsequent research and bias detection. The \"LLM-as-data-generator-for-bias-evaluation\" main idea I don't think is that novel."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Practical and scalable approach to bias detection and strong execution:\nThe adaptive generation loop addresses a real gap: templated prompts are brittle and unrealistic. Automating iterative question mutation and selection is an effective way to surface diverse, lifelike prompts that stress models in ways hand-crafted datasets often miss. While some of the ideas are not conceptually new, the method is implemented cleanly and systematically. The workflow is easy to follow, with explicit prompt templates, judge dimensions, and selection criteria, which enhances transparency and reproducibility.\n\nDataset useful to the community: \nThe resulting CAB dataset appears to contain realistic, natural-language prompts rather than artificial templates, addressing a genuine limitation of prior bias benchmarks. If released with appropriate documentation and safety controls, it could become a useful testbed for bias evaluation and mitigation research.\n\nStrong reproducibility and high levels of details: \nThe authors include prompt examples, scoring guidelines, and discuss their data verification process. Even though much of the system relies on LLM components, the procedural clarity allows replication and potential extension.\n\nThe evaluation is comprehensive: \nThe authors apply the framework to three sensitive attributes (gender, race, religion) and multiple strong target models. The breadth of tested systems and the consistent analysis across them make the study empirically solid. The inclusion of explicit and implicit variants of questions is a nice design choice that highlights differences between overt and subtle bias triggers."}, "weaknesses": {"value": "Limited novelty: \nThe core idea, using LLMs to generate probes for bias detection, is not that novel. The proposed adaptive generation loop is a modest extension of existing LLM-driven dataset generation methods, without introducing fundamentally new technical insights or modeling mechanisms. As a result, the paper reads more as a well-executed systemization of known techniques than a conceptual advance.\n\nJudge reliability and \"overfitting of bias\": \nThe evaluation pipeline hinges on a single LLM judge to assess bias, relevance, acknowledgment, and refusal. Without quantitative calibration against human annotation or independent judges, it’s unclear how reliable or stable these automated scores are. This undermines the scientific validity of both the adaptive optimization and the benchmark’s ground truth. Furthermore, when generator and judge are both LLMs (and particularly if the judge and generator share architecture or training data), there is a bias amplification risk. \n\nLimited generalization, scalability, and external validity: \nThe adaptive generation process optimizes questions for a specific target model or a narrow set of models, raising the risk that CAB primarily captures model-specific artifacts rather than general bias phenomena. The paper does not include transfer or robustness analyses to demonstrate that high-fitness questions generalize across models or settings. Furthermore, the benchmark is constrained to three attributes and English, U.S.-centric contexts, limiting cross-cultural and intersectional coverage.\nThe human filtering stage was performed solely by the paper’s authors, without independent annotators or inter-annotator agreement reporting, leaving potential subjectivity and selection bias unquantified. Finally, despite claims of automation, the approach still depends on substantial human review and on well-defined bias categories and cultural priors. These dependencies constrain scalability and hinder the framework’s applicability to real-world, diverse, or multilingual deployments.\n\nInsufficient statistical and interpretive rigor in evaluation: \nThe evaluation reports only average fitness scores without confidence intervals, significance testing, or analysis of variability due to judge randomness or sampling (k=3 responses per attribute value is likely too small). As a result, it is unclear whether observed differences between models or attributes are statistically meaningful. Beyond descriptive comparisons, the paper offers little interpretive depth connecting specific question characteristics to types or mechanisms of bias, limiting the scientific insight drawn from the results."}, "questions": {"value": "Validity of the LLM judge and evaluation reliability: \nHow did you validate the reliability of these judgments against human annotations?\nHave you measured agreement between the LLM judge and multiple human raters, or tested robustness across different judge models or prompt formulations?\nGiven that small prompt or model changes can shift judgments substantially, how stable are your reported fitness scores?\n\nGeneralization and robustness of discovered questions: \nSince questions are optimized on specific target models, to what extent do they transfer to other architectures or checkpoints?\nHave you evaluated cross-model generalization to verify that CAB captures systemic bias rather than model-specific artifacts?\n\nHuman verification and dataset objectivity: \nCan you clarify how many annotators participated, what criteria were used, and whether inter-annotator agreement was measured?\nWould independent or crowdsourced verification yield consistent results?\nWhy is the question drop rate sort of high (like 130 something / 250, nearly half)?\nCould this pipeline realistically operate as a continuous evaluation system for production models, or is it mainly for controlled benchmark creation?\n\nStatistical and interpretive significance: \nHave you performed significance testing, or estimated variance across random seeds and sampling?\nBeyond numeric scores, can you provide qualitative analyses or case studies linking specific prompt features to bias mechanisms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tVfnUF0lUN", "forum": "QpAoHpV9si", "replyto": "QpAoHpV9si", "signatures": ["ICLR.cc/2026/Conference/Submission20607/Reviewer_rSfg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20607/Reviewer_rSfg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760807391649, "cdate": 1760807391649, "tmdate": 1762934012735, "mdate": 1762934012735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an adaptive genetic algorithm that iteratively generates open-ended questions designed to elicit biased responses from large language models. The approach optimizes questions based on a multi-dimensional fitness score that distinguishes between bias exhibition and acknowledgment across four dimensions: bias strength, relevance to the question, explicit acknowledgment of societal biases, and asymmetric refusals. The authors construct CAB, a human-verified benchmark of 405 questions spanning three sensitive attributes (sex, race, religion), and evaluate nine frontier models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Unlike static templated prompts, the approach iteratively refines questions through generate/replace/refine mutations, systematically searching for bias-eliciting scenarios. \n\nThe four-dimension fitness (bias/relevance/acknowledgment/refusal) enables nuanced analysis beyond aggregate metrics.\n\nFull prompts, detailed hyperparameters, and human filtering statistics support reproducibility."}, "weaknesses": {"value": "Questions were optimized against Claude-Haiku-3.5, Gemini-Flash-Lite, LLaMA-4-Maverick, Hermes-3, and GPT-4-Mini, yet the evaluation includes models from the same model families. Models within families may share training data and architectural choices. The genetic algorithm explicitly evolved questions to maximize bias in these specific models, meaning CAB may measure family-specific training artifacts rather than general societal biases. The claim that using slightly different models ensures fairness lacks supporting evidence. No analysis shows questions optimized on GPT-4-Mini generalize equally to Claude/Gemini families. This fundamentally affects the paper's central claim of enabling fair cross-model comparison. Cross-family transfer analysis quantifying whether optimization advantages specific families may strengthen the paper.\n\nAll findings rest on judgments from a single LLM judge (GPT-5-Mini), which is also from the generating model's family, creating a circular system where questions are evolved to trigger responses a specific model will judge as biased. The absence of human validation and insufficient multi-judge reliability analysis means systematic judge errors could propagate throughout the benchmark.\n\nMinor issues:\n1. No evaluation of the same nine models on prior benchmarks exists. Cannot assess whether CAB reveals new biases or rediscovers known ones, or whether model rankings correlate with established benchmarks."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jyKFKXTZie", "forum": "QpAoHpV9si", "replyto": "QpAoHpV9si", "signatures": ["ICLR.cc/2026/Conference/Submission20607/Reviewer_W22R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20607/Reviewer_W22R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964024590, "cdate": 1761964024590, "tmdate": 1762934012087, "mdate": 1762934012087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a counterfactual bias evaluation framework that generates realistic, open-ended question pairs over sensitive attributes such as sex, race and religion. The framework uses LLM-as-a-Judge to evaluate the target model's answers along four dimensions (bias, relevance, acknowledgment and refusal). The scores obtained during this evaluation is formulated as a fitness score to further refine the dataset and build a high quality benchmark dataset. Authors have designed the policies and rubric for the various prompts very meticulously (manually developing categories for various attributes). The final benchmark dataset is human verified (by authors). Authors show experimentally that framing the questions is very important to elicit the biases in models (and show how explicit vs. implicit framing changes the results by a significant margin). Using the dataset, authors have analyzed various frontier models and show various interesting patterns (GPT5 shows low bias compared to other models)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the paper expands on the recent directions and generates more open-ended realistic interactions with counterfactual pairs (this goes beyond templates and MCQ style assessment) \n- focuses on disentangling bias acknowledgement from actual bias in response and tries to identify those; similarly, the fitness scoring mechanism penalizes irrelevant information (focus on relevance) and refusals. This is critical to building the high quality dataset. \n- the adaptive dataset expansion approach is AI classical (like spreading activation networks and search) that seem to work well in this context and produce good dataset which is a strong contribution of this research."}, "weaknesses": {"value": "- this work utilizes a completely automated pipeline of prompt generators and evaluators heavily relying on LLM-Judges which seems like a recursive problem - using LLMs to judge biases in LLMs - although the authors validate the dataset that the system generates, it still requires many human assessments, using intern-annotator agreements to understand various correlations, use of other metrics (like sentiment or other proxy) to also compare and validate the use of LLM judges here. \n- while the fitness score looks reasonable, this work should include more studies to better understand the choice of function used in fitness metric, the stability of results, use of ablation studies, etc. \n- no comparison to other datasets, baselines, benchmarks on the same models; authors discount use of other popular benchmarks like BBQ that are very popular (even though being quite old now), there are generative, multilingual and multimodal extensions of BBQ already in literature, there is no comparative studies. \n- Also, other benchmarks use many more attributes (beyond sex, race and religion); this work is limited in this context."}, "questions": {"value": "- Does the fitness scoring mechanism correlate with human judgments?\n- Any insights on stability of the fitness function (choice of normalization function, sampling size, etc) \n- Do models that score well in this research also score well on other benchmarks like Open-BBQ ( or others)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v8mG5EGZRk", "forum": "QpAoHpV9si", "replyto": "QpAoHpV9si", "signatures": ["ICLR.cc/2026/Conference/Submission20607/Reviewer_KJmU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20607/Reviewer_KJmU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762183336886, "cdate": 1762183336886, "tmdate": 1762934011705, "mdate": 1762934011705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}