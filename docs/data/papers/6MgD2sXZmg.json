{"id": "6MgD2sXZmg", "number": 25395, "cdate": 1758367458817, "mdate": 1759896722479, "content": {"title": "Deep Cognition: A Multi-Agent Framework for Collaborative Research with Real-Time Cognitive Oversight", "abstract": "Despite advances in large language models, current systems for deep research are limited by an asynchronous, \"input-wait-output\" interaction paradigm. This model creates a critical disconnect between human intent and AI execution, leading to error propagation and an inability to dynamically course-correct during complex problem-solving. We propose that a more effective form of human-AI partnership requires a shift from passive command-giving to cognitive oversight, where humans actively guide and intervene in the AI's thinking process. This perspective treats interaction as a core component of intelligence, rather than a peripheral interface.\n\nWe introduce Deep Cognition, a system designed to enable this paradigm through three technical pillars: transparent and interruptible AI reasoning, fine-grained bidirectional dialogue, and a shared cognitive context. At the core of our system is a layered StateManager architecture and a novel multi-stage budget allocation algorithm. This architecture ingests and normalizes all interaction data (e.g., dialogue trajectories and user artifacts) into a perpetually optimized, high-information-density working memory. By dynamically prioritizing context based on a combination of static heuristics and a time-sensitive scoring function, our system mitigates error cascades and allows the AI to adapt its reasoning pathways based on the user's implicit focus.\n\nWe conduct a comprehensive user study on challenging deep research tasks to evaluate the efficacy of our system. Results show that our approach significantly enhances the user experience, yielding improvements of up to 29.2% in Fine-Grained Interaction and 27.7% in Ease of Collaboration compared to a competitive baseline. Most notably, our system demonstrates a 31.8% to 50.0% points improvement in overall task performance. These results highlight the critical importance of designing interactive AI systems that facilitate continuous human guidance and transparent reasoning, rather than merely responding to isolated commands.", "tldr": "", "keywords": ["Interactive AI Systems", "Human-in-the-Loop", "Multi Agent Framework"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76608e16d84874597e4f482fc64058578bc5eaf7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper suggests an extended DeepResearch system where the agent can proactively ask for clarifing questions."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It is a nice idea to add proactive dialogue for a system like DeepResearch, breaking the input/think/output paradigm. The results are convincing, and resulting system makes much more sense. This idea is also extensible to other tasks, from Math to Q&A."}, "weaknesses": {"value": "1. There are typos in the paper (one in the section heading), which is embarrassing\n2. The abstract says the core is the StateManager architecture and it is never mentioned again the paper again.\n3. The authors are totally unaware of dialogue state tracking (DST) literature in the Conversational AI community. They can also simply web search DSTC for building stateful dialogues. They can also read some survey papers on conversational agents, especially on task completion agents.\n4. The interaction is reduced to agent asking clarifying questions. This is rather limited and the dialogue can be much richer going beyond clarifying questions. Search for dialogue act tagging to see other categories."}, "questions": {"value": "1. Please also add the prompt for triggering clarifying questions in the Appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oB0fIr7eO0", "forum": "6MgD2sXZmg", "replyto": "6MgD2sXZmg", "signatures": ["ICLR.cc/2026/Conference/Submission25395/Reviewer_822V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25395/Reviewer_822V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773052933, "cdate": 1761773052933, "tmdate": 1762943421986, "mdate": 1762943421986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Deep Cognition, a multi-agent system for collaborative deep research that enables real-time human intervention during AI-driven information retrieval and synthesis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper attempts to address user control in automated research systems, which is a reasonable area of investigation. The evaluation framework covers multiple dimensions of both output quality and user experience, demonstrating effort in comprehensive assessment."}, "weaknesses": {"value": "### Fundamental Mischaracterization of Baseline Systems\n\nThe paper's central premise that existing deep research systems lack transparency, interruptibility, and real-time interaction capabilities is factually incorrect. Current commercial systems already provide the core features that Deep Cognition claims as innovations:\n\nOpenAI Deep Research and ChatGPT provide a stop/pause button that allows users to halt generation at any point. ChatGPT's interface shows streaming output in real-time, enabling users to interrupt when they observe undesirable directions. After interruption, users can provide corrective feedback and request continuation or modification. Claude (Anthropic) includes thinking tags that expose intermediate reasoning steps. Claude Sonnet's extended thinking mode shows the model's internal deliberation process. Users can review these reasoning traces and provide feedback to redirect the conversation. Perplexity AI displays search queries as they are generated, shows which sources are being accessed, and provides real-time transparency into the retrieval process. Users can see the information gathering happening and can follow up with refinements. Gemini similarly shows progressive output generation and allows users to intervene with additional instructions or corrections during multi-step tasks.\n\nGiven these capabilities in existing systems, the paper's claimed innovations reduce to minor interface variations rather than fundamental capabilities. The \"cognitive oversight\" paradigm is not a new interaction model but rather a repackaging of features already present in commercial products. This fundamentally undermines the paper's contribution claims.\n\n### Invalid Experimental Comparisons\n\nThe experimental design compares Deep Cognition's interactive mode against baseline systems used in constrained, non-interactive ways. This comparison is invalid for multiple reasons:\n\nFirst, the baselines are evaluated through their API interfaces or limited testing protocols that deliberately exclude their interactive capabilities. The paper does not test OpenAI Deep Research or ChatGPT used as intended—with users providing initial queries, reviewing outputs, interrupting when necessary, and engaging in multi-turn refinement conversations. Instead, baselines are apparently restricted to single-input scenarios, creating an artificial disadvantage.\n\nSecond, the comparison confounds interaction capability with interaction amount. Deep Cognition involves continuous user monitoring for up to 30 minutes with multiple intervention points. Baseline systems are tested without equivalent interaction opportunities. Any performance difference could simply reflect more total user input rather than system design. A fair comparison would equalize either (1) total interaction time, (2) number of feedback rounds, or (3) amount of user guidance provided across all conditions.\n\nThird, the paper claims baselines use an \"input-wait-output\" paradigm as a fundamental limitation. This misrepresents product design choices as technical constraints. These systems adopt streamlined interaction models because many users prefer convenience over control. The paper provides no evidence that intensive real-time monitoring represents what users actually want, nor that the quality improvements justify the additional attention costs.\n\n### Questionable Framing and Positioning\n\nThe paper frames \"cognitive oversight\" as a paradigm shift and claims to challenge assumptions about AI autonomy. This positioning is not supported. Human-in-the-loop machine learning, active learning, interactive machine learning, and mixed-initiative systems have explored human-AI collaboration for decades. The paper does not engage meaningfully with this literature or explain what distinguishes cognitive oversight from these established paradigms beyond terminology.\n\nThe criticism of existing systems as opaque \"black boxes\" mischaracterizes both their capabilities (as discussed above) and their design rationale. Streamlined interaction models may reflect informed product decisions based on actual user preferences rather than technical limitations. The paper assumes that more interaction opportunities equal better design without empirical support for this assumption.\n\n### Methodological Flaws, Limited Scope, and Missing Analyses\n\nSample sizes are inadequate. With 15 evaluation dimensions and small samples, multiple comparison problems and false positive risks are severe. The evaluation relies almost entirely on subjective Likert-scale ratings. Objective metrics are absent: no factual accuracy rates, no citation quality analysis, no systematic task completion efficiency measurement, no resource cost accounting. \n\nPotential biases are uncontrolled. Participants proposed their own research questions (confirmation bias), knew which system was \"new\" (experimenter demand), and were not blinded (expectancy effects). No measures address these threats to validity. Ablation studies are completely absent. The contributions of individual components and design principles are not isolated. Whether reported improvements come from system architecture or simply from increased user engagement remains unknown.\n\nEvaluation is restricted to expert users with research experience. Whether the system benefits non-experts is unknown. Task scope is narrow, focusing exclusively on academic research scenarios. Generalization to other contexts (business intelligence, market analysis, legal research) is unexplored. Resource efficiency is not analyzed. The system generates extensive API calls (5 queries × 20 URLs per cycle, multiple model instances) but provides no cost analysis."}, "questions": {"value": "**Q1**: Given that ChatGPT provides stop functionality, Claude shows thinking processes, and Perplexity displays search queries in real-time, how do you characterize your system's novelty? What specific capabilities does Deep Cognition provide that cannot be achieved through existing commercial systems?\n\n**Q2**: Why did your experimental design not include a control where baseline systems are used optimally—with multi-turn interaction? How would Deep Cognition compare against ChatGPT or Claude with unrestricted conversation?\n\n**Q3**: Your comparison shows Deep Cognition with continuous monitoring outperforms baselines with limited interaction. How much of this difference is attributable to system architecture versus more user input? Could you equalize interaction time and feedback rounds across conditions?\n\n**Q4**: Could you provide evidence that intensive real-time monitoring is what users want? What percentage of tasks benefit from continuous oversight versus asynchronous refinement?\n\n**Q5**: What are the specific algorithmic contributions beyond integrating existing techniques? What innovations could not be replicated with existing frameworks and appropriate interfaces?\n\n**Q6**: Could you report objective quality metrics (factual accuracy, citation quality, resource costs) and effort-adjusted performance? What are actual user time investments and cognitive load?\n\n**Q7**: Why are ablation studies absent? What are the individual contributions of transparency, interruptibility, architecture, and user engagement?\n\n**Q8**: How does \"cognitive oversight\" differ fundamentally from established human-in-the-loop concepts? What constitutes a genuinely new paradigm versus rebranding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "liNNdfNZZJ", "forum": "6MgD2sXZmg", "replyto": "6MgD2sXZmg", "signatures": ["ICLR.cc/2026/Conference/Submission25395/Reviewer_uzS1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25395/Reviewer_uzS1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901230060, "cdate": 1761901230060, "tmdate": 1762943421550, "mdate": 1762943421550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DeepCognition, a multi-agent framework that transform deep research tasks from asynchronous \"input-wait-output\" systems into real-time, human-AI collaborative cognitive systems. It allows users to view, interrupt, and guide the AI's reasoning process during complex research tasks lasting 15-30 minutes and involving hundreds of web resources. \nDeepCognition is built through three technical pillars: transparent and\ninterruptible AI reasoning, fine-grained bidirectional dialogue, and a shared cognitive context. DeepCognition employs a layered state manager architecture, an option-driven clarification mechanism, a distributed sub-browsing agent cluster, and intermediate report generation capabilities. \nEvaluation through user studies and benchmark tests (browsecomp-ZH, X-bench) shows improvements in both interaction quality and report quality when users actively interact with the system.  Furthermore, the benchmark test accuracy reached 72.73%, compared to Gemini/OpenAI: 40.91%, Grok 3: 22.73%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive  multi-agent system design .\n\n2. Interesting and practical problem of interactive deep research system."}, "weaknesses": {"value": "1. The paper compare between interactive Deep Cognition system with expert users actively guiding it versus non-interactive commercial baselines that never receive human intervention during research, \nwhich is a \"expert-guided system versus autonomous system\" comparison. The 72.73% versus 40.91% accuracy difference may reflect the effect of both human expertise as much as architectural innovation. \nMore ablation study would be necessary for a fair comparison. \neg: (a) Deep Cognition in fully autonomous mode without any interaction affordances, not just \"DC w/o Interaction\" where it's unclear what is disabled. (b) Commercial baselines given equivalent human guidance budgets, e.g., same number of interventions\n\n\n2. Small sample size of human study. Only at most 13 participants  is insufficient for drawing strong conclusions, particularly when most metrics are subjective assessments. The paper reports no confidence intervals, significance tests, p-values, or variance measures.\n\n3. potential bias in human study. eg: All participants appear to be Chinese speakers since browsecomp-ZH is Chinese-focused, providing no evidence of cross-linguistic or cross-cultural generalization."}, "questions": {"value": "Can you provide results for Deep Cognition in a fully autonomous mode (with all interaction mechanisms disabled, not just \"w/o Interaction\") on the same benchmarks, to establish whether the architecture itself provides value independent of human guidance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KQpuop1VUb", "forum": "6MgD2sXZmg", "replyto": "6MgD2sXZmg", "signatures": ["ICLR.cc/2026/Conference/Submission25395/Reviewer_JFUQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25395/Reviewer_JFUQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976996579, "cdate": 1761976996579, "tmdate": 1762943421342, "mdate": 1762943421342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the deep research problem via the collaboration between humans and AI, e.g., large language models (LLMs). The author proposes a framework called Deep Cognition to embed real-time human expertise into the LLM reasoning process for complex research tasks by three principles, i.e., transparency, real-time intervention, and fine-grained interaction. The author argues that the main difference between Deep Cognition and the existing deep research projects from existing systems designed by OpenAI, Google, and Perplexity AI is that the user can pause the research progress and input feedback and requirements at any moment. Experiments are conducted in both an objective and a subjective manner, where the author (1) recruited 13 participants with prior research experience to use the proposed system and (2) benchmarked the system on 22 questions from browsecomp-ZH and the first 20 questions from xbench-deep research, showing the effectiveness of the proposed system."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The deep research task is challenging, and its advancement could be crucial for advancing LLM research. The author identifies a relevant issue with the existing research system and proposes a systematic solution to involve users in the research progress. \n\n2. The proposed Deep Cognition can achieve better results than existing LLMs in the deep research task."}, "weaknesses": {"value": "1. Missing critical technical details. As the main difference between the proposed Deep Cognition and the existing LLM is in the real-time intervention for the research progress and dynamically changing the planning for the LLM agent, the author should provide the detailed technical design in the main paper. However, most of Section 2 is only a high-level description, and it is hard to get the technical contribution and insight of the proposed system, e.g., Section 2.4 claimed that the author proposed \"a dynamic research planning generation mechanism\", but there is no detailed description of it in the main paper. The lack of technical details makes it hard to convince the reviewer of the technical contribution claimed by the author.\n \n2. Evaluation metrics are unclear. As the deep research task is complicated, proper evaluation metrics are demanded to comprehensively evaluate the proposed system's capability. However, there is no description of the evaluation metrics for the proposed system in either the main paper or the Appendix. Appendix D should be a section for Evaluation Metrics Design, but it is empty, making it challenging for the reviewer to believe the reported experimental results in the main paper.\n\n3. Experimental Setup may be biased. In Page 6, the author proposes an evaluation study (Study 2) that only selects representative questions from the browsecomp-ZH and xbench-deep research benchmarks, \"given that the expert annotators are native Chinese speakers with domain expertise\". Such an evaluation setup may introduce biases, and it may not be fair to other models, such as OpenAI or Gemini, given that they may not have been optimized specifically for the Chinese domain."}, "questions": {"value": "Please refer to the Weaknesses section for the details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3JMxORCacx", "forum": "6MgD2sXZmg", "replyto": "6MgD2sXZmg", "signatures": ["ICLR.cc/2026/Conference/Submission25395/Reviewer_cEid"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25395/Reviewer_cEid"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762759888145, "cdate": 1762759888145, "tmdate": 1762943421173, "mdate": 1762943421173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}