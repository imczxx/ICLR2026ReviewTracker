{"id": "9HsaIi4ngF", "number": 13871, "cdate": 1758224098060, "mdate": 1759897407418, "content": {"title": "RouterArena: An Open Platform for Comprehensive Comparison of LLM Routers", "abstract": "Today's LLM ecosystem comprises a wide spectrum of models that differ in size, capability, and cost. No single model is optimal for all scenarios; hence, LLM routers have become essential for selecting the most appropriate model under varying circumstances. However, the rapid emergence of various routers makes choosing the right one increasingly challenging. To address this problem, we need a comprehensive router comparison and a standardized leaderboard, similar to those available for models. In this work, we introduce RouterArena, the first open platform enabling comprehensive comparison of LLM routers. RouterArena has (1) a principally constructed dataset with broad knowledge domain coverage, (2) distinguishable difficulty levels for each domain, (3) an extensive list of evaluation metrics, and (4) an automated framework for leaderboard updates. Leveraging our framework, we have produced the initial leaderboard with detailed metrics comparison as shown in Figure 1. We will make our platform open to the public; the current code base is available here: https://anonymous.4open.science/r/RouterArena-1D4B/README.md", "tldr": "We proposed a comprehensive LLM router evaluation framework, with a new dataset, metrics, and ranking system.", "keywords": ["LLM Router", "Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/deaf6fc55532826bdcb2ae32bec59c83a2da7516.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a benchmark for LLM Routers that evaluates different routing systems along multiple axes—including *accuracy*, *cost*, *latency*, and *routing robustness*. Concretely, the authors curate *8,400* queries from 21 open-source datasets and, via an LLM-as-Judge procedure using DeepSeek-V3.1, assign each query to one of three difficulty levels: easy, medium, or hard. On the collected dataset, the paper conducts objective evaluations of both open-source routers (e.g., KNN- and MLP-based methods) and commercial routers (e.g., GPT-5, Azure Router)."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It is necessary and practically valuable to objectively evaluate LLM routers, as this helps users who are unfamiliar with model details select configurations that best fit their needs.\n2. The benchmark combines 5 evaluation perspectives (including cost, accuracy, robustness, latency, and routing optimality). In addition, it supports commercial routers, making the evaluation more complete than prior work."}, "weaknesses": {"value": "### Weaknesses\n\n1. Query selection bias. The query set consists primarily of objective questions and excludes creative/open-ended tasks. For objective questions, users often prioritize accuracy over cost; however, for open-ended or batch-processing tasks (e.g., large-scale data filtering), cost can be the primary concern, leading to different routing preferences. This preference shift can further affect the validity of conclusions drawn in the Experiments section. It is advisable to include open-ended queries and update the results and analyses accordingly.\n\n2. Difficulty grading misalignment with real-world scenarios. Difficulty is determined solely via an LLM-as-Judge setup, which may introduce model-induced bias; human annotation should be incorporated. Moreover, the benchmark should expand to more realistic routing demands, such as *long-context generation*, *tool use*, *code agents*, and *deep research*. In real deployments, achieving good performance often involves *orchestrating multiple models* collaboratively rather than invoking a single model.\n\n3. Limited comparability due to heterogeneous model pools. Different routers operate over different model pools. A router that can access a cheaper yet strong model may rank higher, but this does not necessarily demonstrate a better routing algorithm. Introducing a unified model pool and a corresponding leaderboard would improve comparability.\n\n4. Evaluation dimensions remain limited. Additional aspects—such as *response time* or *response length*—should be considered. For example, if the pool contains a cheaper *reasoning* model and a more expensive *instruction* model, then under the cost formulation\n$ \\text{cost} = C_{\\text{in}} \\cdot N_{\\text{in}} + C_{\\text{out}} \\cdot N_{\\text{out}} $,\ntwo routing strategies might achieve the *same cost* and *same accuracy*, yet differ meaningfully in behavior; how should their relative quality be judged?\n\n### Notation\n\n1. *All displayed formulas lack numbering*, which reduces ease of reference and clarity in discussion."}, "questions": {"value": "See above weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8iXIX7wkBw", "forum": "9HsaIi4ngF", "replyto": "9HsaIi4ngF", "signatures": ["ICLR.cc/2026/Conference/Submission13871/Reviewer_vKx3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13871/Reviewer_vKx3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902660419, "cdate": 1761902660419, "tmdate": 1762924388382, "mdate": 1762924388382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RouterArena, which the authors claim to be the first comprehensive open platform for evaluating and comparing large language model (LLM) routers. The platform aims to meet the growing need for systematic router evaluation and provides: (1) a principled dataset consisting of 8,400 queries spanning 9 domains and 44 categories, organized using the Dewey Decimal Classification system and incorporating Bloom’s taxonomy to define difficulty levels; (2) a comprehensive set of evaluation metrics covering accuracy, cost, routing optimality, robustness, and latency; and (3) an automated evaluation framework that supports both open-source and commercial routers. The authors evaluate 12 representative routers and present a multidimensional leaderboard summarizing their overall performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper addresses a critical gap in the LLM ecosystem, the research motivation is clear and timely.\nS2. The evaluation metrics are comprehensive, with five well-defined dimensions that reflect real-world deployment considerations.\nS3. The automated framework enables dynamic leaderboard updates and supports the evaluation of both open-source and commercial routers."}, "weaknesses": {"value": "W1. The overall contribution of the paper is limited.\nW2. For commercial routers, internal routing decisions are inaccessible, making many metrics uncomputable and thus limiting a full evaluation.\nW3. The use of DeepSeek-V3.1 for automated difficulty annotation may introduce systematic bias; no quantitative bias analysis or annotation-consistency study is provided.\nW4. The robustness test is limited, and the actual evaluation method appears inconsistent with the definition of robustness given in Section 4.\nW5. The paper spends substantial space describing the construction of a dataset with distinct difficulty levels but does not subsequently analyze results based on these levels."}, "questions": {"value": "Q1. Regarding the validation of Bloom’s taxonomy classification—was any human sampling verification conducted, and how consistent were the results with the LLM’s judgments?\nQ2. The robustness definition in Section 4 differs from the implementation described in Section 6.2 (“adding irrelevant keywords”). If only keyword-insertion was used, does this cause inconsistency between the definition and the actual test?\nQ3. Figure 6 looks more like a scatter plot than a curve plot. Would the authors consider renaming it for accuracy?\nQ4. The paper builds a dataset with three difficulty levels, but all evaluations are aggregated. Why not provide router performance broken down by difficulty level?\nQ5. For commercial routers that cannot expose routing decisions, is there a plan to design alternative indicators to enhance ranking reliability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LHAfoFB9LF", "forum": "9HsaIi4ngF", "replyto": "9HsaIi4ngF", "signatures": ["ICLR.cc/2026/Conference/Submission13871/Reviewer_ZQTb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13871/Reviewer_ZQTb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925600589, "cdate": 1761925600589, "tmdate": 1762924387986, "mdate": 1762924387986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"RouterArena,\" a new open-source platform for benchmarking Large Language Model (LLM) routers. The authors argue that as the LLM ecosystem has produced numerous models with varying costs and capabilities, routers have become essential for selecting the right model for a given query. This, in turn, has created a new need for a standardized way to evaluate the routers themselves.\n\n\nAn automated framework and leaderboard designed to be \"live,\" allowing researchers to submit new routers (both open-source and commercial) for evaluation and comparison."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a practical problem. As model routing becomes a standard component in AI stacks, the need for a robust, standardized benchmark to compare routers is high. This work is well-motivated and highly relevant to the community.\n\n2. Principled Dataset Construction: A major strength of this paper is its novel and well-justified dataset construction.\n\n3. Novelty: An automated, \"live\" platform and leaderboard, distinct from a static dataset. It is designed for continuous benchmarking and community engagement, allowing researchers to submit and compare new open-source and commercial routers.\n\n4. Comprehensive, Multi-Dimensional Evaluation: The paper correctly identifies that router performance is multi-faceted. The inclusion of Routing Optimality is a key metric, as it reframes the goal from just \"being correct\" to \"being correct efficiently.\" Measuring Robustness and Latency further strengthens the benchmark's utility for real-world applications.\n\n5. Interesting Initial Analysis and Insights: The initial evaluation of 12 routers provides valuable insights. The findings—that commercial routers do not necessarily lead, that all routers are inefficient, and that performance is a complex trade-off—are important takeaways for the field."}, "weaknesses": {"value": "Discussion of Benchmarking Philosophy: The paper positions itself as superior to prior work like RouterBench (Table 1), but it misses an opportunity for a deeper discussion. RouterArena evaluates live routers (a \"hot\" evaluation), whereas RouterBench uses a large, static dataset of pre-computed outcomes for \"offline\" evaluation. This offline approach is significantly cheaper and faster for iterating on router designs, presenting a different benchmarking philosophy. A more nuanced discussion of the pros and cons of these different approaches would improve the paper's contribution.\n\nDataset Scale: While the dataset is principled in its design, its size (~8,400 queries) is a potential limitation. When spread across 44 categories and 3 difficulty levels, some cross-sections may be too small to draw statistically significant conclusions. The paper would be stronger if it addressed this limitation or included an analysis of the sample size per category."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zPPg5WrziO", "forum": "9HsaIi4ngF", "replyto": "9HsaIi4ngF", "signatures": ["ICLR.cc/2026/Conference/Submission13871/Reviewer_LQ8R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13871/Reviewer_LQ8R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984967491, "cdate": 1761984967491, "tmdate": 1762924387580, "mdate": 1762924387580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles a timely problem—standardized router evaluation—with a clear systemization (dataset + metrics + framework) and substantive empirical coverage (open-source and commercial)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. Well-scoped problem & gap analysis.\n\nS2. Good dataset, metric, and evaluation design.\n\nS3. Comprehensive experiments and visualizations."}, "weaknesses": {"value": "W1. The $\\log 2$ cost normalization with fixed ( $c_{min}⁡ = 0.0044$, $c_{max} ⁡ = 200$ ) and $\\beta=0.1$ may bias rankings toward certain price bands; no sensitivity analysis shown in Sec. 5.\n\nW2. LLM-as-judge labeling (DeepSeek-V3.1) lacks human validation studies or inter-rater checks."}, "questions": {"value": "I think this is a very interesting topic. Specifically, I have the following two questions:\n\nQ1. Your evaluation reports per-query accuracy as a binary outcome and aggregates it—without explicit weighting by Bloom difficulty levels—into the composite Arena score $S_{i,\\beta}$. \n-  Is “completion” strictly 0 or 1 correctness per query? If so, why not support rubric-based partial credit for partially solved answers (e.g., correct plan but incomplete final step)?\n- Some longer (higher-token) first-round answers can enable success in later turns. Do you plan a multi-turn setting that measures cross-round utility versus added first-round cost (e.g., a “round-2 success gain” vs. round-1 token spend)?\n\nQ2. Among answers that are equally correct, a longer response may offer clearer reasoning, citations, or actionable steps, potentially increasing user satisfaction even at a higher token cost. Do you collect any user-satisfaction / explanation-quality signal (human Likert ratings or a calibrated LLM-as-judge) in addition to accuracy/cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OLfcvgNCZH", "forum": "9HsaIi4ngF", "replyto": "9HsaIi4ngF", "signatures": ["ICLR.cc/2026/Conference/Submission13871/Reviewer_T3An"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13871/Reviewer_T3An"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076922284, "cdate": 1762076922284, "tmdate": 1762924387158, "mdate": 1762924387158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}