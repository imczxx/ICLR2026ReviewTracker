{"id": "OXOGZxjCsN", "number": 17774, "cdate": 1758280425328, "mdate": 1759897154830, "content": {"title": "MAD:A Multimodal Anomaly Detection Framework Based on Shared Transformer and Contrastive Learning for Smart Manufacturing", "abstract": "With the advancement of smart manufacturing environments, anomaly detection techniques that integrate heterogeneous composite sensor data are becoming increasingly important. However, there are still technical difficulties in effectively fusing data with different characteristics, such as PRPD images and PD time series. To address these issues, this study proposes a high-performance multimodal framework, MAD, based on a two-step training strategy. First, to reduce the representation differences between modalities, a RealNVP-based normalization flow is introduced to align the representations of each modality into a shared latent space. Second, we use Supervised Contrastive Learning to learn a structured representation space with well-defined boundaries between classes. The aligned and structured representations are then fed into the LIMoE encoder, a Mixture-of-Experts-based shared transformer, to finally classify the types of anomalies. Experimental results demonstrate that the proposed MAD model outperforms the existing SOTA multimodal models. In particular, MAD achieves an AUC of 100.0% and F1-score of 99.98%, which is comparable to that of Perceiver IO, Cross-Modal Transformer, and CFM.", "tldr": "", "keywords": ["Multimodal", "Contrastive Learning", "Normalization Flow", "Representation Learning", "Anomaly Detection"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c8cee105c4f49862f3b0274457c28140992dbd98.pdf", "supplementary_material": "/attachment/69a55d1bb6f9615f035b808214641657279a8347.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on anomaly detection for PRPD images and PD time-series data, proposing a two-step training strategy that first aligns the representations of the two modalities into a shared latent space, followed by a contrastive learning stage to learn a structured representation for anomaly detection. The framework is applied to a specific industrial application, where the method shows promising results in identifying anomalies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly structured and well-motivated. The idea of combining two distinct modalities—image and time series—into a shared representation space is logical and relevant to the target application. The use of contrastive learning to enforce structure and separation between normal and abnormal representations is a sound methodological choice. Overall, the proposed approach appears technically reasonable and conceptually meaningful for the described domain."}, "weaknesses": {"value": "The paper’s experimental validation is not convincing. There is an insufficient comparison against baseline methods. Specifically, the paper lacks evaluations against pure image-based anomaly detection methods and pure time-series anomaly detection baselines. Without these, it is difficult to understand how much of the observed improvement comes from the proposed alignment and contrastive learning strategy rather than standard unimodal baselines.\n\nMoreover, Table 4 looks problematic. The proposed model performs worse than two state-of-the-art baselines (CMT and CFM), calling into question the claimed advantages of the proposed approach. The near-perfect scores (AUC and F1 close to 100 for all models) further suggest a saturated evaluation setup, where all methods perform almost equally well, making it impossible to differentiate performance in any statistically meaningful way.\n\nAnother major limitation is that the dataset is extremely narrow, confined to the specific application described in the paper. There is no evidence that the proposed method generalizes to broader multimodal anomaly detection problems involving both images and time series. As a result, the current evaluation does not adequately support the claimed general effectiveness of the approach."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "25T0R84GXi", "forum": "OXOGZxjCsN", "replyto": "OXOGZxjCsN", "signatures": ["ICLR.cc/2026/Conference/Submission17774/Reviewer_ikxY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17774/Reviewer_ikxY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760595746874, "cdate": 1760595746874, "tmdate": 1762927613787, "mdate": 1762927613787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MAD (Multimodal Anomaly Detector), a multimodal framework designed for anomaly detection in smart manufacturing environments by integrating Phase-Resolved Partial Discharge (PRPD) images and Partial Discharge (PD) time-series signals. The authors address the challenge of heterogeneous modality fusion through a two-stage training strategy that combines representation alignment, contrastive learning, and a shared transformer encoder."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Originality:** The paper presents a novel integration of normalizing flows, supervised contrastive learning, and a LIMoE-based shared transformer within a unified multimodal anomaly detection framework.\n- **Clarity:** The paper is clearly written and well-organized, guiding the reader through the motivation, methodology, and results in a logical progression.\n- **Significance:** The work is highly relevant to real-world applications in smart manufacturing, where reliable multimodal anomaly detection is crucial for predictive maintenance and process reliability."}, "weaknesses": {"value": "**Limited novelty in architectural components:** While the integration of RealNVP, SupCon, and LIMoE is interesting, each component is adapted rather than fundamentally extended. The paper does not propose any substantial modification to these existing techniques—RealNVP (Dinh et al., 2016) and SupCon (Khosla et al., 2020) are applied in a relatively straightforward way.  The novelty therefore lies primarily in the combination and application context (smart manufacturing) rather than in methodological advancement.  The authors could strengthen the paper by clarifying how their adaptation of these components yields unique benefits beyond compositional synergy.\n\n**Dataset limitations and lack of generalization evidence:** The experiments rely solely on a single dataset from AI Hub, consisting of PRPD images and PD time-series signals.  This raises questions about the generalization of MAD to other multimodal industrial datasets or to settings with missing or noisy modalities.  The reported near-perfect performance (AUC = 100.0%, F1 = 99.98%) may reflect dataset simplicity or overfitting.  Including evaluations on at least one independent dataset or a cross-domain transfer experiment would significantly enhance credibility.\n\n**Insufficient analysis of modality interaction:** Although the paper discusses modality alignment via normalizing flows, there is no quantitative analysis of how alignment improves cross-modal correlation or how much each modality contributes after alignment.  For example, reporting mutual information metrics, cross-modal retrieval accuracy, or visualizations of aligned latent spaces could provide deeper insight into the model’s fusion behavior.  The current qualitative t-SNE plots (Figure 3) are helpful but not sufficient for understanding the degree of alignment achieved."}, "questions": {"value": "**1、On modality alignment effectiveness:** \n\nCould the authors provide quantitative evidence of how the RealNVP-based normalization flow improves cross-modal alignment?\n\nFor example, measuring intra-class and inter-class distances before and after alignment, or computing cross-modal similarity metrics (e.g., cosine similarity between corresponding image and signal embeddings). Such results would clarify how much the normalizing flow contributes beyond visual t-SNE evidence.\n\n**2、On generalization and dataset diversity:** \n\nThe reported results achieve nearly perfect accuracy (AUC = 100%, F1 = 99.98%), which raises concerns about potential data bias or overfitting. \n\nHave the authors evaluated MAD on any other industrial or synthetic multimodal datasets, or under domain shift scenarios (e.g., different sensors or environmental conditions)? If not, could they justify why this single dataset sufficiently represents general smart manufacturing conditions?\n\n**3、On modality contribution and ablation:** \n\nThe unimodal experiments show image-dominant performance, implying the PD time-series contributes little. \n\nCould the authors perform a cross-modal ablation or feature attribution analysis (e.g., attention maps or feature importance) to quantify the contribution of each modality after alignment? Understanding whether the flow alignment increases the usefulness of the signal modality would strengthen the multimodal claim.\n\n**4、On deployment efficiency and real-world use:** \n\nThe paper claims that MAD is suitable for real-time applications and edge-cloud scenarios, but no latency, throughput, or energy efficiency metrics are provided. Could the authors report actual inference times, FLOPs per modality, or edge device benchmarks (e.g., Jetson, FPGA) to substantiate the deployment claim? This would make the work more convincing for industrial adoption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W5bT8tA1a6", "forum": "OXOGZxjCsN", "replyto": "OXOGZxjCsN", "signatures": ["ICLR.cc/2026/Conference/Submission17774/Reviewer_5LBR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17774/Reviewer_5LBR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757492273, "cdate": 1761757492273, "tmdate": 1762927613396, "mdate": 1762927613396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MAD, a high-performance Multimodal Anomaly Detector for smart-manufacturing plants that fuse Phase-Resolved Partial-Discharge (PRPD) images and Partial-Discharge (PD) time-series. A two-stage training strategy is proposed: (1) RealNVP normalizing-flows first align the heterogeneous feature distributions into a shared latent space; (2) supervised contrastive learning then sculpts a compact, class-separable representation. The aligned vectors are processed by a lightweight Mixture-of-Experts Transformer (LIMoE) for final classification. Tested on real industrial data, MAD attains 100 % AUC and 99.98 % F1, outperforming single-modal and existing SOTA multimodal baselines such as Perceiver IO and Cross-Modal Transformer, while keeping parameter count low."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The overall architecture diagram (Fig. 1) is clear and informative; every module is visually distinguished.\n2. The experimental section is presented in a straightforward way, and the final anomaly-detection performance (AUC =100 %, F1 ≈ 99.98 %) is among the highest recorded on this data set."}, "weaknesses": {"value": "1. Motivation is unclear: the paper never explains why RealNVP, SupCon, and LIMoE are the right building blocks for anomaly detection on PRPD images and PD time series. A short ablation or qualitative discussion on modal misalignment would remedy this.\n2. Method novelty is limited. RealNVP, SupCon, and LIMoE are all established techniques; section 3 mostly re-describes them and offers no new theoretical or algorithmic twist.\n3. The two-stage pipeline (alignment → contrastive pre-training → CE fine-tuning) is presented as fait accompli. No evidence is given that a single-stage end-to-end objective (e.g., joint anomaly + classification loss) would fail, and no complexity/accuracy trade-off is analysed.\n4. Several figures (notably Fig. 2 and Fig. 5) are low-resolution; axis labels and legends are illegible even when zoomed.\n5. Comparing multi-modal MAD against single-modal CNN/Transformer baselines is intrinsically unfair because extra input almost always boost scores. Indeed, on “surface” and “corona” categories the image-only model equals or slightly surpasses MAD, undermining the value of adding the signal data.\n6. Table 4 relies on 2021–2023 baselines (Perceiver IO, CMT, CFM). More recent multi-modal architectures  are omitted, so the “SOTA” claim is dated.\n7. In Table 4 the F1 of CFM is already 100.0; MAD’s 99.98 is therefore not the top value, yet it is incorrect to use the bold font.\n8. References are incomplete—Perceiver IO, CMT, and CFM are mentioned in the main text but do not appear in the bibliography."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uyMr3PG5zo", "forum": "OXOGZxjCsN", "replyto": "OXOGZxjCsN", "signatures": ["ICLR.cc/2026/Conference/Submission17774/Reviewer_MK23"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17774/Reviewer_MK23"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906462256, "cdate": 1761906462256, "tmdate": 1762927612572, "mdate": 1762927612572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the MAD framework for multimodal anomaly detection in smart manufacturing. The framework integrates PRPD images and PD time series signals. The core innovations include: (1) modality alignment using RealNVP normalizing flow, (2) a two-stage strategy of SupCon pretraining followed by CE fine-tuning, and (3) a LIMoE shared Transformer encoder."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses real-world industrial anomaly detection problems using the AI Hub dataset, with practical application value\n\n2. Five-stage pipeline design is clear and modular, and the two-stage training strategy (SupCon+CE) has clear motivation\n\n3. Includes four sets of comparative experiments and provides a lightweight version (96.9% parameter reduction with only 0.48% performance drop), with relatively complete ablation studies"}, "weaknesses": {"value": "1. RealNVP, LIMoE, and SupCon are all direct combinations of existing methods, lacking theoretical innovation and comparison with other alignment methods (CCA, CORAL).\n\n2. The F1 score for single-modality images has reached 99.99%, while for multimodal it dropped to 99.97%; therefore, I question the necessity of multimodality in this method. \n\n3. Key details are missing, such as Normalizing Flow design principles, LIMoE routing mechanism, and data augmentation strategies, and the hyperparameters are incomplete.\n\n4. There is a lack of failure case analysis, discussion on the physical reasons for class confusion, analysis of MAD_base training instability, and parameter sensitivity study.\n\n5. The quality of figures in the paper needs improvement."}, "questions": {"value": "1. The image single-modal accuracy has reached 99.99%, while the multimodal accuracy has actually dropped to 99.97%. Could you provide experimental evidence for scenarios where multimodality truly has an advantage?\n\n2. Why choose RealNVP instead of simpler alignment methods (Batch Norm, CCA, CORAL)? Could you provide an analysis of the distribution characteristics of PRPD and PD, as well as ablation experiments comparing with other alignment methods?\n\n3. The paper states \"simultaneously routing to 2 experts,\" which seems to contradict the standard MoE sparse activation. How is this different from a simple ensemble?\n\n4. Could you provide the complete hyperparameters and the data augmentation strategy for SupCon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gb7QkTixZc", "forum": "OXOGZxjCsN", "replyto": "OXOGZxjCsN", "signatures": ["ICLR.cc/2026/Conference/Submission17774/Reviewer_jiC8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17774/Reviewer_jiC8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956308129, "cdate": 1761956308129, "tmdate": 1762927612063, "mdate": 1762927612063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}