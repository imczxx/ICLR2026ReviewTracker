{"id": "htOZXpUPFZ", "number": 20980, "cdate": 1758312309858, "mdate": 1763652589866, "content": {"title": "Fluent Alignment with Disfluent Judges: Post-training for lower-resource languages", "abstract": "We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokmål and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.", "tldr": "We propose a preference-optimization method for lower-resource languages that results in fluent language models even when aligned by disfluent reward models.", "keywords": ["language model", "post-training", "fluency", "low-resource languages", "RLAIF"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03e00e7b44838f9f8d47af18c9f8ac28c7f623f5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper aims to assess whether post-training models in low ressource languages for which no natural SFT instruction set exist is better done through (a)  synthetically translating an english SFT set into the target language using a translation model (of varying target language fluency) or (b) using RL to let the target model generate it's own rollouts in the target language and using a **disfluent** reward model to optimize the model towards generating **fluent** text in the target language. In both cases, models are continually pretrained on unsupervised norwegian and lightly SFTed in english beforehand.\n\nInterestingly, this paper shows (b) leads to stronger results, and the target model can become more fluent than the bigger judge model - achieving weak to strong generalization through the combined use of unsupervised CPT of the target model, and the fact that judging fluency for the reward models is an easier task than being fluent in generation themselves."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written. The goals are clearly motivated making reading easy.  The potential impact is clear - enabling simpler training of instruction models on languages where supervised data is scarce.\nAblations are strong and well designed, answering many of the questions I had on the impact of the quality of the translation models for SFT, or judge reward models."}, "weaknesses": {"value": "It would be interesting to have compute intensity comparisons between RL and SFT methods. My intuition is that much more data samples could be seen by the model in the SFT setup under the same compute conditions, perhaps somewhat compensating the performance gap with the RL based method. \n\nOne of the big claims of this paper, is that SFT does not work because even with good translation models, the translations may remain unnatural in some low ressource languages. Since the SFT set used is very small, could it have been possible to manually filter the SFT set by norwegian speakers and have a \"perfectly fluent\" SFT set - thus isolating the RL/SFT debate from translation quality ? More cheaply, a way to compensate the unnaturalness of translated text could also have been to filter translations using the same reward models (maybe generating various rollouts using the translation models and keeping only the most natural as determined by the judges for SFT).  As is, on-policy RL might naturally help the model converge to generations that better appeal to human or model judges irrespective of fluent language. \n\nI would appreciate it if the related work could be slightly extended to include works on translation without/with little parallel data, or works on zero-shot transfer of language abilities (train on an anchor language, evaluate on others). More generally, this problem also maps to works on weak-to-strong generalization, or RLAIF with weak judges and previous papers have already explored this notion of disconnect between judging/doing capacity. I believe discussing this would better contextualize this work and give intuitions on why this is possible."}, "questions": {"value": "1. This setup seems very close to rejection sampling SFT: use the english SFTed model, generate many rollouts in norwegian and train only on the top samples as determined by a LLM as a judge. I'm guessing this might not be possible because post english SFT, the model is not able to produce coherent norwegian so several iterations have to be run? What if this is bootstrapped with few shot ICL in norwegian ? Is RL really key here ?\n\n2. Could the use of techniques such as low-rank adapters, replay data, long learning rate re-warmup, etc. help alleviate the norwegian language collapse when doing english SFT ?\n\n> \"The key principle is to never train the language model on any unnatural text.\"\n\n3. Wouldn't the on-policy sampled text be unnatural at least in the first iterations, even after LLM as a judge filtering ? Could you provide some examples (at least for the norwegian speakers that will understand them, and some qualitative insights into how fluency evolves throughout training, what patterns are learned first, etc) I saw quantitative examples were provided in Figure 3.\n\n\nMy main concern here (summing up most of the weaknesses and question I stated) is disambiguating RL/SFT training dynamics versus the question of training/not training on unnatural samples. Irregardless of if RL is the only way to achieve this, the paper clearly shows judges do not need to be fluent themselves to steer fluent generations which I believe is a nice contribution in this domain."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yqlx5XlAfY", "forum": "htOZXpUPFZ", "replyto": "htOZXpUPFZ", "signatures": ["ICLR.cc/2026/Conference/Submission20980/Reviewer_9f26"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20980/Reviewer_9f26"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761417575457, "cdate": 1761417575457, "tmdate": 1762939432700, "mdate": 1762939432700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to assess whether post-training models in low ressource languages for which no natural SFT instruction set exist is better done through (a)  synthetically translating an english SFT set into the target language using a translation model (of varying target language fluency) or (b) using RL to let the target model generate it's own rollouts in the target language and using a **disfluent** reward model to optimize the model towards generating **fluent** text in the target language. In both cases, models are continually pretrained on unsupervised norwegian and lightly SFTed in english beforehand.\n\nInterestingly, this paper shows (b) leads to stronger results, and the target model can become more fluent than the bigger judge model - achieving weak to strong generalization through the combined use of unsupervised CPT of the target model, and the fact that judging fluency for the reward models is an easier task than being fluent in generation themselves."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written. The goals are clearly motivated making reading easy.  The potential impact is clear - enabling simpler training of instruction models on languages where supervised data is scarce.\nAblations are strong and well designed, answering many of the questions I had on the impact of the quality of the translation models for SFT, or judge reward models."}, "weaknesses": {"value": "It would be interesting to have compute intensity comparisons between RL and SFT methods. My intuition is that much more data samples could be seen by the model in the SFT setup under the same compute conditions, perhaps somewhat compensating the performance gap with the RL based method. \n\nOne of the big claims of this paper, is that SFT does not work because even with good translation models, the translations may remain unnatural in some low ressource languages. Since the SFT set used is very small, could it have been possible to manually filter the SFT set by norwegian speakers and have a \"perfectly fluent\" SFT set - thus isolating the RL/SFT debate from translation quality ? More cheaply, a way to compensate the unnaturalness of translated text could also have been to filter translations using the same reward models (maybe generating various rollouts using the translation models and keeping only the most natural as determined by the judges for SFT).  As is, on-policy RL might naturally help the model converge to generations that better appeal to human or model judges irrespective of fluent language. \n\nI would appreciate it if the related work could be slightly extended to include works on translation without/with little parallel data, or works on zero-shot transfer of language abilities (train on an anchor language, evaluate on others). More generally, this problem also maps to works on weak-to-strong generalization, or RLAIF with weak judges and previous papers have already explored this notion of disconnect between judging/doing capacity. I believe discussing this would better contextualize this work and give intuitions on why this is possible."}, "questions": {"value": "1. This setup seems very close to rejection sampling SFT: use the english SFTed model, generate many rollouts in norwegian and train only on the top samples as determined by a LLM as a judge. I'm guessing this might not be possible because post english SFT, the model is not able to produce coherent norwegian so several iterations have to be run? What if this is bootstrapped with few shot ICL in norwegian ? Is RL really key here ?\n\n2. Could the use of techniques such as low-rank adapters, replay data, long learning rate re-warmup, etc. help alleviate the norwegian language collapse when doing english SFT ?\n\n> \"The key principle is to never train the language model on any unnatural text.\"\n\n3. Wouldn't the on-policy sampled text be unnatural at least in the first iterations, even after LLM as a judge filtering ? Could you provide some examples (at least for the norwegian speakers that will understand them, and some qualitative insights into how fluency evolves throughout training, what patterns are learned first, etc) I saw quantitative examples were provided in Figure 3.\n\n\nMy main concern here (summing up most of the weaknesses and question I stated) is disambiguating RL/SFT training dynamics versus the question of training/not training on unnatural samples. Irregardless of if RL is the only way to achieve this, the paper clearly shows judges do not need to be fluent themselves to steer fluent generations which I believe is a nice contribution in this domain."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yqlx5XlAfY", "forum": "htOZXpUPFZ", "replyto": "htOZXpUPFZ", "signatures": ["ICLR.cc/2026/Conference/Submission20980/Reviewer_9f26"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20980/Reviewer_9f26"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761417575457, "cdate": 1761417575457, "tmdate": 1763656393157, "mdate": 1763656393157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an on-policy, fluency-aware post-training pipeline for lower-resource languages. The key idea is: start from a target-language–pretrained model, give it only a short SFT on high-quality English data to learn instruction-following, and then do online, on-policy RL in the target low-resource language — without ever training on machine-translated target-language responses. Even though the reward source (“judge”) can itself be disfluent, the policy can stay fluent because it only ever updates on its own (already fluent) samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes a three-stage recipe: (1) target-language pretraining (taken as given), (2) short English SFT (1k LIMA items, one epoch) just to teach turn-taking / format, (3) online on-policy RL in the target language, where the policy samples multiple responses per prompt and a multilingual LLM-as-judge gives rewards, plus a KL term with a variance-reduced (Rao–Blackwellized) estimator.\n- The experimental section is unusually careful for a “low-resource alignment” paper: same base model lineage, three post-training routes, human A/B judgments (5 annotators, 15–20h each), explicit win-rate table, and multiple ablations (SFT length, translation model, judge choice, training length).\n- The pipeline (Fig. 2) and the 3-stage story are easy to follow; section 3 very explicitly says: we tried (1) Norwegian RL, (2) translated SFT, (3) Mistral-Nemo."}, "weaknesses": {"value": "- Scope proven only on one language / one family. Everything is on Norwegian Bokmål, which is relatively resourced compared to truly low-resource or morphologically extreme languages (e.g., Amharic, many Niger-Congo languages). It is also Germanic and fairly close to English. So it’s unclear whether the “disfluent judge → fluent policy” effect would hold for typologically distant languages, or for languages whose MT systems produce much noisier translationese. A small second-language experiment (even synthetic / smaller scale) would strengthen generality.\n- The recipe begins with “pretraining on the target language” and even borrows a continuous Norwegian pretraining from Samuel et al. (2025), but the paper explicitly says “we do not cover this pretraining stage.” In practice, the main result (“policy stays fluent”) might be mostly due to starting from a very good Norwegian base — i.e. the method is only as good as your base. The paper could do a clearer disentanglement: what if the base itself is only semi-fluent? The reader can’t currently tell how much credit to give to the RL stage vs the initial base.\n- This paper says the judge only needs “sufficient understanding,” not fluency, and Table 2 reports low correlation — but the judge prompt is not analyzed: how sensitive is the whole pipeline to mis-judgments on subtle register/style errors (exactly the kind of thing that makes translationese detectable)? If the judge systematically likes translationese, the policy may reintroduce it despite being on-policy. A robustness test with deliberately biased judges would make the claim stronger.\n- Base-model ablation. What happens if the target-language base is weaker (fewer continual-pretraining steps, or a multilingual model zero-shot on Norwegian)? Does the proposed RL still “pull” it to a fluent regime, or is strong pretraining actually the dominating factor?"}, "questions": {"value": "- Language Scope and Generality. Could you provide evidence that the proposed “disfluent-judge → fluent-policy” phenomenon holds beyond Norwegian Bokmål? Even a smaller-scale or synthetic experiment on a typologically distant language (e.g., a morphologically rich or non-Indo-European one) would help clarify whether the observed effect generalizes or depends on Norwegian’s proximity to English.\n- Dependence on Base-Model Quality. Your pipeline assumes a target-language-pretrained base that is already highly fluent. How much of the final fluency retention actually stems from this strong base versus the on-policy RL stage itself? Could you include an ablation starting from a weaker or multilingual base (zero-shot in Norwegian) to show whether the method can recover fluency or merely preserve it?\n- Judge Sensitivity and Bias. This paper states that the judge only needs “sufficient understanding,” yet its potential biases toward translationese or anglicized phrasing are not analyzed. Did you test robustness when the judge systematically prefers slightly translated-style outputs? A stress-test with biased or intentionally noisy judges would clarify how resilient the policy is to imperfect reward signals.\n- Please describe the precise prompts and calibration used for the multilingual judge. Are the reward decisions consistent across different prompt wordings or random seeds? Providing examples of disagreements and how they affect updates would make the pipeline more reproducible and interpretable.\n- If the base model were only semi-fluent, could the on-policy RL process improve fluency over time, or does it merely avoid degradation? An explicit experiment varying the amount of target-language pretraining would clarify the scope of applicability for teams without high-quality monolingual bases.\n- For truly low-resource languages that lack both strong pretraining corpora and fluent judges, what minimal requirements (dataset size, judge quality, reward signal noise tolerance) are necessary for this approach to remain effective? Quantifying those thresholds would make the paper more actionable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cKHq8HooRv", "forum": "htOZXpUPFZ", "replyto": "htOZXpUPFZ", "signatures": ["ICLR.cc/2026/Conference/Submission20980/Reviewer_dvfW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20980/Reviewer_dvfW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976857705, "cdate": 1761976857705, "tmdate": 1762939390751, "mdate": 1762939390751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates on-policy reinforcement learning for improving low-resource language capabilities in multilingual language models. The key insight is both counterintuitive and practical: using a reward model (judge) that is not fully fluent in on-policy RL can outperform supervised fine-tuning (SFT) on machine-translated instruction-following data. Existing multilingual alignment approaches often rely on translating English instruction data, which introduces translationese that harms fluency. In contrast, on-policy reinforcement learning can achieve alignment purely through model-generated rollouts, without relying on synthetic or translated data. However, this requires a robust reward model to guide the RL process. The authors show that such a reward model need not be fluent in the target language. Using Norwegian as a case study, the authors conduct human evaluations by five native speakers, demonstrating that on-policy RL guided by a disfluent judge achieves higher fluency compared to translation-based SFT baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The exploration of RL for low-resource language alignment is novel and practically meaningful. It avoids annotation costs and translation artifacts, and shows that even a non-fluent reward model can guide the training fluent policy models in the target language. \n2. The use of human evaluation by native speakers strengthens the credibility of the findings. The results in Table 1 clearly demonstrate the superiority of the proposed approach.\n3. Beyond using a non-fluent judge, the paper further explores different model families as reward models and shows that the final alignment quality in RL is independent of the judge model’s fluency."}, "weaknesses": {"value": "1. Generalization to other languages. All experiments are conducted solely on Norwegian. It remains unclear whether the proposed method generalizes to other low-resource languages or even to medium- and high-resource ones. Would the same conclusions hold across different language families? For higher-resource languages, might the performance gap with SFT narrow? While human evaluation is costly, automated evaluation (similar to the constructed evaluation model in the paper) could help extend this analysis.\n2. Evaluation scope limited to fluency. Fluency is central to the paper’s evaluation, but other aspects such as helpfulness [1] or safety are equally important. There is a risk of reward hacking [2]: the model might prioritize fluency at the cost of informativeness, producing simpler but less useful responses. It remains unclear whether fluency gains come at the expense of other abilities.\n3. Dependence on SFT for initialization. The Norwegian RL pipeline still relies on some SFT data to establish basic instruction-following ability. The claim would be more solid if the authors demonstrated pure on-policy RL starting from a base model, similar to the success of zero RLVR [3], showing that RL alone can enhance the model’s capabilities without prior supervised alignment.\n4. Limited model diversity. The experiments are conducted mainly with the Mistral base model. Extending the study to additional model families (e.g., Qwen, Llama) would strengthen the generality and robustness of the conclusions.\n5. The section on human evaluation lacks important background information about the annotators. To ensure ethical research practices, the authors should specify the annotators' qualifications and confirm that they received fair and reasonable compensation for their work.\n\n---\n\n**References**\n\n[1] Liu, Ryan, Theodore Sumers, Ishita Dasgupta, and Thomas L. Griffiths. (2024) \"How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?.\" In International Conference on Machine Learning, pp. 31844-31865. PMLR.\n\n[2] Liu, Tianqi, et al. (2024) \"RRM: Robust Reward Model Training Mitigates Reward Hacking.\" The Thirteenth International Conference on Learning Representations.\n\n[3] Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., ... & He, Y. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948."}, "questions": {"value": "1. Where does the boundary of the main claim lie? i.e., What is the minimum level of language proficiency required for a reward model to provide effective reward signals? For instance, could a strong English model with only weak, translation-based understanding of Norwegian still serve as an effective judge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RGNLx7vt0X", "forum": "htOZXpUPFZ", "replyto": "htOZXpUPFZ", "signatures": ["ICLR.cc/2026/Conference/Submission20980/Reviewer_E2yZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20980/Reviewer_E2yZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996972139, "cdate": 1761996972139, "tmdate": 1762939327350, "mdate": 1762939327350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}