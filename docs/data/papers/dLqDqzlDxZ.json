{"id": "dLqDqzlDxZ", "number": 16245, "cdate": 1758262271923, "mdate": 1759897252441, "content": {"title": "On the Quantization of Neural Video Codecs", "abstract": "Full-precision floating-point neural image and video codecs pose significant challenges in power consumption, storage requirements, and cross-platform interoperability, particularly when deployed on resource-constrained devices. To address these issues, network quantization techniques have been extensively studied for neural image codecs. However, the quantization of neural video codecs remains largely unexplored. Unlike quantizing neural image codecs, quantizing neural videos codecs requires significantly more effort. Many coding components operate on temporally correlated data and often rely on features propagated from previous frames, introducing additional sensitivity to both cross-platform round-off errors and quantization noise. This work presents the first systematic study of quantization effects across multiple neural video coding frameworks and temporal buffering strategies. Extensive analyzes are conducted to evaluate how various combinations of coding frameworks and temporal buffering strategies respond to various quantization schemes in terms of coding performance and computational complexity. Experimental results confirm the superiority of our mixed-precision quantization to fixed-precision quantization when they are incorporated into state-of-the-art neural video codecs. At a time when the development of neural video codecs is transitioning from maximizing rate-distortion performance to addressing practicality issues, this work offers holistic insights into key design considerations.", "tldr": "", "keywords": ["neural video codecs", "quantization", "temporal buffer"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/756947d0fb996638545d8822cf6103228bbcdb21.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents the first systematic study on quantizing neural video codecs across multiple coding frameworks and temporal buffering strategies, introducing a mixed-precision scheme that slashes complexity by 53–87 % with only 2–4 % BD-rate loss."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper systematically investigates quantization in neural video codecs across diverse frameworks and buffering strategies. The experiments comprehensively cover component-wise analysis, multiple quantization methods, and hybrid buffering for fair evaluation.\n2. The mixed-precision quantization scheme achieves up to 87 % bit-operation and 53 % model-size savings with only a 3.5 % BD-rate penalty, clearly demonstrating a practical performance–complexity trade-off.\n3. The study ties codec quantization to cross-platform reproducibility and temporal drift prevention, providing engineering insights that bridge algorithmic design and system implementation."}, "weaknesses": {"value": "1. The presentation is poor. The paper even lacks an “Abstract” heading on the first page, which is a serious formatting oversight. In addition, there are other presentation issues such as inconsistent font sizes in tables and uneven spacing around some section headings.\n2. The quantization evaluation is performed in a floating-point simulation environment without hardware validation. While standard in algorithmic studies, this setup may not capture integer rounding and overflow behaviors on real accelerators, which are crucial for verifying cross-platform consistency and deployment efficiency.\n3. The study focuses exclusively on the DCVC-FM backbone, leaving uncertainty about transferability to transformer-based or flow-matching codecs.\n4. The paper reports theoretical complexity reductions (bit operations, memory, and model size) but does not include real-device latency or energy measurements. This limits the strength of its claims about practical efficiency.\n5. The paper does not quantify how quantization-induced errors accumulate across long temporal dependencies. While short-sequence BD-rate analysis is provided, a study on long-term drift or stability would strengthen claims of robustness in real-time deployment."}, "questions": {"value": "1. Have the authors analyzed temporal error accumulation theoretically or through long-term video sequences to validate stability?\n2. How does the quantization behavior vary when applied to different backbone architectures such as DVC or FVC?\n3. What are the actual runtime and power benefits on mobile or edge devices, beyond bit-operation and model-size reductions?\n4. Have you considered using an automated bit-width allocation or mixed-precision search method (e.g., gradient-based or reinforcement learning approaches) to replace the current manually tuned configuration for better scalability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rQsKPtHPl1", "forum": "dLqDqzlDxZ", "replyto": "dLqDqzlDxZ", "signatures": ["ICLR.cc/2026/Conference/Submission16245/Reviewer_5r36"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16245/Reviewer_5r36"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816051716, "cdate": 1761816051716, "tmdate": 1762926401578, "mdate": 1762926401578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a detailed analysis of the effects of quantization on video compression models, taking into account the following factors: different coding frameworks (e.g., CC, MRC), temporal buffering strategies (Explicit, Implicit, Hybrid), quantization training  strategies (PTQ, QAT), and each decoder component. Based on this, the authors proposed a mixed-precision compression model built upon the MCR architecture, achieving a good rate–distortion–complexity trade-off."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The RD performance improvement of the proposed model MCR-Hybrid is obvious, as shown in Table 7.\n2. The experiments in Section 4 demonstrate a substantial amount of work and provide insightful implications for the design of quantized models."}, "weaknesses": {"value": "The article lacks some key experiments to demonstrate the model's effectiveness.\n1. As cross-platform consistency is the most important functionality of INT-quantized compression models, the paper lacks validation in this aspect. Specifically, it remains unclear whether encoding and decoding on different GPU models would introduce reconstruction errors.\n2. The comparison of actual encoding and decoding speed is missing in Table 7. Given that it is well known that Decoder BO, including MACs, sometimes fails to reflect the real-time computational cost [r1], such a comparison is necessary.\n3. The analysis of decoder components in Section 4.2 lacks reference to complexity metrics. For Conclusion 1, could the inter-frame main decoder suffer the most from quantization simply because it has the highest complexity?\n\n[r1] Zhaoyang Jia, Bin Li, Jiahao Li, Wenxuan Xie, Linfeng Qi, Houqiang Li, and Yan Lu. Towards practical real-time neural video compression, 2025."}, "questions": {"value": "1. In the experiments, how are non-8-bit integer quantizations such as W14/W12/W10 and A14/A12/A10 actually implemented? Do they actually facilitate hardware deployment and deliver real speed-ups?\n2. Are the training procedures described in Tables 18–20 complete? DCVC-FM is trained using sequences of up to 32 frames, whereas the authors only adopt a training strategy with up to 5 frames and achieve comparable performance. Can this shortened frame training strategy reproduce the performance of DCVC-FM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3QcEyRBnxb", "forum": "dLqDqzlDxZ", "replyto": "dLqDqzlDxZ", "signatures": ["ICLR.cc/2026/Conference/Submission16245/Reviewer_ELjB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16245/Reviewer_ELjB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894437122, "cdate": 1761894437122, "tmdate": 1762926401213, "mdate": 1762926401213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of the quantization in encoder-decoder neural video codecs. It seeks to present a systematic evaluation of quantization in neural codecs, which have been less explored than for neural image codecs. This is a problem of practical importance due to the increasing use of this class of models and the need for memory-efficient architectures. The primary contribution of the paper is a large number of ablative experiments determining how different components of neural codecs perform under varying bit-widths under uniform post-training quantization (PTQ) and quantization aware training (QAT). A supplementary contribution is the exploration of mixed-precision quantization schemes, and the analysis of temporal buffering strategies under quantization. While the paper contains a large amount of experimentation it would benefit from improved focus and organization, which would improve the strength of the contribution as a systematic evaluation and allow for the practical impact for the paper to be increased."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses an important issue. The practical deployment of neural video codecs is increasing, and there is a need for systematic analysis of memory-efficient components in these architectures. While the use of quantization in neural video codecs is not novel (the authors highlight that decoders have frequently incorporated some quantization as part of the encoder or decoder, L128-140), systematic ablation of these features is rarely performed comprehensively and is a valuable contribution. \n* The paper is well presented - with clear language, well-presented tables, and clear figures. A large amount of effort has gone into providing clear visual presentation and complete experimental details - which is appreciated. \n* The paper contains a very large number of ablative experiments. These are conducted with a large number of configurations (PTQ, QAT; decoder configurations; min-max and MSE quantization; buffering strategies), across multiple datasets. The reported metrics (BD-Rate, PSNR, bpp) / rate-distortion curves in the Appendix are appropriate for video compression. \n* While there are some issues of clarity in the characterization of Implicit / Explicit / Hybrid temporal buffers, and the broad classification of residual coding strategies - examining quantization under different architecture classes is well motivated."}, "weaknesses": {"value": "**Major** \n* The main issue in the paper is in terms of contribution and positioning. The paper positions itself primarily as a systematic study of quantization effects on neural video codecs, but also introduces some modifications based on mixed-precision quantization and temporal consistency (e.g. pg 5, L239-240, 'a hybrid approach that merges across explicit and implicit buffering strategies'). The paper may be better positioned by focusing the contribution on one aspect in detail (either a proposed new method or a systematic survey). This may additionally help focus the experiments, which while numerous are inconsistent with the contribution scope of a systematic study (e.g. L252, \"non-uniform quantization falls outside our scope\", and L255 \"... generalizing our findings to the full spectrum of quantization methods and network architectures ... is not our intent.\") or detailed evaluation of a new method. \n* The paper would benefit from including qualitative examples (e.g. comparison of the frames between the different codecs). This is common for video compression papers - and would support the qualitative comments in the paper (e.g. L349-350 \"quantization errors in the motion decoder lead to motion errors ... increasing the bitrate\" would be useful to show visually). \n* The quantization literature could be more complete. There are a large number of quantization works, so in order to position this as \"the first systematic study of quantization effects on neural video codecs\", it would be useful to put the quantization analysis in context. This is important as the quantization described is currently at an engineering level of different methods, rather than providing theoretical motivation or analysis. For example, reviewing the following may be useful to strengthen the theoretical background: \n    * Gholami et al. (2021), A Survey of Quantization Methods for Efficient Neural Network Inference, https://arxiv.org/abs/2103.13630 \n    * Gersho and Gray (1992), Vector Quantization and Signal Compression https://link.springer.com/book/10.1007/978-1-4615-3626-0 \n    * Jacob et al. (2017), Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference, https://arxiv.org/abs/1712.05877 \n\n**Minor** \n* There is a separate large line of research on the use of implicit neural representations for video compression (e.g. NeRV[1]), with a large amount of work looking specifically at quantization effects (e.g. Shi et al. (2025), On Quantizing Neural Representation for Variable-Rate Video Coding [2]). These are also occasionally referred to as neural video codecs. It may be worth including a sentence clarifying that the author's work exclusively looks at quantization effects for encoder-decoder hyperprior style codecs. \n* It would be useful to explicitly describe the weight / activation quantization bits being evaluated in the method section (the results jump into W8A10 [L309], which reads as arbitrary. The section \"Sensitivity of Decoder Components to Quantization\" [L309-319] feels like some sentences are presented out of order, making it difficult to draw clear conclusions from. \n* There are some survey papers which look at quantization of neural video codecs as components of broader analysis which may be useful to refer to (e.g. Gomes et al. (2025), \"End-to-End Neural Video Compression: A Review\", https://ieeexplore.ieee.org/document/10962175)\n\n[1] Chen et al. (2021), NeRV: Neural Representations for Videos, https://openreview.net/forum?id=BbikqBWZTGB \n\n[2] Shi et al. (2025), On Quantizing Neural Representation for Variable-Rate Video Coding, https://openreview.net/forum?id=44cMlQSreK"}, "questions": {"value": "* The characterization of the temporal correlation frameworks (Figure 2 and 3) appears to be a broad classification of existing methods. This makes it difficult to understand which components are proposed as new contributions or analysis for this paper. It would be useful if the authors could clarify this (and how this relates to the additional hybrid approach in Fig 3c). \n* It would be useful to analyse the weight and activation histogram distribution before / following quantization to better understand the sensitivity of the different components to quantization. Evaluating the MSE between the quantized / non-quantized distributions can also help understand the relative importance of bit-widths (beyond the performance level evaluation presented in the paper). E.g. [1], [2]. \n* Table 5 caption - \"X denotes the number of channels\" (X doesn't appear used within the table). \n* There are a large number of 1-2 letter acronyms (RC, CC, MCR, CRC, I, P, M, H, MP) used for tables. Where space permits, it may be beneficial to use the full names to improve readability. \n\n[1] Han et al. (2016), Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization and Huffman Coding https://arxiv.org/pdf/1510.00149 (Figure 4) \n\n[2] Zhao et al. (2019), Improving Neural Network Quantization without Retraining using Outlier Channel Splitting, https://arxiv.org/pdf/1901.09504 (Figure 1)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2qyQwxsWCD", "forum": "dLqDqzlDxZ", "replyto": "dLqDqzlDxZ", "signatures": ["ICLR.cc/2026/Conference/Submission16245/Reviewer_U8x3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16245/Reviewer_U8x3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971344746, "cdate": 1761971344746, "tmdate": 1762926400476, "mdate": 1762926400476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}