{"id": "CtvnVtWZvN", "number": 15265, "cdate": 1758249501309, "mdate": 1759897317218, "content": {"title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models", "abstract": "Reasoning language models improve performance on complex tasks by generating long chains of thought (CoTs), but this process can also increase harmful outputs in adversarial settings. In this work, we ask whether the long CoTs can be leveraged for predictive safety monitoring: *do the reasoning traces provide early signals of final response alignment that could enable timely intervention?* We evaluate a range of monitoring methods using either CoT text or activations, including highly capable large language models, fine-tuned classifiers, and humans. First, we find that a simple linear probe trained on CoT activations significantly outperforms all text-based baselines in predicting whether a final response is safe or unsafe, with an average absolute increase of $13$ in F1 scores over the best-performing alternatives. CoT texts are often unfaithful and misleading, while model latents provide a more reliable predictive signal. Second, the probe can be applied to early CoT segments before the response is generated, showing that alignment signals appear before reasoning completes. Error analysis reveals that the performance gap between text classifiers and the linear probe largely stems from a subset of responses we call *performative* CoTs, where the reasoning consistently contradicts the final response as the CoT progresses. Our findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.", "tldr": "We show that a simple linear probe on CoT activations of misaligned reasoning models can detect unsafe responses earlier and more reliably than text-based methods.", "keywords": ["reasoning language models", "safety", "safety alignment", "chain of thought reasoning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dadaaa8648bf0431e1124609a052e74fc24b2ab9.pdf", "supplementary_material": "/attachment/07f7071f3be5f8372c609ddff12f0346adac722d.zip"}, "replies": [{"content": {"summary": {"value": "The authors examine whether it is possible to predict the alignment (refusal vs non-refusal) of a reasoning model's ultimate response from its reasoning trace. They compare text-based classifiers with linear probes on hidden activations, and find that the probes are superior. They also suggest that activation-based probes can predict the response outcome many reasoning steps ahead. They also conduct error analysis to show systematic cases where the probes beat text-based classifiers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written, clear, and easy to read.\n- Thorough experiments, including multiple models, datasets, and training dataset sizes."}, "weaknesses": {"value": "- Lacks justification of important technical details\n  - The decision to train probes on the last layer of the last token position is an important design decision (which may have serious implications; see the Questions section), and lacks justification in the current manuscript.\n  - There is also no discussion of sampling methodology. It is implicitly assumed that a partial reasoning chain deterministically leads either to refusal or non-refusal. In reality, reasoning model inference uses non-deterministic sampling, and so refusals are stochastic.\n- Motivation is unclear\n  - In my opinion, the motivation is unclear. Why should we care about predicting whether models refuse before they refuse? One of the predominant methods in LLM safety is to run classifiers of the *model output* ([Sharma et al., 2025](https://arxiv.org/abs/2501.18837)), and classify the model output directly.\n- Limited to reasoning models trained via SFT\n  - SotA reasoning models are trained via RL, and it's unclear whether analysis of SFT-trained reasoning models generalize to RL-trained reasoning models. In particular, it is unclear whether the results would hold on models that were trained to reason about whether or not to refuse a request, as in [Guan et al., 2024](https://arxiv.org/abs/2412.16339)."}, "questions": {"value": "- How is the sampling done?\n  - For reasoning models, official documentation generally advises to not use temperature 0. If temperature 0 is used here, then whether a reasoning trace leads to a refusal or not is probabilistic, not deterministic. Have you thought about this? What temperature are you sampling with? Do you resample multiple times per rollout? I think the paper would be strengthened if you clarify and justify your sampling methodology.\n- Why use the last layer of the last token position? Is this a fair design choice?\n  - The probes are trained using activations from the last layer of the last token position. This is immediately preceding the unembedding, so this activation will contain information about the next token prediction. In this case, the next token prediction is very useful in order to determine whether the response will be a refusal or not (the first tokens of refusal responses are drawn from a narrow distribution of refusal phrases).\n  - I fear that this gives an unfair advantage to the probing methods over the text-based classifiers - the probing methods effectively have access to the first token of the response.\n  - One way to try and disentangle things / prove that the probe works beyond this last-token effect would be to give the first token of the response to the text classifiers, so as to try and give them an \"even playing field\".\n  - In my opinion, this issue significantly weakens the main result, namely that activation-based probes outperform text-based classifiers (section 4).\n- Do the results generalize to reasoning models that were actually trained via RL?\n  - The models are limited to reasoning models that were trained via SFT on reasoning traces. Do you expect the results to generalize to reasoning models trained with RL? There are open-source models trained with RL available, such as Qwen 3, and I think it would be worth reproducing the experiments for that model family in order to test this question.\n- Line 251: \"Text-based classifiers rely on semantic cues in the CoT to infer model behavior. For example, if a CoT includes planning steps toward an illegal request, these classifiers will likely predict a harmful outcome.\"\n  - Where is the evidence for this claim? How can we know how the text-based classifiers work? Is this speculation? If so, it'd be good to mention that this claim is speculative.\n- Missing baselines for section 5\n  - How do text-based classifiers perform in the \"future\" setting? Do probes still outperform text-based classifiers in this setting? Or does the advantage disappear?\n- Section 6 - asymmetric error analysis\n  - Did you study the opposite error analysis? E.g., are there cases where the probes systematically fail, but where the text-based classifiers do well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EdhnvMaeOu", "forum": "CtvnVtWZvN", "replyto": "CtvnVtWZvN", "signatures": ["ICLR.cc/2026/Conference/Submission15265/Reviewer_xV2h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15265/Reviewer_xV2h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761592452542, "cdate": 1761592452542, "tmdate": 1762925566787, "mdate": 1762925566787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates methods for predicting the safety alignment of language model responses, specifically focusing on reasoning language models (RLMs) that generate long chains of thought (CoTs). The core finding is that a simple linear probe trained on CoT activations significantly outperforms all text-based monitoring methods, including highly capable large language models (LLMs) and human annotators, in predicting whether a final response will be safe or unsafe. The linear probe demonstrates that alignment signals can be detected from early CoT segments, enabling potential real-time safety monitoring and intervention before the model finishes its full reasoning process. The research suggests that internal, latent representations provide a more reliable signal for safety monitoring across different RLM sizes and safety benchmarks than text-based analysis."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is easy to read and well structured. \n- This paper tackles a relevant problem that has gained significant attention recently. \n- The authors employ three relevant datasets for their experimental procedure, which seems well executed overall and the analysis of results is well conducted."}, "weaknesses": {"value": "- There seems to be an important body of literature missed in this work's background. Real time safety alignment prediction is not novel, and it's been well understood that simple linear discriminators can perform well for this task (see references below).\n- Related work focuses on Reasoning and Chain-of-Thought literature, while ignoring a large bulk of related work on controlled text generation. For example, how does this work sufficiently differ from [1] and [2] for it to be considered a worthwhile contribution? On the same note, recent work published at this conference, such as [3] also seem to incorporate a similar idea (in their case applied to domain certification, but one could argue it's a related concept).\n- One of the key findings of the paper is the possibility of real-time monitoring. However, this has been known and explored in the past.\n- Benchmark methods ignore some recent, popular baselines, such as LlamaGuard and (especially) WildGuard, which should be included in the experimental setting.\n- Overall, it seems like this paper tackles a subset of a larger problem that has been studied for a while now, with findings that have been reported in a similar fashion in other papers. The main difference I see from this paper in comparison to the existing literature I am aware of is the analysis of model alignment in the presence of Chain-of-Thought. In my opinion, this distinction should be clear in the paper and the authors should explain why there is a need to extend this concept to CoT, since at present it looks to me like a trivial extension of existing work.\n\n[1] Yang, K., & Klein, D. (2021). FUDGE: Controlled text generation with future discriminators. arXiv preprint arXiv:2104.05218.\n\n[2] Fonseca, J., Bell, A., & Stoyanovich, J. (2025). Safeguarding large language models in real-time with tunable safety-performance trade-offs. arXiv preprint arXiv:2501.02018.\n\n[3] Emde, C., Paren, A., Arvind, P., Kayser, M., Rainforth, T., Lukasiewicz, T., ... & Bibi, A. (2025). Shh, don't say that! Domain Certification in LLMs. arXiv preprint arXiv:2502.19320."}, "questions": {"value": "- The concept \"linear probes\", in the context of machine learning, that I am aware of, comes from [4]. Is this what you are actually using? It's very unclear to me whether actual linear classifier probes are being used (and how), or whether this is just a simple Logistic Regression trained over 50 PCA components using the model logits. If what is being done is the latter, then previous work have also explored this concept [5].\n- Why aren't the human evaluation results not reported in the main body of the paper as well? This comparison is posed as part of your contributions, yet it's only available in the appendix.\n- How many human annotators were used?\n\n[4] Alain, G., & Bengio, Y. (2016). Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644.\n\n[5] Krause, B., Gotmare, A. D., McCann, B., Keskar, N. S., Joty, S., Socher, R., & Rajani, N. F. (2020). Gedi: Generative discriminator guided sequence generation. arXiv preprint arXiv:2009.06367."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rq5BQN5av6", "forum": "CtvnVtWZvN", "replyto": "CtvnVtWZvN", "signatures": ["ICLR.cc/2026/Conference/Submission15265/Reviewer_5VFH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15265/Reviewer_5VFH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742670434, "cdate": 1761742670434, "tmdate": 1762925566178, "mdate": 1762925566178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to monitor the harmfulness of LRMs' responses based on partial or full CoT reasoning procedure. The authors demonstrate that directly using the CoT texts might not be a proper solution, while conducting linear probing on the model activations is a better solution. Extensive experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Monitoring the harmfulness of the LRMs' final responses based on the CoT procedure is interesting.\n- Although simple, the authors compare multiple baselines and different settings of linear probing (e.g., future-trained and present-trained)."}, "weaknesses": {"value": "- About CoT monitoring methods:\n  - I wonder what the differences are between the fine-tuned BERT classifier and the fine-tuned harmfulness classifier, since both are conducting binary classification.\n  - I'm not sure about your settings. What do you try to predict? For each CoT index, do you try to predict the harmfulness of the final response without altering the original reasoning procedure, or will you interrupt and generate an instant response at each CoT step, and then do the prediction （as lines 78-79 say?\n  -  For the fine-tuned BERT, fine-tuned harmfulness classifier, and the activation-based monitoring, how to deal with the sequence length dimension is unclear.\n  - In Table 2, the probing accuracy of s1.1-7B on XSTest is an outlier. Any idea on the reasons?\n- About early thinking:\n  - After your analysis in Sec. 5, what are your final empirical suggestions? Like, you can keep detecting the harmfulness of the CoT procedure, and if there is an alarm triggered, what should you do?\n- Overall, although simple, this is an interesting exploration of monitoring LLM CoTs. However, there are details that are still unclear. I would like to see the rebuttal for the final decisions."}, "questions": {"value": "Check the Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kzsw7E2vMz", "forum": "CtvnVtWZvN", "replyto": "CtvnVtWZvN", "signatures": ["ICLR.cc/2026/Conference/Submission15265/Reviewer_9Vch"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15265/Reviewer_9Vch"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807616549, "cdate": 1761807616549, "tmdate": 1762925565647, "mdate": 1762925565647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies whether the chain of thought of a LLM provides early insight into whether the eventual response of the model could be misaligned (unsafe). The chain of thought segment of the model is represented in two ways: in a textual manner, and as the activation (embedding) representation. The latter is shown to be an effective indicator of eventual response misalignment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "When access to a model’s activations is available, the paper demonstrates an important takeaway: the activations hold sufficient information to be predictive of eventual misalignment in long thinking or reasoning traces. This can facilitate setting up effective test-time safety guardrails"}, "weaknesses": {"value": "The analysis seems to have an unaccounted pathway for leakage of information, which influences the findings and takeaways. The *activations* at the final token position of the last layer for each partial CoT (Line 163) implicitly encode the **prompt** itself, in addition to the subsequent CoT. This leads to a few issues:\n- This potentially explains the effectiveness of the linear probe: if the prompt itself is indicative of the final misalignment of the response, the CoT segment is not required for the prediction of misalignment. And if this embedding representation of the partial CoT does include information about the prompt (since the LLM has first parsed the prompt before the CoT and encoded it within its activations/parameters) that should suffice.\n- This results in a confounding factor for evaluating the central claim of the paper: it is not *just* partial CoTs that are being tested for being predictive for eventual misalignment.\n- This invalidates the comparison with text based monitoring, since the text of the CoT by itself does not also include the prompt.\n\nIt would be helpful if the authors validate and clarify this. I am open to re-assessment if the authors have followed a procedure that differs from the description in Line 162–163. \n\nIf not, and assuming the updated claim is: activations of an LLM during its CoT processing are predictive of eventual misalignment, a drawback that arises is that such monitoring can only be applied to models whose activations are accessible, and thus not closed-source models."}, "questions": {"value": "- In Line 193, what is the value of $d$? That is, how aggressive is the dimensionality reduction with PCA?\n- How is “foresight” used for training? As per the task description, only “observed” must be used to predict eventual misalignment.\n- It is surprising to see that the prediction accuracy is already very high with even 0-10% CoT (Figure 4). How is that the case? It would be helpful if the authors can provide a couple of examples of such cases. Is this a by-product of the prompt being encoded in activations used as input to the linear probe?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yP4owcOjqO", "forum": "CtvnVtWZvN", "replyto": "CtvnVtWZvN", "signatures": ["ICLR.cc/2026/Conference/Submission15265/Reviewer_r4zw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15265/Reviewer_r4zw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947794419, "cdate": 1761947794419, "tmdate": 1762925564789, "mdate": 1762925564789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}