{"id": "CXdZIcZyTo", "number": 4610, "cdate": 1757727176944, "mdate": 1763083919703, "content": {"title": "Efficient LLM Architectures", "abstract": "Recent LLMs have hundreds of billions of parameters consuming vast resources. Furthermore, the so called \"AI scaling law\" for transformers suggests that the number of parameters must scale linearly with the size of the data.  In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, by comparing theoretical and empirical estimates of the Kullback-Liebler divergence, we derive a natural AI scaling law that the number of parameters in an efficient LLM scales as $D^{\\gamma}$ where $D$ is the size of the training data and $ \\gamma \\in [0.44, 0.72]$, suggesting the existence of more efficient architectures.  Against this backdrop, we propose recurrent transformers, combining the efficacy of transformers with the efficiency of recurrent networks, progressively applying a single transformer layer to a fixed-width sliding window across the input sequence. Recurrent transformers (a) run in linear time in the sequence length, (b) are memory-efficient and amenable to parallel processing in large batches, (c) learn to forget history for language tasks, or accumulate history for long range tasks like copy and selective copy, and (d) are amenable to curriculum training to overcome vanishing gradients. In our experiments, we find that recurrent transformers perform favorably on benchmark tests.", "tldr": "Derives natural \"AI Scaling Law\" and proposes efficient modification of transformers architecture", "keywords": ["Natural AI scaling law", "Kullback Liebler divergence", "recurrent transformer"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7550d451a0eba14dfe6360e917d547972eb95fb2.pdf", "supplementary_material": "/attachment/8f4562e16fe4bf192ddd75f25dadd585cc4f5316.zip"}, "replies": [{"content": {"summary": {"value": "This paper combines PAC learning theory and KL divergence equations to derive a new AI scaling law, which argues that the parameter count $N$ of an efficient LLM should be sublinear to the data size $D$. Based on this, the paper introduces an efficient architecture called the Recurrent Transformer. It adopts only one layer of Transformer, and the input is processed recurrently block by block. The historical hidden state is updated with a forget gate to enable forgetting, and the current block input is concatenated with the historical hidden state. Experiments show that the Recurrent Transformer excels over the standard Transformer with lower computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper derives a new AI scaling law arguing that the parameter count scales sublinearly with the data size, suggesting more efficient architectures. The theoretical analysis is useful and enlightening.\n2. The new architecture, the Recurrent Transformer, is theory-driven and well-supported by the analysis above.\n3. The Recurrent Transformer performs well on long-range copy tasks, which are a weakness for linear models."}, "weaknesses": {"value": "1. There is a critical mismatch between the claims and the experimental scale. The paper derives a scaling law, but all the experiments involve very small-scale models (the maximum size is 11M). However, the emergent abilities of modern LLMs are considered to be something that only models with billions of parameters possess. A single-layer architecture with only 11M parameters outperforming a standard transformer provides little evidence that it is also effective at scales such as 1B, 7B, or larger.\n2. The recurrent form of the transformer introduced in the paper is very similar to Transformer-XL[1]. Both combine the historical hidden state with the current input, and they both process the sequence block by block recurrently. Therefore, the Recurrent Transformer architecture may lack novelty.\n3. The baselines are not sufficient. As the Recurrent Transformer has linear complexity, it would be better to compare it against other linear-complexity models such as Mamba and GLA.\n\n\n[1] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"}, "questions": {"value": "1. Can the authors provide any evidence or at least a reasonable argument to support that 'this single-layer architecture can maintain its effectiveness at the 1B+ parameter scale' ?\n2. Why not choose Mamba or GLA as baselines? These models have extremely strong performance on long-sequence tasks and are also representatives of efficient architectures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WmvhCM6aUw", "forum": "CXdZIcZyTo", "replyto": "CXdZIcZyTo", "signatures": ["ICLR.cc/2026/Conference/Submission4610/Reviewer_aV5S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4610/Reviewer_aV5S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550813533, "cdate": 1761550813533, "tmdate": 1762917468565, "mdate": 1762917468565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Thanks for the reviews"}, "comment": {"value": "Thanks for the reviews.  \n\nTo our knowledge, this paper is the first to analyze the inherent complexity requirements of natural languages in the context of LLMs to derive a \"natural AI scaling law.\"   We show that the popular transformer architecture and the related \"AI scaling Law\" are vastly over-provisioned in that the size of the model is linear in the size of the training data, while the natural requirement is roughly the square root of the size of the training data.   We then propose and experiment with an efficient alternative to transformers.  Our experiments are of limited scale owing to resource constraints.  \n\nWe believe our theoretical results have substantial technical and business implications and suggest the reviewers weight its impact over the experiments.  Attaching an update of the paper incorporating the review suggestions. Additional feedback and questions appreciated."}}, "id": "Gr0rRN34yt", "forum": "CXdZIcZyTo", "replyto": "CXdZIcZyTo", "signatures": ["ICLR.cc/2026/Conference/Submission4610/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4610/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4610/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763083930909, "cdate": 1763083930909, "tmdate": 1763083930909, "mdate": 1763083930909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles an important problem and proposes an interesting architectural idea. The theoretical contribution is novel but needs strengthening. The experimental validation is insufficient for the claims made. With significant revisions addressing the theoretical gaps and experimental limitations, this could become a solid contribution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel theoretical analysis: The connection between Kullback-Leibler divergence and unique sequence counts provides fresh perspective on parameter scaling\n\n2. A Practical architecture: Recurrent transformers offer O(N) complexity vs O(N²) for standard transformers\n\n3. Learnable accumulation parameter: The α parameter that learns to forget/accumulate history is elegant"}, "weaknesses": {"value": "1. Theoretical Issues\n\n- Lemma 1 proof has logical gaps: The transition from dim(F) ≤ |S| to bounds on |S| relies on assumptions about optimal function spaces that aren't justified\n- Assumption 1 is oversimplified: Finite precision with uniform quantization doesn't reflect actual neural network computation\n\n\n2. Experimental Limitations\n\n- **Very limited scale**: All experiments run on 16GB Mac Mini - cannot validate claims about large-scale efficiency\n- **No comparison with recent efficient architectures**: Missing comparisons with Mamba, RWKV, RetNet, and other modern alternatives mentioned in related work\n- **Cherry-picked baselines**: Comparing against \"regular transformers\" without positional encodings, modern optimizations\n\n3. Presentation Issues\n\n- Notation inconsistency: S used for both sequence set and individual sequences\n- Missing details: How exactly is curriculum training scheduled? What are the learning rate schedules?\n- Line 33: \"sparse in that most parameters are negligible\" - needs citation or evidence\n- Line 270: Table reference formatting inconsistent\n- Figure quality: Figures 1-3 have overlapping legends and are hard to read\n- Related work: Missing discussion of recent efficient transformers, eg Mamba, RWKV, RetNet"}, "questions": {"value": "- Can you provide experiments at larger scale (>1B parameters) to validate the D^0.44-0.72 scaling law?\n- Why does the theoretical analysis assume non-duplicative training corpora when real LLM training uses multi-epoch training?\n- How does the recurrent transformer handle variable-length sequences during inference?\n- What is the actual memory footprint comparison with baselines during training and inference?\n- Can you provide ablation studies on:\n    - Block size K\n    - Number of layers\n    - Impact of α initialization\n- How does performance scale with sequence length beyond 4128 tokens tested?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IjBJ6ZCHjI", "forum": "CXdZIcZyTo", "replyto": "CXdZIcZyTo", "signatures": ["ICLR.cc/2026/Conference/Submission4610/Reviewer_JEyk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4610/Reviewer_JEyk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649259976, "cdate": 1761649259976, "tmdate": 1762917468345, "mdate": 1762917468345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an efficient LLM architecture, namely recurrent transformer. The authors first consider the PAC-learning theory and claims that the theoretical-optimal model size should not be linearly scaling with the dataset size. More efficient architecture exists. Then they proposes the recurrent transformers, empirically validated over three toy tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The good part of this research is the timely topic. Efficiency and scaling are central to current LLM research.\n2. Recurrent transformer is lightweight, simple to implement, and tested on synthetic and small real datasets."}, "weaknesses": {"value": "**On the theory side**:\n1. The authors claims better scaling exists, however, the better scaling assumes the empirical fitting is true. The better scaling does not derive from the first principle, but accounting for the empirical scaling laws. \n2. The theory doesn't direct connects to the proposed recurrent transformer architecture. Indeed, the architecture is seemingly directly combines the RNNs and Transformers.\n3. The paper defines a discrete loss $\\sum_{p_s \\neq q_s} p_s$, treating probabilities as equal/not equal. This is atypical for PAC learning and breaks continuity assumptions needed for the generalization bounds it later invokes.\n\n**On the empirical side**:\n1. Scale mismatch. Experiments run on CIFAR-10, toy copy/selective-copy tasks, and nanoGPT Shakespeare. None validate large-scale efficiency claims; results are limited to very small models.\n2. No comparisons. Extensive research on llm architectures proposed very strong baselines, such as mamba, deltaNet. However, this paper none of them, even the RNN.\n\n**Novelty and Originality**:\nI am not famililar with the line of research on LLM architectures, but the combination of RNN and Transformers is seemingly a easy-and-intuitive idea. I expect the authors to discuss the related works extensively."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kRa2E8vKVk", "forum": "CXdZIcZyTo", "replyto": "CXdZIcZyTo", "signatures": ["ICLR.cc/2026/Conference/Submission4610/Reviewer_6W5n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4610/Reviewer_6W5n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833277524, "cdate": 1761833277524, "tmdate": 1762917468101, "mdate": 1762917468101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues the “true” parameter–data scaling should grow sublinearly by tying empirical KL scaling to PAC-learning bounds, not linearly as in transformer-specific fits. It then proposes a recurrent transformer that reuses a single layer over a sliding window with a learnable memory knob to trade off forgetting vs. accumulation. The architecture claims linear-time processing in sequence length, better memory use, and plug-and-play batching while staying competitive in small-scale tests."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The sliding-window recurrence with a single reusable block is a clean, minimalist way to chase efficiency without throwing away attention.\n\n2. The “learn to forget or accumulate” knob aligns with the intuition that language vs. long-range tasks want different memory behavior."}, "weaknesses": {"value": "1. A single small window and one layer may miss cross-block interactions that deeper stacks capture implicitly. \n\n\n2. The experiments run on modest hardware and narrow tasks, leaving open how this scales to modern pretraining or multi-billion-token corpora. \n\n3. The memory knob’s behavior is shown qualitatively, but guidance on when it converges to “forget” vs. “accumulate” is thin."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "cQZqy179gM", "forum": "CXdZIcZyTo", "replyto": "CXdZIcZyTo", "signatures": ["ICLR.cc/2026/Conference/Submission4610/Reviewer_NvCo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4610/Reviewer_NvCo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076028812, "cdate": 1762076028812, "tmdate": 1762917467842, "mdate": 1762917467842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}