{"id": "OR0ySm4l9h", "number": 17708, "cdate": 1758279580328, "mdate": 1759897159254, "content": {"title": "MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models", "abstract": "Text-to-video diffusion models have enabled high-quality video synthesis, yet often fail to generate temporally coherent and physically plausible motion. A key reason is the models' insufficient understanding of complex motions that natural videos often entail. Recent works tackle this problem by aligning diffusion model features with those from pretrained video encoders. However, these encoders mix video appearance and dynamics into entangled features, limiting the benefit  of such alignment. In this paper, we propose a motion-centric alignment framework that learns a disentangled motion subspace from a pretrained video encoder. This subspace is optimized to predict ground-truth optical flow, ensuring it captures true motion dynamics. We then align the latent features of a text-to-video diffusion model to this new subspace, enabling the generative model to internalize motion knowledge and generate more plausible videos. Our method improves the physical commonsense in a state-of-the-art video diffusion model, while preserving adherence to textual prompts, as evidenced by empirical evaluations on VideoPhy, VideoPhy2, VBench, and VBench-2.0, along with a user study.", "tldr": "", "keywords": ["video diffusion models", "physical plausability"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8670876bd9ae5e9c03f0bf8298edbf518b5aaec.pdf", "supplementary_material": "/attachment/330f4d7da0cb5195232ca64dea7357c8267607c2.zip"}, "replies": [{"content": {"summary": {"value": "MoAlign addresses a critical and timely problem of poor temporal coherence and physical implausability in videos generated by text-to-video diffusion models. The authors hypothesize that this failure stems from the entanglement of appearance and motion dynamics withon the model's internal representations. To address this, the authors provide a two stage fine-tuning framework. In the first stage, a motion centric feature subspace is learned by addition of an optical flow prediction head on top of a frozen video encoder. In the second stage, the internal features of a base text-to-video model are aligned with the distilled motion subspace using a soft relational alignment loss. This procedure is designed to internalize the motion knowledge disentangled from the appearance without adding overhead at inference time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work addresses an important and timely problem. Although video models achieve generating visually plausible videos, most models lack the capability of generating physically plausible motion. \n- The two stage framework is conceptually simple and well-explained. The idea of first learning a motion-specific latent subspace and then aligning the diffusion model's representations to that subspace is innovative. This decoupling of motion from appearance is a neat solution to force the model to internalize dynamics without confusion from static content.\n- The paper's primary goal is to improve physical plausibility, and the results clearly demonstrate success. MoAlign consistently outperforms the base model, a fine-tuned baseline, and the re-implemented VideoREPA on the key \"Physical Commonsense\" (PC) and \"Joint\" metrics across both VideoPhy and VideoPhy2.\n- The paper provides a wide range of experiments: four benchmarks + a user study + ablations. The ablation studies confirm the importance of each proposed component (motion-centric features and relational alignment). This thoroughness increases confidence in the approach. The paper also discusses examples of improved physical common sense (e.g., motion of objects following expected physics) which help illustrate the benefits."}, "weaknesses": {"value": "- The paper's most significant flaw is its failure to cite, discuss, or compare against \"VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models\" by Chefer et al. (2025). This omission is critical because VideoJAM addresses the exact same problem with a similar philosophy of incorporating an explicit motion signal, but through a fundamentally different mechanism. Both MoAlign and VideoJAM identify that the standard pixel-reconstruction objective is insufficient and biases models towards appearance at the expense of motion. Both propose to solve this by explicitly incorporating a motion-based objective during training.\n- The improvements are mainly in motion consistency, as ensured by optical flow alignment. This addresses many physics issues (e.g., objects move more naturally). The paper doesn’t deeply analyze failure cases, but one can suspect that while MoAlign reduces obvious physics violations (like unnatural motion trajectories), it might still struggle with scenarios requiring understanding beyond motion (e.g., a generation might conserve motion but still violate gravity in subtle ways, etc.). A discussion of limitations would be useful.\n- Although MoAlign is effective, parts of the approach are incremental. Prior work established that aligning diffusion features with a video model can inject knowledge (VideoREPA’s token relation loss is a direct precursor). MoAlign’s novelty mainly lies in isolating motion features via flow supervision. This is a sensible extension rather than a fundamentally new paradigm. The paper could do more to highlight what is truly new. For example, the idea of a learned motion projection head and the specific loss formulation could be emphasized as key innovations. Without clear differentiation, some readers may interpret MoAlign as a minor variant of VideoREPA (when in fact it has important differences). Ensuring the contributions are framed in contrast to related methods (especially REPA methods and VideoJAM) would strengthen the impact."}, "questions": {"value": "- How does MoAlign compare with VideoJAM (Chefer et al. 2025) in approach and results? Both use optical flow to improve motion realism, VideoJAM adds a flow-prediction objective during training and uses an inner-guidance at inference.\n- Your results consistently show a drop in Semantic Adherence (Table 2) and VBench-2.0 Physics score (Table 3) for all fine-tuned models. While you attribute this to the dataset, could you discuss the possibility that the alignment process itself, by strongly regularizing the model's feature space, inherently creates a trade-off that limits general capabilities? Was a \"Pareto frontier\" of plausibility vs. fidelity observed when tuning the alignment strength $\\lambda$?\n- You report that semantic fidelity is preserved, and user studies even noted improved realism. Can you comment on whether introducing the motion alignment had any side-effects on visual quality or diversity of generations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zcAaMDfMh7", "forum": "OR0ySm4l9h", "replyto": "OR0ySm4l9h", "signatures": ["ICLR.cc/2026/Conference/Submission17708/Reviewer_evkb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17708/Reviewer_evkb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877580414, "cdate": 1761877580414, "tmdate": 1762927546011, "mdate": 1762927546011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MoAlign, to improve the physical plausibility and temporal coherence of T2V diffusion models. This framework first creates a motion-centric feature with a frozen video encoder to predict optical flow, then the diffusion model's features are aligned to this motion subspace with alignment loss. The results show improvements in physical commonsense benchmarks over baselines, without quality drops."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies a key limitation of existing representation between appearance and motion. The proposed two-stage approach is a novel and intuitive solution.\n\n2. Using optical flow as an explicit signal to capture motion-centric representation space is a logical and strong choice into the diffusion model.\n\n3. The paper validates its approach using a wide range of benchmarks."}, "weaknesses": {"value": "1. Although the effectiveness of proposed method is validated by physical-centric benchmarks, it is demonstrated exclusively on CogVideoX-2B. It is unclear if this motion alignment approach would yield similar benefits for other SoTA video diffusion architectures, which may have different internal representations.\n\n2. The claim of improved plausibility is accompanied by a drop in metrics related to motion dynamism. Specifically, MoAlign shows a large reduction in VBench's 'Dynamic Degree' (70.3 $\\rightarrow$ 42.2) and VBench-2.0's 'Dynamic Attribute' (23.8 $\\rightarrow$ 16.5). Is the model learning to be safer by simply reducing overall motion to avoid making physical mistakes?\n\n3. The model also shows a notable drop in VBench-2.0's 'Physics Score' (53.9 $\\rightarrow$ 48.8). The authors attribute this (L410) to a lack of training samples for phenomena like thermotics and materials. I wonder if the model can easily incorporate these missing physics domains via fine-tuning (e.g., SFT or MoAlign method on a few examples) or if it would require a complete retraining."}, "questions": {"value": "Refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XOGE339kJG", "forum": "OR0ySm4l9h", "replyto": "OR0ySm4l9h", "signatures": ["ICLR.cc/2026/Conference/Submission17708/Reviewer_mRuk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17708/Reviewer_mRuk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890787330, "cdate": 1761890787330, "tmdate": 1762927545558, "mdate": 1762927545558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MoAlign, a motion-centric representation alignment framework for video diffusion models. The method learns a motion-specific subspace from a pretrained video encoder by supervising its projection to predict optical flow, and then aligns diffusion features to this subspace through a soft relational alignment loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**1) Relevance and Timeliness**\n\nMotion-centric alignment addresses a timely limitation in large video diffusion models, namely the lack of explicit motion understanding, and the approach fits well within the ongoing trend of representation-based fine-tuning.\n\n**2) Clear Methodology**\n\nThe two-stage design (motion feature learning → diffusion feature alignment) is logically presented and easy to follow, providing a clean conceptual link between motion representation learning and diffusion adaptation.\n\n**3) Comprehensive Evaluation** \n\nThe paper conducts extensive experiments on diverse benchmarks and includes both quantitative metrics and human preference studies, demonstrating consistent improvements in motion and physical realism."}, "weaknesses": {"value": "**1) Literature Coverage**\n\nWhile the introduction claims that prior alignment-based methods mostly focus on visual semantics rather than true motion dynamics, the paper overlooks several recent studies that explicitly target motion-aligned or dynamics-centric representations (e.g.[1], [2]). These works similarly aim to internalize motion dynamics rather than relying on appearance cues. Clarifying how the proposed framework surpasses such motion-aware alignment approaches would help position this work more precisely within the current landscape and better substantiate its claimed novelty and advantage.\n\n**2) Efficiency and Practical Overhead**\n\nAlthough the paper emphasizes that the proposed framework internalizes motion understanding without external simulators or inference-time conditioning, the Stage 1 training involves over 50 K iterations using multiple H100 GPUs. This introduces substantial computational overhead and weakens the claim of simplicity or efficiency. It would be informative to discuss whether this stage truly needs such heavy training to be effective. For instance, how does performance change if the motion projector is trained with fewer iterations or a lighter backbone? \n\n**3) Limited Discussion on Motion Representation Across Layers**\n\nPrior alignment-based works (e.g., REPA, VideoREPA) have shown that the choice of layer for alignment is a critical design factor, often focusing on mid or multiple layers and providing useful insights into how different layers contribute to the denoising process and what types of information they encode. In this work, however, the alignment is applied to a single layer without further discussion, and despite the paper’s focus on learning motion-related representations, it lacks sufficient analysis or discussion of how the aligned layer captures motion information. It would strengthen the paper to elaborate on what insights this approach provides regarding how motion-related representations are distributed across layers, as this understanding could offer a deeper contribution to motion-centric representation learning.\n\n[1] *Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation*  \n[2] ​​*VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models*"}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FGGGsk7AM4", "forum": "OR0ySm4l9h", "replyto": "OR0ySm4l9h", "signatures": ["ICLR.cc/2026/Conference/Submission17708/Reviewer_DP4f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17708/Reviewer_DP4f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017039649, "cdate": 1762017039649, "tmdate": 1762927545082, "mdate": 1762927545082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}