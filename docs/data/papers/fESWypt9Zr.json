{"id": "fESWypt9Zr", "number": 1664, "cdate": 1756902900666, "mdate": 1759898196184, "content": {"title": "Phase-guided Perceptual Alignment for Multi-source Multi-modal Domain Adaptation", "abstract": "Multi-Source Multi-Modal Domain Adaptation (MSM$^2$DA) is a method that leverages data from multiple sources and modalities to train machine learning models capable of generalizing well across various domains.  Existing MSM$^2$DA methods mostly use structural semantic alignment by visual data to enhance the correlation between different modality data, while neglecting the low-frequency perceptual shifts in visual data that hinder cross-modal fusion. However, visual data are particularly sensitive to domain shifts including low-level semantics such as style and illumination variations. To handle this problem, we propose Phase-guided Perceptual Alignment (PGPA) to align the visual styles by transferring low-frequency spectral components from target to source images while preserving high-frequency semantic structures.  Specifically, PGPA decomposes images into amplitude and phase spectra in the Fourier domain, where the amplitude captures style-related low-level statistics  and the phase retains high-level structural semantics. By selectively blending the amplitude of the target image with the phase of the source image, our method improve diversity and ensures domain-invariant style adaptation without distorting critical semantic details. Furthermore, we provide a bound proof that formalizes the effectiveness of our approach, demonstrating that PGPA guarantees improved cross-domain generalization within a specified bound and ensuring theoretical validity. Extensive experiments demonstrate that our approach significantly improves cross-domain generalization tasks.", "tldr": "PGPA", "keywords": ["Multi-Source Multi-Modal Domain Adaptation", "Phase-guided Perceptual Alignment", "Semantic Structures", "Domain-invariant Style"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2705b1c46367010645ccb62c013471ef34fd37b6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- Task: Multi-Source Multi-Modal Domain Adaptation (MSM2DA)\n- Objective: To enhance generalization performance toward the target domain by leveraging multiple source domains and diverse modalities (e.g., image, text, LiDAR)\n- Among the various modalities, the paper focuses particularly on the visual modality, which is highly sensitive to domain differences. The authors propose a Phase-guided Perceptual Alignment (PGPA) method that transfers the low-frequency components (amplitude in the -Fourier domain) of the target domain to the source domain.\n- The performance improvement achieved by the proposed PGPA method is theoretically validate."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper proposes a training-free and architecture-independent approach that aligns styles by applying the low-frequency components of the target domain samples to the source domain samples."}, "weaknesses": {"value": "- The paper lacks sufficient novelty.\n   - All loss functions introduced in Section 3.3 (CMCFA, CDCFA, CDAFA, and UACR) were originally proposed in the existing benchmark [1]; thus, the only new contribution for improving MSM2DA performance is the proposed PGPA method.\n- The approach shows similarity to prior work [2], which also performs domain adaptation by applying the low-frequency components (amplitude in the Fourier domain) of the target domain to the source domain.\nTypographical and formatting issues\n   - Broken text in Figure 2.\n   - Inconsistent use of commas and periods when closing equations.\n   - Line 205: should be $z_s^{(i)}$ to $\\tilde{z}_s^{(i)}$\n   - Line 432: should refer to Table 2 instead of Figure 2.\n   - Line 456: should refer to Figure 3 instead of Table 3.\n   - Two panels labeled “(b)” appear in Figure 3.\n      - The horizontal and vertical axes in the graphs need to be clearly labeled.\n\n---\n[1] Zhao, Sicheng, et al. \"Multi-source multi-modal domain adaptation.\" Information Fusion 117 (2025): 102862.\n\n[2] Yang, Yanchao, and Stefano Soatto. \"Fda: Fourier domain adaptation for semantic segmentation.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020."}, "questions": {"value": "- A comparison with the prior work [1], which also performs domain adaptation by applying the low-frequency components (amplitude in the Fourier domain) of the target domain to the source domain, is needed.\n- Although the paper addresses a multi-modal task, the proposed method is applied only to the image modality. It is unclear whether this limited application is sufficient to demonstrate its effectiveness.\n- In Figure 3 (a), the red line is described as a LOESS Smooth Curve—is this a typographical error, and should it instead read Loss Smooth Curve?\n- It would be helpful to include additional image samples demonstrating the effects of the proposed PGPA method.\n\n---\n[1] Yang, Yanchao, and Stefano Soatto. \"Fda: Fourier domain adaptation for semantic segmentation.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sRCnGdESpp", "forum": "fESWypt9Zr", "replyto": "fESWypt9Zr", "signatures": ["ICLR.cc/2026/Conference/Submission1664/Reviewer_iBM3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1664/Reviewer_iBM3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904247651, "cdate": 1761904247651, "tmdate": 1762915849751, "mdate": 1762915849751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a phase-guided perceptual alignment method for multi-source multi-modal domain adaptation. The method decomposes images into amplitude and phase spectra in the Fourier domain. The amplitude captures style-related low-level statistics, while the phase retains high-level structural semantics. To achieve domain adaptation, the amplitude of the target image is blended with the phase of the source image."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a phase-guided perceptual alignment method to align visual styles between different domains.\n- Good experimental results are achieved on aesthetics assessment and sentiment analysis datasets."}, "weaknesses": {"value": "- The paper's novelty is limited, and the related work section lacks sufficient discussion. Many existing early works [1] [2] already use Fourier transform to blend the amplitude and phase of target and source images for reducing domain shift.\n- The compared methods are outdated. Except for M2CAN, most of the baseline methods are from before 2023.\n- The proposed method is generic but only evaluated on aesthetics assessment and sentiment analysis tasks. Evaluation on more general domain adaptation tasks is needed.\n- What is the key challenge of multi-source domain adaptation compared to single-source domain adaptation? How does the proposed method specifically address multi-source issues?\n- The proposed method does not specifically address multi-modal domain adaptation. It only focuses on visual modality. Therefore, it is not appropriate to include \"multi-modal\" in the paper's title.\n- The presentation needs improvement. For example, Figure 2 is not clearly illustrated.\n\n[1]Yang Y, Soatto S. Fda: Fourier domain adaptation for semantic segmentation. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition 2020 (pp. 4085-4095).\n\n[2]Xu Q, Zhang R, Zhang Y, Wang Y, Tian Q. A fourier-based framework for domain generalization. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition 2021 (pp. 14383-14392)."}, "questions": {"value": "- What are the key difference between the proposed method and existing Fourier domain adaptation method?\n-  What is the key challenge of multi-source domain adaptation compared to single-source domain adaptation? How does the proposed method specifically address multi-source issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "StIO35eVF2", "forum": "fESWypt9Zr", "replyto": "fESWypt9Zr", "signatures": ["ICLR.cc/2026/Conference/Submission1664/Reviewer_cuXh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1664/Reviewer_cuXh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906896275, "cdate": 1761906896275, "tmdate": 1762915849501, "mdate": 1762915849501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Phase-guided Perceptual Alignment (PGPA) for Multi-Source Multi-Modal Domain Adaptation (MSM2DA). MSM2DA aims to enhance machine learning models by utilizing data from multiple sources and modalities to generalize across domains. While existing methods focus on aligning high-level semantic structures in visual data, they overlook low-frequency perceptual shifts like style and illumination variations that hinder cross-modal fusion. PGPA addresses this by transferring low-frequency spectral components (style) from target images to source images while preserving high-frequency semantic content. Using Fourier transform, PGPA blends the amplitude (style) of target images with the phase (semantic details) of source images, ensuring domain-invariant style adaptation. The paper also provides a formal proof to demonstrate the effectiveness of PGPA, showing its ability to improve cross-domain generalization. Extensive experiments validate that PGPA significantly enhances performance in cross-domain tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed PGPA method is new in its approach of using Fourier transformations to handle perceptual discrepancies in the visual modality. By targeting low-frequency components for domain alignment, the method improves domain adaptation stability.\n\n- The paper provides a formal proof demonstrating the effectiveness of PGPA in improving cross-domain generalization. This theoretical underpinning strengthens the validity of the proposed approach.\n\n- PGPA improves the fusion of visual and other modalities by aligning visual styles without affecting semantic structures, ensuring better cross-modal interaction and more accurate feature extraction."}, "weaknesses": {"value": "- While PGPA addresses low-frequency perceptual discrepancies in the visual modality, the paper does not explore how it handles more complex, high-frequency domain shifts or shifts in non-visual modalities, which may limit its broader application.\n\n- Although the paper performs experiments on benchmark datasets, there is little discussion of how the method would perform on real-world, noisy, and unstructured datasets. \n\n- The ablation study suggests that the method's performance is sensitive to the choice of the hyperparameter β, which may require extensive tuning for different datasets or applications. This sensitivity could be a barrier for practical deployment.\n\n- The use of Fourier transform in the alignment process adds computational complexity, especially when applied to large-scale data in real-time scenarios. The comparison or analysis on the computational complexity or runtime would be better presented.\n\n- While PGPA performs well against the baseline methods, the comparison to other advanced multi-modal domain adaptation techniques is somewhat limited. There is no baseline from 2022 to 2024. A more extensive discussion would be better presented."}, "questions": {"value": "How does the proposed PGPA method handle potential conflicts when aligning the low-frequency components from the target domain with the high-frequency structures of the source domain, particularly in more complex multi-modal datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z9JV9UdfYA", "forum": "fESWypt9Zr", "replyto": "fESWypt9Zr", "signatures": ["ICLR.cc/2026/Conference/Submission1664/Reviewer_ijRw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1664/Reviewer_ijRw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972656655, "cdate": 1761972656655, "tmdate": 1762915849141, "mdate": 1762915849141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}