{"id": "7mfPCqVMW3", "number": 16364, "cdate": 1758263754658, "mdate": 1763717535697, "content": {"title": "Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL", "abstract": "Reinforcement learning (RL) has recently become the dominant paradigm for strengthening the reasoning abilities of large language models (LLMs). Yet the rule-based reward functions commonly used on mathematical or programming benchmarks assess only answer format and correctness, providing no signal as to whether the induced Chain-of-Thought (CoT) actually improves the answer. Furthermore, such task-specific training offers limited control over logical depth and therefore may fail to reveal a model’s genuine reasoning capacity. We propose **D**ynamic **R**easoning **E**fficiency **R**eward (**DRER**) — a plug-and-play RL reward framework that reshapes both reward and advantage signals. (i) A **Reasoning Quality Reward** assigns fine-grained credit to those reasoning chains that demonstrably raise the likelihood of the correct answer, directly incentivising the trajectories with beneficial CoT tokens. (ii) A **Dynamic Length Advantage** decays the advantage of responses whose length deviates from a validation-derived threshold, stabilising training. To facilitate rigorous assessment, we also release LogicTree, a dynamically constructed deductive reasoning dataset that functions both as RL training data and as a comprehensive benchmark. Experiments show significant improvements in inference accuracy and logical consistency over the baseline methods at equal training steps, while the average confidence of CoT-augmented answers rises by 30%. The model further exhibits generalisation across diverse logical-reasoning datasets, and the mathematical benchmark AIME24. These results illuminate how RL shapes CoT behaviour and chart a practical path toward enhancing formal-reasoning skills in large language models. All code and data are available in our anonymous repository https://anonymous.4open.science/r/DRER-D34E.", "tldr": "", "keywords": ["Reinforcement Learning for LLMs", "Reasoning", "Reductive Logic", "Benchmark"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/46437cc5d4d78d56cbb36cd4c7df25402b049865.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel reinforcement learning framework, Dynamic Reasoning Efficiency Reward (DRER), aimed at improving the quality of chain-of-thought reasoning in large language models. The key contributions are twofold. First, the DRER framework proposes a more nuanced reward signal that goes beyond simple answer correctness. It incorporates a \"Reasoning Quality Reward\" to credit reasoning steps that increase the likelihood of the correct answer, and a \"Dynamic Length Advantage\" to regulate the length of generated reasoning chains, which helps stabilize training. Second, the authors introduce LogicTree, a new benchmark dataset for deductive reasoning that is programmatically constructed. This dataset serves both as training data and as a tool for evaluating models in a controlled environment that focuses on logical structure over domain knowledge.\n\nThe core idea of rewarding reasoning chains that are demonstrably helpful is a significant step up from simply rewarding correct final answers. This is well-supported by strong experimental results showing that their method not only improves performance on their own benchmark but also generalizes to other established reasoning datasets. The ablation studies and analyses provide convincing evidence for the effectiveness of the different components of their proposed framework.\n\nThe LogicTree dataset is another major strength. By programmatically generating problems and controlling for logical depth, the authors provide a much-needed tool for disentangling pure reasoning ability from domain knowledge. While I have some questions about the generation process (see below), the dataset itself is a valuable resource for the research community.\n\nOverall, the paper is well-written, the ideas are novel, and the empirical evaluation is thorough. The work opens up interesting avenues for future research on improving the reliability and transparency of reasoning in LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Strong Points:**\n\n*   **Novel Reward Framework:** The proposed DRER framework is a significant contribution. Moving beyond binary correctness to reward intermediate reasoning steps that are causally beneficial to the final answer is a promising direction for improving LLM reasoning.\n*   **Practical Training Improvements:** The Dynamic Length Advantage is a practical and useful technique for controlling output length and stabilizing RL-based training of LLMs for reasoning tasks.\n*   **Valuable New Dataset:** The LogicTree dataset is a strong contribution to the community. Its programmatic nature and focus on deductive reasoning provide a valuable tool for rigorously assessing and training the formal reasoning capabilities of models, separate from their factual knowledge.\n*   **Strong Empirical Results:** The paper presents compelling experimental results. The proposed method shows significant improvements in accuracy and logical consistency over baselines. The demonstration of generalization to other logical and mathematical reasoning benchmarks is also a key strength.\n*   **Insightful Analysis:** The analysis of how CoT reasoning impacts the model's predictions (e.g., when it is most effective at correcting a wrong answer) provides valuable insights into the behavior of these models."}, "weaknesses": {"value": "**Weak Points:**\n\n*   **Clarity on Dataset Generation:** The paper could benefit from a more detailed explanation of the lexicalization process used to create the LogicTree dataset. It is not entirely clear how the abstract logical rule trees are translated into natural language.\n*   **Potential for Linguistic Ambiguity:** The process of converting logical forms to natural language is fraught with potential ambiguities (e.g., scalar implicatures, quantifier scope). The paper does not discuss whether these issues are addressed in the dataset generation process.\n*   **Lack of Human Validation:** There is no mention of human validation for the LogicTree dataset. It would be valuable to know if human subjects consistently interpret the natural language problems in a way that aligns with the intended underlying logical structure.\n*   **Minor Errors:** There are a few minor errors in the paper, such as a missing citation for a benchmark and some incorrect terminology for logical rules in a table."}, "questions": {"value": "1.  Could you please provide more detail on the lexicalization process for the LogicTree dataset? Specifically, how are the abstract logical rules and entities mapped to natural language sentences?\n2.  How do you ensure that the generated natural language statements are not subject to linguistic ambiguities, such as scalar implicatures (e.g., the interpretation of \"or\")? Have you considered or tested for such effects?\n3.  Have you conducted any human evaluation studies on the LogicTree dataset? It would be very helpful to see data on how consistently human participants interpret the problems and whether their interpretation aligns with the intended logical form.\n\nThe following are some minor suggestions for improvement:\n\n*   Please add the missing reference for the \"Math500 benchmark\" on page 4.\n*   In Table 1, the names of the logical rules appear to be incorrect. For example, what is labeled \"Conjunction introduction\" seems to be the \"Constructive Dilemma,\" and \"conjunction elimination\" appears to be \"proof by cases.\" Please double-check this terminology.\n*   In Figure 2, the \"question\" is technically a statement to be evaluated for its truth value. It might be clearer to label it as \"Statement\" or \"Claim\" to avoid confusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q4S6NPTLmJ", "forum": "7mfPCqVMW3", "replyto": "7mfPCqVMW3", "signatures": ["ICLR.cc/2026/Conference/Submission16364/Reviewer_Vyr9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16364/Reviewer_Vyr9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894707191, "cdate": 1761894707191, "tmdate": 1762926490277, "mdate": 1762926490277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DRER (Dynamic Reasoning Efficiency Reward), a plug-and-play reinforcement learning (RL) framework designed to improve the quality and efficiency of Chain-of-Thought (CoT) reasoning in large language models (LLMs). Unlike conventional RL approaches that reward only final answer correctness, DRER incorporates:\n1.\tReasoning Quality Reward: Measures whether the CoT tokens increase the model’s confidence in the correct answer by comparing log-likelihoods with and without CoT.\n2.\tDynamic Length Advantage: Penalizes responses that are significantly longer or shorter than a validation-derived length threshold, promoting concise and stable reasoning.\nTo support training and evaluation, the authors also release LogicTree, a synthetically generated dataset of nested deductive reasoning problems with controllable depth, logical diversity, and intermediate sub-questions.\nExperiments on LogicTree and other benchmarks show that DRER improves accuracy, logical consistency, confidence, and token efficiency over baselines like GRPO and DAPO."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe Reasoning Quality Reward directly ties intermediate reasoning steps to answer confidence. The Dynamic Length Advantage may stabilize training and reduce verbosity.\n2.\tAnalyses on various aspects of models’ reasoning capabilities."}, "weaknesses": {"value": "1.\tLogicTree focuses exclusively on propositional deductive reasoning. What about inductive, abductive, analogical, or higher-order reasoning?\n2.\tThe paper compares DRER-augmented training against baselines (GRPO/DAPO). Readers would be interested in component-wise ablation study. For example: does the length regularization alone improve accuracy, or is it only effective when combined with the quality reward?\n3.\tThe paper could consider varying the degree of distributional shift (e.g., by perturbing LogicTree rules, mixing domains, or introducing noise). It’s unclear whether DRER’s gains are robust to OOD conditions, or if they merely reflect superficial transfer within a narrow reasoning regime.\n4.\tThere are some typos in the article, such as citations in lines 44-58."}, "questions": {"value": "1.\tCan DRER be extended to reasoning tasks where ground-truth answers are less well-defined?\n2.\tDoes encouraging higher confidence in answers via CoT increase the risk of overconfident hallucinations when the premises are insufficient?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "G9oGITjtuZ", "forum": "7mfPCqVMW3", "replyto": "7mfPCqVMW3", "signatures": ["ICLR.cc/2026/Conference/Submission16364/Reviewer_cjaS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16364/Reviewer_cjaS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914646356, "cdate": 1761914646356, "tmdate": 1762926489199, "mdate": 1762926489199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new reinforcement learning framework for reasoning enhancement in large language models (LLMs).\nThe method introduces a Reasoning-Quality Reward (RQR) that measures the average log-likelihood difference between the model’s correct answer under reasoning (CoT) and direct answering (No-CoT) modes.\nA positive difference implies that reasoning increases the model’s confidence in the correct answer, which is rewarded via a tanh-based scaling.\nAdditionally, the paper adds a Dynamic Length Advantage (DLA) term that regularizes overly long or short reasoning chains based on length statistics collected from validation rounds.\nExperiments on a self-constructed LogicTree dataset and several reasoning benchmarks (AIME24, MMLU-redux, ProntoQA) show improved reasoning efficiency and robustness compared with GRPO and DAPO baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposes a clear and interpretable framework to assess whether reasoning helps model confidence.\n\n2. Combines reasoning-quality reward with a dynamic length regularization to stabilize training.\n\n3. Experiments are controlled, fair, and internally consistent.\n\n4. Writing and visualization are of high quality.\n\n5. Provides an interesting diagnostic tool for analyzing reasoning effectiveness in LLMs."}, "weaknesses": {"value": "1. The log-likelihood difference is a self-referential reward; it optimizes internal belief alignment rather than genuine reasoning improvement.\nThis is conceptually close to confidence calibration and may lead to reward hacking or “self-confirmation loops.”\n\n2. Dynamic Length Advantage is highly similar to prior adaptive length penalties. The paper does not provide an ablation proving the unique benefit of its specific formulation.\n\n3. The approach requires ground-truth answers and double forward passes, limiting scalability and applicability to open-ended reasoning tasks.\n\n4. Gains on real reasoning benchmarks are modest; LogicTree is synthetic and may not reflect real reasoning complexity.\n\n5. Related Work misses critical discussion of existing “self-consistency” or “critique-guided” RL frameworks (e.g., CFT, CRL, Self-Refine)."}, "questions": {"value": "1. The “Reasoning Quality Reward” is computed from the log-likelihood gap between reasoning and direct-answer modes.\nCan you show that this signal tracks true reasoning correctness rather than just higher output confidence?\nFor instance, do higher rewards actually correspond to more logically valid reasoning chains?\n\n\n2. Since the reward depends on two separate generations per example, it may fluctuate.\nHow stable is this log-likelihood difference during training?\nDid you observe large variance across steps or seeds that could destabilize learning?\n\n\n3. Have you checked whether the model’s reasoning becomes objectively better when judged by an external verifier or a stronger model (e.g., GPT-4 or a logic checker)?\nThat would help confirm the reward improves reasoning, not just internal consistency.\n\n\n4. The length adjustment idea resembles prior work like Adaptive Length Penalty, What is genuinely new here, and how much does your version contribute beyond those methods?\n\n\n5. How could it generalize to open-ended questions or dialogue, where no ground truth exists?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0Oc8xhoiR0", "forum": "7mfPCqVMW3", "replyto": "7mfPCqVMW3", "signatures": ["ICLR.cc/2026/Conference/Submission16364/Reviewer_T1yU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16364/Reviewer_T1yU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968781568, "cdate": 1761968781568, "tmdate": 1762926488843, "mdate": 1762926488843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The core contribution of the paper is the proposal of a process-based reward mechanism, which promotes better reasoning by giving more reward to effective intermediate reasoning steps (CoTs), rather than relying solely on rule-based evaluation of the final answers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Good Research Question: Only RLVR may not be enough to supervise model for reasoning, so more reward on CoT has the potential to be better."}, "weaknesses": {"value": "1. The main results of the paper **rely on the LogicTree training set constructed by the authors and are evaluated using its corresponding test set**, which imposes significant limitations.\n\nIt is unclear **why existing open-source reasoning datasets could not be adapted instead**？ \n\nRelying solely on LogicTree as the training set **does not sufficiently support the claimed “plug-and-play” capability**.\n\n2. Results on Table 4 should be in the place of Table 1 because results from these benchmarks are more reliable on reflecting reasoning abilities and cared about by the community.\n\n3. Typo: Line 190, missing citation"}, "questions": {"value": "1. What is the relationship between this method and your LogicTree dataset? Could this method be applied while leveraging existing open-source reasoning datasets, rather than being restricted to constructing LogicTree?\n\n2. the main experimental results should be evaluated across a variety of commonly used logic/reasoning datasets and compared with more methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4LH5dHGOOM", "forum": "7mfPCqVMW3", "replyto": "7mfPCqVMW3", "signatures": ["ICLR.cc/2026/Conference/Submission16364/Reviewer_qz22"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16364/Reviewer_qz22"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980420239, "cdate": 1761980420239, "tmdate": 1762926486891, "mdate": 1762926486891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}