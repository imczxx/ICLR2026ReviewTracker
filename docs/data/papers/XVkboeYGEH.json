{"id": "XVkboeYGEH", "number": 15038, "cdate": 1758247134315, "mdate": 1759897334022, "content": {"title": "Parameter-Efficient Multi-Source Domain-Adaptive Prompt Tuning for Open-Vocabulary Object Detection", "abstract": "Cross-domain open-vocabulary object detection (COVD) poses a unique and underexplored challenge, requiring models to generalize across both domain shifts and category shifts. To tackle this, we propose MAP: a parameter-efficient Multi-source domain-Adaptive Prompt tuning framework that leverages multiple labeled source domains to improve detection in novel, unlabeled target domains with unseen categories. MAP consists of two key components: Multi-Source Prompt Learning (MSPL) and Unsupervised Target Prompt Learning (UTPL). MSPL disentangles domain-invariant category semantics from domain-specific visual patterns by jointly learning shared and domain-aware prompts. UTPL enhances generalization in the unlabeled target domain by enforcing prediction consistency under text-guided style augmentations, introducing a novel entropy-minimization objective without relying on pseudo-labels. Together, these components enable effective alignment of visual and textual representations across both domains and categories. In addition, we present a theoretical analysis of the proposed prompts, examining their behavior through the lenses of fidelity and distinction. Extensive experiments on challenging COVD benchmarks demonstrate that MAP achieves state-of-the-art performance with significantly fewer additional parameters.", "tldr": "", "keywords": ["domain shift", "open-vocabulary", "object detection"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a641d97e15dcadd4db040cb59e7f51a47b72090c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method called MAP for Cross-domain open-vocabulary object detection where model is trained on multiple source domains and aims to detect novel objects in the unseen target domain. MAP includes 2 main modules: Multi-Source Prompt Learning (MSPL) and Unsupervised Target Prompt Learning (UTPL). MSPL learns both domain-specific and class specific soft prompts while UTPL aims to improve performance on novel objects in target domain through minimizing entropy. \n\nPaper conduct comprehensive experiments on artistic and adverse-weather domain adaptation benchmarks and compare against standard prompt tuning adaptation methods, multiple Open-Vocab object detection models, and multi-source cross domain adaptation related works. Experimental results shows that the proposed method achieves state of the arts compared to benchmarks and ablation studies shows that UTPL, specifically the orthogonal loss on novel classes can significantly improve performance on novel objects while MSPL objectives mainly help with cross-domain generalization"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1- Paper studies is a more challenging setting: Cross-domain open-vocabulary object detection where the model requires generalization to unseen domains while detecting novel objects. This setting better resembles practical and real-world scenarios. \n\n2- Results are comprehensive on different types of domain shift, showcasing their effectiveness\n\n3- Model achieves sota performance compared to all related works, including standard single-source DA methods, open-vocab models, and multi-source DA prompt tuning approaches.\n4- Experimental setup is comprehensive and includes multiple architectures such as regionCLIP, YOLO-World, OVMR, and GDINO, showcasing the effectiveness and generalizability of results and findings."}, "weaknesses": {"value": "Weaknesses\n\n**Single Source DA baselines experimental setup is unfair**\n (1) Single-source DA methods (e.g., DAPL) are trained on only one source domain, while the proposed MAP method is trained on multiple source domains. Hence, this comparison may not showcase the effectiveness of the proposed method (i.e., MSPL and MSPL+UTPL) compared single-source baselines. Hence, its more fair to train single source domains on a *combined source domains*  (i.e. combine data from multiple sources and consider them as 1 source domain) like [1] and/or train MAP( MSPL , MSPL+UTPL) on a single source DA setup for a fair comparison.  \n\n(2) MAP uses domain-specific and class-specific prompts. How’s the prompt designed in baselines like DAPL? Many baseline methods like Coop , DAPL, etc, can naturally be expanded to use class-specific learnable prompts. For instance, DAPL already has its extension in their paper (see Eq. 6). Hence, clarifying the experimental setting for baselines and whether class-specific prompts are used could better help to evaluate the effectiveness of the proposed method. Especially because MSPL is highly similar to DAPL approach \n\n\nWhile UTPL significantly boosts the performance on novel objects, both the prompt design (i.e., using class-specific prompts) and L_novel objective require the availability if C_novel labels during the training. While I acknowledge that the exact annotation for novel categories is not used during the training, the proposed method may be ineffective when novel objects are unknown during the training time.  I’d appreciate if this could be further clarified, as UTPL is the main distinction from prior works (i.e. not including pseudo labels in DA methods). \n\n\n\nCLIPStyler suffers from distorting content and introducing artifacts that can impede its usability in downstream tasks like object detection and segmentation (as shown in examples Fig.3 in the appendix and [6])  can hurt the performance, especially on object detection. While I appreciate the comparison to different augmentation methods (e.g., CycleGan) in the appendix that shows relatively comparable performance, more comparison against more current diffusion-based style transfer models could be helpful, which do not have some of the limitations of traditional approaches like CycleGan and can be used off the shelf. There are many diffusion-based editing or style transfer methods, some examples: [2,3,4,5]\n\n\nSome minor issues:\n\nL_novel in Eq12 is only based on novel class prompts and base class prompts. However, in Fig.1(b) there is a connection between L_novel and target images. So the connection should be removed, or please clarify. \n\nIn Table.2, YOLO-World, for DR SAP (38.42) should be bolded. \n\n\n\nOverall, I believe the paper is studying an important problem and the method is shown to be effective. Hence, addressing the above comments and questions can help better understand the effectiveness and contribution of the method, its contributions, and limitations. Given the rebuttal, I'm open to adjusting my score.\n\n[1] Semantic-aware adaptive prompt learning for universal multi-source domain adaptation\n[2]  Khawar Islam et al., Diffusemix: Label Preserving Data Augmentation with Diffusion Models, CVPR, 2024.\n[3] Generalization by Adaptation: Diffusion-Based Domain Extension for Domain-Generalized Semantic Segmentation, Wacv24\n[4] Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer, ICCV'23\n[5] InstructPix2Pix: Learning to Follow Image Editing Instructions, CVPR'23\n[6] PØDA: Prompt-driven Zero-shot Domain Adaptation"}, "questions": {"value": "- In UTPL target domain and its style transferred versions are used. What would be the performance if corresponding objectives are only applied to the target image? i.e., excluding the CLIPStyle augmentation module? This could better help the effectiveness of this component as it adds some computational overhead. \n- How would the model perform when faced with an “unseen novel category” during inference? \n- How would using a diffusion-based augmentation method impact the performance?\n- [minor] Currently, the distinction of paper (especially MSPL and theoretical analysis) is vague to prior works, especially in the related work section. While theoretical analysis is still interesting even if adopted from another work, clearly explaining differences could better help readers to understand the paper and its contributions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dXnJGf8vId", "forum": "XVkboeYGEH", "replyto": "XVkboeYGEH", "signatures": ["ICLR.cc/2026/Conference/Submission15038/Reviewer_eVdX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15038/Reviewer_eVdX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740825800, "cdate": 1761740825800, "tmdate": 1762925363010, "mdate": 1762925363010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies cross-domain open-vocabulary object detection (OVOD), a challenging problem that requires models to generalize across both domain shifts and category shifts. To jointly address these two types of shifts, the authors propose MAP, a parameter-efficient Multi-source domain-Adaptive Prompt tuning framework.\n\nThe proposed MAP framework consists of two core components: (1) Multi-Source Prompt Learning (MSPL), which learns both shared and domain-aware prompts from multiple source domains; (2) Unsupervised Target Prompt Learning (UTPL), which encourages prediction consistency across augmented target images.\n\nExtensive experiments on multiple benchmark datasets demonstrate that MAP achieves state-of-the-art performance. The paper also provides theoretical analysis to support the proposed design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses the joint challenge of domain shifts and category shifts in cross-domain OVOD through a well-structured prompt-based learning framework. The introduction of MSPL and UTPL offers a unified approach to handle both adaptation and generalization.\n\nExperimental results are comprehensive, covering multiple benchmarks, and the method consistently outperforms prior approaches. Theoretical justifications further enhance the credibility of the framework."}, "weaknesses": {"value": "While the idea of UTPL contributes to the overall framework, its novelty is limited. The use of prediction consistency across augmented views as an unsupervised signal is common in both unsupervised domain adaptation (UDA) and unsupervised prompt tuning literature, as in prior works such as [a]. The main distinction here lies in the augmentation strategy, where the authors employ CLIPstyler to generate augmented samples. However, the paper does not clearly state whether these augmentations are generated dynamically during training or precomputed. If they are generated online, the additional computational cost should be explicitly discussed. Moreover, comparisons with alternative augmentation techniques are necessary to demonstrate that the CLIPstyler-based augmentation leads to superior adaptation.\n\n[a] Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models\n\nThe comparison methods is insufficient. Since the proposed work is closely related to unsupervised prompt tuning, it would be more convincing to include comparisons with recent methods in this area. Furthermore, the fairness of comparisons with few-shot prompt tuning methods such as CoOp should be clarifiet, particularly regarding how data usage and supervision levels are aligned during evaluation.\n\nFinally, the proposed MAP framework, while effective, appears to be task-agnostic. Its design is relatively general and could potentially apply to other tasks such as classification or segmentation. However, the paper lacks components specifically tailored to object detection, such as detection-aware prompt adaptation or task-specific loss formulations. As a result, the methodological novelty for the detection task itself is somewhat limited."}, "questions": {"value": "Please see the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RbEZvtNBat", "forum": "XVkboeYGEH", "replyto": "XVkboeYGEH", "signatures": ["ICLR.cc/2026/Conference/Submission15038/Reviewer_nBMU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15038/Reviewer_nBMU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807134396, "cdate": 1761807134396, "tmdate": 1762925362533, "mdate": 1762925362533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training framework to open-vocabulary object detection, and aims to address the issue of distribution shift (domain and semantic shift) in this area. The authors introduce theoretical analysis for the proposed method. Extensive experiments are conducted."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method seems reseaonable.\n\nExtensive experiments are conducted.\n\nSolving distribution shift is essential in open-vocalbulary tasks. \n\nIt is good to see the authors considering both semantic and domain shift."}, "weaknesses": {"value": "Can you please clarify, in Figure 2, which object is the novel one? It is hard to tell the performance on novel classes from the quality results. \n\nUnFI is not defined in Equatin 15.\n\nIn the theoretical part, I think the Propositions 4.1, 4.2 and 4.3 are more like assumptions or definitions. And for 4.3, the authors mentioned that $\\hat{y}$ is predicted, which means $H(\\hat{y})$ is not a constant. So, only minimizing the conditional entropy may not increase the mutual information. I mean, for example, if the model weights are all 0, the conditional entropy is 0 but the mutual information is 0 too, or please correct me if I am wrong.\n\nFor the proof in 4.4, the Equation 21 is concluded from Equation 20, but the $y$ in the two equations are different. Can you please clarify? and based on this, can you please clarify the theory followed by that?\n\nCan you please clarify why the mask tokens can represent novel classes? And how many tokens are used for this and why?\n\nminor typos: leaning-> learning, $\\mathcal{L}$ novel-> $\\mathcal{L}_{novel}$ (line 439)"}, "questions": {"value": "Please see above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YzhlpFVvzq", "forum": "XVkboeYGEH", "replyto": "XVkboeYGEH", "signatures": ["ICLR.cc/2026/Conference/Submission15038/Reviewer_FVtF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15038/Reviewer_FVtF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872928135, "cdate": 1761872928135, "tmdate": 1762925362153, "mdate": 1762925362153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets Cross-domain Open-Vocabulary Detection (COVD) and proposes MAP, a parameter-efficient framework that unifies Multi-Source Prompt Learning (MSPL) with Unsupervised Target Prompt Learning (UTPL): MSPL learns class-aware and domain-aware prompts, while UTPL adapts to unlabeled targets via text-guided style augmentations and an entropy-minimization consistency loss, avoiding pseudo-labels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It further introduces a novel-class mask and mutual-orthogonality regularization, and analyzes prompt fidelity and distinction. Experiments report state-of-the-art COVD results with minimal extra parameters (backbones frozen; only prompts optimized)."}, "weaknesses": {"value": "[1] Clarify assumptions：Say whether the method needs prior knowledge of novel classes. State which prompts are used at inference. Please explain the selection rule.\n\n[2] Run three small ablations：First remove text guided style augmentation. Second remove entropy minimization and consistency. Third replace hand crafted domain descriptions with automatic or generic ones. Report the change in AP for each test.\n\n[3] Do the gains hold under mAP@[.5:.95]? Were all baselines run with the same single or multi source setting and the same backbone?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qGIeD8oxOy", "forum": "XVkboeYGEH", "replyto": "XVkboeYGEH", "signatures": ["ICLR.cc/2026/Conference/Submission15038/Reviewer_EDfu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15038/Reviewer_EDfu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762048484609, "cdate": 1762048484609, "tmdate": 1762925361751, "mdate": 1762925361751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}