{"id": "GVIei1IdmC", "number": 25390, "cdate": 1758367432227, "mdate": 1759896722576, "content": {"title": "Large Language Models as Nondeterministic Causal Models", "abstract": "Chatzi et al. (2025) recently developed, for the first time, a method for generating counterfactuals of probabilistic Large Language Models. Such counterfactuals tell us what would - or might - have been the output of an LLM if some factual prompt ${\\bf x}$ had been ${\\bf x}^*$ instead. The ability to generate such counterfactuals is an important necessary step towards explaining, evaluating, and comparing, the behavior of LLMs. We argue, however, that their method rests on an ambiguous interpretation of LLMs: they do not interpret LLMs literally, for the method involves the assumption that one can change the implementation of an LLM's sampling process without changing the LLM itself, nor do they interpret LLMs as intended, for their method involves explicitly representing a _nondeterministic_ LLM as a _deterministic_ causal model. We here present a much simpler method for generating counterfactuals that is based on an LLM's intended interpretation by representing it as a nondeterministic causal model instead. The advantage of our simpler method is that it is directly applicable to any black-box LLM without modification, as it is agnostic to any implementation details. The advantage of Chatzi et al.'s method, on the other hand, is that it directly implements the generation of a specific type of counterfactuals that is useful for certain purposes, but not for others. We clarify how both methods relate by offering a theoretical foundation for reasoning about counterfactuals in LLMs \nbased on their intended semantics, thereby laying the groundwork for novel application-specific methods for generating counterfactuals.", "tldr": "By representing Large Language Models as Nondeterministic Causal Models we show that the generation of counterfactuals becomes extremely simple.", "keywords": ["Large Language Models", "counterfactuals", "causal models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ecb2259a8c51ce0330d579f1faaefef0922d4ed6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the question of what is a counterfactual of a string generated by a language model and discusses the exact semantics under multiple interpretations of a language model as a causal model. In particular, the paper proposes framing language models as nondeterministic causal models. This interpretation allows for a simple definition of a string counterfactual as simply another sample from the same language model (given the same prompt). This is in contrast with the interpretation as a deterministic language model, which is closer to what recent work has framed language models as. In particular, the proposed counterfactual sampling is simpler than previous proposed techniques that require access to the full model implementation, make assumptions about the sampling process, and rely on changing model weights. The paper also discusses when each of the interpretations is more useful."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a relevant and interesting question.\n\t- The problem and the setup are well-motivated and explained; for example, it makes it very clear where generating counterfactuals is useful\n\t- It also critically evaluates recent work on framing LMs as causal models\n- The paper discusses the problem very thoroughly\n\t- I feel like it represents a good reference on the topic of counterfactual generation in LMs\n- Provides a new theoretically-backed way of sampling counterfactuals that’s actually substantially easier and more easily applicable than existing methods\n- The discussion of deterministic LMs is helpful and illustrative"}, "weaknesses": {"value": "- I found the paper quite dense and theoretical on the first read. I’m not sure how to address that, though; the examples and connection to existing techniques already help.\n- I understand that the paper is theoretical and it’s primary goal is to formalize what a counterfactual could mean. However, I imagine a practical demonstration of what counterfactuals are and how they differ under different formalisms would be useful.\n- Nit-picky: I found the notation slightly confusing and I believe the exposition could benefit from introducing the notation explicitly; e.g., what are upper and lower-case variable names and how to interpret statements such as subset \n\t- Maybe a more thorough introduction of nondeterministic causal models in the appendix (or the extra page) could be useful"}, "questions": {"value": "- I’m slightly confused by Definition 2; isn’t $\\mathbf{pa_X} \\subset \\mathbf{v}$ always true?\n- Do the frameworks need to change substantially if the assumption of finite length ($k$) is lifted?\n- It seems to me that unlike Chatzi et al.’s and this paper’s frameworks, Ravfogel et al.’s framework works with interventions. Is that a correct reading?\n- I understand the nondeterministic causal model is a properly defined concept, but how practically useful is it to say that observing some outcome tells you nothing about other possible counterfactual distributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cg5XACaAnf", "forum": "GVIei1IdmC", "replyto": "GVIei1IdmC", "signatures": ["ICLR.cc/2026/Conference/Submission25390/Reviewer_oUCw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25390/Reviewer_oUCw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761033894509, "cdate": 1761033894509, "tmdate": 1762943421020, "mdate": 1762943421020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes that one can sample counterfactuals from a language model by simply querying a model again with a different (counterfactual) prompt. This is termed the **simple semantics**.\n\nCompare this to prior work, where the (afaik) more standard approach of maintaining exogenous noise in a structural causal model is done. The work says this is in some way more complicated."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Counterfactual sampling from LLMs is, at least to me, a non-intuitive field of research. Providing clarity and simplification to the field and shared understanding of how to do counterfactual sampling is an important goal."}, "weaknesses": {"value": "The four-page introduction chapters left me somewhat confused. I think the paper can be made more accessible by being more precise earlier on. For instance, the core object of study, an LLM, is not specified as is done more traditionally, i.e., as a distribution over finite strings in some language, but as some sort of conditional. Phrases like \"LLMs are incredibly complicated in detail\" don't say much; there are good standard definitions used in the cited works.\n\nMore seriously, I feel it is a bit of a strawman argument that there is something overly complicated going on in the prior work that uses the Gumbel approach. It only uses the output logits of the LLMs. The talk of changing source code or having access to the internals of LLMs doesn't exactly paint the full picture; it is a small change in sampling given the output logits, which are sometimes available for closed-source models (as opposed to the normalized probabilities). The problem is thus overstated a few times. The central problem is that randomness needs to be **coupled** to talk about counterfactuals; just running vanilla inference can not do this unless the randomness is controlled for somehow. I am also not sure that the prior works (Chatzi and Ravfogel) explicitly say the \"simple semantics\" is unsound -- but at least they would say it answers a different question. Stating that \"Neither Chatzi et al. nor Ravfogel [spelling mistake in paper] offer a theoretical justification of their approach.\" is not right, it directly follows the standard SCM definition of what it means to be a counterfactual. And talk about changing the LLM in the abstract misses the point that Gumbel sampling is the same as categorical sampling, as is covered well in the related work.\n\nSecond, the core point in Theorem 2 on **the collapse of the causal hierarchy** does not feel like the full picture for what we mean by a standard counterfactual. The prior work can ask what the noise is that produces an outcome, and then apply it to a different input. Theorem 2 does not do anything of that sort; it only relies on the input X. Doesn't this just miss the point of what it means to have a pointwise counterfactual rather than a population sample?\n\nFinally, there is no empirical demonstration, despite claims about the approach being directly applicable to any LLM. This would have been good to see.\n\nThe paper raises an interesting perspective, but it currently overstates the complexity of prior work and frames a narrower semantics than standard abduction-based counterfactuals. With a clearer introduction (and a more structured first half), a neutral comparison to Gumbel coupling, a scoped claim for Theorem 2, and a small empirical demonstration, the contribution would be stronger. But the critique of pointwise counterfactuals seems, despite the philosophical discussion, unwarranted, and thus I can not recommend that the paper be accepted.\n\n\nSmaller notes:\n* Temperature 0 as determinism doesn't hold for all sampling methods\n* Black boxes at the end of definitions.\n* missing definition of u,v,r in definition 4.\n* references used without links (cref)\n* again, the prior work comparison needs to be done more fairly and transparently"}, "questions": {"value": "Can you comment on the point above about the collapse of the causal hierarchy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TZUSAWBsmX", "forum": "GVIei1IdmC", "replyto": "GVIei1IdmC", "signatures": ["ICLR.cc/2026/Conference/Submission25390/Reviewer_EKa7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25390/Reviewer_EKa7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655382920, "cdate": 1761655382920, "tmdate": 1762943420773, "mdate": 1762943420773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a new theoretical foundation for generating counterfactuals in Large Language Models (LLMs). Counterfactuals answer the question: \"What would the output ($Y^\\*$) have been if the input ($X$) were different ($X^*$)?\"\n\nThe authors model the LLM's behavior using a nondeterministic causal model, which contrasts with prior work that used deterministic models with complex, implementation-dependent semantics (like Gumbel-based methods) that enforce an arbitrary \"closeness\" between the factual and counterfactual outputs.\n\nThe paper proves that, given the LLM's autoregressive structure, the correct causal interpretation justifies the \"simple semantics\": the counterfactual distribution $P(Y^\\*|do(X^\\*))$ is simply the standard observational distribution $P(Y|X^\\*)$. This framework establishes the simple semantics—which is agnostic to the LLM's internal mechanisms—as the general and theoretically sound approach for counterfactual reasoning in LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* Theoretical Rigor: The paper comes with rigorous mathematic definitions, theorems, and proofs.\n* Potential Practical Improvement: If the claims in the paper hold, the resulting simple semantics would  provide an efficient black-box approach to generate counterfactuals. This is a major advantage over methods that require internal access to the model."}, "weaknesses": {"value": "* **Unconvincing Assumption**: \n    * The paper posits that \"an LLM can be perfectly described as a causal model,\" but this stance is  contested by recent work (e.g., the \"Causal Parrots: Large Language Models May Talk Causality But Are Not Causal\") which argues that current LLMs are not truly causal and are, at best, weak 'causal parrots' that mimic causal language without genuine causal reasoning capability. Could you explain this discrepancy?\n* **Dense presentation, leading to an unclear central argument**: \n    * The paper is rather dense, riddled with complicated formal statements. Despite spending a decent amount of time, I have a hard time understanding the main argument of the paper. I find the main argument rather range. The key idea of counterfactuals is to use observations to re-shape the distributions of certain random variables, and then predict the outcome of the models based on the updated distributions and the modified input. Under this definition, observing the outcome of an LLM invocation will inevitably affect the internal randomness of the LLM. The paper's main argument is performing counterfactuals can ignore the effect of such observations. I find the argument rather counter-intuitive. Could you explain the intuition behind your main argument informally?\n\n* **Lack of Empirical Validation:** The work is entirely theoretical and lacks any empirical results or case studies. A simple experiment illustrating the practical divergence or convergence of the simple semantics compared to prior methods would significantly strengthen the claims for a machine learning audience."}, "questions": {"value": "Could you please address the two questions raised in \"Weaknesses\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3wPckSmoDF", "forum": "GVIei1IdmC", "replyto": "GVIei1IdmC", "signatures": ["ICLR.cc/2026/Conference/Submission25390/Reviewer_x8hu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25390/Reviewer_x8hu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805602761, "cdate": 1761805602761, "tmdate": 1762943420570, "mdate": 1762943420570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper models LLMs as nondeterministic causal models and shows that, under their autoregressive structure, counterfactuals collapse to the model’s ordinary predictive distribution. In other words, when we change the prompt from \\$x\\$ to \\$x^\\* \\$, the counterfactual distribution becomes \\$P(Y | X = x^\\*)\\$; generating a counterfactual simply means prompting the model with \\$x^\\*\\$. The authors contrast this with deterministic causal models, where randomness is shifted into latent variables and counterfactuals become unidentifiable. \n\nThe paper is clearly written and conceptually consistent, but entirely theoretical: there are no figures, algorithms, or experiments, and the main contribution formalizes an intuition already shared by many practitioners."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Clear and rigorous theory: The nondeterministic causal model formalization and theorems are well presented and logically sound.\n\nSimple but useful baseline: The ''simple semantics'' provides a clean theoretical reference point for future research.\n\nReadable and well organized: The paper is coherent, with arguments that flow clearly and are easy to follow."}, "weaknesses": {"value": "**W1. Incremental contribution:**\n    The main claim that simple semantics (i.e., running the LLM again on $x^*$) is a valid way to generate counterfactuals under a nondeterministic causal model is conceptually straightforward and follows almost directly from definitions. The notion of a nondeterministic causal model already exists in the literature, and the relationship between input and output in LLMs naturally satisfies its assumptions. Hence, the main theorem mostly rephrases a known idea rather than offering a substantially new result. In essence, the contribution feels incremental rather than original.\n    \n**W2. Philosophical and abstract presentation:**\n    The paper reads more like a philosophical essay than an ML paper aimed at a broad audience.  It contains no figures, concrete examples, or empirical illustrations that could make the ideas more accessible to the ML community. Even a minimal diagram of the autoregressive causal structure or a simple experiment would make the work easier to evaluate.\n    \n**W3. Limited scope:**\n    The result holds only for root-level (prompt) counterfactuals. It is unclear how or whether the same reasoning applies to more general conditioning (e.g., hidden reasoning states or output tokens)."}, "questions": {"value": "First, please address the weaknesses mentioned above. I also have two specific questions:\n\nQ1. How should practitioners decide between using simple semantics versus previous methods (e.g., Chatzi et al. [1]) for their applications?\n\nQ2. Is there any way to address the identifiability problems for LLMs in deterministic causal models using Backtracking Counterfactuals [2]?\n\n\n\n[1] Ivi Chatzi, Nina Corvelo Benz, Eleni Straitouri, Stratis Tsirtsis, and Manuel Gomez-Rodriguez. 2025. Counterfactual Token Generation in Large Language Models. In Proceedings of the Fourth Conference on Causal Learning and Reasoning.\n\n[2] Von Kügelgen, Julius, Abdirisak Mohamed, and Sander Beckers. \"Backtracking counterfactuals.\" Conference on Causal Learning and Reasoning. PMLR, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Dzhr3ieYMg", "forum": "GVIei1IdmC", "replyto": "GVIei1IdmC", "signatures": ["ICLR.cc/2026/Conference/Submission25390/Reviewer_P1G1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25390/Reviewer_P1G1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917752339, "cdate": 1761917752339, "tmdate": 1762943420252, "mdate": 1762943420252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}