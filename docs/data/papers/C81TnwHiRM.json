{"id": "C81TnwHiRM", "number": 17037, "cdate": 1758271444998, "mdate": 1763645531753, "content": {"title": "ASIDE: Architectural Separation of Instructions and Data in Language Models", "abstract": "Despite their remarkable performance, large language models lack elementary safety features, making them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as the root cause of the success of prompt injection attacks. In this work, we propose a new architectural element, ASIDE, that allows language models to clearly separate instructions and data at the level of token embeddings. ASIDE applies an orthogonal rotation to the embeddings of data tokens, thus creating clearly distinct representations of instructions and data tokens without introducing any additional parameters. As we demonstrate experimentally across a range of models, instruction-tuning LLMs with ASIDE (1) achieves substantially higher instruction-data separation without performance loss and (2) makes the models more robust to prompt injection benchmarks, even without dedicated safety training. Additionally, we provide insights into the mechanism underlying our method through an analysis of the model representations.", "tldr": "A new architectural element, ASIDE, that allows language models to clearly separate between instructions and data on the level of embeddings", "keywords": ["large language models", "instruction-data-separation", "conditional embedding mechanism", "LLM safety", "prompt injections"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4970f9f17666106a1854c46215b14a070e3a61a5.pdf", "supplementary_material": "/attachment/4a5a8aee323058e6185a848e93069309f8a76fcb.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a new method to defend against prompt injection attack by separating instructions and data at the level of token embeddings through orthogonal rotation. The advantage of the proposed method is that it does not rely on safety-related data for training while achieving good performance on both normal tasks and safety tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. This method is novel to my knowledge.\n\n2. The safety performance of the proposed method is promising.\n\n3. The proposed method can achieve better safety without the need of safety data.\n\n4. Many different benchmark datasets are used for evaluation."}, "weaknesses": {"value": "1. It is not very clear on the selection of orthogonal rotation. Why is it better than other kinds of transformation? Is there any theoretical analysis on this?\n\n2. Orthogonal rotation is simple, which is good. But does it can well fit different data distributions, tasks and models?\n\n3. It would be better if more normal tasks such as reasoning-related tasks are incorporated into experiments to evaluate the impact of the proposed method on model performance."}, "questions": {"value": "Please refer to above comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uAk7oR0s5Z", "forum": "C81TnwHiRM", "replyto": "C81TnwHiRM", "signatures": ["ICLR.cc/2026/Conference/Submission17037/Reviewer_Rtoh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17037/Reviewer_Rtoh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546488637, "cdate": 1761546488637, "tmdate": 1762927054268, "mdate": 1762927054268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ASIDE (Architecturally Separated Instruction-Data Embeddings), a novel, parameter-free architectural modification to Large Language Models (LLMs) aimed at mitigating prompt injection vulnerabilities. The authors identify the lack of intrinsic separation between instructions (which should be executed) and data (which should be processed) as a root cause of these attacks.\n\nASIDE's core mechanism is to create distinct representations for instructions and data at the very first layer. It achieves this by applying a fixed, parameter-free 90-degree orthogonal rotation to the token embeddings of all inputs designated as \"data,\" while leaving \"instruction\" token embeddings unchanged. This modified model is then instruction-tuned using a standard (non-adversarial) dataset.\n\n\nThe paper empirically demonstrates across a range of models (including Llama, Qwen, and Mistral) that ASIDE:\n\n* Achieves substantially higher instruction-data separation (measured by the SEP score).\n\n* Maintains model utility and performance, comparable to standard fine-tuned models (measured by SEP Utility and Alpaca Eval 1.0).\n\n* Improves robustness against both indirect and direct prompt injection benchmarks, even without any dedicated safety training.\n\nThe authors supplement these findings with a strong set of interpretability analyses, including linear probing and causal interventions, to validate that the architectural separation persists through the model's layers and is causally linked to the improved safety."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Novelty and Elegance: The proposed method is simple, highly novel, and elegant. Using a fixed, parameter-free orthogonal rotation is a clever architectural solution that avoids the overhead of additional parameters or complex, learnable components.\n\n\n\n* Strong Empirical Validation: The claims are convincingly supported by experiments across a wide and diverse set of modern LLMs (Llama 2 7B/13B, Llama 3.1 8B, Qwen 2.5 7B, Qwen3 8B, and Mistral 7B). The improvement in instruction-data separation (SEP score, Figure 2a) is consistent and significant over all baselines.\n\n\n\n\n* Practical Safety Improvement: A key contribution is that ASIDE provides a measurable and \"free\" improvement in robustness, particularly against indirect prompt injections (Table 1), without requiring any adversarial data or specialized safety fine-tuning. This makes it a highly practical method for improving the baseline safety of LLMs.\n\n\n* Excellent Interpretability and Analysis: This is a standout strength of the paper. The authors provide deep insights into why ASIDE works.\n\n* Clarity: The paper is exceptionally well-written, with clear illustrations (especially Figure 1) and a logical flow that makes the method and its evaluation easy to follow."}, "weaknesses": {"value": "*Scope Limited to Pre-Defined Roles: The primary limitation is that ASIDE requires the functional role (instruction vs. data) of input tokens to be specified a priori by the system implementer. This is a reasonable assumption for many integrated applications (e.g., RAG, email clients). Still, it is not directly applicable to general-purpose, multi-turn chatbots where a single user turn might contain a mix of data (e.g., continuing a story) and new instructions (e.g., \"now change the character's name\"). The authors acknowledge this limitation.\n\n* Limited Justification for Rotation Choice: The paper specifies a 90-degree ($\\frac{\\pi}{2}$) isoclinic rotation and justifies it based on computational efficiency (it simplifies to coordinate swapping and negation). While practical, this leaves the theoretical justification underexplored. The paper would be strengthened by an ablation study comparing this specific rotation to other angles (e.g., 45 degrees) or other types of parameter-free orthogonal transformations to show that 90 degrees is an optimal or robust choice."}, "questions": {"value": "1. Multi-turn Scenarios: The paper's scope is explicitly limited to single-turn, system-level applications where roles are pre-defined. Could you elaborate on how you envision ASIDE being adapted for a multi-turn conversational setting? For instance, could a classifier be trained to assign \"instruction\" or \"data\" roles to spans of a user's turn before the embedding rotation is applied?\n\n\n\n2. Choice of Rotation: Your justification for the 90-degree rotation is its computational efficiency. Did you experiment with other rotation angles (e.g., 45, 180 degrees) or other types of parameter-free orthogonal transformations? How sensitive is the model's SEP score and ASR to this specific choice?\n\n3. ASIDE vs. ISE Mechanism: The finding in Figure 3 that ISE's linear separability degrades in deeper layers while ASIDE's does not is a key differentiator. What is your hypothesis for this? Why do you believe the network can \"undo\" or \"ignore\" a learnable offset (ISE) more easily than it can a fixed rotation (ASIDE)?\n\n\n4. Direct Injection Nuance: The robustness gains on direct prompt injection (Table 1) appear less pronounced than on indirect injection, with negligible improvement on the RuLES benchmark. Does this suggest ASIDE is primarily effective against attacks based on role confusion (data-as-instruction), rather than attacks that try to override a known instruction (jailbreaks)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T8Jhh5J2Ud", "forum": "C81TnwHiRM", "replyto": "C81TnwHiRM", "signatures": ["ICLR.cc/2026/Conference/Submission17037/Reviewer_x1XQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17037/Reviewer_x1XQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758192225, "cdate": 1761758192225, "tmdate": 1762927053837, "mdate": 1762927053837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ASIDE, a lightweight architectural modification that enforces structural separation between instruction and data embeddings in large language models. By applying orthogonal transformations to distinguish instruction and data tokens, ASIDE improves robustness against prompt injection attacks without introducing extra parameters. Experiments show significant gains in instruction–data separation and lower attack success rates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is simple, easy to understand, and conceptually interesting.\n\n2. The proposed method is computationally efficient and does not introduce any additional parameters.\n\n3. The paper includes insightful analyses explaining why ASIDE achieves better instruction–data separation performance."}, "weaknesses": {"value": "1. As stated in the paper, the proposed method is designed as a defense against prompt injection attacks. Therefore, I believe the paper should include comparisons with the most recent fine-tuning-based defenses such as StruQ [1], SecAlign [2], and Meta-SecAlign [3]. The current baselines used for comparison appear relatively weak.\n\n2. Although the ASR of ASIDE in Table 1 is lower than that of other baselines, some numbers remain quite high (exceeding 60%), suggesting that ASIDE may not be a very effective defense against prompt injection in practice.\n\n3. I find the utility results not very convincing. Specifically, the results on AlpacaEval may be over-estimated, since the training set (Alpaca-Clean) is very similar to AlpacaEval, potentially leading to distribution overlap. Moreover, Qwen-based models (which already possess strong instruction-following capabilities) perform well on SEP but worse than other methods on AlpacaEval, indicating possible evaluation bias.\n\n4. I think the authors’ statement “we use plain pretrained models rather than instruction- or safety-tuned models to avoid biasing the safety evaluations” somewhat unconvincing. As we know, the utility of the pretrained Llama models is very poor (also as shown in Figure 2), and such models generally lack instruction-following capacity. In realistic deployments, using instruction-tuned models is unavoidable. Therefore, the paper should additionally evaluate ASIDE on instruction-tuned versions of those models, and compare their utility with the original instruction-tuned baselines, or at least report the results of the instruction-tuned models. This would better demonstrate the compatibility between ASIDE and instruction tuning.\n\n[1].Chen, Sizhe, et al. \"{StruQ}: Defending against prompt injection with structured queries.\"\n\n[2].Chen, Sizhe, et al. \"Secalign: Defending against prompt injection with preference optimization.\" \n\n[3].Chen, Sizhe, et al. \"Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks.\""}, "questions": {"value": "Please see the weakness part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "101MsEPSsl", "forum": "C81TnwHiRM", "replyto": "C81TnwHiRM", "signatures": ["ICLR.cc/2026/Conference/Submission17037/Reviewer_YbgE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17037/Reviewer_YbgE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951027552, "cdate": 1761951027552, "tmdate": 1762927053359, "mdate": 1762927053359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}