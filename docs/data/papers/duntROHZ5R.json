{"id": "duntROHZ5R", "number": 5991, "cdate": 1757949797622, "mdate": 1759897940763, "content": {"title": "GAVEL: Towards Rule-Based Safety through Activation Monitoring", "abstract": "Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as \"making a threat\" and \"payment processing\", that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time using a multi-label classifier. This enables practitioners to flexibly configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. We further release datasets, code, and an initial CE vocabulary with misuse rulesets to bootstrap community collaboration. Our results show that compositional, rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance.", "tldr": "A Rule-Based Approach to Activation-Based AI Safety", "keywords": ["AI Safety", "Activation-Based Monitoring", "Rule-Based Detection", "Large Language Models", "Misuse Detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/118cea17e72aaf0cb9971b8911185bc18457e974.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a framework that enhances AI safety by combining activation-level monitoring with rule-based logic. Instead of relying on broad misuse datasets, GAVEL models interpretable Cognitive Elements (CEs)—fine-grained behavioral units such as “making a threat” or “requesting payment information.” These CEs capture internal model activities and enable safety rules to be defined as logical predicates (e.g., refuse if “creating content” AND “personal information”), allowing practitioners to flexibly enforce policies without retraining detectors.\n\nGAVEL demonstrates higher precision and lower false positives than existing safeguards like content moderation APIs or traditional activation classifiers. It generalizes well across multiple LLMs and languages, while maintaining real-time efficiency with less than 1% computational overhead. By promoting community sharing of CEs and rule sets, GAVEL establishes a scalable, interpretable, and auditable foundation for transparent AI governance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper’s main strengths lie in its originality, rigor, clarity, and significance. It introduces a novel paradigm—rule-based activation safety—that bridges activation monitoring and cybersecurity-inspired rule frameworks, making AI safety both interpretable and configurable. The concept of Cognitive Elements (CEs) as modular, activation-level primitives is innovative, enabling composable and auditable safeguards that overcome key limitations of existing misuse detectors. The methodology is high-quality and well-executed, with comprehensive experiments across multiple models, domains, and languages, all supported by reproducible code and datasets. The writing is clear, logically structured, and visually supported by informative figures and tables. Overall, the paper makes a significant and timely contribution, offering a practical, interpretable foundation for scalable and collaborative AI safety governance."}, "weaknesses": {"value": "While the paper is strong overall, several weaknesses limit its generalizability and depth of validation. The primary concern is the dependence on synthetic datasets generated by GPT-4/5 for evaluating misuse scenarios, which may not fully capture the complexity or unpredictability of real-world adversarial behavior. This raises questions about how GAVEL would perform under naturally occurring or adaptive attacks. Additionally, while the framework demonstrates strong detection accuracy, the evaluation focuses narrowly on detection rather than enforcement—there is limited analysis of how rule violations translate into safe, consistent behavioral corrections during long or open-ended interactions. The rule creation and CE definition process also relies heavily on manual design, which may not scale efficiently without further automation or community infrastructure. Finally, although the paper discusses interpretability, it would benefit from more qualitative analysis or case studies showing how GAVEL’s outputs aid human auditors in practice."}, "questions": {"value": "How well would GAVEL generalize to real-world misuse cases beyond synthetic GPT-generated data? Could results differ with naturally occurring adversarial prompts?\n\nCan the creation of Cognitive Elements and rules be automated or standardized, perhaps with LLM assistance, to improve scalability?\n\nHow do the authors plan to extend GAVEL from detection to active enforcement (e.g., refusal, redaction, or steering) while ensuring stable model behavior?\n\nHas robustness to adversarial evasion at the activation level been tested, and how resilient is GAVEL compared to surface-text moderation methods?\n\nCould the authors include qualitative examples or visualizations showing how human auditors interpret rule violations and CE activations in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper shows no ethical concerns in its methods, data, or claims. It uses synthetic GPT-4/5 datasets responsibly, is transparent about procedures, and focuses on safety and governance as research topics, not violations. No privacy or bias issues are evident, and the work aligns with ICLR’s ethical standards."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EDQCzR09Uw", "forum": "duntROHZ5R", "replyto": "duntROHZ5R", "signatures": ["ICLR.cc/2026/Conference/Submission5991/Reviewer_x4Ks"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5991/Reviewer_x4Ks"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760563402542, "cdate": 1760563402542, "tmdate": 1762918397705, "mdate": 1762918397705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the adaptation of a new paradigm inspired by rule-sharing practices in cybersecurity. This rule-based activation safety system models activations as cognitive elements, fine-grained, interpretable factors such as \"making a threat\" and \"payment processing.\"  From this, they build a framework that defines predicate rules over these element factors, using them for real-time classification. The paper tries to address three limitations identified by the authors: poor precision, limited flexibility, and lack of interpretability.\n\nThe method is evaluated on four approaches: loss-based fine-tuning, reading vector projections, content moderation APIs, and activation classification. These methods were evaluated against a synthetic dataset constructed for this work."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Overall, the method is simple and doesn't require retraining\n- Derives the method from cybersecurity practices"}, "weaknesses": {"value": "- The paper is not well-written. See some examples below.\n   - In line 213, it was unclear whether this is the same $d$ from line 201.\n   - Figure 2 should be earlier in the paper (page 1 or 2)\n   - Many choices in the paper are not well-explained. Here are two examples of this  (1) why are attention outputs instead of activations of the linear layers? (2) Why is GAVEL detection an RNN? \n- I would like to see a baseline of GPT-4 in Table 3.\n- Rule-based systems are inherently subjective in nature"}, "questions": {"value": "- Is there a way to control the TPR and FPR? \n- Gemma and Qwen3 seem to be lower-weighted ACC in Figure 3. Could this be because the model's multilingual nature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aGuvjJvHkx", "forum": "duntROHZ5R", "replyto": "duntROHZ5R", "signatures": ["ICLR.cc/2026/Conference/Submission5991/Reviewer_Hrih"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5991/Reviewer_Hrih"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761104132059, "cdate": 1761104132059, "tmdate": 1762918397413, "mdate": 1762918397413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This papers presents a novel framework for activation-based safety monitoring. In particular, the paper proposes to elicit cognitive elements from model activations, and then defining predicate rules over CEs and detects violations in real time using a multi-label classifier.  Finally, the proposed approach is evaluated on a self-generated datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is to most extents clearly written and easy to follow\n- Promising results on the self-generated benchmarks and informative an informative analysis in Figure 4.\n- Informative Ablations (e.g. Transformer layer ablation and not just assuming some gold layer)"}, "weaknesses": {"value": "- **Boolean Predicate Rules:** While the grounding in concept space sounds intriguing, it remains unclear to me if the boolean logic is the right top-level tool. For instance, many nuance things are hard to express in boolean logic and it seems kind of a loss of flexibility (one of the core benefits of the current LLM-based paradigm) \n- **Connection to SAEs:** Despite the paper's main motivation to build onto concept spaces, it seems odd to me that sparse autoencoders (SAEs) are not mentioned a single time. Especially as SAEs would offer \"free\" access to concept-level abstractions.\n- **Interference of Concepts:** The authors state that \"each activation set in the training collection H is curated to isolate a single CE a time\", and list two advantages. However, the limitations of this approach remain open. For instance, how well can the resulting extraction deal with tokens where multiple concepts interfere? \n- **Only Self-Generated Benchmarks:** While it's definitely a good evaluation tool to construct targeted datasets to test a proposed method, one should also evaluate the proposed method on existing benchmarks.\n- **Baseline Comparisons:** Since you construct specific predicate rules for each dataset, did you also test a specialized prompt per category, and tested a vanilla model with this specialized prompt? How much does rule specialization to the specific dataset matter?\n- **Lack of Details:** The paper lacks quite a lot of important deltas\n  - Only very details on the RNN are given (would be hard to reproduce the paper)\n  - No details on how the datasets were generated are given (also checked the appendix)"}, "questions": {"value": "**Additional Question (not related to weaknesses)**\n- **New Adversarial Attack Surface:** While GAVEL defends against surface-level \"representation attacks\" , it introduces a new attack vector: the CE detector itself. A sophisticated adversary could, in principle, craft a \"CE-level jailbreak\"—a prompt that achieves a harmful outcome without triggering the specific activation signatures the multi-label classifier is trained to detect. This vulnerability is not explored in the paper.\n\nI am more than happy to raise my score if the authors can address my concerns and questions, and in particular make the paper \"complete\" such that it could be reproduced. In the current state, it would be hard to reproduce."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RgCzrvxdkS", "forum": "duntROHZ5R", "replyto": "duntROHZ5R", "signatures": ["ICLR.cc/2026/Conference/Submission5991/Reviewer_ihDH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5991/Reviewer_ihDH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836365225, "cdate": 1761836365225, "tmdate": 1762918396690, "mdate": 1762918396690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Author Comment: Initial Response and Plan for Revisions"}, "comment": {"value": "Thank you for your thoughtful and constructive review of \"GAVEL: Towards Rule-Based Safety through Activation Monitoring.\" We genuinely appreciate the time you invested in our work and the helpful feedback you provided.\n\nWe are actively working through the points you raised. Rather than waiting to address everything at once, we plan to post responses incrementally as we complete each set of experiments and analyses. This will allow us to share results with you more promptly while maintaining the rigor and detail each point deserves.\n\nWe look forward to engaging with your feedback and will begin posting updates soon."}}, "id": "PPR4ZtBZLd", "forum": "duntROHZ5R", "replyto": "duntROHZ5R", "signatures": ["ICLR.cc/2026/Conference/Submission5991/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5991/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5991/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762977824433, "cdate": 1762977824433, "tmdate": 1762977824433, "mdate": 1762977824433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GAVEL, a framework for improving LLMs safety by monitoring internal activations. It proposes the use of Cognitive Elements, as well as logical rules to detect and respond to unsafe or policy-violating behaviors. The framework is inspired by rule-sharing practices in cybersecurity and aims to foster community collaboration in defining and maintaining safety standards."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The use of Cognitive Elements allows for composable safety rules that are explainable\n- GAVEL operates with minimal computational overhead\n- The framework has good performance across different LLM architectures and languages"}, "weaknesses": {"value": "- The process of defining Cognitive Elements and composing rules is currently manual and may not scale easily"}, "questions": {"value": "- Are there avenues/ possibilities for automating the rule creation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AdKF9fPEpk", "forum": "duntROHZ5R", "replyto": "duntROHZ5R", "signatures": ["ICLR.cc/2026/Conference/Submission5991/Reviewer_VVtu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5991/Reviewer_VVtu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027791669, "cdate": 1762027791669, "tmdate": 1762918395667, "mdate": 1762918395667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Authors Comment - General"}, "comment": {"value": "We are deeply grateful to all reviewers for their thorough evaluation and insightful feedback. Your patience and constructive criticism have been invaluable in strengthening this work. We have carefully addressed each comment raised, and our detailed responses are provided below.\n\n**Status of revisions**: We are currently completing additional experiments requested by reviewers, including extended benchmark evaluations and robustness analyses. Once these experiments conclude, we will promptly update the reviewers with the complete results and the revised manuscript on OpenReview.\n\n**Addressing Scalability and Interpretability**: In response to reviewer concerns about scalability and interpretability, we are pleased to announce two interactive demonstrations that substantiate GAVEL's practical viability:\n\n1. **Regarding scalability of the rule-based system**: We are excited to let the reviewers know that we have published an online app that automates the entire process of rule composition and CE dataset generation using LLM agents. The tool demonstrates that GAVEL can be set up in a very quick, and automated manner minimizing any manual effort required. The reviewers can access this demo here: [Gavel Automatation Tool](https://gavelautomatedruleandcegenerationpipeline-b2ffsm7zwt6onjp2aago.streamlit.app/)  \n   The reviewers are encouraged to experiment with their own safety scenarios to observe the automation capabilities firsthand.  \n2. **Interactive GAVEL Visualization Interface:** To address questions about interpretability and how human auditors interact with GAVEL in practice, we have deployed a live demonstration running GAVEL on Mistral. This interface allows reviewers to input arbitrary prompts, observe real-time CE activations during generation, visualize which rules are triggered and why, and explore the system's decision-making process interactively.  \n   Access the demo here: [GAVEL Interactive Demo](https://44vpkxck01df4f-8501.proxy.runpod.net/)\n\n**Code release commitment:** The complete source code for both tools (above), along with the full GAVEL framework implementation, all trained models, evaluation scripts, and datasets, will be released as a public GitHub repository upon paper acceptance. We are committed to full reproducibility and community adoption.\n\nWe look forward to your feedback and remain available for any clarifications you may request.\n\nThe Authors"}}, "id": "piNRmUz657", "forum": "duntROHZ5R", "replyto": "duntROHZ5R", "signatures": ["ICLR.cc/2026/Conference/Submission5991/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5991/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission5991/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763738284271, "cdate": 1763738284271, "tmdate": 1763738284271, "mdate": 1763738284271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}