{"id": "EB92tITeNq", "number": 4105, "cdate": 1757601503552, "mdate": 1759898052649, "content": {"title": "SAC: Adaptive Learning Rate Scaling with Architectural Constraints", "abstract": "The design of optimizers for modern Large Language Models (LLMs) is governed by the critical trade-off between performance, memory footprint, and computational throughput. High-accuracy methods, such as those exploiting gradient preconditioning techniques, are often memory-intensive and may introduce significant computational overhead, while efficient ones like Galore may not reach the same performance level. In this work, we present Scaling with Architectural Constraints (SAC), an optimizer wrapper that navigates these competing demands for the first time. SAC enhances existing adaptive optimizers by modulating per-parameter learning rates with lightweight, hierarchical constraints derived from model architectures. On the C4 pre-training benchmark, SAC+AdamW achieves state-of-the-art perplexity from 60M to 3B model sizes, converging faster without incurring the high costs of complex preconditioning. It also enhances training stability, showcasing robustness across varied learning rates and batch sizes. Qualitatively, empirical analysis shows that SAC fosters a more coordinated optimization process, leading to improved gradient dynamics. Its versatility has been further validated by the strong results across downstream tasks and domains, including long sequence modeling, parameter-efficient fine-tuning, image classification with diverse models like ViTs and CNNs, and evaluations on multimodal benchmarks.", "tldr": "This paper proposes an optimizer wrapper (SAC) for modern DNNs, which constrains the adaptive learning rate with hierarchical optimization states estimation and equalization scaling at hierarchical levels.", "keywords": ["Language Modeling", "Optimization", "Neural Network Architectures", "LLMs", "MLLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe9e7fcb9f8fcef61725dc9516416fd8e446755d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SAC (Scaling with Architectural Constraints), a light‑weight wrapper around Adam‑family optimizers that multiplies the usual Adam update by an architecture‑aware scale factor $S_t$. The factor is computed online from robust gradient statistics at several structural granularities (block, layer, and optionally group/parameter), so that the effective per‑parameter learning rate becomes\n$\n\\alpha_\\theta = \\eta\\cdot c_l \\cdot s_b \\cdot r_\\theta,\\qquad \\theta\\in\\mathcal P_{l,b},\n$\nwith a layer factor $c_l$ that equalizes update magnitudes across depth based on a ratio of MAD dispersions, optionally EMA‑smoothed, a block factor $s_b$ that redistributes learning‑rate “budget” across blocks within a layer via a temperatured softmax, and an optional within‑block term $r_\\theta$. The combined factor is applied multiplicatively to the base Adam‑like step."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The Optimizer Design Plane surfaces the Coordinated Optimization Zone and motivates adding spatial structure to Adam‑style methods. \n2. Using MAD for dispersion plus a budgeted softmax to keep a per‑layer LR budget is principled and effective; ablations show MAD’s advantage. \n3. The pipeline and partition are easy to follow."}, "weaknesses": {"value": "1. Eq. (7) defines $\\phi_{l,b}$ via log‑RMS, whereas Algorithm 1 computes a MAD‑based log‑ratio; the paper doesn’t state which variant produced Tables 1–6. \n2. The text says SAC adds “negligible overhead,” but Table 1 shows sizable increases in optimizer‑step time for SAC+AdamW (e.g., 350M: 0.0045 s → 0.0401 s; 1B: 0.0762 s → 0.1089 s). It would be better to provide end‑to‑end wall‑clock (including forward/backward) and clarify communication costs for global/layer statistics. \n3. Core knobs $\\gamma,\\beta,\\rho$ and clamp bounds $[S_{\\min},S_{\\max}]$ are not specified per experiment in Appendix; defaults and sensitivity are essential. \n4. Many gains are modest; results lack seeds/variance/CIs across Tables 1–6. \n5. The text says SAC “matches or surpasses PEFT baselines,” yet Table 5 shows DoRA has a higher average than LoRA+SAC. Either add SAC+DoRA or soften the claim. \n6. (i) The optional $r_\\theta$ is introduced but not implemented in Algorithm 1. (ii) SAC scales the data‑driven step but not decoupled weight decay (Algorithm 1), thereby changing their relative strengths.\n7.  Appendix Table A2 uses different LR grids per optimizer (e.g., SAC+AdamW fixed at (10^{-2}) across sizes, while AdamW baselines use size‑dependent smaller peaks). Ensure identical sweep budgets and report best‑of‑sweep for each optimizer with seeds."}, "questions": {"value": "1. Which definition was used in Tables 1–6. Eq. (7) (log‑RMS) or Algorithm 1’s MAD‑based score? If both were tried, how do results differ? Please align equation, pseudocode, and code. \n2. What exact values were used for $\\gamma,\\beta,\\rho,S_{\\min},S_{\\max}$ in each table (LLM, vision, MLLM)? Can you add sensitivity plots and recommended defaults? \n3. Provide end‑to‑end wall‑clock per step and fraction due to reductions for global/layer MADs; how does overhead scale with data/tensor parallel sizes? \n4. Did you try scaling decoupled weight decay by $c_l s_{l,b}$? If so, how did this affect stability/convergence vs. the current choice (Algorithm 1)? \n5. How does clipping (used in long‑sequence setups) interact with $c_l$ (which inflates when dispersion is small)? Any clamping or pre‑clip computation? \n6. Any experiments with head/row/column‑wise $r_\\theta$? If not used, please clarify and update Eq. (3) to match the implemented variant. \n7. For Table 1 and Table 4, could you report best‑of‑equal‑budget LR sweeps (same grid, same seed counts) and include error bars? \n8. Can you evaluate SAC+DoRA or SAC+Fira to support the statement “matches or surpasses PEFT baselines”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B4Bc6bJOtv", "forum": "EB92tITeNq", "replyto": "EB92tITeNq", "signatures": ["ICLR.cc/2026/Conference/Submission4105/Reviewer_FoCz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4105/Reviewer_FoCz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859952387, "cdate": 1761859952387, "tmdate": 1762917179056, "mdate": 1762917179056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SAC (Scaling with Architectural Constraints) — a wrapper around adaptive optimizers (e.g., Adam/AdamW) that computes hierarchical, architecture-aware scaling factors for parameter updates. SAC aims to combine temporal smoothing (as in Adam) with spatial coordination (across layers and blocks), using robust gradient statistics to dynamically adjust learning rate multipliers at multiple levels (block/layer/global). The method is simple, computationally light (as experiments demonstrate), and shows empirical improvements across LLM pre-training, vision tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "__1) Motivated idea__\nThe idea of adding “architecturally coordinated optimization” to adaptive optimizers is understandable and eliminates their real limitations, which ignore structural correlations between parameters.\n\n__2) Design__ \nSAC is easy to implement into existing optimizers. This makes it possible to use this framework in practice.\n\n__3) Comprehensive experiments__\nThe paper includes evaluations across multiple domains (language, vision, PEFT) and performs several ablations."}, "weaknesses": {"value": "__1) Limited novelty and conceptual depth.__\nDespite the fact that the method is well developed, its conceptual novelty is weaker than stated. Adaptive architecture-based or layer-based scaling was investigated earlier. Combining these concepts by adding a structured scaling factor to the step looks heuristic. From my point of view, SAC can be viewed more as a pragmatic simplification than as a fundamentally new paradigm.\n\n__2) Lack of theoretical grounding.__\nThe article does not provide an analysis of convergence or stability, as well as a theoretical justification for using MAD-based scaling. The choice of parameters (logarithmic average, the ratio of the layers $\\frac{\\delta_{global}}{\\delta_{layer}}$) is heuristic. Without even a simple proof sketch or binding, statements about “coordinated optimization zones” remain more conceptual than formal.\n\n__3) Experimental problems.__\n\na) The authors do not provide any statistical studies of the results obtained. This greatly reduces the significance of their results.\n\nb) The improvement of the SAC wrapper over the baseline optimizers provides an improvement, however insignificant (0.2-1%). Coupled with the fact that the authors do not provide statistical studies, it is impossible to say with certainty that SAC improves the basic optimizers.\n\nc) In experiments with large language models, the authors validate the methods on models with a maximum of 3B parameters. It would be great to see the robustness of their method to more stable models of a larger size with more than 7B parameters.\n\n__4) Concerns of adapting across architectures.__\nSAC depends on multiple ad hoc components: MAD statistics, logarithmic rescaling, clamping $[S_{min}, S_{max}]$, and hand-defined grouping of layers/blocks. This raises concerns about generality and reproducibility across architectures (e.g., hybrids, MoE models)."}, "questions": {"value": "__Overall Assessment__\n\nSAC is a well-implemented wrapper for basic adaptive optimizers. However, its conceptual and theoretical novelty is limited, and the paper currently lacks sufficient rigor to justify claims of a new optimization paradigm. Although the experimental section looks fundamental, it has its drawbacks, such as the lack of statistical significance of improvements to the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3AZiKXiZg2", "forum": "EB92tITeNq", "replyto": "EB92tITeNq", "signatures": ["ICLR.cc/2026/Conference/Submission4105/Reviewer_WE76"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4105/Reviewer_WE76"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907907477, "cdate": 1761907907477, "tmdate": 1762917178844, "mdate": 1762917178844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **SAC (Scaling with Architectural Constraints)** — an optimizer *wrapper* that enhances adaptive optimizers (e.g., AdamW, Adam-mini, Shampoo) by **modulating per-parameter learning rates through lightweight, architecture-aware constraints**.  \n\nThe key insight is that modern LLM optimization is dominated by **temporal smoothing** (historical gradients) but remains blind to **spatial structuring** (architectural hierarchy). SAC addresses this by introducing hierarchical scale factors — at parameter, group, layer, and block levels — to coordinate updates within the model’s structure, without adding significant computational cost.\n\nExtensive experiments cover:\n- **LLM pretraining (C4 dataset, LLaMA models 60M–3B)**  \n- **Vision classification (CIFAR-100, ImageNet-1K)**  \n- **Long-sequence modeling (Gated DeltaNet 1.3B)**  \n- **PEFT and multimodal fine-tuning (LLaVA, LoRA, etc.)**\n\nEmpirically, SAC+AdamW achieves state-of-the-art perplexity with minimal overhead, outperforming strong baselines like MARS, Muon, and Shampoo by up to **30% improvement in convergence or accuracy metrics**."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. **Strong conceptual novelty** — identifies and operationalizes the underexplored “spatial structuring” dimension in optimizers.  \n2. **Extensive empirical validation** — over 20 benchmarks across NLP, vision, and multimodal tasks, with consistent performance gains.  \n3. **High practical value** — easily implementable, compatible with standard frameworks (PyTorch), and adds minimal overhead.  \n4. **Robustness and interpretability** — hierarchical factors (layer, block, group) make optimization more interpretable, as seen in Figure 3’s LR distributions.  \n5. **Cross-domain generality** — SAC’s success in both LLMs and CNN/Transformer models is uncommon for optimizer work.  \n6. **Comprehensive ablation and comparison** — with state-of-the-art optimizers like Muon, MARS, and APOLLO."}, "weaknesses": {"value": "1. **Lack of theoretical grounding:**  \n   While empirical results are strong, there is no formal proof or convergence bound. The method’s stability under gradient noise or distributed variance is unaddressed.  \n\n2. **Over-reliance on empirical heuristics:**  \n   Hyperparameter selection (γ, β, ρ) is justified qualitatively but not theoretically. An analysis of sensitivity or scaling behavior would strengthen the argument.  \n\n3. **Limited interpretability of scaling dynamics:**  \n   Although Figure 3 shows per-block LR distributions, it remains unclear *why* specific architectural hierarchies (e.g., block-wise vs. layer-wise) improve performance.  \n\n4. **Missing discussion on training cost vs. accuracy trade-offs at ultra-large scale (≥7B):**  \n   Appendix A hints at robustness, but empirical validation stops at 3B.  \n\n5. **Minor editorial issues:**  \n   Some redundancy between main and appendix results; writing could be more streamlined in methodology sections."}, "questions": {"value": "1. How sensitive is SAC to architectural partitioning granularity? Would arbitrary grouping (not aligned to model structure) still yield improvements?  \n2. Could the hierarchical scale factors be *learned dynamically* (e.g., via meta-learning) instead of hand-designed MAD statistics?  \n3. What are the convergence characteristics under high learning rate regimes or mixed-precision instability?  \n4. How does SAC interact with low-rank fine-tuning (e.g., LoRA/Q-LoRA) in terms of memory scaling?  \n5. For distributed training, does aggregation of per-layer statistics introduce synchronization overhead at very large scales (100B+ params)?  \n6. Can the authors provide theoretical justification (even approximate) for the observed stability improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "stbDD2wj1g", "forum": "EB92tITeNq", "replyto": "EB92tITeNq", "signatures": ["ICLR.cc/2026/Conference/Submission4105/Reviewer_TB2v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4105/Reviewer_TB2v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965632288, "cdate": 1761965632288, "tmdate": 1762917178635, "mdate": 1762917178635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SAC (Scaling with Architectural Constraints), a lightweight wrapper placed on top of optimzier. SAC decomposes the effective learning rate into hierarchical factors at block/layer/group/parameter levels and multiplies these with the base per-parameter rate to coordinate updates across the model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Clarity and thoroughness**： The methodology is clearly articulated，and easy to understand.\n* **Scoping that fits distributed training**: The partition/indexing scheme is claimed to align with TP/FSDP and add only a small state and constant-time index lookups. This is important, as otherwise the added techniques is not general due to very complex design.\n* **drop-in idea with imporvement**: Clean wrapper with some performance gain at the cost of overhead."}, "weaknesses": {"value": "* Theory gap (conditioning & mechanisms): The paper’s own “Limitations” admits that rigorous theoretical analysis would help clarify mechanisms, especially effects on model conditioning. I feel the current methods are too heuristic with too many hyperparameters.\n* Sensitivity/generalization knobs: The approach introduces several new hyperparameters, lacking a guide for how to select them"}, "questions": {"value": "* The method introduces many hyperparameters; how to choose them, especially without a theory-driven approach?\n* The optimizer introduces extra overhead, actually. Do you expect a bigger/smaller overhead in the FSDP/TP training setting?\n* Can the method be extended to Zero-2/3 with model parallelism? If so, what is the overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6kSyAF1tEP", "forum": "EB92tITeNq", "replyto": "EB92tITeNq", "signatures": ["ICLR.cc/2026/Conference/Submission4105/Reviewer_NwGh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4105/Reviewer_NwGh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978739426, "cdate": 1761978739426, "tmdate": 1762917178336, "mdate": 1762917178336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}