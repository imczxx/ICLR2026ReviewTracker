{"id": "Kwbc8FXjJP", "number": 17254, "cdate": 1758273939090, "mdate": 1763638888730, "content": {"title": "JacobiGAD: Jacobi Polynomial–Powered Heterogeneous Graph-Level Anomaly Detection", "abstract": "Heterogeneous graph-level anomaly detection is vital for applications such as fraud detection and drug discovery, yet remains challenging due to mixed features, complex structures, and severe class imbalance. This paper introduces JacobiGAD, a unified framework that addresses these challenges through three key innovations. First, learnable multiscale filters based on Jacobi Polynomials adapt to different node and edge types, fusing multiple graph views to enhance anomaly signals. Second, these polynomials enable efficient approximation of targeted functions and naturally encode diverse geometries. Third, a Ricci Flow-inspired loss amplifies gradients for rare anomalies, mitigating class imbalance without distorting graph embeddings, ensuring stable convergence. Extensive experiments on real-world benchmarks show JacobiGAD outperforms the best baseline by up to 2.79\\% (AUROC), 7.78\\% (AUPRC), 7.11\\% (Recall@k), and 5.96\\% (F1-score) on average.", "tldr": "", "keywords": ["Heterogeneous Graph", "Anomaly Detection", "Jacobi Polynomials"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4968e77652640cdaea2445a0df78416d60d38c23.pdf", "supplementary_material": "/attachment/486d419603247e1a07168af1217133710f6a747a.zip"}, "replies": [{"content": {"summary": {"value": "JacobiGAD is a framework for heterogeneous graph-level anomaly detection that integrates a Jacobi-polynomial–based graph neural network (JPGNN) with a Ricci Flow Adaptive Curvature Enhancement (RFACE) loss."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* The use of Jacobi polynomials as a learnable graph filter is new; theoretical analysis (Theorems 2–7) proves convergence, information preservation, and bounded approximation error.\n\n* Multi-view fusion via Jacobi filters yields both injectivity and signal-to-noise amplification proportional to the number of views.\n\n* Links Jacobi bases to Laplace–Beltrami eigenfunctions across Euclidean, spherical, and hyperbolic geometries, suggesting geometric generality."}, "weaknesses": {"value": "* The paper is extremely difficult to follow. There is no clear narrative flow or visual guidance — the entire text consists mainly of dense equations, scattered theorems, and tables of numbers. While the method itself actually follows a relatively straightforward pipeline (JacobiGAD = Feature Alignment + Multi-view Fusion + JPGNN + RFACE Loss), the paper fails to convey this structure clearly. Including an overview figure or intuitive illustrations would make it much easier for readers to grasp. The numerous proofs and theoretical claims should be placed in the appendix rather than interrupting the main story.\n\n* The authors claim that “most existing anomaly detection models can only handle homogeneous graphs,” which is an oversimplified and somewhat misleading statement. Ironically, their own results show that heterogeneous models underperform homogeneous ones, without offering any explanation for this phenomenon. Moreover, the related work section cites multiple heterogeneous graph classification methods, yet the paper asserts that such models cannot handle heterogeneity — this contradiction undermines the central motivation. In reality, many heterogeneous graph-level anomaly detection approaches already exist (e.g., [1–4]), and the authors should clearly position their work among them.\n\n* The authors have not released their code, making it impossible to verify whether the reported training procedures truly match the described algorithm. Additionally, evaluation is limited to performance metrics (AUROC, AUPRC, etc.) without consideration of computational aspects such as training time, memory consumption, or scalability. For a model introducing high-order polynomial filters, such analysis is essential for a fair comparison.\n\n[1]HRGCN: Heterogeneous Graph-level Anomaly Detection with Hierarchical Relation-augmented Graph Neural Networks.\n[2]Chi-Square Wavelet Graph Neural Networks for Heterogeneous Graph Anomaly Detection.\n[3]FiGraph: A Dynamic Heterogeneous Graph Dataset for Financial Anomaly Detection\n[4]Deep Graph Anomaly Detection: A Survey and New Perspectives."}, "questions": {"value": "See weaknesss"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0QzOg5Npz8", "forum": "Kwbc8FXjJP", "replyto": "Kwbc8FXjJP", "signatures": ["ICLR.cc/2026/Conference/Submission17254/Reviewer_pgtn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17254/Reviewer_pgtn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734767240, "cdate": 1761734767240, "tmdate": 1762927205847, "mdate": 1762927205847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Modifications in the revised version of our manuscript"}, "comment": {"value": "We express our gratitude to all the reviewers for their thorough and constructive feedback. Taking into consideration the valuable comments provided by the reviewers, we intend to incorporate the following modifications in the revised version of our manuscript.\n\n---\nSection 4\n- add a figure of overview for JacobiGAD and the corresponding description (Reviewer **pgtn W1**)\n\nSection 5\n- shorten the main comparison experiment Tables 1, 2, and 3 (Reviewer **PBrV W1** and Reviewer **pgtn W1**)\n\n- add more detailed explanations for Tables 1, 2, and 3 (Reviewer **PBrV W1** and Reviewer **pgtn W1**)\n\n- move part of Ablation Study section from Appendix to Section 5 (Reviewer **PBrV W1** and Reviewer **pgtn W1**)\n\n- move Hyperparameter Sensitivity section from Appendix to Section 5 (Reviewer **PBrV W1** and Reviewer **pgtn W1**)\n\nAppendix\n- add a detailed description of the construction of datasets (Reviewer **jUAp W3**)\n\n- add the training time comparison and memory cost comparison in Appendix C (Reviewer **pgtn W3**)\n\n- move the remaining comparison Tables 1, 2, and 3 to Appendix E (Reviewer **PBrV W1** and Reviewer **pgtn W1**)\n\n- add comparison with Focal loss in Appendix H (Reviewer **jUAp Q2**)\n\n---\n\nThese modifications have been included in the revised version of our manuscript, which has been highlighted in blue to facilitate the reviewing process."}}, "id": "l3OuaF0ng8", "forum": "Kwbc8FXjJP", "replyto": "Kwbc8FXjJP", "signatures": ["ICLR.cc/2026/Conference/Submission17254/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17254/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17254/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763638947669, "cdate": 1763638947669, "tmdate": 1763638947669, "mdate": 1763638947669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies heterogeneous graph-level anomaly detection, a task complicated by mixed node/edge types, irregular structures, and extreme class imbalance. The authors propose JacobiGAD, a unified framework leveraging learnable multi-scale filters based on Jacobi polynomials to adaptively capture diverse graph patterns and fuse multiple structural views. The polynomial design also enables efficient approximation of targeted functions across different graph geometries. Additionally, a Ricci-Flow-inspired loss is introduced to strengthen gradients on scarce anomalies while preserving stable embedding optimization. Experiments on multiple real-world benchmarks demonstrate improvements over strong baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-written, no obvious typo.\n2. With many theories to prove the effectiveness."}, "weaknesses": {"value": "1. unsufficient experiment: This paper only report the main experiment in main text and even don't have ablation study.\n\n2. Too many theory: I sincerely admit the importance of propose a theory to explain the effective of the method from the perspective of math, but too many theory seems not appropriate in ICLR, maybe it's suit for AISTATS or some conference focus on theory.\n\n3. Anomaly detection in Heterogeneous Graph seems not a new task and have done by many works. [1,2]\n\n[1] Fast memory-efficient anomaly detection in streaming heterogeneous graphs\n\n[2] Thgnn: An embedding-based model for anomaly detection in dynamic heterogeneous social networks."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gUtGcxYYUP", "forum": "Kwbc8FXjJP", "replyto": "Kwbc8FXjJP", "signatures": ["ICLR.cc/2026/Conference/Submission17254/Reviewer_PBrV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17254/Reviewer_PBrV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809762257, "cdate": 1761809762257, "tmdate": 1762927204801, "mdate": 1762927204801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces JacobiGAD, a novel end-to-end framework for heterogeneous graph-level anomaly detection (GAD). JacobiGAD proposes two core technical contributions. First, it employs a spectral Graph Neural Network (GNN) that uses learnable Jacobi Polynomials as filters. These filters are designed to adapt to different node/edge types, fuse information from multiple graph views, and capture diverse geometric patterns (Euclidean, Spherical, Hyperbolic). Second, the paper introduces a Ricci Flow-inspired loss function (RFACE) to combat class imbalance by dynamically amplifying gradients for rare anomalous classes. The authors provide a suite of theoretical results to justify their design choices, covering aspects like information preservation, approximation efficiency, and loss convergence."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-structured and clearly articulates its core problem.\n2. The authors test their model on an impressive 15 datasets, including a private industrial dataset, which demonstrates its applicability to real-world problems."}, "weaknesses": {"value": "1. The method is trained in a supervised strategy, which optimizes for known anomalous modes but offers no explicit mechanism for handling unseen anomaly types or distribution shifts. As a result, the detector may overfit to the labeled anomaly patterns and fail to flag novel or rare patterns at test time. \n2. Theorem 2 claims the \"optimal choice\" of basis is Jacobi Polynomials. The proof sketch suggests it's an excellent choice due to its flexibility and orthogonality, but calling it \"optimal\" for any graph distribution is a very strong claim that may not hold universally. It is recommended that the authors provide a more detailed proof.\n3. Several datasets and transformations are unclear. For node classification datasets converted into graph-level via BFS, key details (BFS depth, subgraph size, sampling strategy, balancing, multiple seeds) are missing. The listed biological datasets (MCF-7, MOLT-4, etc.) and the very high numbers of node/edge types raise questions about provenance and preprocessing."}, "questions": {"value": "1. Regarding Theorem 2 (Optimality): The argument for Jacobi Polynomials being \"optimal\" relies on assumptions about the optimization landscape and spectral density. Could you elaborate on the conditions under which this optimality holds?\n2. Could you provide a clearer intuitive comparison between RFACE and Focal Loss? Both seem to achieve a similar goal of up-weighting hard/rare examples. What is the key advantage of the proposed dynamic adjustment based on the loss gradient over a simpler modulation factor based on prediction confidence like in Focal Loss?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0qIfAjYhRJ", "forum": "Kwbc8FXjJP", "replyto": "Kwbc8FXjJP", "signatures": ["ICLR.cc/2026/Conference/Submission17254/Reviewer_jUAp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17254/Reviewer_jUAp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17254/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986285248, "cdate": 1761986285248, "tmdate": 1762927203987, "mdate": 1762927203987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}