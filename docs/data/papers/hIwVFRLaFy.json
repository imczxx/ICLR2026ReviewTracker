{"id": "hIwVFRLaFy", "number": 19539, "cdate": 1758297055531, "mdate": 1759897033892, "content": {"title": "VisualPrompter: Semantic-Aware Prompt Optimization with Visual Feedback for Text-to-Image Synthesis", "abstract": "The notable gap between user-provided and model-preferred prompts poses a significant challenge for generating high-quality images with text-to-image models, compelling the need for prompt engineering.\nCurrent studies on prompt engineering can effectively enhance the style and aesthetics of generated images. \nHowever, they often neglect the semantic alignment between generated images and user descriptions, resulting in visually appealing but content-wise unsatisfying outputs. \nIn this work, we propose VisualPrompter, a novel training-free prompt engineering framework that refines user inputs to model-preferred sentences. \nVisualPrompter utilizes an automatic self-reflection module that identifies absent concepts in the generated images, followed by a target-specific prompt optimization mechanism which revises the prompts in a fine-grained manner. \nBy deconstructing prompts, introducing new elements at the atomic semantic level, and then reassembling them, our model is able to maintain semantic consistency and integrity throughout the optimization process.\nExtensive experiments demonstrate the effectiveness of VisualPrompter, which achieves new state-of-the-art performance on multiple benchmarks for text-image alignment evaluation. \nAdditionally, our framework features a plug-and-play design, making it highly adaptable to various generative models.", "tldr": "A novel training-free prompt engineering framework that refines user inputs for better productions.", "keywords": ["prompt engineering", "image generation", "diffusion model", "text-to-image synthesis"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2f079e10fc4c74f4a598edaee4bbb035b51c2fd.pdf", "supplementary_material": "/attachment/2d8c935ae47496f2bc87726d7baaa5ed04b169fb.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the semantic misalignment problem in text-to-image (T2I) generation, where sometimes model-preferred output images fail to match  user-provided prompts. The authors propose VisualPrompter, a training-free, semantic-aware prompt optimization framework that refines user prompts through atomic-level semantic decomposition based on the Davidsonian Scene Graph (DSG). \nBy parsing prompts into fine-grained semantic concepts through the Self-Reflection Module (SERE) and reconstructing coherent, model-preferred prompts via Target-Specific Prompt Optimization (TSPO), the method effectively produces prompts that better capture user intent and enhance text–image semantic alignment.The addressed problem is highly practically relevant for creative image generation and the proposed approach is easy to reproduce in real-world applications"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- By explicitly addressing the problem of semantic omissions, the authors provide a fresh direction for prompt engineering research, shifting the focus from “visual beauty” to semantic faithfulness.By detecting and repairing semantic omissions between user text and generated images, the framework improves  intent alignment, which are crucial for real-world creative and design applications.\n- The approach of decomposing prompts into atomic semantic units (entities, attributes, relations), using a Visual-Language Model (VLM) to detect absent ones, and then reassembling an optimized prompt with an LLM is conceptually new and technically creative  on the target problem. The integration of LLM reasoning and VLM verification within a self-reflective pipeline demonstrates strong originality.\n- The use of two authoritative benchmarks, DSG-1k (ICLR 2024) and TIFA v1.0 (ICCV 2023), is appropriate and technically justified, as both measure fine-grained semantic alignment between text and image — directly matching the paper’s objective."}, "weaknesses": {"value": "- Limited diversity of baselines: All three comparative methods (NeuroPrompts, Promptist, BeautifulPrompt)  share similar reinforcement-learning-based optimization paradigms.  The omission of  other omitted categories may weaken the empirical scope.\n- Improvements over the baseline are modest (≈ 4–5 points on DSG/TIFA).  Given that VisualPrompter adds several modules and increases inference time (Table 6), the cost–benefit balance remains questionable.Since all reasoning and evaluation rely on Qwen2 and Qwen2-VL, it is also unclear how robust the approach is under different model backbones.\n- The paper provides limited empirical evidence on the effectiveness of individual modules. More detailed analyses would strengthen the work — for example, examining which concept types (e.g., objects, attributes, or relations) or which modules (SERE vs TSPO) contribute most to performance gains."}, "questions": {"value": "1. Please clarify the definition  of “Baseline.”\nWhat exactly is the “Baseline” in Tables 1–3?  Is it simply the raw user prompt or an internal standardized prompt? Why does this baseline outperform some optimized methods?\n2. Please justify the selection of comparison methods.\n  Given that the Baseline achieves higher semantic alignment scores than other existing methods, please explain why NeuroPrompts, Promptist, and BeautifulPrompt were chosen as baselines of optimized methods.  Additionally, why were other families of methods (eg, multi-objective optimization methods, diffusion-specific) not considered for inclusion in the comparison?\n3. Robustness and Ablation\n  The paper would benefit from a more detailed ablation analysis showing the relative contribution of each module. Which component—SERE (Self-Reflection) or TSPO (Target-Specific Prompt Optimization) drives the largest improvements across different types of semantic concepts (e.g., objects, attributes, relations)? It would also be helpful to report whether the model’s gains are consistent across different diffusion backbones (e.g., SD v1.5 vs Flux-dev) and whether any module exhibits degradation or instability when applied to more complex or compositional prompts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YDb2EoYbHA", "forum": "hIwVFRLaFy", "replyto": "hIwVFRLaFy", "signatures": ["ICLR.cc/2026/Conference/Submission19539/Reviewer_5nxL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19539/Reviewer_5nxL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913904670, "cdate": 1761913904670, "tmdate": 1762931426095, "mdate": 1762931426095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VisualPrompter, a training-free, plug-and-play framework that uses self-reflection to detect missing concepts and performs atomic-level prompt edits to improve semantic alignment between user descriptions and text-to-image outputs. Experiments show state-of-the-art results on multiple text–image alignment benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Visual Prompter leverage visual-language models (VLMs) for question–answer-based detection of missing semantic concepts in generated images, aligns with human intuition and exhibits high interpretability.\n2. Visual Prompter significantly outperforms current state-of-the-art prompt engineering methods in multiple benchmarks."}, "weaknesses": {"value": "1. The user study compares Visual Prompter only with the baseline (original prompts), rather than with other prompt optimization methods. \n2. Lacks comparison with recent methods, such as 《TIPO: Text to Image with Text Presampling for Optimal Prompting.》\n3. In Figure 11, the original prompts themselves are ambiguous and unnatural for human expression, such as “person next to person” or “bottle on the left of bottle.” I would like to see the performance of VisualPrompter on more natural and human-like prompts, such as those mentioned in Figure 1."}, "questions": {"value": "Questions：\n1. VisualPrompter uses Qwen2-VL as the visual question answering model; however, SEMANTIC ACCURACY is also evaluated using Qwen2-VL as the assessment model. Would this introduce a bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZTVnYYLu0P", "forum": "hIwVFRLaFy", "replyto": "hIwVFRLaFy", "signatures": ["ICLR.cc/2026/Conference/Submission19539/Reviewer_wv9m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19539/Reviewer_wv9m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921776150, "cdate": 1761921776150, "tmdate": 1762931425617, "mdate": 1762931425617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to improve the semantic alignment between generated images and user descriptions. \n\n\nIt proposes VisualPrompter, a training-free prompt engineering framework that iteratively refines user prompts. VisualPrompter has a self-reflection module that analyzes generated images, and a target-specific prompt optimization that revises the prompt later. The method is plug-and-play and achieves state-of-the-art alignment on multiple image generation benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Easy to use: the proposed VisualPrompter is model-agnostic and plug-and-play, making it highly adaptable to various generative models.\n- Good results: VisualPrompter outperforms many baselines on multiple benchmarks and multiple generative models, as shown in Table 1."}, "weaknesses": {"value": "- Auxiliary LLM bias: Introducing an additional LLM in the loop may inject its own biases, especially there’re multiple LLM calls. \n- Compute overhead and latency: the generate - analyze - revise cycles may be significantly more expensive than a single forward pass. In addition, LLMs were called multiple times in one image generation, which might be costly. \n - Limited contribution: modules are not novel. For example, regarding the reflection module, the LLM Expander, the LLM Composer, similar techniques can be found in references. It'll be great if author could explain what's the unique contribution and novelty in VisualPrompter."}, "questions": {"value": "- Robustness: How is the performance with difficult prompts, for example, long prompts, multilingual prompts, code-mixed prompts? Could you show any qualitative failure cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0ek78147XH", "forum": "hIwVFRLaFy", "replyto": "hIwVFRLaFy", "signatures": ["ICLR.cc/2026/Conference/Submission19539/Reviewer_cJ9v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19539/Reviewer_cJ9v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964432559, "cdate": 1761964432559, "tmdate": 1762931425192, "mdate": 1762931425192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}