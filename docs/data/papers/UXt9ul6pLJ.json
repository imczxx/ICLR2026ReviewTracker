{"id": "UXt9ul6pLJ", "number": 8558, "cdate": 1758090916854, "mdate": 1759897776587, "content": {"title": "PSPO: Trainable Potential-Based Reward Shaping with Internal Model Signals for Post-Training Policy Optimization of Large Language Models", "abstract": "Reinforcement learning from human feedback (RLHF) has become the de-facto paradigm for aligning large language models (LLMs), yet mainstream algorithms either incur the memory overhead of a value head (PPO) or remain vulnerable to sparse and miscalibrated rewards (GRPO, DPO). We propose Potential-Shaped Policy Optimization (PSPO), a lightweight, critic-free framework that converts coarse scalar feedback into dense, context-aware signals by learning a trainable potential function. A 22.7M-parameter MiniLM network (the Potential Network) ingests inexpensive internal model signals (token embeddings, attention entropy, policy entropy) to produce adaptive shaping terms, while an alternating optimization scheme stably co-trains the policy and potential without extra rollouts. On eight English and Chinese mathematical-reasoning benchmarks, a 14B Qwen2.5 model trained with PSPO achieves strong accuracy under our matched setup (68.1\\% on GSM8K; 41.6\\% on MATH) and exceeds PPO/DPO/GRPO by up to12.5 points. PSPO remains critic-free and adds only $<\\!3\\%$ wall-clock overhead in our measurements, while yielding interpretable token-level reward attributions. These results highlight signal-aware reward shaping as a practical route toward efficient, stable, and scalable RLHF for large language models.", "tldr": "We introduce PSPO, a critic-free RLHF framework that uses trainable potential functions over internal LLM signals to convert sparse scalar rewards into dense token-level feedback.", "keywords": ["Reinforcement Learning", "Large Language Models", "Reward Shaping", "Policy Optimization", "Critic-Free Methods", "Internal Model Signals"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa7a0cc6de45260e04426ca15b6e0f086e3717e4.pdf", "supplementary_material": "/attachment/d25f3ff0626844112a8cac587d780c651962ed61.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a critic RL algorithm for reasoning models, PSPO, which learns a potential function $\\Phi_\\theta$ with a mini model from the LM's internal signals. Using shaped rewards $r_t' = r_t + \\gamma \\Phi_\\theta(s_{t}) - \\Phi_\\theta(s_{t-1})$, it reshape sparse rewards into dense, and the optimal policy is not altered.\nIn practice, advantages approximate the group normalized $R_{\\text{final}} - \\Phi_\\theta(s_t)$.\nIn experiments on several reasoning benchmarks, PSPO outperforms PPO, DPO, and GRPO. Ablation experiments are conducted to show the importance of each component."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed potential network is small (≈22 M parameters) and adds < 3 % computational overhead, making the method efficient.\n- Experiments are conducted are multiple reasoning benchmarks (English + Chinese).\n- The two-phase update (policy phase + potential phase) is conceptually simple and practically effective."}, "weaknesses": {"value": "- This paper is not well-written, and it is hard to understand the exact details of their algorithms (see my question 1). The authors should provide a rigorous and detailed illustration of their workflow in the revision. The training objective of the RL algorithm is even not in the main content. There are also mixed abuses of notations like $r$, $s$, $s'$, $t$, $s_t$, $G_t$ throughout the paper, which are not well clarified.\n\n- The improvement on benchmarks is marginal in table 1, and the performance of base model is too bad (see my question 4). And the experiments are only conducted one kind of base model, qwen2.5-14B, making the improvements of PSPO not convincing.\n\n- Proposition 4.1 is trivially correct as it's an old result in [1], however it is not stated clearly in the paper. Please add a citation for Prop. 4.1.  The proofs in Section 4.1 is not related to the main algorithm, since the trust region approach is deployed in the algorithm, which makes the potential and drift trivially bounded. Therefore, the contribution of Section 4 is minimal.\n\n- It is unclear whether the $\\phi_\\theta$ will converge (and converge to what?). And the paper interprets $\\phi_\\theta$ as \"momentum\", which is very unclear (see my question 6).\n\n[1] Ng et al. Policy invariance under reward transformations: Theory and application to reward shaping."}, "questions": {"value": "1. Section 3.6 is very confusing: (i) should $r$ and $r'$ be $r(s')$ and $r'(s')$? (ii) how is the rtg calculated? (iii) how is it normalized? (iv)  what is the expression of $\\hat A_t$?\n2. What's the difference between PSPO (-shaping) and PSPO (-interalsignals)?\n3. What's the difference between PSPO (-shaping) and GRPO/Dr.GRPO [1]?\n4. Why is the performance so low on GSM8k? The Qwen2.5-14B is reported to obtain 90.2 on that benchmark, while PPO, DPO, GRPO, PSPO are all below 70 in this paper.\n5. Why is a small model, MiniLM, able to predict the potential function, is there any intuition?\n6. What's the difference between the \"momentum\" and Q/advantage function? It seems that the rtg $\\hat A_t$ is just $\\bar r$ (the group-normalized reward), since for LLMs each trajectory only has one reward signal. Then why are we optimizing $(\\phi_\\theta(s')-\\phi_\\theta(s)-r)^2$, what does this mean?\n\n[1] Liu et al. Understanding R1-Zero-Like Training: A Critical Perspective. https://arxiv.org/abs/2503.20783\n\n[2] Qwen2.5 Technical Report. https://arxiv.org/pdf/2412.15115"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k75euyZwjC", "forum": "UXt9ul6pLJ", "replyto": "UXt9ul6pLJ", "signatures": ["ICLR.cc/2026/Conference/Submission8558/Reviewer_DSHa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8558/Reviewer_DSHa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761359980152, "cdate": 1761359980152, "tmdate": 1762920412576, "mdate": 1762920412576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors tackle the problem of sparse rewards in the classic RLHF and RLVR style algorithms for LLM in the tasts with terminal rewards. Although a critic model in PPO algorithm provides a dense signal at each token, it is often unstable due to joint training of the policy and the critic. To avoid depending on a critic and still providing a dense signal during training, the authors propose a new method Potential-Shaped Policy Optimization (PSPO), which leverages a learned potential model to shape the per-step reward and provide additional dense signal at per token level within the LLM RL setting.\nUnlike previous reward shaping works, which keep potential functions static, the authors train a tiny LM (3 orders of magnitude smaller than policy LLM) on top of policy model's internal signals including - final layer token embeddings, attention entropy and policy entropy. \nTo stabilize training they conduct alternating policy and potential LM training while reusing the same set of rollouts. This overall makes their training overhead <3% on top of existing critic free methods. \nTo train the policy, they use GRPO style critic free loss but with dense advantage signal (per-token advantage is re-shaped with potential LM's discounted score difference). While to train the potential LM, they use un-shaped advantage weights as targets w/ mean squared error loss.\n\nOverall, experiments with Qwen 2.5 14B model on 8 different benchmarks from math, knowledge and reasoning domains (English and Chinese language) showed that their method consistently outperformed PPO, GRPO and ablations in all tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- PSPO introduced a very light overhead on the optimization loop due to use of small \n- The overall idea of using potential score model to avoid critic is sound and inspired by previous work."}, "weaknesses": {"value": "- The paper definitely seems to have used a lot of AI in its writing. For example, I have noticed multiple paraphrases of introduction to various components, method explanation and results discription repeated in first 2 pages and the method section (Section 3). Much of this text feels superfluous and repeatitive. Subsequently, the bulk of the equations and core algorithm is moved to appendix which disrupts the reading flow.\n- The paper cites other dense reward shaping works in the related works, for example, LM-Critic, PAR, ONI etc. but didn't compare with them in the Table 1. There are also other kinds of dense signal introducing methods without additional critic overhead such as on-policy distillation (GKD) [1]. The current work would benefit from more thorough comparison with these works.\n- Although I understand there can be resource constraints, the dense reward signal will be most effective in the newer reasoning models and long response tasks. To strengthen my confidence in the results, I would have really appreciated experiments with newer reasoning models (qwen3 4b) instead of <1k response length Qwen 2.5 14B model.\n- The paper multiple time mentions PSPO is effective strategy in miscalibrated rewards, but doesn't give experimental justification for this claim.\n\n[1] Agarwal, Rishabh, et al. \"On-policy distillation of language models: Learning from self-generated mistakes.\" The twelfth international conference on learning representations. 2024."}, "questions": {"value": "Questions about the equations and notations:\n1. The advantage $A^{\\text{shape}}$ is defined as \"normalized token-level return to go\" in section 3.6. What is the exact forumla used here? Readers would benefit from an explicit definition.\n2. Similarly what is mathematical definition of \"unshaped advantage $\\hat{A}_t$ with a small trust region\"? \n\nIf these are defined in the appendix, somewhere then they should really be part of the main text of the paper.\n\nOther questions:\n- The shaped reward is defined as $r' = r + \\gamma \\Phi_{\\theta} (s') - \\Phi_{\\theta} (s)$. Here $r$ is a terminal reward and $\\gamma=1.0$ in all the experiments. IIUC unshaped advantage would be rollout group level fixed scalar that will be used as the target for all potential differences? Although, I'm also using a subscript $t$ in $\\hat{A}_t$ which implies there are differences at each token. An exact definition would clarify this important detail.\n- The figure 1 PSPO is not in full agreement with the text. The Reference Model KL and Reward model are combined to get $r'$ which is not what the notation says."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V8DXhzLVuX", "forum": "UXt9ul6pLJ", "replyto": "UXt9ul6pLJ", "signatures": ["ICLR.cc/2026/Conference/Submission8558/Reviewer_gqrc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8558/Reviewer_gqrc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802795401, "cdate": 1761802795401, "tmdate": 1762920412021, "mdate": 1762920412021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Potential-Shaped Policy Optimization (PSPO), a critic-free reinforcement learning framework for post-training alignment of large language models. The central innovation is a trainable \"Potential Network\" that leverages internal LLM diagnostics—specifically token embeddings, attention entropy, and policy entropy—to generate dense, context-aware reward signals that augment sparse external rewards. The method employs alternating optimization between the policy network and the potential network under trust-region constraints, eliminating the need for a traditional value head and reducing memory overhead. The authors provide theoretical analysis establishing policy invariance under bounded potential drift and demonstrate the approach through experiments on eight English and Chinese mathematical reasoning benchmarks (GSM8K, MATH, and their variants), reporting consistent improvements over PPO, DPO, and GRPO baselines with minimal additional computational cost (<3% overhead). The paper claims four main contributions: (1) the first framework integrating trainable potential-based reward shaping for LLM alignment, (2) novel exploitation of internal model signals for adaptive shaping, (3) theoretical guarantees for alternating optimization stability, and (4) empirical validation demonstrating both effectiveness and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Methodological efficiency: Proposes the use of a trainable potential function leveraging internal policy diagnostics, allowing adaptive, dense, and context-dependent rewards without the need for a memory-intensive value head.\n\n- Relatively strong theoretical grounding. The paper provides detailed theoretical analysis, including formal propositions and proofs showing policy invariance under bounded potential drift, which helps justify its design choices."}, "weaknesses": {"value": "1. Critical experimental credibility gap undermining confidence. The reported accuracies (GSM8K 68.1%, MATH 41.6%) are 20-35 percentage points below public benchmarks for similar models[1][2], yet no training curves, data specifications, or convergence analysis are provided to explain this gap. Without this transparency, it is impossible to distinguish legitimate controlled comparison from systematic implementation errors or insufficient training, fundamentally undermining experimental validity.\n\n[1] First Return, Entropy-Eliciting Explore\n\n[2] GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning\n\n2. Core hypothesis lacks direct empirical validation. The central claim that \"internal signals reflect uncertainty and reasoning progress\" is asserted but never validated through correlation analysis between signals and actual rewards. While ablations show signals improve performance, this only demonstrates utility to the learned function rather than validating the claimed mechanism, leaving open the possibility of spurious correlations like simply rewarding response length through position-aware features.\n\n3. Theoretical guarantees are incomplete and stability undefined. The concept of \"mutual stability\" is never formally defined, and Proposition 4.1 provides only conditional guarantees requiring Σδₖ < ∞ without proving the algorithm achieves this bound. No empirical verification of drift values or oscillation analysis is provided, and transient behavior when the potential function is poorly trained early in optimization is not analyzed.\n\n4. Severely limited generalizability contradicting generic claims. All experiments use exclusively Qwen2.5-14B (single architecture) and only mathematical reasoning tasks (single task family), directly contradicting claims of \"generic\" and \"model-agnostic\" solution. The method's explicit focus on \"multi-step reasoning\" suggests it may be tailored to chain-of-thought tasks and fail on single-turn QA, creative generation, or code synthesis where notions of \"progress\" differ fundamentally.\n\n5. Insufficient training budget without convergence analysis. The 300M token budget (45K steps) provides no evidence of convergence through training curves or comparison with longer runs. The relative improvements could reflect faster convergence rather than superior final performance, fundamentally changing the nature of the contribution from \"better performance\" to merely \"faster convergence under limited budgets.\""}, "questions": {"value": "Q1: Can you explain the performance gap and provide convergence evidence? Your results (GSM8K 68.1%, MATH 41.6%) are 20-35 percentage points below published benchmarks for similar models. Please provide: (a) training curves showing loss and accuracy over 45K steps for all methods, (b) complete specification of your RL training data (size, source, quality control), (c) explanation for the performance gap—have you validated your baseline implementations against public versions? (d) results with longer training budgets (450M-600M tokens) to demonstrate convergence.\n\nQ2: Can you validate that internal signals actually reflect reasoning progress? Please provide direct evidence through: (a) correlation analysis (Spearman/Pearson ρ) between internal signals and final rewards, (b) partial correlations controlling for confounds like sequence length and position, (c) visualization comparing signal trajectories for correct versus incorrect solutions, (d) control experiments with uninformative signals (random noise, shuffled embeddings) to rule out spurious correlations. Can you specifically address whether the position-aware features simply reward response length rather than reasoning quality?\n\nQ3: Can you formally define mutual stability and prove your algorithm achieves it? Please provide: (a) formal mathematical definition of \"mutual stability\" referenced in Section 1.2, (b) proof that Algorithm 1 achieves the bounded cumulative drift (Σδₖ < ∞) required by Proposition 4.1—how do trust region constraints ensure this bound? (c) empirical plots of δₖ over training iterations, (d) analysis of potential harmful bias during early training when the potential function is poorly initialized.\n\nQ4: Can you demonstrate generalizability beyond mathematical reasoning? Please provide: (a) results on at least one other model architecture (LLaMA-3, Mistral, DeepSeek) to validate \"model-agnostic\" claims, since internal signals are architecture-dependent, (b) evaluation on at least one non-mathematical task (code generation, instruction following, or safety alignment) to support \"generic solution\" claims. If infeasible, please provide theoretical analysis characterizing when PSPO works versus fails—what task properties make it effective?\n\nQ5: Can you characterize training dynamics and distinguish convergence speed from final performance? Please clarify: (a) does PSPO achieve faster convergence, better asymptotic performance, or both? Provide accuracy-vs-steps curves for all methods. (b) When does the potential network converge—plot ||φₖ - φₖ₊₁|| over iterations. (c) Have you evaluated with longer training (600M+ tokens)—do baselines catch up or does PSPO maintain advantages? This distinction fundamentally changes whether your contribution is efficiency or performance improvement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q3uEJYJnOC", "forum": "UXt9ul6pLJ", "replyto": "UXt9ul6pLJ", "signatures": ["ICLR.cc/2026/Conference/Submission8558/Reviewer_3hH9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8558/Reviewer_3hH9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825904668, "cdate": 1761825904668, "tmdate": 1762920411682, "mdate": 1762920411682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PSPO (Potential-Shaped Policy Optimization), a new and efficient method to fine-tune large language models with reinforcement learning from human feedback. Instead of relying on sparse or delayed rewards, PSPO learns a trainable potential function that turns coarse feedback into dense, token-level signals. This potential is a small network (22.7M parameters) that uses internal model information—like token embeddings, attention entropy, and policy entropy—to provide richer reward signals. PSPO trains the policy and the potential function in two alternating steps: one updates the policy using shaped rewards, and the other updates the potential using return residuals. This design avoids the need for a value head (critic) and adds less than 3% extra training cost. On eight English and Chinese math reasoning benchmarks, a 14B Qwen2.5 model trained with PSPO outperforms PPO, DPO, and GRPO by up to 12.5 points, achieving 68.1% on GSM8K and 41.6% on MATH."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes learning a *trainable potential* over cheap internal LLM diagnostics (final-layer embeddings, attention entropy, policy entropy) and injecting it via potential-based shaping to make sparse RLHF rewards token-level and trajectory-aware — while preserving policy invariance in theory. This is a principled, interpretable extension of classical potential-based shaping to LLM RLHF.\n2. Under a matched setup on a large backbone (Qwen2.5-14B), PSPO shows consistent, sometimes large, improvements over competitive baselines (PPO, DPO, GRPO) across 8 English/Chinese math benchmarks (e.g., GSM8K 68.1% vs. baselines; MATH 41.6%), and also improves open-ended instruction metrics (ShareGPT, HelpfulQA). The head-to-head comparisons and Table 1 / Table 8 support the claim that the method meaningfully improves difficult, sparse-reward tasks."}, "weaknesses": {"value": "1.  Assumptions in the theoretical guarantees may be strong / under-validated. Policy invariance results require bounded cumulative drift of $\\Phi$ (Lipschitzness in $\\theta$, bounded gradients, trust-region/diminishing steps). While the paper sketches how AdamW+cosine decay or trust regions can satisfy these, practical LLM training often violates such neat bounds (non-Lipschitz behavior, rare large updates). More empirical diagnostics (empirical $\\delta_k$ envelopes, sensitivity to trust-region settings, failure cases) should be included to bridge theory and practice.\n2. The shaped reward is $r^\\prime (s, a, s^\\prime) = r (s, a, s^\\prime) + \\gamma \\Phi\\_{\\theta} (s’) - \\Phi\\_{\\theta} (s)$, so any systematic bias in the scalar reward r can be propagated or amplified by $\\Phi$. The authors acknowledge this and propose mitigations (red-teaming, ensembling, trust-region caps), but the paper lacks a focused empirical stress-test showing behavior when the reward model is miscalibrated or adversarial. This is an important practical failure mode for RLHF pipelines."}, "questions": {"value": "1. The paper describes training the Potential Network $\\Phi_{\\theta}$ to fit unshaped, centered advantages. Could the authors clarify *why* this specific target was chosen over alternatives (e.g., TD returns, critic estimates)? How sensitive is performance to the choice of baseline or normalization used in advantages?\n2. The theoretical section and Appendix C mention an trust region on successive $\\Phi$ updates. What exact $\\tau_{k}$ (trust region radius) values are used in experiments, and how are they scheduled over time? How often does the projection step actually activate? Quantitative statistics (e.g., percentage of batches that trigger the cap) would make the stability argument more convincing.\n3. The paper fixes $\\gamma = 1$ in most experiments. Have the authors tried smaller $\\gamma$ values (e.g., 0.9, 0.95) to control long-range propagation? How does changing $\\gamma$ affect the empirical stability and final accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VvJl0OuUsE", "forum": "UXt9ul6pLJ", "replyto": "UXt9ul6pLJ", "signatures": ["ICLR.cc/2026/Conference/Submission8558/Reviewer_V5DK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8558/Reviewer_V5DK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917696078, "cdate": 1761917696078, "tmdate": 1762920411229, "mdate": 1762920411229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}