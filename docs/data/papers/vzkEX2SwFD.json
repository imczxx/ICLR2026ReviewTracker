{"id": "vzkEX2SwFD", "number": 4360, "cdate": 1757666152692, "mdate": 1763550927426, "content": {"title": "Activation Steering with a Feedback Controller", "abstract": "Controlling the behaviors of large language models (LLMs) is fundamental to their safety alignment and reliable deployment. However, existing steering methods are primarily driven by empirical insights and lack theoretical performance guarantees. In this work, we develop a control-theoretic foundation for activation steering by showing that popular steering methods correspond to the proportional (P) controllers, with the steering vector serving as the feedback signal. Building on this finding, we propose Proportional-Integral-Derivative (PID) Steering, a principled framework that leverages the full PID controller for activation steering in LLMs. The proportional (P) term aligns activations with target semantic directions, the integral (I) term accumulates errors to enforce persistent corrections across layers, and the derivative (D) term mitigates overshoot by counteracting rapid activation changes. This closed-loop design yields interpretable error dynamics and connects activation steering to classical stability guarantees in control theory. Moreover, PID Steering is lightweight, modular, and readily integrates with state-of-the-art steering methods. Extensive experiments across multiple LLM families and benchmarks demonstrate that PID Steering consistently outperforms existing approaches, achieving more robust and reliable behavioral control.", "tldr": "We propose Proportional–Integral–Derivative (PID) Steering, a principled framework that leverages the full PID controller for activation steering in LLMs.", "keywords": ["activation steering", "behaviour control", "alignment", "PID control", "mechanistic interpretability", "language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/34fc455f00bdf9b2be5dd2379a3f793113e626fc.pdf", "supplementary_material": "/attachment/d28259fbe057df3cf91cb90e9c947071d1ef1682.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to address the lack of theoretical guarantees, presence of steady-state errors, and overshoot issues in existing activation steering methods. It analogizes the inter-layer activation propagation process in LLMs to a dynamical system, framing popular methods as instances of a Proportional (P) controller. Building on this, it introduces the Proportional-Integral-Derivative (PID) controller from control systems theory and proposes the PID Steering framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work provides a unified control-theoretic perspective, categorizin existing methods (e.g. ActAdd, DirAblate, Mean-AcT) as specific types of P controllers, thereby logically leading to the integration of the PID controller concept and the proposal of the PID Steering framework.\n\n2. The paper goes beyond conceptual proposal by providing rigorous mathematical derivations, including error dynamic modeling and PID controller discretization.\n\n3. PID Steering is designed as a lightweight, plug-and-play module that can be seamlessly integrated into various existing steering frameworks (e.g., Angular Steering, Mean-AcT) by replacing their original steering vector construction, facilitating widespread adoption."}, "weaknesses": {"value": "1. Manual tuning of PID parameters: the PID controller parameters $K_p, K_i, K_d $ appear to require manual tuning. Finding optimal settings involves significant trial and error, which is somewhat cumbersome.\n\n2. Lack of analysis on $I, D$ Independent contributions and combinatorial effects. The experimental section fails to demonstrate the independent contributions and combinatorial effects of the $P, I,$ and $D$ modules, since the authors claim their method is a plugin fashion. Lack of the experiment setting to demonstrate the benefits by introducing the $I$ or $D$ to the $P$-class controllers.\n\n3. Incomplete experimental comparison with cited methods: While the introduction and background sections systematically review mainstream activation steering methods, Section 5 does not include comparative tests against all mentioned methods (e.g., ActAdd). This prevents intuitive verification of the PID framework's optimization effects across different underlying $P$ controllers."}, "questions": {"value": "1. Do the trial-and-error costs associated with manual PID parameter tuning vary across different task scenarios? Could parameters be highly sensitive and difficult/costly to tune in certain task types?\n\n2. If testers used harmful prompts for evaluation, would PID Steering suffer performance loss?\n\n3. Why the activation steering vector is added to every layer?  If added to only part optimal selected layer(s), the method offer advantages over identifying and intervening?\n\n4.  Unclear Visual Representation in Figures: Figure 3, as key evidence supporting the core argument, lacks clarity in visualization and exposition. One theoretical cornerstone is that the integral term eliminates steady-state error. However, the PI curve in the figure ultimately displays saturation and large overshoot, obscuring this advantage and making it difficult for readers to appreciate the effectiveness of this step. Adding a subplot specifically comparing P and PI could highlight this advantage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MygxNrVV59", "forum": "vzkEX2SwFD", "replyto": "vzkEX2SwFD", "signatures": ["ICLR.cc/2026/Conference/Submission4360/Reviewer_A2rW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4360/Reviewer_A2rW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927950678, "cdate": 1761927950678, "tmdate": 1762917315773, "mdate": 1762917315773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a control-theoretic framework for activation steering in LLMs, named PID Steering. The motivation here is that traditional activation steering methods, such as Activation Addition, Directional Ablation, and Mean Activation Transport,modify internal model activations to control behaviors like toxicity or refusal, but they lack formal guarantees and often rely on empirical heuristics.\n\nBased on this the authors introduce PID for alignment, removing residual bias and mitigating overshoot when aligning.\n\nResults demonstrate how PID control improves convergence and stability, reducing steady-state errors. Empirical experiments across multiple models (Gemma2, LLaMA3, Qwen2.5) and tasks (toxicity mitigation, jailbreak prevention, and image style control) show that PID Steering outperforms prior activation steering methods, achieving more consistent and interpretable control without harming overall model performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes an interesting methods of control-theoretic interpretation of activation steering.\n\n- The proposed PID Steering introduces components that correct long-term drift and prevent oscillations making it robust.\n\n- The approach is tested across diverse tasks (toxicity, jailbreaks, style transfer) and modalities (text and images), showing robustness and generality."}, "weaknesses": {"value": "- Selecting appropriate PID gains (Kp, Ki, Kd) is nontrivial, and may result in suboptimal solutions.\n\n- While results are broadly positive, the paper lacks detailed ablation studies on scenarios where PID might underperform or destabilize.\n\n- Claims that PID Steering is lightweight are mentioned but not benchmarked in terms of latency or inference-time overhead.\n\n- It is unclear to me how controller parameters generalize across model architectures or domains without retraining.\n\n- The much smaller models such as Gemma is also used in this paper. Although, larger models (24B or 70B) are not tested, I was wondering if the authors could already make a size comparison between the models and present the observed differences?"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZNHQrzU05a", "forum": "vzkEX2SwFD", "replyto": "vzkEX2SwFD", "signatures": ["ICLR.cc/2026/Conference/Submission4360/Reviewer_hCNg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4360/Reviewer_hCNg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042229972, "cdate": 1762042229972, "tmdate": 1762917314904, "mdate": 1762917314904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel perspective on activation steering for large language models and diffusion models by introducing a control-theoretic framework for managing model behavior. The paper provides theoretical analysis using input-to-state stability ISS and demonstrates empirical gains on toxicity reduction, jailbreak resistance, and style control tasks across several models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It is novel to connect the control theory and activation steering in LLMs.\n2. The paper provides a solid mathematical formalization of steering as a dynamic control process. \n3. Experiments are comprehensive, covering both text (toxicity, jailbreak) and vision (style transfer) domains."}, "weaknesses": {"value": "1. Lack of baselines: The experiments compare primarily against the sequential steering vector ****baseline and, to a lesser extent, static activation addition. This is an insufficiently broad comparison. Many other steering methods, such as CAA and ITI, could serve as competitive baselines. \n2. The paper evaluates toxicity using LLaMA-3-8B both as the generator and as the evaluator, which introduces a self-evaluation bias: the same model family that produces text also judges its toxicity. Such evaluations are not independent and can underreport toxicity. A separate toxicity classifier such as GPT-4 or at least cross-model correlation analysis should be used for reliable measurement.\n3. The paper does not report ablations on the $K_P, K_I, K_D$, though the performance of control systems is typically sensitive to them. Similarly, computational overhead, such as inference latency and memory increase is unreported, which is important for large models.\n4. The theoretical analysis assumes locally linearized activation dynamics (Eq. 29–32). While this yields tractable ISS proofs, the approximation error may be significant in deep nonlinear transformers, especially under strong steering perturbations."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iLAht6SRVJ", "forum": "vzkEX2SwFD", "replyto": "vzkEX2SwFD", "signatures": ["ICLR.cc/2026/Conference/Submission4360/Reviewer_NQp6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4360/Reviewer_NQp6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120147159, "cdate": 1762120147159, "tmdate": 1762917314691, "mdate": 1762917314691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Revision"}, "comment": {"value": "Incorporating the comments and suggestions from all reviewers, besides fixing typos and notations, we have made the following main changes in the revised paper:\n\n**1. New baselines and expanded comparisons:** We have added additional activation-steering baselines (ActADD [1], AURA [2], ITI-C [3]) to the toxicity experiments (Tables 1 and 2). CAA results will be added in the next response round.\n\n**2. Additional model-size experiment:** We have added a new experiment on Gemma2-27B-Instruct for the jailbreak task, reported in Table 2 (Section 5.2). Overall, ITI and RePE degrade strongly on large models, whereas PID scales better and achieves the highest ASR. \n\n**3. New Ablation Studies on $K_p, K_i, K_d$:** We conducted a full ablation over $K_p, K_i, K_d$ for Jailbreaking on Gemma-2-9B-it and Toxicity mitigation on Gemma2-2B. The results are reported in Figure 6 of Section 5.4. \n\n**4. Improved Explanation of Figure 3 (P vs PI vs PID):** We have added a clearer explanation for Figure 3, from line 249 to line 265.\n\n**5. Analysis Beyond Local Linearization:** We added a new section in Appendix B.8 that analyzes how the higher-order nonlinear terms enter the error dynamics and discusses their impact on the closed-loop behaviour.\n\nWe will update this summary in our next response once we have obtained additional experimental results.\n\n**References**\n\n[1] Li et al. “Inference-Time Intervention: Eliciting Truthful Answers from a Language Model”, NeurIPS, 2024, URL: https://proceedings.neurips.cc/paper_files/paper/2023/file/81b8390039b7302c909cb769f8b6cd93-Paper-Conference.pdf\n\n[2] Turner et al. “Steering Language Models with Activation Engineering”, OpenReview preprint, 2025, URL: https://openreview.net/forum?id=2XBPdPIcFK\n\n[3] Suau et al. “Whispering Experts: Neural Interventions for Toxicity Mitigation in Language Models”, ICML, 2024, URL: https://openreview.net/forum?id=2P6GVfSrfZ"}}, "id": "L9533FrTxu", "forum": "vzkEX2SwFD", "replyto": "vzkEX2SwFD", "signatures": ["ICLR.cc/2026/Conference/Submission4360/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4360/Authors"], "number": 31, "invitations": ["ICLR.cc/2026/Conference/Submission4360/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763554822744, "cdate": 1763554822744, "tmdate": 1763560644064, "mdate": 1763560644064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}