{"id": "EbV2YIKU9o", "number": 14858, "cdate": 1758244761924, "mdate": 1759897345083, "content": {"title": "3DRot: 3D Rotation Augmentation for RGB-Based 3D Tasks", "abstract": "RGB-based 3D tasks, e.g., 3D detection, depth estimation, 3D keypoint estimation, still suffer from scarce, expensive annotations and a thin augmentation toolbox, since most image transforms, including resize and rotation, disrupt geometric consistency.\nIn this paper, we introduce 3DRot, a plug-and-play augmentation that rotates and mirrors images about the camera's optical center while synchronously updating RGB images, camera intrinsics, object poses, and 3D annotations to preserve projective geometry-achieving geometry-consistent rotations and reflections without relying on any scene depth.\nWe validate 3DRot with a classical 3D task, monocular 3D detection.\nOn SUN RGB-D dataset, 3DRot raises $IoU_{3D}$ from 43.21 to 44.51, cuts rotation error (ROT) from 22.91$^\\circ$ to 20.93$^\\circ$, and boosts $mAP_{0.5}$ from 35.70 to 38.11. By comparison, Cube R-CNN adds 3 other datasets together with SUN RGB-D for monocular 3D estimation, with a similar mechanism and test dataset, increases $IoU_{3D}$ from 36.2 to 37.8, boosts $mAP_{0.5}$ from 34.7 to 35.4. Because it operates purely through camera-space transforms, 3DRot is readily transferable to other 3D tasks.", "tldr": "3DRot: a plug-and-play, RGB-based 3D rotation augmentation.", "keywords": ["3D object detection", "data augmentation", "monocular RGB", "geometric consistency", "homography", "projective geometry", "6D pose"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0aca2d6ff8f086c3c55a593bd363ca6a1a1a0e90.pdf", "supplementary_material": "/attachment/3a64cf1f6aae28c1718232c1d2c83d90ff0e8d3e.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes 3DRot, which introduces a 3D rotation augmentation for RGB-based 3D tasks by rotating/mirroring scenes around the camera's optical center while preserving projective geometry through synchronized updates of RGB images, camera intrinsics, and 3D annotations.  This method provides a data-efficient solution that significantly improved performance on monocular 3D detection, achieving gains comparable to multi-dataset training using only a single dataset."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "++ The augmentation does not require depth information.\n\n++ The 3DRot augmentation has been validated to be effective on the RGB-based 3D detection task."}, "weaknesses": {"value": "-- While the authors acknowledge that standard flipping (reflection) augmentation violates chirality, they claim to use a chirality-preserving method. However, the methodology is only briefly mentioned in the ablation study. A detailed explanation of how chirality is preserved should be provided in the Method section to ensure clarity and readability.\n\n-- The depths and NOCS maps in Figure 2 do not appear to be utilized in the experiments. It seems necessary to incorporate them into relevant tasks for coherence; otherwise, they may seem superfluous.\n\n-- The claim of cross-domain generalization is not sufficiently substantiated. The experimental validation is limited to a single task and two datasets, raising concerns that the reported gains may be specific to that particular experimental setup. To robustly support this claim, the authors should demonstrate the effectiveness of their method across a wider range of tasks and datasets. Furthermore, the potential need to re-tune parameters of the augmentation strategy for different domains remains an open question.\n\n-- Overly simple derivations and basic formulas can be omitted from the main text to free up space for experimental details.\n\n-- A limitation of the study is the absence of a comparison with other augmentation techniques like 3D copy-paste, both in isolation and combined. This omission makes it difficult to discern the specific advantages of the proposed method.\n\n\n-- A discussion of the limitations and potential failure cases of the proposed augmentation method should be included.\n\nMinor: \n\n-- The dimensionality is usually denoted using \\mathbb{R}, but this paper uses several different (and inconsistent) notations."}, "questions": {"value": "-- The method relies on camera intrinsic parameters. How would its performance be affected by approximated or inaccurate camera intrinsics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CCFpUSfFQE", "forum": "EbV2YIKU9o", "replyto": "EbV2YIKU9o", "signatures": ["ICLR.cc/2026/Conference/Submission14858/Reviewer_kBMV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14858/Reviewer_kBMV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761224996182, "cdate": 1761224996182, "tmdate": 1762925211830, "mdate": 1762925211830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a geometric image augmentation for RGB-based 3D vision tasks (e.g., 3D detection, 3D keypoint estimation). The augmentation rotates the camera around its optical center in a way that preserves projective geometry and therefore the 2D–3D correspondences between images and 3D labels. The authors give a mathematical derivation of the augmentation and present experiments showing improved performance on indoor 3D detection benchmarks when used with a Cube R-CNN detector and a DINO-X backbone."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tRigorous derivation: The mathematical formulation that guarantees preservation of 2D–3D correspondences is clean and convincing.\n-\tEmpirical ablation: The paper includes ablations about flipping/rotation configurations and shows that the proposed rotation improves accuracy compared to no-rotation.\n-\tConceptual simplicity: The idea is intuitive and could be widely useful if shown to be robust across models and datasets."}, "weaknesses": {"value": "Limited Evaluation:\n\n- The authors did not compare their method with alternative augmentation strategies. Only the relative gain against no augmentation is reported. Other augmentations could include non-geometric augmentation (jittering) or geometric ones as discussed in related work.\n- The biggest gain in performance in Table 3a) is achieved by leveraging DINO-X as the backbone of Cube R-CNN which is not the contribution of this paper.\n- The augmentation strategy is only evaluated on one model (DINO-X + Cube R-CNN).\n\nLimited Contribution:\n\n-\tBased on the conducted evaluation, the contribution of this work seems to be limited to the community."}, "questions": {"value": "-\tIs there a reason that 3DRot is only evaluated on indoor scenes? Does the technique also apply for outdoor scenes (e.g. KITTI dataset) for example?\n-\tWhat are the boundaries for the rotation angles? As their absolute value increases, more and more pixels are padded, hindering model’s training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ffXTKFFTZV", "forum": "EbV2YIKU9o", "replyto": "EbV2YIKU9o", "signatures": ["ICLR.cc/2026/Conference/Submission14858/Reviewer_JwxW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14858/Reviewer_JwxW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748232618, "cdate": 1761748232618, "tmdate": 1762925211425, "mdate": 1762925211425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a data augmentation method that work on the rotation of the viewing direction of the camera. The method rotates and mirrors images about the camera's optical center. The method does not need depth information. The author introduces the math foundations for the transformations of pixel locations based on which the remap is defined. The data augmentation method is evaluated by the performance gain of the 3D detection task."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of augmenting the data using camera view direction change is a nice and interesting idea.\n- The authors presented detailed math derivation to backup the method.\n- The method does not need depth information.\n- The performance gain is notable on 3D detection task on various datasets."}, "weaknesses": {"value": "- I would suggest the authors to take a closer look at related works. It would be quite surprising if the same or similar idea has never been used by others. It is likely that people already used this augmentation method for quite a long time yet did not publish it because it seems obvious and trivial. I would suggest the authors put more effort in investigating the prior works.\n- Though depth information is not required, this method still needs camera intrinsics. Such information may not be available for random image data. So this may limited the scope of this method being useful.\n- I am not sure if 3D detection is a good task for evaluating the data augmentation method, since the performance gain seems to be modest. Would other tasks be better where the performance gain is more significant?"}, "questions": {"value": "- Roll rotation seems to be OK which is what vanilla random rotation augmentation already did. But yaw and pitch may have some problems. The principal point is supposed to be at the center of any image. But after changing the view direction, the principal point will not be at the center. How did the authors deal with such situation? Maybe this has been mentioned in the paper but I may have missed it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a1eGx1M2HG", "forum": "EbV2YIKU9o", "replyto": "EbV2YIKU9o", "signatures": ["ICLR.cc/2026/Conference/Submission14858/Reviewer_c3Pg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14858/Reviewer_c3Pg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010454538, "cdate": 1762010454538, "tmdate": 1762925210630, "mdate": 1762925210630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a 3D rotation augmentation for monocular 3D object detection. The main proposed design is to apply 3D rotations and reflections on the camera's optical center, and update the camera intrinsics, 3D annotations, and RGB images accordingly. Experimental results are conducted on the 3D object detection task to validate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed 3D rotation augmentation is reasonable and supported by theoretical derivations and proof.\n- Experimental results on the 3D detection task show the effectiveness of the proposed augmentation.\n- The paper is well written and easy to follow."}, "weaknesses": {"value": "- The experimental evaluation of the proposed augmentation is not thorough:\n  - The paper claims the augmentation for RGB-based 3D tasks, but only the 3D object detection task is evaluated. Evaluation on more tasks, e.g., monocular depth estimation, would be beneficial. \n  - The 3D object detection is only conducted on 10 categories and only on indoor SUN RGB-D. Evaluation on more diverse categories and larger datasets that include both indoor and outdoor scenes will improve the validation of the generalizability of the proposed method. \n  - Only one algorithm framework is utilized for the benchmark. Can this augmentation benefit more 3D detection frameworks?\n- Considering that more and more large-scale and diverse datasets are proposed, e.g., ScanNet, Matterport3D, and ARKitScenes, it's not clear whether the proposed augmentation still provides a large performance gain in these larger-scale real-world datasets."}, "questions": {"value": "See the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SEamQ8NgrX", "forum": "EbV2YIKU9o", "replyto": "EbV2YIKU9o", "signatures": ["ICLR.cc/2026/Conference/Submission14858/Reviewer_8Fwf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14858/Reviewer_8Fwf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762279579043, "cdate": 1762279579043, "tmdate": 1762925210219, "mdate": 1762925210219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}