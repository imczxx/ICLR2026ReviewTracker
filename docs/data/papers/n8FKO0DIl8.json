{"id": "n8FKO0DIl8", "number": 9272, "cdate": 1758116884099, "mdate": 1759897734080, "content": {"title": "Statistical Guarantees in the Search for Less Discriminatory Algorithms", "abstract": "Recent scholarship has argued that firms building data-driven decision systems in high-stakes domains like employment, credit, and housing should search for “less discriminatory algorithms” (LDAs) (Black et al., 2023). That is, for a given decision problem, firms considering deploying a model should make a good-faith effort to find equally performant models with lower disparate impact across social groups. Evidence from the literature on model multiplicity shows that randomness in training pipelines can lead to multiple models with the same performance, but meaningful variations in disparate impact. This suggests that developers\ncan find LDAs simply by randomly retraining models. Firms cannot continue retraining forever, though, which raises the question: What constitutes a good-faith effort? In this paper, we formalize LDA search via model multiplicity as an optimal stopping problem, where a model developer with limited information wants to produce strong evidence that they have sufficiently explored the space of models. Our primary contribution is an adaptive stopping algorithm that yields a high-probability upper bound on the gains achievable from a continued search, allowing the developer to certify (e.g., to a court) that their search was sufficient. We provide a framework under which developers can impose stronger assumptions about the distribution of models, yielding correspondingly stronger bounds. We validate the method on real-world lending datasets.", "tldr": "We establish statistical guarantees for the search for less discriminatory algorithms, where model producers are required to retrain models until a fairer one is found.", "keywords": ["fairness", "anytime-valid inference", "sequential decision-making"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e5907e23118f08753c5cf5d750a9478c7ef3e480.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper seeks to give anytime-valid bounds on the expected marginal utility of randomly training an additional model in search of a Least Discriminatory Algorithm. The paper is organized in three levels of generality -- first, the authors tackle the full-information regime where the distribution and the population utilities are known. Then, the authors generalize their results, with high probability and under various assumptions, to unknown model sampling distributions. Finally, the authors generalize the results to finite data with an additional assumption that the 'selection effect' (the measurement error conditioned on the current empirical disparate impact) does not decrease with additional training runs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper has a clear and precise motivation and framing of results. \n\n2. The paper's organization as a gradual movement towards generality, with some additional thoroughly explained assumptions, made the paper very easy to read and understand.\n\n3. I do not have the expertise to know whether the proposed bounds on the probability of sampling a new minimum value of a random variable are significant, but to this reviewer they are interesting and potentially useful.\n\n4. I am convinced that the proposed bounding approach and algorithm can be useful to firms seeking to meet regulatory requirements (which may not yet exist) around finding LDAs, supposing that their training procedures are capable of finding satisfactory models."}, "weaknesses": {"value": "1. Relating to strength #4, I am unconvinced that knowledge that a firm satisfied a strong bound using the provided approaches would actually certify anything of value to a regulator or a court. It is entirely plausible that a firm could have an algorithm A that either is not sufficiently random, or is biased in some way (neither necessarily intentionally), so as to yield models which are very discriminatory, even when a non-discriminatory model may exist. Using the above approach could inadvertently allow a firm to certify that they had sufficiently searched for LDAs when in fact they hadn't.\n\n2. Although the additional assumptions in the theorems are well-described, I am not left with a sense of whether or not they are plausible in real data distributions. \n\n3. Relating to weakness #2, I think that additional experiments including the tightened bounds using assumptions A1-A3 would be useful to the paper. As it stands, it is difficult to determine how much those additional assumptions help beyond allowing smaller values of $\\bar{\\mu}$ in the algorithm.\n\n4. I think the experiments would benefit from many more iterations than the 60 plotted. Also, the starting values of both bounds at iteration 0 seem very small -- is there really such tiny expected marginal utility in training another model, even after only training a single model?\n\n5. This is neither a weakness nor a strength, so I am putting it last. The framing of the paper as a search for LDAs does not capture the full generality of the results. As the authors suggest, the results apply to any loss function and to any repeated (randomized) model fitting procedure. The search for LDAs is one potential application of these methods, but I feel that this could have been emphasized adjacent to other potential applications instead of as the main thrust of the paper."}, "questions": {"value": "1. How would a model developer determine which of the assumptions would be suitable for their data? How could they defend that choice to a judge or regulator?\n\n2. The bounds on the ground truth in the experiment are very wide -- is it plausible to run the same experiment on a synthetic distribution with a known ground truth?\n\n3. At the end of section 4, you state that, \"Empirically, Algorithm 1 performs well in the sense that it \"overshoots\"... by tens of models...\" How can I see this relationship from the plots? Is this by drawing a horizontal line from each point on the brown curve to the right until it touches the pink curve? \n\n4. Techniques exist in the literature to find a diverse set of models of a particular model class without randomized re-training. Some examples include the Rashomon set of decision trees, a sampling of the Rashomon set of XGBoost models, and a sampling of the Rashomon set using dropout in neural networks. What makes your approach desirable over these methods, which are more likely to directly find an LDA instead of randomly training and hoping for one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FLxcUsUwWo", "forum": "n8FKO0DIl8", "replyto": "n8FKO0DIl8", "signatures": ["ICLR.cc/2026/Conference/Submission9272/Reviewer_TPVF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9272/Reviewer_TPVF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761584381079, "cdate": 1761584381079, "tmdate": 1762920919647, "mdate": 1762920919647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper formalizes the practice of retraining to find less discriminatory algorithms (LDAs), models with comparable utility but reduced disparate impact, by casting it as an optimal-stopping problem. Given model multiplicity, the authors define a decision rule: stop retraining once the expected marginal improvement in disparity from training one more model falls below a cost/benefit threshold. \nThey provide an adaptive stopping algorithm with anytime-valid, high-probability upper bounds on the marginal improvement, so that when the algorithm halts, one can certify that continued search is unlikely to pay off. Empirical studies are conducted to validate the method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Turning “good-faith LDA search” into an auditable optimal-stopping problem with an explicit threshold $\\gamma$ is a nice formulation. The adaptive rule provides high-probability upper bounds on the marginal gain from one more retrain, enabling a certificate that a search was “sufficient” at the data-dependent stopping time. The paper motivates the problem clearly, is well-written, and easy to follow."}, "weaknesses": {"value": "1. **Bounding $\\mu$ in Section 3.2.**  \n   The paper presents several upper bounds on \\( \\mu(u) \\) under different assumptions on the underlying density. It is not immediately clear how conservative these bounds are in practice. Could the authors comment on the **tightness** of these bounds (e.g., instances where they are known to be sharp vs. loose), and perhaps provide empirical or theoretical comparisons across the proposed choices?\n\n2. **Online learning formulation (infinite-data regime).**  \n   Consider the infinite-data setting where we observe i.i.d. $Q_t$ exactly but $P$ is unknown. Can we estimate $u_p^\\star$ in a data-driven way at each round? For example, at round $t$ define $$ \\hat g(u) = \\frac{1}{t}\\sum_{j=1}^t (u - Q_j){1}\\{u>Q_j\\},$$\n   and set $\\hat u_p = \\sup_{u\\in[0,1]} \\hat g(u) \\le \\gamma$. One could update $hat g$ each round (or every few rounds) and stop when $U_\\tau \\le \\hat u_p$. Would this constitute a **valid** approach within your framework, and under what conditions (if any) would it inherit your guarantees?\n\n3. **Assumption 3.4.**  \n   The intuition for Assumption 3.4 is not fully clear, particularly why it should hold for **any** $P$ and **any** $\\hat P$? What is meant precisely by “regression to the mean is at least constant”? A more natural route might be to index the assumption by **sample size**, yielding an explicit bound between $P$ and $\\hat P$ that vanishes as $n \\to \\infty$. Could the authors either (i) give **verifiable sufficient conditions** ensuring Assumption 3.4 under common validation-reuse protocols, or (ii) reformulate it in a sample-size–dependent way that makes its asymptotic validity transparent?"}, "questions": {"value": "Please see the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DOc80wbH5L", "forum": "n8FKO0DIl8", "replyto": "n8FKO0DIl8", "signatures": ["ICLR.cc/2026/Conference/Submission9272/Reviewer_gjyJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9272/Reviewer_gjyJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881612053, "cdate": 1761881612053, "tmdate": 1762920918916, "mdate": 1762920918916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper formalizes the search for less discriminatory algorithms (LDAs) as an optimal stopping problem grounded in the context of model multiplicity and fairness-aware model selection. It proposes an adaptive stopping algorithm (Algorithm 1) that provides a high-probability upper bound on the marginal benefit of retraining models, effectively giving a statistical “certificate” for when a firm can reasonably stop searching for fairer models. The theoretical framework integrates ideas from anytime-valid inference and optimal stopping theory, establishing guarantees that the expected marginal gain from retraining is below a user-specified threshold. Empirical evaluations on three fairness-sensitive datasets (Adult, Folktables, HMDA) and multiple model classes (logistic regression, random forests, neural networks) demonstrate the method’s ability to approximate the optimal stopping point with reasonable accuracy. The discussion highlights implications for algorithmic fairness compliance, proposing the approach as a tool to certify “good-faith” searches for LDAs in high-stakes domains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- Theoretical novelty: The framing of LDA search as an optimal stopping problem is original and mathematically sound. The derivation of anytime-valid upper bounds for marginal gains extends prior work in statistical inference and stopping theory.\n\n- Practical relevance: The work connects theoretical constructs to regulatory and compliance debates in algorithmic fairness, addressing a pressing question of how firms can demonstrate sufficient fairness efforts.\n\n- Methodological rigor: The paper clearly delineates three regimes (full-information, infinite-data, finite-data) and progressively builds the theoretical results with appropriate assumptions and proofs.\n\n- Statistical guarantees: The introduction of an adaptive, distribution-agnostic stopping rule that provides high-probability bounds strengthens the method’s interpretability and generality.\n\n- Empirical validation: The experimental section, while modest, is well aligned with the theoretical claims. The algorithm’s overshoot relative to the full-information optimum provides empirical evidence for its reliability."}, "weaknesses": {"value": "- Limited empirical scope: The empirical evaluation, though methodologically correct, uses small-scale settings with standard datasets. There is limited evidence of robustness in larger or more complex model retraining pipelines.\n\n- Assumption strength: Several theoretical results depend on distributional assumptions that may not hold in realistic ML training scenarios with non-iid retraining or adaptive hyperparameter tuning.\n\n- Connection to fairness metrics: While the framework generalizes beyond disparate impact, the empirical focus remains narrow. It does not analyze whether the stopping rule’s behavior changes under alternative fairness definitions (e.g., equal opportunity, demographic parity).\n\n- Practical deployment considerations: The paper lacks discussion of computational cost, reproducibility challenges, and real-world compliance integration.\n\n- Clarity of exposition: The writing is dense in the theoretical sections (e.g., Section 3) and could better motivate the intuition behind the derived bounds for a mixed audience of ML and applied fairness researchers."}, "questions": {"value": "1. How sensitive is the stopping time to violations of Assumption 3.4 (non-decreasing selection effect)? Would the algorithm’s guarantees degrade gracefully under mild violations?\n\n2. Could the method be extended to handle adaptive retraining (where the next model’s training depends on previous outcomes), which is common in fairness optimization pipelines?\n\n3. How should practitioners select or validate the threshold γ in regulatory or organizational contexts?\n\n4. Would the method still provide valid guarantees if retraining involved data reweighting or feature modification, rather than randomness in initialization or batch ordering?\n\n5. How does the choice of fairness metric influence the stopping behavior and the empirical coverage rates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bmL60DIWd6", "forum": "n8FKO0DIl8", "replyto": "n8FKO0DIl8", "signatures": ["ICLR.cc/2026/Conference/Submission9272/Reviewer_evA5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9272/Reviewer_evA5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888283310, "cdate": 1761888283310, "tmdate": 1762920918525, "mdate": 1762920918525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a statistical criterion to determine when to stop training (sampling) models on a finite dataset while trying to minimize a quantity of interest. The main statistical guarantee is that the algorithm will stop at a point where training an additional model will unlikely lead to an improvement. The paper focuses mainly on fairness metrics, but the approach is in theory applicable to any quantity of interest that is averaged over the test set e.g. model performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is very easy to follow, despite containing a lot of theory. To accomplish this, the manuscript starts from an ideal scenario :known population risk $Q$ and known marginal distribution of said risk $P_0$. Then each assumption is subsequently relaxed. First, acknowledging that $P_0$ is unknown means that we must find an upper bound on expected improvement that holds with high probability. Further relaxing our knowledge of $Q$ to that of its empirical counterpart $\\widehat{Q}$ is then explained to derive the final form of the algorithm. Structuring the paper this way is very pedagogical and helps the reader understand each step of the derivation.\n\nThe algorithm is well motivated based on Theorem 3.5. \n\nThe correctness of the algorithm is demonstrated empirically on three datasets and model types."}, "weaknesses": {"value": "## Extending Experiments\n\nWhile the presented experiments highlight that the algorithm is correct (the upper bounds holds with probability at least 95% in general), they could be extended to highlight interesting trade-offs and other applications beyond fairness.\n\nFor instance, the appendix presents Algorithm 2 as an alternative that uses of subset of the trained models to estimate the upper bound in conditional expected improvement $\\overline{\\mu}$. However, the tightness of this algorithm is not compared empirically with Algorithm 1. There might be interesting trade-offs between algorithms 1 and 2 in terms of number of model trainings required for the bounds to go below $\\gamma$. Algorithm 2 has a tighter $\\overline{\\mu}$ but it requires separate samples to first compute the quantile $C$ and the bound $\\overline{p}_t(\\delta/3)$ is looser because of the union bound. Consequently, it would be pertinent to add Algorithm 2 to Figure 1.\n\nAnother way the experiments could be extended is to apply Algorithm 1 to another use-case. For example, I think it is perfectly applicable to hyperparameter optimization via random search. Applying Algorithm 1 to find the hyperparameters of the Random Forests and MLP used in the experiments would highlight the versatility of the method. To avoid diluting the main message of the paper (which is about less discriminative alternatives), these additional experiments could be placed in a dedicated appendix.\n\n\n## More details on Assumption 3.4\n\nAssumption 3.4 is the key to replace population risk $Q$ with the empirical risk $\\widehat{Q}$ in the algorithm. While this assumption is motivated by citing the existing literature, it would be better to assess whether it holds in the experiments. This should be doable since the experiments are designed so that population distributions are known. \n\nFigure 4 shows that mis-coverage is higher for more expressive models (RFs and MLPs). It would be interesting to see if assumption 3.4 is indeed less likely to hold for these models."}, "questions": {"value": "In Equation 1, shouldn't the expectation be $\\mathbb{E}\\_{\\mathbb{P}\\_0\\times \\mathbb{P}\\_0}[U\\_{\\tau} - U\\_{\\tau+1} | \\widehat{U}\\_{\\tau}]$? This is because $\\mathbb{P}_0$ is the marginal distribution of a single pair $(\\widehat{U}, U)$, while in Equation 1 involves two pairs $(\\widehat{U}\\_{\\tau}, U\\_{\\tau})$ and $(\\widehat{U}\\_{\\tau + 1}, U\\_{\\tau+1})$? I assume that $U\\_{\\tau}$ remains a random variable when we condition on $\\widehat{U}\\_{\\tau}$?\n\nThe theorems bound the expected improvement resulting from training **one** additional model. But let's assume that I can train $B$ models in parallel with no additional costs. How easy would it be to extend the theorems to bound the expected improvement from training $B$ additional models? Would it be as simple as applying a union bound? Maybe it is possible to do better than that, since the union bound will become artificially loose at $B$ increases?\n\nThe datasets used in the experiments are quite large. Would the approach provide a good coverage on smaller datasets e.g. COMPAS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9vb7uZfPKs", "forum": "n8FKO0DIl8", "replyto": "n8FKO0DIl8", "signatures": ["ICLR.cc/2026/Conference/Submission9272/Reviewer_bHC9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9272/Reviewer_bHC9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946432620, "cdate": 1761946432620, "tmdate": 1762920918151, "mdate": 1762920918151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}