{"id": "NkusGmHBRS", "number": 9261, "cdate": 1758116608958, "mdate": 1763650049729, "content": {"title": "Optimal Trade-offs between Regret and Estimation in Capacitated Multinomial Logit Bandits", "abstract": "Online decision-making involves a fundamental trade-off between two objectives. The first is *regret minimization*, which aims to maximize cumulative reward; the second is *parameter estimation*, which aims to learn the underlying model for downstream tasks. While this trade-off is well-studied in multi-armed bandits (MAB), it remains far less understood in multinomial logit (MNL) bandits, where the decision space is combinatorially large.  The only prior work Zuo & Qin (2025) is limited to the uncapacitated case and lacks a tight characterization of the dependence on the number of items $N$. In this work, we establish tight trade-off bounds between regret and customer attraction estimation error for capacitated MNL bandits, with a sharp dependence on $N$. To match these bounds, we introduce an algorithm that achieves the optimal trade-off, providing the first complete characterization of *Pareto optimality* in this setting. The lower-bound technique underlying our results is broadly applicable and also strengthens existing results for MAB. Beyond attraction estimation, our analysis further extends to customer preference estimation error, where the same guarantees continue to hold. As a further application, our framework addresses the joint assortment and pricing problem, yielding new insights into the regret-estimation trade-off in broader contexts.", "tldr": "", "keywords": ["Multinomial Logit Bandits", "Pareto Optimality", "Assortment Optimization"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bda36f2e64897774af0dea8a25ec7cfdb8c550a1.pdf", "supplementary_material": "/attachment/828371d523fb48e9023f1d151249475c17ad00ea.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the trade-off between regret minimization and parameter estimation in Multinomial Logit (MNL) bandits.\nBuilding on the work of Zuo & Qin (2025), the authors extend the analysis to the capacitated setting, where the assortment size is limited to \n$K$. They establish tight lower and upper bounds on the trade-off between regret and customer attraction estimation error, and further propose an algorithm that achieves the optimal trade-off rate.\nAdditionally, the framework is extended to the joint assortment and pricing problem, demonstrating its broader applicability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides the first tight characterization of the regret–estimation trade-off in capacitated MNL bandits, addressing an open problem left by Zuo & Qin (2025).\nHowever, the comparison with prior work may be **misleading**, since the definition of estimation error used here differs from that in Zuo & Qin (2025).\nWhile I agree with the authors that their definition—based on direct estimation of attraction parameters (as noted in Footnote 1)—is more appropriate than using relative pairwise differences, the claimed improvement over Zuo & Qin (2025) overlooks the fact that the two metrics correspond to different performance objectives, making the comparison not entirely fair.\nIn my view, the main contribution of this paper lies in redefining the estimation error in a more appropriate way and establishing the corresponding optimal trade-offs under this new formulation, rather than in claiming an improvement over the previous work."}, "weaknesses": {"value": "**1. Technical concern in the proof of Theorem 3.2:**\nIn the instance construction, the revenue is set to 1 for one item and 0 for all others. Since the revenues are known to the algorithm, it can simply ignore the zero-revenue items and always select the item with revenue 1. Such a simple policy would then incur zero regret, which would make the lower bound on the trade-off measure equal to zero.\nCould the authors clarify how this issue is resolved in your setup?\nIf I have misunderstood the setting, please correct me. I would be happy to raise my score once this point is addressed.\n\n**2. Performance comparison with Zuo & Qin (2025):**\nAs mentioned above, the definition of estimation error used in this paper differs from that in Zuo & Qin (2025). Therefore, I believe that the authors’ claim of improving upon the previous results is somewhat overstated and potentially misleading, since the two formulations measure different performance objectives and are not directly comparable.\n\n\n**3. Experiments:**\n1) In Figure 1, only the regret and estimation errors are reported. However, it would be more informative to also include the trade-off metric.\nIncluding this metric would make the results more convincing, especially if it demonstrates that the trade-off measure remains approximately independent of $\\alpha$, consistent with the theoretical claims.\n\n2) The confidence intervals are not shown in Figure 1. \n\n3) No baseline algorithms are included for comparison. At a minimum, the authors could compare the regret performance of their proposed method with that of the algorithm from Zuo & Qin (2025)."}, "questions": {"value": "1. In Figure 1, the estimation error appears unchanged. However, unless I am missing something, in Algorithm 3 the parameter estimates are still being updated during the regret minimization phase.\nCould the authors clarify why the estimation error remains constant even though the parameters are being updated?\n\n2. What happens if $\\alpha > 1/2$? Does the analysis break down in this regime? If so, please clarify which steps in the proofs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3qijRhBchF", "forum": "NkusGmHBRS", "replyto": "NkusGmHBRS", "signatures": ["ICLR.cc/2026/Conference/Submission9261/Reviewer_Ga2Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9261/Reviewer_Ga2Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761116751919, "cdate": 1761116751919, "tmdate": 1762920912871, "mdate": 1762920912871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the fundamental trade-off between parameter estimation and regret minimization in the context of capacitated Multinomial Logit bandits. The paper's primary contribution is a tight characterization of this trade-off. The authors first establish a minimax lower bound, proving that for any algorithm, the product of the estimation error and the square root of the regret must be at least $\\Omega(\\sqrt{N})$. They then propose an algorithm that can be tuned by a parameter $\\alpha \\in [0, 1/2]$. This algorithm is proven to achieve a matching upper bound of $\\widetilde{\\mathcal{O}}(\\sqrt{N})$ for the same trade-off metric, making it Pareto-optimal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper resolves the dependency on $N$, which was left open by prior work (Zuo & Qin, 2025). The $\\Omega(\\sqrt{N})$ lower bound and matching algorithm significantly improve upon the previous $\\Omega(1)$ and $\\widetilde{\\mathcal{O}}(N^{3/4})$ bounds for the uncapacitated case.\n2. The paper is exceptionally well-written. The use of a table (Table 1) to compare related work is very helpful."}, "weaknesses": {"value": "1. The model assumes product revenues $r_i$ are known and fixed. In many real-world scenarios, the revenue or margin might be part of the optimization problem or uncertain. A discussion of how this assumption impacts the results or how it could be relaxed in future work would be valuable.\n2. The notation in Step 14 of Algorithm is confusion.\n3. The argument for Theorem 3.3 currently relies on asymptotic arguments (i.e., big-O notation). A more rigorous proof should make the argument non-asymptotic by providing explicit constants to substantiate the claimed sufficient condition for Pareto optimality.\n4. Definition 2.1 lacks sufficient precision. It is unusual to define a comparison between two quantities based on an imprecise metric. For example, consider $A=(1,2)$ and $B=(2,1)$. Depending on Definition 2.1, one could claim that either $A$ Pareto dominates $B$ or $B$ Pareto dominates $A$, which illustrates the ambiguity in the current definition."}, "questions": {"value": "see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9Q0xhklEsu", "forum": "NkusGmHBRS", "replyto": "NkusGmHBRS", "signatures": ["ICLR.cc/2026/Conference/Submission9261/Reviewer_HS3S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9261/Reviewer_HS3S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749517698, "cdate": 1761749517698, "tmdate": 1762920912426, "mdate": 1762920912426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates online MNL bandits with two objectives: minimizing cumulative regret and estimating the attraction vector $v$. The first contribution is an information theoretic lower bound: to attain estimation error $e_T(v,\\hat v)\\le \\varepsilon$, one needs on the order of $N/\\varepsilon^{2}$ samples. The authors then prove a fundamental trade-off \n$\ne_T(v,\\hat v)\\sqrt{\\mathrm{Reg}_T}\\ge \\tilde{\\Omega}(\\sqrt{N}),\n$\nwhich formalizes that both goals cannot be achieved simultaneously beyond this rate. A pure-exploration procedure meets this optimal product but suffers linear regret in $T$. To move along the Pareto frontier, the paper proposes an explore-then-exploit algorithm that delivers sublinear regret while controlling $e_T$, though with a worse dependence on the value scale $V$. Experiments support the theory and illustrate the regret–estimation trade-off."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a clear and formal treatment of the trade-off between regret minimization and value estimation in online MNL bandits.  \n - The lower bound $e_T(v,\\hat v)\\sqrt{\\mathrm{Reg}_T}\\geq\\tilde{\\Omega}(\\sqrt{N})$ is interesting in MNL that establishes the Pareto frontier between learning and exploration.  \n- The theoretical results are supported by experiments that effectively demonstrate the empirical trade-off.  \n - I read the proofs in general and it looks correct to me based on the parts I have checked."}, "weaknesses": {"value": "- The motivation for focusing on *value* estimation (estimating $v$) is not entirely clear. In most practical MNL applications, the primary objective is to maximize expected revenue rather than the latent value vector $v$ itself. Since the ultimate performance metric in these problems is revenue-based, it is unclear why directly estimating $v$ becomes a natural or meaningful intermediate goal.\n- The algorithmic contributions are relatively straightforward. The method achieving the optimal trade-off is essentially a *pure-exploration* algorithm, while the one balancing estimation error and regret is a classic *explore-then-learning* procedure that simply switches to a known MNL-UCB algorithm from prior work. There is limited algorithmic innovation or adaptivity: the trade-off parameter $\\alpha$ is also preset rather than learned. Consequently, while the theoretical analysis is neat, the algorithmic side feels largely incremental.\n- The dependence on the value scale $V$ appears suboptimal and is not theoretically justified. For example, the pure-exploration algorithm exhibits a $V^{3/2}$ dependence, while the explore-then-learning algorithm incurs a $V^2$ factor. Since the lower bounds presented in the paper do not contain any explicit $V$ term, it remains unclear whether these dependencies are information-theoretically necessary or simply artifacts of the analysis. This weakens the optimality claim of the trade-off frontier, especially in practical settings where $V$ can be very large (like contextual case where $v_i=\\exp(\\langle x,\\theta_i\\rangle)$)."}, "questions": {"value": "- 1. Can the authors clarify the motivation behind focusing on *value* estimation rather than directly estimating choice probabilities or maximizing expected revenue? In what types of downstream applications would accurate recovery of $v$ itself be the key goal?\n- 2. The current analysis exhibits an explicit dependence on the value scale $V$ in the upper bound, yet the lower bound does not include $V$. Can the authors provide intuition or formal arguments suggesting whether this dependence is fundamental? In practical or contextual MNL settings where $v_i = \\exp(\\langle x, \\theta_i \\rangle)$ and $V$ can be exponentially large, so characterizing the dependency on $V$ can be important in practice.\n- 3. The algorithms considered are largely non-adaptive. Do the authors expect that an online, single-phase approach (e.g., optimism-based or posterior-sampling) could achieve a similar Pareto frontier without a hard phase separation between exploration and exploitation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QrtkTbKZ5q", "forum": "NkusGmHBRS", "replyto": "NkusGmHBRS", "signatures": ["ICLR.cc/2026/Conference/Submission9261/Reviewer_4TxG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9261/Reviewer_4TxG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952080177, "cdate": 1761952080177, "tmdate": 1762920911962, "mdate": 1762920911962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the Pareto trade-off between regret minimization and parameter estimation in capacitated MNL bandits. To this end, the paper shows a lower bound on $e_T(\\hat{v},v).R(T)$ (Theorem 3.2). They also point out a mistake in older, similar proofs and correct it in their presentation. From an algorithm design perspective, they present a two-phase algorithm (Algorithm 2) that achieves this lower bound, utilizing a tunable parameter. Overall, the presentation is neat and contributions are novel, and in some way, they close a \"gap\" in the joint-regret and parameter estimation line of works for the MNL bandit."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1)Their theory results are state of the art, e.g., dependence w.r.t $N$. They also correct a \"mistake\" in the previous literature\n2) The main text is largely well-written and explained well.\n3)Their lower bound hardness construction, Theorem 3.1 proof, could be of independent interest.\n4)While other aspects of the paper: two-phase algorithm, alpha parameter tuning appear standard and reasonable"}, "weaknesses": {"value": "1) Line 7 in Algorithm 3 is computationally heavy. The authors should discuss this or point out if there is an efficient way to implement Line 7\n2)Assumption 4.2 (large horizon) is very strong. This may not be valid at all for a variety of problems.\n\nI do not have much complaints with the paper."}, "questions": {"value": "1) In Table 1, the revenue-based lower bound is $\\Omega(1)$. can this be improved?\n2)have the authors thought about anytime algorithm instead of a two-phase algorithm? Can a doubling-trick be used here?\n\nPlease also look into the other weaknesses as above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YT6WD2DyDn", "forum": "NkusGmHBRS", "replyto": "NkusGmHBRS", "signatures": ["ICLR.cc/2026/Conference/Submission9261/Reviewer_3nSy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9261/Reviewer_3nSy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963840629, "cdate": 1761963840629, "tmdate": 1762920911655, "mdate": 1762920911655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}