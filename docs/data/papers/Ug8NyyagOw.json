{"id": "Ug8NyyagOw", "number": 6227, "cdate": 1757959604043, "mdate": 1763700101208, "content": {"title": "JDM: Joint Distribution Modeling for Fine-Grained Text-to-Video Generation", "abstract": "Text-to-video (T2V) generation enables AI systems to create videos from textual descriptions, with applications in entertainment, education, and content creation. Recent advances in video diffusion models have improved visual quality, yet they struggle with fine-grained text-video alignment, often leading to attribute mismatches, incorrect object interactions, and compositional failures. In this paper, we identify that this limitation stem from a predominant focus on video reconstruction rather than explicitly learning structured text-video correspondences. To address this, we propose Joint Distribution Modeling (JDM), a novel framework that enhances fine-grained alignment by modeling the joint distribution of video content and object masks. Unlike prior methods that rely on external constraints, JDM inherently learns structured mappings between textual descriptions and video regions, improving compositional consistency. We theoretically demonstrate that JDM improves text-video alignment by directly optimizing for fine-grained correspondences rather than relying on implicit learning from data. Experimental results show that JDM significantly enhances alignment while maintaining high video quality. Furthermore, JDM unifies video generation and segmentation within a single framework, paving the way for more structured and controllable text-to-video synthesis.", "tldr": "", "keywords": ["Diffusion Model", "Video Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8aee7a96058c479c94885ca23844857ed8b0b2bc.pdf", "supplementary_material": "/attachment/2b81c4c24936e84049e467bb2b03713aae90e8dc.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Joint Distribution Modeling (JDM), a novel framework designed to improve fine-grained text-to-video (T2V) alignment and compositional consistency. The authors identify that conventional T2V diffusion models struggle with attribute mismatches and semantic leakage because they focus on video reconstruction and only enforce a global text-video correspondence, leaving fine-grained mappings to emerge implicitly from the data. JDM addresses this by directly modeling the joint distribution of the video content and the object masks conditioned on the text. By training the model to predict both the video and its constituent object masks simultaneously, JDM inherently learns structured mappings between textual descriptions and specific video regions. The framework unifies T2V generation and segmentation, which the authors theoretically demonstrate improves fine-grained text-video alignment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The central contribution is the shift in modeling philosophy, from Layout-to-Video to the joint distribution.\n2. The work directly targets a fundamental failure mode in diffusion models: the lack of precise attribute binding and semantic leakage.\n3. JDM successfully unifies two distinct computer vision tasksâ€”video generation and video segmentation.\n4. The paper includes a theoretical demonstration that optimizing the joint distribution directly enhances fine-grained text-video alignment."}, "weaknesses": {"value": "1. The reported training setup is prohibitively resource-intensive for most academic labs. The model was trained on a cluster of 80 Nvidia RTX 4090 GPUs for 30,000 steps. This extreme resource requirement effectively renders the work non-reproducible for the general research community.\n2. There is no analysis of the model's sensitivity to imperfect object masks. If the training masks contain noise or inconsistencies (a common issue in large-scale automated segmentation), the model may learn spurious correlations between noisy text-mask pairs.\n3. While JDM is trained to generate masks, the quality of these generated masks during inference (when they are unknown) is critical for validating the unified framework claim. The paper needs a dedicated analysis showing the quality of the segmented output generated by JDM.\n4. The demonstrated failure modes (semantic leakage) primarily relate to spatial composition and attribute binding. The paper should include a more rigorous evaluation of fine-grained temporal consistency and motion alignment, which are equally important for T2V generation."}, "questions": {"value": "There are some areas in this paper that need improvement, which are provided in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xdZ3Y3Uhgu", "forum": "Ug8NyyagOw", "replyto": "Ug8NyyagOw", "signatures": ["ICLR.cc/2026/Conference/Submission6227/Reviewer_no2b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6227/Reviewer_no2b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761377070491, "cdate": 1761377070491, "tmdate": 1762918557990, "mdate": 1762918557990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of fine-grained text-video alignment in text-to-video (T2V) generation, where existing models often fail to correctly bind attributes and actions to specific objects. The authors propose a novel framework called Joint Distribution Modeling (JDM), which learns structured text-video correspondences by modeling the joint distribution of video content and object masks. Unlike methods that require external constraints during inference, JDM inherently learns to map textual descriptions to specific video regions, improving compositional consistency. The approach simultaneously generates both the video and its corresponding mask from text, enhancing alignment without sacrificing visual quality and unifying generation and segmentation within a single model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors propose an effective pipeline to jointly model the visual content and the fine-grained regional information. It only requires a textual description during inference and thus is more applicable compared with existing layout-to-video methods.\n\n2. The joint distribution modeling is theoretically sound, and I believe the analysis of existing T2V diffusion models on tackling compositional text is reasonable, which further provides strong theoretical support for the motivation of the proposed method.  \n\n3. The experimental results are good. The authors conduct experiments on two pretrained T2V models with different backbone types to demonstrate the effectiveness. Experiments are also conducted on several benchmarks together with the human user studies to make the verification more comprehensive."}, "weaknesses": {"value": "1. Experiments are insufficient. First, though the proposed method outperforms some T2V methods, the fine-grained generation capability in comparison with existing layout-to-video methods is unclear. I believe some discussions are needed since the authors claim their benefits are in contrast with layout-to-video methods. Besides, the authors select two early T2V models (ModelScopeT2V from 2023 and  CogVideoX-2B from 2022), whose original performance is relatively low. How will the proposed method perform when switching to some more frontier T2V methods?\n\n2. The conditional independence assumption proposed in Equation (7) (i.e., p(y^i, y^j | xt) = p(y^i | xt) p(y^j | xt)) is a very strong assumption. Although the authors have proposed a dynamic weighting scheme to mitigate situations where it is violated, are there any qualitative examples or analysis to demonstrate the prevalence of such violations in the training data? For instance, for a prompt like \"a person is riding a horse,\" where \"person\" and \"horse\" are highly correlated, how does your method handle this?"}, "questions": {"value": "1. There is an unresolved tension between the low quality of the generated masks for the ModelScopeT2V+JDM model (e.g., IoU of 0.326) and the reported improvements in text-video alignment. How does training stably and effectively proceed with such imperfect and potentially noisy mask supervision? What impact does this noise have on the learning process?\n\n2. The description of the dynamic loss weighting scheme based on Conditional Mutual Information (CMI) is unclear. What is the specific scope for finding the \"nearest neighbors\" in the approximation method for CMI in Equation (12)?\n\n3. Have the authors specifically evaluated the temporal consistency of the object masks in the generated videos (for example, whether the mask for the same object is stable across different frames)? Does the JDM framework implicitly improve this aspect, or will explicit temporal constraints be necessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "36xoIBH5hZ", "forum": "Ug8NyyagOw", "replyto": "Ug8NyyagOw", "signatures": ["ICLR.cc/2026/Conference/Submission6227/Reviewer_MU8G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6227/Reviewer_MU8G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802490644, "cdate": 1761802490644, "tmdate": 1762918557675, "mdate": 1762918557675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Joint Distribution Modeling (JDM), a framework to enhance fine-grained text-to-video (T2V) generation. The authors identify that conventional T2V models, focusing on video reconstruction, fail to learn fine-grained correspondences. JDM addresses this by jointly modeling the distribution of video content and its corresponding object masks, conditioned on regional text descriptions. This approach aims to enforce explicit, fine-grained text-video alignment during training. The method is applied to two existing T2V models and demonstrates significant improvements in compositional generation and semantic alignment on benchmarks like VBench and T2VCompBench."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Effective Empirical Results: The method achieves substantial improvements in fine-grained text-video alignment across multiple metrics and base models, effectively mitigating issues like attribute leakage.\n\n2. Addresses a Key Problem: The paper tackles the critical and challenging problem of compositional generation in T2V models, which is of significant interest to the community.\n\n3. Unifies Generation and Segmentation: The ability to generate both video and corresponding object masks simultaneously is a valuable contribution with practical applications."}, "weaknesses": {"value": "1. Limited Novelty and Overstated Contribution: The conceptual framework is highly similar to VideoJAM, which jointly models video and motion (optical flow). This paper adapts it to video and semantics (masks), but this connection is not discussed. The contribution seems to be primarily driven by the meticulous data pipeline for extracting regional masks and captions, rather than a novel modeling paradigm. The term \"Joint Distribution Modeling\" overstates the technical contribution, as the implementation does not explicitly model a distribution but rather learns from concatenated features via attention.\n\n2. Missing Ablation Study: The Dynamic Loss Weighting is presented as a key component for handling dependencies between concepts. However, its actual impact is not verified through an ablation study, leaving its importance unclear."}, "questions": {"value": "1. Could the authors clarify the novelty of JDM compared to prior works like VideoJAM? The core contribution appears to be the fine-grained data pipeline, not the joint training framework itself. Is \"joint distribution modeling\" an accurate description, or is the mechanism more simply attention over concatenated video and mask features?\n\n2. Please provide an ablation study on the Dynamic Loss Weighting mechanism to demonstrate its contribution to the final performance.\n\n3. What is the rationale behind sampling a subset of objects during training (as depicted in Figure 2), instead of using all detected objects as conditions? How does this strategy impact model performance and generalization?\n\n4. In the training pipeline, how are the individual regional captions used to enforce fine-grained learning, given they are concatenated into a single prompt that is fed to the model? The diagram does not make this crucial link clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "swZMjAWewh", "forum": "Ug8NyyagOw", "replyto": "Ug8NyyagOw", "signatures": ["ICLR.cc/2026/Conference/Submission6227/Reviewer_7Hof"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6227/Reviewer_7Hof"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838675918, "cdate": 1761838675918, "tmdate": 1762918557254, "mdate": 1762918557254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of poor fine-grained text alignment in T2V models. The authors propose a framework that jointly trains a diffusion model to generate both the video and corresponding object masks. This is intended to enforce a more detailed correspondence between text and video regions. A key part of their method is a dynamic loss weighting strategy, which uses CMI to adaptively adjust the loss when regional text descriptions are highly correlated."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a detailed derivation for jointly modeling the mask and video in diffusion models. Through this derivation, it suggests that the loss should be reweighted by the strength of correlation of the local captions, which is an interesting idea.\n\n- The paper conducts experiments on commonly used VBench and T2VCompBench and demonstrates its effectiveness on several key metrics."}, "weaknesses": {"value": "- The paper's core idea overlaps significantly with existing work [1, 2], which has already shown that jointly training a video generation model to produce supplementary outputs (like optical flow, depth, or segmentation masks) can improve generation quality. The paper lacks a comparison or discussion with these closely related works.\n\n- The paper lacks ablation studies to demonstrate the effectiveness of the proposed regional mask and the Dynamic Loss Weighting strategy, which are the most significant differences from previous work in implementation. The paper neither includes a comparison of JDM-Mask with CMI reweighting versus JDM-Mask without it, nor does it provide a comparison between using regional masks corresponding to the input text and using simple per-object segmentation. Without these key experiments, it is difficult to justify how the experimental results support the paper's theoretical derivation. On the contrary, the paper provides a comparison in Appendix A.3 of different joint training signals, which shows that joint training with HED or Depth achieves performance nearly on par with the proposed JDM-Mask. This suggests that the primary benefit may come from any auxiliary joint training signal, rather than the regional masks and CMI-based dynamic weighting, which are presented as key contributions.\n\n[1] VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models\n\n[2] Unified Dense Prediction of Video Diffusion"}, "questions": {"value": "- The results in Table 1 and Figure 4 show that JDM, while improving most fine-grained metrics, appears to cause a performance regression in some other categories (e.g., a noticeable drop in \"Dynamic Degree\" for ModelScope). Could the authors elaborate on the reasoning here?\n\n- Are the HED and Depth experiments in Appendix A.3 also using regional HED/Depth signals that correspond to the local prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W6Wl0CohMz", "forum": "Ug8NyyagOw", "replyto": "Ug8NyyagOw", "signatures": ["ICLR.cc/2026/Conference/Submission6227/Reviewer_8VA3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6227/Reviewer_8VA3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867545844, "cdate": 1761867545844, "tmdate": 1762918556763, "mdate": 1762918556763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}