{"id": "Xq64xkQCak", "number": 5712, "cdate": 1757928420989, "mdate": 1763669923733, "content": {"title": "Temporal Geometry of Deep Networks: Hyperbolic Representations of Training Dynamics for Intrinsic Explainability", "abstract": "This paper investigates how multilayer perceptrons (MLPs) can be represented in non-Euclidean spaces, with emphasis on the Poincaré model of hyperbolic geometry. We aim to capture the geometric evolution of their weighted topology and self-organization over time. Instead of restricting analysis to single checkpoints, we construct temporal parameter-graphs across $T$ snapshots of the optimization process. This reflects the view that neural networks encode information not only in their weights but also in the trajectory traced during training. Drawing on the idea that many complex networks admit embeddings in hidden metric spaces where distances correspond to connection likelihood, we present a geometric and temporal graph-based meta learning framework for obtaining dynamic hyperbolic representations of the underlying neural parameter graphs. Our model embeds temporal parameter-graphs in the Poincaré ball and learns from them while maintaining equivariance to within-snapshot neuron permutations and invariance to permutations of past snapshots. In doing so, it preserves functional equivalence across time and recovers the network’s latent geometry. Experiments on regression and classification tasks with trained MLPs show that hyperbolic temporal representations expose how structure emerges during training, offering intrinsic explanations of self-organisation in a given model training environment.", "tldr": "We encode MLP training traces as parameter graphs embedded in the Poincaré ball, processed by hyperbolic attention with recurrent kernel evolution. Outputs predict links and signed weights, with Riemannian optimization refining temporal embeddings.", "keywords": ["Graph Meta Networks", "Temporal Hyperbolic Embeddings", "Neural Weights as Data"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/94fafe64fa4d4fb9ef5e247f1799adf850e82f87.pdf", "supplementary_material": "/attachment/1aae0f3b97ee52611fdf33de424a17c607f4e511.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes to incorporate temporal and geometric considerations into study neural networks as data points (e.g. in INRs). The authors propose a methodology the process INR trajectories using the Poincare ball model of hyperbolic geometry to use for metanetwork purposes (process NNs as datapoints e.g. for classification of INRs or predicting generalistion from the trained weights themselves)\n\nI have given a 4 but would give a 3 if there was the option."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper seems correct and seems to have succeeded in adding temporal and geometric considerations into a framework for meta-networks.\n2. There is a lot of background material involved in the paper. this is both a strength, as there is a lot of technical detail to get right, but also  a weakness (see below)."}, "weaknesses": {"value": "I'd say overall my criticisms of the paper stem from the fact that I found it hard to read. This is by no means my area, but even still I think the authors could have made several improvements to the presentation:\n\n1. The motivation isn't very strong. The paper focuses on the technical problem (of adding temporal and geometric aspects to meta-networks), but not enough of the paper addresses why this is a problem to focus on.\n2. Likewise, the empirical results for the methodology (the proposed GTH-GMN) of this paper are quite bare. There are 6 terms in the loss function, each of which are motivated in section 3: how is the empirical performance of GTH-GMN affected without each of the 6 terms? This is essential for motivating the method. Why do we need two optimisation steps? The paper is missing quite a lot of ablations imo. Relatedly, is it essential to have temporal *and* geometric aspects to the framework: what happens if you remove one (how does performance vary as the number of checkpoints changes)?\n3. The contributions section (last paragraph of section 1) isn't very clear imo. The paragraph reads as if the contributions are methodological, are there any theoretical challenges that the authors needed to overcome in order to create the methodology (or was it mainly a case of applying and combining existing theory into a method). Likewise, I think a background section is missing between the related work and the research method sections in order to provide context for the reader of the most relevant background info (e.g. what is the most relevant methodological baseline and how does it work? Is this GMN, based on the name GTH-GMN?)\n4. How much more expensive is it to have to track the full trajectory that say the final checkpoint?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6leVYQTlrM", "forum": "Xq64xkQCak", "replyto": "Xq64xkQCak", "signatures": ["ICLR.cc/2026/Conference/Submission5712/Reviewer_3BLp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5712/Reviewer_3BLp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760898245952, "cdate": 1760898245952, "tmdate": 1762918212531, "mdate": 1762918212531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "A General Comment To All The Reviewers:"}, "comment": {"value": "First of all, we would like to thank all the reviewers for taking the time to read our paper and for providing their valuable and thoughtful feedback. Most importantly, we appreciate and feel encouraged by the strong recognition of our motivation as well as the overall consensus over the novelty of our work (oHUv, fCBW, dcw3). Additionally, we also appreciate that some of you (fCBW, oHUv) found the visualisations of our training dynamics interesting, sensing their implications towards intrinsic explainability. We also noted that the reviewers have not raised any major concerns regarding the technical validity of our approach in representing neural parameter temporal graphs within hyperbolic spaces, specifically the Poincaré ball, and have at times appreciated the mathematical machinery involved in the methodology.\n\nHowever, we also hear your concerns clearly regarding the paper being too mathematically dense, in need of empirical ablations to justify the loss terms, and the requirement to address computational cost and scalability. Moreover, the most crucial weakness of our paper that was unanimously highlighted was the readability of the work. We have taken these concerns regarding the manuscript's density, lack of component validation, and scalability constraints, as well as readability, very seriously.\n\nAll revisions in the manuscript are highlighted in blue to assist the reviewing process. We note that while the changes may appear extensive at first glance, they are primarily presentational adjustments designed to mitigate mathematical density and improve readability, without altering the underlying semantics or the core essence of the work. Crucially, no new equations or formalisms have been introduced; instead, we have expanded the exposition of the existing formalism to facilitate comprehension.\n\n\nReadability Issues. In response, we have undertaken a comprehensive revision of the paper. To address the consensus that the method section (Section 3) was \"hard to read,\" we have:\n\n1. Restructured Sections 3.2, 3.3, and 3.4 to prioritise intuitive explanations of the hyperbolic-attention graph and temporal mechanisms before introducing the mathematical formalisms. \n\n2. Moreover, in Section 3.4, we now provide a less mathematically dense exposition of our optimisation terms, with the formalisms moved to Appendix G, along with a fundamental explanation of the necessity of using a two-phased optimisation schema.\n\n3. Additionally, we have added more clarity in our motivation (Line 43- 48) to make the fundamental aspect of “why this problem needs to be focused” clear. Moreover, we address the concerns regarding the clarity of the last paragraph in the Introduction by underscoring its interdisciplinary nature and highlighting it as a non-trivial assimilation of concepts, ideas, as well as findings from different fields such as temporal learning, graph-based representations, weight space learning, hyperbolic geometry, network science and physics. We also highlight our belief that this work covers a research gap in representing deep neural network traces in hyperbolic spaces and could be used as a baseline for improving performance in future works, while still being a useful tool for studying the intrinsic evolution of geometry over time during network optimisation.\n\nWe have also reformatted the related works section on hyperbolic and temporal graph learning (Line 137-161) by making it free of acronyms of past methods and relying on explaining the concepts rather than unnecessarily focusing on the terms and abbreviations of the approaches.\n\nContinued Below.."}}, "id": "Xohfz6Iiet", "forum": "Xq64xkQCak", "replyto": "Xq64xkQCak", "signatures": ["ICLR.cc/2026/Conference/Submission5712/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5712/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5712/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763670502815, "cdate": 1763670502815, "tmdate": 1763673046172, "mdate": 1763673046172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper models the training process of neural networks as a sequence of evolving graphs. Each of these graphs is embedded in hyperbolic space. The authors introduce a Hyperbolic Graph Meta-Network (GTH-GMN) that learns to encode the temporal evolution of these graphs using hyperbolic attention and recurrent updates. This geometric representation is intended to provide intrinsic explainability by revealing structure in training dynamics. Experiments on small-scale tasks show consistent improvements over prior Euclidean and static graph baselines, along with interpretable visualizations of how networks evolve during training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's main strength is in its main idea: to think about the training of a neural network not just as a sequence of parameter updates, but as a geometric process that unfolds in hyperbolic space. This view allows the authors to tie together many ideas from disparate fields. The resulting metrics for tracking training seem to capture meaningful structure in how networks evolve, and the visualizations provide some insights into the learning process."}, "weaknesses": {"value": "I should preface my comments by noting that I am not an expert in the specific technical areas this paper draws on, which is reflected in my relatively low confidence score. I found the paper quite challenging to follow at times, largely due to its dense mathematical notation and the level of background knowledge it assumes from the reader. Despite the impressive mathematical machinery, I am not entirely convinced that the insights gained from this approach justify the conceptual and computational complexity it introduces."}, "questions": {"value": "1. How sensitive are the results to the choice of curvature or manifold model? For example, if the same temporal graph encoder were trained in Euclidean or spherical space, would the geometric patterns and performance differences still hold?\n\n2. Can the authors clarify what specific types of interpretability their method enables? Beyond visualizing trajectories, are there measurable insights (such as neuron redundancy, layer specialization, or early-stopping indicators) that can be extracted from the hyperbolic embeddings?\n\n3. The experiments focus on relatively small MLPs and simple datasets. How might this framework scale to larger architectures, such as Transformers or CNNs, where parameter spaces and symmetries are more complex? Are there computational or conceptual challenges expected in extending the approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8wnLVubh2m", "forum": "Xq64xkQCak", "replyto": "Xq64xkQCak", "signatures": ["ICLR.cc/2026/Conference/Submission5712/Reviewer_dcw3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5712/Reviewer_dcw3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752456419, "cdate": 1761752456419, "tmdate": 1762918212283, "mdate": 1762918212283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for studying MLP training dynamics through hyperbolic geometry. The authors construct temporal parameter graphs from checkpoints during training and embed them in the Poincaré ball using hyperbolic graph attention networks (HGAT) with kernel meta-evolution. The approach maintains permutation equivariance and ties edge weights to hyperbolic distances via power-law relationships, providing interpretable geometric representations of network self-organization during training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel geometric perspective: The application of hyperbolic geometry to temporal parameter graphs is original and well-motivated. The connection to network science findings (scale-free, hierarchical organization) provides strong theoretical justification.\n- Temporal modeling: Unlike most prior meta-learning work that analyzes single checkpoints, this work explicitly models training trajectories.\n- Comprehensive methodology: The signed weight regressor with magnitude-distance power law (Eq. 13) and conformal sign prediction (Eq. 15) shows careful design. The two-phase optimization (Euclidean + Riemannian) is well-executed."}, "weaknesses": {"value": "- Limited performance gains: On CIFAR-10 generalization prediction, the method achieves τ=0.846±0.004, notably below NFN baselines (0.922-0.934). While the authors acknowledge this is \"expected,\" it raises questions about practical utility. The gap suggests the geometric compression may discard accuracy-correlated information that end-to-end methods capture.\n- Architectural limitations: The approach is restricted to MLPs. The authors mention this limitation but don't provide a clear path to extending to CNNs, Transformers, or other architectures that dominate modern deep learning. This severely limits practical applicability and impact.\n- Computational cost not addressed: The paper doesn't discuss training time, memory requirements, or scalability. Hyperbolic operations, Riemannian optimization, and temporal processing likely add significant overhead.\nTables 5-6 mention \"practical caps\" but no runtime comparisons are provided.\n- High variance in some experiments: Sinusoid task: 1.06±0.24 MSE vs GMN's 1.13±0.08 - the 3x higher variance is concerning and poorly explained. The authors attribute this to \"known sensitivities in Riemannian optimization\" but don't investigate mitigation strategies."}, "questions": {"value": "- Can you provide runtime/memory comparisons with baselines? How does the approach scale with model size?\n- What specific architecture modifications would be needed to extend beyond MLPs? Is this fundamentally intractable?\n- Could you provide ablations showing the contribution of each loss term (Eq. 32)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8HpAXKW49a", "forum": "Xq64xkQCak", "replyto": "Xq64xkQCak", "signatures": ["ICLR.cc/2026/Conference/Submission5712/Reviewer_fCBW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5712/Reviewer_fCBW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851727593, "cdate": 1761851727593, "tmdate": 1762918211408, "mdate": 1762918211408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission builds on prior work that seeks to develop methods for identifying the behavior of a random deep neural network (DNN), as well possibly train the DNN. The authors use temporal information (i.e., the optimization history) to learn models with greater accuracy. In addition, the authors embed the snapshots of the DNNs (across training) in a hyperbolic geometry. This can aid in performance and interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work was well motivated. The utility of metanetworks was clearly presented and the idea that optimization trajectories could be leveraged for further improvement was good. \n\n2. The authors test on several different experimental set-ups, finding strong performance on in all cases. While they are not always best, their method is not so far off. \n\n3. The visualization of the hyperbolic embedding (Figure 2) was very interesting and points to possibly new ways to understand DNN training."}, "weaknesses": {"value": "1. I think the biggest weakness of this work is that it is so dense. I found the last paragraph of the Related Works Section and Sec. 3.2-3.4 just full of acronyms, method names, and details. I'm sure that this is partially due to the fact that I am not so familiar with this area, but it felt very difficult to follow.\n\n2. The result of having so much detail crammed into the Methods section was that then there was little room for discussion on the experiments. It's not necessary to have all the model and experimental details, but it wasn't always clear to me what was even really being tested in the 3 experiments in Sec. 4. What exactly was tested in \"Classification of Images via INR traces\"? Which image is being shown to a MLP? if so, this seems to not be so aligned with the motivation in the Introduction. Similarly, I was confused as to what a \"sinusoid–MLP\" is and how the developed method was being used. \n\n3. A minor point, but one way to study the optimization trajectory is to use tools from dynamical systems. These tools can be invariant to node ordering and can extract interpretable and comparable structure (e.g., https://proceedings.neurips.cc/paper_files/paper/2024/hash/2a07348a6a7b2c208ab5cb1ee0e78ab5-Abstract-Conference.html). How might the authors' work be extended/improved by including such dynamical characterization?"}, "questions": {"value": "1. What exactly was tested in \"Classification of Images via INR traces\"?\n\n2. What is a \"sinusoid–MLP\" and what exactly was being tested in \"Predicting Sine Wave Frequency\"? \n\n3. How might the authors' method be improved with - instead of passing in many optimization graphs - the dynamics of the optimization were first filtered with dynamical systems approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qgGzmFnmm1", "forum": "Xq64xkQCak", "replyto": "Xq64xkQCak", "signatures": ["ICLR.cc/2026/Conference/Submission5712/Reviewer_oHUv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5712/Reviewer_oHUv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934120243, "cdate": 1761934120243, "tmdate": 1762918210378, "mdate": 1762918210378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}