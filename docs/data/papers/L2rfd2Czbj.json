{"id": "L2rfd2Czbj", "number": 7329, "cdate": 1758016207378, "mdate": 1763747200641, "content": {"title": "wd1:  Weighted Policy Optimization for Reasoning in Diffusion Language Models", "abstract": "Improving the reasoning capabilities of diffusion-based large language models (dLLMs) through reinforcement learning (RL) remains an open problem. The intractability of dLLMs likelihood function necessitates approximating the current, old, and reference policy likelihoods at each policy optimization step. This reliance introduces additional computational overhead, and can lead to large variance and estimation error in RL objective -- particularly in computing the policy ratio for importance sampling. To mitigate these issues, we introduce wd1, a novel ratio-free policy optimization approach that reformulates the objective as a weighted log-likelihood, requiring only a single approximation for the current parametrized policy likelihood. We formally show that our proposed method can be interpreted as energy-guided discrete diffusion training combined with data unlearning, thereby confirming its theoretical soundness. In experiments, wd1 outperforms diffusion-based GRPO (d1) while requiring lower computational cost, achieving up to a $+43\\\\%$ improvement in accuracy. Furthermore, we extend wd1 to denoising-stepwise weighted policy optimization (wd1++), which surpasses concurrent RL for dLLMs methods, attaining improvement $+6.2\\\\%$ on MATH500 ($44.2\\\\%$) and $+2.5\\\\%$ on GSM8K ($84.5\\\\%$) over d1 with only 20 training steps.", "tldr": "We propose a novel policy optimization method for dLLMs reasoning, reducing the error caused by log-likelihood approximation error.", "keywords": ["Diffusion Language Models", "Reinforcement Learning", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe30f3db68502aefcab71d6106f1b6b6d3872a65.pdf", "supplementary_material": "/attachment/ff56ef6acefcd9a755d36f4ba3726617d2bfa549.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a weighted log-likelihood objective function for fine-tuning dLLMs via reinforcement learning. Compared to other RL-based approaches, the proposed method, *wd1*, improves the reasoning ability of dLLMs and also reduces the computational overhead due to the ratio-free policy optimization. In addition, the authors provide a theoretical insight to connect their method with energy-guided diffusion sampling, by viewing the energy function as the negative advantage. In experiments, the proposed method is evaluated on four reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation and the main idea is clear, and the overall paper is well written and organized. \n- The proposed weighted log-likelihood objective is interesting.\n- In experiments, the proposed method outperforms other RL-based fine-tuning approaches."}, "weaknesses": {"value": "1. The connection between the proposed method and the energy-guided diffusion model looks good but overstated. And the motivation is confusing: if fine-tuning the dLLM with wd1 is equivalent to training a energy-guided diffusion model, why don't you just fine-tuning dLLMs with the energy-based objective? The authors probably need to provide an ablation study or analysis on it.\n\n2. In experiments, the proposed method is built on LLaDA and only evaluated on four benchmarks. But as a large language model, it should be also evaluated on more datasets such as those in the LLaDA paper (Table 2) to illustrate its scalability.\n\n3. It would be better to give more discussion and related works for the “ratio-free” policy optimization. In Table 2, the difference of the training cost between two approaches is actually small. What's other advantages of using this “ratio-free” policy optimization?\n\n4. This RL approach designs different reward functions for different tasks and datasets, which would be unpractical for a foundation LLM. And the proposed model is trained individually on each dataset, which might be unfair to LLaDA model because it is not specifically fine-tuned on these datasets."}, "questions": {"value": "Please see \"Weaknesses\" and, what's the main limitation of this work? It would be better to provide a discussion on it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XUVkcM1wOx", "forum": "L2rfd2Czbj", "replyto": "L2rfd2Czbj", "signatures": ["ICLR.cc/2026/Conference/Submission7329/Reviewer_irEQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7329/Reviewer_irEQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761475321474, "cdate": 1761475321474, "tmdate": 1762919444868, "mdate": 1762919444868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces wd1, a novel reinforcement learning algorithm tailored for diffusion Large Language Models. Unlike traditional on-policy RL, wd1 leverages the closed-form optimal solution of the KL-regularized RL problem. It projects the current model toward this optimum using weighted regression on samples generated by behavior policies.\n\nThe authors also identify a significant issue regarding the under-penalization of negative samples and propose a complementary penalty term to address it. Experiments on popular math and reasoning benchmarks demonstrate that wd1 achieves significant improvements over the d1 baseline. Furthermore, ablation studies successfully validate the effectiveness of the algorithm's key design choices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Wd1 employs a clever approach to bypass the computation of the log-probability ratio, which is often intractable or computationally expensive for diffusion LLMs.\n\nThe introduced penalty term for negative samples is a notable contribution. It appears to effectively address a long-standing issue in weighted behavior-cloning methods—namely, the insufficient penalization of negative samples. The proposed solution is both simple and effective."}, "weaknesses": {"value": "wd1 appears highly sensitive to the coefficient that mixes positive weights w+ and negative weights w-. In Table 9, varying this coefficient lambda by 0.1 results in an accuracy drop of over 10%, which suggests the model may require careful tuning."}, "questions": {"value": "In Figure 2 (MATH500), all tested RL algorithms seem to cause a significant drop in reward compared to the baseline. Could the authors elaborate on why this might be the case?\n\nSome copy-edits: Lemma 1, A_t(x_0) should be A_t(x_t)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GYeAgpSd7Y", "forum": "L2rfd2Czbj", "replyto": "L2rfd2Czbj", "signatures": ["ICLR.cc/2026/Conference/Submission7329/Reviewer_cbic"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7329/Reviewer_cbic"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965181896, "cdate": 1761965181896, "tmdate": 1762919444407, "mdate": 1762919444407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces wd1 and wd1++, a novel RL algorithm for dLLMs reasoning finetuning. Instead of requiring likelihood approximation in previous work, wd1 is ratio-free and from weighted log-likelihood maximization. Such a formulation reduces the estimation variance and the compute cost. The paper provides a theoretical foundation and demonstrates the connection with energy-based model training. Empirically, wd1 and wd1++ demonstrates superior performance on four benchmarks, while requiring fewer rollouts and no SFT."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The idea is well-motivated and paper is well-organized and easy to follow.,\n- The paper has strong theoretical foundation and analysis.,\n- The experiments are thorough and verify the effectiveness of the methods and design choices.,"}, "weaknesses": {"value": "- From my understanding, one key design choice is to choose reverse KL instead of forward KL, which makes the optimization analytic and ratio-free. However, the paper lacks the discussion on this motivation and its impact, e.g., how it will affect the constraint.,\n- The limitations are not discussed in detail. I didn’t find any place to explicitly discuss the limitations of the method.,"}, "questions": {"value": "- Given the wd1 objective, is it easy to collapse to some certain solutions? how diversity the finetuned output is?,\n- Regarding the exponential weights, how easy to tune the weight parameters? is the performance sensitive to the scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gOv5fNcd8f", "forum": "L2rfd2Czbj", "replyto": "L2rfd2Czbj", "signatures": ["ICLR.cc/2026/Conference/Submission7329/Reviewer_kDWu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7329/Reviewer_kDWu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7329/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762529907846, "cdate": 1762529907846, "tmdate": 1762919444082, "mdate": 1762919444082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}