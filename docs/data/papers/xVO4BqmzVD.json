{"id": "xVO4BqmzVD", "number": 15030, "cdate": 1758247037276, "mdate": 1759897334489, "content": {"title": "SpecRA: Monitor Degenerative Repetition in LLM Agents using Randomized FFT", "abstract": "LLM-based agents also suffer from \"degenerative repetition\" like chatbots, which leads to task failure and results in significant waste of computational resources and API costs until token limit is reached. Existing methods require modification of training process or customization of model deployment, and detection algorithms are brittle to approximate or structural recurrence. We therefore introduce SpecRA, a simple yet effective algorithm for detection of self-repetitions in text. Via a randomized projection from the large LLM vocabulary onto a unit-norm complex sequence, our method leverages the power of the Fast Fourier Transform (FFT) to compute the sequence's autocorrelation. Peaks in the autocorrelation function robustly reveal the underlying periodicity of the content, with tolerance to minor variations. Through an analysis of 813 repetitive samples identified from 1.13M records of anonymized agent outputs, we build a taxonomy of repetition modes in agents and show that SpecRA offers a lightweight, non-intrusive mechanism for constructing more reliable and cost-efficient LLM agents accross both standard open-source model deployments and proprietary models.", "tldr": "", "keywords": ["Degenerative Repetition", "Agent Reliability", "LLM Agents", "Repetition Detection", "Spectral Analysis", "Signal Processing"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/efee99ce81bd907961ab17a7fc7db9e353faeda5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the issue of degenerative repetition in large language model (LLM)-based agents, where models get stuck in repetitive cycles, leading to wasted computational resources and increased API costs. The authors propose SpecRA, a fast, spectral method for detecting self-repetitions in LLM outputs. By leveraging the Fast Fourier Transform (FFT) and a randomized token projection, the method analyzes the periodicity of token sequences and identifies when repetition occurs. The paper presents a detailed taxonomy of repetition modes and demonstrates that SpecRA can reliably detect and classify repetitions across a range of LLM outputs. The authors argue that their method provides an efficient, non-intrusive mechanism to reduce computational waste and improve agent reliability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of applying spectral analysis to detect degenerative repetition in LLM outputs is novel. SpecRA offers a unique approach compared to existing penalty-based methods by focusing on post-generation detection through signal processing, providing a lightweight and non-intrusive solution."}, "weaknesses": {"value": "1. Problem Importance: The problem of degenerative repetition is important, but the paper does not convincingly argue why it is a significant issue that warrants a completely new approach. Simple methods like repetition penalty or temperature adjustments can mitigate repetition without much loss in performance, and these are commonly used in practice. Also for long repetitive responses, techniques like early truncation could resolve these issues in a more cost-efficient manner, and the paper doesn’t explore these simpler alternatives in detail. While SpecRA is interesting, it’s unclear if the problem is as pressing as suggested.\n2. Threshold Management: SpecRA requires setting a threshold for repetition detection, which can be cumbersome. The paper mentions that this threshold must be carefully adjusted for different vocabularies, which might require reconfiguration for each model. This adds a layer of complexity that could make the approach harder to deploy in production."}, "questions": {"value": "Why is degenerative repetition such a critical issue that requires a new solution like SpecRA? Can existing solutions like repetition penalty or temperature adjustments not achieve similar results without introducing the complexity of SpecRA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4mtnXxOuCQ", "forum": "xVO4BqmzVD", "replyto": "xVO4BqmzVD", "signatures": ["ICLR.cc/2026/Conference/Submission15030/Reviewer_8wU1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15030/Reviewer_8wU1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760693587295, "cdate": 1760693587295, "tmdate": 1762925357881, "mdate": 1762925357881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SpecRA, a new algorithm to detect degenerative repetition in LLM agents. It works by projecting the token stream onto a random complex-valued sequence and then using the FFT to efficiently compute its autocorrelation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The method's primary strengths are its efficiency, and its robustness to the approximate repetitions that break traditional exact-match detectors.\n- The paper is very well motivated and is easy to read. The paper also openly mentions the limitations of the method, which is highly appreciated."}, "weaknesses": {"value": "- In L195, the authors claim that the randomized projection into the complex plane makes the detector \"robust to lexical variation\" while preserving the overall periodic structure. Can the authors explain the connection between this projection and improved robustness to lexical variation?\n- The decision to utilize only the real component of $R_l$ should be explicitly justified. While I could infer the motivation behind it, the exclusion of the imaginary component is not theoretically grounded in the text. A brief explanation would improve the conceptual clarity of the proposed method.\n- The motivation behind the randomized projection step remains unclear. Mapping a vocabulary space of approximately 200K tokens into a 360$\\degree$ complex plane appears arbitrary and potentially lossy. The authors mention this limitation briefly, but it would be more compelling to compare this approach with projection strategies, such as those based on embedding spaces or a higher-dimensional latent space. \n- The experimental results are interesting but lack comparative context. Without evaluation against baseline models, for instance, n-gram based detectors or other lexical similarity measures, it is difficult to assess the effectiveness of SpecRA."}, "questions": {"value": "All the questions and suggestions have been listed in the weaknesses.\n\nWhile I do appreciate the authors addressing several limitations that I thought of while reading the paper (high FPR, failure in insertions / deletions & more), SpecRA, by itself, is a tool that would not be very useful realistically. Regardless, I would push this paper towards acceptance - based on the author's responses & for the theoretical insights and formulations provided by the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0f9H40RHLu", "forum": "xVO4BqmzVD", "replyto": "xVO4BqmzVD", "signatures": ["ICLR.cc/2026/Conference/Submission15030/Reviewer_vaVL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15030/Reviewer_vaVL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781808056, "cdate": 1761781808056, "tmdate": 1762925357018, "mdate": 1762925357018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SpecRA, a spectral-based algorithm for detecting degenerative repetition in LLM-based agents, where models become trapped in recursive loops generating near-identical sequences that cause task failure and computational waste. The method works by projecting each token to a unit-magnitude complex number via randomized phase assignment, then computing autocorrelation using FFT via the Wiener-Khinchin theorem to efficiently identify periodic patterns with O(W log W) complexity. The paper also provides theoretical guarantees including false-positive bounds and detection efficacy under $\\epsilon$-mismatch conditions (Theorem 1), demonstrates robustness to substitutions, insertions, and deletions through synthetic experiments, and analyzes repetitive samples from >1M agent traces to build a taxonomy of four failure modes such as structural repetition and syntactic degradation. The authors recommend practical detection thresholds for different use cases."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Novel and theoretically grounded approach: The paper presents a creative solution by recasting text repetition detection as a signal processing problem. The use of randomized phase projection with FFT-based autocorrelation is elegant and well-motivated.\n\n+ Strong theoretical foundation: Provides rigorous probabilistic guarantees through Lemma 1 (false-positive bounds using Hoeffding's inequality) and Theorem 1 (detection power under approximate periodicity), making the approach principled rather than heuristic.\n\n+ Practical guidance: Provides actionable threshold recommendations based on empirical percentiles from diverse corpora, facilitating real-world adoption.\n\n+ Clear presentation: The paper is well-written with good motivation, clear problem formulation, and effective visualizations (especially Figure 1).\n\n+ Comprehensive empirical validation: The paper includes synthetic experiments (Section 6.1), real-world corpus analysis across multiple domains (Wikipedia, GitHub, agent traces), and builds a valuable taxonomy of repetition failure modes."}, "weaknesses": {"value": "+ Missing baseline comparisons: The empirical analysis (Section 6) lacks comparisons with existing detection methods. Even if exact string matching and edit-distance methods have limitations, quantitative comparisons on the same test sets would strengthen claims about SpecRA's advantages in accuracy, speed, and robustness.\n\n\n+ Taxonomy validation: The classification of 549 repetitive samples into four categories appears to be manual. The paper lacks details on: (a) inter-annotator agreement if multiple annotators were used, (b) whether the 264 excluded samples introduce selection bias, (c) validation that categories are mutually exclusive and comprehensive.\n\n+ Real-world deployment details: The paper mentions \"1.13M records of anonymized agent outputs\" but provides minimal context about: the agents' tasks, which models were used, what triggered the repetitions, and how representative this dataset is."}, "questions": {"value": "+ Baseline performance: Can you provide quantitative comparisons against n-gram overlap, suffix trees, or approximate string matching algorithms (e.g., using sliding windows with edit distance) on your test sets? Even if they're slower, understanding the accuracy trade-offs would be valuable.\n\n+ False negative analysis: What is the false negative rate of SpecRA on your agent trace dataset? Are there patterns that SpecRA consistently misses, and what characterizes them?\n\n+ Interaction with decoding parameters: How does SpecRA's detection rate vary with temperature, top-p, or other sampling parameters? Do higher temperatures reduce repetition occurrence or just change repetition patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z8r7AKhuYJ", "forum": "xVO4BqmzVD", "replyto": "xVO4BqmzVD", "signatures": ["ICLR.cc/2026/Conference/Submission15030/Reviewer_UZ8b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15030/Reviewer_UZ8b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878549589, "cdate": 1761878549589, "tmdate": 1762925356648, "mdate": 1762925356648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors claim to introduce a new approach for detecting repetition degeneration in texts generated by LLMs, based on the discrete Fast Fourier Transform (FFT). The authors claim that the FFT is more scalable and robust for natural language repetitions than existing n-gram (k-mer)- and edit distance-based approaches. Authors provide proof that their method has a controllable false positive rate and exhibits exponentially improved performance with increasing attention frame size.In this papers the authors claim to introduce a new approach for the detection if repetition degeneration of texts generated by LLMs based on the discrete Fast Fourier Transform (FFT). The authors claim that FFT is more scalable and robust for natural language repetitions than the existing n-gram (k-mer) and edit distance- based approaches. Authors provide a proof that their method has a controllable false positive rate and exponentially improved performance with the increase in the attention frame size."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The detection of repetition degeneration in LLM output is a critical step in both its day-to-day usage, as highlighted by the authors, as well as in model training to suppress unwanted repetition at the Reinforcement Learning stage. The overall approach undertaken by the authors is sound and likely applicable in practice. Additionally, authors introduce an additional criterion for performance evaluation- \"timely detection\", which is relevant in the context of repetition detection but not a statistical criterion commonly used in other settings."}, "weaknesses": {"value": "- While the overall citations are well-selected and consistent with the text, authors repeatedly cite (Holtzman et al., 2019) as the paper introducing or using the repetition penalty (eg, L040-047). However, this paper focused only on nucleus sampling, addressing the repetition text through a temperature-based sampling. It does not use repetition penalties.\n\n- The authors do not sufficiently justify the need for their method. For instance, the k-mer sliding approaches that authors discard as insufficiently scalable (L091-097) can be applied to UTF-8 encodings of the character n-grams, effectively reducing the vocabulary back to 16 characters, making direct bioinformatics k-mer approaches computationally feasible again. Similarly, it is unclear how frequently the partial repetition problem is used to justify their approach. \n\n- The authors do not seem to evaluate the computational performance of the proposed method, one of its advantages compared to existing methods, as claimed by the authors.\n\n- Finally, the contribution of the paper is hard to identify and seems minor at first glance. Teglanceepetition analysis has been commonly cited as an application of the FFT to texts (e.g., https://math.stackexchange.com/questions/422948/fourier-transform-of-text; https://cp-algorithms.com/algebra/fft.html). The author's addition appears to be the random embedding of text in the complex plane before applying the FTT, but it is unclear why this step is important, except that it makes the proofs of Lemma 1 and Theorem 1 simpler. I am not sure, however, that such a contribution could justify acceptance to a conference of the notoriety of ICLR. \n\n\nMinor comment: In Lemmas 1 and Theorem 1, tau is commonly used to denote time or position index in frequency analysis and integration; using it as a threshold may be confusing to readers from that background."}, "questions": {"value": "- How frequent are the real-world scenarios in which the repetition degeneration occurs with minor variations, as suggested on L044-047? Most of the literature and reports on the topic suggest exact repetitions as the dominant degeneration mode.\n\n- Is it possible to switch from a custom embedding of the tokens to the embedding provided by the model tokenizer? This is likely to lead to better performance by removing the need for a separate embedding step.\n\n- Could you please clarify what you mean by \"constant energy\" on L049?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "LLM usage: The Authors report extensive LLM usage, utilizing an LLM that they refuse to identify, for performing the original derivation of critical sections of the proof, which is a central contribution of the paper. While proof automation and assistance with LLMs are active areas of research, they are known to be somewhat haphazardous areas with extensive room for subtle errors that are hard to spot to non-expert human reviewers; and that often imitate existing proofs.\n\nBased on a subtle citation mismatch with a high overall quality, I also suspect that the introduction and overview of existing methods were LLM-generated, as well as potentially the code implementation of FFT. It is unclear how it would connect with the commonly used LLM deployment Python code, although potentially doable within the stack of commercial LLM providers."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Kyw3MWPG7", "forum": "xVO4BqmzVD", "replyto": "xVO4BqmzVD", "signatures": ["ICLR.cc/2026/Conference/Submission15030/Reviewer_B63k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15030/Reviewer_B63k"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996348025, "cdate": 1761996348025, "tmdate": 1762925356324, "mdate": 1762925356324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}