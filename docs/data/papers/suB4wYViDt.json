{"id": "suB4wYViDt", "number": 17615, "cdate": 1758278369159, "mdate": 1763741350404, "content": {"title": "STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation Quantization", "abstract": "Quantization is the key method for reducing inference latency, power and memory footprint of generative AI models.\nHowever, accuracy often degrades sharply when activations are at low bit widths. \nRecent work suggests that invertible linear transformations (e.g. rotations) can aid quantization, by reparameterizing feature channels and weights. \nIn this paper, we propose Sequence Transformation and Mixed Precision (STaMP) quantization, a novel strategy that applies linear transformations along the sequence dimension to exploit the strong local correlation in language and visual data. \nBy keeping a small number of tokens in each intermediate activation at higher precision, we can maintain model accuracy at lower (average) activations bit-widths.\nWe evaluate STaMP on recent LVM and LLM architectures, demonstrating that it significantly improves low bit width activation quantization and complements established activation and weight quantization methods including recent feature transformations.", "tldr": "STaMP applies sequence transforms with mixed-precision quantization to exploit token activation correlations, enabling accurate low bit width inference for LLMs and LVMs while complementing existing feature transforms and weight quantization methods.", "keywords": ["Quantization", "Efficient Deep Learning", "Activation Quantization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e04f70c21f0824b404db8272a83940bfe0aa159c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes STaMP (Sequence Transformation and Mixed Precision), a novel post-training activation quantization method for LLMs and LVMs. Unlike prior work that applies invertible feature transformations across the channel dimension, STaMP introduces linear sequence transformations to exploit correlations between neighboring tokens. By concentrating activation energy into a few tokens and assigning them higher precision under a fixed average bit budget, STaMP effectively reduces quantization error. The authors approximate the optimal Karhunen–Loève Transform using efficient DCT or DWT implementations and derive theoretical bounds on quantization loss. Extensive experiments on PixArt-Σ, SANA, and LLaMA/Qwen models show consistent improvements over existing methods such as SmoothQuant, QuaRot, and SVDQuant, both in SQNR and downstream performance, with minimal computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces sequence-domain transformations for activation quantization, a direction largely unexplored compared to prior channel-wise or feature-domain approaches. It can be combined with existing quantization methods and feature transformations without retraining or altering model weights.\n2. Extensive results on both LVMs (PixArt-Σ, SANA) and LLMs (LLaMA 3, Qwen 2.5) show clear improvements in SQNR, perplexity, and visual quality under low-bit settings (W4A4), confirming robustness and generality.\n3. The paper provides clear figures (e.g., Figs. 2–6) illustrating how energy concentration improves quantization performance and complements feature transforms."}, "weaknesses": {"value": "1. The paper does not quantify the additional latency or memory cost of applying and inverting sequence transforms in large-scale models. Without such analysis, it is difficult to assess whether the efficiency gains from lower-bit activations outweigh the added transform cost in practical deployment scenarios.\n2. For the decoding stage of LLMs, STaMP may not be directly applicable, since each newly generated token would require recomputing the sequence transform matrix L."}, "questions": {"value": "1. How would STaMP operate during autoregressive decoding, where tokens are generated sequentially? Since the sequence transform matrix L require access to the full sequence, does it mean that we need to recompute L in every decoding step?\n2. Could you provide quantitative measurements of the runtime and memory overhead introduced by applying and inverting the sequence transform in large transformer models? How does this cost compare to the savings from reduced activation bitwidth?\n3. How is the mix-precision processed in STaMP? Specifically, since there are 64 tokens kept at 8 bits, and the rest uses 4 bits, how do you run the mix-precision model efficiently?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "er2fTHmTKs", "forum": "suB4wYViDt", "replyto": "suB4wYViDt", "signatures": ["ICLR.cc/2026/Conference/Submission17615/Reviewer_jGRZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17615/Reviewer_jGRZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761181407385, "cdate": 1761181407385, "tmdate": 1762927477470, "mdate": 1762927477470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces STaMP (Sequence Transformation and Mixed Precision), a novel post-training quantization (PTQ) framework aimed at addressing low-bit activation quantization in large generative models (LLMs and LVMs). The core innovation, which distinguishes this work from prior art focused on feature-dimension transforms, is the application of linear transformations (e.g., DWT) along the sequence dimension. This method seeks to exploit local token correlations to compact activation energy into a minority of transformed tokens. Subsequently, the framework employs a mixed-precision strategy, allocating higher bit-widths to these high-energy tokens while quantizing the remainder more aggressively. The authors demonstrate through experiments that STaMP can serve as an orthogonal module that  consistently improves the performance of various existing PTQ methods under a W4A4 setting, without requiring any retraining."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. High Novelty: The primary strength of this work lies in its high degree of originality. By shifting the focus of quantization transforms from the feature dimension to the sequence dimension, the authors introduce a new research paradigm.\n 2. Solid Experimental Results: The paper's claims are supported by consistent and significant performance improvements across a range of models (LLMs and LVMs), datasets, and strong baselines. \n3. Excellent Complementarity: The experiments clearly demonstrate that STaMP is an orthogonal improvement that can be combined with state-of-the-art methods like QuaRot to achieve further gains. \n4. Clear Motivation and Exposition: The analogy to classical media compression provides a strong theoretical basis and an intuitive understanding of the proposed method."}, "weaknesses": {"value": "1. Unaddressed Implementation Overhead: The most significant weakness is the failure to discuss or quantify the runtime overhead of the per-token mixed-precision scheme on current hardware. The lack of native hardware support may necessitate inefficient dequantization operations, which would severely impact the method's practical speedup. \n2. Unquantified Transform Cost: While the computational cost of the DWT/DCT transform is theoretically small, the paper fails to provide a simple quantitative analysis to substantiate this claim (e.g., by comparing it to the computational load of an FFN layer in a Transformer block), leaving its relative overhead ambiguous."}, "questions": {"value": "1. Regarding practical implementation on current mainstream GPUs (e.g., NVIDIA A100/H100), could the authors elaborate on a viable strategy for the proposed mixed-precision scheme? What is the anticipated latency overhead introduced by dequantization or conditional logic compared to a uniform W4A4 implementation? \n2. To contextualize the cost of the proposed transform, could the authors provide a brief computational complexity analysis comparing the $O(ds)$ cost of DWT to the $O(s \\cdot d \\cdot d_{ffn})$ cost of a typical Transformer FFN layer, thereby quantifying the relative overhead introduced by the transform itself? \n3. In the LLM experiments, a fixed strategy of keeping the first 64 tokens in high precision was applied to all methods. Have the authors considered whether this is an optimal strategy for the baseline methods, which do not employ sequence transforms? An ablation study that implements and compares against a dynamic, magnitude-based token selection strategy for the baselines could further strengthen the fairness of the experimental comparison. \n4. To strengthen the paper's motivation, particularly the analogy to media compression, further insight is requested. Technologies like JPEG use energy compaction to more aggressively compress high-frequency information, which is less perceptible to the human eye. What is the conceptual equivalent in the context of STaMP? What kind of information corresponds to the low-energy components after the DWT/DCT transform, which are subsequently assigned lower precision? A more intuitive explanation of which aspects of the token sequence are deemed \"less critical\" by the transform would significantly enhance the persuasiveness of the paper's motivation.\n$\\textbf{If these issues can be resolved, I will consider giving a higher score.}$"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c3GYQ9TTCk", "forum": "suB4wYViDt", "replyto": "suB4wYViDt", "signatures": ["ICLR.cc/2026/Conference/Submission17615/Reviewer_sLp3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17615/Reviewer_sLp3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818373757, "cdate": 1761818373757, "tmdate": 1762927477034, "mdate": 1762927477034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Sequence Transformation and Mixed Precision(STaMP), a new PTQ method that operates along the sequence dimension, different from most recent methods that focus on utilizing rotation to modify along the feature dimension. The key idea is to exploit local correlation between sequential tokens by applying a sequence-wise linear transform (e.g., DCT, DWT) before quantization, followed by a mixed-precision strategy that assigns higher bit-widths to a subset of tokens carrying higher \"energy.\""}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. New Perspective: The paper introduces quantization transformations along the sequence dimension—a novel and orthogonal approach compared to existing methods. This direction effectively exploits temporal or spatial token correlations often overlooked by prior research.\n\n2. Solid Theoretical Analysis: The work offers a well-defined mathematical treatment of quantization error and establishes a theoretical upper bound (Theorem 1) that connects token energy, bit allocation, and resulting error.\n\n3. Synergistic with Existing Methods: The proposed approach integrates seamlessly with leading quantization strategies (e.g., QuaRot, SmoothQuant, FlatQuant, SVDQuant), delivering complementary and cumulative performance improvements."}, "weaknesses": {"value": "1. Missing Real-Time and Latency Evaluation: The paper does not report inference speed, latency, or hardware efficiency metrics—critical aspects for assessing quantization performance. Given that the multiplication of XL occurs online, it would be valuable to analyze its impact during inference.\n\n2. Absence of Calibration Set Ablation: There is no investigation into how the size of the calibration dataset influences performance, which is an essential factor for post-training quantization (PTQ) methods."}, "questions": {"value": "See Above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oQdn7LFvxa", "forum": "suB4wYViDt", "replyto": "suB4wYViDt", "signatures": ["ICLR.cc/2026/Conference/Submission17615/Reviewer_jfMb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17615/Reviewer_jfMb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865730263, "cdate": 1761865730263, "tmdate": 1762927476632, "mdate": 1762927476632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Shared Response"}, "comment": {"value": "We thank the reviewers for the insightful comments and questions which provided valuable feedback to improve our submission; and for recognizing the novelty, clarity and applicability of STaMP. We hereby address the shared concern about lack of assessment of the overhead for the proposed transform and mixed precision scheme. First, we want to underline that the real-time performance is heavily influenced by many hardware-related factors (e.g. presence of Tensor cores) and software support for some specific operation (int4/int8 matmuls), which vary greatly depending on the platform (e.g. desktop vs mobile). For this reason, other than relying solely on CUDA latency, we also highlight on number of floating point (flops) and binary operations (bops), which are indicative of the overall number of required arithmetic operations. We articulate our analysis in two parts covering the performance of the DWT and mixed precision scheme, respectively. Recognizing the relevance of these aspects, results are also included in the main text of the paper (Section 5.5, Appendix B.3).\n\n\nTo summarize, STaMP (with DWT) introduces an overhead comparable to other methods in literature (such as Fast Hadamard Transform) with little overhead compared to vanilla A4W4. The latency measurement, in practice, are heavily influenced by the support for some specific operations (such as int4 matmuls). Nevertheless, we believe that demonstrating the applicability of more aggressive activation quantization methods can drive the demand for specific hardware/software solutions to further improve on the energy and latency for the inference process of large generative models."}}, "id": "GWvT3edLQa", "forum": "suB4wYViDt", "replyto": "suB4wYViDt", "signatures": ["ICLR.cc/2026/Conference/Submission17615/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17615/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17615/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763736106928, "cdate": 1763736106928, "tmdate": 1763736106928, "mdate": 1763736106928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The paper proposes STaMP quantization to mitigate accuracy loss in low-precision activation quantization for LLMs and LVMs.\n2. Unlike existing feature-wise methods, STaMP applies linear transformations along the sequence dimension to exploit local correlations in language and visual data.\n3. It uses a mixed-precision scheme: a small number of high-energy tokens are kept at higher precision to maintain accuracy with lower average bit-widths.\n4. STaMP complements established feature transformations and weight quantization methods without conflicting with them.\n5. Experiments on models like Llama 3 and PixArt-Σ demonstrate consistent accuracy improvements in 4-bit activation quantization.\n6. For efficiency, STaMP adopts practical transforms instead of complex KLT, balancing energy concentration and computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is really well-written in both figures, tabs, equations, and texts. \n2. The paper contains experiments on both SD and LLM. \n3. The method design features strong complementarity and is compatible with the existing technical system."}, "weaknesses": {"value": "1. I a m confused  with the argument in QuaRot that rotation only occurs in the feature dimension, because some weights (such as up/gate projection) are left-multiplied by the rotation matrix.\n2. The experiments on Large Language Models is insufficient: (1) The model size is relatively small, and it has not been scaled up to the magnitude greater than 8B; (2) There are only PPL experiments, with the lack of mainstream zero-shot reasoning experiments, which are more crucial for verifying the effectiveness of PTQ.\n3. The paper introduces left-invertible matrices, but it does not provide a detailed and clear explanation of how such matrices are integrated into the weight parameters of the original architecture (like QuaRot and SpinQuant).\n4. As a work focusing on quantization, the paper does not report experimental results related to accelerated inference."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o9jjZagCK8", "forum": "suB4wYViDt", "replyto": "suB4wYViDt", "signatures": ["ICLR.cc/2026/Conference/Submission17615/Reviewer_F9G6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17615/Reviewer_F9G6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936534255, "cdate": 1761936534255, "tmdate": 1762927476177, "mdate": 1762927476177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}