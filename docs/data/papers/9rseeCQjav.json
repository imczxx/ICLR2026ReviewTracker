{"id": "9rseeCQjav", "number": 22940, "cdate": 1758337303267, "mdate": 1759896839394, "content": {"title": "Generalization Analysis of SGD in Linear Regression under Covariate Shift: A View from Preconditioning", "abstract": "Recent years have witnessed the widespread success of stochastic gradient descent (SGD)-type algorithms across various problem domains, including those involving covariate shift tasks. However, the underlying mechanisms that enable SGD to generalize effectively in covariate shift settings, as well as the specific types of covariate shift problems where SGD demonstrates provable efficiency, remain insufficiently understood. This paper investigates SGD in the context of linear regression under a canonical covariate shift problem. Our analysis is two-fold: First, we derive an upper bound for the target excess risk of SGD, incorporating two critical practical techniques—momentum acceleration and step decay scheduling. Second, we analyze SGD's performance by framing it as a preconditioned estimator, enabling us to identify conditions under which SGD achieves statistical optimality. We demonstrate that SGD attains optimal performance in several commonly studied settings. Additionally, we demonstrate that there exist separations between several commonly used methods.", "tldr": "", "keywords": ["Covariate Shift", "Linear Regression", "Minimax Optimality", "SGD with Momentum"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6dc3273b352fa0c6b010849dc09f2b310ef3fe51.pdf", "supplementary_material": "/attachment/c9eba3f937f246fcd81a8cbe8a8a9129a693ce0d.pdf"}, "replies": [{"content": {"summary": {"value": "The work of #22940 studies why (A)SGD can still perform well when there exists covariate shifts, i.e. a mismatch between the training data and the test data. Specifically, the authors focus on linear regression problems $\\langle\\mathbf{w}, \\mathbf{x}\\rangle$, where the input data changes between training and testing, but the way outputs depend on inputs stays the same. The paper analyzes SGD with common techniques like momentum and step-size decay and shows that, under certain conditions it can achieve near-optimal performance, meaning it generalizes well to new data. One new insight is viewing SGD as a type of *preconditioned* estimator, which helps explain when and why it works. The authors also compare SGD with other methods like ridge regression and find that SGD can actually be better in many situations (the so-called separations)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper offers a novel perspective on accelerated stochastic gradient descent under covariate shift, particularly when both momentum and geometrically decaying step sizes are used. To the best of my knowledge, this fulfills a technical gap in the series of work.\n\n- While the content is dense (as expected), the writing and overall structure is clear and easy to read. The paper not just providing upper bounds, it also includes minimax optimality conditions and separation results that position ASGD as superior to both ridge regression and vanilla SGD."}, "weaknesses": {"value": "- The main result Theorem 6 (central risk bound) is proved under $M S=S M$. The authors call this a mild requirement, yet in practice $M$ encodes target task geometry (e.g., evaluation metric or interpolation space) and rarely commutes with the source covariance $S$, especially under real covariate shift where rotations between $S$ and $T$ are the rule, not the exception. This essentially narrows the scope of the main guarantee, while the instance-wise bound which mentioned for the general case is delayed to the appendix and not made first-class\n\n\n\n-  Algorithm 1 cycles over $\\left\\lfloor\\log _2 n\\right\\rfloor$ stages, resets step-sizes $\\delta^{(\\ell)}, \\gamma^{(\\ell)} \\leftarrow\\left(\\delta_0, \\gamma_0\\right) / 4^{\\ell-1}$, and assumes a fresh sample each inner iteration $\\left(x_i, y_i\\right)$. For a fixed dataset, this is equivalent to sampling with replacement, but the proof machinery leans on independence across the *semi-stochastic* recursion\n\n\n\n\n\n- The UTKFace covariate shift that subsampling by age is purely synthetic and thus weaken the empirical evidence of the claims. There is no check that $P(y \\mid x)$ is invariant in the feature space. Meanwhile the work omits other standard IW-ERM or kernel methods known to be competitive under covariate shift"}, "questions": {"value": "- Is your theorem 8 necessary? Based on the discussions in sec. 5, it seems that the (11) is only a sufficient condition for optimality of ASGD. Could authors consider either show necessity (or near‑necessity) under your modeling assumptions, or exhibit a broad class where failure of (11) provably prevent the ASGD optimality?\n\n- I am wondering that whether authors can report the actual schedule that used to produce all figures. They said ‘grid search’ but didn't give ranges, seeds or selection protocol like selected $\\alpha, \\beta, \\delta, \\gamma$ across runs. Otherwise, the ASGD > ridge claim is not really convincing.\n\n\n\nmiscellaneous:\n\n- first page, 'algorithms’s'\n\n- L116: 'establishes upper bounds in linear for vanilla SGD', is that 'linear regression'?\n\n- one need to unify the notations (e.g. $\\otimes$ everywhere for Kronecker; reserve $\\odot$ for Hadamard) in appendix, like (28)\n\n-  consider explicitly stating the $\\log _2 n$ to $\\ln n$ conversion when you introduce $K=n / \\log _2 n$"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VibAjihRm3", "forum": "9rseeCQjav", "replyto": "9rseeCQjav", "signatures": ["ICLR.cc/2026/Conference/Submission22940/Reviewer_Uptf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22940/Reviewer_Uptf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789154228, "cdate": 1761789154228, "tmdate": 1762942446348, "mdate": 1762942446348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies learning under covariate shift in the setting of linear regression. The paper compares SGD and ASGD, and argues that SSGD achieves optimality for a broader class of instances."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The assumptions and theorems are followed with comprehensive discussions.\n2. The theory is well supported by proofs."}, "weaknesses": {"value": "1. The flow of the paper may cause difficulties in understanding. Specifically,\n- The introduction of ASGD is a bit absurd. One may wonder why we consider ASGD is the main focus instead of vanilla SGD. See Question 1. I would recommend introducing properties of SGD, e.g.,  Corollary 12, before introducing ASGD.\n- The definition of \"optimality\" does not seem clear to me in Theorem 8.\n2. It looks like the abstract and the main body convey quite different messages. The abstract is mainly about why SGD performs well even in the case of covariate shift (which, according to my understanding, remains unanswered), but the main body seems to focus on the advantage of ASGD compared with SGD in the case of covariate shift.\n3. The comparison between ASGD and SGD looks vague and of inferior importance to me.\n- The argument of ASGD achieves optimality for a broader class of problem instances is not clearly illustrated. Specifically, Corollary 12 does not seem to provide a clear comparison between SGD and ASGD. It would be good if the authors could guide me to a similar theorem for ASGD.\n- As mentioned in Weakness 2, the main question, why SGD performs well in covariate shift, does not have a clear answer to me. Compared with this main question, the role of momentum in SGD-like methods may not be as important."}, "questions": {"value": "1. How to interpret Eq. (7)? Is this condition used anywhere else in the paper?\n2. Ferbach et al. (2025) studied different ASGD variants. Will the results of this paper be any different?\n\nFerbach et al. Dimension-adapted Momentum Outscales SGD. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i2gnr22V3c", "forum": "9rseeCQjav", "replyto": "9rseeCQjav", "signatures": ["ICLR.cc/2026/Conference/Submission22940/Reviewer_VMen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22940/Reviewer_VMen"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800336429, "cdate": 1761800336429, "tmdate": 1762942446064, "mdate": 1762942446064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical analysis of SGD for linear regression in the presence of covariate shift. It is a common problem in ML where the distribution of input data used for training a model differs from the distribution of data the model will encounter when deployed, while the underlying relationship between inputs and outputs remains the same. The authors deal with the question why SGD-type algorithms, which are typically trained only on the source data, generalize so well to the target data without explicit knowledge of its distribution.\n\nThey derive a tight upper bound on the prediction error for ASGD, incorporating practical techniques as momentum and exponentially decaying step sizes. Introduce a novel perspective by viewing ASGD as a preconditioned estimator. The analysis demonstrates scenarios when ASGD can be superior to other methods as standard ridge regression."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Analyzing SGD as a preconditioned estimator is innovative. Addressing a practical problem in ML and providing theory. Theorem 6 connects the algorithm performance with the spectral properties of the source and target data distributions. Theory covers accelerated methods, establishes minimax optimality."}, "weaknesses": {"value": "The focus is strictly on linear regression though the motivation comes from the success of SGD in complex domains like large language models. The theory does not directly apply to the non-linear, non-convex optimization problems that prevail in modern ML. This is why I think the practical impact on the broader ML community is questionable.\n\nThe manuscript is dense with complex notation and very technical. It is difficult to parse for anyone not already an expert in the field. Intuitive explanations could guide the reader through the technical arguments.\n\nThe central conclusion is that SGD generalizes well under covariate shift when the target task can be solved using the information in the principal directions of the source data. So, the paper shows that transfer learning works best when the source and target tasks share relevant features, which is not surprising or fundamentally new.\n\nWhile this is strong theoretical work, it may be better suited for a more specialized venue in statistics or theoretical machine learning."}, "questions": {"value": "The assumption in Theorem 6 is that constraint matrix M and the source covariance S commute. Why do you believe it is mild? Why it is representative of real-world problems? Isn't it just a mathematically convenient condition, that simplifies the spectral analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OgbTtzT2oe", "forum": "9rseeCQjav", "replyto": "9rseeCQjav", "signatures": ["ICLR.cc/2026/Conference/Submission22940/Reviewer_Ybga"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22940/Reviewer_Ybga"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869463434, "cdate": 1761869463434, "tmdate": 1762942445781, "mdate": 1762942445781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides an upper bound on the target excess risk for estimators obtained via (Accelerated) Stochastic Gradient Descent in the context of linear regression. Specifically, for ASGD, the authors derive the necessary conditions on the step sizes and moments under which the algorithm achieves the minimax optimal rate of $\\tilde{O}(1/n)$, where the second moment of the covariates ($S$), the second moment of the target variable ($T$), and the elliptical constraint on the true parameter ($M$) are all given. The suboptimal target excess rate for the ridge regression $O(1/\\sqrt{n}) is also derived. The experimental results shown are consistent with the derived rate."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The authors provide a rigorous derivation of the theoretical upper bound on the target excess risk for the given estimators in linear regression problem.\n\n- The presented theoretical framework is convenient in the sense that it can deduce and compare the risk of estimating methods for the linear regression problem such as ridge regression or SGD."}, "weaknesses": {"value": "- The description on the experimental setup regarding Figure 1 seems to be insufficient. Please refer to the Questions section. Also, it is difficult to understand which experimental setting each solid line and dashed line represents.\n\n- The presented analysis on the target excess risk is inherently limited to the linear regression model. In the covariate shift problem occurring in various fields, such as LLM, the relationship between $x$ and $y$ is typically far more complex than a linear model.\n\nTypo in line 83 - 84, ASGD achieves the optimal $\\tilde{O}(1/\\sqrt{n})$ rate... : The rate for ASGD and ridge regression are swapped."}, "questions": {"value": "- Why is the UTK-face dataset used to show the target excess risk with respect to $n$?. What is the covariate $x$ with $d=10$? How is S, T, and M chosen? How is the excess risk and the true parameter $w^*$ computed? Also, please elaborate which experimental setting each solid line and dashed line in Figure (a)-(d) represents.\n\n- Can you deduce a similar analysis on the sparse regression problem where it is known that the number of the nonzero entries of $w^*$ is far less than $d$? Note that such problem is usually solved with LASSO or ElasticNet."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Wb3c7k5vMA", "forum": "9rseeCQjav", "replyto": "9rseeCQjav", "signatures": ["ICLR.cc/2026/Conference/Submission22940/Reviewer_JPLS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22940/Reviewer_JPLS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934813984, "cdate": 1761934813984, "tmdate": 1762942445425, "mdate": 1762942445425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}