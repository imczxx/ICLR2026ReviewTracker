{"id": "dgH6f9V8HJ", "number": 949, "cdate": 1756824907189, "mdate": 1759898233921, "content": {"title": "Golyadkin Confronts Perceptual Distance and Curvatures:  Coping with   Ambiguity", "abstract": "The adversarial vulnerability of classifiers reveals a core divergence: ML systems make distinctions without difference; biological systems tolerate difference without distinction-- and survive because of it. \n\n Adversarial vulnerability is analyzed through decision boundaries and distance-based perturbation models. However, the distances used do not match true perceptual distances and the overall approach fails to account for the misalignments with perceptual topology and geometry. We discuss contexts in which the perceptual distance is computable. In particular, we discuss image recognition  contexts in which the perceptual distance between any two inputs is finite. The finiteness underpins an inherent, and informally accepted by the ML community, vulnerability of classifiers defined on such images, rendering all labels susceptible to adversarial attacks. \n\nThis demonstrates why some valiant attempts to achieve robustness may be doomed. And yet, biological systems function and thrive despite or may be even because of the ever-present ambiguity. Systems function not because they are robust but because they are sufficiently conceptually coherent. The notions of coherency, conceptual coherency, coherency failure rate, and the conceptual margin of a labeled data set are defined and discussed in this paper.\n\nWe define latent adversarial vulnerability, showing that vulnerability arises not only from adversarial perturbations but also through conceptual drift along perceptual Sorites, and introduce perceptual curvature which can be used to identify latent adversarial vulnerability regions.", "tldr": "Robustness may not be possible and may not be needed -- coherence is sufficient.", "keywords": ["Doppelgangers", "perceptual distance", "perceptually indistinguishable", "robustness", "conceptual accuracy", "testing accuracy", "conceptual coherence", "perceptual curvature"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5e5ff9b0d9fc8788192c9c3f96e81cdeea96cade.pdf", "supplementary_material": "/attachment/34d0cd5c79dd78836436ea731069bc74e9d502df.zip"}, "replies": [{"content": {"summary": {"value": "The paper reframes adversarial robustness through perceptual topology: it models perception via an indiscriminability graph, defines an extended perceptual distance $d_\\infty$ (finite within a “perceptual tile,” infinite across tiles), and argues that robustness often fails because ML decision regions cut across these tiles. It proposes replacing strict robustness with conceptual coherence, introducing metrics such as coherency failure rate and conceptual margin; when indiscriminability aligns with encounter probability and the margin is large enough, one can train classifiers that are perfectly accurate and coherent on the labeled set. The authors identify latent adversarial vulnerabilities that emerge through perceptual Sorites chains—even near points made safe by adversarial training—and introduce perceptual (Ricci) curvature to flag regions where such vulnerabilities lurk, illustrating ideas with grayscale/MNIST examples."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Introduces a perceptual-topological lens on robustness (indiscriminability tiles, $d_\\infty$, conceptual margin, coherence, perceptual curvature), offering a unifying vocabulary for why small perturbations can fool classifiers.\n* Provides principled definitions and diagnostics that connect human indistinguishability to classifier behavior, pointing to testable conditions (e.g., margin thresholds) and dataset design levers.\n* Motivations are intuitive (e.g., Sorites-style chains) and the conceptual framework is consistently developed, facilitating cross-talk with psychophysics and geometry.\n* Reorients goals from absolute $\\ell_p$-robustness to intra-tile conceptual coherence, with potential impact on benchmark construction, risk localization (via curvature), and future defense strategies."}, "weaknesses": {"value": "*  No concrete algorithms or complexity analysis for estimating $d_\\infty$, perceptual curvature, or conceptual margin from finite data.\n*  Lacks systematic experiments (e.g., CIFAR/ImageNet) demonstrating diagnostics, risk localization, or gains over standard robust training.\n*  Several key claims appear as observations without full theorems, assumptions, or proofs; boundary conditions and counterexamples are unclear."}, "questions": {"value": "Can you report systematic results on CIFAR-10/100 (or an ImageNet subset)? Include: (1) correlations between your diagnostics (coherence failure, curvature, margin) and attack success/robust accuracy;  (2) comparisons and ablations vs. adversarial training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7icSR3i2lU", "forum": "dgH6f9V8HJ", "replyto": "dgH6f9V8HJ", "signatures": ["ICLR.cc/2026/Conference/Submission949/Reviewer_ZYBz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission949/Reviewer_ZYBz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842480474, "cdate": 1761842480474, "tmdate": 1762915647565, "mdate": 1762915647565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper raises a fair concern in the definition of adversarial examples, that is the pixel space is different from the perceptual space, and we need to develop methods and theoretical framework to measure distance in perceptual space instead of the pixel space to address this concern. The paper proposes the following contribution: (1) propose definition of \"perceptual tile\", \"phenomenal neighborhood\" based on Weber and Fechner indiscriminable measurement, (2) propose definition for delta-epsilon coherence measurement and coherence failure rate, (3) propose concept of conceptual margin and derive a bound to relate conceptual margin to the size of adversarial vulnerability. The paper mainly propose definitions, following with simple examples. No algorithm or empirical results are included."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper attempts to address a fair concern in the field of adversarial robustness, which is the adversarial robust needs to be measured on the perceptual space and not the perturbation in the pixel space. To address this concern, the authors propose a new measurement for robustness, which is \"delta-epsilon coherence\". The strength of the paper are:\n1. The authors attempt to address a well-defined concern, which is methods to measure perceptual distance, and not pixel distance, and define the coherence of a classifier based on this measurement.\n2. To my limited knowledge, the proposed concepts and definitions are novel in the adversarial robust community. And the proposed definition is precise and well-defined.\n3. The authors demonstrate theoretically the importance of the \"conceptual margin\" in the dataset to define a theoretical bound on adversarial coherent (Eq. 17)."}, "weaknesses": {"value": "While the paper attempts to address a well-defined concern and proposed sound definition to measure coherence, the paper results are mainly based on illustrative examples and lack the significant results and comparison with current measurements and techniques to support their claims. While \"delta-epsilon coherence\" is a promising measurements to evaluate the classifier robustness. I would like to see either:\n1. More detailed theoretical comparison of \"delta-epsilon coherence\" with current adversarial robustness measurement.\n2. Empirical results on the application of this measurement on real dataset, and how they differ with the current adversarial robust measurements. L470 mentioned  \"An actionable program to compute conceptual margins is presented in the supplementary materials.\", but I didn't find any supplementary materials.\n\nThe paper also lacks the \"Related Works\" section that should include any previous works that try to attempt this problem.\n\nAlso, the paper's organization is quite difficult to follow (please see more details in the \"Suggestion\" below)\n\nSuggestion:\n1. The introduction section of the paper is a little bit confusing to see what is the paper outline and main contribution. To make it easier for readers to follow. I suggest:\n\t1. include an outline in the Introduction (for example, \"the paper is structured as follow, in section 1, ..., in section 2, ..., etc...)\n\t2. Clearly list the contribution of the paper as a list (for example, \"Here's our 3 main contribution\")\n2. In Section 2, the author list many different definitions and it's difficult to follow the relevance between them. I suggest to include a sentence that introduce all the definitions and how they relevant to each other (e.g, \"In this section, we will introduce the following definitions:...\")"}, "questions": {"value": "Question:\n1. (Fig 2) Is `Fig.2` an original contribution of this neural circuit? Or it has appeared in other computational neuroscience papers?\n2. (Section 3, L197) How does the network in `Fig.2` estimate the perceptual distance? I thought the Figure caption said that it would return a binary output for the `phenomenal neighborhood` ? How do we compute $d_{\\infty}$ in general?\n3. (Section 3, L299) Do we have a proof for this statement? \"A direct computation shows that every classifier R(γ) is (erf(wγ) − erf(γ/w)) coherent\"\n4. (Section 4, L470) \"An actionable program to compute conceptual margins is presented in the supplementary materials.\" -> Do we have the supplementary materials? I didn't see it in the main paper.\n5. I thought the paper \"Carandini and Heeger (2012)\" is written about \"normalization is a canonical neural computation\", but the author cited that paper to claim \"computing perceptual distance is a canonical neural computation\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "faeZJaxAf1", "forum": "dgH6f9V8HJ", "replyto": "dgH6f9V8HJ", "signatures": ["ICLR.cc/2026/Conference/Submission949/Reviewer_zoRc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission949/Reviewer_zoRc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926495176, "cdate": 1761926495176, "tmdate": 1762915647373, "mdate": 1762915647373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Disclaimer: I am a computational neuroscientist by training, and have little to no knowledge of topology. I am thus not qualified to comment on the technical soundness of the arguments presented in this paper. \n\nThis submission explores properties of achievable classifiers that are defined on a particular type of topological space defined by the union of a set of \"perceptual tiles.\"\nUnder one particular extended distance metric (the degree of separation on a graph where two points have an edge iff they are perceptually indistinguishable) points on a given particular perceptual tile have a finite distance between them whereas points on distinct perceptual tiles are infinitely far apart.\nThe author's provide an argument that some well studied stimulus spaces can be understood as having instantiations of this topology when using notions of indiscriminability from classic literature on perception. \nA key result is that one consequence of this topology is that, if there are samples with different labels on a single perceptual tile, there will necessarily exist adversarial examples for any classifier. \nThus the author's argue that a more pragmatic goal to pursue is \"pragmatism\" or classifiers that accept the existence of adversarial examples for some set of rare instances rather than perfect robustness.\nThe paper goes on to describe settings where inducing defenses defenses to a particular vulnerability will always induce vulnerabilities elsewhere, and links this phenomenon (\"latent vulnerabilities\") and defines a curvature metric that can identify such vulnerabilities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Theoretical explorations of what computational properties are achievable for classifiers are valuable.\n  - I.e. this paper proposes \"shifting the goalpost\" in the domain of robust learning from robustness everywhere to robustness with high probability almost everywhere. Which could be a useful paradigm shift. \n- The documentation of a setting where inducing robustness at some data points (i.e. training points) will necessarily introduce vulnerabilities elsewhere is potentially very interesting (especially if this effect could be documented empirically)."}, "weaknesses": {"value": "- Some claims are stated without sufficient evidence: \n  - Lines 40-41: \"Until recently it was accepted that true perceptual distance cannot be easily defined and computed\" I am not familiar with any such claim in the literature and would appreciate a citation. More generally this bit of exposition misses much of a large body of literature on perceptual metric learning. See for example LPIPS, MS-SSIM, DISTS papers. \n  - Lines 111-115: \"Still, humans, and likely other organisms, employ context-relevant perceptual distance d∞ to quantify the perceptual similarity between inputs\" What evidence is there that this distance metric is particularly relevant for human perception? I did not see any quantitative comparison to human behavioral data in the cited paper (Kamberov, 2024)\n  - Lines 152-154: What evidence is there that distance computation is a canonical computation in real neural circuits? This claim seems critically dependent on the one from 111-115, which I already feel is under supported. This is related to the claim on lines 199-200: \"Perceptual discrimination and the estimation of small perceptual distances are canonical neural computations,\" what examples from the neuroscience literature support this claim?\n\n- Some aspects of the presentation felt jarring to me. For example on line 99 the \"Williamson indistinguishability relation\" is introduced, but I did not know what this was and there was no citation provided. Similarly the introduction to Weber's law and the extension of Fechner felt insufficient. \n\n\n- Nit: Line 228 \"Assuming that Weber–Fechner’s observations hold: namely, that perceptual discrimination thresholds scale logarithmically with stimulus intensity\". I think this is a mis-statement of Weber's law: discrimination thresholds scale linearly with intensity, and when this relation is integrated you can reach the conclusion that the perception of stimulus strength scales logarithmically with intensity."}, "questions": {"value": "- It is unclear how to determine a priori if a classification problem is \"coherent\" in the language of this paper: i.e. is there a way in general to determine if there are examples of distinct classes within a single perceptual tile given access to a finite dataset?\n\n- Is it possible to identify real systems for which latent vulnerabilities exist? In my opinion the strength of the contribution would be dramatically increased if (1) it could be shown in a practical system that inducing robustness around point A leads to decreased robustness around point B, and (2) that the perceptual Ricci curvature could be used to identify the risk of this type of vulnerability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "hEsJPIHpIG", "forum": "dgH6f9V8HJ", "replyto": "dgH6f9V8HJ", "signatures": ["ICLR.cc/2026/Conference/Submission949/Reviewer_kokB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission949/Reviewer_kokB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934744176, "cdate": 1761934744176, "tmdate": 1762915647250, "mdate": 1762915647250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper claims to formalize the coherence in classification. The paper first provides the background notions, including the indiscriminability relation, perceptual distance, and conceptual accuracy. The paper provides a single example that measuring perceptual distance is feasible. The paper also attempts to argue that adversarially robust classifiers cannot be achieved due to the lack of unambiguous concepts in classification problems. Rather than pursuing the adversarial robust classifier, the paper claims that we can instead achieve a low coherency failure rate in practice. Then, the paper shifts the focus to perceptual generalization and latent adversarial vulnerability. The paper explains the latent adversarial vulnerability with the perceptual drift, then defines the perceptual curvature that can be used to identify the perceptual drift."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "I don’t see a clear strength in this paper. At least, the paper is trying to do something new."}, "weaknesses": {"value": "1. The paper’s motivation is unclear. The paper provides several definitions, the realization of which is not even guaranteed. Not only do people lack the ability to quantify the proposed definitions, but it is also unclear whether computing (or approximating) the quantities is tractable. If all the concepts cannot be implemented in practice, I don’t see a reason why the machine learning community should care about those notions at all.\n2. The paper writing is generally bad. The topic of discussion changes without enough context, making it difficult for the reader to follow what the paper is trying to say.\n3. The paper’s contents are largely speculative, lacking sufficient support or evidence. The paper provides at most one or two examples of the proposed concepts, which is clearly not enough to demonstrate anything.\n4. Even the reference paper on which this paper is grounded has the same problem. The reference paper presents only one example of an “adversarial doppelganger” whose background is extremely dark (which made it harder to recognize whatever perturbation in that region), and there is no experimental support that this unrecognizable perturbation is common. The reference paper is also speculative, as it contains only speculations that cannot be experimentally verified.\n5. If I understand correctly, the indiscriminability relation depends on a “specific subject”. (The reference paper at least mentions this, but this paper does not clearly say this.) Therefore, all the derived concepts, such as perceptual topology, degrees of separation, and perceptually coherent class partition, depend on the subject and are not well-defined. Measuring a classifier’s conceptual accuracy by measuring the mismatch does not make sense because there is no single notion of perceptual coherence, and it depends on the subject that is used to define the perceptual coherence. The author should clarify how a unified measure of conceptual accuracy can be established across different subjects.\n6. The network presented in Example 0 is specific to a single example, and there is no guarantee that it generalizes to real-world examples. A more concrete example that reflects a real-world setting should be provided.\n7. Example 2 is not proper because the class label was not classified by a model trained to classify such an image. While CoPilot may have an image classification model that aided its answer, we have no information about the underlying model, and it is possible that CoPilot utilized different models for each query.\n8. Example 3 just contains a specific choice that fits the proposed conceptual coherence. However, there is no evidence that the perceptual neighborhood is generally measurable, so the proposed coherency failure rate could be useless in practice.\n9. The Example of the latent adversarial vulnerability phenomenon is also hypothetical, and there is no evidence that the attacker can intentionally induce such a phenomenon. In other words, there is no clear evidence that the attacker can realize the proposed Ouroboros attacks. If it is a realizable concept, then the paper should provide a prototype implementation.\n10. The proposed concept of perceptual curvature has no practical value. In Section 4, the paper says, “However, to date, we do not know how to ‘code’ these computations.” Why should the adversarial machine learning community care about some concept whose realization is unclear?"}, "questions": {"value": "1. Choose a clearer title that directly describes what this paper is about.\n2. If I understand correctly, many proposed notions should depend on the subject who is perceiving the context. If my understanding is incorrect, please justify why the perception is not dependent on the subject. If the perception depends on the subject, specify the perceptual context to which the paper refers.\n3. Provide empirical evidence that the proposed concepts are common in the machine learning context. I’m not asking for one or two examples that fit the proposed definition, or literature that does not consider machine learning practice. If there is no such evidence, justify the reason why the machine learning community should care.\n4. Design and perform more systematic experiments. As pointed out in the Weaknesses, querying CoPilot to classify three images is not a well-designed experiment.\n5. At least, I agree on one statement. “Rather than searching for high-accuracy robust classifiers, …, it is meaningful to identify classifiers that achieve high accuracy while maintaining adversarial vulnerability below the maximal tolerable threshold.” In my opinion, the adversarial machine learning community implicitly knows this, but we just pursue the former because it is more cost-effective. Attempting to demonstrate that the latter would yield a better result is a commendable effort; however, I disagree that this paper effectively makes the point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 0}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CpvCqnP9re", "forum": "dgH6f9V8HJ", "replyto": "dgH6f9V8HJ", "signatures": ["ICLR.cc/2026/Conference/Submission949/Reviewer_o38w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission949/Reviewer_o38w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146638726, "cdate": 1762146638726, "tmdate": 1762915647127, "mdate": 1762915647127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}