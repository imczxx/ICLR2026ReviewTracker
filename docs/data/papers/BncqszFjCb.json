{"id": "BncqszFjCb", "number": 7824, "cdate": 1758037773847, "mdate": 1759897829833, "content": {"title": "Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting", "abstract": "We present Quantile Rendering (Q-Render), an efficient rendering algorithm for 3D Gaussians that involve high-dimensional feature vectors. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples 3D Gaussians that have dominant influence along ray. The dominance is determined by transmittance change analysis, and the only sampled Gaussians will participate in subsequent rendering steps such as alpha blending. In our framework, Q-Render is integrated with a conventional 3D neural network that operates on 3D Gaussians, named Gaussian Splatting Network (GS-Net), to predict Gaussian features. We evaluate GS-Net on open-vocabulary 3D semantic segmentation using two benchmarks: (1) ScanNet and (2) LeRF-OVS, where 3D Gaussians are enriched with CLIP-based language features. Extensive experiments show that Q-Render under GS-Net consistently outperforms existing methods, achieving superior results and highlighting its potential as an effective bridge between 2D foundation models and 3D Gaussian representations. Furthermore, our Q-render achieves ~43.7× speed gains against previous methods when rendering 512-D feature maps.", "tldr": "Efficient rendering algorithm for high-dimensional feature rendering from 3D Gaussians.", "keywords": ["Gaussian Splatting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0135e69c43bc010dcab221dbaa47d6994b91e295.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Quantile Rendering (Q-Render), an efficient feature rendering algorithm for 3D Gaussian Splatting. Q-Render sparsely samples quantile Gaussians that dominate transmittance along each ray, cutting complexity from O(NC) to O(N+KC). Integrated into a Gaussian Splatting Network (GS-Net), it enables scalable training with high-dimensional CLIP features for open-vocabulary 3D segmentation. Experiments on ScanNet and LeRF-OVS show state-of-the-art accuracy and up to 43.7× faster rendering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Efficient Rendering Design: Q-Render introduces a principled quantile-based sampling strategy that substantially reduces computation for high-dimensional feature rendering without sacrificing accuracy.\n2. Solid Integration & Generality: The method integrates seamlessly into 3D Gaussian Splatting pipelines and generalizes well across neural backbones, bridging 2D foundation models and 3D representations effectively.\n3. The paper is well written and clearly organized."}, "weaknesses": {"value": "1. In figure 6, I think the performance improvement of Q-Render is minor and I think the motivation of the authors apply the quantile rendering is not very strong.\n2. The feed-forward pipeline for 3d scene understanding has already been applied in early methods, like SIU3R (SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment) and SegMASt3R (SegMASt3R: Geometry Grounded Segment Matching).\n3. No demo has been submitted; therefore, the performance of the proposed method cannot be effectively demonstrated."}, "questions": {"value": "1. What are the number of input views?\n2. What is the training time / GPU memory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p9BYgclEXG", "forum": "BncqszFjCb", "replyto": "BncqszFjCb", "signatures": ["ICLR.cc/2026/Conference/Submission7824/Reviewer_hrzW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7824/Reviewer_hrzW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7824/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760959213735, "cdate": 1760959213735, "tmdate": 1762919867682, "mdate": 1762919867682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Quantile Rendering (Q-Render), a novel and efficient rendering algorithm for high-dimensional features in 3D Gaussian Splatting (3D-GS). Traditional 3D-GS-based rendering densely accumulates all Gaussians intersecting each ray, which becomes computationally prohibitive when rendering high-dimensional embeddings such as 512-D CLIP features. Q-Render addresses this by sparsely sampling “quantile Gaussians”, those that contribute most significantly to the transmittance change along the ray."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors evaluate on two large-scale benchmarks with extensive ablations, qualitative visualization, and speed analyses.\n\n2. The approach achieves >40× rendering speedup for 512-D features while improving mIoU—impressive for real-world scalability.\n\n3. Bridges 2D foundation models (CLIP, SAM) with 3D Gaussian representations—a timely and valuable direction for the ICLR community."}, "weaknesses": {"value": "1. The quantile sampling justification is intuitive but lacks a quantitative analysis of approximation error relative to volume rendering.\n\n2. Although ablations are provided, an adaptive or learned K would make the method more robust and generalizable.\n\n3. The paper primarily focuses on indoor datasets (ScanNet, LeRF-OVS). Outdoor or multi-view generalization tests would strengthen the claim of scalability.\n\n4. Only MinkUNet and PTv3 are explored. An analysis of architecture-agnostic behavior would enhance generality."}, "questions": {"value": "1. Could the authors provide a theoretical bound or empirical analysis quantifying the approximation error between Q-Render and full volume rendering?\n\n2. Is it possible to make K adaptive based on the transmittance variance along each ray?\n\n3. How sensitive is the approach to the noise in Gaussian opacity or density estimation?\n\n4. Can Q-Render be applied to RGB image rendering or only to feature maps?\n\n5. What is the memory footprint of Q-Render compared to top-K or compressed-feature methods?\n\n6. Have the authors tested Q-Render for dynamic scenes or time-varying Gaussians?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lzblX362aS", "forum": "BncqszFjCb", "replyto": "BncqszFjCb", "signatures": ["ICLR.cc/2026/Conference/Submission7824/Reviewer_wYC5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7824/Reviewer_wYC5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7824/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652742232, "cdate": 1761652742232, "tmdate": 1762919867271, "mdate": 1762919867271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed quantile rendering to accelerate the rasterization of 3D Gaussians. Specifically, the proposed method selects a subset of critical 3D Gaussians that have a significant influence on the final rendering results and skips the rest. The experiments on 3D open vocabulary segmentation show that the proposed method can speed up the rendering while achieving state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The idea is interesting and well-motivated.\nThe performances shown in the experiments are good."}, "weaknesses": {"value": "1. Paper presentation: In the abstract, the GS-Net and open-vocabulary 3D semantic segmentation are mentioned, without introducing their relationship with the Q-Render, making it hard to follow. The topic of the paper is unclear. If the proposed method is designed for general high-dimensional feature rendering, why is it only evaluated on the 3D open-vocabulary semantic segmentation task? If the method is specifically designed for 3D open-vocabulary semantic segmentation, then there is a lack of introduction to the specific problem.\n2. Unreliable experiments: The final method used for 3D open-vocabulary semantic segmentation consists of two parts: the per-Gaussian feature extracted by GS-Net and the Q-Rendering procedure. It is hard to justify the separate contribution of the two parts to the performance gain in Tab. 3. To my understanding, as Q-Render only requires Gaussians with high-dimensional features as input, it is not hard to disentangle the two parts: (1) Replace the rendering algorithm of the baselines using Q-Render and compare their performance. (2) Replace the feature extraction model of the baselines using the GS-Net and compare their performance.\n3. Minor typos: in L.199, the reference line 6 of Algorithm 1 should be enclosed in parentheses."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qUw8XmoRXI", "forum": "BncqszFjCb", "replyto": "BncqszFjCb", "signatures": ["ICLR.cc/2026/Conference/Submission7824/Reviewer_J4U2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7824/Reviewer_J4U2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7824/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976590236, "cdate": 1761976590236, "tmdate": 1762919866892, "mdate": 1762919866892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Q-Render, an efficient rendering algorithm for high-dimensional feature rendering in 3D Gaussian Splatting. Q-Render sparsely samples dominant Gaussians through transmittance change analysis. The authors integrate Q-Render with a 3D neural network (GS-Net) and evaluate it on two open-vocabulary 3D semantic segmentation benchmarks: ScanNet and LeRF-OVS, demonstrating superior performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a quantile sampling strategy that identifies critical Gaussians through transmittance analysis, which is theoretically motivated and practically effective.\n\n2.  The method demonstrates superior performance and efficiency on both ScanNet and LeRF-OVS open-vocabulary 3D semantic segmentation benchmarks, achieving ~43.7× speed gains when rendering 512-D feature maps."}, "weaknesses": {"value": "1. The paper lacks theoretical analysis of Q-Render's approximation error to volume rendering or convergence guarantees. The mathematical justification for the normalization operation (line 20 in Algorithm 1) is insufficient.\n\n2. While K significantly impacts performance, the paper lacks an adaptive strategy for selecting K. Why is uniform partitioning (k+1)/(K+1) chosen? Have adaptive thresholds been considered?\n\n3. The paper does not sufficiently analyze when Q-Render might fail or how it performs on scenes with non-uniform transmittance distributions.\n\n4. In Table 2, the authors \"reproduce\" baseline results but use different training and evaluation setups, which may not provide a fair comparison.\n\n5.  The necessity of de-voxelization is not independently validated. The paper lacks failure case analysis showing scenarios where Q-Render underperforms."}, "questions": {"value": "1. Can you provide an error bound for Q-Render's approximation to volume rendering? What are the theoretical guarantees for the normalization step?\n\n2.  Is the optimal value of K related to scene complexity and feature dimensionality? Can you design an adaptive strategy for K selection?\n\n3. Performance anomaly: Why does Q-Render with K=40 achieve higher mIoU (50.85) than volume rendering (49.02) in Table 6? Does this suggest that sparse sampling has a regularization effect?\n\n4.  How can the information loss from voxelization (Figure 7) be quantified? Are there better voxelization strategies to mitigate this loss?\n\n5. Threshold design: Is uniform partitioning of transmittance thresholds optimal? Have you considered scene-adaptive thresholds based on local transmittance distributions?\n\n6.  How is de-voxelization specifically implemented? Is K fixed during both training and inference? How do you handle regions with very small transmittance changes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0pVKN31DMp", "forum": "BncqszFjCb", "replyto": "BncqszFjCb", "signatures": ["ICLR.cc/2026/Conference/Submission7824/Reviewer_Fhos"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7824/Reviewer_Fhos"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7824/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002124459, "cdate": 1762002124459, "tmdate": 1762919866439, "mdate": 1762919866439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}