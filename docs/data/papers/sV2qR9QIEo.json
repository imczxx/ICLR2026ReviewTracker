{"id": "sV2qR9QIEo", "number": 4650, "cdate": 1757734751396, "mdate": 1759898021645, "content": {"title": "SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping", "abstract": "Recent studies on Visual Autoregressive (VAR) models have highlighted that high-frequency components, or later steps, in the generation process contribute disproportionately to inference latency. However, the underlying computational redundancy involved in these steps has yet to be thoroughly investigated. In this paper, we conduct an in-depth analysis of the VAR inference process and identify two primary sources of inefficiency: ***step redundancy*** and ***unconditional branch redundancy***. To address step redundancy, we propose an automatic step-skipping strategy that selectively omits unnecessary generation steps to improve efficiency. For unconditional branch redundancy, we observe that the information gap between the conditional and unconditional branches is minimal. Leveraging this insight, we introduce unconditional branch replacement, a technique that bypasses the unconditional branch to reduce computational cost. Notably, we observe that the effectiveness of acceleration strategies varies significantly across different samples. Motivated by this, we propose **SkipVAR**, a sample-adaptive framework that leverages frequency information to dynamically select the most suitable acceleration strategy for each instance. To evaluate the role of high-frequency information, we further introduce multiple high-variation benchmark datasets that evaluate the performance in terms of fine details. Extensive experiments show that SkipVAR achieves over 0.88 average SSIM with up to 1.81$\\times$ overall acceleration and 2.62$\\times$ lossless speedup on the GenEval benchmark.", "tldr": "", "keywords": ["efficiency", "var", "frequency"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf2333700b21d2bc4b2c1e59a7a40be8edb141c4.pdf", "supplementary_material": "/attachment/b28d37045cdc17213ef7d5f913105551e3cf1216.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the high inference latency in Visual Autoregressive (VAR) models by investigating and mitigating computational redundancy. The authors identify that later autoregressive steps provide minimal visual improvement while costing the majority of latency (Steps 11–13 account for 69% of total inference time). They further observe that the unconditional branch in classifier-free guidance models offers diminishing returns in later stages. To solve this, the paper proposes SkipVAR, a novel, sample-adaptive framework that uses a lightweight decision model to dynamically select the best acceleration strategy: skip 0%, 50% or 100% based on its frequency characteristics. SkipVAR combines an automatic step-skipping strategy to omit unnecessary generation steps and unconditional branch replacement to bypass the computationally costly unconditional branch."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors observe that high frequency components does not impact the image quality for certain subsets of images and devise a VAR acceleration decision model that determines the acceleration strategy based on the frequency information.\n2. The authors attain a speedup of 1.81 with an average SSIM of 0.88."}, "weaknesses": {"value": "1. The comparison against existing approaches for VAR acceleration has only been performed on the basis of objective metrics like SSIM. However, a comparison on subjective metrics like CLIP score is necessary for a more holistic comparison.\n2. This approach is only beneficial if the high frequency components do not impact image quality. For example, as the authors point out, this approach will not be beneficial for generating realistic portraits.\n3. While the authors compare against token-based approaches, it will be interesting to see how this method compares to layer skipping methods like [1, 2]\n4. For Table 2, the blue and green row colors are barely visible.\n\n[1] Andrey Gromov et al., The Unreasonable Ineffectiveness of the Deeper Layers, ICLR 2025.\n[2] Anhao Zhao et al., SkipGPT: Each Token is One of a Kind, ICML 2025."}, "questions": {"value": "1. Please show performance and have discussion in comparison to Speculative decoding for VAR , for example: LANTERN [1].\n2. How does SkipVAR perform on GenEval and DrawBench datasets using metrics like ImageReward and CLIP Score?\n3. The authors only test performance wth three granularity levels: 0%, 50% and 100%. Did the authors check if better performance can be obtained using finer granularity or are there any challenges associated with it?\n\n[1] LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding, ICLR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WrfBtgqpEj", "forum": "sV2qR9QIEo", "replyto": "sV2qR9QIEo", "signatures": ["ICLR.cc/2026/Conference/Submission4650/Reviewer_euwc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4650/Reviewer_euwc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958231848, "cdate": 1761958231848, "tmdate": 1762917491066, "mdate": 1762917491066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To my understanding, the authors start from the observation that VAR (Visual Autoregressive) models spend a lot of redundant computation in later sampling steps, and they introduce two light-weight inference-time acceleration tricks: (i) step-skipping, which skips late-stage scaling or denoising steps, and (ii) unconditional branch replacement, which replaces the unconditional CFG branch with the conditional one’s output. To decide when these shortcuts are safe, they train a tiny decision model (mainly a logistic regression) using two handcrafted high-frequency features: high-frequency difference with Sobel operator (HF_Diff) and high-frequency ratio (HF_Ratio) with Fourier transform.  As one of main empirical contributions, their approach applied to Infinity-2B and 8B VAR achieves up to 1.81x speed-up while keeping plausible SSIM, and up to 2.62x acceleration on GenEval benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Sample-adaptive decisions: Unlike prior acceleration methods that use a fixed global ratio or policy (e.g., FastVAR), this work chooses between step-skipping and branch replacement per-sample and per-scale, which makes the approach much more practical. The results on frequency-sensitive vs. frequency-robust subsets clearly demonstrate the benefit of this adaptive decision process.\n\n2. Simple and interpretable features: The combination of HF_Diff (local edge stability) and HF_Ratio (global high-frequency ratio) proves more reliable than using either feature alone, as shown in their tables. It seems that both SSIM/LPIPS and SSIM-HF/LPIPS-HF improve together.\n\n3. Clarity of method overview: Figure 5 presents the overall pipeline--decision step N, downsampled decoding, feature extraction, and policy application--in a clear and easy-to-follow way."}, "weaknesses": {"value": "I'm not an expert in this area, but based on my understanding, I have the following concerns and questions.\n\n1. Sensitivity to decision step and threshold\n\nAccording to paper (with appendix), the default setup uses N = 10 with SSIM thresholds {0.88, 0.86, 0.84}, but the paper doesn’t really explore how performance changes with different step counts (which may vary by model or resolution) or different thresholds. I notice that there’s a brief comparison between SSIM-based and LPIPS-based criteria in the appendix (which says SSIM as more stable), yet a more systematic sweep over N and threshold values would make the analysis much more comprehensive and complete.\n\n2. About branch replecement\n\nReplacing the unconditional branch with the conditional one in CFG effectively collapses into simply $y = y_c$, meaning the efffective CFG scale becomes 1. The authors argue that this is reasonable since the conditional and unconditional branches converge in later steps, but they don't provide concrete measurements of how they close these two quantites are. It also seems unclear whether just reducing the CFG scale to 1 in the later stages would yield the same effect. I think this part is very important since it is the very motivation behind this work.\n\n3. About wall-clock time\n\nCould the authors report the overall wall-clock speed-up, including the decision process and VAE decoding, for DrawBench, HPSv2, and GenEval? It would help clarify how much of the reported acceleration remains when all inference-time components are accounted for."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xf9ZChfpLZ", "forum": "sV2qR9QIEo", "replyto": "sV2qR9QIEo", "signatures": ["ICLR.cc/2026/Conference/Submission4650/Reviewer_HFYL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4650/Reviewer_HFYL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961578154, "cdate": 1761961578154, "tmdate": 1762917490836, "mdate": 1762917490836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript presents SkipVAR, a training-free decoding framework that accelerates visual autoregressive (VAR) image generation models by trimming late-stage computation with curtailed quality loss. The manuscript points out two recurring inefficiencies: 1) final refinement steps often make tiny changes, and 2) the unconditional branch in classifier-free guidance becomes redundant late in sampling. Based on this observation, SkipVAR pauses mid-generation to compute high frequency difference (Sobel) and high-frequency ratio (FFT), then either 1) early stops the remaining steps or 2) replaces the unconditional branch with the conditional one. On Infinity models, SkipVAR reaches ~1.8× speedups at near-baseline quality, and a hybrid variant attains ~2.6× on GenEval."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Discovers and addresses(with minimal training) the issue of late-stage high frequency redundancy and redundant CFG passes in VAR generation.\n- Two intuitive signals (Sobel and FFT) enable per sample decisions and preserve high frequency detail better than token pruning/merging at similar speedups."}, "weaknesses": {"value": "- Reported speedups exclude VAE and post-processing, so end-to-end latency improvements in production are likely smaller; end-to-end measurements are needed.\n- Heavy reliance on classifier-free guidance, since a major gain comes from dropping the unconditional branch; applicability to non-CFG or single-branch decoders is unclear.\n- In Table 3, the strongest ~2.6× result does not use the decision model, which obfuscates the efficacy of the decision model."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6j2CeB1Wka", "forum": "sV2qR9QIEo", "replyto": "sV2qR9QIEo", "signatures": ["ICLR.cc/2026/Conference/Submission4650/Reviewer_webr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4650/Reviewer_webr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983845939, "cdate": 1761983845939, "tmdate": 1762917490315, "mdate": 1762917490315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the significant inference latency in Visual Autoregressive (VAR) models, which is primarily caused by computationally expensive high-frequency generation steps. The authors identify two key inefficiencies—step redundancy and unconditional branch redundancy—and crucially observe that the impact of these redundancies is highly sample-dependent. They propose SkipVAR, a sample-adaptive framework that uses lightweight frequency features to dynamically select the optimal acceleration strategy (either step-skipping or unconditional branch replacement) for each image. Extensive experiments demonstrate that SkipVAR achieves substantial speedups of up to 1.81x while maintaining high visual fidelity (0.88 SSIM), effectively balancing speed and quality where fixed acceleration methods fail."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents novel and significant observations on the important problem of VAR inference latency, identifying sample-dependent redundancy.\n\n* Based on these observations, the paper proposes a natural and reasonable method that adaptively selects acceleration strategies.\n\n* The various design choices, such as the choice of decision models, are backed by a persuasive rationale."}, "weaknesses": {"value": "1. **Limited Generalizability**\n\t* The paper claims to accelerate \"Visual Autoregressive Modeling,\" yet its entire experimental validation rests exclusively on a single model family: the Infinity-2B/8B family. This is a significant limitation. The core assumptions driving the method, such as the specific patterns of high-frequency redundancy or the convergence of L1 loss between conditional and unconditional branches, may be unique to the Infinity architecture rather than fundamental properties of all VAR models. Without validation on other models that also follow the VAR paradigm (e.g., Tian et al., 2024), the paper fails to sufficiently demonstrate that its findings are generalizable even within its own target model family. **To substantiate the paper's claims, experimental results demonstrating SkipVAR's acceleration performance are necessary on at least one other VAR model family**.\n\t\n2. **Biased Evaluation and Questionable Claims of Superiority over FastVAR**\n\t* The paper's central claim of superiority over key baselines like FastVAR rests heavily on SSIM and LPIPS metrics. However, this comparison appears biased due to a circular evaluation framework. The SkipVAR decision model is explicitly trained to select strategies that preserve a predefined SSIM threshold (e.g., 0.84). Evaluating the model with the same metric it was optimized for does not provide an objective measure of its superiority; it merely confirms that the model met its training objective.\n\t* Furthermore, this reliance on SSIM is problematic. As the authors concede in Appendix L , SSIM is a measure of similarity to the original (non-accelerated) output, not a measure of absolute generation quality. A low SSIM score, as seen in FastVAR's results, indicates that the generated image differs from the original, not necessarily worse in perceptual quality. The possibility remains that while FastVAR produces images that are less faithful to the original generation path, they might maintain or even exceed the overall generation quality on other standard metrics (e.g., FID, CLIPScore, HPSv2, etc.), a comparison which this paper does not rigorously explore. Therefore, **more rigorous experimental results are required to definitively conclude that SkipVAR offers both superior acceleration and better generation quality than FastVAR**.\n\t\n3. **Ambiguous Positioning of the ``SkipVAR-hybrid (w/o DM)`` Variant**\n\t* The paper introduces a ``SkipVAR-hybrid (w/o DM)`` variant, which presents a significant logical contradiction to the paper's core argument for an adaptive strategy. In Table 3, this fixed-strategy (non-adaptive) model is shown achieving a 2.62x speedup and a 0.72 GenEval score, outperforming both the primary adaptive SkipVAR variants (e.g., 1.77x speedup, 0.70 score) and the main competitor, FastVAR (2.53x, 0.68 score).\n\t* This single data point undermines the central argument for an adaptive decision model (DM), as it suggests a fixed strategy is superior in both speed and (at least on this metric) quality. While Table 4 later shows this variant performs poorly on SSIM/LPIPS, the authors fail to provide a clear narrative for its inclusion. Its purpose is ambiguous: is it intended to demonstrate a higher possible speedup (perhaps to show a configuration that outperforms FastVAR on GenEval), or to highlight the flaws of the GenEval metric? As presented, it creates confusion and weakens the justification for the paper's core contribution.\n\t\t\n4. **Questionable Practical Utility of the Adaptive Framework**\n\t* The core value proposition of the adaptive framework is its ability to handle sample-specific needs (i.e., frequency-sensitive vs. -robust) without significant quality degradation. However, the paper's own results cast doubt on its practical utility. According to Table 2b, when the decision model identifies a sample as \"frequency-sensitive,\" the resulting speedup is only 1.28x, a marginal gain.\n\t* Conversely, \"frequency-robust\" samples are accelerated up to 1.99x. This implies that the average speedup figures (e.g., 1.81x in Table 4) are not derived from a balanced acceleration, but are heavily skewed by aggressively skipping \"easy\" samples. If the primary function of the complex adaptive mechanism for \"hard\" samples is to simply not accelerate them significantly, its practical value proposition over a simpler, more conservative fixed strategy (e.g., SkipVAR-hybrid (w/o DM)) is questionable."}, "questions": {"value": "* In several metrics in Table 2 and 3 (e.g., Paint in 2(a), Align in 2(b), and Color Attri. and Overall in Table 3), the application of the proposed acceleration strategies results in a slight increase in performance compared to the original, non-accelerated model. How should these increases be interpreted? Are they simply statistical noise, or does this suggest that SkipVAR's strategies can incidentally mitigate certain generation artifacts or semantic errors present in the original model, thereby leading to a genuine improvement in quality? If so, what is the mechanism for this improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U7vdegveEL", "forum": "sV2qR9QIEo", "replyto": "sV2qR9QIEo", "signatures": ["ICLR.cc/2026/Conference/Submission4650/Reviewer_LJ9B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4650/Reviewer_LJ9B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4650/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762367611428, "cdate": 1762367611428, "tmdate": 1762917489398, "mdate": 1762917489398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}