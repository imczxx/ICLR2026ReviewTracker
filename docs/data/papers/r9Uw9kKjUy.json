{"id": "r9Uw9kKjUy", "number": 18526, "cdate": 1758288776397, "mdate": 1763750344374, "content": {"title": "Matcha: Multi-Stage Riemannian Flow Matching for Accurate and Physically Valid Molecular Docking", "abstract": "Accurate prediction of protein-ligand binding poses is crucial for structure-based drug design, yet existing methods struggle to balance speed, accuracy, and physical plausibility. \nWe introduce Matcha, a novel molecular docking pipeline that combines multi-stage flow matching with learned scoring and physical validity filtering. \nOur approach consists of three sequential stages applied consecutively to progressively refine docking predictions,\neach implemented as a flow matching model operating on appropriate geometric spaces ($\\mathbb{R}^3$, $\\mathrm{SO}(3)$, and $\\mathrm{SO}(2)$). \nWe enhance the prediction quality through a dedicated scoring model and apply unsupervised physical validity filters to eliminate unrealistic poses.\nCompared to various approaches, Matcha demonstrates superior performance on Astex and PDBbind test sets in terms of docking success rate and physical plausibility.\nMoreover, our method works approximately $25 \\times$ faster than modern large-scale co-folding models.", "tldr": "", "keywords": ["Molecular Docking", "Flow Matching", "Riemannian Flow Matching", "Protein-Ligand Interaction"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e5a77ec96fc07c549a521f0469d2c5901eb51d95.pdf", "supplementary_material": "/attachment/71afd88300ed94857e1343ef8abd8f104c9c42b2.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce Matcha, a Riemannian flow matching model for molecular docking, which achieves strong results in comprehensive benchmarks. By not directly incorporating geometric symmetries, Matcha's model architecture provides fast inference at scale. Nonetheless, a few concerns remain regarding the authors' evaluation and discussion of their proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Incorporating no geometric symmetries directly into Matcha's (DiT-based) model architecture is nice to see, following trends from recent works such as AlphaFold 3.\n2. The authors' benchmarks are comprehensive and informative, following many best practices in the field.\n3. Using the 35M parameter version of ESM to avoid overfitting is a clever idea. I haven't seen other works try this."}, "weaknesses": {"value": "1. Matcha doesn't encode protein side-chain atoms, only carbon-alpha (Ca) atoms. This could fundamentally limit its applicability in atomically precise docking tasks such as protein cryptic pocket docking. It'd be good for the authors to discuss this limitation and how it might affect the interpretation of their docking results for Matcha.\n2. The authors' analysis of the evaluation impact of using different alignment methods (in the appendix) is nice to see, but it still raises the question: \"Why do Matcha's reported benchmarking metrics for the PoseBusters Benchmark (v2) dataset differ significantly from those reported in existing benchmarks such as those of AlphaFold 3 and PoseBench?\". For example, PoseBench's reported docking success rates for NeuralPLexer and Chai-1 (using PyMOL for protein-ligand pocket-based alignment) are around 20% and 55%, respectively, whereas the success rates reported for them in this work are around 2% and 30%, respectively. This seems like a possible concern regarding whether these methods were (methodologically) evaluated correctly for such input data."}, "questions": {"value": "1. Does Matcha's inference code support the prediction of multi-ligand docking targets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oMhPpQVN5l", "forum": "r9Uw9kKjUy", "replyto": "r9Uw9kKjUy", "signatures": ["ICLR.cc/2026/Conference/Submission18526/Reviewer_5gS4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18526/Reviewer_5gS4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761156314700, "cdate": 1761156314700, "tmdate": 1762928219632, "mdate": 1762928219632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overview of Changes"}, "comment": {"value": "We sincerely thank all reviewers for their thorough analysis of our work and valuable feedback. Your comments have been extremely helpful in strengthening our manuscript. Below, we summarize the key improvements made to the revised version in response to your collective points.\n\n**Key Improvements in the Manuscript:**\n\n1.  **Added Ablation Studies:** In response to requests from reviewers **4hTx, D8Dr, and 9w7F**, we have added a detailed stage-wise ablation study to Appendix D.1. The results demonstrate the contribution of each of the three pipeline stages, showing that the first two stages are most critical, while the third stage provides small but consistent improvements, especially on more challenging datasets.\n\n2.  **Expanded Baseline Comparison:** Following the suggestion from reviewer **9w7F**, we have conducted a comparative analysis with two recent methods: **DynamicBind** and **SurfDock**. The results, added in Appendix E, show that MATCHA significantly outperforms both of these methods in accuracy across all test sets while remaining substantially faster at inference. Also, we’ve added new benchmarks to Figure 6.\n\n3.  **Clarified Data Usage and Terminology:**\n    *   We have corrected an error in our baseline nomenclature: all results previously attributed to `DiffDock` actually correspond to `DiffDock-L`. We have updated the text accordingly and apologize for this oversight.\n    *   Addressing concerns from reviewers **D8Dr** and **LNqp** about potential data leakage, we have added a detailed description of the MOAD dataset splitting procedure to Section 3.1. This procedure follows the DiffDock-L practice and includes pocket-based clustering to remove near-duplicates from the test sets.\n\n4.  **Analysis of Sensitivity and Design Justifications:**\n    *   We have added an ablation study on the number of Euler steps to Appendix D.2, showing that 10 steps provide an optimal speed-accuracy trade-off.\n\n5. **Analysis of Benchmarking Discrepancies:** To address questions from reviewers **5gS4** and **LNqp** regarding performance comparisons on PoseBusters V2, we have added a new analysis in **Appendix F**. This analysis compares model performance on the full PoseBusters V2 set (n=308) as in our paper versus the 130-complex subset used in PoseBench. The results indicate that co-folding models exhibit a performance drop on a more rigorous set, suggesting their reported results may be inflated due to training data overlap. In contrast, MATCHA's performance remains stable.\n\nWe believe these changes and additions significantly strengthen the paper. We thank you again for your time and effort, which have been useful in improving this work."}}, "id": "XiPBMWgEwg", "forum": "r9Uw9kKjUy", "replyto": "r9Uw9kKjUy", "signatures": ["ICLR.cc/2026/Conference/Submission18526/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18526/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18526/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763750389063, "cdate": 1763750389063, "tmdate": 1763750389063, "mdate": 1763750389063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Matcha, a Riemannian flow-based model that predicts the rotation, translation, and torsion angles of ligands. The authors aim to demonstrate that Matcha achieves a favorable speed–accuracy tradeoff, which is crucial in practical settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Matcha decouples translation prediction for rotation & torsion prediction, enabling a natural extension to pocket-informed settings.\n- The paper is clearly written and easy to follow."}, "weaknesses": {"value": "- Matcha can be viewed as a flow matching version of DiffDock with a DiT-style architecture, which limits its methodological novelty/contribution.\n- While Matcha demonstrates fast inference speed and comparable results on Astex and PDBBind benchmarks, it demonstrates poor performance on PoseBusters V2 and DockGen benchmarks. \n- Matcha lacks several recent/important baselines, such as DiffDock-L, DynamicBind, SurfDock (i.e., those in [PoseX](https://arxiv.org/abs/2505.01700v2)). Incorporating these models could provide a more convincing evaluation of Matcha's performance."}, "questions": {"value": "1. Have the authors considered a more comprehensive comparison with other recent deep learning-based docking models?\n2. What is the motivation for including an additional pose refinement model as the final step? How critical is it to overall performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RsuKMiPiZX", "forum": "r9Uw9kKjUy", "replyto": "r9Uw9kKjUy", "signatures": ["ICLR.cc/2026/Conference/Submission18526/Reviewer_9w7F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18526/Reviewer_9w7F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761466852621, "cdate": 1761466852621, "tmdate": 1762928218727, "mdate": 1762928218727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MATCHA, a three-stage rigid-receptor docking pipeline that performs Riemannian flow matching on translation, global rotation, and ligand torsions. A DiT-style backbone with distance/direction attention biases predicts velocity fields; a separate scoring model ranks candidates after unsupervised PoseBusters-style physical-validity filtering. On ASTEX and PDBbind-time splits, MATCHA reports strong RMSD & PB-valid rates and fast inference versus co-folding models. Performance drops on DOCKGEN and PoseBusters V2, where co-folding pretraining breadth helps OOD pockets. Training uses PDBbind + Binding MOAD, inference samples about 40 poses and selects after filtering + scoring"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clean formulation of flows on SO(2)/SO(3) with SLERP-based conditional velocities and a practical Euler rollout; torsion-only internal DOFs preserve bond geometry.\n- Three independently trained stages (translation to refine translation/angles to sharpen all) are intuitive, grounded and effective.\n- Competitive PB-valid success, clear speed/throughput analysis, and a lightweight scoring head geared for screening loops."}, "weaknesses": {"value": "- DOCKGEN and PoseBusters V2 results drop. Analysis attributes this to co-folding pretraining breadth, but there’s no granular breakdown (pocket geometry shift, ligand size/rotor count, charge states, metal cofactors).\n- While overall very sound, the proposed approach is incremental to existing paradigms."}, "questions": {"value": "- How did you ensure no overlap/near-overlap between MOAD training entries and PDBbind time-split test? Any interface-similarity or sequence-identity thresholds at the pocket? Will you release global dedup manifests?\n- How sensitive is MATCHA to Euler step count, loss weights, and removing distance/direction biases? Does an equivariant variant help or hurt given the DiT choice?\n- How do results change with alternate different packers, or AF-predicted pockets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SfeYXsaMa8", "forum": "r9Uw9kKjUy", "replyto": "r9Uw9kKjUy", "signatures": ["ICLR.cc/2026/Conference/Submission18526/Reviewer_LNqp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18526/Reviewer_LNqp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932719983, "cdate": 1761932719983, "tmdate": 1762928217898, "mdate": 1762928217898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MATCHA, a novel multi-stage pipeline for molecular docking. The method utilizes Riemannian flow matching to progressively refine the ligand's pose across translation, rotation, and torsional degrees of freedom. The pipeline consists of three sequential flow matching models for coarse-to-fine refinement, a separate learned scoring model for ranking candidate poses, and unsupervised physical validity filters to eliminate unrealistic structures. The authors evaluate MATCHA on several standard benchmarks, claiming it achieves a state-of-the-art balance between accuracy, computational efficiency, and physical plausibility, being significantly faster than co-folding models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.   The paper presents a novel and well-motivated application of Riemannian flow matching to the problem of molecular docking, which is a significant departure from the more common diffusion-based generative models.\n2.   Extensive experiments are performed on various import benchmark. MATCHA demonstrates performance on the ASTEX and PDBBind test sets, outperforming many existing methods, especially on the combined metric of geometric accuracy and physical validity (RMSD ≤ 2Å & PB-valid).\n3. The method is shown to be highly efficient, with an inference time approximately 25 times faster than large-scale co-folding models and a more efficient training process than other deep learning baselines."}, "weaknesses": {"value": "1.  The model's performance significantly decreases on benchmarks designed to test generalization, such as POSEBUSTERS V2 and DOCKGEN. While the authors acknowledge this, it remains a major limitation, suggesting the model may not perform reliably on novel protein targets that are structurally dissimilar from its training set.\n\n2.  The source of performance improvement is not clearly isolated. The model is trained on an expanded dataset (PDBBind plus BINDING MOAD), which is larger than that used for some key baselines. The paper lacks an ablation study to disentangle the effects of the larger training set from the novel architecture. Furthermore, the contribution of the individual components of the three-stage pipeline is not validated, making it difficult to assess if all stages are necessary for the final performance.\n\n3. The POSEBUSTERS benchmark was specifically constructed to evaluate the generalization of models trained on PDBBind. By adding the BINDING MOAD dataset to its training, MATCHA may have been exposed to data more similar to the test set, potentially inflating its generalization performance. A detailed analysis of the structural similarity between the added training data and the test sets is needed for a fairer assessment of the model's true generalization ability."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sWmPCqBcXj", "forum": "r9Uw9kKjUy", "replyto": "r9Uw9kKjUy", "signatures": ["ICLR.cc/2026/Conference/Submission18526/Reviewer_D8Dr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18526/Reviewer_D8Dr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979147626, "cdate": 1761979147626, "tmdate": 1762928217459, "mdate": 1762928217459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MATCHA, a novel multi-stage pipeline for protein-ligand docking. The approach pioneers the use of Riemannian Flow Matching on non-Euclidean manifolds, structured within a coarse-to-fine framework. The method models the ligand's degrees of freedom across their corresponding geometric manifolds: translation in R 3, global rotation in SO(3), and internal torsions in SO(2)m. The pipeline employs three sequential, independently trained flow matching models to progressively refine the docking pose, progressing from a global translational search to fine-grained adjustments of all degrees of freedom. Architecturally, the model is based on a DiT-like structure, which incorporates spatial biases into its attention mechanism to effectively capture 3D geometric relationships. A separate scoring model and a physical validity filter are then used to screen the candidates and select the final pose. The authors demonstrate that their method achieves superior performance in terms of both docking success rate and physical plausibility. Furthermore, it operates approximately 25x faster than modern, large-scale co-folding models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper is written in a standard and concise manner; the methods and experiments are easy to understand and unambiguous, and the experiments are fairly sufficient.\n2.\tThe method is the first to apply Riemannian Flow Matching to the field of molecular docking, opening a new research direction with great potential for the field.\n3.\tThe method pragmatically deconstructs the complex docking problem; through its \"coarse-to-fine,\" three-stage pipeline design, it demonstrates an efficient framework for multi-scale generative tasks that is both reliable and original.\n4.\tThe method achieves an excellent and practically significant balance between speed and accuracy. A major advantage of the model is its excellent ability to generate physically plausible conformations."}, "weaknesses": {"value": "1. The paper proposes a complex multi-stage pipeline (3 generative models + 1 scoring model) but does not provide ablation studies to prove the rationale for this design. It cannot be determined if all components are necessary.\n2. The paper does not sufficiently discuss and evaluate the rigid protein assumption and the semi-flexible ligand treatment.\n3. The loss function optimized by the flow matching generative process may not be strongly correlated with true binding affinity or pose correctness, and the three-stage design can easily lead to the propagation and amplification of errors stage by stage.\n4. The multi-stage pipeline is quite engineered, and the training burden is also quite large. Training on existing datasets like PDBBind does not guarantee the method's generalizability, and the four test sets are not particularly convincing.\n5. The evaluation metrics are overly reliant on RMSD; are there other metrics?"}, "questions": {"value": "1.\tThe method relies on a post-processing filter to remove physically implausible poses. Does this mean that MATCHA's generative process routinely produces a large number of poses that do not conform to basic physicochemical principles?\n2.\tThe method uses random rotations for data augmentation. Could this be replaced with an equivariant graph neural network?\n3.\tThe algorithm description mentions that the final loss is a linear combination (a weighted sum) of the three components (translation, rotation, torsion). How were these weights determined?\n4.\tThe inference process uses 10 fixed steps. What is the rationale for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0HXlCVuzE0", "forum": "r9Uw9kKjUy", "replyto": "r9Uw9kKjUy", "signatures": ["ICLR.cc/2026/Conference/Submission18526/Reviewer_4hTx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18526/Reviewer_4hTx"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173778792, "cdate": 1762173778792, "tmdate": 1762928217057, "mdate": 1762928217057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}