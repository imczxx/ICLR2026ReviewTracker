{"id": "1uu4Hg2Nlk", "number": 13296, "cdate": 1758216130489, "mdate": 1763695886559, "content": {"title": "Eliminating VAE for Fast and High-Resolution Generative Detail Restoration", "abstract": "Diffusion models have attained remarkable breakthroughs in the real-world super-resolution (SR) task, albeit at slow inference and high demand on devices. To accelerate inference, recent works like GenDR adopt step distillation to minimize the step number to one. However, the memory boundary still restricts the maximum processing size, necessitating tile-by-tile restoration of high-resolution images. Through profiling the pipeline, we pinpoint that the variational auto-encoder (VAE) is the bottleneck of latency and memory. To completely solve the problem, we leverage pixel-(un)shuffle operations to eliminate the VAE, reversing the latent-based GenDR to pixel-space GenDR-Pix. However, upscale with \n$\\times$8 pixelshuffle may induce artifacts of repeated patterns. To alleviate the distortion, we propose a multi-stage adversarial distillation to progressively remove the encoder and decoder. Specifically, we utilize generative features from the previous stage models to guide adversarial discrimination. Moreover, we propose random padding to augment generative features and avoid discriminator collapse. We also introduce a masked Fourier space loss to penalize the outliers of amplitude. To improve inference performance, we empirically integrate a padding-based self-ensemble with classifier-free guidance to improve inference scaling. Experimental results show that GenDR-Pix performs 2.8$\\times$  acceleration and 60% memory-saving compared to GenDR with negligible visual degradation, surpassing other one-step diffusion SR. Against all odds, GenDR-Pix can restore 4K image in only 1 second and 6 GB.", "tldr": "", "keywords": ["Diffusion", "Super-Resolution", "Adversarial distillation", "Model Compression"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/92912ed41a1f13c62882fed04982aebb1f89cb66.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper identifies the VAE as the principal latency and memory bottleneck in one-step diffusion SR, then removes it by operating in pixel space with pixel-unshuffle/shuffle. It proposes a two-stage adversarial distillation, a masked Fourier loss to suppress periodic artifacts induced by large-factor shuffling, a random-padding augmentation for discriminator stability, and a padding-based classifier-free guidance that doubles as a lightweight self-ensemble. Experiments indicate about 2.8× speed-up and roughly 60% memory savings versus a GenDR baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides convincing evidence that VAE dominates both time and memory.\n\n- The artifact analysis in frequency space is insightful, and the masked Fourier loss directly targets the shuffle-periodic spikes observed empirically.\n\n- Efficiency results are strong, including 4K processing without tiling in certain settings, and the ablations for discriminator choice, RandPad and MFS are helpful."}, "weaknesses": {"value": "- My main concern is that the conceptual novelty is incremental relative to prior encoder-removal and pruning approaches although the authors emphasized the differences to AdcSR.\n- Its robustness across scale factors and severe degradation is under-explored.\n- The effect of RandPad to boundaries is not analysized and discussed."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HyNoRdX9jh", "forum": "1uu4Hg2Nlk", "replyto": "1uu4Hg2Nlk", "signatures": ["ICLR.cc/2026/Conference/Submission13296/Reviewer_mmPH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13296/Reviewer_mmPH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880920343, "cdate": 1761880920343, "tmdate": 1762923964536, "mdate": 1762923964536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents GenDR-Pix, a pixel-space adaptation of the one-step diffusion-based super-resolution method GenDR, aimed at addressing the memory and latency limitations of VAE-based pipelines for high-resolution images. By leveraging pixel-(un)shuffle operations, the authors remove the VAE entirely, but note that high upscaling (×8) can introduce repeated-pattern artifacts. To mitigate this, they propose a multi-stage adversarial distillation strategy, using generative features from previous stages to guide discrimination, along with random padding to augment features and prevent discriminator collapse, and a masked Fourier space loss to penalize amplitude outliers. Additionally, a padding-based self-ensemble with classifier-free guidance is integrated to improve inference performance. Experiments show that GenDR-Pix achieves 2.8× faster inference and 60% memory reduction compared to GenDR, with negligible visual degradation, and is capable of restoring 4K images in 1 second using 6 GB of memory, outperforming other one-step diffusion SR methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of achieving efficient high-resolution image restoration by removing the VAE is interesting and promising.\n\n2. The paper is well-written."}, "weaknesses": {"value": "1.Baselines are VAE latent-space-based models, and forcing the removal of the VAE to operate in pixel space introduces certain risks. As shown in Tables 2 and 3, the proposed method does not achieve the best results. The VAE removal strategy requires more thorough analysis and theoretical justification, which is currently lacking in the paper. The authors do not explain the underlying motivation beyond efficiency, nor do they clarify why this approach works in practice.\n\n2. If the goal is to reduce GPU memory consumption, there are simpler and more effective alternatives, such as tiled VAE, which is already commonly used in high-resolution image generation models.\n\n3. The current experiments are conducted only on SD2.1. To fully validate the effectiveness of the proposed method, experiments should be performed on stronger models, such as SDXL or FLUX-based models. A broader range of evaluations is necessary to convincingly demonstrate its effectiveness.\n\n4. If removing the VAE can indeed improve efficiency, why not adopt a non-VAE-based generative model instead?"}, "questions": {"value": "No"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OE2YJpVNGV", "forum": "1uu4Hg2Nlk", "replyto": "1uu4Hg2Nlk", "signatures": ["ICLR.cc/2026/Conference/Submission13296/Reviewer_occV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13296/Reviewer_occV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899100390, "cdate": 1761899100390, "tmdate": 1762923964014, "mdate": 1762923964014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to eliminate the VAE bottleneck in one-step diffusion-based super-resolution models to enable efficient on-device image restoration. The authors focus on the problem of high latency and memory consumption caused by the VAE in latent-space diffusion models. The authors then try to address this limitation by refining the VAE encoding and decoding, which are often in low efficiency, especially for high-resolution inputs. The authors propose to use a pixel-space diffusion SR model that replaces the VAE encoder and decoder with pixel-level operations and model compression techniques. Evaluations based on some image super-resolution datasets show the effectiveness of the proposed method in terms of restoration quality, inference speedups and memory reduction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Overall, this paper focuses on a meaningful topic in improving the efficiency of the diffusion-based restoration process. The whole idea is easy to understand and seems to be effective. The authors identify the VAE as the primary bottleneck in both latency and memory usage of the diffusion-based SR models. The authors aim to eliminate the VAE to improve efficiency without significantly compromising visual quality. The experimental results show reductions in memory and time costs, making the proposed SR method accessible for real-world tasks."}, "weaknesses": {"value": "I appreciate the authors’ efforts to design a more effective diffusion quantization method. Here, I summarize my major concerns and questions in three parts. \n\n1. The authors propose to replace the VAE with pixel-unshuffle and shuffle operations, so the diffusion procedure is from latent space to traditional pixel space. However, it is unclear about the effectiveness of this strategy. The authors could give a deeper theoretical justification for why latent-space diffusion can be replaced by pixel-space diffusion.\n2. The authors try to prevent discriminator collapse and improve generalization by randomly padding images before feeding them into the discriminator. Could you give some ablation studies on how the padding affects the data augmentation performance?\n3. In equation 2, the authors utilize the masked Fourier space (MFS) loss based on a band-rejection filter. Could you give more details on the mask initialization and how the mask collaborates with the random padding?"}, "questions": {"value": "Please clarify my concerns in the weakness part. I'm not an expert in this field, but I think this paper may have some merits. I would like to check the authors' rebuttal to decide my final rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "luuO1EczyC", "forum": "1uu4Hg2Nlk", "replyto": "1uu4Hg2Nlk", "signatures": ["ICLR.cc/2026/Conference/Submission13296/Reviewer_6W9z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13296/Reviewer_6W9z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974705141, "cdate": 1761974705141, "tmdate": 1762923963625, "mdate": 1762923963625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GenDR-Pix, a one-step super-resolution model that removes the VAE encoder and decoder, enabling direct pixel-space generation. To stabilize training and reduce artifacts, the authors introduce a two-stage adversarial distillation framework along with MFS loss, RandPad, and PadCFG. Trained on 20M images, the model achieves faster inference and lower memory usage while maintaining high perceptual quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation and simplification of architecture: \nThe paper explores removing the VAE encoder and decoder in one-step super-resolution, which reduces system complexity and avoids reliance on latent-space operations.\n\n- Practical efficiency improvements: \nBy operating entirely in pixel space, the method achieves notable gains in inference speed and memory efficiency, as supported by quantitative results.\n\n- Targeted technical solutions: \nThe paper introduces specific techniques (e.g., MFS loss, RandPad, PadCFG) to address known challenges like checkerboard artifacts and unstable guidance in pixel-space generation.\n\n- Empirical validation: \nThe proposed method is evaluated on multiple benchmarks, with ablation studies that reasonably support the effectiveness of key components."}, "weaknesses": {"value": "none"}, "questions": {"value": "Recent one-step RealISR methods such as (e.g., OSEDiff), commonly leverage pre-trained T2I diffusion models  as initialization. With such strong priors, these models require only 10k–20k training steps to adapt to the SR task. In contrast, the authors of this paper move away from this paradigm: they remove both the VAE encoder and decoder, adopt a GAN-based training scheme, and train from scratch on 20M images using adversarial and perceptual losses.\n\nThis shift raises a fundamental question: Are pre-trained T2I diffusion models truly necessary for high-quality one-step super-resolution, or can a GAN-based model trained with sufficient data achieve comparable results without them?\n\nMore specifically:\n\n- Has the author tried training the student model without any teacher guidance, relying solely on GAN and perceptual losses?\n- Has the author evaluated using a randomly initialized teacher, instead of one derived from a pre-trained T2I model?\n\nThese experiments would help clarify whether the benefits come primarily from the pre-trained semantic priors of T2I models or from the data scale and the adversarial learning strategy itself.\n\nAnother important aspect is the role of training data. The authors employ 20 million images for training, which is significantly larger than the datasets used in previous one-step SR works.\n\nTo what extent does the scale and quality of this dataset, as well as the number of training iterations, contribute to the final performance?\n\nFor example:\n\n- Has the model been evaluated under reduced data regimes (e.g., 1M or 5M images)?\n- How sensitive is the performance to the number of training steps?\n- Is the high perceptual quality primarily a result of the large-scale data, rather than architectural or training innovations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VfrBx5ZuAW", "forum": "1uu4Hg2Nlk", "replyto": "1uu4Hg2Nlk", "signatures": ["ICLR.cc/2026/Conference/Submission13296/Reviewer_bRU9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13296/Reviewer_bRU9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13296/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997934856, "cdate": 1761997934856, "tmdate": 1762923963340, "mdate": 1762923963340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}