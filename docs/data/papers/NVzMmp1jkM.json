{"id": "NVzMmp1jkM", "number": 13726, "cdate": 1758221653071, "mdate": 1759897417134, "content": {"title": "RECODE: Reasoning Through Code Generation for Visual Question Answering", "abstract": "Multimodal Large Language Models (MLLMs) struggle with precise reasoning for structured visuals like charts and diagrams, as   pixel-based perception lacks a mechanism for verification. To address this, we propose to leverage derendering---the process of reverse-engineering visuals into executable code---as a new modality for verifiable visual reasoning. Specifically, we propose RECODE, an agentic framework that  first generates multiple candidate programs to reproduce the input image. It then uses a critic to select the most faithful reconstruction and iteratively refines the code. This process not only transforms an ambiguous perceptual task into a verifiable, symbolic problem, but also enables precise calculations and logical inferences later on. On various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K, RECODE  significantly outperforms methods that do not leverage code or only use code for drawing auxiliary lines or cropping. Our work demonstrates that grounding visual perception in executable code provides a new path toward more accurate and verifiable multimodal reasoning.", "tldr": "", "keywords": ["visual reasoning agent", "code generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1656436adf8d45129290b5631044703df739c1b1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces RECODE, a multimodal agentic framework for visual question answering that approaches visual reasoning on structured images  by derendering these inputs into executable code. The RECODE agent generates multiple candidate programs to represent the generative logic of the input image, employs a critic to select and iteratively refine the most faithful code, and then leverages both code and visual modalities for precise, verifiable reasoning. Comprehensive results across benchmarks show that RECODE significantly improves accuracy over both direct pixel-based VQA and recent tool-augmented agents."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.  The integration of a closed self-improving loop (generation, critic selection, iterative code refinement) offers a robust agentic architecture. As illustrated in Figure 1, the pipeline systematically closes the gap between generated and true underlying code, enabling higher accuracy and verifiability.\n2. The paper convincingly reframes structured visual reasoning as a code generation/derendering problem, moving from ambiguous perception towards a verifiable, interpretable, symbolic representation. This is a meaningful shift in thinking for VQA on charts and diagrams."}, "weaknesses": {"value": "1. Much of the empirical validation especially for the initial proof-of-concept and ablation studies around derendering operates on data where ground-truth generative code can be plausibly extracted or synthesized. While the pipeline is motivated as general, there’s insufficient evidence that it scales robustly to complex real-world scientific visuals with lossy, noisy rendering, rasterization artifacts, or hand-drawn figures without programmatic sources. The argument that RECODE generalizes “beyond clean benchmark imagery” is not substantiated in results or examples.\n2. The entire experiment was only conducted on the Gemini2.5 pro, and the benchmark measurements were relatively few, lacking generalizable discussions.\n3. The ablation and main experiments do not address settings with highly cluttered or multi-pane charts, severe occlusion, transparent overlays, or dense text. There’s only limited qualitative evidence (Appendix A.6) that RECODE remains robust as the generative complexity increases. Without experiments on images pushing these boundaries, claims of general applicability are unproven.\n4. Some code snippets in the appendix (A.6, A.7) and their outputs suffer from typographical or formatting errors (e.g., code blocks with syntax issues or unmatched parentheses), which could hinder replication or adaptation. This led to a very messy layout of the entire paper.\n5. Some recent advances on multimodal visual code reasoning (e.g., SWE-bench Multimodal, CodeV, SceneCraft, 3D-GPT) that tackle similar cross-modal code generation or VQA topics are not cited or discussed, limiting the contextualization of the contribution.\n6. While some error cases are mentioned (Section 4.3, discussing multi-line color ambiguities), the paper lacks a candid characterization of persistent failure cases. For example, illustrations where OCR is unreliable, or compositional logic in diagrams exceeds single-pass code generation capacity—even after iterative refinement.\n7. Details regarding the number of refinement rounds and candidate generations per round are sparsely justified (“two rounds, five candidates per round” is asserted as default). It is not clear how sensitive major results are to these settings, or whether performance plateaus or collapses with more iterations. There is insufficient reporting on convergence/stability."}, "questions": {"value": "1. Can the authors provide quantitative or qualitative results on inputs with high visual noise, hand-drawn charts, or non-programmatically-generated figures to better test generalization?\n2. How does RECODE perform when the ground-truth code is deeply non-deterministic, or uses uncommon libraries/functions not in the agent’s training distribution? Is there a clear drop-off in accuracy, or does refinement reliably recover?\n3. Could the authors elaborate on what (if any) failure cases persist after two or more refinement rounds? Is there a systematic way to detect \"irrecoverable\" mismatches, such as OCR failures or data occlusion, beyond visual similarity in the critic?\n4. What are the sensitivity and convergence behaviors as the number of candidates and refinement rounds is increased (beyond the default values used in the paper)?\n5. Do the authors have strategies or insights on expanding to settings with multiple heterogeneous subplots or compositional charts, where subfigure logic or relative positioning becomes essential to the answer?\n6. Given that the method relies on visual code synthesis, what are the possible risks regarding privacy, security, or copyright, especially for proprietary or sensitive scientific visuals?\n7. Can the author conduct tests and verifications on more benchmarks or more base models to verify the generalization of the method？"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The core method could be used to reverse-engineer proprietary, copyrighted, or watermark-protected images into reusable code, possibly violating data licensing or intellectual property. This should be discussed by the authors in the context of responsible research and deployment, especially since code reverse engineering may have implications on research integrity and legal compliance. While no immediate evidence indicates intent for misuse or direct harm, this is a broader risk of automation in this space."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KpFva2ednk", "forum": "NVzMmp1jkM", "replyto": "NVzMmp1jkM", "signatures": ["ICLR.cc/2026/Conference/Submission13726/Reviewer_kcXj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13726/Reviewer_kcXj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760606750369, "cdate": 1760606750369, "tmdate": 1762924268945, "mdate": 1762924268945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "For challenging visual-related questions such as those for charts or diagrams, the authors propose RECODE to generate executable codes that can reproduce the input images and use the generated code as a symbolic representation to help answering the visual-related questions. They iteratively refine the generated code by comparing the original image and the image generated by the code, The authors select the critic function as pixel-based MSE carefully to choose the best candidate code in each round. The experimental results are positive by showing RECODE surpasses open-sourced and even closed-sourced MLLMs in several chart/diagram VQA benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is explained clearly. The authors use gemini2.5 pro for testing the accuracies of QA under three conditions: (1) given images (2) given GT code and (3) given both images and the GT code. Their preliminary results show the value of derendering the images into code. The code can help the visual reasoning. \n\n2. The authors experiment on different benchmarks and the positive results are consistent. \n\n3. Each choice has ablation studies. The presentation is very clean and readable."}, "weaknesses": {"value": "1. The authors provide ablation studies about adding OCR in the code generation process, which is reasonable. It would be interesting by adding a baseline of using OCR results + image only as inputs. \n\n2. Because of the refinement process, I think the cost time will be more expensive. The authors should provide some details about it. \n\n3. In the table 4, the authors try refinement round from 0 to 2. If the number of round becomes larger, will the performances be influenced and what will be the trend? Also, have the authors tried to change the base model to see if the best number of round will be changed as well?"}, "questions": {"value": "Please refine the Figure 2, some of the text are not clear enough. \n\ntypo: \nLine 248: LLM -> MLLM? I guess the \"LLM\" here indicates Gemini Pro 2.5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hV6tHmrkEE", "forum": "NVzMmp1jkM", "replyto": "NVzMmp1jkM", "signatures": ["ICLR.cc/2026/Conference/Submission13726/Reviewer_EbgW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13726/Reviewer_EbgW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890632453, "cdate": 1761890632453, "tmdate": 1762924268535, "mdate": 1762924268535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RECODE, a framework that tackles visual question answering by converting charts and diagrams into executable code. It generates multiple code candidates to reproduce the input image, selects the best one using a critic, and iteratively refines it for higher fidelity. Once the code accurately represents the visual, the model reasons over both the image and code to answer questions. Experiments on CharXiv, ChartQA, and Geometry3K show improvements over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe paper is clearly written, and the experiments on several benchmarks make it straightforward to see how the method helps to chart-related benchmarks.\n-\tThe idea of representing visuals through executable code is interesting and reasonable."}, "weaknesses": {"value": "-\tThe experiments rely only on Gemini-2.5 Pro, which makes it hard to tell whether the proposed pipeline generalizes across different MLLMs. I would suggest adding results with at least one open-source model and maybe GPT-5 for comparison.\n-\tThe evaluated benchmarks are relatively narrow, focusing mainly on chart and geometry datasets. It would be better to include more diverse or harder visual reasoning benchmarks (e.g., Humaneval-V [1]) to test broader applicability.\n-\tI think the paper could analyze more failure cases. Right now, it mostly shows successful examples, but understanding where the derendering or refinement fails would make the approach clearer and help others reproduce or improve it.\n-\tThe computational cost of multi-candidate generation and iterative refinement isn’t discussed in detail. It would be useful to report runtime or cost analysis to understand the trade-off between accuracy and efficiency.\n\n\n\n[1] https://arxiv.org/pdf/2410.12381"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XwQyXza8n8", "forum": "NVzMmp1jkM", "replyto": "NVzMmp1jkM", "signatures": ["ICLR.cc/2026/Conference/Submission13726/Reviewer_v3Nj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13726/Reviewer_v3Nj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989187482, "cdate": 1761989187482, "tmdate": 1762924267990, "mdate": 1762924267990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}