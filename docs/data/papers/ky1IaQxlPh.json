{"id": "ky1IaQxlPh", "number": 13424, "cdate": 1758217653422, "mdate": 1759897438474, "content": {"title": "SAFE-LLM: A Unified Framework for Reliable, Safe, And Secure Evaluation of Large Language Models", "abstract": "Large Language Models (LLMs) exhibit robust abilities but are still vulnerable to factual hallucinations, unsafe responses, and adversarial attacks — issues hindering deployment in safety-critical applications. Current benchmarks assess significant but disjointed facets of risk and do not capture principled uncertainty quantification or defense compositional analysis. We propose SAFE-LLM, a cohesive, auditable evaluation framework for Reliability, Safety, and Security of LLMs. SAFE-LLM offers: (i) a fine-grained taxonomy of risk situations; (ii) standardized metrics (Hallucination Rate, Safety Compliance Index, Jailbreak Success Rate, Prompt Injection Compromise Rate) with finite-sample and sequential con-\nfidence guarantees; (iii) theoretical results on coverage, sequential error control, sample complexity, defense composition, and adaptive adversary bounds; and (iv) a defense-aware benchmarking protocol and reporting format. We demonstrate how SAFE-LLM fills specific gaps in existing practice, outline the road to real-world audits, and address the social impact of taking SAFE-LLM as a standard for\nreliable LLM deployment.", "tldr": "", "keywords": ["Large Language Models (LLMs)", "Reliability", "Hallucination Detection", "Safety", "Jailbreak Attacks", "Prompt Injection", "Statistical Guarantees", "Robustness", "Trustworthy AI"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc01f677c7efce75e0d29ec819e1c6341bc12586.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SAFE-LLM, a unified and statistically principled framework for evaluating reliability, safety, and security of large language models (LLMs). The authors argue that existing evaluation efforts—such as TruthfulQA, RealToxicityPrompts, and JailbreakBench—focus on isolated risk dimensions and lack statistical rigor, reproducibility, and defense composition analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work fills a clear gap by proposing the first unified, statistically rigorous evaluation standard covering reliability, safety, and security dimensions simultaneously."}, "weaknesses": {"value": "1. Lack of empirical demonstration. While the theoretical framework is strong, the paper does not include a full empirical validation or case study showing SAFE-LLM applied to actual LLMs.\n\n2. Limited exploration of adaptive adversarial evaluation. The paper does not demonstrate how SAFE-LLM performs under strong adaptivity (e.g., iterative jailbreak optimization).\n\n3. Potential overlap with existing frameworks."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "muBAstgxM8", "forum": "ky1IaQxlPh", "replyto": "ky1IaQxlPh", "signatures": ["ICLR.cc/2026/Conference/Submission13424/Reviewer_xnDw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13424/Reviewer_xnDw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760837924534, "cdate": 1760837924534, "tmdate": 1762924047957, "mdate": 1762924047957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SAFE-LLM, a conceptual evaluation framework based on statistical principles. This framework can be used to evaluate the reliability, safety, and security of LLMs. The framework includes a risk taxonomy, a set of standardized quantitative metrics, five theoretical results grounded in classical statistical inference, and a defense-stack architecture identifying key measurement points in the model pipeline. Its goal is to establish a statistically principled and auditable methodology for LLM safety evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes the SAFE-LLM framework for evaluating the reliability, safety, and security defenses of LLMs. It emphasizes the importance of statistical indicators such as confidence intervals and sequential inference, compensating for the limitations of current evaluation methods that rely only on point estimates.  \n\n2. For adaptive red teaming, it proposes time-uniform confidence control, which can maintain statistical reliability during open-ended, dynamic, and even adversarial evaluation processes."}, "weaknesses": {"value": "1. The analysis in the related work section is too superficial. It only lists the names of related methods and benchmarks without any synthesis or critical comparison. It also does not illustrate how SAFE-LLM differs from existing “holistic” frameworks in concept or practice.  \n2. I cannot find any case study or experiment applying SAFE-LLM to a specific model in the paper. Therefore, it is impossible to evaluate the actual feasibility and value of the framework.  \n3. There is no in-depth analysis of how the theoretical principles are applied in the paper.  \n4. Theorem 3 assumes conditional independence among defenses, which is rarely true in practice. The paper does not provide any empirical or theoretical methods to address this issue."}, "questions": {"value": "See weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8iN9Fg0yCb", "forum": "ky1IaQxlPh", "replyto": "ky1IaQxlPh", "signatures": ["ICLR.cc/2026/Conference/Submission13424/Reviewer_FKzK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13424/Reviewer_FKzK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550917455, "cdate": 1761550917455, "tmdate": 1762924047714, "mdate": 1762924047714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SAFE-LLM, a unified, statistically principled framework for evaluating the reliability, safety, and security of large language models. The authors identify critical fragmentation in existing evaluation efforts where benchmarks like TruthfulQA, RealToxicityPrompts, and JailbreakBench each measure isolated aspects of model risk without uncertainty quantification or compositional defense analysis. SAFE-LLM aims to provide a foundation for auditable, statistically defensible LLM safety assessments, aligning with frameworks such as NIST AI RMF and the EU AI Act."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Maybe the proposed unified framework is clear and meaningful."}, "weaknesses": {"value": "-\tThe current work is primarily theoretical and framework-oriented. It lacks concrete experimental instantiations demonstrating SAFE-LLM in action. Even a small-scale case study applying the protocol to existing models (e.g., GPT-4, Claude, or LLaMA-2) would strengthen its practical credibility.\n-\tTheorems assume independence between defenses when deriving compositional guarantees (Theorem 3), which may not hold in real systems where correlated defenses interact (e.g., adversarially trained LLMs and output judges relying on shared embeddings). The paper acknowledges this but does not empirically validate mitigation methods, such as empirical correlation bounds.\n-\tWhile metrics such as HR and SCI are well-defined, adjudication rubrics for labeling “violations” or “non-violations” remain abstract. More detailed operational definitions or inter-annotator reliability results would improve reproducibility.\n-\tThe framework’s reliance on pre-registration, stratified sampling, and human adjudication may pose challenges for scaling to very large test sets or rapid model iteration cycles. Practical automation or active-learning approaches could be discussed.\n-\tWithout more concrete examples or experimental details, I suspect this is an LLM-generated paper. The main body of the paper is more like a notebook, not a scientific paper."}, "questions": {"value": "-\tHow would SAFE-LLM integrate with existing benchmark infrastructures such as HELM or HuggingFace Evaluate? Could the authors provide implementation guidelines or an open-source template?\n-\tThe paper promises pre-registration templates and adjudication rubrics. Are there concrete plans for public release or integration with regulatory bodies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KBaEtIw50k", "forum": "ky1IaQxlPh", "replyto": "ky1IaQxlPh", "signatures": ["ICLR.cc/2026/Conference/Submission13424/Reviewer_Zew4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13424/Reviewer_Zew4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955435321, "cdate": 1761955435321, "tmdate": 1762924047477, "mdate": 1762924047477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}