{"id": "oxT5WJtjHE", "number": 6902, "cdate": 1758001037460, "mdate": 1759897884930, "content": {"title": "RPS: Information Elicitation with Reinforcement Prompt Selection", "abstract": "Large language models (LLMs) have shown remarkable capabilities in dialogue generation and reasoning, yet their effectiveness in eliciting user-known but concealed information in open-ended conversations remains limited. In many interactive AI applications, such as personal assistants, tutoring systems, and legal or clinical support, users often withhold sensitive or uncertain information due to privacy concerns, ambiguity, or social hesitation. This makes it challenging for LLMs to gather complete and contextually relevant inputs. In this work, we define the problem of information elicitation in open-ended dialogue settings and propose Reinforcement Prompt Selection (RPS), a lightweight reinforcement learning framework that formulates prompt selection as a sequential decision-making problem. To analyze this problem in a controlled setting, we design a synthetic experiment, where a reinforcement learning agent outperforms a random query baseline, illustrating the potential of policy-based approaches for adaptive information elicitation. Building on this insight, RPS learns a policy over a pool of prompts to adaptively elicit concealed or incompletely expressed information from users through dialogue. We also introduce IELegal, a new benchmark dataset constructed from real legal case documents, which simulates dialogue-based information elicitation tasks aimed at uncovering case-relevant facts. In this setting, RPS outperforms static prompt baselines, demonstrating the effectiveness of adaptive prompt selection for eliciting critical information in LLM-driven dialogue systems.", "tldr": "", "keywords": ["information elicitation", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/070dd990389e72b8a6e7bd5534ac7170ed0e7b0e.pdf", "supplementary_material": "/attachment/e3a6ca62fee590d97ec65132edcf63ac221b9623.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of enabling large language models (LLMs) to adaptively elicit user-known but concealed information in open-ended conversations — a key challenge in domains like law, healthcare, and counseling where users may withhold sensitive facts. The authors propose Reinforcement Prompt Selection (RPS), a lightweight reinforcement learning framework that formulates prompt selection as a sequential decision-making task. Instead of relying on static or handcrafted prompts, RPS learns a policy over a pool of prompt strategies to maximize information gain through dialogue, using a normalized reward function that stabilizes learning. The method is first validated in a synthetic Gaussian Mixture Model environment that simulates user disclosure bias, and then tested on IELegal, a newly introduced benchmark built from real legal case records designed to mimic realistic, biased user behavior. Experimental results show that RPS consistently outperforms static and LLM-generated prompting strategies, demonstrating its effectiveness in adaptively uncovering relevant and sensitive information. The work provides a principled foundation for interactive AI systems that can engage in information-seeking dialogues with socially aware and context-sensitive adaptability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper explicitly defines the Information Elicitation (IE) problem — how to extract information users know but hesitate to share — and positions it as a fundamental limitation of current LLM-based dialogue systems. It introduces a socially grounded motivation, bridging LLM dialogue modeling with human disclosure behavior (positive, neutral, negative information). This gives conceptual clarity and interdisciplinary depth rarely seen in prompt-optimization work.\n2. Reinforcement Prompt Selection (RPS) is a lightweight yet principled RL framework that models prompt choice as a sequential decision problem, rather than as static prompt engineering or token-level optimization. The use of normalized information gain as a reward is technically sound — it stabilizes training and avoids reward vanishing as the dialogue progresses. The framework is domain-agnostic, modular (separable from the base LLM), and computationally efficient compared to token-wise RLPrompt-style methods.\n3. The authors provide both controlled synthetic validation (via a Gaussian Mixture Model with disclosure bias) and realistic domain evaluation (the new IELegal dataset). The dual evaluation design (synthetic + real-world) effectively demonstrates the method’s adaptability and robustness to both unbiased and biased disclosure settings. Quantitative metrics (KL divergence, semantic similarity via Sentence-BERT) are clearly justified and aligned with the objective of information recovery.\n4. The IELegal dataset is an important contribution: a legally grounded benchmark derived from real criminal cases, simulating realistic user reluctance and factual complexity. It provides structured factual annotations, bias labeling (positive/neutral/negative), and multi-turn dialogue simulations, offering the community a concrete platform to study information elicitation in sensitive domains.\n5. The work aligns well with practical dialogue systems in law, healthcare, and education — where adaptive questioning is essential for completeness and trustworthiness. The ethical statement is careful: experiments use only anonymized, public data; there’s no deployment risk."}, "weaknesses": {"value": "1. While the application to information elicitation is new, the underlying reinforcement-based prompt optimization mechanism is conceptually similar to prior works such as RLPrompt (Deng et al., 2022) and TEMPERA (Zhang et al., 2022). The main technical step — selecting discrete prompt templates via an RL policy — is incremental rather than fundamentally new, relying on established DQN/DDPG setups without introducing algorithmic innovation. The novelty thus primarily lies in **problem framing and dataset construction**, not in RL or LLM adaptation mechanics.\n2. Both evaluation setups (GMM environment and IELegal benchmark) depend on synthetic or scripted user behavior, where user disclosure follows pre-defined rules (positive/neutral/negative bias). These simulations lack real psychological variability — users in real conversations may conceal information for nuanced emotional, cultural, or contextual reasons. This gap makes it unclear whether the model would generalize to true human dialogues, where “concealment” is not linearly related to sentiment polarity.\n3. The normalized information gain reward presupposes that an accurate “ground-truth information set” $I$ exists and that semantic distance $d(\\hat{I}_{t}, I)$ can be computed. In practical LLM dialogue, true user information is inaccessible, and similarity-based proxies (e.g., Sentence-BERT embeddings) can misjudge factual correctness or relevance. Hence, the **reward design is not deployable in real-world systems**, limiting practical applicability to simulation-only settings.\n4. The baselines are primarily rule-based or handcrafted prompt strategies plus one LLM-generated adaptive method. Missing comparisons include stronger meta-prompting or self-play baselines, such as: (1) LLMs fine-tuned via _preference optimization_ (RLHF, DPO, PPO-style methods); (2) Active questioning models (e.g., Socratic prompting or iterative retrieval-augmented agents). Without these, the performance gap may be overstated relative to more capable contemporary methods.\n5. While the authors acknowledge ethical risks, the system’s objective — to elicit concealed user information — has potentially intrusive implications. There is no detailed discussion of safeguards, consent mechanisms, or human oversight, which would be essential in sensitive applications (law, health, personal dialogue)."}, "questions": {"value": "1. The reward depends on a distance $d( \\hat{I}\\_{t}, I )$, but what distance metric ensures semantic and factual fidelity? How sensitive is the learning outcome to the choice of Sentence-BERT similarity? Is the normalization (division by the previous distance) numerically stable when $d( \\hat{I}_{t-1}, I)$ becomes small?\n2. How large is the prompt pool, and how are the prompts generated or curated? Are they manually designed or learned automatically? How sensitive is the policy performance to the number or diversity of prompt templates?\n3. Why were only handcrafted prompt strategies and one LLM-generated policy included as baselines?\n4. The RL reward requires access to ground-truth $I$. How could this be replaced or approximated in real applications?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The system’s objective to elicit concealed user information has potentially intrusive implications."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b7KV1wzXsG", "forum": "oxT5WJtjHE", "replyto": "oxT5WJtjHE", "signatures": ["ICLR.cc/2026/Conference/Submission6902/Reviewer_kz24"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6902/Reviewer_kz24"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761261548011, "cdate": 1761261548011, "tmdate": 1762919145312, "mdate": 1762919145312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical challenge for Large Language Models (LLMs) in open-ended dialogues: effectively eliciting information that users know but may intentionally conceal (e.g., sensitive information in legal or medical scenarios). It proposes a lightweight reinforcement learning framework called Reinforcement Prompt Selection (RPS). The core contribution is modeling the Information Elicitation (IE) problem as a sequential decision-making task. Instead of generating prompts token-by-token, the RPS framework learns a policy network that adaptively selects the optimal prompting strategy from a predefined pool (containing strategies like exploratory or adversarial prompts). This selection is based on the current dialogue history and acquired information, aiming to maximize information gain in subsequent interactions. To validate the method, the paper conducts two types of experiments: 1. ​​Synthetic Experiments: In a controlled Gaussian Mixture Model (GMM) environment, it demonstrates that an RL agent using RPS outperforms random baselines in information recovery accuracy (measured by KL divergence). 2. ​​Real-world Scenario Experiments: It constructs a new legal domain benchmark dataset,  IELegal(with IELegal-base and IELegal-augment subsets), based on real legal cases that simulate a user (defendant) concealing unfavorable facts. Results show that RPS significantly outperforms various static prompt baselines and an \"adaptive generative questioning\" baseline in extracting key facts from simulated users."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper accurately identifies a core pain point for LLMs in high-stakes applications (e.g., law, healthcare): users intentionally concealing information due to privacy, social desirability, or conflicting interests. Framing this as the IE problem and incorporating the social psychology concept of information valence (positive, neutral, negative) is highly relevant and forward-thinking.\n\n2. The RPS framework strikes an excellent balance between efficiency and effectiveness. Compared to token-level prompt generation methods like RLPrompt, selecting from a predefined prompt pool significantly reduces computational overhead and deployment difficulty, making it more suitable for real-time dialogue systems.\n\n​​3. The IELegal dataset is a significant contribution. Based on real cases, with structured fact annotations and simulated user disclosure biases, it provides a much-needed benchmark for the sub-field of \"eliciting concealed information.\""}, "weaknesses": {"value": "1. Both the GMM environment and the LLM/rule-based user simulation in IELegal (e.g., \"omit or partially conceal unfavorable information if the question is not direct or precise\") are far from the complex psychology of real humans (e.g., changing trust, emotional fluctuations, active deception). The lack of evaluation with real human users (Human-in-the-Loop) casts doubt on RPS's real-world effectiveness.\n\n​​2. The IELegal dataset is relatively small (1000 cases per subset) and confined to the legal domain. The medical and financial scenarios mentioned in the introduction are not validated, leaving the cross-domain generalization capability of RPS (and the cost of migrating the prompt pool) unclear.\n\n​​3. The \"adaptive generative questioning\" baseline is relatively weak (it lets an LLM generate a strategy rather than execute one). A fairer and stronger comparison would be against state-of-the-art LLM agent frameworks (e.g., ReAct, Self-Ask) or a powerful LLM granted the same prompt pool (as in-context examples) and instructed to \"select the best strategy and generate a question.\""}, "questions": {"value": "1. The core of RPS relies on a predefined prompt pool, constructed for IELegal based on domain knowledge (legal interview techniques). If migrating RPS to a new domain (e.g., medical consultation), what would be the manual cost of building this pool? Are there plans for automated or semi-automated prompt pool construction?\n\n2. ​​In a real scenario where a user is determined to conceal information (e.g., outright denying a key fact in a legal consultation), RPS would receive near-zero rewards consistently. Could this lead to policy collapse or getting stuck in ineffective questioning loops?\n\n3. ​​Do the authors plan to (or have they conducted preliminary) test RPS with real human users to validate its effectiveness beyond simulated environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hdEisBoYwj", "forum": "oxT5WJtjHE", "replyto": "oxT5WJtjHE", "signatures": ["ICLR.cc/2026/Conference/Submission6902/Reviewer_P4sL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6902/Reviewer_P4sL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807780566, "cdate": 1761807780566, "tmdate": 1762919144770, "mdate": 1762919144770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents RPS, which provides a framework for building LLMs capable of eliciting concealed information via dialogues. The key to the method is learning a policy over a pool of different prompts that determines the next query for the LLM to ask. The policy is trained over the reward of maximizing information gain. The authors evaluate RPS on synthetic dialogue, and a real-world legal case benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper formalizes the problem of information elicitation, which is an important and practical problem. \n\n2.  The approach is lightweight by training a policy over a finite set of prompts, rather than open-ended token generation. This avoids having to train the LLM itself, which reduces computational overhead.\n\n3. The evaluation on legal cases is interesting, and can be used broadly as a benchmark for open-ended dialogue."}, "weaknesses": {"value": "1. The biggest weakness lies in its limited evaluation. Specifically, the evaluation is limited to synthetic and legal case dialogues. This limits the generalizability of the findings for all open-ended dialogues requiring information elicitation. Furthermore, in both domains the user is simulated using a simple prompt to discourage revelation of information. It is likely this does not model the complexity of dynamics of a real-world conversations; specifically, the simulated users in the evaluation have no temporal dynamics.  \n\n2. The performance is measured based on similarity of sentence embeddings. It is unclear if such reward truly captures the nuances of how good the policy did, compared to human or even LLM-based evaluations of the full dialogue.\n\n3. The authors compare against a variety of prompting baselines. While the baselines do consider a variety of strategies, and even adapting between them, the comparison feels unfair as RPS requires additional training and additional overhead during inference. The results would be stronger if the authors also compared to a training baseline, or even more sophisticated inference such as GRIPS that the authors mentioned in related work."}, "questions": {"value": "1. Currently, RPS considers a concise but potentially restrictive action space of predefined prompts. Do the authors believe the current set of prompts is sufficient for general information elicitation? If not, how would one expand the action space to fit to more complex tasks?\n\n2. The authors introduce a normalized reward signal based on information gain. What happens if the unnormalized information gain was used as the reward?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j3hkzkIpIr", "forum": "oxT5WJtjHE", "replyto": "oxT5WJtjHE", "signatures": ["ICLR.cc/2026/Conference/Submission6902/Reviewer_b7ep"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6902/Reviewer_b7ep"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762242190758, "cdate": 1762242190758, "tmdate": 1762919144422, "mdate": 1762919144422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of eliciting \"user-known but concealed information\" in open-ended dialogues with Large Language Models (LLMs). Authors state this is an important problem in sensitive domains like legal or clinical support, where users may withhold information due to privacy concerns or social hesitation. (I am not sure about this statement though) \n\nThe authors propose Reinforcement Prompt Selection (RPS), a lightweight RL framework. Instead of generating prompts token-by-token (which is computationally expensive), RPS learns a policy (using DQN) to select the best prompt from a predefined pool of strategies (e.g., \"Exploratory,\" \"Confrontational\") at each turn of the conversation. The goal is to maximize the amount of information recovered from the user.\n\nTo evaluate RPS, the authors introduce a new benchmark dataset, IELegal, constructed from real-world, anonymized legal cases. They test their method in both a synthetic Gaussian Mixture Model (GMM) environment and on the IELegal benchmark (using an LLM to simulate a biased user). The results show that RPS outperforms static, fixed-prompting baselines in recovering concealed information."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper clearly formulates the information elicitation problem. As LLMs become integrated into sensitive applications, the ability to adaptively elicit relevant information may be helpful in certain scenarios (like legal).\n\n- The core idea of selecting from a prompt pool rather than generating prompts is a practical simplification. This makes the RL problem more tractable, reduces computational overhead, and is a sensible design choice for real-world deployment. However, \n\n- The creation and public release of the IELegal dataset is valuable (from real legal documents). \n\n- The work sits at the intersection of reinforcement learning, sequential decision-making, and large language models. These are very active areas of research for the ICLR community."}, "weaknesses": {"value": "- The core idea of selecting from a prompt pool rather than generating prompts is a practical simplification. This makes the RL problem more tractable, reduces computational overhead, and is a sensible design choice for real-world deployment. However, if computational overhead is not a constraint, one can utilize the selected prompt (or relevant information) to craft/generate a better prompt. This would make the real system more usable. \n- the empirical evaluation is solid but still somewhat small (1k-case) legal dataset, and all “users” simulated by LLMs. Also, it is unclear how English prompts with Chinese legal cases confound the LLM's results. More datasets would be useful to consider to add more weight to the research analysis. \n- the work doesn’t justify the choice of algorithms (DQN, etc) vs more modern or simpler alternatives (contextual bandits, supervised classifier over prompts, PPO, GRPO). The paper mentions RLprompt work in related work, but doesn't compare to this method either (or GRIPS); it may not be suitable to compare this work to RLPrompt, etc; the baselines used in this paper look reasonable but clearly not SOTA"}, "questions": {"value": "- Why did you not consider comparing RPS to other RL-based prompt optimization methods like RLPrompt, even if adapted to a selection framework? A PPO-based agent on the same discrete action space would be a more direct and stronger baseline.\n- This research's motivation is to \"uncover information users may intentionally withhold.\" Could you elaborate on the safeguards necessary to prevent this from being \"weaponized\" as a tool for social engineering? How do you distinguish between ethical \"elicitation\" and manipulative \"extraction\"?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "- I don't really think ethics review is needed here, however, since the paper is on the topic of information elicitation, it will be good for responsible AI reviewers to take a quick look at this work."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Lv6IvuUmy", "forum": "oxT5WJtjHE", "replyto": "oxT5WJtjHE", "signatures": ["ICLR.cc/2026/Conference/Submission6902/Reviewer_TpMQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6902/Reviewer_TpMQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762356463785, "cdate": 1762356463785, "tmdate": 1762919143798, "mdate": 1762919143798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}