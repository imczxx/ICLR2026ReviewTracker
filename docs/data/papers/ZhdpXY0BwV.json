{"id": "ZhdpXY0BwV", "number": 5106, "cdate": 1757848928840, "mdate": 1759897994314, "content": {"title": "Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation", "abstract": "Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear.\nHumans easily solve textbook-style spatial reasoning problems with over 95\\% accuracy, but we find that most leading MLLMs fail to reach even 60\\% on the same tasks. This striking gap highlights spatial reasoning as a fundamental weakness of current models.  To investigate this gap, we present MathSpatial, a unified framework for evaluating and improving spatial reasoning in MLLMs.  MathSpatial includes three complementary components: (i) MathSpatial-Bench, a benchmark of 2K problems across three categories and eleven subtypes, designed to isolate reasoning difficulty from perceptual noise; (ii) MathSpatial-Corpus, a training dataset of 8K additional problems with verified solutions; and (iii) MathSpatial-SRT, which models reasoning as structured traces composed of three atomic operations—Correlate, Constrain, and Infer.  Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial achieves competitive accuracy while reducing tokens by 25\\%. MathSpatial provides the first large-scale resource that disentangles perception from reasoning, enabling precise measurement of spatial reasoning skills in MLLMs. More broadly, MathSpatial offers a comprehensive foundation for understanding how MLLMs handle mathematical spatial reasoning. Our code and datasets will be released upon paper acceptance.", "tldr": "", "keywords": ["Spatial Reasoning，Multimodal Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/af6dc3ae5df0eb2c819e75c1214bd892834b31af.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces MathSpatial, a comprehensive framework for evaluating and improving mathematical spatial reasoning in multimodal large language models (MLLMs). It presents a new benchmark (MathSpatial-Bench), a large-scale training corpus (MathSpatial-Corpus), and a structured reasoning trace methodology (MathSpatial-SRT), revealing a significant gap between human and model performance on spatial reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Structured Reasoning Framework: Proposes MathSpatial-SRT, which decomposes reasoning into interpretable atomic operations, enhancing transparency and diagnosis.\n- Comprehensive Evaluation: provides evaluation with other similar benchmarks and helps identify the gap being filled by the paper\n- Artifacts contribution: along with an evaluation benchmark, the paper also produces artifacts such as training corpus and reasoning traces to enable SFT/RL on models to help bridge the identified gap."}, "weaknesses": {"value": "- Need for a more comprehensive model evaluation: The paper's focus seems to be on Qwen2.5-VL and does not cover a wide plethora of open-source models such as Llama-4, Kimi, DeepSeek. Would be great test the benchmark on these model families as well, for a complete picture.\n\n- Lack of Data Contamination study : the paper lacks a robust data contamination study and plan to mitigate such concepts - extremely crucial for the benchmark's relevance in the near future.\n\n- Limited scope to evaluate mathematical reasoning: the gap identified - \"lack of mathematical reasoning in MLLMs\" is limited to public educational repositories and textbooks, curated and standardized in this paper. There are recent papers (e.g - MaRVL-QA) that cover a wider variety of tasks and also a bigger benchmark (~80k instances)"}, "questions": {"value": "Major flags identified and covered in the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "suLZhtNIlq", "forum": "ZhdpXY0BwV", "replyto": "ZhdpXY0BwV", "signatures": ["ICLR.cc/2026/Conference/Submission5106/Reviewer_9ctw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5106/Reviewer_9ctw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761510158001, "cdate": 1761510158001, "tmdate": 1762917877560, "mdate": 1762917877560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the poor mathematical spatial reasoning of MLLMs by introducing `MathSpatial`, a comprehensive framework. It consists of `MathSpatial-Bench` for evaluation, a large-scale `MathSpatial-Corpus` for training, and a novel structured reasoning method (`SRT`). The authors demonstrate that fine-tuning an open-source model using their framework improves accuracy while significantly reducing token usage. `MathSpatial` provides the first systematic resource to diagnose and improve this critical weakness in current models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**1. Addresses a Clear and Critical Research Gap in MLLM Capabilities.**\nThe paper correctly identifies and targets a well-known, fundamental weakness of current MLLMs: abstract spatial reasoning. While models have shown impressive performance on perception-oriented tasks like image captioning, their ability to perform structured, multi-step geometric reasoning remains poor. By focusing on this specific bottleneck, the paper makes a timely and highly relevant contribution to the field.\n\n\n**2. Introduction of a Novel and Interpretable Reasoning Framework (`MathSpatial-SRT`).**\nThe paper's proposal to decompose spatial reasoning into three atomic operations—`Correlate`, `Constrain`, and `Infer`—is a notable  contribution. This `MathSpatial-SRT` framework offers a structured and interpretable alternative to the often unstructured and opaque free-form \"Chain-of-Thought\" reasoning. This structure has several benefits:\n*   **Interpretability:** It makes the model's reasoning process transparent and easier to debug.\n*   **Error Diagnosis:** It allows researchers to pinpoint exactly where reasoning fails (e.g., in correlating views vs. inferring properties).\n*   **Targeted Supervision:** It provides a mechanism for more precise intermediate supervision during training."}, "weaknesses": {"value": "**1. Evaluation is Confined to an In-Distribution Test Set, Limiting Generalizability Claims.**\n\nThe paper's primary evaluation is conducted on `MathSpatial-Bench`, a test set created using the same data sources and processing pipeline as the training set, `MathSpatial-Corpus`. This \"in-distribution\" evaluation setup poses a significant risk of **distributional overfitting**. The fine-tuned model (`MathSpatial-7B`) may be learning dataset-specific artifacts—such as common diagrammatic styles, question phrasings, or recurring geometric patterns—rather than a truly generalizable spatial reasoning skill.\n\nConsequently, key claims, such as the **25% reduction in token usage**, are not robustly supported. This efficiency gain is likely a byproduct of the model learning to generate outputs in the specific, concise `SRT` format it was trained on, a format perfectly aligned with the test set. Without evaluation on out-of-distribution data, it is impossible to know if this accuracy and efficiency would transfer to problems from different sources, making the paper's broader claims about \"improving spatial reasoning in MLLMs\" unsubstantiated.\n\n\n\n**2. Lack of External Validation on Established Benchmarks.**\n\nA critical methodological omission is the failure to validate the fine-tuned model's performance on external, publicly available benchmarks for mathematical and spatial reasoning (e.g., **GeoEval**, **3DSRBench**, or the geometry section of **MMMU**). In benchmark-driven research, the gold standard for demonstrating the effectiveness of a new dataset or training method is to show that it improves performance on a range of existing, independent tasks.\n\nBy only reporting results on its own test set, the paper presents its findings in a vacuum. It proves that the training method works for the specific data distribution it created but fails to provide evidence of its utility for the wider research community. This lack of cross-benchmark generalization testing weakens the argument that `MathSpatial-Corpus` provides a fundamental improvement to a model's core reasoning capabilities.\n\n**3. The Training Methodology is a Standard Application of Knowledge Distillation with Limited Novelty.**\n\nThe core training strategy involves using a powerful \"teacher\" model (GPT-4o) to generate synthetic reasoning traces, which are then used to fine-tune a smaller \"student\" model (`Qwen2.5-VL-7B`). This is a well-established technique known as **knowledge distillation**. While effective, the method itself is not novel and has been widely used across many domains.\n\nThe paper's primary innovation does not lie in the training methodology but rather in (a) the design of the structured reasoning format (`MathSpatial-SRT`) and (b) the significant engineering effort in curating the dataset. The framing of the paper should more accurately reflect this, positioning its contribution as a novel *application* of a standard technique to a new structured format and a valuable new dataset, rather than implying the training process itself is a new invention. This misattribution of novelty obscures the paper's true contributions.\n\n\n**4. Unsupported Claims about Human Performance:** The paper heavily relies on the \"striking gap\" between MLLMs (<60%) and humans (>95%) to motivate the entire work. However, the methodology for obtaining this human performance baseline is not detailed. Key questions are unanswered:\n    *   How many human participants were there?\n    *   What was their demographic (e.g., students, experts in geometry, crowd-workers)?\n    *   What were the exact instructions and conditions for the test?\n    Without this information, the 95%+ figure is not scientifically rigorous and weakens the paper's core motivation.\n\n\n**5. The \"Minimal Sufficient Set\" Claim for SRT is Weak:** The paper proposes `Correlate`, `Constrain`, and `Infer` as a \"minimal sufficient set\" of atomic operations for spatial reasoning (Propositions 1 & 2). The proofs provided in Appendix B.4 assert that the decomposition works without providing a rigorous argument for why it is both sufficient for all problems and truly minimal. \n\n\n**6. Limited Architectural Diversity in Experiments:** The primary fine-tuning experiments are conducted on the Qwen2.5-VL series (3B and 7B). While this demonstrates the framework's effectiveness on one model family, its generalizability is not proven. A stronger methodology would involve fine-tuning models from different architectural families (e.g., Llama, InternVL) to show that `MathSpatial-SRT` provides benefits beyond a single, specific architecture."}, "questions": {"value": "1. What was the methodology for establishing the 95%+ human performance baseline?\n\n2. How was the \"Base-CoT\" baseline in the ablation study (Figure 7) generated and trained? Was the Qwen-7B model fine-tuned on a separate corpus of free-form Chain-of-Thought solutions? Who generated these solutions (e.g., the original model, GPT-4o)? \n\n3. How robust is the GPT-4o-based data generation pipeline?** The quality of the entire `MathSpatial-Corpus` depends on GPT-4o's ability to generate correct and consistent `SRT` traces. The paper mentions a 10% error rate detected by a validation process, but what is the final, verified error rate in the training corpus?\n\n4. What steps were taken to ensure that no semantic duplicates or near-duplicates exist between the 8K training corpus and the 2K benchmark set? Given that both datasets are sourced from the same educational repositories, how did the authors guarantee a clean split to prevent data leakage and ensure the benchmark provides a fair test of generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "W0rM8nMFmi", "forum": "ZhdpXY0BwV", "replyto": "ZhdpXY0BwV", "signatures": ["ICLR.cc/2026/Conference/Submission5106/Reviewer_Lcyn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5106/Reviewer_Lcyn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938898637, "cdate": 1761938898637, "tmdate": 1762917877291, "mdate": 1762917877291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the significant gap between human and Multimodal Large Language Model (MLLM) performance on mathematical spatial reasoning tasks. The authors identify three key challenges in existing research: perceptual confounds in benchmarks, scarcity of large-scale training data, and the black-box nature of current reasoning methods. To tackle these issues, they introduce MathSpatial, a comprehensive framework consisting of three components:\nMathSpatial-Bench: A 2,000-problem benchmark designed with clean geometric figures to isolate reasoning from perceptual noise.\nMathSpatial-Corpus: A large-scale training dataset of 8,000 problems with human-verified solutions.\nMathSpatial-SRT (Structured Reasoning Traces): A novel framework that decomposes spatial problem-solving into three atomic operations: Correlate, Constrain, and Infer. This enables interpretable and verifiable supervision, moving beyond unstructured Chain-of-Thought. Experiments demonstrate that even state-of-the-art MLLMs like GPT-5 struggle to reach 60% accuracy on MathSpatial-Bench, where humans score over 95%. The authors show that fine-tuning an open-source model using their corpus and SRT framework yields competitive accuracy while being significantly more token-efficient, providing a clear path for improving MLLM spatial reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper does an exceptional job of framing the problem, clearly identifying the \"three core challenges\" (perceptual confounds, data scarcity, black-box reasoning) and using the stark human-vs-MLLM performance gap to motivate the work.\n* The MathSpatial framework, combining MathSpatial-Bench and MathSpatial-Corpus, is a great contribution in itself. The meticulous data curation process ensures a high-quality resource that fills a clear gap in the existing landscape of spatial reasoning benchmarks by focusing specifically on reasoning.\n* MathSpatial-SRT decomposes reasoning into the basic operations of Correlate, Constrain, and Infer which is an elegant idea. It offers a concrete path toward building more interpretable and reliable reasoning systems, a critical goal for the field. The theoretical justification via Propositions 1 and 2 further strengthens this contribution.\n* The breadth of models tested, the inclusion of a human baseline, the insightful error analysis, and the well-executed ablation studies collectively provide a strong and convincing validation of the paper's claims and the value of its contributions."}, "weaknesses": {"value": "* The work intentionally focuses on static, 2D/3D educational geometry problems to isolate reasoning. While this is a core strength, it naturally raises questions about the direct transferability of models trained on MathSpatial to noisy, dynamic, real-world scenarios. The paper would be stronger with a brief discussion or preliminary experiment exploring this generalization gap.\n* The MathSpatial-SRT traces are initially generated by GPT-4o. Although this is a practical and common methodology, it introduces a dependency on a powerful closed-source model for creating the training data. While the authors mention a multi-stage validation pipeline, more quantitative details about this process (e.g., the rate of corrections required) would be beneficial to assess the quality and potential biases of the generated traces."}, "questions": {"value": "1. Have you performed any preliminary experiments to assess how MathSpatial-7B's improved reasoning capabilities hold up when faced with perceptual noise (e.g., evaluating it on a subset of a more perception-heavy benchmark like 3DSRBench)?\n2. Could you elaborate on the validation process for the GPT-4o generated SRTs? For instance, what was the \"role-playing scheme\" used for validation, and what percentage of the initial traces were flagged for errors and required manual correction?\n3. The {Correlate, Constrain, Infer} primitive set is elegant and sufficient for the tasks in MathSpatial. Do you have thoughts on whether this set would need to be expanded to cover more complex spatial reasoning domains, such as those involving physics (e.g., stability, friction) or temporal dynamics (e.g., trajectory prediction)?\n4. You report a 25% reduction in reasoning tokens. Could you clarify what this is being compared against (e.g., the baseline Qwen2.5-VL-7B producing free-form CoT)? Is this token reduction an inherent feature of the structured SRT format, or an emergent property of the model learning to be more concise during fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "akQ837EjhF", "forum": "ZhdpXY0BwV", "replyto": "ZhdpXY0BwV", "signatures": ["ICLR.cc/2026/Conference/Submission5106/Reviewer_ynjK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5106/Reviewer_ynjK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980285343, "cdate": 1761980285343, "tmdate": 1762917876906, "mdate": 1762917876906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a spatial reasoning benchmark MathSpatial. This benchmark measures reasoning performance along 3 problem settings, while also addressing scale and scope of problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The scale of problems collated in this benchmark is much higher than that of comparable works in the domain.\n2. The representation of spatial reasoning as a sequence of atomic operations that can be assessed separately provides an important framework to identify failures in model reasoning patterns and improves interpretability\n3. The limitations and potential social impact of this work are well thought-out."}, "weaknesses": {"value": "1. For proposed benchmarks, the integrity of annotations and problems is of the utmost importance. This work does not explore how the verification and review process is ensured to be bias-free and robust to any variances, eg. by demonstrating annotator agreement metrics for review and solution verification, or rubrics for quality assurance."}, "questions": {"value": "Suggestions:\n1. Table 2 could benefit from highlighting high-performing models per category to help compare model performances at a glance"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZsWov8aTVc", "forum": "ZhdpXY0BwV", "replyto": "ZhdpXY0BwV", "signatures": ["ICLR.cc/2026/Conference/Submission5106/Reviewer_819v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5106/Reviewer_819v"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995035712, "cdate": 1761995035712, "tmdate": 1762917876602, "mdate": 1762917876602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}