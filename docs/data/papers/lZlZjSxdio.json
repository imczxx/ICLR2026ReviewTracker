{"id": "lZlZjSxdio", "number": 1578, "cdate": 1756893724775, "mdate": 1759898200188, "content": {"title": "Rethinking LLM Evaluation: Can We Evaluate LLMs with 200× Less Data?", "abstract": "As the demand for comprehensive evaluations of diverse model capabilities steadily increases, benchmark suites have correspondingly grown significantly in scale. Despite notable advances in redundancy reduction and subset-level performance prediction, a systematic framework that effectively integrates these methods to ensure both prediction accuracy and ranking consistency is still largely elusive. In this paper, we first perform a sample-level analysis of benchmark redundancy and identify several highly similar samples that can be eliminated. Besides, we frame benchmark compression as an optimization problem with the aim of score reconstruction. Building on these, we then propose EssenceBench, a coarse-to-fine framework utilizing an iterative Genetic Algorithm (GA), which takes the advantages of fitness-based subset search and attribution-based sample search. Compared to previous methods, our approach yields superior compression results with lower reconstruction error and markedly higher efficiency. In particular, on the HellaSwag benchmark (10K samples), our method preserves the ranking of all models shifting within 5% using 25$\\times$ fewer samples, and achieves 95% ranking preservation shifting within 5% using only 200$\\times$ fewer samples.  The source code will be made available upon acceptance of the paper.", "tldr": "We propose a benchmark compression method that efficiently accelerates the evaluation of large language models (LLMs).", "keywords": ["Data Selection", "Data Pruning", "Large Language Model", "Benchmark Compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6daeebac7d1bbecf725a47f78455acbe8f2dfd1f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes EssenceBench, a coarse‑to‑fine framework for compressing LLM benchmarks so that rankings and overall scores can be reconstructed with far fewer evaluation items. The pipeline (i) performs coarse filtering to remove redundant items using text similarity (embedding cosine) and ranking similarity (correlation of model outcomes), (ii) runs a Genetic Algorithm to select subsets that minimize a GAM‑predicted reconstruction error (RMSE) of full‑set accuracies across models, and (iii) applies attribution‑guided selection using an Explainable Boosting Machine (EBM) to score per‑item contribution, group items (high/low/random attribution), and re‑search within the best group; this is iterated to improve diversity and convergence. Experiments on five benchmarks (GSM8K, ARC, HellaSwag, WinoGrande, MMLU) report lower RMSE and better rank preservation than MetaBench and other baselines, with large reported reductions (e.g., 25×–200× fewer samples on HellaSwag) and faster compression."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Casting benchmark compression as minimizing an error between full‑set and subset‑based scores is crisp and aligns the objective with leaderboard reconstruction\n2. The paper quantifies both text redundancy and ranking redundancy and visually demonstrates non‑trivial redundancy on popular datasets\n3. The three‑stage design is easy to follow and practically implementable."}, "weaknesses": {"value": "1. The duplication filter “keeps the earlier item and discards the later one if similarity exceeds thresholds”, which introduces order bias and potential instability across random permutations of the dataset. No robustness check to ordering is reported.\n2. The paper mentions bge‑m3 embeddings and ranking correlations, but concrete threshold values, normalization, and sensitivity are absent\n3. Fig 2C shows wall clock hours, but the hardware, eval setup is not clear. How do you compare the eval time?"}, "questions": {"value": "1. In Section 3.3, the fitness uses a GAM g that maps subset accuracy s_j to full‑set accuracy y_j. It is unclear whether g is retrained per candidate subset or pre‑trained on a pool of subsets. The compute/variance implications differ drastically and affect the efficiency claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IE3PPKWEUc", "forum": "lZlZjSxdio", "replyto": "lZlZjSxdio", "signatures": ["ICLR.cc/2026/Conference/Submission1578/Reviewer_TShU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1578/Reviewer_TShU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763593438, "cdate": 1761763593438, "tmdate": 1762915822900, "mdate": 1762915822900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "For addressing the growing cost and redundancy of large-scale LLM benchmarking, the study presents a benchmark compression framework, EssenceBench. It aims to preserve ranking fidelity while substantially reducing evaluation cost for large language models. The approach combines redundancy-aware filtering with a genetic algorithm and attribution-guided refinement to identify representative subsets of benchmark data. Experimental results on five datasets show consistent improvements over prior methods. It highlights the potential of EssenceBench to make large-scale LLM evaluation more efficient and scalable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method addresses an increasingly important and practical challenge in LLM evaluation concerning benchmark cost without losing ranking reliability.\n2. The integration of fitness-based subset selection with attribution-based sample selection provides an effective paradigm for benchmark compression while maintaining the intrinsic properties of datasets (e.g., diversity).\n3. Experimental results across five benchmarks and multiple baselines demonstrate the strong empirical performance of EssenceBench."}, "weaknesses": {"value": "1. The sample redundancy phenomenon has been widely observed in prior studies on dataset quality assessment. The paper should more clearly differentiate EssenceBench from these existing efforts to highlight the necessity of Genetic Algorithms.\n2. The study lacks the computational or time complexity analysis of the proposed framework, which may better demonstrate the feasibility of EssenceBench for datasets with different scales.\n3. The parameter sensitivity analysis is limited, which only investigates the effects of the number of generations (gens) and refinement rounds (rounds). Other key hyperparameters (e.g. $\\tau_{text}$, $\\tau_{ranking}$ and $N_{\\mathcal{P}}$) that likely have a substantial impact on compression quality are omitted. It raises the concern regarding parameter search complexity and the overall robustness of the proposed framework.\n4. The experimental evaluation narrowly focuses on the extrinsic metric of prediction error (RMSE) and ranking consistency. The experiments lack some quantitative metric to evaluate intrinsic quality and characteristics of the compressed datasets (e.g., diversity, bias and so on)."}, "questions": {"value": "1. Could the evaluation be extended with objective metrics that assess the intrinsic properties of the compressed datasets (e.g., diversity [1], bias [2] and so on)\n2. An analysis of the genetic algorithm's computational complexity and a justification for the number of generations required for convergence are needed.\n3. A more comprehensive sensitivity analysis is required for the hyperparameters to verify the robustness of the proposed framework, along with a discussion on the parameter search strategy.\n4. The definition 4 lacks sufficient details about how the ranking scores $\\mathbf{r}$ are computed from several LLMs. \n5. The case study in Table 3 offers an interesting glimpse, but a broader analysis is needed to fully support the claims about model behavior. Could you include a dedicated appendix section with more examples across different datasets to better illustrate the duplication-aware filtering in EssenceBench?\n6. In Figure 2, the legends of MetaBench and EssenceBench are mislabeled.\n7. Part of notations are inconsistent. For example, Equation 12 defines $j \\in \\mathcal{I}(m)$ while the main text refers to $j \\in \\mathcal{I}(\\mathcal{D}_{filtered})$\n\n[1] On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey. 2024.\n\n[2] Bias and fairness in large language models: A survey. 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U6QmHP6jvc", "forum": "lZlZjSxdio", "replyto": "lZlZjSxdio", "signatures": ["ICLR.cc/2026/Conference/Submission1578/Reviewer_MmMV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1578/Reviewer_MmMV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932916439, "cdate": 1761932916439, "tmdate": 1762915822686, "mdate": 1762915822686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that sample-level redundancy makes many LLM benchmarks wasteful. The authors present EssenceBench, a coarse-to-fine pipeline that first removes redundant items, then searches for a compact subset using a genetic algorithm steered by a learned accuracy predictor, and finally refines the selection with attribution-based grouping to preserve coverage. The selection goal is to use a k-item subset to reconstruct full-benchmark scores. EssenceBench reports lower reconstruction error and strong rank fidelity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- well-motivated problem. this paper addresses what I believe is an important and practical problem of expensive LLM evaluations\n- the experimental results are thorough and consistent\n- paper is written clearly with good explanation and clear pseudo-code and a good appendix. also has thorough ablations and analysis.\n- I like the formalization of LLM benchmark compression as an optimization problem and use GA + attribution refinement"}, "weaknesses": {"value": "- experiments focus on static, multi-choice benchmarks. I think applicability to open-ended, interactive, or multi-modal tasks may be limited or at the very least is untested\n- the GA + EBM pipeline involves several hyperparams and the thing costs should be discussed more explicitly as its likely to be expensive. Other methods like SMART filtering [1] don't require this tuning but seem to enable accurate compression. The filtering method could be adapted to your setup as a baseline or complementary approach.\n- the coarse filtering step relies on manually set thresholds τ_text and τ_ranking. How sensitive is the method to these choices? Different benchmarks may require different thresholds, limiting generalizability.\n\n[1] Gupta, Vipul, et al. \"Improving model evaluation using smart filtering of benchmark datasets.\" arXiv preprint arXiv:2410.20245 (2024)."}, "questions": {"value": "- I think generalizability is my biggest question/concern. Will the compressed benchmarks work for new model architectures not in the train set? How often do these benchmarks need to be recomputed, etc?\n- I would like to see a computation cost trade-off? How many GPU hours to compress each benchmark? It might be helpful to add a clear table showing benchmark size, compression time, compressed size, and number of evaluations needed to break even. This will be nice for practitioners wondering if they should use this method. \n- How were τ_text and τ_ranking chosen? Is there a principled way to set these automatically?\n- Is there a theoretical justification for using GA specifically? Did you compare against other optimization approaches? Why is this particularly suitable?\n- I'm curious if this approach can be extended to eval tasks where scoring is more complex (e.g. using LLM as a judge)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ygic24oPLt", "forum": "lZlZjSxdio", "replyto": "lZlZjSxdio", "signatures": ["ICLR.cc/2026/Conference/Submission1578/Reviewer_ykSp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1578/Reviewer_ykSp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994162213, "cdate": 1761994162213, "tmdate": 1762915822316, "mdate": 1762915822316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose using genetic algorithm to compress LLM evaluation set. Experiments show ranking is preserved with a small shift."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Paper is clearly written, well motivated, and generally easy to follow.\n2. Authors identify and analyze the redundancy of the widely used LLM benchmarks.\n3. Compressing LLM benchmark by 25x (lossless) to 200x (5%) shows the potential for a far smaller “validation set” or “test set”."}, "weaknesses": {"value": "1. The main benchmarks being used (GSM8K, ARC, HellaSwag, WinoGrande, MMLU) are considered saturated for the frontier LLMs these days. Newer LLMs are challenged with harder benchmarks, and harder benchmarks contain fewer instances since they are more challenging to come up with. I encourage authors to address why their overall method may adapt to modern and future benchmarks.\n2. Similar to above, authors did not test newer benchmarks. For example, how would authors method extend to agentic benchmarks (SWEBench / tau2-bench) and multimodal benchmarks (MMMU etc.) I believe a compression of those benchmarks are needed at the moment. And after all benchmark compression is a one-off effort, releasing compressed benchmarks on author’s end would be a good contribution to the community."}, "questions": {"value": "1. Original benchmark performance is treated as ground truth in the paper. However, there remains doubts on that. For example, suppose if a benchmark has 50% of problems as 1+1=2, this would definitely mean that the compressed benchmark will behave differently than the original benchmark, but this isn’t a necessarily bad thing. Maybe a compressed benchmark at a certain level could better reflect an LLM’s capability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "27FiTUWhZU", "forum": "lZlZjSxdio", "replyto": "lZlZjSxdio", "signatures": ["ICLR.cc/2026/Conference/Submission1578/Reviewer_hqMF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1578/Reviewer_hqMF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762122929362, "cdate": 1762122929362, "tmdate": 1762915821921, "mdate": 1762915821921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}