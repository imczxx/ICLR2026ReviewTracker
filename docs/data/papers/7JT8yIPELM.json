{"id": "7JT8yIPELM", "number": 3716, "cdate": 1757504044562, "mdate": 1759898073728, "content": {"title": "MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments", "abstract": "Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8\\% memory-related tasks and no cross-episode learning evaluation. We introduce \\textbf{MemGUI-Bench}, the first comprehensive benchmark designed to assess both short-term and long-term memory in mobile GUI agents. Our contributions include: (1) a systematic memory taxonomy with analysis of 11 prominent agents; (2) 128 tasks across 26 applications where 89.8\\% challenge memory through cross-temporal and cross-spatial information retention; (3) \\textbf{MemGUI-Eval}, an automated evaluation pipeline with novel \"Progressive Scrutiny\" and 8 hierarchical metrics for memory fidelity and learning effectiveness; and (4) comprehensive assessment revealing significant memory deficits across all evaluated agents. Our experiments expose 4-10× performance gaps between memory-intensive and standard tasks, demonstrate the potential of explicit long-term memory mechanisms, and identify 7 distinct failure modes through systematic analysis. MemGUI-Bench establishes crucial empirical baselines for developing more capable and human-like GUI agents. Code and results: \\url{https://anonymous.4open.science/r/MemGUI-Bench-Anonymous}.", "tldr": "", "keywords": ["agent", "mllm", "memory", "nlp"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f9d83b41ba5f6fcc6b35754f6fab55be347b7d6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a benchmark to evaluate short- and long-term memory usage among mobile GUI agents. This paper aggregates tasks from varied applications and compares 11 agent frameworks, revealing their relative performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Large-Scale Effort.**\n> This paper analyzes 11 agents and aggregates tens of applications, covering major works in the domain of mobile manipulation.\n\n2. **Comprehensive agent support and evaluation protocol.** \n> As introduced in section sec 3.2, this work supports a unified pipeline to ensure robust agent evaluation. Also, the metrics proposed in sec 4 enable memory-targeting evaluations, with human-annotated references."}, "weaknesses": {"value": "1. **Limited practical utility among agent developments.**\n> First, memory seems like a useful component designed in *some* works, yet not a universal feature that needs to be incorporated by agents. Therefore, evaluating memory is an interval, intermediate self-check for some agents, rather than a universal correctness metric such as task success rates.\n\n> Second, this work structure agent memory possibly inspired by how human memory works (this point is less justified as well), yet this may not generalize across all agent designs or be useful for the most performant agent at all. For example, this work separates short-term and long-term memory, yet agents may become effective in long context modeling thus no need to split memory by time. It may be more reasonable to frame this work as an empirical analysis on *some* agent frameworks that share similar memory structure.\n\n> Lastly, it is unclear whether the established memory taxonomy generally applies to domains (web browsing, computer use) and tasks (personal assistant, work) beyond mobile manipulation.\n\n2. **Inaccurate definition of memory.**\n> From the description in section 2, it may be more accurate to frame the two categories as \"in-session\" and \"cross-session\" memory, as \"short-term\" and \"long-term\" are somewhat vague therefore cause confusion.\n\n3. **Uncertain quality of evaluation examples.**\n> It is unclear (based on the description in section 3.1) how the applications are selected, how the examples are created (synthesized by LM, manually annotated by human, recorded from real-human activities, etc.?), and what principles are integrated into the examples during this process.\n\n4. **Shallow analysis findings.**\n> The findings in section 5 lack practical implications, beyond that agent A is better in short/long-term memory. Deeper analysis, especially supported by clearer, more fine-grained memory aspects, such as procedural workflow/fact retention, cross-app retention, should be much more informative for future agent development works. This limitation in analysis may be somewhat incurred by the lack of clarity in task design (weakness pt3).\n\n5. **Presentation needs to be improved.**\n> There are multiple places where the tables and figures are not placed properly, e.g., Table 1. Meanwhile, improving the writing to elaborate the motivation and detailed procedures clearer, could be helpful for readers to understand this work."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ifYjZHoUVK", "forum": "7JT8yIPELM", "replyto": "7JT8yIPELM", "signatures": ["ICLR.cc/2026/Conference/Submission3716/Reviewer_DFxX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3716/Reviewer_DFxX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761504881059, "cdate": 1761504881059, "tmdate": 1762916942860, "mdate": 1762916942860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MemGUI-Bench, a benchmark for mobile GUI agents focused on memory. It reports: (i) a “systematic memory taxonomy,” (ii) 128 tasks across 26 apps, with ~90% designed to stress memory, (iii) MemGUI-Eval with “Progressive Scrutiny” and 8 hierarchical metrics, and (iv) broad evaluations arguing existing agents have major memory deficits (4–10× gap between memory-intensive vs “standard” tasks) and that explicit long-term memory helps. Code link is provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Timely problem framing. Mobile GUI agents are rising; a purpose-built memory benchmark is valuable and under-served. The cross-temporal/cross-spatial emphasis aligns with real usage.\n\n- Scale & coverage. 128 tasks / 26 apps is non-trivial for interactive GUI evaluation; the claimed memory-task share (~89.8%) suggests deliberate design rather than incidental memory.\n\n- Evaluation pipeline ambition. “Progressive Scrutiny” + hierarchical metrics aim to move beyond pass/fail, which is the right direction for agent memory diagnostics."}, "weaknesses": {"value": "- **Generalization beyond the curated suite**: The benchmark spans 26 apps, but are they category-balanced (commerce, productivity, social, finance), regionally representative, and covering UI paradigms (infinite scroll, nested modals, webviews)? Without a sampling rationale and held-out app categories, it’s unclear if results generalize or if models “learn the test.”\n\n- **Memory vs. perception/exploration confound**: It’s unclear whether measured failures are truly memory failures versus UI perception, layout parsing, long-horizon exploration, or tool-use orchestration. Without perception-controlled variants (e.g., textual UI abstractions; identical observation streams with/without memory demand), a 4–10× gap could reflect harder vision or search rather than memory per se. The paper needs ablations isolating memory load from these factors. (They currently only assert memory difficulty; details are missing on how confounds are neutralized.)"}, "questions": {"value": "I don't have any questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lw9HrrNSab", "forum": "7JT8yIPELM", "replyto": "7JT8yIPELM", "signatures": ["ICLR.cc/2026/Conference/Submission3716/Reviewer_LqfD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3716/Reviewer_LqfD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820130128, "cdate": 1761820130128, "tmdate": 1762916942606, "mdate": 1762916942606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MemGUI-Bench is a memory-centric benchmark for mobile GUI agents. It comprises (i) short-term vs. long-term memory taxonomy; (ii) a task suite of 128 tasks across 26 apps with 89.8% memory-intensive cases and 64 mirror pairs to test cross-episode learning. Further, the paper introduces MemGUI-Eval, an automated evaluation pipeline with 8 hierarchical metrics (IRR, MTPR, FRR, etc.) and pass@k support. Finally, the authors present a comprehensive evaluation of 11 agents showing large gaps between memory and non-memory tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper identifies a clear gap in the existing mobile-agent benchmarks. MemGUI-Bench bridges the gap with a clear focus.\n- The modular feature of MemGUI-Bench eval makes it easy to integrate with existing benchmarks.\n- Surprising low performance of existing state-of-the-art methods on the benchmark, substantiating its claims on the memory-gap."}, "weaknesses": {"value": "- Paper formatting for Table 1 and Figure 4.\n\n- Lack of qualitative examples to showcase the memory-gap in state-of-the-art model like UI-TARS. Adding such examples will demonstrate the need of the benchmark more clearly.\n\n- L27: \"First comprehensive benchmark for GUI-agent memory\" is plausible, but Table 4 shows prior memory tasks exist (e.g., SPA-Bench has 40/340). I would suggest qualifying to \"first comprehensive, memory-centric benchmark with pass@k and a staged LLM-as-judge evaluator.\""}, "questions": {"value": "- Does authors have an explanation for Table 3 numbers? Why does the pass@k with increasing k not increasing for some models but increasing for others? Further, does the performance saturate after a certain k in all the models. It would be nice to see such a curve for open-source models at least.\n\n- Can the authors add a test-time compute normalized evaluation? For each agent, fix a compute budget - tokens/step, steps/episode and show SR/IRR/FRR deltas under equal budgets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a4gxT9gHtm", "forum": "7JT8yIPELM", "replyto": "7JT8yIPELM", "signatures": ["ICLR.cc/2026/Conference/Submission3716/Reviewer_ryFo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3716/Reviewer_ryFo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762273886465, "cdate": 1762273886465, "tmdate": 1762916941815, "mdate": 1762916941815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}