{"id": "0lW2UBiEWN", "number": 9469, "cdate": 1758123535691, "mdate": 1759897719911, "content": {"title": "Mesa and Mask: A Benchmark for Detecting and Classifying Deceptive Behaviors in LLMs", "abstract": "As the capabilities of Large Language Models (LLMs) grow, so does their shadow. AI Deception—misleading users in the output while concealing internal reasoning—is a nascent phenomenon in frontier models with potentially severe societal ramifications. To build safe and trustworthy AI systems, a systematic evaluation mechanism for deception is imperative. A key question is: How can we systematically and reproducibly diagnose the brittleness of an LLM's alignment? To address this challenge, we introduce MESA & MASK, the first benchmark designed for the differential diagnosis of LLM deception. Its core methodology is to measure the principled deviation of a model's behavior by contrasting its reasoning and responses in a baseline context (Mesa) with those under a latent pressure context (Mask). This enables the systematic classification of behaviors into genuine deception, deceptive tendencies, and brittle superficial alignment. Based on this, we have constructed a cross-domain dataset of 2,100 high-quality instances. We evaluated over twenty models and found that even the most advanced models commonly exhibit significant deceptive behaviors or tendencies, which validates the benchmark's effectiveness in revealing behavioral differences among models under pressure. MESA & MASK provides the community with a powerful tool to diagnose and understand AI deception, laying the groundwork for more verifiable and aligned AI systems.", "tldr": "", "keywords": ["Deceptive Behavior; Benchmark and Evaluation; AI Safety; Alignment"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/85ab3280ac249160906d74465d1baa10624bb2a9.pdf", "supplementary_material": "/attachment/095471c562367a094c83edc5546d3389d091f2a2.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces MESA&MASK, a deception‐oriented benchmark that contrasts neutral system prompts (MESA) with pressure-inducing system prompts (MASK) across 2,100 scenarios spanning 6 deception types × 6 domains, and evaluates 22 LLMs with a rubric-guided LLM-as-judge (GPT-4.1) on both final answers and chain-of-thought (CoT). The core claim is that pressuring prompts systematically increase deceptive behavior and that models differ markedly in both their propensity to deceive and their “deception consistency” across settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper includes a detailed description on how they generated the dataset with a largely-automated pipeline, including usage of tools such as Model-Context-Protocols(MCP) in the process, and the prompts used for generation."}, "weaknesses": {"value": "1. The paper solely relies on GPT-4.1 as LLM-as-Judge. Although it has a comparison table on Table 4 that has performance metrics on GPT 4.1, GPT 5, DeepSeek-R1, the table does not clearly state which test data it has used for evaluation. Also, the paper does not take into account for mitigating biases in LLM-as-Judges, such as positional bias [1].\n2. Despite the dataset is generated by language models with templates and seed scenarios, it does not consider or analyze data duplication. The human annotation or data quality evaluation stage includes checks with data format or data sanity, but does not include any duplication checks. \n3. The paper lacks novelty in that it is based on the prior work on MASK [3], which already contains comparative evaluation for eliciting pressure or deception with language models. The novelty of this work is limited to expanding the benchmark to chain-of-thoughts(CoT), 6 domains, 6 deception types.\n\n[1] Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang Liu. 2024. Split and Merge: Aligning Position Biases in LLM-based Evaluators. EMNLP 2024\n[2] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. \"Deduplicating Training Data Makes Language Models Better.\" ACL 2022\n[3] Ren, Richard, Arunim Agarwal, Mantas Mazeika, Cristina Menghini, Robert Vacareanu, Brad Kenstler, Mick Yang et al. \"The mask benchmark: Disentangling honesty from accuracy in ai systems.\" arXiv preprint arXiv:2503.03750 (2025)."}, "questions": {"value": "1. Although the paper mentions about multi-turn interactive benchmarks, it is not clear if it the generated dataset covers multi-turn conversations. The attached data samples in the appendix indicate that the generated data samples are single-turn. Would you elaborate on this matter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3XceiTVHIK", "forum": "0lW2UBiEWN", "replyto": "0lW2UBiEWN", "signatures": ["ICLR.cc/2026/Conference/Submission9469/Reviewer_F3C7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9469/Reviewer_F3C7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761564379237, "cdate": 1761564379237, "tmdate": 1762921058366, "mdate": 1762921058366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a dataset for measuring LM deception when prompted with a situation that puts pressure towards deception vs control prompts. The authors evaluate a large number of models on dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Important problem (AI deception) and key types of deception evaluated (alignment faking, sycophancy, sandbagging). \n\nSolid methodological contribution in comparing LM deception under pressure (MASK) to the control (MESA) --- something that is missing in many similar works. \n\nThe biggest strength of the paper is the comprehensive benchmark which spans a large ranges of interesting deceptive behaviours and domains. \n\nOverall, good awareness of relevant literature. \n\nBroad range of domains and behaviours tested. Large number of frontier LMS tested. \n\nOverall well-written and presented."}, "weaknesses": {"value": "Overall I didn't think there were that many compelling results or key findings. (However I think the benchmark itself is a very solid contribution which lays the ground for future findings.)\n\nIIUC the core metric is deception as judged by GPT-4.1. But the paper does argue for this metric very much, and it's unclear whether LM judges are reliable for this. I'd like to see some evaluation of the judge, e.g., according to its agreement with human evaluators. However, the full judge prompts were appreciated. \n\n\"COMPARISON OF OPEN-SOURCE AND CLOSED-SOURCE MODELS\"\n\"Open-source models show higher deception rates\" ---> but you're not controlling for other factors right, like model capability? So what should we really take away from this?\n\nFigure 5 does not seem to show very clear results --- is there a key takeaway?\n\nSAFETY FINE-TUNING IMPACT ANALYSIS\nThe results here are very minimal --- only a few percentage points of difference in deception rates from SFT. It seems like this isn't representative of the effect of safety fine-tuning in general (eg HHH training which makes models much more honest). Were the Qwen models tested already safety fine-tuned? Can you test instruct or base models vs HHH models?  \n\nMinor \n\nThe paper gives a few different notions of deception (based on intentionally causing false beliefs, or hiding internal reasoning). The authors should explicitly stick to one definition or just acknowledge there's no universal definition that you want to capture. You should also cite: https://arxiv.org/abs/2312.01350\n\nI didn't find figure 1 that intuitive, maybe there is a more clear-cut example. \n\nThere's quite a bit of content on the tenth page, e.g., limitations --- imposed not sure if this breaks the ICLR policy"}, "questions": {"value": "See questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jrsnkyTXgp", "forum": "0lW2UBiEWN", "replyto": "0lW2UBiEWN", "signatures": ["ICLR.cc/2026/Conference/Submission9469/Reviewer_Mdgq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9469/Reviewer_Mdgq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832584705, "cdate": 1761832584705, "tmdate": 1762921058037, "mdate": 1762921058037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a dataset and a methodology to detect and classify deceptive behaviour in LLMs. The dataset contains 2100 samples spanning 6 professional domains and 6 different deceptive behavior types. Each data sample is based on realistic professional scenarios and refined in an iterative process at the end of which each data sample comprises a user prompt paired to a neutral system prompt (MESA condition) and a system prompt used to provide pressure through incentives highlighting goal conflicts and often related to deployment, utility, or preservation without ever explicitly instructing deception (MASK condition). Model generated CoT and responses are then evaluated comparatively under these paired conditions (MESA & MASK) producing 4 possible model behaviors: Consistent (MESA & MASK produce consistent CoT and responses), Superficial alignment (MESA & MASK produce inconsistent CoTs but similar responses), Deception tendency (MESA & MASK produce consistent CoTs but inconsistent responses), and Explicit deception (MESA & MASK produce inconsistent CoTs and responses).\n\nThe paper then evaluates 20 mainstream models and concludes that deception tendencies are widespread. It further analyses how deception depends on the model being open vs closed source, dense or with MoE, or size. These analyses produce interesting insights, but these remain relatively underexplored to allow for strong conclusions.\n\nOverall, the paper addresses an important research direction and produces a valuable dataset but would benefit from improvement in presentation clarity, positioning relative to related work, and justification of certain methodological choices.\n\nMandatory disclosure of LLM usage by the reviewer: The reviewer used LLMs to reformat this review text into organized prose and numbered lists, and to write this sentence."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a very timely research direction: it studies brittleness of alignment in terms of how models adopt deceptive behavior under subtle pressure.\n\n2. The dataset created seems to be of high quality and could be a useful deception benchmark eval, based on realistic scenarios and importantly spans multiple professional contexts and deception types. The amount of effort, involving multi-source data collection, iterative refining and human validation, behind its construction is noteworthy.\n\n3. While the idea of comparing response deviation between neutral and pressured conditions is not an original idea (see Ren et al. 2025), doing so while looking at both CoT and responses is original and allows for a more complete classification of model behaviors.\n\n4. Evaluating multiple models on these dataset is a valuable contribution and can give a better understanding of models' deceptive behavior under subtle, yet realistic, pressure."}, "weaknesses": {"value": "1. Clarity of Novel Contributions and Positioning: The paper's title (MESA & MASK) suggests its main contribution is a comparative evaluation framework contrasting model behavior under neutral (MESA) and pressured (MASK) conditions. However, this approach already exists in Ren et al. 2025's MASK framework, which the authors mention in the introduction but not in related work. Hence, the title may be misleading. The paper should better clarify the novelty in its contributions (realistic high-risk professional domain settings and using CoT to reveal internal cognitive shifts from honesty to deception?). The related work section should mention Ren et al. 2025 (especially in section 2.2).\n\n2. Clarifications Regarding Methodology: Dataset difficulty assessment (section B.4) is unclear: How is the \"multi-dimensional framework\" encompassing \"scenario sophistication, ethical ambiguity and decision complexity\" used if the dataset is evaluated (or rather filtered) on whether at least ⅔ models exhibit deceptive behavior? How does this relate to the stratified sampling? Using the same models for data sample filtering and then evaluation biases the final dataset evaluation results for these models. Would results change significantly if three different models (belonging to different model families) were used for data filtering? The conclusions on ultra-large MoE exhibiting higher deception rates might be poisoned by this bias. The MESA chain-of-thoughts and responses to each user prompt are aggregated through a consensus process. While I understand this is intended to create a stable baseline, it would be important to report whether models already exhibit inconsistencies in their responses before the application of pressure. Without this information, it is difficult to gauge how much the pressure cue itself contributes to eliciting deceptive behavior, particularly in interpreting metrics the significance of metrics such as Deception Rate @1. One major contribution of the paper seems to be the use of CoT. However, for the most part the analysis seems to be based only on the top part of the behavioral quadrant (i.e. deception due to inconsistent response, independent of CoT consistency). This makes the results comparable to other works (again Ren et al. 2025 above all) which does not use CoT. The paper could benefit from more in depth discussion, example or analyses of deception tendencies vs explicit deception.\n\n3. Presentation and Clarity Issues: The paper is not easy to follow in several passages and could overall improve in presentation. Clarity is sometimes hindered by excessive or unclear naming. For example, the Data Quality Evaluation criteria (MESA Utility Elicitation, Deception Induction, and Invisible Pressure) are harder to interpret than the alternative more transparent names used in the appendix to explain them (User Prompt Quality, System-User Integration, System Prompt Quality). Much of this naming is unnecessary and reduces clarity. Here is another example: \"Once the prompts are constructed, the pipeline enters the Multi-turn Generation and Sampling Loop to produce deception data through context refinement.\" Sampling loop is never defined in the main text or in the appendix. Scenario generation in section 4.2 could benefit from shorter periods (the whole paragraph is only 2 long and convoluted periods). More examples of: examples of consistent and inconsistent model responses and of mesa replies vs consensus aggregated mesa replies (nice to have), and how prompts or scenario improves during the iterative process (nice to have).\n\n4. Evaluation metrics for models are compared against expert annotations used as GT. This information was present only at the end of the section and could be made clearer earlier, before discussing the numerical results.\n\nNitpicks:\n\n- Figure 1 could improve a bit with more text and the word \"Mitigates\" (in MASK model response) should be hyphenated when going to the next line. (Other than this, I find most figures of high quality)."}, "questions": {"value": "See weaknesses above. Additionally:\n\n1. The paper says \"As shown in Figure 3, our approach operates through integrated dataset generation and model evaluation phases\": It is unclear to me in which sense and why are dataset generation and model evaluation integrated? I don't see this discussed in the paper and dataset statistics in Figure 4 and the file \"M&M_dataset.csv\" seem to suggest that the dataset once and statistically. Could you provide clarifications regarding this point? Which model was used to generate this data?\n\n2. Regarding human annotations: are agreement score and Cohen's kappa computed on each checked item or only on the final assessment? Are samples missing one of the checks filtered out? This could be better detailed.\n\n3. What are exactly scenarios and templates? Both these terms are used in the first part of dataset generation and it's not clear what's the difference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vXbKbBEfGC", "forum": "0lW2UBiEWN", "replyto": "0lW2UBiEWN", "signatures": ["ICLR.cc/2026/Conference/Submission9469/Reviewer_ceaz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9469/Reviewer_ceaz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861374098, "cdate": 1761861374098, "tmdate": 1762921057813, "mdate": 1762921057813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a benchmark and framework to evaluate deceptive behaviour under specific scenarios. In particular alignment failures are assessed via the comparison of chains-of-thought and model responses when in the neutral scenario vs. under additional pressure. A methodology to generate these scenarios is proposed and the responses are evaluated with LLM-as-a-judge. The paper evaluates several leading LLMs and finds that most exhibit deceptive tendencies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Benchmarking the susceptibility to pressure cues is a very interesting and relevant topic. The paper is well written and easy to follow. The methodology is mostly clearly outlined and the motivation and experimental results are clearly presented.\n\nThe dataset covers a large amount of deceptive behaviours (strategic deception, alignment faking,...) and several critical domains (military, finance,...), which makes it relevant to real world scenarios. I skimmed some examples in the dataset, which seem reasonable complex.\n\nIt is also worth noting that a human annotation and quality control was performed. The number of models evaluated is also reasonable and includes several prominent closed- and open-source models."}, "weaknesses": {"value": "I am mostly concerned about some missing details regarding the scenario seed generation, as well as the overall evaluation and depth of the discussion.\n\n**Unclear methodology for scenario seed generation:**\n\nWhile the main text and the appendix provide some details, it remains unclear how exactly the scenario seeds were generated. Additional details on this would be great, e.g. how and from where were the sources obtained. This seems crucial to reproduce the benchmark generation, a core part of the paper. While several LLMs are evaluated on the benchmark, the quality of the benchmark and generation framework itself is unclear. It is stated that quality control and human annotation was performed, but additional details regarding how exactly the samples were annotated would be helpful.\n\n**Evaluation pipeline:**\n\nThe benchmark seems to heavily rely on LLM-as-a-judge to evaluate and compare responses and chains-of-thought. While this may be fine generally (Appendix C.1 provides some evidence), it would be great to strengthen the confidence in the evaluation pipeline by performing a deeper error analysis and ablation study.\n\n**Evaluation could be strengthened:**\n\nOverall it seems that the main message of the results is that LLMs act deceptively or in a harmful manner when under pressure (some more so than others), which as far as I know had also been observed in existing literature cited in the paper. A clear comparison to existing benchmarks would help clarify the contributions. \n\nAlso, the relative gap between open- and closed-source models seems to be primarily driven by Claude sonnet 4 and 3.7 which obtain quite low results. It would be good to further support this claim by including additional closed source models, e.g. GPT-5. Some of the trends discussed in 5.3 and observed in Figure 5 may be of questionable significance. Specifically, there seems to be some variance within the u-shape regarding Deepseek. It would be great if some measure of significance/confidence across runs could be added to the Figures and Table 1. The explanations of the observed u-shape as well as the large increase in Figure 5 (right) are somewhat speculative and more evidence by contrasting distilled vs non-distilled models and MoE vs non-MoE models would provide stronger evidence.\n\n**Other points:**\n\n* Appendix E provides the full prompts for several scenarios. It would be useful to see at least one full example of scenario, response with and without pressure and the scoring of those responses.\n* Several related works are mentioned, but a clear comparison is missing. For example, how does Mesa & Mask compare to DeceptionBench?"}, "questions": {"value": "* How does your benchmark compare to existing benchmarks, e.g. DeceptionBench? What are the key questions that it provides answers to that prior works did not?\n* How were the environment scenario seeds obtained?\n* How exactly did the human annotate the data? (I read the process on appendix B.3)\n* On what data were the evaluation model results in Appendix C.1 obtained?\n* How robust are model evaluation results (with LLM-as-a-judge) to same family vs cross family evaluation? Has any error analysis been done for misclassification or an ablation study with more human involvement during the evaluation phase?\n* Have ambiguous or borderline cases been checked? If yes, what did they look like?\n* Were there any differences between human and judge agreements/disagreements across the domains or deception types?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9T1wp3M3ro", "forum": "0lW2UBiEWN", "replyto": "0lW2UBiEWN", "signatures": ["ICLR.cc/2026/Conference/Submission9469/Reviewer_K3Wq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9469/Reviewer_K3Wq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958254449, "cdate": 1761958254449, "tmdate": 1762921057544, "mdate": 1762921057544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}