{"id": "CtcKEAojoE", "number": 20464, "cdate": 1758306447157, "mdate": 1759896976299, "content": {"title": "Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model", "abstract": "Modeling multi-modal time-series data is critical for capturing system-level dynamics, particularly in biosignals where modalities such as ECG, PPG, EDA, and accelerometry provide complementary perspectives on interconnected physiological processes. While recent self-supervised learning (SSL) advances have improved unimodal representation learning, existing multi-modal approaches often rely on CLIP-style contrastive objectives that overfit to easily aligned features and misclassify valid cross-modal relationships as negatives, resulting in fragmented and non-generalizable embeddings. To overcome these limitations, we propose ProtoMM, a novel SSL framework that introduces a shared prototype dictionary to anchor heterogeneous modalities in a common embedding space. By clustering representations around shared prototypes rather than explicit negative sampling, our method captures complementary information across modalities and provides a coherent “common language” for physiological signals. In this work, we focus on developing a Pulse Motion foundation model with ProtoMM and demonstrate that our approach outperforms contrastive-only and prior multimodal SSL methods, achieving state-of-the-art performance while offering improved interpretability of learned features.", "tldr": "We demonstrate that aligning heterogeneous wearable sensor modalities, such as PPG and accelerometry, through a shared dictionary of learned prototypes is an effective pre-training strategy.", "keywords": ["biosignals", "time-series", "foundation model", "multimodal learning", "prototype-based learning", "self-supervised learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/18109d47dd3f06302921d63c6c1f174ce274db61.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents ProtoMM, a prototype-based self-supervised learning method for biosignals (ECG, PPG). ProtoMM aligns views of one sample of the same and different modalities, exploiting within- and between-modality information."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow\n- The experimental setup is clear showing the advantages of the method compared to the baselines"}, "weaknesses": {"value": "1/ Outdated baselines.\n\n1a/ Foundation models: The main weakness of the work is the fact that the baselines are outdated. There exist several modern works that use foundation models for biosignals; in all cases the procedure is different to the one proposed in this work, but we cannot understand how well the proposed method works without modern baselines. \n\n[a] Xu et. al.,  RelCon: Relative Contrastive Learning for a Motion Foundation Model for Wearable Data, ICLR 2025\n[b] Abbaspourazad, et. al., Large-scale Training of Foundation Models for Wearable Biosignals, ICLR 2024\n[c] Narayanswamy et. al., Scaling wearable foundation models, ICLR 2025 \n[d] Saha et. al., Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for Wearable Applications across Lab and Field Settings, ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies \n\n1b/ Self supervised methods: \nThere exist several more recent SSL methods operating on biosignals that could be explored or adapted. See some examples below: \n\n[e] Tag et. al., Electrocardiogram Report Generation and Question Answering via Retrieval-Augmented Self-Supervised Modeling, ICASSP 2025\n[f] Mordacq et. al., ADAPT: Multimodal Learning for Detecting Physiological Changes under Missing Modalities, MIDL 2024 \n[g] Shen et. al., CIMSleepNet: Robust Sleep Staging over Incomplete Multimodal Physiological Signals via Contrastive Imagination, NeurIPS 2024 \n\nGiven the 1a/ and 1b/ categories, right now the proposed method seems standalone without proper bibliographic discussion and without convincing baselines and comparison of other methods.\n\n2/ Modalities. \nThe paper has evaluated only two modalities, which makes claims such as “general multimodal framework” not convincing. It would be useful to have at least one higher-level modality (e.g., EDA, ECG, or text). \n\n3/ Datasets. \nThe datasets seem outdated and it seems that there exist newer and more within scope datasets: \n[i] EEVR: A Dataset of Paired Physiological Signals and Textual Descriptions for Joint Emotion Representation Learning, NeurIPS 2024\n[ii] WildPPG (A Real-World PPG Dataset of Long Continuous Recordings), NeurIPS 2024 \n[iii] Stressid: a multimodal dataset for stress identification, NeurIPS 2023 \n \n4/ Methodology\nWhile intuitively well-motivated, the paper lacks a deeper theoretical justification of why prototype consistency across modalities results in better disentanglement or regularization. Furthermore, there is no discussion on the convergence or stability of the Sinkhorn-based assignment. \n\n5/ Missing ablations\n- Several ablations are missing (hyperparam robustnes, number of prototypes, temperature, choice of \\alpha, etc). These could really help understand what works and why in the proposed method. \n- It remains unclear whether ProtoMM embeddings are better than contrastive baselines so a baseline with contrastive learning would help.. \n\n6/ Frozen encoders. \nAll downstream evaluations use frozen encoders with linear probes thus making the method dependent on the quality of the encoder. \n\n7/ Visualizations and analysis. \nAlthough there exist some visualizations, we cannot understand the importance of training with both losses neither the importance of prototypes. It would have been helpful to have these. For example, we cannot understand if the shared prototype space is truly modality-agnostic, or if some modality-specific subclusters have emerged. \n\n8/ Prototypes.\nPrototypes are the main contribution of the work but we cannot understand how they operate. Besides visualization, we would now what happens for instance when we have one dominant modality or how many we need to have or if we need additional latents to represent them or what would happen if we have longer sequences. Analysing them and having findings transferable to larger datasets and other modalities would strengthen the work.  \n\n9/ Failure cases are not discussed."}, "questions": {"value": "It would have been useful to have answers to the weaknesses above. \nSome additional questions are: \n\nQ1: it would be interesting to have the computational overhead of the Sinkhorn-based prototype updates compared to contrastive methods\n\nQ2 (W6) How does ProtoMM perform when fine-tuned end-to-end on downstream tasks, compared to linear probing?\n\nQ3 Are the learned prototypes stable across training runs, or do they vary significantly with initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TnpdKm8HNk", "forum": "CtcKEAojoE", "replyto": "CtcKEAojoE", "signatures": ["ICLR.cc/2026/Conference/Submission20464/Reviewer_1JH9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20464/Reviewer_1JH9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858936001, "cdate": 1761858936001, "tmdate": 1762933907179, "mdate": 1762933907179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces ProtoMM, a self-supervised multimodal model for time-series data that uses a shared prototype dictionary to align embeddings across modalities and capture both shared and modality-specific information. The model consistently outperforms unimodal and multimodal baselines in experiments across three datasets and downstream tasks, demonstrating strong interpretability and generalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The shared prototype dictionary provides an innovative alternative to contrastive learning methods, effectively addressing the limitations in negative pair construction in multimodal settings.\n\n2. The model considers both shared and modality-specific information, deriving a shared cross-modal representation while preserving unique modality information."}, "weaknesses": {"value": "1. Although the proposed method outperforms most baselines, the improvements are relatively small. Reporting standard deviations would help assess the statistical significance.\n\n2. In Table 2, results are shown only for the alpha=0.5 setting and two extreme cases (0 and 1). Additional intermediate values should be included to verify whether the performance trend is consistent rather than random.\n\n3. Same for Table 3. It would be great if the authors could show the results across a wider range of choices of alpha."}, "questions": {"value": "Within-modality samples and cross-modality pairs are different. The within-modality samples are derived from augmentations, while each modality is from distinct sources. Therefore, the semantic meaning of distance (or similarity) differs. \nHas the model design considered this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x8qkZvR2Uk", "forum": "CtcKEAojoE", "replyto": "CtcKEAojoE", "signatures": ["ICLR.cc/2026/Conference/Submission20464/Reviewer_hWpB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20464/Reviewer_hWpB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762475352836, "cdate": 1762475352836, "tmdate": 1762933906552, "mdate": 1762933906552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present ProtoMM, a prototype-based self-supervised framework for multimodal time-series learning, designed specifically for pulse motion foundation models using PPG (photoplethysmography) and accelerometry data. The key idea is to replace traditional contrastive alignment (which requires explicit positive/negative pair mining) with a shared prototype dictionary that serves as semantic anchors across modalities. Each signal modality is encoded separately, projected into a shared prototype space, and trained via a Multimodal Prototype Prediction Loss, combining within-modality and between-modality consistency objectives. Empirical results across three datasets and six downstream tasks show that ProtoMM often outperforms unimodal and multimodal baselines. The authors also provide qualitative evidence of interpretability, showing that learned prototypes correspond to meaningful physiological and behavioral states."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a novel adaptation of SwAV’s prototype mechanism to a multimodal, multi-view learning setting.\n- The study is highly relevant, addressing a practical and timely challenge in wearable sensing and multimodal foundation modeling.\n- The proposed objective is generalizable, featuring a tunable parameter $\\alpha$ that effectively controls the trade-off between within- and between-modality learning, with strong empirical support.\n- The experimental evaluation is comprehensive, spanning multiple datasets and benchmarks under a consistent architecture with fair and transparent comparisons.\n- The learned prototypes shows correspondence with semantically meaningful physiological patterns that enhance understanding of model behavior.\n- The paper is clearly written, well-structured, and effectively motivates the proposed approach."}, "weaknesses": {"value": "- The theoretical grounding of the approach is limited, as the benefits of prototypes over contrastive losses are supported mainly by empirical evidence. A formal analysis of how prototypes mitigate false negatives or enhance cross-modal alignment would substantially strengthen the work.\n- The mathematical and biological motivation for the proposed Multimodal Prototype Prediction Loss is underdeveloped, with only a brief intuition provided in lines 194-195. A more detailed justification would better support the design and relevance of this loss function.\n- Several closely related baseline methods (e.g., SLIP, FOCAL) are introduced only in the experimental section, rather than in the related work discussion. Including them earlier would clarify the paper’s positioning and help readers assess the novelty of the proposed approach more fairly.\n- The ablation study is somewhat shallow, as it explores only three α values (0, 0.5, and 1). Testing a finer range would yield more informative results regarding the model’s sensitivity and stability.\n- The interpretability evaluation is purely qualitative, lacking quantitative metrics such as clustering purity or correlations with physiological labels. Moreover, interpretability is not compared against baseline models, making it unclear whether these insights are unique to the proposed method.\n- Minor typos in lines 058, 073."}, "questions": {"value": "- How were the hyperparameters, such as $\\alpha$ and the encoder architecture parameters, selected? Were they tuned empirically or determined through heuristic choices?\n- In equations (5)-(7), there appear to be $MA^2$ individual within-modality losses and $M^2A^2$ between-modality losses. Would this not still lead to an imbalance between the two loss types, particularly when $\\alpha = 0.5$?\n- Could the authors clarify the definitions of $\\mathbf{z}_t$ and $\\mathbf{q}_s$ in equation (4)?\n- In Lines 361–364, why would the hypothesized advantages of the prototype-based approach not extend to unimodal settings, especially given that empirical results show contrastive methods outperforming the prototype-based variant?\n- In the interpretability experiment, could the authors provide quantitative validation showing that the learned prototypes correspond to specific physiological states?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sERJOqQYSV", "forum": "CtcKEAojoE", "replyto": "CtcKEAojoE", "signatures": ["ICLR.cc/2026/Conference/Submission20464/Reviewer_rgHi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20464/Reviewer_rgHi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762580298383, "cdate": 1762580298383, "tmdate": 1762933906156, "mdate": 1762933906156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ProtoMM, a self-supervised learning framework for multimodal time-series\ndata that addresses the limitations of traditional methods in modality alignment by using a shared\nprototype space. ProtoMM outperforms existing methods in several tasks, particularly in stress\ndetection and activity recognition. Additionally, prototype visualization enhances the model's\ninterpretability. This approach offers an innovative solution for multimodal self-supervised\nlearning"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The ProtoMM framework addresses the issue of negative sample sampling in\nmultimodal self-supervised learning by introducing a shared prototype space. Particularly in the\napplication of biosignals, ProtoMM effectively captures complementary information both within\nand between modalities, providing a novel solution.\n\n\nThe model framework is highly versatile and can seamlessly be applied to different types\nof time-series modalities."}, "weaknesses": {"value": "Although ProtoMM outperforms the existing baseline models on some metrics, its\nperformance improvement is very limited (about 0.01-0.02), and it is difficult to determine\nwhether the improvement is due to the effect of the method itself or the experimental\nrandomness;\n\n\n The paper claims that ProtoMM can simultaneously capture within-modality (unique)\nand between-modality (shared) information, but does not design a direct and objective\nexperiment to verify it. The authors only infer indirectly that the model learns on both types\nof information based on the optimal model performance when α=0.5, which lacks support."}, "questions": {"value": "How can you prove that ProtoMM can effectively capture intra and inter-modality information?\n\nHow do you demonstrate that prototyping is more effective on biological time series data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2Mu6x8To2C", "forum": "CtcKEAojoE", "replyto": "CtcKEAojoE", "signatures": ["ICLR.cc/2026/Conference/Submission20464/Reviewer_xWEy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20464/Reviewer_xWEy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762840924681, "cdate": 1762840924681, "tmdate": 1762933905652, "mdate": 1762933905652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}