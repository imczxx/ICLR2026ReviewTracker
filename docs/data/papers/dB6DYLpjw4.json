{"id": "dB6DYLpjw4", "number": 2321, "cdate": 1757058302636, "mdate": 1763738039707, "content": {"title": "Neural Mutual Information Estimation in Real Time via Pre-trained Hypernetworks", "abstract": "Measuring statistical dependency between high-dimensional random variables is\nfundamental to data science and machine learning. Neural mutual information\n(MI) estimators offer a promising avenue, but they typically require costly test-\ntime iterative optimization for each new dataset, making them impractical for\nreal-time applications. We present *FlashMI*, a pretrained, foundation model-like\narchitecture that eliminates this bottleneck by directly inferring MI in a single\nforward pass. Pretrained on large-scale synthetic data covering diverse distributions\nand dependency structures, *FlashMI* learns to identify distributional patterns and\npredict MI directly from the input dataset. Comprehensive experiments demonstrate\nthat *FlashMI* matches state-of-the-art neural estimators in accuracy while achieving\n100× speedup, can seamlessly handle varying dimensions and sample sizes through\na single unified model, and generalizes zero-shot to real-world tasks, including\nCLIP embedding analysis and motion trajectory modeling. By reformulating\nMI estimation from an optimization problem to a direct inference task, *FlashMI*\nestablishes a practical foundation for real-time dependency analysis.", "tldr": "A pre-trained attention-based model for statistical dependence quantification, accurate, fast and differentiable", "keywords": ["statistical dependence", "transformers", "hypernetwork", "mutual information"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce1e7ad9e98d38e397f3c6309c0cde188d30c194.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes FlashMI, a pretrained neural architecture for real-time mutual information (MI) estimation. Traditional neural MI estimators (e.g., MINE, InfoNCE) require dataset-specific iterative optimization, which makes them impractical for large-scale or streaming scenarios.\nFlashMI reformulates MI estimation as a direct inference task instead of an optimization problem. A hypernetwork generates critic parameters in a single forward pass, enabling efficient estimation.\nThe model adopts a dual-path architecture (joint/marginal branches) with cross-attention mechanisms consistent with the Donsker–Varadhan formulation. FlashMI is pretrained on large-scale synthetic distributions covering diverse dependency and marginal patterns, enabling zero-shot generalization to unseen data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The main contribution is conceptually elegant—the paper redefines MI estimation as a one-step inference problem instead of an optimization task. This paradigm shift offers both theoretical insight and practical value for scalable dependency estimation.\n2.he dual-path hypernetwork with cross-attention effectively models relationships between joint and marginal distributions. The inclusion of a noise-padding mechanism enables flexible handling of different input dimensions, enhancing robustness and adaptability.\n3.FlashMI achieves substantial computational gains without sacrificing estimation accuracy, making it particularly appealing for real-time and streaming applications."}, "weaknesses": {"value": "1.Limited evaluation on high-dimensional data:\nCurrent experiments focus on low- to mid-dimensional inputs (up to ~20D). The paper mentions potential scaling via slicing or fine-tuning, but provides no empirical evidence or analysis of high-dimensional performance trends.\n2.Dependence on synthetic pretraining:\nThe model’s success heavily depends on the diversity of its synthetic pretraining data. However, the paper does not quantify the variety or coverage of these distributions, making it difficult to assess robustness across real-world domains."}, "questions": {"value": "1.Is the critic parameter generation theoretically guaranteed to approximate the optimal critic, or is it purely empirical?\n2.How are the synthetic pretraining distributions designed to capture real-world dependency patterns? Are there metrics to measure their diversity or coverage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gOIeD7cmHw", "forum": "dB6DYLpjw4", "replyto": "dB6DYLpjw4", "signatures": ["ICLR.cc/2026/Conference/Submission2321/Reviewer_M54o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2321/Reviewer_M54o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916250376, "cdate": 1761916250376, "tmdate": 1762916193476, "mdate": 1762916193476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a core bottleneck of existing neural mutual information (MI) estimators: the need for costly and time-consuming iterative optimization for each new dataset at test time. To overcome this, the authors propose FlashMI, a pre-trained, foundation-model-like architecture. FlashMI reformulates MI estimation from an \"optimization problem\" to an \"inference problem.\" At its core is a dual-path, attention-based Hypernetwork. This hypernetwork takes the entire dataset (as a sequence of samples) as input and directly generates the optimal parameters for the \"critic network\" from the Donsker-Varadhan (DV) formulation in a single forward pass. FlashMI is pre-trained on large-scale, diverse synthetic data (covering various distributions and dependency structures), allowing it to learn general distributional patterns. The model flexibly handles varying input dimensions (via noise padding) and sample sizes (via attention). Extensive experiments demonstrate that FlashMI matches state-of-the-art (SOTA) neural estimators in accuracy while achieving over 100x speedup. Furthermore, it successfully generalizes in a zero-shot manner to real-world applications, such as CLIP embedding analysis and motion trajectory modeling, showing its significant potential as a tool for real-time dependency analysis."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1.\tExceptional Efficiency: The key strength is the 100x+ speedup by replacing per-dataset optimization with a single forward pass. This makes real-time neural MI estimation practical.\n2.\tNovel Architecture and Generalization: The dual-path hypernetwork is an innovative design that handles variable inputs. Its effectiveness is proven by the strong zero-shot generalization from synthetic training to real-world tasks (e.g., CLIP, motion data), which is a significant result.\n3.\tComprehensive Evaluation: The experimental design is rigorous, testing against optimization-based, pre-training-based, and traditional MI estimators."}, "weaknesses": {"value": "1.\tThe method still relies on slicing for high-dimensional data (e.g., 512-dim), which is an approximation and limits the core method's direct applicability in such settings.\n2.\tThe model's impressive inference speed comes at the cost of a very high pre-training budget (hardware and time). Its generalization is also entirely dependent on the diversity of the synthetic pre-training data."}, "questions": {"value": "- In the CLIP experiment (512-dim), the caption for Figure 4 mentions \"5-sliced MI using 25 random projections\". How does this work exactly? Does this mean $k=5$ (as in the k-sliced MI definition) or $S=25$ (the number of projections)?\n\n-  What is the practical value of the max dimension $D$ mentioned on page 6? (Appendix A.1, Alg 1 seems to imply $d_{max}=8$). Please clarify the practical $D$ used and how k-slicing works with it.\n\n- Figure 3 shows that performance (AUC) drops noticeably for small sample sizes (e.g., $n < 400$). How does FlashMI's robustness in this low-sample regime compare to MINE (which can optimize specifically for that small dataset)? Does the hypernetwork need to \"see\" a sufficient number of samples to accurately infer the distribution's properties?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pjUhzlQqMt", "forum": "dB6DYLpjw4", "replyto": "dB6DYLpjw4", "signatures": ["ICLR.cc/2026/Conference/Submission2321/Reviewer_WrB1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2321/Reviewer_WrB1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974297719, "cdate": 1761974297719, "tmdate": 1762916193133, "mdate": 1762916193133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new mutual information (MI) estimation method based on neural networks. More specifically, the proposed method aims to compute MI in real time. The main bottleneck in this problem is the time required for MI estimation. To address this issue, the authors propose using a frozen model that is separately trained on synthetic data. After training with synthetic data, mutual information is estimated using the DV representation. Through experiments, the paper shows that the proposed method compares favorably with existing approaches.\n\nThe idea of using synthetic data is interesting. However, a similar approach could be implemented simply by extending the MINE method with synthetic data. Moreover, it lacks to comparing to important previous work. Therefore, I feel that the novelty may lie more in the model architecture than in the overall framework itself. This point should be carefully verified."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using synthetic data for pretraining would be interesting.\n2. The proposed method is much faster than existing methods."}, "weaknesses": {"value": "1. DV based representation learning for mutual information is not new. For example, the following paper has already worked on MI based representation learning. \n\n   Neural Methods for Point-wise Dependency Estimation, NeurIPS 2020.\n\n2. Although the synthetic data pre-training is interesting, the approach is used in the computer vision community. Using it for mutual information is new, but it is not significantly novel."}, "questions": {"value": "1. Regarding independence testing, can the proposed method control the false positive rate?\n\n2. It seems possible to train MINE with synthetic data and then use the trained model to estimate MI, similar to the proposed method. Would the proposed approach still outperform MINE in this setting?\n\n3. Similar to Q4, I feel that the novelty of this paper mainly lies in the proposed model architecture. Could the authors provide an ablation study to support this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OWPGpQ7okY", "forum": "dB6DYLpjw4", "replyto": "dB6DYLpjw4", "signatures": ["ICLR.cc/2026/Conference/Submission2321/Reviewer_GWbp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2321/Reviewer_GWbp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762317694335, "cdate": 1762317694335, "tmdate": 1762916192570, "mdate": 1762916192570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}