{"id": "gVn0FbzvBk", "number": 12414, "cdate": 1758207631276, "mdate": 1759897511341, "content": {"title": "FedProx-based heterogeneity-aware parameter-free federated learning", "abstract": "We propose parameter-free Federated Learning (FL) algorithms based on FedProx. Learning rate-free optimization has been studied in single-node settings, with DoG and its extension DoWG exhibiting strong theoretical and empirical performance. To exploit their success in multi-node FL, we leverage a key insight: a structural similarity between the lemmas for convergence analyses of DoG/DoWG and those of the proximal point algorithm that underlies FedProx. Based on this, we propose two novel FedProx-based algorithms--FedProxLoD and FedProxWLoD--which adaptively determine the proximal weight, serving as FL analogues of DoG and DoWG. We show tight heterogeneity-aware convergence rates that explicitly reflect the impact of data heterogeneity across clients and demonstrate that the proposed algorithms can outperform DoG and DoWG as heterogeneity decreases. Through large-scale numerical experiments on both convex and non-convex models, we validate the effectiveness of the proposed methods. Notably, FedProxWLoD achieved competitive performance with pre-tuned beseline algorithms under moderate data heterogeneity settings.", "tldr": "FedProx-based heterogeneity-aware parameter-free federated learning algorithms are proposed.", "keywords": ["Federated learning", "parameter-free optimization", "FedProx"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/90cd8bf7df08e8c926ee441f5f2af404adf52ae0.pdf", "supplementary_material": "/attachment/9c41763313a694e591c8f5f2a3026085297abcfd.zip"}, "replies": [{"content": {"summary": {"value": "This paper develops a parameter-free method for federated learning focusing on FedProx. It allows dynamic adjustment of learning rates as well as the proximal-term weights based on historical gradients and losses, so that they do not need to be manually tuned. A convergence analysis is provided for convex objectives. Experiments on three datasets are conducted to showcase the competitive performance of the proposed approach compared to STOA FL algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The parameter-free approach is well motivated for federated learning, which typically requires expensive training and hyper-parameter tuning. \n\n- The reviewer took a quick read over the proofs and the convergence result looks solid. Importantly, it matches the order with centralized learning. \n\n- The paper is very well written and easy to read."}, "weaknesses": {"value": "- Assumption 6 looks problematic. When clients have heterogeneous data distributions, how can one assume there is a shared minimizer for each client's local objective? Can the author clarify on this or is this just a typo?  Also, in line 118, what does monotonic decrease mean? Is it with respect to time slot t? If so, this is a very strong assumption stating that each step the model is shifting closer to the minimizer, which is unlikely to hold. Let's take a step back and say this is true when one very lucky, but wouldn't this be a result instead of an assumption? \n\n- My major concern with the parameter-free approach (e.g., in (2)) is that, while it saves some computation on hyper-parameter tuning, it requires storage of all historical models/gradients in order to compute the hyper-parameters. This tradeoff needs to be specifically discussed and evaluated in the paper, e..g., one could use a table comparing memory usage across all benchmarks. \n\n- In Lemma 2, \\Delta is function of \\mu, but at the same time, in (6), \\mu is a function of \\Delta. Can the authors provide explicit formulas for both terms for clarify?\n\n- The reviewer is curious why this paper focuses on FedProx. The parameter-free approach seems to naturally apply to other FL approaches such as FedAvg and FedOpt. Is there a reason for building the approach based off FedProx?\n\n- For the experiment on FMNSIT, why pick a half-frozen MLP? How is the other layer (including activation function) designed? If the authors intended to use a convex model, why not use something like logistic regression?\n\n- Another major concern pertains to the current experiment results. In Table 2, all methods achieve very similar performance on the three datasets.  If we further look at Fig. 2aiii and 2ciii, the proposed method achieves the worst performance compared to baselines. While this might be due to a mild data heterogeneity, but if we look at Table 2 and Fig.3 with a larger heterogeneity, the proposed method falls behind with a larger gap to baselines. It would be more meaningful to showcase the computation saving  (as well as the memory usage) of the proposed method."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z1jXvzCf7N", "forum": "gVn0FbzvBk", "replyto": "gVn0FbzvBk", "signatures": ["ICLR.cc/2026/Conference/Submission12414/Reviewer_cLk6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12414/Reviewer_cLk6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926356812, "cdate": 1761926356812, "tmdate": 1762923307741, "mdate": 1762923307741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper adapts the DoG/DoWG parameter-free optimization methods to the federated setting by combining them with FedProx. Specifically, the proximal weight $\\mu$ in FedProx is set to be the square root of the sum of the client losses divided by how far the global model is from the start. The clients solve their client subproblems by using DoG, which then makes the entire algorithm learning-rate free and makes it so $\\mu$ does not need to be tuned. Because the method is able to adapt to the heterogeneity of the federated learning problem, it has a heterogeneity-dependent learning rate which demonstrates that it can improve as heterogeneity decreases."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The method makes a thoughtful adaptation of the DoG/DoWG-style parameter-free optimization methods to the federated setting. They are able to rigorously define the analogous quantities relevant to setting $\\mu$, and take care to ensure the communication cost is not too large as a result\n- They are able to derive a convergence rate depending on heterogeneity, which provides an option for optimization when problems are not too heterogeneous\n- Experimental results are fair and trustable, with good tuning of baselines. They are able to show that their algorithm is able to outperform other parameter-free methods and achieve results comparable to the best empirical result from tuned methods\n- The method does not require manual tuning, which is great\n- Highly readable paper, very well written"}, "weaknesses": {"value": "- There are other works that have derived heterogeneity-dependent convergence rates in FL. Should probably compare against them. For example one notable one is below but please survey the literature more thoroughly:\n\nWoodworth, Blake E., Kumar Kshitij Patel, and Nati Srebro. \"Minibatch vs local sgd for heterogeneous distributed learning.\" Advances in Neural Information Processing Systems 33 (2020): 6281-6292.\n\n- I am unconvinced about the setting for the paper. FL and on-device learning has evolved significantly since the original FedAvg paper was published, but the paper does not seem to have kept up with the developments. It is still working on assumptions and settings that were relevant 5 years ago. For example, FL deployments today rely on pretraining, LLMs, or even a combination of both, but the paper does not mention this or compare against it. Pretraining in particular is critical because it is known that it can help avoid the harmful effects of heterogeneity. See the following works\n\nNguyen, John, et al. \"Where to begin? on the impact of pre-training and initialization in federated learning.\" arXiv preprint arXiv:2206.15387 (2022).\n\nHou, Charlie, et al. \"Private federated learning using preference-optimized synthetic data.\" arXiv preprint arXiv:2504.16438 (2025).\n\nWu, Shanshan, et al. \"Prompt public large language models to synthesize data for private on-device applications.\" arXiv preprint arXiv:2404.04360 (2024).\n\n- Is communicating loss differences acceptable from a deployment perspective (privacy)? Again I think the authors should more carefully examine the motivation for the setting they study. The paper does not consider privacy and I understand it is out of the scope of the work, but you need to convince the reader that the setting is practical somehow.\n- Are there speedruns (or similar) in the FL optimization community? Without them it is hard to determine whether proposed algorithms are actual improvements. See https://kellerjordan.github.io/posts/speedrun/"}, "questions": {"value": "See weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5RMxL8WEqi", "forum": "gVn0FbzvBk", "replyto": "gVn0FbzvBk", "signatures": ["ICLR.cc/2026/Conference/Submission12414/Reviewer_DPv8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12414/Reviewer_DPv8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762022089143, "cdate": 1762022089143, "tmdate": 1762923307385, "mdate": 1762923307385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces two \"parameter-free\" FL algorithms, FedProxLoD and FedProxWLoD, inspired by two algorithms from the centralized setting: DoG and DoWG. By noticing a structural similarity between the convergence analyses of DoG/DoWG and the Proximal Point Algorithms, the paper suggests using adaptive proximal weights based on adaptive learning rates for Stochastic Gradient Descent and Gradient Descent as described in DoG and DoWG. Theoretically, the analysis claims to show that as heterogeneity diminishes, the algorithms offer improved convergence guarantees compared to DoG and DoWG. Numerical results show that the proposed \"parameter-free\" FedProx-based algorithms perform comparably to the tuned baseline algorithms under moderate and low heterogeneity."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The algorithm adopts the adaptive learning rate schemes to develop parameter-free FL algorithms with different aggregation schemes to ensure convergence. \n\nThe paper presents mildly non-conventional contributions. It incorporates the concept of \"parameter-free\" algorithms using DoG and DoWG algorithms in FL. The paper includes adaptive schemes for the proximal coefficient in FedProx and formulates the algorithm to ensure convergence by introducing double merging and calculating the loss difference. \n\nThe paper is well-written, with clearly stated assumptions, detailed notations and results, and a well-articulated motivation for the research. The theorems and results were systematically laid out."}, "weaknesses": {"value": "**Justification for poor soundness: major error; see below** \n\nThe bound in Lemma 10 is derived as $K$ approaches infinity, but we do not consider this in the non-asymptotic analysis. Intuitively, if $K$ were to approach infinity, the client's local update would drift too far from the global update, causing the client's drift to become unbounded and preventing convergence. Therefore, the bound on the difference between server and client updates in Lemma 10 should be re-examined, as they influence the overall convergence analysis. Additionally, there is no guarantee that $\\eta^\\prime\\leq \\frac{1}{\\mu^(t)}$ is sufficiently small. \n\n**The submission has limited significance**\n\nIn practice, to compute $\\Delta^{(t+1)}$ in Line 12, $f(x^{(t+1)})$ cannot be computed on the server, as data cannot be stored there; therefore, the routine outlined in Algorithm 4 is the only option. Including experiments based on Algorithm 4 would thus provide a clearer view of the empirical performance of the proposed algorithms. Comparing with baseline algorithms is unfair if the data is stored on the \nserver to compute $f(x^{(t+1)})$. Furthermore, to implement FedProx for deep neural networks, SGD-based updates are performed on \nthe clients for K rounds, as shown in Algorithm 3. This approach employs a fixed learning rate for one communication round, repeated 100 times per client, which can be relatively large, as it is determined by the server based on collective client information. How does the performance of the algorithm vary when a decaying learning rate (e.g., using cosine annealing) is used at the clients as \nopposed to the learning rate in Algorithm 3? In that case, does this \"parameter-free\" method for the learning rate $\\eta$ along with proximal coefficient $\\mu^{(t)}$ offer any benefits over a tuned learning rate at the clients?\n\nFinally, although the centralized algorithms use the term \"parameter-free\", the reviewer would like to suggest that there is no reason to carry it forward; how about calling it just \"hyper-parameter free\"?"}, "questions": {"value": "The facts in Table 1 are incorrect. FedAvg (Karimireddy et al., 2020) is analyzed assuming a smooth function and $(G-B)$ bounded gradient dissimilarity, rather than under the assumption of a $G$-Lipschitz function. The $(G-B)$ bounded gradient dissimilarity is also a measure of heterogeneity. Similarly, FedProx (Li et al., 2020) also employs the $B$-bounded dissimilarity assumption. The claim that only the proposed algorithms have a heterogeneity-aware convergence rate is unjustified. Accordingly, the derived convergence rates for FedProxLoD and FedProxWLoD are based on stronger conditions of a $G$-Lipschitz function compared to existing heterogeneity-aware FL algorithms, which are derived under the $L$-smoothness assumption. Additionally, assumption 6, which states that there exists a consistent minimiser $x^*$ for each client $f_i$, is essentially the interpolation condition discussed in the paper and is a stronger assumption. Thus, the given convergence rates are under stronger conditions of- 1) $G$-Lipschitz function, 2) interpolation condition, and 3) full gradient computation and full client participation. Based on these assumptions, the convergence rates are not tighter than those of existing rates. Could you please comment on this?\n\nThe DoG algorithm has a high probability convergence bound for the optimality gap $f(x_T)-f(x^*)$, and DoWG has deterministic convergence rates for GD with the learning rate scheme provided by DoWG; however, Theorem 1 combines these two different notions of convergence and presents a single theorem for both, which is again not factually correct. Lemma 2 shares a parallel structure with Lemma 1 and provides motivation for using DoG/DoWG-inspired schemes for the adaptive proximal coefficient. However, the similarity feels forced, as $\\mu^(t)$ is multiplied and divided by the second term on the right-hand side to create a structurally similar lemma. If, instead, the right-hand side is minimized over $\\mu^(t)$ without explicitly including it in the second term, and an adaptive formulation of \\mu^(t) is based on this, how would that affect convergence? \n\nMoreover, in classical SGD and even in FL, $\\eta$ is tuned for improved convergence guarantees. DoWG and DoG utilize the gradient norm squared and the distance to the initial point, based on the effective step size in Normalised GD, for optimal performance. What is the intuition behind using an adaptively constructed proximal coefficient?\n \nIn line 395, there is a minor typo; it is stated, “We use a batch size of 64, and examine three levels of data heterogeneity.” However, the experiments are included only for two values of $\\alpha$, i.e., $\\alpha=1,0.1$. \n\nAppendix A reproduces the proofs of DoWG without proper citations, making them redundant. The paper should include only essential parts and direct readers to find detailed proofs in the DoWG paper. The equation on line 929 is incorrect. It should be $\\nabla f_i(x_i^t)+\\frac{1}{\\eta'}(y - x_i^t)+\\mu(yx^t) = 0$. Expression in line 1165 is incorrect and should be omitted.  \n\n**REPRDOCUBILITY**: Insufficient amount of details available. Could you please justify that the information provided in the paper are sufficient for the reproducibility of the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MJHDwAOQhh", "forum": "gVn0FbzvBk", "replyto": "gVn0FbzvBk", "signatures": ["ICLR.cc/2026/Conference/Submission12414/Reviewer_gYee"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12414/Reviewer_gYee"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762354522777, "cdate": 1762354522777, "tmdate": 1762923307083, "mdate": 1762923307083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}