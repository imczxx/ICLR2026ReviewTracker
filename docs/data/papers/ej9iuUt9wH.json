{"id": "ej9iuUt9wH", "number": 23397, "cdate": 1758343167946, "mdate": 1759896816882, "content": {"title": "RPC-Bench: A Fine-grained Benchmark for Research Paper Comprehension", "abstract": "Leveraging large foundation models for document understanding has emerged as a rapidly advancing research area. Unlike general-purpose documents, research papers constitute a particularly challenging domain, as they are characterized by complex figures, detailed tables, and highly specialized scientific knowledge. However, existing benchmarks pay limited attention to evaluating the fine-grained capabilities of current models in comprehending research papers at scale. To address this gap, we propose RPC-Bench, a large-scale fine-grained question-answering benchmark constructed from review-rebuttal exchanges of high-quality academic papers, with each paper available in two input formats (pure text and rendered page images) enabling evaluation of both large language models (LLMs) and visual language models (VLMs). We design a fine-grained taxonomy aligned with the research flow of academic papers to guide annotation. We also define an elaborate LLM–human interaction annotation framework to support large-scale labeling and quality control. Following the LLM-as-a-Judge paradigm, we develop a scalable framework that evaluates models on correctness-completeness and conciseness, with high agreement to human judgment. Experiments show GPT-5 leads with a 66.54% correctness-completeness score, dropping to 35.05% after conciseness adjustment. In addition, multimodal LLMs perform better on pure text than visual–text inputs, highlighting the need for improved visual integration in scholarly document understanding.", "tldr": "We present RPC-Bench, a fine-grained multimodal benchmark for academic papers with a taxonomy-driven annotation scheme and multi-dimensional evaluation, showing that current LLMs and VLMs struggle with thorough research paper comprehension.", "keywords": ["Research Paper Comprehension", "Multimodal Benchmark", "LLM-as-Judge"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b051e5f764b8b6a308c316c9f96bc23a46ff7f82.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces RPC-Bench, a large-scale benchmark for evaluating fine-grained comprehension of academic papers. The dataset is constructed from real review–rebuttal exchanges sourced from OpenReview, and includes 4,050 papers and 46.3K question–answer pairs. The authors propose a taxonomy aligned with the research workflow (concepts, methods, experiments), support both textual and visual inputs, and adopt an LLM-as-a-Judge evaluation framework that jointly assesses correctness, completeness, and conciseness. Experiments across 19 models reveal significant performance gaps, especially for multimodal inputs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Supporting both rendered-page images and parsed text allows fair comparison between LLMs and VLMs—a valuable contribution to document AI.\n\n2. The nine-category fine-grained classification reflects a deep understanding of academic discourse and enables nuanced model evaluation."}, "weaknesses": {"value": "1. The core limitation lies in using reviewer questions as comprehension targets. By nature, reviewers ask questions precisely because the paper is unclear, incomplete, or ambiguous—not because the information is present but difficult to extract. Consequently, many questions lack ground-truth answers in the original manuscript, making them unsuitable for evaluating “comprehension.” Instead, answering them often requires external knowledge, author intent inference, or speculative reasoning—tasks fundamentally different from understanding what is explicitly or implicitly stated in the paper. This undermines the benchmark’s validity as a measure of paper understanding.\n\n2. The paper states that only the validation and test sets were manually annotated, while the training set relies on LLM-generated QA pairs. However, it provides no quantitative analysis of annotation reliability (e.g., inter-annotator agreement), no error analysis of LLM-generated questions, and no metrics on answerability (i.e., the proportion of questions actually answerable from the paper alone). The filtering criteria (“lack of substantive academic content,” etc.) are described qualitatively but lack operational definitions, raising concerns about reproducibility and dataset robustness.\n\n3. The paper mentions filtering by citation count and acceptance status but does not clarify how these choices affect question difficulty or topical bias. More critically, it does not report how many of the final QA pairs correspond to answerable queries based solely on the paper content—a key prerequisite for any comprehension benchmark."}, "questions": {"value": "1. What fraction of the QA pairs in the test set are answerable solely from the original paper (excluding the rebuttal)? Could the authors provide an “answerability” annotation or estimate?\n\n2. Did the authors measure inter-annotator agreement during human validation? If so, what were the Kappa or Fleiss’ scores for category assignment and answer correctness?\n\n3. Given that reviewer questions often target omissions or weaknesses, how do the authors justify framing RPC-Bench as a comprehension benchmark rather than a critique response or gap-filling benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GvwFWrCG3b", "forum": "ej9iuUt9wH", "replyto": "ej9iuUt9wH", "signatures": ["ICLR.cc/2026/Conference/Submission23397/Reviewer_M3V8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23397/Reviewer_M3V8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650190457, "cdate": 1761650190457, "tmdate": 1762942645281, "mdate": 1762942645281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper constructs a benchmark for real-world question-answering (QA) tasks using peer review data from OpenReview. In the data collection and filtering stage, a Large Language Model (LLM) was employed to help select eligible data. Finally, to ensure the validity and authenticity of the evaluation, a multi-dimensional LLM-as-a-Judge evaluation framework was proposed and aligned with human judgments. The experimental results demonstrate that this evaluation framework is consistent with human evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper makes significant contributions in the areas of data collection, filtering, and metric construction. Furthermore, for the proposed metrics, the authors employ a comprehensive methodology to evaluate the consistency between the LLM-as-judge and human evaluations. This process, to some extent, demonstrates the validity of the metrics."}, "weaknesses": {"value": "Leveraging question-answering (QA) data from OpenReview to evaluate the reading comprehension capabilities of models is an intriguing approach. Compared to QA datasets generated by rule-based methods, the OpenReview data is indeed more challenging and requires a model to achieve a deeper understanding of the paper's content.\n\nHowever, review questions inherently possess two characteristics that complicate their use for evaluation:\n\n1.  **Open-endedness**: For a single question, there may be multiple reasonable answers. This ambiguity poses a significant challenge to ensuring a fair and consistent evaluation.\n2.  **Information Insufficiency**: Many review questions cannot be answered by solely relying on the content of the paper. For instance, questions concerning the novelty of the work often require a broad knowledge background in the field, while specific questions about experimental details might only be answerable by the original authors.\n\nUsing such questions for model evaluation is thus inherently problematic from a fairness perspective. In fact, the evaluation of open-ended questions is a notoriously difficult problem, yet this paper dedicates limited discussion to addressing this critical issue."}, "questions": {"value": "It would be beneficial if the authors could clarify their approach to the following challenges in dataset construction:\n\n- The open-ended nature of review-style questions means they often lack a single ground-truth answer; responses from various perspectives may all be considered correct.\n- Many review questions demand external knowledge that cannot be sourced from the primary article alone. Answers may require synthesizing information from related literature or even new experimental results. Consequently, a model confined to the input paper would naturally face significant difficulties in addressing these questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dfEFIFmJiu", "forum": "ej9iuUt9wH", "replyto": "ej9iuUt9wH", "signatures": ["ICLR.cc/2026/Conference/Submission23397/Reviewer_zt2U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23397/Reviewer_zt2U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729026612, "cdate": 1761729026612, "tmdate": 1762942644127, "mdate": 1762942644127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **RPC-Bench**, a new large-scale, fine-grained benchmark designed to evaluate the comprehension capabilities of Large Language Models (LLMs) and Visual Language Models (VLMs) on research papers. The authors identify that existing benchmarks are often limited in scale, rely on synthetic questions, or lack nuanced evaluation metrics.\n\nTo address this, RPC-Bench is constructed from **4,050 high-quality academic papers** and their **46.3K authentic review-rebuttal exchanges** sourced from OpenReview. This real-world data provides complex, expert-level questions. The benchmark supports two input formats: pure text (Markdown) and rendered page images, enabling the evaluation of both LLMs and VLMs.\n\nA key contribution is the paper's **fine-grained taxonomy**, which categorizes questions based on the research workflow (Concepts, Methods, and Experiments) and their intent (what, how, why). Data was generated using a collaborative **LLM-human framework**, where LLMs decomposed and rewrote review-rebuttal pairs into a QA format, and human annotators (Master's level or higher) validated and refined the test and validation sets.\n\nFor evaluation, the paper proposes a scalable \"LLM-as-a-Judge\" protocol that measures three dimensions: **correctness**, **completeness**, and **conciseness**. These are combined into an \"F1-like\" score (harmonic mean of correctness and completeness) and an \"Informativeness\" score (F1-like penalized by lack of conciseness).\n\nExperiments on 19 models (LLMs, VLMs, DCMs, and RAG) reveal significant limitations in current SOTA systems. The best model, GPT-5, achieved a 66.54% F1-like score, which dropped to 35.05% on the conciseness-penalized \"Informativeness\" metric. A major finding is that multimodal models consistently performed worse with visual-text inputs than with text-only inputs, highlighting a critical gap in visual reasoning for scholarly documents."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Authentic and Realistic Data Source:** The benchmark's foundation in real peer review-rebuttal exchanges from OpenReview is a major strength. This moves beyond synthetic QA and captures the genuine, nuanced, and complex questions that domain experts ask, providing a more challenging and realistic test of comprehension.\n2.  **Novel and Robust Evaluation Protocol:** The paper thoughtfully moves beyond simplistic metrics like ROUGE or BERTScore, which it demonstrates are insufficient for this task. The proposed \"LLM-as-a-Judge\" framework with its multi-dimensional metrics (correctness, completeness, conciseness) offers a much more nuanced and semantically meaningful assessment.\n3. **Validation of the Evaluation Framework:** The authors validate their LLM-as-a-Judge protocol by comparing it against human preferences on a 300-instance sample. The high agreement found (e.g., ~0.85 average correlation for the GPT-5 judge) builds significant trust in the benchmark's results.\n4. **Dual-Modality Support:** The inclusion of both pure-text and rendered-page (image) inputs is a significant contribution. It allows the benchmark to evaluate and compare LLMs and VLMs directly, leading to the important finding that current multimodal models struggle to effectively integrate visual information from scholarly documents.\n5. **Fine-Grained Taxonomy:** The taxonomy, structured around the research flow (Concepts, Methods, Experiments) and \"what/how/why\" questions, is highly logical. This fine-grained categorization enables detailed error analysis, revealing *where* models fail (e.g., performing better on \"Concept Understanding\" than on \"Experimental Analysis\").\n\n6.  **Scalable Annotation Pipeline:** The \"LLM-human interaction annotation framework\" is a practical and scalable approach. Using powerful LLMs (like GPT-40) for initial data decomposition and other LLMs for rewriting, followed by human validation and refinement, balances scalability with quality control."}, "weaknesses": {"value": "1. **Unverified Training Data:** The paper explicitly states that \"only the validation and test sets were manually annotated, while the training set retained QA pairs generated by LLMs\". This means the vast majority of the QA pairs (39,203 for training vs. 6,152 for val and 2,787 for test) are of unverified, and likely lower, quality. This could negatively impact any models fine-tuned on this data, potentially teaching them to replicate LLM-generated artifacts.\n2. **Potential Domain Bias:** The data is sourced exclusively from OpenReview. While this is a high-quality source, it is heavily dominated by Computer Science and related fields (like AI/ML). The paper does not analyze or discuss the domain distribution, so the benchmark's applicability to research papers from other domains (e.g., life sciences, humanities, social sciences) is unclear. Moreover, not all reviews are published, so the potential selection bias is not mentioned nor mitigated in this case.\n3. **Artificial Input Constraints:** The methodology involves practical but significant truncations. Text inputs are \"truncated if it exceeds the model's context window\", and image-based inputs are limited to the \"first 15 pages\". This means the benchmark does not fully test whole-document comprehension, as relevant information may appear in appendices or beyond the 15-page or context-window limit.\n4. **Ambiguity in the \"Informativeness\" Metric:** The \"Informativeness\" score is defined as $F1\\text{-like} \\times (\\text{Conciseness} / 5)$, which heavily penalizes verbosity. However, the paper's own analysis notes that models sometimes produce \"overly long outputs\" that consist of \"repetitive, non-informative text\" as a *failure mode*. The finetuning analysis also shows conciseness is more easily learned than correctness or completeness. This suggests that heavily coupling the F1-like score with conciseness might be an overly aggressive penalty that conflates"}, "questions": {"value": "**Questions for the Authors:**\n\n1.  **On the Quality of the Training Data:** You state that only the validation and test sets were manually annotated, while the 39.2K pairs in the training set consist of LLM-generated data. What analysis was performed to validate the quality of this large, unverified training set? How can we be sure that models fine-tuned on this data are not simply learning to replicate the specific generative artifacts or failure modes of the LLMs (GLM-4-Plus and DeepSeek-V3) used to create it, rather than learning true research comprehension?\n\n2.  **Rationale for the \"Informativeness\" Metric:** The \"Informativeness\" metric is defined as $F1\\text{-like} \\times (\\text{Conciseness} / 5)$, which results in a significant score drop (e.g., GPT-5 from 66.54% to 35.05%). Your fine-tuning analysis suggests conciseness is \"more readily learn[ed]\" than correctness or completeness. Why did you choose to penalize the primary F1-like score so heavily with conciseness, rather than reporting it as a separate, complementary metric? Does this not risk conflating stylistic adherence with core comprehension?\n\n3.  **Impact of Input Truncation:** You note that text inputs are truncated if they exceed the context window and image inputs are limited to the first 15 pages. How frequently did this truncation occur for the models tested? How can this benchmark claim to test full-paper comprehension if the model may be prevented from accessing relevant information located in appendices or later sections of the paper, which are often critical for answering detailed experimental or methodological questions?\n\n5.  **Interpreting the VLM Performance Drop:** A key finding is that VLMs perform significantly *worse* with multimodal inputs than with text-only inputs. This is counter-intuitive, as figures and tables should provide *additional* grounding. Your own case study (Example 2) highlights the necessity of multimodal grounding. What is your hypothesis for this performance collapse? Is it a failure of the models to parse rendered text, an inability to integrate visual information with long-range textual context, or an artifact of the 15-page limit on visual inputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0EIDcWd6Ho", "forum": "ej9iuUt9wH", "replyto": "ej9iuUt9wH", "signatures": ["ICLR.cc/2026/Conference/Submission23397/Reviewer_yz9y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23397/Reviewer_yz9y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910818168, "cdate": 1761910818168, "tmdate": 1762942643820, "mdate": 1762942643820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RPC-Bench, a fine-grained benchmark for evaluating LLMs and MLLMs in research paper comprehension. The dataset is built from OpenReview review–rebuttal interactions and covers nine reasoning categories reflecting different aspects of research understanding. This paper also propose an LLM-as-a-Judge evaluation framework based on correctness-completeness and conciseness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper uses authentic and reliable data sources, constructing the benchmark from OpenReview review–rebuttal dialogues rather than synthetic data, and filtering for high-quality papers.\n2. It defines a nine-category framework that comprehensively covers multiple cognitive dimensions of research understanding.\n3. Beyond traditional evaluation metrics, this paper introduces a three-dimensional assessment combining correctness, completeness, and conciseness to better capture model performance."}, "weaknesses": {"value": "1. The experiment analysis is not sufficiently in-depth. Although the benchmark defines nine categories, the experiments only report scores and brief observations without detailed analysis of each category. Moreover, the paper compares LLMs and VLMs but does not analyze why RAG-based methods fail to show advantages.\n2. The motivation emphasizes that research paper comprehension requires domain knowledge and deep reasoning, and the taxonomy covers methodological and analytical capabilities. However, the experiments do not isolate or quantify the effect of domain-specific knowledge, making it unclear whether performance gaps stem from reasoning difficulty or knowledge limitations.\n3. The case study is too shallow. It lacks representative failure examples and does not illustrate model errors in different resources. More interpretable examples of LLM-as-a-Judge scoring would strengthen the analysis beyond the simple prompt examples in the appendix."}, "questions": {"value": "Please refer to the Weakness. Additionally,\n1. Can the authors provide statistics on the field diversity of papers included in RPC-Bench?\n2. Are there any plans to expand RPC-Bench beyond computer science papers, or to include cross-disciplinary samples to test generalization of research comprehension?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1vTUQDP0jo", "forum": "ej9iuUt9wH", "replyto": "ej9iuUt9wH", "signatures": ["ICLR.cc/2026/Conference/Submission23397/Reviewer_W4at"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23397/Reviewer_W4at"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004166266, "cdate": 1762004166266, "tmdate": 1762942643562, "mdate": 1762942643562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}