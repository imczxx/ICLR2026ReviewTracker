{"id": "vfbeleLBWv", "number": 22911, "cdate": 1758336998948, "mdate": 1759896840321, "content": {"title": "Trust The Typical", "abstract": "Current approaches to LLM safety fundamentally rely on a brittle cat-and-mouse game of identifying and blocking known threats via guardrails. We argue for a fresh approach: robust safety comes not from enumerating what is harmful, but from \\emph{deeply understanding what is safe}. We introduce \\textbf{T}rust \\textbf{T}he \\textbf{T}ypical \\textbf{(T3)}, a framework that operationalizes this principle by treating safety as an out-of-distribution (OOD) detection problem. T3 learns the distribution of acceptable prompts in a semantic space and flags any significant deviation as a potential threat. Unlike prior methods, it requires no training on harmful examples, yet achieves state-of-the-art performance across 18 benchmarks spanning toxicity, hate speech, jailbreaking, multilingual harms, and over-refusal, reducing false positive rates by up to 40x relative to specialized safety models. A single model trained only on safe English text transfers effectively to diverse domains and over 14 languages without retraining. Finally, we demonstrate production readiness by integrating a GPU-optimized version into vLLM, enabling continuous guardrailing during token generation with less than 6\\% overhead even under dense evaluation intervals on large-scale workloads.", "tldr": "Our paper presents T3, an efficient, out-of-distribution-based safety method that models the features of \"safe\" prompts to achieve state-of-the-art performance in detecting jailbreaks and toxic content while mitigating overrefusal.", "keywords": ["LLM Safety", "Out-of-Distribution Detection", "Jailbreaking", "Representation Learning", "Selective Generation", "Anomaly Detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/18189cc841f3c452b4270a119c242ca121515d0b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces T3, a proactive guardrail framework that learns the distribution of safe text rather than listing harmful content, enabling it to detect out-of-distribution (OOD) inputs or generations that deviate from safety norms. Using three sentence embedding models, T3 computes per-point PRDC (Precision, Recall, Density, Coverage) metrics, which are then processed through Gaussian Mixture Models (GMM) or One-Class SVMs (OCSVM) to assign anomaly scores, supported by theoretical analysis on normalization and density shifts. Evaluated on 18 benchmarks, T3 achieves state-of-the-art AUROC and dramatically lowers FPR@95 (up to 40× reduction), while showing strong zero-shot generalization across domains and languages using only English safety data. Integrated into vLLM as an online guardrail, it adds minimal overhead (1.5–6%) even with frequent evaluations, demonstrating both effectiveness and practicality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### 1. Clear and well-motivation\nThe paper elegantly reframes safety detection as a typicality-based OOD problem, avoiding reliance on ad-hoc harm taxonomies. The probabilistic formalization (Figure 1, §2) is conceptually clean and mathematically grounded.\n\n### 2. Strong empirical gains\nAcross 18 benchmarks, T3 improves AUROC by +8–9 points and reduces FPR@95 by ~17 points over the best prior models (PolyGuard, LlamaGuard). Notably, it reaches AUROC ≈ 0.98 and FPR@95 ≈ 0.02 on toxic/adversarial datasets (Tables 1–2)."}, "weaknesses": {"value": "### 1. Incomplete Design Justification (PRDC Architecture)\nWhile extending PRDC/Forte from vision to text is conceptually reasonable, the core architecture—multi-embedding fusion, kNN-based PRDC computation, and GMM/OCSVM aggregation—resembles a recombination of existing OOD recipes rather than a principled innovation.\nThe paper lacks a necessity or sufficiency analysis for its key design choices: Do all four PRDC metrics (Precision, Recall, Density, Coverage) need to be used? and Which of them actually drive safety detection performance?\nAlthough the authors mention that using only Precision + Density reduces accuracy (Appendix C.5), no quantitative contribution analysis or statistical significance test is provided. Without such evidence, the marginal utility of each PRDC component remains unclear.\n\n### 2. Weak Theory–Calibration Linkage\nThe theoretical results—particularly the expectation bounds and coverage gap formalized in Theorem A.2 and equations (1)/(2)—are not concretely tied to how thresholds or scores are calibrated in practice.\nThe paper merely states that final scores are derived via “negative log-likelihood under GMM/OCSVM, followed by sigmoid normalization” (§ 3 Implementation), yet it never explains how this process determines the FPR@95 target.\nIt is also ambiguous whether thresholds are globally shared or per-benchmark tuned using validation data, which has direct implications for fairness and data leakage.\nA transparent cross-validation protocol or calibration curve analysis is needed to bridge the theoretical guarantees with operational thresholding.\n\n\n### 3. Baseline Implementation detail (Vision → Text)\nMany vision-based OOD baselines were re-engineered for text by training auxiliary binary logistic classifiers to produce logits, weights, or pseudo-gradients (§ B.1).\nThis conversion departs from each method’s original assumptions and introduces additional tuning flexibility that may compromise fairness.\nConsequently, the paper’s claim that baseline methods “catastrophically fail” might reflect architectural mismatch rather than an inherent limitation of the baselines themselves.\nA fairer evaluation would involve text-native OOD detectors—for example, Mahalanobis, MSP, Energy, or kNN methods implemented on pretrained language classifiers—to isolate the effect of modality rather than reimplementation choices.\n\n### 4. Fairness of Safety-Model Evaluation\nOpen-source safety systems (PolyGuard, DuoGuard, LlamaGuard, Omni, Perspective) are highly sensitive to input formatting, including prompt–response structure, sequence length, tokenizer, and language code.\nWhile the authors claim a “standardized score normalization” procedure (§ B.2, B.3), they omit critical configuration details—thresholding, tokenization, multilingual aggregation—which can drastically affect performance.\nFor instance, PolyGuard was evaluated by attaching an identical dummy response to every prompt (§ B.3), a setup that may advantage or disadvantage the model relative to its original prompt-only configuration.\nTo ensure comparability, the paper should report both (a) results under its standardized setup and (b) each model’s recommended configuration from the original publications, including prompt template, context length, and language parameters."}, "questions": {"value": "Please refer to the Weaknesses section for detailed comments.\nIn addition, the figure legends, x-axis, and y-axis labels are difficult to read and should be improved for clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wZVwSUuNP1", "forum": "vfbeleLBWv", "replyto": "vfbeleLBWv", "signatures": ["ICLR.cc/2026/Conference/Submission22911/Reviewer_YTSa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22911/Reviewer_YTSa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540829541, "cdate": 1761540829541, "tmdate": 1762942434253, "mdate": 1762942434253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article introduces the Trust The Typical (T3) framework, which treats harmfulness detection as an OOD detection problem. By learning the distribution of safe prompts, it is capable of detecting unsafe prompts as their distribution differs significantly from safe prompts. T3 is built on top of the FORTE framework, utilizing three sentence transformers. Each embedding returned by a sentence Transformer is then used to calculate four per-point metrics for manifold estimation. This gives $4K$ features, where $K=3$ in this work, which are utilized as features for GMM and One-Class SVM. The T3 framework is evaluated on a wide range of harmfulness and benign datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* T3 is evaluated on many datasets and compared to a wide range of alternative methods, showing its advantage over other methods.\n* The usage of manifold per-point PRDC metrics and treating harmfulness is a novel idea for addressing safety in LLMs.\n* The work is well written and is easy to follow."}, "weaknesses": {"value": "* I find it confusing that the T3 method is bolded in tables 1 and 2, even when it is not the best-performing method.\n* The Authors do not provide enough details of how FPR@95TPR is calculated. Some benchmarks that the Authors use only have examples of one class, making the calculation of this metric impossible.\n* The Authors have not utilized the newest guard models, such as WildGuard[1] and newer versions of Llama Guard.\n* T3 is not evaluated on more demanding safety datasets such as WildGuardMix[1].\n* For LlamaGuard, the logits used for the calculation of AUROC are based on the number of violated categories instead of using the logits of the token “safe,” which, in my opinion, would be a better quantification of the uncertainty of this model.\n\n\n[1] Han, Seungju, et al. \"Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms.\" Advances in Neural Information Processing Systems 37 (2024): 8093-8131."}, "questions": {"value": "* How is the threshold for FPR@95TPR calculated on a datasets that only contain safe or harmful examples?\n* I would like the Authors to provide updated scores for LlamaGuard when using the probability of “safe” token for the calculation of AUROC.\n* Can the Authors explain why, for other OOD methods, they have only utilized Qwen3-Embedding-0.6B instead of other models?\n* Why is XSTest AUROC much higher in Figure 3 than in Table 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QL1u3urSJv", "forum": "vfbeleLBWv", "replyto": "vfbeleLBWv", "signatures": ["ICLR.cc/2026/Conference/Submission22911/Reviewer_vwwn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22911/Reviewer_vwwn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816028875, "cdate": 1761816028875, "tmdate": 1762942434021, "mdate": 1762942434021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new approach to language model safety, arguing that robustness should come from understanding what is safe rather than identifying what is harmful. The authors present Trust The Typical (T3), a framework that formulates safety as an out-of-distribution (OOD) detection problem. T3 learns the distribution of acceptable prompts in a semantic space and flags significant deviations as potential threats. Unlike previous methods that rely on harmful data for training, T3 uses only safe examples and achieves state-of-the-art performance across 18 benchmarks covering toxicity, hate speech, jailbreaking, multilingual harms, and over-refusal, reducing false positive rates by up to 40 times compared to specialized safety models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written.\n2. The theoretical analysis is grounded."}, "weaknesses": {"value": "1. If harmful prompts are phrased in statistically typical language and differ only in minor wording, T3’s OOD detector may fail to flag them, missing context-dependent cases.\n2. T3’s performance depends on how comprehensively the safe corpus captures benign query diversity; overly narrow or biased data may lead to false alarms on novel but harmless inputs. \n3. T3 detects distributional anomalies. Could some forms of undesired content be statistically typical and thus fail to trigger an OOD alarm, even though they are clearly against policy?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "je2BMECeRf", "forum": "vfbeleLBWv", "replyto": "vfbeleLBWv", "signatures": ["ICLR.cc/2026/Conference/Submission22911/Reviewer_fCqB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22911/Reviewer_fCqB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867302414, "cdate": 1761867302414, "tmdate": 1762942433792, "mdate": 1762942433792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed to frame LLM safety from OOD perspective which significantly advances the performance of existing methods. The paper also demonstrated potential integration with vLLM for minimum over-head usage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. the paper is well written and easy to follow\n2. the new perspective proposed by the paper is quite novel\n3. extensive experiments show that the proposed method is very effective and significantly outperforms baseline methods on various benchmarks.\n4. integration into vLLM shows promising application of the method for production use."}, "weaknesses": {"value": "1. it may be hard for the method to work well with datasets with a large amount of borderline prompts such as the ones showed in the paper or attack methods that intentionally resemble those in-distribution ones such as [1].\n\n\n\n\n[1] Luo, Xuan, et al. \"A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness.\" arXiv preprint arXiv:2509.14297 (2025)."}, "questions": {"value": "1. how does the method work for attack methods such as [1]\n2. did the authors see any problem of directly using the sentence embedding, will utilizing a lightweight LLM enhance the prompt (such as generating some extract analysis info) help with the detection process?\n\n[1] Luo, Xuan, et al. \"A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness.\" arXiv preprint arXiv:2509.14297 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XdVZhnGKCf", "forum": "vfbeleLBWv", "replyto": "vfbeleLBWv", "signatures": ["ICLR.cc/2026/Conference/Submission22911/Reviewer_cqPh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22911/Reviewer_cqPh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983729152, "cdate": 1761983729152, "tmdate": 1762942433621, "mdate": 1762942433621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}