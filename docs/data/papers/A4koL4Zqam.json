{"id": "A4koL4Zqam", "number": 14312, "cdate": 1758232713262, "mdate": 1759897377496, "content": {"title": "TOM-SWE: User Mental Modeling For Software Engineering Agents", "abstract": "Recent advances in coding agents have made them capable of planning, editing, running, and testing complex code bases. Despite their growing ability in coding tasks, these systems still struggle to infer and track user intent, especially when instructions are underspecified or context-dependent. To bridge this gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary software-engineering (SWE) agent with a lightweight theory-of-mind (ToM) partner agent dedicated to modeling the user's mental state. The ToM agent infers user goals, constraints, and preferences from instructions and interaction history, maintains a persistent memory of the user, and provides user-related suggestions to the SWE agent, while preserving privacy and minimizing context window load. In two software engineering benchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task success rates and user satisfaction. Notably, on the stateful SWE benchmark, a newly introduced evaluation that provides agents with a user simulator along with previous interaction histories, ToM-SWE achieves a substantially higher task success rate of 59.7% compared to 18.1% for OpenHands, a state-of-the-art SWE agent. Furthermore, in a three-week study with professional developers using ToM-SWE in their daily work, participants found it better aligned with their intent and useful 86% of the time, underscoring the value of stateful user modeling for practical coding agents.", "tldr": "", "keywords": ["agents; software engineering; theory of mind"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/753cf3a4d9c2d1367368603dac3616211987e2bc.pdf", "supplementary_material": "/attachment/f4e9e1fd021c5b65180ec0adb84766d2d638e169.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a framework for incorporating theory-of-mind modeling into a software engineering agent (ToM-SWE). The approach involves delegating to a special theory-of-mind agent for intent modeling and storing user preference memory. The framework is evaluated on ambiguous SWE-bench and a newly introduced benchmark, stateful SWE-bench, as well as a live user study. The stateful SWE-benchmark is created by mining user profiles from real user interactions on the OpenHands platform and creating a user simulator."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper makes a good case for improving user modeling for interactive agents software engineering scenarios.\n- The paper establishes simple but meaningful categories for common user preferences in software engineering agents.\n- The proposed augmentation with the ToM agent performs well on interactive SWE-bench variants, as well as receiving good feedback in an online trial with human software engineering."}, "weaknesses": {"value": "- The paper claims that the advantage of the two agent solution is ”reduced context distraction and specialized optimization”. This seems plausible but I believe the paper provides limited evidence beyond the simple RAG baseline (which already performs quite well with Claude 4). For example, I’d imagine one could prompt the main agent better to think about user intents, or the user profile analysis could be an offline step (aggregating past sessions +  user profile → updated user profile), that is included in the context of the main coding agent.\n- The results of the online user study is problematic without a baseline or control group that uses the regular CodeAct product. Specifically, metrics like accept/partial/reject rates are hard to interpret without knowledge of the baseline."}, "questions": {"value": "- Do the two agents ever communicate more than once for a single user query? I.e. how often does the agent call the `consult_tom` function in response to a user query? And does the response differ when called multiple times for the same user query?\n- It would be really helpful to see a complete timeline of the back-and-forth between the main agent and the ToM agent across multiple user sessions.\n- Can I interpret ambiguous SWE-bench to be in some sense a special case of stateful SWE-bench but with a fixed and particular “multi-step” type of user profile?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RFiNZRZJaB", "forum": "A4koL4Zqam", "replyto": "A4koL4Zqam", "signatures": ["ICLR.cc/2026/Conference/Submission14312/Reviewer_cL3T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14312/Reviewer_cL3T"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670590910, "cdate": 1761670590910, "tmdate": 1762924749690, "mdate": 1762924749690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. Multi (paired) agent SWE-harness, one managing user goals/memory/preferences from interaction history, with the other agent managing actual task performance but taking in instruction/context from ToM agent.\n\n1a. In practice, this looks like extra verification and receiving extra guidance from a supervising agent.\n\n2. Contribution of Stateful SWE benchmark, which incorporates an LLM powered user simulator on top of SWE-bench style issues, where agents are asked to resolve the issues while also measuring user satisfaction. \n\n2a. The authors collect sessions to generate 15 “user” profiles and use GPT-5 to transform the SWE-bench issue statement into a user instruction aligned with a given user profile. This user instruction is used as the new initial issue statement into SWE-bench style evaluation, and the SWE-agent is allowed interaction with the simulated user as well (Line 129).\n2b. In addition to resolution rate, this benchmark also measures user rating and satisfaction (Figure 3)\n\n3. Evaluation on stateful SWE benchmark and ambiguous SWE-bench using the OpenHands platform."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel hierarchical agentic architecture that helps improve user satisfaction on SWE-style tasks.\n2. Contribution of a new benchmark, Stateful SWE benchmark, which evaluates how well agents sustain meaningful interactions over time (evaluates long-term memory demands) with an interesting user-simulator based approach.\n3. Human study with real-world developers to validate ToM agent’s importance is very novel and shows strong results of ToM and user satisfaction."}, "weaknesses": {"value": "1. From my read of Section 3, I could not immediately tell what the difference of stateful SWE-bench from SWE-bench was in terms of problem_statement or other task inputs and outputs: I think this section would benefit from a diagram showing how SWE-bench issues were mapped and a specific example of what an instance of the mapping looks like (i.e. is the problem_statement modified and if so, in what way?).\n\n1a. Also, how big is this new benchmark, is it 15 x 500 = 7500 total instances? If that’s the case, it seems like if a base model can already solve an instance in SWE-bench, there may be some bias or non-representative performance in the modified SWE-bench (may not be the case, but more clarity in the main paper here would help given it’s a core contribution).\n\n2. I understand why one would use ambiguous/stateful SWE-bench type benchmarks to evaluate user intent (as normal SWE-bench does not really evaluate interactive user interaction or long-term memory). However, the paper presents a very core contribution of a huge increase of ToM-CodeAct over CodeAct, which doesn’t seem that surprising for the following:\n\n2a. My mental model is that, seems that most of CodeAct (and likely CodeRAGAct) underperformance here is strictly from the prompt being somehow underspecified in this new benchmark and CodeAct can’t interact with the user for clarification or retrieve previous trajectories?\n\n2b.Could the authors elaborate more on this point: specifically (1) if a simulated user is part of the benchmark and if at evaluation time, the agent can interact with this and (2) whether CodeAct tries to interact with the simulated user? If not, this would attribute the under-performance to ambiguity in the initial problem statement, rather than ToM’s architecture. \n\n3 The paper lacks deeper analysis into the tradeoffs between user satisfaction and task resolution.\n\n3a. In the paper you mentioned that “user satisfaction does not equal solving the task”: this seems like there are two modes: (1) “we solve the task but in a way that makes the user unhappy” and (2) “we don’t solve the task but the user is happy”? (1) seems more harmless, but (2) seems considerably more risky: is higher really better here since the user might be fooled into thinking the task is solved?\n\n3b. Is there correlation analysis between resolution rate and user satisfaction to clarify this more?"}, "questions": {"value": "1. Diversity of 15 developer profiles: is there selection bias here from actual programmers, or analysis to observe how diverse these are in practice? \n2. Nit: Figure 5 is hard to read and feels like it would be easier to read if inverted in color (darker/more intense color is higher number)\n3. Are there numbers comparing CodeAct to ToM acceptance numbers (or ToM to any baseline accept numbers)? The paper seems to primarily only present accept or reject, would be good to contextualize this against normal accept/reject numbers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eb8jc09b0s", "forum": "A4koL4Zqam", "replyto": "A4koL4Zqam", "signatures": ["ICLR.cc/2026/Conference/Submission14312/Reviewer_ofCv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14312/Reviewer_ofCv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889633705, "cdate": 1761889633705, "tmdate": 1762924748884, "mdate": 1762924748884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides:\n\n- A multi-agent setup that includes a SWE agent paired with a ToM (theory of mind) agent, where the latter attempts to help to resolve ambiguities and improve the adherence of the SWE-agent to coding styles and preferences of the user.\n- A new benchmark \"stateful SWE-bench\"\n- A human study that tests the ToM-SWE agent with real human developers"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The issue that is addressed by the contribution is relevant: Human work and interaction with agents is sequential, and having agents learn and persist user preferences across individual tasks is an important part.\n\nTo measure performance on sequential tasks, the paper introduce a new benchmark SWE-bench sequential, that allows to study tasks in sequence, and to model interaction with an opinionated human developers.\nWhile some details are still unclear to me, this seems like an original and useful contribution."}, "weaknesses": {"value": "## Readability\n\n* For some time I did not understand what the actual score from Stateful SWE-bench score as shown in Fig. 4 actually is. The paper states (Section 3) it's the \"user simulator satisfaction scores\" and points to appendix A.6.2, but A.6.2 lists 5 different scores. But then in Fig. 4., what is shown (if I follow the main text) is actually the unittest based resolution rate and the \"satistfaction score\" is an extra score that is shown in Table 1. This could be make clear.\n* Fig 1: I really like the screenshots, but I still don't understand what's happening above with the arrows (why is the normal SWE-agent setup connected by an arrow to the ToM-SWE setup? Aren't they two separate examples?)\n* Fig 5: Had a hard time to understand what the numbers mean here. Maybe there's also a few simpler things you could have considered, for example by just comparing the averages of user satisfaction for the case of resolved vs unresolved task?\n* Fig 6: Very hard to understand (especially without reading the main text in detail): I assume x axis is avg. ToM cost persession as opposed to the total cost per session? Is the dashed orange line a linear regression? I also think the plot might be easier to understand when not color coding model providers (the information is already in the labels) and potentially removing the bar chart inset. Would also be good to mention which resolved score this is in the plot fig description."}, "questions": {"value": "* How does the performance of the ToM SWE-agent compare to just having a CLAUDE.md file with the user profile that is attached to the conversation history? Perhaps even with a setup where the agent is told it can also edit the same file to incorporate user feedback (and is told to do so at the end of each trajectory). I wonder if a simpler single-agent memory system without a multi-agent setup can lead to similar success.\n* In the Ambiguous SWE-bench paper, there was a helpful plot showing resolve rates for their benchmark for three settings hidden (underspecified), interaction (agent gets to reqeust information), and full (fully specified). A similar plot might be helpful for the sequential benchmark as well. \n* The appendix is very loose on details about the stateful SWE-bench benchmark. Without showing the prompt templates it's not possible to really understand what the user simulator satisfaction score is. Ideally, I would like to also see some concrete example outputs of the LM judge.\n* It would also be very helpful to include a complete example of a developer profile instead of highly abbreviated summaries.\n* Section 5: Do I understand it correctly that the user triggers the TOM agent explicitly with `/tom_give_suggestions` but the SWE agent also can do it? For the data shown in Fig. 7 how many times did the SWE agent ask and how often did the user explicitly request the tom agent? (and I assume even if the SWE agent requests that information itself, the user is still prompted to accept/modify/reject?) \n* I did not understand the categories (Code understanding, development, etc.). Is this the overall task the TOM-SWE agent is given? Or is this specific to the interaction between the ToM and the user. \n* It seems like the acceptance rate of ToM-SWE agent consultations only captures whether or not these conversation replies were deemed correct by the users, but not if they had a significant impact on the acceptance of edits that were finally made by the SWE agent. Or in other words, do you have any metrics from the human study that show the overall satisfaction rate of TOM-SWE agent compared with the normal CodeAct satisfaction rate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ym2rEeFy5O", "forum": "A4koL4Zqam", "replyto": "A4koL4Zqam", "signatures": ["ICLR.cc/2026/Conference/Submission14312/Reviewer_XBeR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14312/Reviewer_XBeR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958844179, "cdate": 1761958844179, "tmdate": 1762924748525, "mdate": 1762924748525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}