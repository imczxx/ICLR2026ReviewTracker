{"id": "3MyTiaYpVs", "number": 14939, "cdate": 1758245853236, "mdate": 1759897340311, "content": {"title": "Randomized Feature Squeezing against  Unseen  $ {l_p} $ Attacks without Adversarial Training", "abstract": "Deep learning has made tremendous progress in the last decades; however, it is not robust to adversarial attacks. The most effective approach is perhaps adversarial training, although it is impractical because it requires prior knowledge about the attackers and incurs high computational costs. In this paper, we propose a novel approach that can train a robust network only through standard training with clean images without awareness of the attacker's strategy. We add a specially designed network input layer, which accomplishes a randomized feature squeezing to reduce the malicious perturbation. It achieves excellent robustness against unseen ${l_0,l_1,l_2}$ and $ {l_\\infty} $ attacks at one time in terms of the computational cost of the attacker versus the defender through just 100/50 epochs of standard training with clean images in CIFAR-10/ImageNet. The thorough experimental results validate the high performance. Moreover, it can also defend against unlearnable examples generated by One-Pixel Shortcut which breaks down the adversarial training approach.", "tldr": "", "keywords": ["Randomized Feature Squeezing", "Unseen  $ {l_p} $ Attacks"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76a1b5dee87f4e47c1169d7b514a16c4ea2d521e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "For more effective and efficient defense against adversarial attacks, this paper introduces a novel approach that can train a robust network only through standard training with clean images without awareness of the attacker’s strategy. The proposed method achieves excellent robustness against unseen attacks at one time in terms of the computational cost of the attacker versus the defender through just 100/50 epochs of standard training with clean images in CIFAR-10/ImageNet."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to understand.\n2. Experiments have been conducted on both CIFAR-10 and ImageNet."}, "weaknesses": {"value": "1. The writing quality needs improvement.\n2. The description in Section 4.1 is too subjective, lacks theoretical or comprehensive empirical support, and shows little innovation.\n3. The font size in Table 2 is too large, causing the numbers to merge together, which makes it difficult to read.\n4. The results in Table 2 are confusing. All accuracy are shown as xx.x0%. It seems that the evaluation is performed on 1k data instead of 10k for CIFAR-10 and 5k for ImageNet, which reported in the paper.\n5. Although the paper proposes an interesting idea, the evaluation is unreliable. Since random factors (Gaussian noise) are introduced in the input layer, gradient-based attacks may fail. The method should be re-evaluated using AutoAttack within the rand version.\n6. The paper lacks comparison with more advanced methods, such as diffusion-based AP methods."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WKvab6tYoO", "forum": "3MyTiaYpVs", "replyto": "3MyTiaYpVs", "signatures": ["ICLR.cc/2026/Conference/Submission14939/Reviewer_NjGZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14939/Reviewer_NjGZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760613296454, "cdate": 1760613296454, "tmdate": 1762925278286, "mdate": 1762925278286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an input obfuscating layer to defend against unseen $l_p$ bounded adversarial attacks. The obfuscating layer is simplified with a sign function during test time. The paper shows the effectiveness with WideResNet and ConvNeXT on CIFAR-10 and ImageNet-1k."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is computationally efficient. Its training is almost as same as standard training with clean images.\n- The proposed method shows effectiveness against different attacks while considering obfuscated gradients.\n- The proposed method also shows effectiveness against unlearnable examples.\n- The paper provides the difference between test-time input transformation and training-time one as ablation studies.\n- The paper explains the importance of randomness in its method by using a deterministic model."}, "weaknesses": {"value": "- Although the proposed defense is computationally efficient, there is about 5% clean accuracy drop and lower robust accuracy, especially for $l_2$ and $l_\\infty$ compared to adversarial training on ImageNet-1k.\n- Although BPDA is embedded by default in the evaluation, the reviewer thinks adaptive attacks should be discussed.\n- By design, existing adversarial attacks are ineffective with a small noise budget due to extreme squeezing. There are no results with higher adversarial budgets. The reviewer thinks that if using even 8/255 instead of 4/255, the existing attacks can be more successful.\n\nMinor:\n- The writing can be improved. In its current form, the paper uses more spoken English. The community may prefer a formal tone in the paper.\n- In the abstract, 100/50 may appear to be 100 divided by 50. It is better to say 100 or 50 epochs.\n- In Table 3, Clean and BPDA are a bit confusing, although Clean is for the column and BPDA is for the row. It is better to restructure the tables to be clearer.\n- In the caption of Table 4, there should be a space between training and (N.T).\n- Please take care of all punctuation."}, "questions": {"value": "- In Table 4, are the training transformation and the test transformation in the evaluation the same? N.T trained models for N.T in attacks and T.T for T.T? Have you tried the attacks in N.T and then the resulting adversarial examples in T.T? If so, what is the performance difference?\n\n- Have you tried the proposed defense on bigger models? The reviewer wonders what feature squeezing will do to bigger models. How about vision transformers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bCUnfiruip", "forum": "3MyTiaYpVs", "replyto": "3MyTiaYpVs", "signatures": ["ICLR.cc/2026/Conference/Submission14939/Reviewer_hGLu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14939/Reviewer_hGLu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629255745, "cdate": 1761629255745, "tmdate": 1762925277464, "mdate": 1762925277464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a randomized smoothing method which works as an input layer and defends against both unseen and one pixel attacks. The proposed defense does not utilize any prior knowledge from the attacker ends."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Compared to competitive methods the proposed method does not require any adversarial training which is a general and efficient way of defending against adversarial attacks.\n\n2. This is a lightweight defense with minimum additional parameters and no extra training cost.\n\n3. The proposed method is grounded by proper theoretical analysis and the impact of defending against unseen attack is high."}, "weaknesses": {"value": "Evaluation of Attacks: While the attack claims there  that two prior works can defend against  adversarial training and in the evaluation their performance against the BPDA attack on l_infinity is higher the proposed defense. My concern is that in that case they outperform the proposed method. Even though the authors argue that adversarial training is costly but now it is possible to achieve adversarial training for free (https://proceedings.neurips.cc/paper_files/paper/2019/file/7503cfacd12053d309b6bed5c89de212-Paper.pdf). \n\nModel architecture: Evaluation on conventional model architecture is great, but the paper would benefit from evaluating on recent ViT architecture or even VLM evaluation. Since this should defend against unseen attack why not test on Object detection model  attacks as well."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eNp4joalZr", "forum": "3MyTiaYpVs", "replyto": "3MyTiaYpVs", "signatures": ["ICLR.cc/2026/Conference/Submission14939/Reviewer_tA7Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14939/Reviewer_tA7Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762126841, "cdate": 1761762126841, "tmdate": 1762925276948, "mdate": 1762925276948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new defense mechanism, Randomized Feature Squeezing (RFS), that enhances adversarial robustness without using adversarial training. The approach introduces a specially designed input layer that performs randomized feature squeezing through reciprocal and multiplicative transformations combined with Gaussian noise. This layer is trained end-to-end on clean data and removed during inference, leaving a simplified test framework. Experiments on CIFAR-10 and ImageNet show strong robustness against unseen attacks, as well as unlearnable examples (OPS), while maintaining reasonable clean accuracy and lower computational cost compared to adversarial training methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. About design. The proposed randomized feature squeezing layer is original and well-motivated, addressing adversarial robustness without adversarial data augmentation.\n2. About evaluation. Experiments include both black-box and white-box attacks across multiple norms and datasets, showing consistent and competitive performance.\n3. About efficiency. The approach dramatically reduces training cost and time compared to traditional adversarial training, demonstrating practical feasibility."}, "weaknesses": {"value": "1. Potential gradient obfuscation. Despite the authors’ discussion, the reliance on reciprocal and Sigmoid/Sign operations raises concerns about obfuscated gradients, and the robustness might partly stem from non-differentiability rather than genuine defense.\n2. Limited theoretical analysis. While empirical performance is strong, there is little formal justification or theoretical insight into why feature squeezing inherently yields robustness across norms.\n3. Evaluation scope. Robustness is measured on subsets and with fixed attack budgets, which might limit the generalizability of results. Moreover, it seems that more adversarial attack methods should be evaluated.\n4. Ablation insufficiency. Although ablation studies are included, the influence of key hyperparameters (σ, α, kernel size) on both clean and robust accuracy is not deeply explored.\n5. Reduced clean accuracy. The clean accuracy, especially on ImageNet, is lower than adversarially trained models, suggesting a trade-off that could be more explicitly quantified."}, "questions": {"value": "1. How do you ensure that the robustness does not primarily result from gradient masking? Could you provide quantitative analysis such as gradient norms or adaptive attack results beyond BPDA and EOT?\n2. What is the sensitivity of the model to the noise level σ and sparse loss weight α? Can improper tuning drastically reduce robustness?\n3. Would combining RFS with light adversarial or noise-based training further improve clean accuracy without large cost?\n4. Can this approach generalize to other architectures (e.g., ViTs) or tasks (e.g., segmentation, detection)?\n5. Could you elaborate on how RFS handles real-world attacks where perturbations may not conform strictly to lp norms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QE2T91lU4o", "forum": "3MyTiaYpVs", "replyto": "3MyTiaYpVs", "signatures": ["ICLR.cc/2026/Conference/Submission14939/Reviewer_fuWq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14939/Reviewer_fuWq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987812915, "cdate": 1761987812915, "tmdate": 1762925276661, "mdate": 1762925276661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}