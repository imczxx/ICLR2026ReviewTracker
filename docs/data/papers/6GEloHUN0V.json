{"id": "6GEloHUN0V", "number": 21815, "cdate": 1758322220363, "mdate": 1759896901679, "content": {"title": "Training-Free Pseudo-Fusion Strategies for Composed Image Retrieval via Diffusion and Multimodal Large Language Models", "abstract": "Composed Image Retrieval (CIR) is an emerging paradigm in content-based image retrieval that enables users to formulate complex visual queries by combining a reference image with an auxiliary modality, usually text-based. This approach supports fine-grained search where the target image shares structural elements with the user query but is modified according to the provided auxiliary text. Conventional CIR methods rely on multimodal fusion to combine visual and textual features into a joint query embedding. In this work, we propose PEFUSE (for pseudo-fusion), a training-free framework that leverages pre-trained models to bridge modalities via generative conversion. We introduce two novel strategies: uni-directional and bi-directional conversion, both implemented using diffusion models and multimodal large language models. These methods reformulate CIR as either intra-modal or cross-modal retrieval, bypassing the need for dedicated training. \nExtensive experiments on standard benchmarks show that our approach achieves competitive or superior performance compared to state-of-the-art methods, highlighting the efficacy and flexibility of our pseudo-fusion paradigm for composed retrieval.", "tldr": "", "keywords": ["Composed Image Retrieval", "Modality Conversion", "Modality Fusion", "Generative Models", "Training-free Methods"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9471b8cebbefdd182f92bae9937f9f5f01d2f724.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a PEFUSE, a training-free framework to address the CIR task. The proposed method utilizes diffusion models and multimodal large language models with uni-directional and bi-directional conversion to transform the CIR task to T2I and I2I tasks. Experiments validate the method achieves comparable performance compared with previous methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThe presentation of this paper is clear and easy for reading."}, "weaknesses": {"value": "1. Insufficient contribution. The proposed method simply utilizes two existing tools, namely, MLLM and diffusion model, to transform the reference image and relative caption into a target caption or image. Using MLLM to generate target caption is a very old and existing technique, proposed in many previous literatures, such as CIReVL[1], LDRE[2]. Using the diffusion model based on the generated target caption, is very simple without any technical contributions, while the results also show the inferior performance, as stated in Table 1, 2, 3.\n\n2. The proposed method show poor performance compared with previous works. For example, using CLIP as a fair comparison, the proposed PEFUSE performs poor compared with many old baselines, such as Pic2Word. If based on OpenCLIP or SigLIP2, please consider re-implement more previous SOTA methods for fair comparison, including literatures such as CIReVL [1], LDRE[2]. \n\n3. Time Consumption. As you mention that methods such as ImageScope, require more inference time with multiple models, it’ll be better to provide a detailed quantitative comparsion between your proposed method and others on inference time.\n\n4. Ablation study is missing. More ablation study, such as the choice of MLLM, diffusion model, should be conducted. \n\n5. Overall presentation. The presentation and experiments are conducted in a poor manner. Why only the results of uni-directional conversion appeared in the main text? More in-depth analysis should be conducted to show the relationship between uni-directional and bi-directional conversion. Also, the notation of ‘text-to-text’, ‘image-to-text’ between uni/bi-directional conversion should be defined separately, causing ambiguity for reading.\n\n[1] Karthik, Shyamgopal, et al. \"Vision-by-language for training-free compositional image retrieval.\" arXiv preprint arXiv:2310.09291 (2023).\n[2] Yang, Zhenyu, et al. \"Ldre: Llm-based divergent reasoning and ensemble for zero-shot composed image retrieval.\" Proceedings of the 47th International ACM SIGIR conference on research and development in information retrieval. 2024."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ditAI5DATg", "forum": "6GEloHUN0V", "replyto": "6GEloHUN0V", "signatures": ["ICLR.cc/2026/Conference/Submission21815/Reviewer_bXvg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21815/Reviewer_bXvg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644074027, "cdate": 1761644074027, "tmdate": 1762941940592, "mdate": 1762941940592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free *‘pseudo-fusion’* approach: by converting (reference image + modified text) into a single modality (text or image), the composite retrieval problem is transformed into a conventional unimodal retrieval task. This approach then uses a Diffusion model and a Multimodal LLM (MLLM) to achieve *‘unidirectional/bidirectional’* conversion. Experiments on **Fashion-IQ**, **CIRR**, and **CIRCO** demonstrate that text-to-image conversion generally outperforms other conversion methods and is competitive with some trained methods in several zero-shot scenarios. Overall, this is a systematic study of combining existing powerful generative and multimodal models to achieve zero-shot CIR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Comprehensive experimental design.** A systematic experimental design was implemented using multiple datasets (**Fashion-IQ**, **CIRR**, and **CIRCO**), multiple search engines (**CLIP**, **OpenCLIP**, and **SigLIP2**), and multiple transformation modes ($\\(T \\rightarrow I\\$), $\\(I \\rightarrow I\\)$). Scale and hyperparameter sensitivity analyses were also performed. This high coverage facilitates understanding of the impact of different factors.\n\n2. **Insightful empirical conclusions.** Author found that text\\(\\rightarrow\\)image conversion is generally good, with **OpenCLIP** outperforming **CLIP** in this scenario; Diffusion model artifacts lead to poor \\(I \\rightarrow I\\) performance, etc. These are all useful guidelines for subsequent work."}, "weaknesses": {"value": "1. **Relatively Incremental Academic Contribution.** As described in the paper, the method is primarily a combination of existing modules. To gain stronger acceptance, at least one or more clear new mechanisms should be emphasized and validated, or deeper theoretical/analytical support should be provided.\n\n2. **Insufficient Consideration of Resources, Latency, Cost, and Privacy.** The model relies on large-scale MLLM and diffusion models (such as **Qwen2.5-VL** and **sdxl-instructpix2pix**), resulting in high runtime costs, inference latency, and memory usage (especially in large-scale retrieval, when multiple images/text must be generated for each query). The paper lacks an analysis of inference time, GPU/CPU costs, or deployability.\n\n3. **Insufficient Ablation and Control Experiments.** There is a lack of comparison between different modules, as well as ablation analysis of caption generation or MLLM prompts.\n\n4. **Insufficient Methodology and Analysis of Related Methods.** The two retrieval pipelines (MLLM-based and Diffusion Model-based) in the paper are not described in detail and appear to be an aggregation of existing methods."}, "questions": {"value": "1. **Lack of qualitative visualization and failure case analysis.** The paper does not include qualitative visualizations of retrieval results or analyses of failure cases, which are important for illustrating the model’s strengths and limitations.\n\n2. **Insufficient model efficiency analysis.** The use of large MLLMs and Diffusion models significantly increases retrieval overhead. It is recommended to compare the inference cost of the proposed method with related approaches to demonstrate the model’s feasibility and efficiency.\n\n3. **Missing ablation studies.** It is unclear whether the prompt design affects the quality of the generated text/image and the final retrieval performance. Does image generation actually improve retrieval compared to directly using MLLM-generated text? If the $text\\(\\rightarrow\\)image$ mode performs best, does that imply the image generation step is essential? Although experiments show that $text\\(\\rightarrow\\)image$ conversion achieves the best performance, the underlying mechanism is not sufficiently explained.\n\n4. **Insufficient methodological details and comparison.** The paper does not provide enough information on the specific implementation of retrieval based on MLLMs and Diffusion models. What is the difference between the Diffusion Model-based pipeline and the IP-CIR [2] or CIG [1] methods? Similarly, how does the MLLM-based pipeline differ from OSrCIR [3] or CoTMR [4]?\n\n5. **Unclear justification for comparable performance.**  The $I \\rightarrow T$ retrieval pipeline in the paper appears similar to OSrCIR [3]. OSrCIR [3] achieves a Recall@5 of 67.25 on the CIRR dataset using the ViT-G/14 backbone, while this paper reports comparable performance using the smaller ViT-B/32 backbone. Please explain the reason for this result.\n\n[1]. L. Wang. Generative Zero-Shot Composed Image Retrieval. \n[2]. You Li, Fan Ma, and Yi Yang. Imagine and Seek: Improving Composed Image Retrieval with an Imagined Proxy. \n[3]. Yuanmin Tang, Xiaoting Qin, Jue Zhang, Jing Yu, Gaopeng Gou, Gang Xiong, Qingwei Ling, Saravan Rajmohan, Dongmei Zhang, and Qi Wu. Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval.\n[4]. Zelong Sun, Dong Jing, and Zhiwu Lu. CoTMR: Chain-of-Thought Multi-Scale Reasoning for Training-Free Zero-Shot Composed Image Retrieval."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "h16IR7Y7bn", "forum": "6GEloHUN0V", "replyto": "6GEloHUN0V", "signatures": ["ICLR.cc/2026/Conference/Submission21815/Reviewer_tXSS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21815/Reviewer_tXSS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903826856, "cdate": 1761903826856, "tmdate": 1762941940341, "mdate": 1762941940341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a training-free \"pseudo-fusion\" framework for Composed Image Retrieval (CIR) that converts the query (reference image+modifying text) into a single modality using off-the-shelf MLLMs (to text) and diffusion models (to images), which reduces the probem to text-to-image or image-to-image retrieval task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents an original training-free perspective on Composed Image Retrieval (CIR), leveraging existing multimodal models to reduce CIR to I2T/I2I problem.\n\n- The method is systematically evaluated across multiple standard CIR benchmarks (Fashion-IQ, CIRR, CIRCO) and different backbones (CLIP, OpenCLIP, SigLIP2), providing a thorough empirical study that supports the framework’s generality and practicality.\n\n- Writing: the paper seem to be well-written and comfortable to read."}, "weaknesses": {"value": "- Missing baselines: While the authors report results with three retrieval backbones (CLIP, OpenCLIP, and SigLIP2) combined with their method, the paper lacks proper training-free baseline comparisons. Specifically, the zero-shot CIR performance of these backbones - e.g., directly combining SigLIP2 image and text embeddings for retrieval - should be reported for context. Such baselines are crucial to fairly assess the real contribution of the proposed pseudo-fusion mechanism in the absence of additional training.\n\n- Backbone influence and attribution of gains: the comparison against prior training-free methods raises concerns about whether improvements stem from the proposed approach or simply from stronger underlying models. The use of a specific MLLM backbone (e.g., Qwen2.5-VL-7B-Instruct) may disproportionately influence results. It remains unclear how much of the reported performance gain would persist if previous methods were also paired with this newer MLLM. An ablation controlling for the backbone choice would clarify the true contribution of the proposed pseudo-fusion strategy versus advancements in foundation model capabilities.\n\n- Figure clarity and visual communication: Although the paper is generally well written, Figure 1—which serves as the main illustration of the proposed method, is confusing and visually overloaded. The figure contains too many intersecting arrows and mixed settings, making it difficult to follow the data flow and distinguish between the different stages of the pseudo-fusion process. For instance, the meaning of the colored arrows passing through the retrieval model and the duplicated sets of arrows in each branch are unclear. A clearer, modular reorganization, separating the different modes or stages, would significantly improve reader comprehension."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KZiX5FixOA", "forum": "6GEloHUN0V", "replyto": "6GEloHUN0V", "signatures": ["ICLR.cc/2026/Conference/Submission21815/Reviewer_C4aY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21815/Reviewer_C4aY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984334499, "cdate": 1761984334499, "tmdate": 1762941940019, "mdate": 1762941940019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors focus on zero-shot composed image retrieval. Specifically, they leverage diffusion models and MLLMs to bridge different modalities, enabling both intra-modal and cross-modal retrieval. Experimental results demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed training-free method converts multimodal queries into a single modality, which can be directly integrated into existing retrieval systems.\n2. Both unidirectional and bidirectional modality conversion paradigms explore the potential of applying diffusion models and MLLMs to the zero-shot CIR task."}, "weaknesses": {"value": "1. The designed method primarily focuses on implementing various experiments, but it lacks in-depth analysis of how to derive high-quality text or images that are well-suited for the CIR task.\n2. The generation of images and text for the query side significantly drives retrieval latency."}, "questions": {"value": "As listed above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AFhYOVc6Jl", "forum": "6GEloHUN0V", "replyto": "6GEloHUN0V", "signatures": ["ICLR.cc/2026/Conference/Submission21815/Reviewer_CCuD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21815/Reviewer_CCuD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986342146, "cdate": 1761986342146, "tmdate": 1762941939258, "mdate": 1762941939258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}