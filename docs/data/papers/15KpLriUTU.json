{"id": "15KpLriUTU", "number": 16691, "cdate": 1758267690986, "mdate": 1759897224601, "content": {"title": "PUM-Net: Plastic Unified Memory Network with Associative Interaction for Long-Context State Space Models", "abstract": "Recent Mamba-family State Space Models (SSMs) leverage linear recurrent dynamics to achieve efficiency in long-sequence modeling. However, their exponential decay kernels inevitably cause rapid forgetting, making it difficult to recall information once sequence lengths far exceed the training horizon. Inspired by advances in Transformer-based architectures such as Native Sparse Attention (NSA), which employ internal chunk memory to alleviate long-term forgetting, we extend Mamba with a chunk-wise internal long-term memory that improves the retrieval of distant context. While this enhances in-context recall, a more fundamental challenge for long-context models is the ability to access and integrate vast external world knowledge without compromising efficiency. Existing retrieval-augmented generation (RAG) approaches attempt to address this by appending retrieved documents to queries, which substantially increases training cost and fails to fully integrate internal and external memory representations. To overcome these limitations, we propose the Plastic Unified Memory Network (PUM-Net), a unified dual-memory architecture that, for the first time, enables joint pre-training over both dynamic internal memory and static, pre-encoded external knowledge. This plastic unification allows external memory to refine internal states during training, enabling bidirectional interaction without inflating sequence length, thereby supporting more effective long-context modeling and achieving substantial improvements across long-range challenging benchmarks.", "tldr": "", "keywords": ["State Space Model", "Long Context", "Internal Memory", "External Memory"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf765fd6e34b8efc3d037179daea5b3566db079a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method to enhance the capability of state space models with additional internal and external memory. The external memories are pre-computed and derived from a large corpus, using the final memory state from each passage. The internal memories are computed dynamically by materializing the memory state after each chunk of the input sequence. The authors further develop methods to retrieve the memory and to fuse the memory output together.\n\nThe empirical results demonstrate the effectiveness of the proposed architecture, especially for improving the long-context performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is generally well-written, clear, and easy to follow. The authors propose a method to enhance the long-context performance of state space models by introducing/recording additional external/internal memory states.\n\n2. The empirical study strongly supports the effectiveness of the proposed architecture."}, "weaknesses": {"value": "1. The experiments are done using Mamba as the backbone. The paper would be stronger to have experiments with more recent RNN architectures, e.g., Mamba-2 and gated delta net, to demonstrate that the effectiveness of the proposed method is robust against different RNN architectures.\n\n2. On a high level, the proposed method effectively provides a new way to combine Mamba and softmax attention (especially given Bi-Directional Cross-Memory Interaction) to attend additionally materialized external and internal states. There are multiple components presented in Section 3.2.2., making the proposed architecture a bit complicated. For example, could there be a simpler way to use the additionally materialized memory instead of cross attention? This paper lacks an ablation study to demonstrate the effectiveness of each proposed change."}, "questions": {"value": "1. Could the authors comment on whether a simpler way to use the additionally materialized memory instead of cross attention could also work?\n\n2. Could the authors comment on whether the external memory construction would lead to leakage? and how to make sure the construction of the external memory doesn't lead to leakage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AhsRRBiQx8", "forum": "15KpLriUTU", "replyto": "15KpLriUTU", "signatures": ["ICLR.cc/2026/Conference/Submission16691/Reviewer_jtZv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16691/Reviewer_jtZv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722021015, "cdate": 1761722021015, "tmdate": 1762926746069, "mdate": 1762926746069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new model architecture on top of the Mamba model. Specifically, it maintains a session-specific and an external key-value memory. The authors propose some techniques for retrieving and integrating information from there two memories efficiently. Empirical results show that the proposed model can outperform both vanilla Mamba and Transformer models, while being more efficient in training and inference than vanilla attention and Native Sparse Attention."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The method for integrating session-specific and external knowledge is novel, and some of the empirical results show that it gives considerable performance gains over the vanilla Mamba model.\n- The method is also more efficient than one variant of sparse attention in both training and inference."}, "weaknesses": {"value": "- The results reported in Table 1 and Figure 2 is strange and unconvincing. For such large values of perplexity (some of the values are even worse than random guessing), the model is outputting gibberish. Thus, this results do not support the claim that PUM-Net has better length generalization than the baselines. On the positive side, the perplexity is lower in general. However, this might be a result of the fact that PUM-Net has access to an external corpus, which the baselines do not have.\n- Since PUM-Net contains both attention and Mamba layers, it can be viewed as a kind of Transformer-Mamba hybrid architecture. Thus, I think it would be fair to include hybrid architectures as baselines as well.\n- According to Appendix G, PUM-Net has linear-time complexity. However, during decoding, each token needs to attend to the entire set of the session-specific memory states. Is this part linear in terms of complexity?\n- It is unclear from Figure 4 that PUM-Net has superior passkey retrieval abilities. The number of green cells appears to be similar to the baselines.\n- Most of the empirical results in the paper is based on a 130M model, which is way too small compared to the ones used in real applications. Moreover, it seems in Section 4.3 that the authors have also finetuned a 2.8B Mamba model. Why are the experiments prior to this section conducted with the 130M model instead of the larger 2.8B model?"}, "questions": {"value": "- There is an extra period in Line 142.\n- Can you provide the loss curves for the transformer baseline model as well?\n- Can you provide the results of Figure 5, but with a larger model (e.g., up to 10B parameters)? Also, can you provide the numbers in this figure for the vanilla Mamba model as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ipUzd1VoFq", "forum": "15KpLriUTU", "replyto": "15KpLriUTU", "signatures": ["ICLR.cc/2026/Conference/Submission16691/Reviewer_CVkt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16691/Reviewer_CVkt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732034369, "cdate": 1761732034369, "tmdate": 1762926745315, "mdate": 1762926745315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PUM-Net, a dual-memory SSM: a chunked **internal** memory to mitigate exponential-decay forgetting in Mamba, and a **static external** memory of pre-encoded key/value representations. A staged parallel training scheme generates queries to both memories and fuses them via a gated, bi-directional interaction module. Experiments on long-range language modeling, a synthetic passkey retrieval benchmark, and LongBench QA suggest improved long-context performance and efficiency over Mamba and attention variants. The idea is appealing, but key empirical and methodological issues make the current evidence insufficient."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The architecture cleanly factors long-range retention (internal chunk memory) from world-knowledge access (external memory) and fuses them with a lightweight gated mechanism, avoiding quadratic attention. The staged scan + retrieval design is conceptually neat and worth exploring further."}, "weaknesses": {"value": "1. **Empirical validity and evaluation design are questionable.**\n\n   * The reported perplexities include implausible magnitudes and discontinuities (e.g., explosive PPL at long lengths for baselines, but a dramatic drop for PUM-Net on CodeParrot at 4k where PPL≈1.11 while baselines ≈4.5), and scientific conclusions hinge on these numbers. Such scales typically flag a bug in scoring, tokenization, or loss masking.\n   * The methodology measures **PPL only on the last 100 tokens of each long window** during evaluation—this departs from standard PPL computation and can bias results toward models whose final-step states are explicitly augmented by retrieval/fusion, overstating gains relative to baselines that don’t fuse late. Please recompute using standard per-token averaging over the full window and include confidence intervals/replicates. \n\n2. **Unfair or incomplete baselines for external knowledge integration.**\n\n   * The paper does **not** compare against strong retrieval baselines that concatenate evidence (RAG) at inference, nor against retrieval-in-the-hidden-state methods (e.g., kNN-LM-style caches) or memory-augmented recurrent/Transformer variants. Since PUM-Net’s core claim is “deep fusion without sequence inflation,” it must be compared to (i) a tuned RAG pipeline at equal wall-clock/throughput budget and (ii) a hidden-state retrieval baseline with similar nearest-neighbor machinery and k.\n   * For LongBench QA, the “external memory per question = top-1 Wikipedia page” setup risks **question-conditioned memory selection** advantages that a fair RAG baseline would also enjoy. Without those baselines, it’s hard to attribute improvements to the *architecture* rather than to the retrieval heuristic. Please add apples-to-apples comparisons with matched retrieval corpora and time budgets. \n\n3. **Complexity/efficiency claims are underspecified and possibly optimistic.**\n\n   * The paper claims big speedups over NSA and Flash-Attention using single-block microbenchmarks, but **per-token dual retrieval** (ANN over external keys + dense top-k over internal chunk keys) is performed at each time step. The figures appear to **exclude** retrieval time and index I/O; if so, the headline speedups aren’t end-to-end.\n   * The text also asserts “zero-overhead external memory during training,” yet stage 2 performs batched retrieval and cross-memory attention at each step. Please clarify exactly what is excluded from timing, report **end-to-end** wall-clock (including retrieval/indexing), and provide memory/latency scaling vs. k and corpus size N. Otherwise the efficiency story is incomplete."}, "questions": {"value": "- Table 2 note mentions “LongBench-E” but this term is not defined earlier; clarify or remove."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SinhGIi3lE", "forum": "15KpLriUTU", "replyto": "15KpLriUTU", "signatures": ["ICLR.cc/2026/Conference/Submission16691/Reviewer_EZCr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16691/Reviewer_EZCr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146866454, "cdate": 1762146866454, "tmdate": 1762926744895, "mdate": 1762926744895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new method called PUM-Net thataugments a Mamba SSM with a dual-memory system to improve long-context performance with two different types of memory."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It shows better perplexity on the 64k sequence than the Mamba baseline, exceeding its 4k training length\n2. It demonstrates much higher throughput and lower memory use compared to attention-based long-context models (NSA, Flash-Attention)."}, "weaknesses": {"value": "1. The claims regarding efficiency and throughput are misleading due to the omission of a direct speed comparison with the vanilla Mamba baseline. The proposed PUM-Net block is substantially more complex than a standard Mamba block—it involves a two-stage process and consumes more memory (Fig. 5, right). This complexity represents a significant trade-off, not a \"free\" upgrade, and almost certainly results in slower performance.\n2. the external knowledge is static, meaning it will inevitably become stale. The evaluation performance will inevitably determined by how well this external knowledge is trained.\n3. Since the memory creation process is not simple and relied on additional models, I am not sure about the scalability and universality of the method."}, "questions": {"value": "1. Can you provide latency and throughput benchmarks comparing PUM-Net directly against the vanilla Mamba baseline, not just attention models.\n2. How does the brute-force search for the internal memory scale as sequence length (and thus, chunk count) increases?\n3. What is the cost and workflow to update the static external memory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LM4mPgsN8B", "forum": "15KpLriUTU", "replyto": "15KpLriUTU", "signatures": ["ICLR.cc/2026/Conference/Submission16691/Reviewer_4tNR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16691/Reviewer_4tNR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762743005164, "cdate": 1762743005164, "tmdate": 1762926744209, "mdate": 1762926744209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}