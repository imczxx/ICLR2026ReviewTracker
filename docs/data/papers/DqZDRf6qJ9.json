{"id": "DqZDRf6qJ9", "number": 20939, "cdate": 1758311837069, "mdate": 1763565730190, "content": {"title": "BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation", "abstract": "Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain–computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce $\\textbf{BrainDistill}$, a novel implantable motor decoding pipeline that integrates a neural decoder with a distillation framework. First, we propose $\\textbf{TSKD}$, a task-specific knowledge distillation method that projects task-relevant teacher embeddings into compact student models. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. To evaluate the framework, we define the task-specific ratio ($\\textbf{TSR}$), a new metric that quantifies the proportion of task-relevant information retained after projection. \nBuilding on this framework, we propose the Implantable Neural Decoder ($\\textbf{IND}$), a lightweight transformer architecture that combines linear attention with continuous wavelet tokenization, optimized for on-chip deployment. \nAcross multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.", "tldr": "A novel motor decoding pipeline that integrates task-specific knowledge distillation and efficient implantable neural decoder", "keywords": ["Motor Decoding; Knowledge Distillation; Quantization Aware Training; Brain Computer Interface"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de2fc319895463971abda04cba9e4891b94db2bf.pdf", "supplementary_material": "/attachment/1646ba7f0e83f9d9679a4d048d1741bab87a2bad.zip"}, "replies": [{"content": {"summary": {"value": "Braindistill: implantable motor decoding with task-specific knowledge distillation presents three main contributions: a linear attention Transformer (IND), a distillation technique (TKSD), and metric to evaluate the data-specific effectiveness of a projection (TSR). This work provides compelling empirical evidence for all three components, albeit mostly on private datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Advantage over the state-of-the-art on the datasets considered is clear\n2. Ablations are thorough and validate the choices of the authors\n3. The stability shown under quantization is a nice bonus"}, "weaknesses": {"value": "1. While the paper advertises 6 datasets (3 private and 3 public), the relevant performance comparisons against baseline models are only performed on the 3 private ones, and the public ones are left for ablations. The same benchmarking must be performed on the public ones to ensure unbiased results.\n2. A lighter version of EEGPT (the teacher model) needs to be included as a baseline. As it stands, it is not clear whether EEGPT performs better than IND because of the size or because of the architecture.\n3. Related to the point above, I assume that EEGPT is chosen due to its large size (100M), while the others are considerably smaller. This should be made clear, and the parameters of all models also need to be reported.\n4. The paper is quite disjointed and tough to parse. I found it rather difficult to read and decipher how all the components go together."}, "questions": {"value": "1. It’s not fully clear what “training from scratch” means for the student model. For distillation, from my understanding the teacher is trained on 159 sessions (X_{offline}), then the student is distilled using X_{recalib}, then the student is tested on X_{online}. For the “scratch” case, is the student trained on X_{offline}, X_{recalib}, or both at the same time?\n2. The definition of X_{offline} is not fully clear. Is it patient-specific or not?\n3. \\Delta is not defined in Eq. 1\n4. What’s the advantage of not using the same classifier during the two phases of distillation?\n5. The division of the splits is quite confusing at first, should make clear that, e.g., 1-1 means training on the training split of session 1 and testing on the testing split of session 1.\n6. The second term of L_{TKSD} is highly reminiscent of ridge regression, you might find works on the Tikhonov factor useful for the determination of \\lambda.\n7. For clarity, it should be specified that the classifier must be a single layer + non-linearity"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IXGc4v8Okq", "forum": "DqZDRf6qJ9", "replyto": "DqZDRf6qJ9", "signatures": ["ICLR.cc/2026/Conference/Submission20939/Reviewer_JvuC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20939/Reviewer_JvuC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761229396614, "cdate": 1761229396614, "tmdate": 1762939037927, "mdate": 1762939037927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new novel distillation pipeline for motor decoding via bringing student / teacher embedding space together by minimising two objectives. In addition, another contribution of the paper is the utilization of the overlap between projecting spaces as a metric for the distillation error."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a solid teacher - student model. The maths behind the methodology is also well-described and has a nice flow. The fact that it goes beyond EEG to also ECoG and Spikes is also very interesting. \n\nWriting:\nPaper is well-written and good structured."}, "weaknesses": {"value": "The main objective of the paper is not clear. Is the main purpose the distillation methodology or the IND architecture which (as the authors claimed is pretty basic) ? This should be better described.\n\nOverall:\nThe paper shows some merits but it would be interesting to have my questions answered."}, "questions": {"value": "1. In my opinion the paper’s main contribution is the distillation methodology rather than the IND architecture. Have you tried to add this framework on other models like the ones you compare with ? The results would be interesting to be added here.\n2. I would also like to see comparison with other PERF methods like LoRA. A recent work for EEG [1].\n3. How about comparison with SOTA foundation models ?\n\n[1]: Na Lee, Konstantinos Barmpas, Yannis Panagakis, Dimitrios Adamos, Nikolaos Laskaris, & Stefanos Zafeiriou (2025). Are Large Brainwave Foundation Models Capable Yet ? Insights from Fine-Tuning. In Forty-second International Conference on Machine Learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jXjpeC3spn", "forum": "DqZDRf6qJ9", "replyto": "DqZDRf6qJ9", "signatures": ["ICLR.cc/2026/Conference/Submission20939/Reviewer_C3Rp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20939/Reviewer_C3Rp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620181676, "cdate": 1761620181676, "tmdate": 1762939037177, "mdate": 1762939037177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revised manuscript based on the comments"}, "comment": {"value": "We thank all the reviewers for acknowledging the value of this work, and providing constructive feedbacks and insightful suggestions. We have addressed all of the concerns to the best of our ability and revised the manuscript accordingly, including updated results. The major updates in the revised version are as follows:\n\n### **High-level discussion of projection approach**\n\nIn response to Reviewer kMgJ and AFsC, we have added a paragraph discussing the differences and the advantages of our projection methods, compared with existing task-oriented distillation approaches. Besides, we have added a new baseline, the inverse projection method, presented in Table 3.\n\n### **Model size information**\n\nIn response to Reviewer JvuC, we have provided the sizes of teacher models and baseline models in section 3.1 and section 3.2.\n\n### **New distillation baselines**\n\nIn response to Reviewer KMgJ, we have added two task-oriented distillation baselines, and benchmarked them on Human-C, BCIC-2A, BCIC-2B and FALCON-M1, with the updated results presented in Table 2 and Table 7.\n\n### **Quantization baselines**\n\nIn response to Reviewer KMgJ, we included the comparison between our QAT method and I-ViT, with the updated result presented in Table 6.\n\n### **Foundation model as a decoder baseline**\n\nIn response to Reviewer C3Rp, we added LaBraM as a baseline decoder, and benchmark it on Human-C, Monkey-R and Human-D. The updated results are presented in Table 2, Figure 3, Table 5.\n\n### **Quantifying TSR with correlation**\n\nIn response to Reviewer kMgJ and AFsC, we have included mutual information and relative reconstruction error as baseline metrics, and computed correlations between task performance and projection metrics, showing the advantage of TSR. The updated results are presented in Table 4 and Table 8.\n\n### **Applying TSKD to other baseline decoders**\n\nIn response to Reviewer C3Rp, we have applied TSKD to three baseline methods on Human-C, and compare all the distillation methods with EEGConformer on BCIC-2A and BCIC-2B. The results are presented in Table 10 and Table 15.\n\n### **Adding ablation results on public data**\n\nIn response to Reviewer JvuC, we have added an ablation experiment for IND on Human-D, and presented the updated results in Table 13."}}, "id": "k3nDtaEwzS", "forum": "DqZDRf6qJ9", "replyto": "DqZDRf6qJ9", "signatures": ["ICLR.cc/2026/Conference/Submission20939/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20939/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20939/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763565781983, "cdate": 1763565781983, "tmdate": 1763565781983, "mdate": 1763565781983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BrainDistill, a framework for efficient and deployable neural decoding in implantable brain–computer interface (BCI) systems. The approach integrates a Task-Specific Knowledge Distillation (TSKD) method with a lightweight transformer-based Implantable Neural Decoder (IND). TSKD compresses teacher embeddings into a low-dimensional, task-relevant subspace, guided by a new metric called the Task-Specific Ratio (TSR) that quantifies how much task-related information is preserved after projection. IND further combines continuous wavelet tokenization with quantization-aware linear attention to enable low-power, integer-only inference suitable for on-chip deployment.\nExperiments across ECoG, EEG, and spike datasets demonstrate consistent improvements in decoding accuracy and robustness over prior distillation baselines and traditional decoders, while the quantized IND achieves a reported 3× reduction in power consumption (5.66 mW) with minimal accuracy loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe motivation, i.e. bridging the gap between large neural decoders and implantable hardware, is timely and clearly articulated. TSKD addresses a concrete limitation of standard distillation (feature mismatch and capacity gap) with a principled projection-based approach.\n2.\tThe two-step projection method (supervised compression followed by fixed alignment) is well-designed. TSR provides an interpretable quantitative measure that correlates with distillation quality and offers practical diagnostic value.\n3.\tThe integer-only quantization with learnable clipping ranges is implemented carefully and validated with realistic energy estimates. \n4.\tThe framework is tested on a wide range of neural modalities (ECoG, EEG, spikes) and datasets, consistently outperforming KD, SimKD, VkD, and RdimKD. Ablations on tokenization and projection methods support the claims well."}, "weaknesses": {"value": "1.\tIt would be helpful to understand whether TSKD’s projections depend critically on the quality of the teacher classifier and how sensitive TSR is to teacher miscalibration.\n2.\tThe paper assumes TSR correlates with downstream accuracy, but this relationship is only shown qualitatively. Quantitative correlation plots between TSR and decoding performance across projection types would strengthen the claim.\n3.\tThe power numbers appear simulation-based rather than measured. Including hardware prototype details or synthesis-level validation would improve the credibility of the “implantable” claim.\n4.\tSome mathematical derivations (Eqs. 1–5) are dense. The paper would benefit from a clearer high-level description of intuition behind Eq. (4) and the projection procedure."}, "questions": {"value": "1) Does TSR quantitatively predict decoding accuracy, and is it a reliable task relevance metric?\n2) Are the hardware power savings realistic and fully measured (not simulation-only)?\n3) Are distillation baselines implemented under strictly identical conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QmmXVvKCwj", "forum": "DqZDRf6qJ9", "replyto": "DqZDRf6qJ9", "signatures": ["ICLR.cc/2026/Conference/Submission20939/Reviewer_AFsC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20939/Reviewer_AFsC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908445087, "cdate": 1761908445087, "tmdate": 1762939036196, "mdate": 1762939036196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BrainDistill, a pipeline designed for implantable brain–computer interface (BCI) decoders that operate under strict power constraints. The authors propose the Task-Specific Knowledge Distillation (TSKD) method that projects teacher embeddings into a task-relevant subspace for more efficient student learning and a lightweight transformer-based decoder using Continuous Wavelet Transform (CWT) tokenization and linear attention for quantization and implantation. The authors use Task-Specific Ratio (TSR) metric to measure how much task-relevant information is preserved during projection. They evaluate the approach on human and primate datasets (ECoG, EEG, spike data) and demonstrate reduced power consumption via their quantization scheme for integer-only inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Implantable BCIs impose unique hardware limits and the authors correctly identify power efficiency as a major bottleneck. So, the quantization analysis and power consumption estimates are relevant to practical deployment.\n\n2. The mathematical exposition of projection-based distillation is useful for understanding of feature compression.\n\n3. Covers three neural recording modalities (ECoG, EEG, spikes) and shows decoding performance improvements across these modalities."}, "weaknesses": {"value": "1. No comparison is done against other task-oriented or projection-based distillation methods (e.g. [1-3]) under identical training conditions.\n\n2. There is no ablation comparing IND architecture vs. TSKD itself.\n\n3. Architecturally, the model is very similar to [1-3] and similar task-specific KD approaches and novelty of the method is under question. There is no support for the following: \"However, existing KD methods primarily aim to preserve teacher embeddings as fully as possible (Miles et al., 2024; Zhou et al., 2025; Guo et al., 2023), which becomes problematic when the student model lacks the capacity to mimic complex teacher features, resulting in limited performance gains.\"\n\n4. There is no baseline comparison between IND and other quantization methods e.g. [4, 5].\n\n5. The core of the reported results in the main text are from a private dataset (Human-C). Furthermore, no code is provided which hinders reproducibility.\n\n6. Figures 2 and 3 are mostly descriptive and do not provide any insights or explanations of model performance.\n\n[1] Less is more: Task-aware layer-wise distillation for language model compression\n\n[2] Task-oriented feature distillation\n\n[3] Improving Knowledge Distillation using Orthogonal Projections\n\n[4] Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\n\n[5] Post-training 4-bit quantization of convolution networks for rapid-deployment"}, "questions": {"value": "1. What distinguishes TSKD from other supervised subspace alignment approaches? Can you point out to any novelty beyond changing the dimensionality of the projection output?\n\n2. Why is TSR a better metric than simpler reconstruction loss or mutual info measures for projection quality?\n\n3. How much of the performance gain arises from CWT tokenization versus the linear attention module (ablations)?\n\n4. Can the method be extended beyond motor decoding (e.g., speech or visual BCIs)?\n\n5. How can the results be verified? All code and private datasets should become publicly available."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Yes. There are two private ECoG dataset (one monkey and human). No data on compensation or ethical data collection protocols is provided in the manuscript."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7i1zfkH8ns", "forum": "DqZDRf6qJ9", "replyto": "DqZDRf6qJ9", "signatures": ["ICLR.cc/2026/Conference/Submission20939/Reviewer_kMgJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20939/Reviewer_kMgJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938198080, "cdate": 1761938198080, "tmdate": 1762939035760, "mdate": 1762939035760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}