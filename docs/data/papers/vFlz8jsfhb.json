{"id": "vFlz8jsfhb", "number": 23522, "cdate": 1758344935576, "mdate": 1759896810632, "content": {"title": "Drill-Down Analysis of LLM Hallucination Patterns in Text-to-SQL", "abstract": "Despite impressive benchmark scores, Large Language Models (LLMs) can still produce flawed and incorrect responses for Text-to-SQL tasks. While prior work has decomposed complex SQL queries in an attempt to improve LLM benchmark performance, few have systematically analyzed hallucination propagation patterns within these decomposed structures. We present a drill-down evaluation framework that decomposes complex SQL queries and questions from the BIRD-mini dataset Li et al. (2023), allowing for a fine-grained analysis of hallucination propagation. Through our analysis, we report three key findings: (1) Recurrent Hallucinations: Many hallucinations persistently propagate from early, structurally simple sub-queries through to final steps, indicating systematic misalignment. (2) Final-Step Emergence: Fewer, but specific hallucination types emerge in the final step, suggesting a distinct failure mode tied to query complexity. (3) History Amplifies Recurrence: While contextual information between sub-queries can help to reduce the frequency of emergent hallucinations, it consequently increases the recurrence of early-stage hallucinations. This framework establishes a methodology to better understand LLM weaknesses and failure modes for Text-to-SQL systems.", "tldr": "This paper proposes a drill-down evaluation framework to analyze hallucination patterns of large language models (LLMs) in Text-to-SQL by decomposing complex queries into progressive sub-queries and providing key insights into their failure modes.", "keywords": ["Drill-Down", "Hallucination", "Decomposition", "Text-to-SQL", "Error Propagation"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f1c7b6aa755deb595d24a9e74c954a74d7d846f7.pdf", "supplementary_material": "/attachment/fe972fa4e4536bc052934f620d4d2a8958a3b0ba.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a \"drill-down\" framework to analyze how and when Large Language Models (LLMs) make errors (hallucinate) when generating SQL queries from natural language questions. Instead of just checking if the final SQL is correct, the authors decomposed complex problems from the BIRD-mini dataset into a series of simpler, progressive steps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The core contribution, the \"drill-down evaluation framework,\" is a significant strength. By decomposing complex SQL queries into a sequence of progressive sub-queries and sub-questions, the authors move beyond simple, final-step accuracy. This method allows for a much-needed, fine-grained analysis of how and when errors are introduced, which is more valuable than just knowing if an error occurred.\n\n2. The paper introduces a useful temporal dimension to error analysis by defining \"Recurrent Hallucinations\" (errors that propagate from simple steps) and \"Emergent Hallucinations\" (errors that appear only in the final, complex step). This distinction provides a new and powerful lens for understanding LLM failure modes.\n\n3. The discovery of the \"duality\" of contextual history is a major finding. The insight that providing history amplifies recurrent hallucinations (reinforces early mistakes) while simultaneously reducing emergent ones (protects against new errors) is non-obvious and has direct implications for designing multi-turn, conversational Text-to-SQL systems."}, "weaknesses": {"value": "1. Limited Dataset Scale: The analysis relies on the BIRD-mini dataset, which is a small subset (500 original questions) of the full BIRD benchmark. While the authors expanded this into 1,383 sub-questions, the findings (especially the specific probabilities of recurrent vs. emergent errors) may not generalize to the wider variety and \"long-tail\" complexity of the full BIRD dataset or other complex Text-to-SQL benchmarks.\n\n2. Potential Artifacts from Sub-Question Generation: The framework relies on an LLM to generate the natural language sub-questions (an SQL-to-NL task). While the authors justify this as being more reliable than NL-to-SQL, this generation step is not perfect and could introduce its own artifacts or subtle \"hallucinations\" into the evaluation dataset itself. The paper does not appear to analyze the error rate or potential bias of this LLM-based dataset creation step.\n\n3. Scope of \"Hallucination\": The paper defines \"hallucination\" in a functional, post-hoc way (i.e., the generated SQL is incorrect according to the taxonomy). This is a practical and necessary limitation for an empirical study, but it doesn't fully capture the cognitive or generative process of why the LLM produced the wrong tokens. Terms like \"cognitive load\" (Section 4) are used as high-level metaphors rather than as measurable mechanisms.\n\n4. \"Alignment\" Claims: The paper uses strong terms like \"systematic misalignment\" and \"a path to better alignment.\" While the findings are certainly relevant to the goals of AI alignment, the paper is fundamentally an error analysis that diagnoses problems. It does not propose or test a new alignment method or solution. The conclusions might slightly overstate the paper's direct contribution to alignment methodology."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uFis3nvBba", "forum": "vFlz8jsfhb", "replyto": "vFlz8jsfhb", "signatures": ["ICLR.cc/2026/Conference/Submission23522/Reviewer_Tw4D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23522/Reviewer_Tw4D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810183009, "cdate": 1761810183009, "tmdate": 1762942697510, "mdate": 1762942697510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a drill-down framework to analyze hallucination patterns in Text-to-SQL tasks LLMs. The authors decompose SQL queries and their corresponding natural language questions from the BIRD-mini dataset, enabling fine-grained temporal tracking of hallucinations as they propagate through multi-step query construction. They define two key hallucination types: recurrent (persisting from early steps to final query) and emergent (appearing only at the final step), and analyze six frontier LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed drill-down methodology offers a clear and reproducible way to dissect LLM behavior in multi-step reasoning tasks. The idea of decomposing SQL into executable sub-queries for hallucination tracking is conceptually elegant and technically well-motivated. Generally I think this research problem is novel and interesting in this domain.\n2. The combination of schema-based and logic-based hallucination taxonomies with temporal abstractions (recurrent vs. emergent) gives the study a better analytical depth rarely seen in hallucination papers.\n3. Six frontier closed-source models were evaluated with both EX and Soft-F1 scores, demonstrating methodological rigor and careful experimental design. Results are clearly visualized, with consistent trends across models."}, "weaknesses": {"value": "1. **Limited scalability and dataset scope:** the study relies solely on the BIRD-mini dataset. Although justified for computational feasibility, this restricts generalization to broader domains or more diverse SQL structures.\n2. Since both dataset decomposition and LLM evaluations involve closed-source models (Claude and GPT), the work’s reproducibility and community accessibility are limited.\n3. While the analysis is strong, the paper stops short of proposing concrete mitigation strategies or architectural insights. The findings illuminate what fails but not how to fix it."}, "questions": {"value": "Could this drill-down framework be directly applied to other datasets (e.g., Spider or KaggleDBQA)? If so, what challenges arise in adapting decomposition and annotation pipelines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8yLvoniHH1", "forum": "vFlz8jsfhb", "replyto": "vFlz8jsfhb", "signatures": ["ICLR.cc/2026/Conference/Submission23522/Reviewer_f9KX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23522/Reviewer_f9KX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850026264, "cdate": 1761850026264, "tmdate": 1762942697263, "mdate": 1762942697263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a drill-down evaluation framework for analyzing hallucination propagation in Text-to-SQL tasks. Authors decompose SQL queries from the BIRD-mini dataset into sub-queries and sub-questions to track hallucinations across different reasoning steps. They categorize hallucinations into schema-based and logic-based types, introducing two temporal dimensions: recurrent (persistent) and emergent (final-step) hallucinations. Experiments were conducted using six modern LLMs (Claude 3.5/3.7, GPT-4-turbo, GPT-4o-mini, GPT-4.1-mini, GPT-4.1-nano). The authors claim consistent hallucination patterns across models and highlight that contextual history tends to reduce emergent hallucinations but amplify recurrent ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and timely issue, hallucination behavior in Text-to-SQL models, which is directly related to model reliability and alignment.\n2. The proposed drill-down framework is clearly structured and easy to follow, providing a systematic way to trace hallucination propagation across query steps. The introduction of recurrent and emergent hallucinations adds a useful temporal dimension to existing classification schemes.\n3. The comparison of six modern LLMs (Claude and GPT series) strengthens the empirical validity of the findings and shows consistent failure patterns across models."}, "weaknesses": {"value": "1. The work lacks clear novelty both in dataset construction and in the definition of hallucination categories. The so-called drill-down framework mainly repackages existing dataset decomposition and annotation strategies without introducing a fundamentally new methodological contribution. From the dataset perspective, decomposing SQL queries into sub-queries is not a new approach and has already been explored in prior Text-to-SQL studies. From the definition perspective, the proposed recurrent and emergent hallucinations are essentially incremental extensions of the taxonomy established in TA-SQL (Qu et al., 2024), rather than conceptually novel categories.\n2. While the paper provides a quantitative description of hallucination patterns, it lacks deeper analytical or theoretical insight into the underlying causes of these behaviors. The findings primarily replicate observations from prior studies such as TA-SQL (Qu et al., 2024) and the in-context Text-to-SQL error analysis by Shen et al. (2025). Although the inclusion of multiple LLMs broadens the empirical coverage, the analysis offers limited depth in interpreting model mechanisms or proposing practical strategies for hallucination mitigation.\n3. The reliance on the small BIRD-mini dataset substantially limits the generalizability of the results. Although the authors justify this choice by citing computational constraints, this rationale is not sufficiently persuasive for an ICLR-level contribution. Since the proposed framework aims to analyze hallucination behavior across complex query structures, evaluation on a more comprehensive dataset，such as Spider and its derived variants or, at minimum, stronger validation on the full BIRD benchmark would be necessary to demonstrate the robustness and broader applicability of the findings.\n4. The paper does not discuss annotation reliability or inter-rater consistency, which raises concerns about the validity of the taxonomy-based analysis.The absence of such validation details weakens confidence in the reproducibility and objectivity of the reported hallucination patterns."}, "questions": {"value": "1. The idea of decomposing SQL queries for hallucination analysis is interesting but not novel. Prior frameworks such as CHASE-SQL and TA-SQL (Qu et al., 2024) have already explored task decomposition and hallucination categorization. Please articulate more clearly what methodological innovation distinguishes your drill-down framework from these existing approaches. For instance, how does your analysis pipeline provide new insights that cannot be achieved through prior decomposition-based methods?\n2. Strengthen the connection between hallucination recurrence and model alignment theory. Currently, the analysis remains descriptive, focusing on pattern quantification rather than underlying causes. A deeper theoretical interpretation of why recurrent and emergent hallucinations occur, and what this implies about model reasoning or attention dynamics, would significantly improve the contribution.\n3. The paper does not clearly describe how hallucination annotations were conducted, whether through automated rule-based comparison or LLM-assisted classification. Although an “automated annotation pipeline” is mentioned, the paper provides no information on its validation or reliability. Could the authors clarify how the annotation process was implemented and verified? Specifically, how was consistency ensured across instances?\n4. The reliance on the BIRD-mini dataset limits the generalizability of findings. Consider extending the analysis to larger or more diverse datasets (e.g., Spider and its variants) or at least validating the results on the full BIRD benchmark. This would demonstrate that the framework scales effectively and that the reported hallucination behaviors are not dataset-specific artifacts.\n5. In sections such as Preliminaries (Section 3) and Hallucination Taxonomy (Section 4), the authors devote extensive space to reintroducing material that largely reproduces content from prior work. This repetition diminishes the visibility of the paper’s own contribution. The authors are encouraged to condense these sections or move the background material to the appendix to improve focus and clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YdizxF1qtB", "forum": "vFlz8jsfhb", "replyto": "vFlz8jsfhb", "signatures": ["ICLR.cc/2026/Conference/Submission23522/Reviewer_khhh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23522/Reviewer_khhh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866318784, "cdate": 1761866318784, "tmdate": 1762942697063, "mdate": 1762942697063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a drill-down evaluation framework for analyzing hallucination patterns in Large Language Models (LLMs) when used for Text-to-SQL tasks. The authors use the BIRD-mini dataset to decompose complex SQL queries into progressive sub-queries and analyze hallucination propagation. The study identifies three findings: (1) recurrent hallucinations, (2) final-step emergence, and (3) the impact of history on recurrent hallucinations and emergent hallucinations."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThe writing is generally clear and concise, with complex concepts well explained. \n\n2.\tThe paper attempts to study an important research question: hallucinations of LLMs."}, "weaknesses": {"value": "1.\tThe authors introduce two new hallucination categories—recurrent hallucinations and emergent hallucinations—but the motivation behind defining these specific types is not clearly explained.\n\n2.\tThe reasoning behind the formulation of the three research questions is not well articulated. Moreover, the paper does not provide clear or explicit answers to these questions.\n\n3.\tTable 3 serves primarily as a validation of the experimental setup rather than a key result. Including it in the main paper distracts from the core findings. It would be more appropriate to move this table to the appendix, with only a brief summary of its results mentioned in the main text.\n\n4.\tThe contribution of this paper is weak. The drill-down framework provides limited guidance on how to address or mitigate LLM hallucination issues in practice.\n\n5.\tSection 3.2 should be in the experiment part."}, "questions": {"value": "1.\tThe entire experimental setup is built around the BIRD-mini dataset. How would the proposed framework perform on other Text-to-SQL benchmarks or domains? Could it be generalized or extended to broader settings beyond BIRD-mini?\n\n2.\tIn Section 3.3, the authors mention that applying their framework to the full BIRD-dev dataset would lead to a \"prohibitive analysis cost.\" Does this imply that the framework is not scalable to larger datasets? How do the authors plan to address scalability challenges given the increasing size of modern Text-to-SQL benchmarks?\n\n3.\tIn Section 6.1, Figures 3 and 4 present conditional probabilities of recurrent and emergent hallucinations. Could the authors also report the unconditional probabilities of these hallucination types to provide a more complete understanding of their overall prevalence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "50jUR9dbF9", "forum": "vFlz8jsfhb", "replyto": "vFlz8jsfhb", "signatures": ["ICLR.cc/2026/Conference/Submission23522/Reviewer_dKsh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23522/Reviewer_dKsh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23522/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988561315, "cdate": 1761988561315, "tmdate": 1762942696807, "mdate": 1762942696807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}