{"id": "pNwCWIBHBC", "number": 9243, "cdate": 1758116080945, "mdate": 1759897735768, "content": {"title": "Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning in LLMs", "abstract": "Large Language Models (LLMs) are increasingly applied to complex tasks that require extended reasoning. In such settings, models often benefit from diverse chains-of-thought to arrive at multiple candidate solutions. This requires two competing objectives: to inject enough stochasticity to explore multiple reasoning chains, and to ensure sufficient accuracy and quality in each path. Existing works pursue the first objective by increasing exploration at highly uncertain steps with higher temperature or larger candidate token sets, while others improve reliability by rejecting samples with low confidence post generation, implying that low confidence correlates with low answer quality. These two lines of thought are in conflict, as they conflate different sources of uncertainty. To resolve this, we argue that the decoding rule should be calibrated by *correctness*, not confidence alone. We should sample from tokens with higher estimated correctness, and reduce sampling where expected correctness is low. We propose simple strategies that achieve this goal: **Greedy-Threshold** makes sampling greedy at very low confidence steps. **Calibrated-TopK** and **Calibrated-ε** set truncation threshold based on estimated rank-wise correctness. Together, our findings challenge prevailing heuristics about decoding under uncertainty, showing consistent gains across math and general reasoning benchmarks.", "tldr": "LLM sampling should be reduced at high uncertainty tokens", "keywords": ["Sampler", "model uncertainty", "LLM reasoning", "min-p", "calibration", "chain-of-thought", "self-consistency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0291859fa0e361f479d43b1b3d18a8dce60f56f1.pdf", "supplementary_material": "/attachment/4abed2f174d46f711802c4478451ecceaf492855.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a new method that samples based on correctness during the decoding process. The authors conduct comprehensive experiments to demonstrate the effectiveness of their approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea is both novel and elegant. The authors approach the problem from a fresh perspective and achieve the goal of evaluating correctness without relying on complex architectures.\n\n2. The motivation behind the work is strong, and the authors provide a comprehensive and convincing analysis.\n\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. The experimental results indicate only marginal improvements. In the main results table, the accuracy gain is approximately 1%, which appears to be relatively minor.\n\n2. It's valuable that the paper includes results on GPT-OSS; however, the evaluations on Qwen are limited to smaller model sizes. It would strengthen the paper to include results on larger models, such as the 7B or 8B variants, or even larger if available."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "E0ZCZbwgcz", "forum": "pNwCWIBHBC", "replyto": "pNwCWIBHBC", "signatures": ["ICLR.cc/2026/Conference/Submission9243/Reviewer_cpeR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9243/Reviewer_cpeR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836189718, "cdate": 1761836189718, "tmdate": 1762920894879, "mdate": 1762920894879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "paper presents an interesting and ambitious analysis of Correctness-First Decoding for Better Reasoning in LLMs. It’s well-structured and follows a logical argument, but several areas could be strengthened for clarity, rigor, and reader engagement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction effectively frames the research question and contextualizes the problem within existing literature.\n2. The Methods section provides an appropriate research design and statistical treatment. The inclusion of comparative baselines is a strong point.\n3. The experimental design and analytical approach are sound and appropriate for the research question."}, "weaknesses": {"value": "1. The abstract summarizes methods more than findings; it doesn’t highlight the key quantitative results or main contributions.\n2. Some information about sample selection, data splits, or parameter settings is missing, which could affect reproducibility.\n3. The discussion section mostly restates results instead of analyzing their implications or addressing possible alternative explanations.\n4. The conclusion doesn’t emphasize broader implications or concrete future directions, making the ending feel abrupt."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZBh225Cr4d", "forum": "pNwCWIBHBC", "replyto": "pNwCWIBHBC", "signatures": ["ICLR.cc/2026/Conference/Submission9243/Reviewer_2drS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9243/Reviewer_2drS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916884086, "cdate": 1761916884086, "tmdate": 1762920894294, "mdate": 1762920894294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that current decoding for reasoning mixes up two kinds of uncertainty:  good uncertainty (many valid continuations) and bad uncertainty (the model is just wrong). So instead of sampling more when the model is low-confidence, they propose “correctness-first” decoding: Greedy-Threshold (go fully greedy when max prob < τ), Calibrated-TopK (adapt k per step using a confidence×rank correctness grid), and Calibrated-ε (a continuous version that maps prob to expected correctness).On GSM8K, MMLU-Pro, BBH, and AIME (with GPT-OSS) these rules consistently give small-but-real maj@k / pass@k gains and can be layered on top of normal samplers which I found to be good."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Very clear problem framing\n- Easy to implement and add to existing samplers and inference is cheap\n- The experiments were on broader side and showed consistency across range of models"}, "weaknesses": {"value": "- The paper seems to relies on task/data calibration; when calibration is noisy or OOD, gains shrink , so it’s not always free wins.\n- From the paper it seems like improvements are incremental, few points  maj@k, big models already strong.\n- The claim low confidence = epistemic, so always sample less is shown mainly on math/reasoning; creative/open-ended generation is only discussed, not tested"}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aXtfaWneR7", "forum": "pNwCWIBHBC", "replyto": "pNwCWIBHBC", "signatures": ["ICLR.cc/2026/Conference/Submission9243/Reviewer_7oG6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9243/Reviewer_7oG6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995249082, "cdate": 1761995249082, "tmdate": 1762920893786, "mdate": 1762920893786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper re-examines predicting tree search in reasoning models by characterising the difference in uncertainty pertaining to randomness and that due to model uncertainty (aleatoric vs. epistemic uncertainty).  The authors then retrofit the sampling model to differentiate between these two potential sources of uncertainly through their proposed **Greedy-Threshold** sampling, where they calibrate the model's own certainty prediction (as modeled by final-layer logit) against that of the ground truth through Calibrated-$\\epsilon$ mapping."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* I enjoyed reading this paper.  It is well-structured and presents well-founded arguments for the need for a calibration grid and its derivates that are then used to decide on rollouts.\n* Analysis that mid-confidence bins benefit from diverse sampling is intuitive and shown in S2.2.  This is intuitive but needed to be shown in contrast to the prevailing notion that low-confidence requires diverse sampling. \n* The paper uses open-weight models Qwen2.5 and Llama, and later the recent GPT-OSS 20B as an advanced LRM.  This aids reproducibility.  \n* The paper also experiments over various task datasets that are relevant to show cross-task applicability."}, "weaknesses": {"value": "* I wished more information from the Appendix on failure cases (e.g., A.3 and A.6) could make it into the paper proper.  It helps to give a more concrete form with respect to sampling when grounded to an example.\n* Same with respect to A.8, the relation to temperature could be given stronger theoretic guidance and derivation.  This part should tie in elegantly, but in the current submission, does not give a sufficiently unified presentation.\n* The non-monotonicity of the calibration grids on page 23 deserve a bit of discussion.  The bottom of the grids somewhat reinforce your arguments, but here is is not clear whether sample sparsity also plays a role."}, "questions": {"value": "* Why choose 10 bins?  Could you better fit this as an optimisation parameter where the bin sizes also correlate with transitions in the calibration bin to improve performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DeAwGbtMb5", "forum": "pNwCWIBHBC", "replyto": "pNwCWIBHBC", "signatures": ["ICLR.cc/2026/Conference/Submission9243/Reviewer_2ZR5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9243/Reviewer_2ZR5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762048583530, "cdate": 1762048583530, "tmdate": 1762920893426, "mdate": 1762920893426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}