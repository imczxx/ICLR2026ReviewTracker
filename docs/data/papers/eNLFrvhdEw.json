{"id": "eNLFrvhdEw", "number": 1822, "cdate": 1756946495201, "mdate": 1759898184014, "content": {"title": "GASDU: Gauss--Southwell Dynamic Update for Efficient LLM Fine-Tuning", "abstract": "Parameter-efficient fine-tuning (PEFT) is crucial for adapting large language models (LLMs), yet existing methods trade off accuracy, latency, and compute: some add inference-time modules, others fix a static parameter set that can drift from evolving gradients, and dynamic variants can be costly. We propose \\textbf{GA}uss--\\textbf{S}outhwell \\textbf{D}ynamic \\textbf{U}pdate (\\textsc{GASDU}), which performs \\emph{periodic Gauss--Southwell-$k$} selection: every $M$ steps it uses the current gradients to select the $k$ largest-magnitude coordinates and updates only those entries while reusing the mask until the next refresh. The Top-$k$ selection is implemented in a streaming, tile-wise way to avoid materializing dense gradients, making the amortized refresh cost negligible. Theoretically, under a local Polyak--Łojasiewicz condition, we prove that \\textsc{GASDU} enjoys a linear convergence rate scaled by a measurable gradient-retention factor and show that the factor degrades sublinearly within each refresh window. This sublinear decay implies that a moderate $M$ can maintain a high retention factor, which in turn explains \\textsc{GASDU}'s near–full–fine-tuning behavior. Empirically, \\textsc{GASDU} sustains high retention between refreshes at an extreme parameter budget (0.01\\%) and consistently outperforms strong PEFT baselines and closely tracks or exceeds full fine-tuning across diverse commonsense and arithmetic reasoning benchmarks and LLMs (LLaMA-2/3 and GPT-OSS-20B).", "tldr": "A novel dynamic sparse fine-tuning method for efficient LLM adaption.", "keywords": ["parameter-efficient fine-tuning; LLM Post-training; large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0ef7b1cbab22be7fc5b4f94e3e505015d42432e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a parameter-efficient fine-tuning (PEFT) method for large language models (LLMs) that updates only a dynamically selected subset of parameters. Instead of using static masks or low-rank adapters or incurring high overhead from dynamic sparse methods, GASDU periodically performs Gauss–Southwell–k selection. Linear convergence is proved under a local Polyak–Łojasiewicz (PL) \ncondition. Empirically, GASDU is shown to match full fine-tuning using only 0.01% of parameters, achieving speedup and memory savings."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written. Anonymous reproducibility repository provided.\n\n- The use of Gauss–Southwell-$k$ coordinate selection in a PEFT context is original and well-motivated.\n\n- Provides a clean convergence proof under a local PL condition, which introduces the gradient retention factor, a useful measurable diagnostic linking sparsity, update cadence, and convergence rate.\n\n- Benchmarks span arithmetic reasoning and commonsense tasks. The ablation on refresh period $M$ is convincing."}, "weaknesses": {"value": "- Only 0.01% update budget tested; performance across larger budgets would help understand scaling.\n\n- The authors could clarify the effect of $k$.\n\n- On line 777, one citation is not properly rendered."}, "questions": {"value": "- Theoretically, how does the choice of $k$ impact the convergence behavior of the proposed algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qbQkrnUvJ0", "forum": "eNLFrvhdEw", "replyto": "eNLFrvhdEw", "signatures": ["ICLR.cc/2026/Conference/Submission1822/Reviewer_tmdL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1822/Reviewer_tmdL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700142849, "cdate": 1761700142849, "tmdate": 1762915900487, "mdate": 1762915900487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a PEFT method based on dynamic sparse updates, i.e., dynamically selecting the top-k largest-magnitude gradients for updates and retaining/refreshing the selected mask every M steps. The paper provides both theoretical analysis and an efficient implementation of the method. The approach is evaluated on LLaMA and GPT-OSS models, showing improved fine-tuning accuracy and comparable throughput."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1). The paper is well written, and the proposed method is reasonably novel.\n\n2). It provides theoretical proofs that the proposed sparse update achieves a linear convergence rate and that the masks can be reused during each refresh period.\n\n3). The paper presents an efficient implementation that avoids materializing the full gradient matrix, achieving throughput comparable to other PEFT methods."}, "weaknesses": {"value": "1). Unlike LoRA, the proposed dynamic update method appears to require one full model per downstream task, which limits its scalability for multi-task deployment in practice.\n\n2). The baselines used for comparison are relatively weak. The paper relies on basic baseline settings, whereas both LoRA-based and fixed-mask methods have advanced significantly over the past year (see a few selected references below). Without comparison to state-of-the-art methods, it is difficult to assess the true advantages of the proposed approach.\n\nReferences:\n\n•\tLoRA-One: http://arxiv.org/abs/2502.01235\n\n•\tLoRA-Pro: http://arxiv.org/abs/2407.18242\n\n•\tLoRA-GA: http://arxiv.org/abs/2407.05000\n\n•\tSMT: https://openreview.net/forum?id=GbgCRJedQ7\n\n•\tDiablo: http://arxiv.org/abs/2506.03230"}, "questions": {"value": "In addition to the weaknesses noted above:\n\n1). In Table 3, the ablation on M shows large variance—results for M between 1 and 100 appear to have little impact and show no clear trend. Can the authors provide an explanation?\n\n2). Is the proposed method (GASDU) implemented with DeepSpeed’s FusedAdam optimizer? If so, how are the first and second moments handled?\n\n3). Can this algorithm be applied in tensor-parallel or fully sharded data-parallel settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2umMdSorpn", "forum": "eNLFrvhdEw", "replyto": "eNLFrvhdEw", "signatures": ["ICLR.cc/2026/Conference/Submission1822/Reviewer_RGWn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1822/Reviewer_RGWn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835279434, "cdate": 1761835279434, "tmdate": 1762915900073, "mdate": 1762915900073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GASDU, a dynamic sparse fine-tuning method that periodically selects top-k gradient coordinates (Gauss–Southwell rule) and updates only those parameters, reusing the mask for several steps. It provides convergence analysis under a local PL condition and conducted numerical experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Simple and well-motivated dynamic sparse update rule.\nClear theoretical analysis with convergence guarantees under a PL assumption."}, "weaknesses": {"value": "1. The proposed method appears to work only with plain SGD. When combined with adaptive optimizers such as Adam, the dynamic masking conflicts with momentum updates: maintaining moment estimates for all parameters contradicts the claimed memory efficiency, while periodically resetting them typically leads to instability and poor convergence. If only plain SGD is used, there is little need for a separate algorithm—SGD updates can be directly fused into backpropagation (i.e., updating parameters as gradients are computed and then clearing them), which essentially replicates the proposed masking behavior.\n2. The paper lacks comparisons with recent LoRA variants (e.g., LoRA-One, LoRA-GA, MiLoRA), which substantially improve fine-tuning quality and represent stronger baselines.\n3. Because the proposed algorithm updates entries across the original weight matrices rather than using lightweight adapters, it loses one of the key benefits of PEFT—the ability to store and switch between multiple small task-specific adapters. GASDU cannot easily support such modularity.\n4. The theoretical analysis mainly restates the classical PL convergence result with minor modifications for a masked gradient. The provided theory does not offer clear insight into why the proposed method should outperform existing approaches or how it explains the observed empirical gains."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CPlbRbhVT2", "forum": "eNLFrvhdEw", "replyto": "eNLFrvhdEw", "signatures": ["ICLR.cc/2026/Conference/Submission1822/Reviewer_VzyC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1822/Reviewer_VzyC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928723241, "cdate": 1761928723241, "tmdate": 1762915899694, "mdate": 1762915899694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a new fine-tuning method that updates only a tiny fraction of model weights by refreshing a sparse mask every few/M steps using a Gauss–Southwell selection. By selecting only top-k gradients, the method retains training efficiency through lower memory and compute costs. The authors further show improved convergence speeds and that the approach can maintain on par performance (or at least better than previous PeFT) with full fine-tuning despite using a subset of the parameters. Along with the performance improvement comes comparable (to PeFT methods) speedup and memory efficiency over full-finetuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Broad model compatibility: GASDU demonstrates consistent effectiveness across several large language model families, including LLaMA-2, LLaMA-3, and GPT-OSS-20B. This cross-architecture success indicates that the method is not tightly coupled to any specific model design, highlighting its robustness and potential for general adoption in diverse fine-tuning scenarios.\n2. Theoretical foundation: The method is supported by a convergence analysis under the Polyak–Lojasiewicz condition, providing formal assurance of stable and predictable optimization behavior. By introducing the gradient-retention factor as a measurable quantity, the authors establish a link between theoretical guarantees and empirical performance, strengthening confidence in the method’s reliability.\n3. Practical and efficient design: GASDU’s streaming top-k selection mechanism eliminates the need for dense gradients, substantially lowering per-iteration computational cost and memory usage. This design enables the method to achieve performance that lands somewhere between lightweight approaches like LoRA and more resource-intensive full fine-tuning, maintaining both efficiency and accuracy."}, "weaknesses": {"value": "1. The current experiments cover commonsense reasoning tasks. To better assess the generality of GASDU, the authors should also evaluate it on benchmarks involving longer input sequences and contextual dependencies.\n\n2. The discussion of sparsity-based PEFT methods is missing. This work focuses on updates to a sparse selection of parameters, along with some online update refreshing. There are some recent relevant works in the domain of PeFT, such as S2FT (NeurIPS 2025)[1] and SparseLoRA (ICML 2025)[2], Galore[3]. Including these would strengthen the discussion on sparsity and help illustrate the novelty of refresh-based updates on a sparse set of weights.\n\nReferences:\n\n[1] Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen, \"S2FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity\", NeurIPS 2025\n\n[2] Samir Khaki, Xiuyu Li, Junxian Guo, Ligeng Zhu, Chenfeng Xu, Konstantinos N. Plataniotis, Amir Yazdanbakhsh, Kurt Keutzer, Song Han, Zhijian Liu, \"SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity\", ICML 2025\n\n[3] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, Yuandong Tian, \"GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection\", Arxiv 2024"}, "questions": {"value": "1. Since most tested tasks produce short outputs, such as those in commonsense reasoning, it would strengthen the paper to include evaluations on longer-context and multi-step generation tasks -- like MT-Bench for dialogue or HumanEval for code generation, and arithmetic reasoning benchmarks -- to show how well GASDU scales to extended contexts and more complex output generations. \n\n2. Could the authors provide some insights into why the retention factor decreases for the first half of training, followed by a steep increase, and maintains a high value for the remainder of training? Previous works, such as SparseLoRA[2] and STEP[4] have shown that sparsity is more sensitive in the early stages and hence choose to keep it dense (i.e, have full retention ratios on PeFT at the beginning), before introducing sparsity in later stages. However, interestingly, the trend in Figure 4.0 points to a less aggressive sparsity in the later stage. \n\n[4] Yucheng Lu, Shivani Agrawal, Suvinay Subramanian, Oleg Rybakov, Christopher De Sa, Amir Yazdanbakhsh, \"STEP: Learning N:M Structured Sparsity Masks from Scratch with Precondition\" ICML 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VltbiwVP4P", "forum": "eNLFrvhdEw", "replyto": "eNLFrvhdEw", "signatures": ["ICLR.cc/2026/Conference/Submission1822/Reviewer_RecR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1822/Reviewer_RecR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990164462, "cdate": 1761990164462, "tmdate": 1762915899540, "mdate": 1762915899540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}