{"id": "DscflMFynS", "number": 18735, "cdate": 1758290529501, "mdate": 1762999749173, "content": {"title": "VIDES: VIDEO EDITING IN SECONDS WITH ONE-STEP DIFFUSION MODELS", "abstract": "Text-guided video editing with diffusion models is prohibitively slow, hindered by costly multi-step sampling and inversion. We present VIDES, the first framework to successfully adapt one-step text-to-image (T2I) models for high-quality video editing, addressing the core challenges of inversion, editability, and temporal consistency. To bypass slow iterative inversion, we train a learnable encoder that predicts the initial noise for each frame in a single forward pass. This encoder is trained with a novel Structure-Aware Editing (SAE) loss on a curated dataset of structurally-aligned image pairs, teaching it to preserve the source video's geometry during edits. For temporal coherence, we introduce Unified-Frame Editing (UFE), a technique that concatenates frame latents to facilitate cross-frame attention in a single generation step; for long videos, a sliding-window strategy with an anchor frame maintains global consistency. Our extensive experiments demonstrate that VIDES achieves editing quality comparable or superior to state-of-the-art multi-step methods, while operating approximately 155 times faster. This breakthrough paves the way for practical, real-time video editing applications.", "tldr": "", "keywords": ["Video Editing", "Diffusion Models", "Generative Models", "Text editing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b776c3f6cdd85e19a19651574abda9a0b60de506.pdf", "supplementary_material": "/attachment/4105fea3522ed19959e01303b8da3ec2411c7f04.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents VIDES, a framework for ultra-fast text-guided video editing using one-step diffusion models. Conventional video editing based on multi-step diffusion suffers from extreme latency (hours for a few minutes of video). VIDES introduces three key innovations to make one-step editing feasible and high-quality:\n1. A learnable inversion encoder that predicts the initial noise for each frame in one forward pass, eliminating costly multi-step inversion.\n2. A Structure-Aware Editing (SAE) loss trained on structurally aligned image pairs generated by prompt perturbation, ensuring geometry preservation during edits.\n3. A Unified-Frame Editing (UFE) mechanism that concatenates frame latents for joint processing, leveraging cross-frame attention for temporal consistency, with a sliding-window and anchor-frame strategy for long videos. Extensive experiments demonstrate a ~155× speedup over prior diffusion-based video editing methods while maintaining comparable or superior visual quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Unified-Frame Editing elegantly leverages global attention for cross-frame consistency.\n2. Extensive experiments: both short and long videos, multiple baselines, detailed ablations (SAE loss, UFE, sliding window, anchor frame).\n3. Practical scalability — runs on a single GPU and aligns well with industry use cases for real-time editing."}, "weaknesses": {"value": "1. Limited theoretical justification.\nThe encoder’s success is empirically shown but not theoretically characterized. There is no analysis of how its learned latent space aligns with that of the one-step generator.\n\n2. Dependence on dataset synthesis.\nThe “prompt perturbation” method for generating structure-aligned pairs is clever but synthetic, potentially limiting generalization to real-world video data.\n\n3. Possible overfitting to static structure.\nSince the model is trained on image pairs, it may not fully capture dynamic motion cues or 3D consistency, especially in non-rigid or fast-moving videos.\n\n4. Lack of generalization tests.\nEvaluation is limited to short clips (≤90 frames). There is no discussion of performance on videos with complex motion or strong occlusions.\n\n5. Missing resource analysis.\nWhile the paper claims 155× acceleration, a more detailed runtime breakdown (encoder vs. editing, memory footprint) would enhance reproducibility and credibility.\n\n6. Marginal novelty in components.\nEach component (encoder inversion, structure-aware training, latent concatenation) builds on existing paradigms, though their joint effectiveness is commendable."}, "questions": {"value": "1. How does VIDES perform on open-domain videos (e.g., YouTube, handheld footage) with uncontrolled motion?\n2. Can the SAE loss generalize beyond synthetic pairs to real edit pairs or user-provided source–target pairs?\n3. Does concatenating frame latents ever cause spatial artifacts due to large receptive fields near boundaries?\n4. How does the learned encoder generalize across different one-step diffusion backbones (e.g., SDXL-Lightning vs. DMD2)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i6mbPrqRB4", "forum": "DscflMFynS", "replyto": "DscflMFynS", "signatures": ["ICLR.cc/2026/Conference/Submission18735/Reviewer_6uqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18735/Reviewer_6uqy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761316116659, "cdate": 1761316116659, "tmdate": 1762928447988, "mdate": 1762928447988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely appreciate the time and effort you have dedicated to reviewing our paper. Your detailed and insightful feedback has provided us with valuable perspectives on both the strengths and areas for improvement in our work.\n\nAfter carefully considering your comments, we recognize that our paper requires significant revisions.\n\nOnce again, thank you for your constructive critiques and suggestions. Your feedback has been incredibly helpful in guiding us toward improving our research."}}, "id": "t1hBDEGDLA", "forum": "DscflMFynS", "replyto": "DscflMFynS", "signatures": ["ICLR.cc/2026/Conference/Submission18735/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18735/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762999748505, "cdate": 1762999748505, "tmdate": 1762999748505, "mdate": 1762999748505, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present VIDES, a fast text-based video editing method that leverages a single step image diffusion model. First, they train a single-step inversion encoder which outputs a noise latent that reconstructs the input image when passed through the diffusion model. A novel prompt augmentation technique is employed during the inversion encoder training to improve the editability of the noise latents during inference. Second, the authors present their Unified-Frame-Editing technique, which allows applying the image editing pipeline for video editing in a temporally consistent manner. It incorporates a sliding window of frames for local edit consistency, and a global anchor frame selection strategy for global edit consistency. The authors provide extensive experiments comparing their method to other image-based video editing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe method is fast. It leverages a single step image generator together with a trained single step image inversion model. While the video is processed in a frame-wise manner, batched processing and the single-step inference pipeline make up for it. \n2.\tThe Structure-Aware-Editing (SAE) loss is a nice novelty, enabling the inversion network to output noise maps that are more flexible for editing during inference. \n3.\tThe authors provide several ablations on the design choices for their training pipeline and multi-frame inference strategy."}, "weaknesses": {"value": "Major:\n\n1.\tProvided qualitative results are not good. This is my main concern.\n\n   a.\tThe provided edited results have weak temporal consistency. For example, in the SM files ‘dog_on_car’ edited to tiger - the stripes on the fur change location with each frame. Maybe incorporating some temporal regularization during the inversion process to uncover more correlated noise maps could help with this issue?\n\n\n   b.\tSome of the edited results do not preserve details from the original video well. As seen in the ‘edited_woman_boxing’ edit in the SM files, the background is not consistent with the source video. Maybe localizing the edits using the subject cross attention between text and image tokens could help with maintaining loyalty to the source background?\n\n\n   c.\tThe edited results are noisy. As seen in the ‘edited_vangogh style’ example for the watchtower. Are the extracted noise maps following the expected noise statistics for the diffusion model? An evaluation is recommended.\n\n\n2.\tAnchor frame – The editing method selects a single anchor frame to enforce global edit consistency, but for dynamically changing videos it might be problematic. For example, if the scene includes a rotating object the anchor frame will not be able to enforce consistency across all views. Maybe adding several anchor frames such as in [1] [2] could benefit the method. It would be interesting to see the performance of the method on more dynamically changing examples.\n\nMinor:\n\n1.\t195 – slightly confusing sentence. “the encoder predicts” – sounds like it references the text encoder from earlier in the sentence.\n2.\t“preserving structure preservation” – could be phrased differently\n\n\n\n[1] Geyer et al. 2024. TokenFlow: Consistent Diffusion Features for Consistent Video Editing\n\n[2] Cohen et al. 2024. Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices"}, "questions": {"value": "See weaknesses.\nAdditionally, can the authors provide more video results beyond those 9 included in the supplementary files?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ustp2zcOym", "forum": "DscflMFynS", "replyto": "DscflMFynS", "signatures": ["ICLR.cc/2026/Conference/Submission18735/Reviewer_c9a7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18735/Reviewer_c9a7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733049786, "cdate": 1761733049786, "tmdate": 1762928447093, "mdate": 1762928447093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a fast video editing method leveraging one-step text-to-image diffusion models.Starting from the motivation that three bottleneck, (1) time consuming multi-step inversion process, (2) spatial and (3) temporal inconsistency when using T2I model for video editing, the authors propose training inversion network with Structure-aware editing loss and unified frame editing. Qualitative and quantitaive experiments are conducted with various basline with Vbench metric."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "From the explicit problems in the video editing task, this paper is well motivated and proposes a good research direction that uses T2I model for video editing for efficiency. Training an inversion network with structure-aware editing loss, which uses prompt condition perturbation, is quite novel and impressive. The authors show a lot of quantitative experiments with various baseline comparisons which makes the proposed method confident."}, "weaknesses": {"value": "1. **Poor and Inefficient experiments**\nThe supplementary visual results reveal substantial flickering and noticeable artifacts throughout the provided videos. The generated videos exhibit clear temporal inconsistency with background distortions, and object-level artifacts are evident. For example, in the “dog on car” → “cat on car” editing example, the cat’s face intermittently appears and disappears at the lower-left corner of the frame.\nSuch a level of artifact raises serious concerns regarding the robustness of the proposed method and suggests that the model does not perform reliably. In addition, the **temporal flickering score** in the **VBench** evaluation is not reported, further limiting the quantitative assessment of temporal consistency. Moreover, the paper lacks sufficient video comparison results against baseline models, which are necessary for a fair and comprehensive evaluation of visual quality.\n\n2. **Lack of Inference Time Evaluation**\nOne of the paper’s main claimed contributions is fast video editing. However, there is no quantitative comparison or analysis of inference time against baseline models. Detailed inference time evaluations under varying numbers of frames are also required to substantiate the claimed efficiency.\n\n3. **Proposed Unified-Frame Editing: Not Novel and Questionable for Efficiency**\nThe proposed unified-frame editing design is not novel. The idea of injecting additional tokens into the attention mechanism is a well-known technique already proposed in several prior works [1,2]. Furthermore, when the video sequence becomes longer, the method may suffer from degraded global consistency due to the limited receptive range of the sliding window.\n\n4. **Insufficient Information on the Inversion Network Training**\nThe paper does not report sufficient details regarding the training of the inversion network, such as the dataset used or the number of training images. In addition, while the SAE loss introduces a noise weight term $\\lambda$, no quantitative analysis or ablation results on this parameter are provided. Such missing details make it difficult to reproduce and verify the claimed improvements.\n\nThe paper is well-motivated, and the idea of training an inversion network with SAE loss is novel and interesting. However, the overall qualitative performance is unsatisfactory: the generated videos exhibit **prominent artifacts** and **severe temporal flickering**. Given these significant issues in both robustness and visual quality, I am unable to recommend acceptance of the paper in its current state and must, regrettably, recommend rejection.\n\n---\n\n[1] Geyer, Michal, et al. \"Tokenflow: Consistent diffusion features for consistent video editing.\"\n\n[2] Wu, et al. \"Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation.\""}, "questions": {"value": "1. Why use Gaussian noise for perturbing the prompt condition? I guess there are lots of design choices for perturbing the text condition, such as synonym substitution (e.g, dog-> cat), dropout, and randomly reordering the text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AsV6sQTcX1", "forum": "DscflMFynS", "replyto": "DscflMFynS", "signatures": ["ICLR.cc/2026/Conference/Submission18735/Reviewer_Kcgp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18735/Reviewer_Kcgp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898925275, "cdate": 1761898925275, "tmdate": 1762928446427, "mdate": 1762928446427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VIDES, a diffusion-based framework for video editing using a one-step inversion and one-step generation, achieving significant speed improvements over traditional multi-step diffusion-based methods. Specifically, they train a dedicated encoder to produce more faithful inversions that facilitate effective prompt-based edits. After each frame is inverted individually using this encoder, the resulting feature maps are concatenated and processed jointly by a text-to-image (T2I) generator. This design enables the generator’s self-attention mechanism to operate globally across all frames, promoting both spatial and temporal coherence in the edited video."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The problem the authors tackle—fast video editing—is highly relevant and ambitious. Achieving robustness in this setting could make a substantial contribution to the field of video editing using generative modeling.\n\n2) Training a dedicated encoder within the combined inversion-and-editing paradigm is an interesting conceptual direction that could inspire future research."}, "weaknesses": {"value": "1) Technical justification: The paper’s main novelty—the learnable encoder—is not clearly justified.\n\na) The motivation for the learnable encoder stems from the authors’ claim that multi-step DDIM inversion faithfully reconstructs source frames, which is inaccurate. Even standard multi-step DDIM inversion (with a finite number of steps, as typically used) cannot perfectly reconstruct images. Moreover, in lines 73–77, the authors argue that one-step inversion is problematic due to the subsequent one-step generator. However, the issue lies not only in the generator but also in the poor inversion quality itself. Overall, the theoretical justification for this argument is weak.\n\nb) The encoder is trained by perturbing the text embeddings, assuming that using the same noise with slightly different prompts produces well-aligned edited images. In practice, this does not hold—small perturbations may preserve alignment but result in negligible edits, while larger perturbations may yield misaligned outputs ([arXiv:2304.06140]). The dataset creation (e.g., wolf → small perturbation) theoretically does not guarantee alignment between source and edited images.\n\n(c) If this encoder works as claimed, it could already enable extremely fast image editing, yet this potential contribution is not emphasized.\n\n2) Lack of quantitative support for runtime: the paper argues that VIDES is ~155× faster than multi-step diffusion-based methods, but no information or quantitative evidence supporting this claim is provided. It is unclear whether this runtime refers to end-to-end editing or only inversion/generation separately. The UNet processes concatenated feature maps (k×width), which can lead to very large self-attention matrices. \n\n3) Experimental validation: only 5 output videos are provided in the supplementary material, which is insufficient to evaluate the method’s performance.\n\n4) Comparisons to baselines: the paper does not compare to modern text-to-video (T2V) models, which are now standard.\n\n5) Presentation and clarity:\n\na) The paper contains repeated statements, excessive “first, second, third” references, and unnecessary summary at the end of the method section. Equations 1 and 3 are practically identical.\n\nb) Figure 1 (“multi” and “one” part) lacks a clear caption, and the terms are not properly defined when first introduced.\n\nc) Line 342: the use of w and k is confusing; both refer to the number of frames?\n\nd) Line 297: the claim that the UNet works for arbitrary input sizes is not fully supported, particularly for very wide concatenated inputs (k×width), which could introduce artifacts when applied to resolutions or aspect ratios that were not seen during training."}, "questions": {"value": "All my concerns and question were raised in the Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6sucBwv1YC", "forum": "DscflMFynS", "replyto": "DscflMFynS", "signatures": ["ICLR.cc/2026/Conference/Submission18735/Reviewer_qVB8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18735/Reviewer_qVB8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901662782, "cdate": 1761901662782, "tmdate": 1762928445461, "mdate": 1762928445461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}