{"id": "w3Ik8HUyTT", "number": 14153, "cdate": 1758229334104, "mdate": 1759897387398, "content": {"title": "ViPRA: Video Prediction for Robot Actions", "abstract": "Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present *Video Prediction for Robot Actions* (**ViPRA**), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict *both future visual observations and motion-centric latent actions*, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked *flow-matching decoder* that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, ViPRA explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code [here](https://vipra-robot.github.io/).", "tldr": "We present ViPRA, a framework that turns video prediction models into robot policies by learning latent action priors from unlabeled videos and refining them with flow-matching to enable high-frequency control with minimal labeled data.", "keywords": ["vision-language-action models", "robotics", "video prediction", "imitation learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d77d83d92f45f6510cce9dc35419e36d7e107445.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ViPRA, a hierarchical video-based robot control framework that learns discrete latent action representations from large-scale passive human and robot videos and then maps these to continuous robot actions via a flow-matching decoder. The approach jointly predicts future visual observations and latent action sequences during pretraining, and requires only a small amount of teleoperated data for downstream adaptation. The method is evaluated on SIMPLER simulation tasks and several real-world tabletop manipulation tasks, showing improved performance over recent latent-action and vision-language-action baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work provides evidence that hierarchical visuomotor control can benefit from large-scale passive human and robot videos, reducing dependence on action-labeled demonstrations.\n\n- The system attains competitive performance on physical manipulation tasks with only 100–200 teleoperated demonstrations, suggesting good data efficiency in downstream adaptation.\n\n- The integration of flow-matching for action decoding leads to stable, high-frequency command generation suitable for real hardware, addressing common control smoothness limitations in discrete latent policies.\n\n- The paper includes both simulation (SIMPLER) and real-world evaluations, with comparisons against several strong recent methods in the latent-action and VLA literature."}, "weaknesses": {"value": "- **Limited technical novelty.**\nAside from data sacle, this work appears to be primarily an engineering integration of existing components rather than a new conceptual contribution (i.e. latent action learning, chunked action decoding, and flow-matching control) Similar hierarchical designs have been explored in recent systems such as UniVLA, LAPA, and UniPI.\n\n- **Scaling claim not sufficiently validated.**\nThe paper attributes improvements to pretraining with large-scale passive human videos, but no controlled comparison is provided against training with robot-only video or reduced data subsets. Such experiments would help confirm that data scale, rather than architecture or training strategy, drives the gains.\n\n- **Generalization scope is limited.**\nThe approach is framed as cross-embodiment and generalist, yet evaluations focus on similar 7-DoF manipulators and relatively simple tabletop tasks (pick and place). Broader embodiment diversity or clearer claim boundaries would improve alignment between motivation and evidence.\n\n- **Ablation coverage.**\nThe system includes multiple engineered components (optical flow consistency, VQ bottleneck, multi-stage training). More detailed ablations isolating the effect of each would aid in understanding which design elements are critical to performance.\n\n- **Compute and practicality not discussed.**\nAlthough labeled data requirements are low, the approach requires substantial large-scale video pretraining. Reporting approximate compute/time requirements would help practitioners assess feasibility and compare with alternative methods.\n\nOverall, this work offers a valuable scaling demonstration with solid empirical results, but the methodological novelty is limited, placing it at a borderline accept."}, "questions": {"value": "- One of the most exciting implications of this work is that dynamics priors learned from human interactions may transfer across embodiments. Do the authors have any preliminary evidence or insights into what kinds of human video content (e.g., fine manipulation vs. gross motion) most help downstream control? Any failure cases that suggest limits of human-to-robot transfer?\n\n- Have the authors attempted to manipulate latent action tokens intentionally to achieve specific motion patterns (e.g., alter motion direction, smoothness, or speed)? Observing consistency or interpretability here could reveal whether the learned latents are physically meaningful beyond reconstruction.\n\n- Since the method leans heavily on scaling with passive data, at what point does additional video stop helping? Are there any observed regressions or negative-transfer effects when including noisier human videos from less structured environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KsLzYx9XGm", "forum": "w3Ik8HUyTT", "replyto": "w3Ik8HUyTT", "signatures": ["ICLR.cc/2026/Conference/Submission14153/Reviewer_1YWh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14153/Reviewer_1YWh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761602459031, "cdate": 1761602459031, "tmdate": 1762924615684, "mdate": 1762924615684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a pretraining-finetuning framework, ViPRA, that learns discrete latent actions from videos without action labels by minimising perceptual and optical flow consistency objectives. Furthermore, this work proposes to use a flow matching decoder to convert the discrete actions to smoother continuous actions, which greatly prevent local discontinuities of previous works. The authors experiments on both simulation and real-world manipulation tasks and shown that the proposed method can achieve stronger performance."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Very comprehensive related works, greatly aiding the readers in positioning the paper's contributions. \n- The writing was easy to follow and clear, and while the background section is in the appendix, the context provided is enough to have a intuitive understanding of the proposed method. \n- The proposed method regarding representation learning that leverages joint-prediction of pixels and latent actions seems intuitive and shows good performance.\n- The proposed method regarding converting discrete actions to continuous control via flow matching is to my knowledge novel. \n- Conducted comprehensive experiments and analysis, and the experiment results seem impressive (if we can ignore a major weakness for the moment)."}, "weaknesses": {"value": "- Major weakness: For Table 1 and Figure 3, which are the main results in the main manuscript, it is unclear how many seeds/runs/rollouts are experimented and how these values are calculated, so it is not possible to draw any conclusion about the (statistical) significance of the results presented at the moment. \n- \"Up to 22Hz\" in the abstract and introduction is a bit misleading in my opinion, because the real-world experiments were done in 3.5Hz. (i.e. experiments with 22Hz were not done to show the efficacy). \n- Since this work listed on real-world speed as a contribution, several comparison works (LAPA[1]) are not experimented on real world evaluations (i.e. Figure 3) but only in simulation, so it’s unclear how much speed is gained compared to previous works.\n- Minor issues:\n  - Figure 5 is introduced in the manuscript before Figure 4.\n  - Table 6 is not referred nor explained (I assume it’s for appendix E.3, E.4). Furthermore, in Table 6 ViPRA is labelled but should be ViPRA-FM I think.\n  - Apart from KV caching, which is more of an engineering technique for me, it is not obvious why inference speed is gained, since at inference the proposed method performs an extra flow-matching step compared to previous works. \n  - The robot for the real world experiments in Figure 6/8 is not clearly written (I assume it’s Franka).\n  - Not sure how to interpret the results where in LIBERO tasks, $\\pi_0$ and UniVLA both outperform the proposed method ViPRA."}, "questions": {"value": "I will repost some questions that were mentioned in the weakness section here for clarity.\n\n\n$$\\textbf{Suggestions}$$\nS1: Clarify about how the evaluation metrics is calculated in Table 1 and Figure 3.     \nS2: It would be great if the authors can either amend the claim of 22Hz a bit, or perform some real world experiments at 22Hz.     \nS3: Could provide some clarity about ViPRA-AR since it's not really explained anywhere.    \nS4: Minor: I assume the optical flow model RAFT is pretrained and frozen although it is not explained anywhere. It would be great if the authors could clarify.     \nS5: Minor: Could improve page 22’s readability a bit by aligning them properly (e.g. at the top).    \n\n$$\\textbf{Questions}$$\nQ1: Some related works suggest that pixel-level reconstruction is somewhat not as efficient [1][2], what do the authors think about this? Since this work proposes to learn latent actions, can we argue that it would be more effective to learn latent representation features rather than the full pixel reconstruction?     \nQ2: If I understand correctly, in Table 1, compared to LAPA[3] and UniVLA[4], the proposed ViPRA performs quite better in full success rate, showing that leveraging video prediction is quite helpful to learn robust representations for SIMPLER tasks. However, in LIBERO’s experiments, UniVLA performs best. What do the authors think about this?    \nQ3: Following up on Q2, UniVLA is the best performing framework in LIBERO-10, the authors attributed this to UniVLA being optimised for LIBERO. Can the authors further clarify what this means? For example, what kind of performance can we expect on SIMPLER if we somehow optimise UniVLA for it?    \nQ4. Can we also use KV caching for other related works (LAPA, UniVLA) to get improved speed as well (I believe UniVLA already uses action chunking)? If so, it’s unclear to me how much speed is gained compared to previous works.     \n\n---\n\n[1] G. Zhou et al., DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning, arXiv 2024  \n[2] R. Sun et al., Learning Latent Dynamic Robust Representations for World Models, ICML 2024   \n[3] S. Ye et al., Latent Action Pretraining from Videos, ICLR 2025   \n[4] Q.Bu et al., UniVLA: Learning to Act Anywhere with Task-centric Latent Actions, RSS 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fcc2MwQsfo", "forum": "w3Ik8HUyTT", "replyto": "w3Ik8HUyTT", "signatures": ["ICLR.cc/2026/Conference/Submission14153/Reviewer_cNx8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14153/Reviewer_cNx8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919629215, "cdate": 1761919629215, "tmdate": 1762924615265, "mdate": 1762924615265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ViPRA, a framework for learning robot policy from action-free videos by incorporating video prediction and latent policy learning. The key idea is to pretrain a video–language model to jointly predict both (i) future visual frames and (ii) motion-centric latent actions that summarize local dynamics, guided by perceptual and optical flow consistency losses. These latent representations are mapped to continuous action space through a flow-matching decoder trained on a small number of teleoperated demonstrations.\nThe paper claims that this “video prediction + latent action” pretraining allows robots to leverage large-scale unlabeled human and robot videos, achieving improvements on both simulation benchmark and real-world tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a major challenge in robot learning—leveraging large-scale actionless videos for control. Using video prediction to inject physical dynamics into latent actions is conceptually coherent and builds upon trends in world-model-based control. Evaluation spans both simulation (SIMPLER benchmark) and real-world manipulation (Franka bimanual setup). ViPRA gets better performance comparing against plausible baselines including LAPA, UniVLA, π0, and diffusion-policy variants."}, "weaknesses": {"value": "1. **Contribution**\n- Unclear novelty: It is not fully clear whether ViPRA introduces a fundamentally new paradigm, or whether it can be viewed as a hybrid of LAPA (latent action tokenization) and UVA (unified video and action prediction).\n\n2. **Codebook Design**\n- Codebook size (|C| = 8) appears extremely small compared to typical VQ-based latent action works (e.g., 128–8192).\n- No ablation or justification is provided for this choice, nor evidence that such a small capacity suffices to capture motion diversity.\n- The paper should include:\n(i) Ablations over codebook size (8 / 32 / 128 / 512 / 8192).\n(ii) Quantitative codebook utilization metrics (entropy, perplexity, diversity).\n\n\n3. Data Scale, Composition, and Scalability\n- Scaling behavior: Although the model claims to leverage “large-scale actionless videos,” the dataset (~400K clips) remains moderate.\nThere is no scaling analysis showing how performance varies with the number of pretraining videos.\n- A performance–vs–data-size curve would clarify whether ViPRA is still data-limited.\n- Human–robot ratio: The impact of mixing human and robot videos is not studied. Different ratios may have large effects on transfer and generalization; sensitivity curves or ablations are needed.\n- Generalization limitation: Without scaling or compositional studies, it is unclear if the model can extend to larger internet-scale video corpora.\n\n4. Latent–Action Semantics and Alignment\n- The alignment between learned latent actions and ground-truth actions (on datasets where GT is available) is not quantified, which would verify whether latent tokens actually capture actionable dynamics rather than visual motion.\n- Would the latent action learned from multiple sources enable a unified action space between multiple embodiments? \n\n5. Optical-Flow Supervision and Robustness\n- The $L_{flow}$  loss is claimed in the contribution but lacks ablation and robustness analysis.\n- How much performance degrades without L_flow?\n- How stable is RAFT-based flow under high ego-motion, blur, or occlusion?\n- Would learned or multi-frame flow estimators perform better?\n\n6. Missing or Incomplete Baselines\n- UVA is cited but not directly compared, despite strong overlap in video-conditioned policy learning. Including UVA as an explicit baseline would help contextualize improvements."}, "questions": {"value": "Please refer to the weakness part. I would like to raise my score once the concerns are resolved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eBuGBj23k3", "forum": "w3Ik8HUyTT", "replyto": "w3Ik8HUyTT", "signatures": ["ICLR.cc/2026/Conference/Submission14153/Reviewer_mq1Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14153/Reviewer_mq1Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995500650, "cdate": 1761995500650, "tmdate": 1762924614548, "mdate": 1762924614548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ViPRA, a framework that converts video prediction models into robot policies by learning motion-centric latent actions from unlabeled human and robot videos. Its core contributions are a method to extract these physically-grounded latent actions using perceptual and optical flow losses, a pretraining strategy that jointly predicts future video frames and action sequences, and a flow-matching decoder that enables smooth control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Creatively combines video prediction, latent actions, and flow matching in a novel \"what\" (future state) + \"how\" (latent action) pretraining paradigm.\n\n2. The paper is well-structured and logically presented, with a clear narrative.\n\n3. The qualitative analysis on latent action representations is interesting."}, "weaknesses": {"value": "1. The authors did not perform a systematic ablation study on the loss components for the latent action model.\n\n2. The real-world tasks, while commendable, are primarily table-top pick-and-place variants. The paper does not demonstrate generalization to tasks requiring significant non-prehensile manipulation (e.g., pushing, sliding, re-orienting), dynamic environments."}, "questions": {"value": "1. How does the model reconcile the fundamental kinematic and dynamic differences between human and robot arms when transferring latent actions? Is there a negative transfer from the \"noise\" of human motion?\n\n2. The entire framework is dependent on the visual perspective of the training videos. Latent actions are also inherently grounded in pixel space. How would performance degrade if the test-time camera viewpoint is different from the training data？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c7wcvaaBrL", "forum": "w3Ik8HUyTT", "replyto": "w3Ik8HUyTT", "signatures": ["ICLR.cc/2026/Conference/Submission14153/Reviewer_jonM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14153/Reviewer_jonM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997848990, "cdate": 1761997848990, "tmdate": 1762924613970, "mdate": 1762924613970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors address the question of how to convert a video prediction model into a robot policy.  They show that instead of directly predicting actions, the video model can predict future visual observations and motion-centric latent actions.  A flow matching decoder to map  these latent actions to robot-specific action sequences.  The entire system runs at 22Hz for low-latency control.  Authors show that the method outperforms baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method is view agnostic, gets data from humans or robots without the need for action labels, can be applied across robot embodiments, and enables low-latency control.\n\n- The appendix provides a lot of detail on the approach and results in the paper.  This is great for reproducibility."}, "weaknesses": {"value": "- While the paper claims that the method can generalize across embodiments, I didn't seen strong evidence to substantiate this.  Experiments seem to be on one manipulator arm.\n\n- It will be good to include the limitations of the proposed work in the paper - this is currently missing."}, "questions": {"value": "- Fig. 2: The connections between the left / right figures is unclear.  Where does the left figure fit in on the right?  Where does the output of the left figure (the latent actions) go as input on the figure on the right? Is the \"Latent Action Embedding E_\\phi\" block in the right figure represented by the entire left figure?\n\n- If I understand correctly, the paper mentions both discrete latent actions as well as continuous latent actions.  If this is right, then it's unclear what is the difference between these and why the need for both discrete and continuous latent actions.  (The continuous latent actions are then decoded to continuous action chunks.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IRoA6pAwJR", "forum": "w3Ik8HUyTT", "replyto": "w3Ik8HUyTT", "signatures": ["ICLR.cc/2026/Conference/Submission14153/Reviewer_QDdF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14153/Reviewer_QDdF"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762107192525, "cdate": 1762107192525, "tmdate": 1762924613589, "mdate": 1762924613589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}