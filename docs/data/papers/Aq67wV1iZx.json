{"id": "Aq67wV1iZx", "number": 13396, "cdate": 1758217375928, "mdate": 1759897440428, "content": {"title": "Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks", "abstract": "Object orientation understanding represents a fundamental challenge in visual perception that underpins critical real-world applications like robotic manipulation and augmented reality. However, current vision-language benchmarks fail to isolate and evaluate this core capability, often conflating it with positional relationships (such as above/below or proximity between objects) and general scene understanding. To address this, we introduce \\textbf{DORI} (\\textbf{D}iscriminative \\textbf{O}rientation \\textbf{R}easoning \\textbf{I}ntelligence), a comprehensive hierarchical benchmark that establishes object orientation perception as a primary evaluation target. DORI rigorously assesses four essential dimensions of object(s) orientation comprehension: frontal alignment, rotational transformations, relative directional relationships, and canonical orientation understanding. DORI provides valuable insights on how existing multi-modal systems process and understand object orientations through carefully curated tasks from 14 sources that spans $67$ object categories across synthetic and real-world scenarios. Our evaluation of $18$ state-of-the-art vision-language models using DORI reveals critical limitations: even the best models achieve only $54.2\\%$ accuracy on coarse tasks and $33.0\\%$ on granular orientation judgments, with performance deteriorating substantially for tasks requiring reference frame shifts or compound rotations. These findings demonstrate the urgent need for dedicated orientation representation mechanisms in future architectures, as models show a systematic inability to perform precise angular estimations, track orientation changes across multiple viewpoints, and understand compound rotations—suggesting fundamental limitations in their internal 3D spatial representations. As the first diagnostic framework specifically designed for advancing orientation awareness in multimodal systems, DORI offers immediate implications for improving robotic control, 3D scene reconstruction, and human-AI interaction in physical environments", "tldr": "", "keywords": ["Orientation Understanding", "3D Scene Understanding", "MLLM Probing", "Benchmark Dataset", "Computer Vision"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/858683b4e0835ed88125d4e899efd88a3e5906c4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a hierarchical benchmark that evaluates MLLM’s ability to understand and reason about orientation. The paper evaluates multiple MLLMs on the benchmarking and shows poor performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The motivation behind the curated dataset is well justified. The curated dataset is diverse and contains common objects that shall be viewed by MLLM during pre-training. \n* The paper has a clear presentation of the dataset and complete evaluation of the popular open-source and closed-source MLLMs.\n* This dataset has a potential be improved to 3D. If so, this will be very beneficial for active learning and robotic pre-training, etc.."}, "weaknesses": {"value": "* I have a concern about whether the \"counter-clockwise\" and \"clockwise\" are consistently defined. In a 3D setting, when talking about rotation, we always need to specify the direction of the z-axis. But such information is not provided in the dataset.\n* I also have a concern about whether \"face toward\" is well defined. This clearly requires the described object to have a \"face\" that is visually decidable. If the object is a human, it is simple. But for other objects like tables or sofa, this language may not apply.\n* This is not a weakness. Since the tasks proposed in the paper mostly require 3D reasoning, it may make the dataset stronger if 3D point cloud or depth are also provided for those simulated images."}, "questions": {"value": "No question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mHOWMdWzF0", "forum": "Aq67wV1iZx", "replyto": "Aq67wV1iZx", "signatures": ["ICLR.cc/2026/Conference/Submission13396/Reviewer_X2sf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13396/Reviewer_X2sf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524996873, "cdate": 1761524996873, "tmdate": 1762924031352, "mdate": 1762924031352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the DORI benchmark, developed to specifically evaluate how well Multimodal Large Language Models (MLLMs) understand object orientation. DORI uses a cognitive science-informed approach, assessing orientation perception across multiple facets including how objects face the viewer, how they change with rotation, their orientation relative to other objects or viewpoints, and their typical 'right-side-up' state. The evaluation includes both basic categorical questions and more demanding fine-grained angular questions. It is applied to a substantial amount of real and synthetic images (over 13k images from 14 sources) with structured prompts. Experiments involving 18 MLLMs indicated difficulties in this dataset, particularly in making precise orientation judgments versus simpler classifications. Notable performance declines happen when tasks required understanding rotations or shifts in perspective. The findings suggest current models may lack robust internal mechanisms for representing and reasoning about object orientation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The most noticeable contribution is the dataset collected. The benchmark's hierarchical structure, decomposes orientation related questions into four dimensions, which care frontal alignment, rotational transformations, relative orientation and canonical orientation, is quite inspiring. The inclusion of both coarse and fine-grained questions allows a more comprehensive assessment of model proficiency\n\n2. The paper effectively identifies and addresses a limitation in existing MLLM, which is the lack of ability to assess object orientation understanding, separate from general spatial reasoning. This dataset helped to evaluate more detailed object orientation understanding ability in MLLM.\n\n3. DORI is constructed from a quite diverse set of images (13,652 images from 14 sources), which include both real-world and synthetic data. The evaluation is conducted across 18 different MLLMs, providing a quite holistic benchmark evaluation."}, "weaknesses": {"value": "1. The presentation of the paper can be improved. Limited examples are provided for the VQA questions involving canonical orientation. I have remaining concerns on these types of questions since canonical orientation or frontal alignment itself might remain inherently ambiguous for certain object types, like symmetric ones. This might introduce noise into the ground truth and evaluation, and I am interested in seeing how they are addressed more detailedly.\n\n2. Another limitation is the absence of empirical validation showing that performance on DORI actually correlates with MLLM capabilities in real-world applications like robotic manipulation or autonomous navigation. The paper claims relevance but doesn't demonstrate a predictive link between benchmark scores and success on applied tasks requiring orientation understanding. \n\n3. The paper's writing can be improved. Some tables exceed text width. In main experiments tables, using vertical axis to separate different task is recommended. Overall, there are some minor presentation issues in the paper."}, "questions": {"value": "Please refer to the weakness.\n\nWill this dataset be released?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XYuLmJ2r3e", "forum": "Aq67wV1iZx", "replyto": "Aq67wV1iZx", "signatures": ["ICLR.cc/2026/Conference/Submission13396/Reviewer_9Ynm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13396/Reviewer_9Ynm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704483630, "cdate": 1761704483630, "tmdate": 1762924030820, "mdate": 1762924030820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces DORI, a benchmark designed to evaluate the orientation perception ability of current multimodal large language models (MLLMs). DORI comprises 13,652 images from 14 sources, forming a total of 33,656 samples. It assesses four key aspects of object orientation understanding: frontal alignment, rotational transformations, relative directional relationships, and canonical orientation comprehension. The results show that even the best-performing models achieve only 54.2% accuracy on coarse-level tasks and 33.0% on fine-grained orientation judgments, with performance degrading significantly on tasks involving reference frame shifts or compound rotations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed benchmark addresses an important problem.\n2. The writing is generally clear and easy to follow.\n3. The related work section is detailed and clearly explains the limitations of existing benchmarks.\n4. The proposed benchmark is novel in terms of its practical usability."}, "weaknesses": {"value": "1. Figure 1 is not very clear and does not effectively convey the definitions of the four task categories. In particular, the examples for rotational transformation and relative orientation appear very similar, making it difficult to distinguish between them.\n2. Although the paper cites many works from related fields such as cognitive science to explain how humans understand rotation, the definitions of the four subproblems lack clear logic and structure. The relationships among them are not well articulated, leaving it unclear whether the proposed categorization is both complete and necessary.\n3. For some objects, the front face is inherently ambiguous (e.g., a table). Although the paper mentions that specific prompt designs are used to define the front face for tested models, such strategies cannot fully resolve these ambiguities. This raises concerns about the correctness and answerability of certain questions.\n4. Based on the above, I suspect that some samples may be ambiguous. However, the paper does not describe any quality control process to ensure dataset accuracy. For a benchmark, it is generally expected that every sample be manually verified to guarantee correctness.\n5. While the paper evaluates several state-of-the-art models, it omits important models such as the InternVL series, and for the Qwen family, only the 3B variant is tested without including larger models.\n6. Lines 418–419: The observed difference may stem from variations in training data, so this conclusion should not be drawn too hastily.\n7. Table 5: Please correct the label from “GPT-4 O” to “GPT-4o.”\n8. The paper claims that the proposed systematic approach isolates orientation understanding from scene perception skills and minimizes confounding factors such as object recognition difficulty, scene clutter, linguistic ambiguity, and contextual distractions that affect existing benchmarks. However, no experiments or examples are provided to substantiate these claims."}, "questions": {"value": "1. In Section 3.1, the definition of the viewing plane is not clearly explained."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UnjA7cIVf4", "forum": "Aq67wV1iZx", "replyto": "Aq67wV1iZx", "signatures": ["ICLR.cc/2026/Conference/Submission13396/Reviewer_Vs7f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13396/Reviewer_Vs7f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975233292, "cdate": 1761975233292, "tmdate": 1762924030357, "mdate": 1762924030357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DORI, a diagnostic benchmark for orientation understanding in MLLMs across four dimensions (frontal alignment, relative orientation, rotational transformation, canonical orientation). \nIt uses standardized MCQ prompts (with a Cannot be determined option) and reports that models handle coarse judgments better than granular angles; token-based fusion appears stronger than linear projection. \nLoRA fine-tuning on DORI reportedly transfers to external spatial benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper cleanly isolates orientation understanding into complementary abilities (frontal, relative, rotational, canonical) and tests them with a coarse–granular design that probes both category and precise angle reasoning. \nThe benchmark is well engineered, and broad model coverage exposes consistent weaknesses. \nFindings are actionable, making DORI a practical diagnostic tool for geometry-sensitive applications."}, "weaknesses": {"value": "- Since prompting is part of the measurement apparatus, please ablate the components to quantify their contribution and ensure models aren’t over-relying on the scaffold rather than vision.\n\n- Ground-truth fidelity and metric design: While synthetic sources yield precise angles, human-annotated natural images can have ambiguous frontality (e.g., symmetric furniture) and unknown camera intrinsics, which may distort a fixed discrete angle taxonomy.\n\n- Architectural claims need stronger controls: The observation that token-based integration > linear projection is compelling but potentially confounded by pretraining data or instruction tuning.\n\n- Human study scale and reporting: Human evaluation covers 30 examples per type with seven experts. This is useful but small."}, "questions": {"value": "- Have you tried free-form numeric responses (regression-style) and then quantized at evaluation time? Do model rankings persist? Please share results with permuted answer choices and removed examples section to quantify prompt-component effects.\n\n- For the claim that token-based fusion > linear projection, can you provide experiments with the same visual backbone and identical instruction-tuning, changing only the fusion scheme? Any results with feature token counts swept to test capacity vs mechanism? (If it's not possible, that's understandable)\n\n- Would you consider scaling the human study to 300–500 items with crowdworkers + expert adjudication, and report results to better anchor the human–model gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GfWrzlRRNw", "forum": "Aq67wV1iZx", "replyto": "Aq67wV1iZx", "signatures": ["ICLR.cc/2026/Conference/Submission13396/Reviewer_dzt4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13396/Reviewer_dzt4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994783580, "cdate": 1761994783580, "tmdate": 1762924029731, "mdate": 1762924029731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}