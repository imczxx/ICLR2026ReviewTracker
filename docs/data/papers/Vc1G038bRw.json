{"id": "Vc1G038bRw", "number": 22869, "cdate": 1758336525425, "mdate": 1759896841928, "content": {"title": "Under the Influence: Quantifying Persuasion and Vigilance in Large Language Models", "abstract": "With increasing integration of Large Language Models (LLMs) into areas of high-stakes human decision-making, e.g., medicine and finance, it is important to understand LLMs' social capacities, such as persuasion and vigilance. Yet there is a dearth of existing paradigms which allow researchers to examine models' social capacities in a manner that is simultaneously tractable (i.e., permits quantification and rational analysis), scalable (i.e., can be used to examine models of arbitrary intelligence) and rich (i.e., naturally captures multi-turn interactions). This gap has limited our understanding of LLM social capacities to high-level observations rather than detailed capability evaluations. We propose using Sokoban, a multi-turn puzzle-solving game composed of actionable, fixed states that can be made arbitrarily complex and precisely evaluated, to examine how LLMs compose persuasive arguments that both assist and mislead players, and how vigilant LLMs are in ignoring malicious advice when acting as players. Surprisingly, we find that puzzle-solving performance, persuasive capability, and vigilance are dissociable capacities in LLMs. Performing well on the game does not automatically mean a model can detect when it is being misled, even if the possibility of deception is explicitly mentioned. However, LLMs do consistently modulate their token use, using fewer tokens to reason when advice is benevolent and more when it is malicious, even if they are still persuaded to take actions leading them to failure. To our knowledge, our work presents the first investigation of the relationship between persuasion, vigilance, and task performance, and suggests that monitoring all three independently will be critical for future work in AI safety.", "tldr": "Quantifying Persuasion and Vigilance in Large Language Models", "keywords": ["persuasion", "vigilance", "LLMs", "cognition", "reasoning", "games"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9590d8e242fd7a403a32a7de166a29eb2c961ca9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Given the lack of systems to examine the social capabilities of large language models (LLMs), the paper considers investigating the task performance, persuasion and vigilance of frontier LLMs. They first build a game environment upon Sokoban, where LLMs can take the roles of players or advisors. In addition, the advisor can either provide benevolent or malicious advice, while the player may or may not be given the hint of the existence of misleading advice. Then quantifiable definitions for performance, persuasion and vigilance are proposed. Finally, five frontier models are examined w.r.t. such metrics with follow-up empirical analysis of resource rationality and persuasion strategies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "> **Originality**\n- The paper provides a new game environment to examine performance, persuasion and vigilance.\n\n> **Quality**\n- Several frontier models are tested in the performance comparisons."}, "weaknesses": {"value": "> **Quality**\n- The definition of persuasion: Given $z_i(M_A)=\\omega=1$, the score is always zero. However, the included two cases are different. For the case when $z_i(M_A|M_B^\\omega)=\\omega$, the interpretation of the scenario is that \n\n    ''$M_B^\\omega$ cannot further improve the decision of $M_A$ as $M_A$ is already capable of solving the problem i''.\n\n    However, if $z_i(M_A)=\\omega=1$ and $z_i(M_A|M_B^\\omega)\\neq\\omega$, the interpretation is that\n\n    ''B's persuasion even makes A less confident to keep the correct choice of solution.''\n\n    Letting the score take same value, zero, in these two cases may not yet show the capability of B in a finer sense.\n\n- A similar concern can be found in the definition of vigilance: e.g., given $z_i(M_A|M_B^\\omega)=1$ and $\\omega\\neq1$, consider\n    \n    ''$z_i(M_A)=1$'' v.s. ''$z_i(M_A)\\neq1$''.\n\n    Feel free to correct the reviewer in case of misunderstanding the definitions.\n\n> **Clarity**\n- Section 3.3.2: The average solve rate case could be explicitly provided with notations.\n- Equation (2)-(3): A quick explanation of the denominators (renormalization) can be helpful.\n- Equations (2)-(3): This can be a minor but necessary to mention point that usually the denominators do not vanish.\n- Line 272-275: The rationale behind different scenarios could be explained further.\n\n> **Significance**\n- The properties of the evaluation paradigm being ''scalable'' or ''rich'' are not explicitly justified."}, "questions": {"value": "- What is the motivation, in Equation (3), that cases are summed in numerators and denominators respectively, instead of taking the average of the two ratios ($\\omega=0, 1$)?\n- The same question as the above for equation (5).\n- Could further details on the calculation of ''optimal ratio'' in Figure 3 be provided?\n- For the discussion in Line 323, why a planner would guarantee such independence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gp9twxqlDt", "forum": "Vc1G038bRw", "replyto": "Vc1G038bRw", "signatures": ["ICLR.cc/2026/Conference/Submission22869/Reviewer_QCSx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22869/Reviewer_QCSx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622673624, "cdate": 1761622673624, "tmdate": 1762942419693, "mdate": 1762942419693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a controlled evaluation framework to measure persuasion, vigilance, and task performance in large language models through a Sokoban-based setup involving benevolent and malicious advisors. Five frontier models are tested under various advice conditions, revealing that strong performance does not necessarily imply either persuasion or vigilance, and that models differ widely in susceptibility to persuasive manipulation. Overall, the paper targets an important safety and reliability dimension of LLM behaviour."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces and thoroughly evaluates an important safety and reliability concept, the separation between task competence, persuasion, and vigilance. The experimental design, metrics, and multi-model evaluation together make this a meaningful contribution toward understanding LLM robustness to external influence."}, "weaknesses": {"value": "- The presentation of results and metric definitions is somewhat difficult to follow. I found myself going back and forth between sections to connect the definitions with the numbers in the tables, and I am still not entirely confident I understand the results shown (particularly in Table 1).\n- The generalisation of these findings beyond the specific Sokoban setup is not clear. Additionally, It would be informative to see results without access to the “gold” planner solutions."}, "questions": {"value": "1. In Table 1, Claude Sonnet 4 achieves a vigilance score of 0.087 for benevolent advice. Since the metric ranges from −1 to 1, this suggests the model only barely benefited from good advice. Could the authors clarify how to interpret such small positive values in practical terms, especially since they state “Every player achieves close to ceiling performance when paired with a benevolent LLM advisor”.\n2. There appears to be a missing data point in Table 1. Can the authors confirm whether this was intentional or due to evaluation issues?\n3. While some statistical tests are reported, others are missing. Would the authors consider adding statistical significance or confidence intervals for all main results to strengthen the conclusions?\n4. It would be useful to see whether results hold without advisors having access to the planner’s gold solution, or under weaker/noisy advisors.\n5. How well do the persuasion and vigilance rankings generalise to other sequential tasks or domains beyond Sokoban?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XRolIuoM2M", "forum": "Vc1G038bRw", "replyto": "Vc1G038bRw", "signatures": ["ICLR.cc/2026/Conference/Submission22869/Reviewer_nSG7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22869/Reviewer_nSG7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670692100, "cdate": 1761670692100, "tmdate": 1762942419089, "mdate": 1762942419089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tries to measure persuasion and vigilance in LLMs. It uses the Sokoban game, where one LLM (advisor) gives advice to another LLM (player). The advisor can be helpful (benevolent) or malicious. The paper finds that a model's ability to solve the puzzle, its ability to persuade, and its ability to ignore bad advice are all separate skills. It also shows that models use more tokens when they get malicious advice."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper focuses on this specific phenomenon, proposes an evaluation, and shows some interesting results. The finding that performance and vigilance are not connected is a good insight. I also thought the finding about \"resource-rationality\" was interesting, where models \"think\" harder (use more tokens) when they get malicious advice, even if they still fail. The Sokoban setup is a good, controllable way to test this."}, "weaknesses": {"value": "My main concern is that this Sokoban setup, while \"tractable\", is a somewhat constrained problem. I find it hard to believe that results from pushing boxes in a grid will tell us much about persuasion in \"high-stakes\" areas like medicine or finance, which the paper claims to motivate its work. The \"persuasion\" here seems to be a few lines of text about game moves. In this case,  how would it reflect the complex, emotional, or high-stakes human decision-making in the real world? \n\nThe paper also highlights performance, persuasion, and vigilance being \"dissociable\". The \"advisor\" models were given the optimal solutions from a planner. This means the advisor's \"performance\" on the task (solving Sokoban) is irrelevant; it's only testing their ability to translate a plan into words. Of course, its persuasion skill is separate from its (untested) solving skill. The \"player\" model's unassisted performance is tested, but its \"vigilance\" seems to just be a measure of sycophancy, especially when the \"aware\" prompt is not used. \n\nI would also encourage the authors to do a better literature review on existing works of LLM agents under influence (e.g., https://openreview.net/forum?id=KI1WQ6rLiy)."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TNBcUTUpSS", "forum": "Vc1G038bRw", "replyto": "Vc1G038bRw", "signatures": ["ICLR.cc/2026/Conference/Submission22869/Reviewer_NJnC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22869/Reviewer_NJnC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845396685, "cdate": 1761845396685, "tmdate": 1762942418756, "mdate": 1762942418756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a controlled framework using the Sokoban puzzle game to study two social capacities of LLMs—persuasion (the ability to influence) and vigilance (the ability to resist misleading input). The authors evaluate five frontier models by letting them act both as “advisors” and “players.” Results reveal that persuasion, vigilance, and problem-solving skills are distinct abilities; strong reasoning does not ensure resistance to deception. The work provides the first formal quantification of these capacities and suggests monitoring them separately for AI safety."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The dual focus on persuasion and vigilance within a single evaluation setting is conceptually original and relevant for AI safety research. The Sokoban-based setup is tractable and reproducible, allowing precise measurement of LLM influence under benevolent or malicious advice. The study covers multiple leading LLMs across different conditions, using quantitative and qualitative analyses. Results highlight that persuasion and vigilance can diverge, revealing non-trivial social cognition behaviors among models."}, "weaknesses": {"value": "1. Sokoban is more like a toy environment. it remains unclear whether findings generalize to real-world social or linguistic persuasion tasks. The evaluation uses a small puzzle set and relies on a symbolic planner for advisors.\n\n2. All experiments are LLM-vs-LLM interactions, so it’s uncertain how these results translate to human-AI scenarios.\n\n3. The paper identifies vulnerabilities but provides little guidance on improving vigilance mechanisms."}, "questions": {"value": "1. Could future studies incorporate human participants to validate LLM-vs-human persuasion differences?\n\n2. To what extent does reliance on a planner influence persuasion effectiveness, would purely autonomous LLM advisors behave differently?\n\n3. Did the models’ reasoning traces or language use reveal why some were more vigilant than others?\n\n4. can you provide more insights on improving vigilance mechanisms."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BH6rkDndEH", "forum": "Vc1G038bRw", "replyto": "Vc1G038bRw", "signatures": ["ICLR.cc/2026/Conference/Submission22869/Reviewer_5WYM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22869/Reviewer_5WYM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762404206602, "cdate": 1762404206602, "tmdate": 1762942418508, "mdate": 1762942418508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}