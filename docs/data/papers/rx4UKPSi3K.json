{"id": "rx4UKPSi3K", "number": 10370, "cdate": 1758168550873, "mdate": 1763698138734, "content": {"title": "Chance-constrained Flow Matching for High-Fidelity Constraint-aware Generation", "abstract": "Generative models excel at synthesizing high-fidelity samples from complex data distributions, but they often violate hard constraints arising from physical laws or task specifications. A common remedy is to project intermediate samples onto the feasible set; however, repeated projection can distort the learned distribution and induce a mismatch with the data manifold. Thus, recent multi-stage procedures attempt to defer projection to clean samples during sampling, but they increase algorithmic complexity and accumulate errors across steps. This paper addresses these challenges by proposing a novel training-free method, Chance-constrained Flow Matching (CCFM), that integrates stochastic optimization into the sampling process, enabling effective enforcement of hard constraints while maintaining high-fidelity sample generation. Importantly, CCFM guarantees feasibility in the same manner as conventional repeated projection, yet, despite operating directly on noisy intermediate samples, it is theoretically equivalent to projecting onto the feasible set defined by clean samples. This yields a sampler that mitigates distributional distortion. Empirical experiments show that CCFM outperforms current state-of-the-art constrained generative models in modeling complex physical systems governed by partial differential equations and molecular docking problems, delivering higher feasibility and fidelity.", "tldr": "", "keywords": ["flow matching", "constrained generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/098c55ee39fe3544b12326cd1ef1042577777b6c.pdf", "supplementary_material": "/attachment/d794097144d994271cfc5426d8247b69d8bdbbe0.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents Chance-constrained Flow Matching algorithm(CCFM)  aimed for a generation of samples given a certain constraint condition.  CCFM introduces the constraint indirectly by applying the chance-modulated constraint, which is then shown to be mathematically convertible to deterministic constraint in the form of euclidean projection.    By repeatedly applying this constraint projection at the time of the inference, the algorithm produces samples  that satisfy the constraint. The paper shows the efficacy of the method  on PDE and Molecular Docking."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "###1 \nBy \"regularizing\" the constraint via  probabilistic formulation, the method outperforms other generative models with constraint. \n\n###2 \nThe converstion of the probabilstic constraint to determistic projections are mathematically justified, along with the guarantee that the intermediate constraint would also make the finally generative samples feasible."}, "weaknesses": {"value": "While the paper tackles an important problem and supports the algorithm is several theoretical guarantees,  the paper seems to lack several components necessary for clarity. Please see the Questions section. \n\nMinor points: In (6), $min_x$ instead of $min_y$?   In (11), is the constraint defined interms of $x_t'$?"}, "questions": {"value": "###1 What is the mathematical goal of the model? What is CCFM guaranteed to generate? For a moment I thought the goal was to generate $P_{target}(\\cdot | constraint)$, but I guess that the model generates $\\pi_{constraint}$ #  $P_{target}$, the pushforward of the Constraint Projection operator to the target distribution?   Is there any theoretical guarantee for what CCFM is producing at the end? \n\n###2 The probabistic parameter $\\alpha$, after all, seems to be acting as the regularizer of the projection operator ; as the regularization strength, its scheduling seems vital in the training.  As a method, both a good heuristics and theoretical support seems warranted.  \n\n###3 The Projection operator seems to require \"if\" statement for each sample in most of the applications discussed here.  Wouldn't this pose a computational risk? It is being claimed that CCFM achieves faster runtime than PCFM, what is the essential computational gain? \n\n###4 In section 4,  CCFM is motivated by the problem of previous works applying repeated projections to the noisy samples---however, the methods suffering from this problem do not seem to be compared against in the experimental section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Nothing in Particular."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZIN0qYuzc7", "forum": "rx4UKPSi3K", "replyto": "rx4UKPSi3K", "signatures": ["ICLR.cc/2026/Conference/Submission10370/Reviewer_fztK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10370/Reviewer_fztK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381978388, "cdate": 1761381978388, "tmdate": 1762921693223, "mdate": 1762921693223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper extends recent repeated projection schemes for training-free constrained generation via projection, to a chance-constrained stochastic optimization framework. This formal viewpoint leads to a practically-relevant algorithm with theoretical guarantees for relatively idealistic settings and positive experimental results on practically relevant problems with general complex constraint sets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a new rigorous viewpoint on constrained generation by transporting standard understanding in stochastic optimization theory. I regard this connection and formalized problem as highly principled and valuable.\n\n- The problem treated (i.e., constrained generation for flows/diffusion) is highly practically relevant and it is still arguably an open problem.\n\n- The paper is well-structured, and well written.\n\n- Both the theoretical analysis presented within sec. 4.3, and experiments within sec. 5 seem mostly convincing."}, "weaknesses": {"value": "- (main concern) Sec. 4.2 explains that the presented opt. problem with probabilistic constraints is intractable and Prop. 1 reformulates it so that it becomes tractable via a deterministic reformulation. But it seems to me that Prop. 1 only tackles the case of linear/quadratic constraints, which arguably never capture typical constraints. I could not understand how the (arguably most common) case of non-convex constraints is managed. As of now, this seems to render the presented chance-constrained algorithmic machinery very limited.\n\n- (main concern) The paper seems to propose an improved repeated-projection approach.  Nonetheless, reading the experiments sec. I could not clearly understand where to find a comparison with current repeated-projection schemes which do not leverage the chance-constrained formulation to determine the intermediate constraints. Where is it? Is it missing? I believe this experimental comparison would be essential to assess the gain of the core contribution of the paper, which is arguably the chance-constrained-based method as a way to improve repeated-projection schemes.\n\n- I have fundamental doubts about the 'entire' repeated-projection approach. It seems to me that constraints are typically defined at the data level (e.g., g(x_0)). Due to the Markovian structure of the diffusion/flow process, a large amount of works [e.g., 1] interpret the sampling process as an MDP. This makes it possible to reduce constrained generation to constrained planning, a standard problem in RL/control, e.g. as done in [2] for the fine-tuning case. This standard RL/control viewpoint renders it possible to automatically derive intermediate deterministic (value) functions which maximization would lead to reward maximization at the last time-step. While the standard projection onto the constraint set C in previous repeated-projection works seems to me to make very little sense, this work arguably determines reasonable constraints for intermediate steps via the linear optimal transport viewpoint. But these constraints are probabilistic (and hence leading to intractable opt. problems), while methods exploiting the dynamic programming structure of the problem seem to lead to formulations corresponding to standard planning/RL tasks, which are arguably easier. Moreover, due to the control theoretic interpretation of classifier guidance [see 3], this logic extends also to classifier guidance given a proper weighting parameter of the 'penalty' term (which can be computed e.g. via Augmented Lagrangian schemes as in [2]). What is the authors opinion about this point?\n\n- (writing, minor) I found the structure of presentation quite confusing. In particular, it seems to me that the chance-constrained viewpoint is leveraged as a way to develop algorithmic machinery to tackle a deterministic constraint problem. I would suggest to first introduce the formal problem on the data-level (i.e., last time-step constraint) and only afterwards introduce the chance-constrained framework implying probabilistic constraints at intermediate steps. Currently, I understood only late in the text that the data-level objective is deterministic rather than probabilistic. Since also a data-level probabilistic objective would make sense, I found this presentation quite misleading.\n\n**References**\n\n[1] Training Free Guided Flow Matching with Optimal Control, 2024.\n\n[2] Constrained Molecular Generation via Sequential Flow Model Fine-Tuning, 2025.\n\n[3] Variational Control for Guidance in Diffusion Models, 2025."}, "questions": {"value": "I am happy to change my score if convincing answers to the posed questions are provided.\n\n- Sec. 2 mentions that Classifier guidance (CG) schemes cannot provide feasibility guarantees. Could the authors clarify (formally) what they refer to with feasibility guarantee and why CG schemes cannot achieve it? \n\n- How is optimization carried out for non-linear/quadratic constraints not leading to deterministic reformulations of intermediate probabilistic opt. problems?\n\n- See questions within the Weaknesses sec. of the review."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RCLKOrUCXS", "forum": "rx4UKPSi3K", "replyto": "rx4UKPSi3K", "signatures": ["ICLR.cc/2026/Conference/Submission10370/Reviewer_xbCA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10370/Reviewer_xbCA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929634492, "cdate": 1761929634492, "tmdate": 1762921692842, "mdate": 1762921692842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Chance-Constrained Flow Matching (CCFM), a training-free modification of flow matching designed to enforce hard constraints during sampling. The method reformulates the standard projection step as a chance-constrained program, allowing constraints to be satisfied with high probability on noisy intermediate states while preserving fidelity to the clean target distribution. Theoretical results claim equivalence between intermediate and final projections under an optimal transport path assumption, and tractable Gaussian surrogates are derived for linear and quadratic constraints. Experiments are presented on two scientific domains: molecular docking and PDE solution generation, showing improved feasibility and efficiency over existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### **Strengths**\n\n- The idea of combining flow matching with chance-constrained programming is conceptually interesting and introduces a probabilistic treatment of feasibility within generative flows, which is novel and potentially useful for scientific modeling.\n- The theoretical exposition is clear and internally consistent under its assumptions, and the connection between intermediate and final feasible sets is elegant (Theorem 4).\n- The method is simple to implement, training-free, and shows promising qualitative results on molecular docking and PDE benchmarks, indicating some practical value."}, "weaknesses": {"value": "### **Weaknesses**\n\n- The experimental evaluation is limited in scope and depth. Only two scientific domains are tested, each with narrow baselines and minimal ablation. The probability scheduler, risk level, and theoretical assumptions (convexity, linearity) are not systematically validated. Since ICLR emphasizes experimental rigor and breadth, the current empirical evidence is insufficient to support the paperâ€™s general claims.\n- The claimed equivalence between noisy and clean projections relies on restrictive assumptions that are unlikely to hold in realistic nonconvex cases like docking, yet no empirical study quantifies the resulting approximation error or its impact on feasibility.\n- The docking and PDE comparisons lack strong baselines and statistical rigor. Metrics are presented without confidence intervals or robustness analyses, and the reported efficiency gains are small or inconsistent across tasks, making it difficult to assess the true advantage of CCFM."}, "questions": {"value": "### **Questions**\n\nBesides the implied ones based on the weaknesses above:\n\n- Do the authors plan to test on other classes of constraints to demonstrate broader generality? \n- Can the authors provide sensitivity analyses for the probability scheduler and risk level across both domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RJL4nrD4dr", "forum": "rx4UKPSi3K", "replyto": "rx4UKPSi3K", "signatures": ["ICLR.cc/2026/Conference/Submission10370/Reviewer_1myh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10370/Reviewer_1myh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977531017, "cdate": 1761977531017, "tmdate": 1762921692418, "mdate": 1762921692418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}