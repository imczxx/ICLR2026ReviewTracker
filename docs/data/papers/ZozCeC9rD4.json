{"id": "ZozCeC9rD4", "number": 17882, "cdate": 1758281624012, "mdate": 1759897147849, "content": {"title": "Numerical Sensitivity and Robustness: Exploring the Flaws of Mathematical Reasoning in Large Language Models", "abstract": "Large language models (LLMs) have made significant progress in the field of mathematical reasoning, but whether they have true the mathematical understanding ability is still controversial. To explore this issue, we propose a new perturbation framework to evaluate LLMs' reasoning ability in complex environments by injecting additional semantically irrelevant perturbation sentences and gradually increasing the perturbation intensity. At the same time, we use an additional perturbation method: core questioning instruction missing, to further analyze the LLMs' problem-solving mechanism. The experimental results show that LLMs perform stably when facing perturbation sentences without numbers, but there is also a robustness boundary. As the perturbation intensity increases, the performance exhibits varying degrees of decline; when facing perturbation sentences with numbers, the performance decreases more significantly, most open source models with smaller parameters decrease by nearly or even more than 10\\%, and further increasing with the enhancement of perturbation intensity, with the maximum decrease reaching 51.55\\%. Even the most advanced commercial LLMs have seen a 3\\%-10\\% performance drop. By analyzing the reasoning process of LLMs in detail, We find that models are more sensitive to perturbations with numerical information and are more likely to give incorrect answers when disturbed by irrelevant numerical information. The higher the perturbation intensity, the more obvious these defects are. At the same time, in the absence of core questioning instruction, models can still maintain an accuracy of 20\\%-40\\%, indicating that LLMs may rely on memory templates or pattern matching to complete the task, rather than logical reasoning. In general, our work reveals the shortcomings and limitations of current LLMs in their reasoning capabilities, which is of great significance for the further development of LLMs.", "tldr": "We use sentence-level perturbation to explore the mathematical reasoning capabilities of large language models.", "keywords": ["Large Language Models", "Mathematical Reasoning", "Sentence-level Perturbation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e9f4ded9a59f3221bf734aa3d248e42cfabd2aea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to perturb GSM8K and AIME25 from three perspectives: \n1. Add irrelevant sentences without numbers.\n2. Add irrelevant sentences with numbers.\n3. Remove some question instructions.\nThe authors claim that the performance of LLMs on perturbed datasets decreases more significantly when problems are perturbed with numbers than without numbers.\nThe decline trend is more obvious when the perturbation intensity is larger.\nThey also find that LLMs can solve part of the problems whose core problem instructions are removed."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The perspective to check perturbation with and without numbers, in some sense, is interesting."}, "weaknesses": {"value": "1. The novelty is a huge issue: The idea of inserting irrelevant context is very similar to GSM-IC. Additionally, the idea of removing the core question instruction is exactly the same as GSM-symbolic and GSM-Plus. The idea of increasing perturbation intensity is similar to E-GSM (which is not discussed in this paper).\n\n2. Many important details about the dataset construction are not revealed, e.g.,\n+ In L205-206, how do you conduct the so-called \"detailed inspection\"? How many annotators are involved? What is their background? Are they well-trained or not? What is the inter-agreement score among different annotators?\n+ In 203-L209, the authors mention that they use GPT-4o to construct a fixed set of candidate sentences. What prompts did the authors use?\n+ In L210-L215, how do you make sure: inserting sentences from another question will not change the final answer of the original question? It might be possible that the answers are changed and become wrong.\n\n3. The evaluation setting is not revealed, which is very important for reproducibility: In L262-L265, what is the maximum response length, and have you ever checked the clip ratio for reasoning models? For AIME25, how many times do you sample to report the accuracy?\n\n4. In Section 4.2.3, you claim that the models rely on memorization to solve problems. I believe this is more related to test set contamination, and I suggest the authors go deeper in this direction.\n\n5. In 203-L209, the added sentences are from a fixed set, and the candidate set is relatively small. Compared to the type \"with numbers\", the choices to insert are smaller. This might influence the conclusion that the performance is more likely to decline for perturbation with numbers than without numbers.\n\n6. The analysis throughout this paper is very superficial:\n+ In Figure 4, the authors report the average accuracy across the whole dataset. However, in my opinion, the authors should check the question-level influence when increasing the perturbation intensity. \n+ What are the intrinsic reasons why LLMs have such a phenomenon under the perturbation?\n+ How to solve this kind of instability facing perturbations?"}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LTdB4P3NDl", "forum": "ZozCeC9rD4", "replyto": "ZozCeC9rD4", "signatures": ["ICLR.cc/2026/Conference/Submission17882/Reviewer_RcEa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17882/Reviewer_RcEa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761038540498, "cdate": 1761038540498, "tmdate": 1762927708695, "mdate": 1762927708695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the robustness of large language models (LLMs) in mathematical reasoning through a sentence-level perturbation framework. The authors introduce semantically irrelevant perturbation sentences, both with numbers and without numbers, inserted into math problems from GSM8K and AIME25 benchmarks at progressively higher “perturbation intensities.” They also design a “core questioning instruction missing” variant to test whether LLMs rely on genuine reasoning or memorization.\n\nExperiments across multiple open-source and commercial LLMs (Qwen2, DeepSeek, GPT-4o, Gemini, LLaMA, Gemma) show that models are highly sensitive to numerical perturbations, while purely semantic perturbations cause smaller but still notable degradation. The authors conclude that current LLMs may rely on pattern matching and memory rather than logical mathematical reasoning, and that perturbations reveal the limits of “robust reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Extensive evaluation across 13 models, two benchmarks, and multiple perturbation levels. Results are reproducible and statistically clear.\n- Highlights a critical limitation—numerical distractibility—that impacts real-world deployment (e.g., in finance or science). The “memorization over reasoning” finding aligns with broader concerns in the field."}, "weaknesses": {"value": "- Similar “distractor sentence” or “context perturbation” methodologies already exist (Shi et al. 2023; Mirzadeh et al. 2024; Huang et al. 2025). The work does not propose a fundamentally new paradigm, only a modest variation.\n- The paper claims to “guarantee that all factual statements are semantically irrelevant to the training samples” (lines 203–206). However, this assumption is not verifiable nor logically guaranteed under the proposed generation procedure.\nThe authors use GPT-4 to generate “20 factual statements semantically irrelevant to the test set” and then randomly insert them into problems. This ensures irrelevance with respect to a single question instance, but not with respect to the broader training or evaluation corpus (e.g., GSM8K, AIME25). Since the factual sentences are drawn from open-domain factual content (math, history, physics, etc.), many of them may still share semantic overlap or topical tokens (numbers, quantities, or entities) with other problems in the benchmark or even with pretraining data. Thus, it is impossible to guarantee that the perturbations are truly “semantically irrelevant” globally. More details are needed to prove your statement.\n- The two perturbation types (“with numbers” vs. “without numbers”) are constructed using two entirely different pipelines: 1, Non-numeric perturbations are GPT-generated factual sentences; 2, Numeric perturbations are sampled from other GSM8K problems (by taking sentences containing numbers). This design introduces uncontrolled confounding variables — the sentence sources, styles, syntax, and lexical distributions differ across these two groups. Therefore, when the paper reports that “performance drops much more sharply with perturbations containing numbers” (Section 4.2.1, Figure 3), it is unclear whether the performance gap truly reflects numerical sensitivity or simply dataset bias. Because the “with-numbers” perturbations are drawn from the same benchmark dataset, they likely carry GSM8K-like phrasing, mathematical entities, and structural cues that overlap with real math problems — effectively producing domain-in-distribution noise, while the GPT-generated sentences are out-of-domain semantic noise. This asymmetry makes the two groups incomparable and invalidates the claim that the only difference lies in “presence of numbers.”\n- Numerous typos and grammatical errors reduce professionalism and clarity (e.g., lines 14 true the, 101 without containing numbers, 179–180). A thorough proofread is essential.\n\n## References\n- Shi et al. 2023, Large Language Models Can Be Easily Distracted by Irrelevant Context\n- Mirzadeh et al. 2024, GSM‑Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\n- Huang et al. 2025, MATH‑Perturb: Benchmarking LLMs’ Math Reasoning Abilities against Hard Perturbations"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mp5S6f0Mjx", "forum": "ZozCeC9rD4", "replyto": "ZozCeC9rD4", "signatures": ["ICLR.cc/2026/Conference/Submission17882/Reviewer_d6tC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17882/Reviewer_d6tC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560680802, "cdate": 1761560680802, "tmdate": 1762927708276, "mdate": 1762927708276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates LLM's robustness to context-irrelevant perturbations in queries. The authors perturb the original query via inserting semantic-irrelevant sentences with or without numbers, and by varying the number of perturbation instances per query. They found that LLMs are more sensitive to numerical perturbations, and performances decline further as number of perturbation increases. They also tried removing the final core question statement in the GSM8K dataset, and found that LLMs tend to find the answers even without being instructed, suggesting memorization or data contamination."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The experiments are straight forward and covers a variety of LLMs.\n3. The core question removal is an interesting discovery."}, "weaknesses": {"value": "1. While the authors claim to propose a novel perturbation method, similar approaches have been explored in the literature. For example, GSM-IC [1] (which you referenced in line 80) found that LLMs can be distracted by irrelevant context, which directly contradicts your statement *\"...still remain open that whether semantically irrelevant perturbations can affect the problem solution\"*. Coleg [2] also investigated extending question context length with (mostly) irrelevant details, and proposed a training strategy to increase model's robustness to such perturbations.\n2. The authors only exposed the LLM's weakness to irrelevant perturbations, but did not propose an effective way to resolve such weakness. This finding alone lack novelty and contribution, since it was already explored in the literature.\n3. The experiments are not comprehensive enough. From my understanding, the reason LLMs tend to be distracted by irrelevant context, is that they are explicitly trained to utilize all available informations in the query during instruction tuning. Based on this hypothesis, it's better if the authors directly prompt the LLMs to ignore irrelevant context, and use their performance in this setting as an additional baseline.\n\n[1] [Large Language Models Can Be Easily Distracted by Irrelevant Context](https://arxiv.org/abs/2302.00093)\n\n[2] [Can LLMs Solve longer Math Word Problems Better?](https://arxiv.org/abs/2405.14804)"}, "questions": {"value": "1. Since the perturbations are all semantic-irrelevant, would it be more appropriate to classify LLM's weakness as poor question understanding / condition extraction rather than flawed reasoning / numerical sensitivity?\n2. Would LLMs be more robust to such perturbations when prompted to ignore irrelevant context?\n3. What's the contribution of this paper, given that previous works have already studied the effects of irrelevant perturbations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2oGCVVBqs1", "forum": "ZozCeC9rD4", "replyto": "ZozCeC9rD4", "signatures": ["ICLR.cc/2026/Conference/Submission17882/Reviewer_jDUk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17882/Reviewer_jDUk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992201418, "cdate": 1761992201418, "tmdate": 1762927707912, "mdate": 1762927707912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the numerical sensitivity and robustness of large language models (LLMs) in mathematical reasoning tasks.\nThe authors observe that models often fail when the problem statement is perturbed with irrelevant sentences containing numbers, even if those numbers are unrelated to the question. To systematically study this, they propose a new evaluation framework that injects irrelevant sentences—either numerical or non-numerical—at varying levels of intensity into GSM8K and AIME problems. Additionally, they introduce a “missing question instruction” setup to test whether models rely on memorization or true reasoning. Experiments reveal that most models, especially open-source ones, experience substantial accuracy drops (up to 50%) under numerical perturbations, while closed-source reasoning models (e.g., GPT-4o, o3, Gemini) are more robust but still affected. The results highlight that current LLMs lack the ability to filter out irrelevant numerical information and rely heavily on superficial pattern matching rather than genuine reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper systematically explores multiple perturbation types (numerical vs. non-numerical) and intensities, scaling from single-sentence insertions to twice the original problem length. The inclusion of both GSM8K and AIMEprovides a well-rounded evaluation across difficulty levels.\n\n2. Wide model coverage – The study evaluates a broad spectrum of models, including open-source LLMs (Qwen, DeepSeek, LLaMA, Gemma) and proprietary reasoning models, offering valuable cross-model insights into robustness differences between architectures and training paradigms."}, "weaknesses": {"value": "1. The core finding that models are brittle to irrelevant or noisy context—has been partially demonstrated in prior works such as GSM-Plus and MathCheck. The paper could benefit from a deeper discussion on what unique insight it contributes beyond confirming existing robustness issues, or how its new findings could be applied.\n\n2.Several promising analyses are missing: (1) The perturbation length is relatively small. Scaling the irrelevant text by 10× or 100× could better approximate real-world long-context reasoning and reveal how degradation behaves across positions (problem placed at the beginning, middle, or end of the context). (2) It would be valuable to test whether simple prompting strategies (e.g., instructing the model to ignore irrelevant numbers) can mitigate this vulnerability. (3) Conducting fine-tuning or robustness training on the proposed perturbed data could show whether these weaknesses can be systematically reduced, thus validating the framework’s utility beyond evaluation."}, "questions": {"value": "1. Have the authors tested longer-context scenarios (e.g., 10×–100× text length) to see whether accuracy degradation continues linearly or plateaus?\n\n2. Can explicit prompt instructions (such as “use only numbers directly related to the question”) reduce the negative effect of numerical noise?\n\n3. Would training or fine-tuning on the proposed perturbed datasets improve model robustness against irrelevant information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DnUDgLQWbX", "forum": "ZozCeC9rD4", "replyto": "ZozCeC9rD4", "signatures": ["ICLR.cc/2026/Conference/Submission17882/Reviewer_vKWg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17882/Reviewer_vKWg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995219095, "cdate": 1761995219095, "tmdate": 1762927707401, "mdate": 1762927707401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}