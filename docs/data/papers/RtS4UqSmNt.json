{"id": "RtS4UqSmNt", "number": 20469, "cdate": 1758306487239, "mdate": 1759896976009, "content": {"title": "Steering the Herd: A Framework for LLM-based Control of Social Learning", "abstract": "Algorithms increasingly serve as information mediators -- from social media feeds and targeted advertising to the increasing ubiquity of LLMs. This engenders a joint process where agents combine private, algorithmically-mediated signals with observational learning from peers to arrive at decisions. To study such settings, we introduce a model of controlled sequential social learning in which an information-mediating planner (e.g., an LLM) controls the information structure of agents while they also learn from the decisions of earlier agents. The planner may seek to improve social welfare (an altruistic planner) or to induce a specific action the planner prefers (a biased planner). Our framework presents a new optimization problem for social learning that combines dynamic programming with decentralized action choices and Bayesian belief updates.\n\nIn this setting, we prove the convexity of the value function and characterize the optimal policies of altruistic and biased planners, which attain desired tradeoffs between the costs they incur and the payoffs they earn from induced agent choices. The characterization reveals that the optimal planner operates in different modes depending on the range of belief values. The modes include investing the maximum allowed resource, not investing any resource, or the investment increasing or decreasing with increase in the belief. Notably, for some ranges of belief the biased planner even intentionally obfuscates the agents' signals. Even under stringent transparency constraints—information parity with individuals, no lying or cherry‑picking, and full observability—we show that information mediation can substantially shift social welfare in either direction. We complement our theory with simulations in which LLMs act as both planner and agents. Notably, the LLM-based planner in our simulations exhibits emergent strategic behavior in steering public opinion that broadly mirrors the trends predicted, though key deviations suggest the influence of non-Bayesian reasoning—consistent with the cognitive patterns of both human users and LLMs trained on human-like data. Together, we establish our framework as a tractable basis for studying the impact and regulation of LLM information mediators that corresponds to real behavior.", "tldr": "We introduce, analyze, and simulate (via LLMs) a model of controlled social learning to study how algorithms can influence social beliefs, decisions, and welfare via information design.", "keywords": ["Social learning", "LLMs", "optimal control", "information design", "dynamic programming"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44bd39f3b8704df2959f1a0b6e0ca906816b57f2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The model introduces a binary persuasion problem of a long-lived designer interacting with a sequence of short-lived agents that learn from the prior action sequence. This problem is related to social learning among LLM agents as well as human societies. They prove tractability of the model with Bayesian agents and investigate non-Bayesian LLM agents in simulations."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper thoroughly sets up its problem, and is very well-written. I found the integration of experiments alongside a (mostly) theory paper convincing, as the authors stated departures from their Bayesian assumption in the beginning of their experiments section."}, "weaknesses": {"value": "The paper considers and names several restrictions in particular on the information structure and short-livedness of agents, but does not say something on how these affect outcomes. While I understand that for the tractability of the theoretical model all of them are essential, the fact that the authors run experiments would allow for an investigation of what other dynamics are possible with variations on, e.g., the stringent public information assumptions."}, "questions": {"value": "- Does your setting admit a revelation principle?\n- (LLM-)Experimentally, which assumptions lead to significantly different dynamics compared to your current simulation exercise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yDMFrveKkn", "forum": "RtS4UqSmNt", "replyto": "RtS4UqSmNt", "signatures": ["ICLR.cc/2026/Conference/Submission20469/Reviewer_v6hQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20469/Reviewer_v6hQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843478892, "cdate": 1761843478892, "tmdate": 1762933912325, "mdate": 1762933912325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a formal framework for controlled social learning where a centralized planner (e.g., an LLM-based mediator) chooses the precision of private signals sent to sequential agents. The planner can be altruistic (maximizing social welfare) or biased (steering agents toward a preferred action), and faces costs for altering signal precision. The authors characterize optimal planner policies as functions of evolving public belief, prove structural properties (e.g., convexity of the altruistic value function), and show how informational externalities shape long-run outcomes. LLM-based simulations validate the theory, revealing that adaptive planners can substantially influence collective beliefs and welfare and that modern LLMs can exhibit emergent strategic behaviors—highlighting both opportunities and risks of algorithmic information mediation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The understanding and introduction of Bayesian persuasion (information design) are accurate.\n- The entire article is presented well, with a clear structure and easy to understand."}, "weaknesses": {"value": "**1**\n\nCurrently, this manuscript appears to lack sufficient literature review, which makes the position and contribution problematic.\n\nContribution 1 is not as claimed. The authors claim:\n> We introduce the first formal model that integrates a dynamic control problem for a centralized information planner with the mechanism of sequential social learning.\n\nThere is a series of articles that the authors have not considered: the combination of Bayesian persuasion (information design) and RL. In fact, in the past 5 years, there have been a large number of variants such as online persuasion and multi-receiver variants, etc. (while the authors' citations on information design are the latest only up to 2000).\n\nTalking about RL is not off-topic—considering that the authors position their article as a control problem (dynamic programming). Then this series of articles must at least be discussed.\n\nFarsighted agents is also a rapidly developing topic in the persuasion community. The social learning mentioned in this manuscript is indeed a novel point, but it is also a common problem in the multi-agent RL field. Therefore, the claim of contribution 1 is incorrect.\n\nIn addition, the citations on LLM on information design / Bayesian persuasion are not comprehensive enough (only focusing on the work of two or three labs), and the discussion is too limited.\n\n**2**\n\nThis manuscript is submitted to the Primary Area: alignment, fairness, safety, privacy, and societal considerations.\n\nHowever, this manuscript is about a stylized dynamic programming problem with a lot of assumptions. According to the authors' position, it seems that the entire related part should be in Section 6: EVALUATION VIA LLM-BASED SIMULATION.\n\nThe steps in section 6 by the authors are: “analyze the behavior of LLM agents to identify key deviations from Bayesian rationality”. The remaining logic seems to be: LLM is non-Bayesian, but LLM can still exhibit results that do not deviate much from the theoretical analysis results of Bayesian-rational agents.\n\nRegarding the player setting, there are 2 gaps:\n\n- non-Bayesian players vs human; non-Bayesian is a very rough concept, as long as it does not perfectly conform to Bayesian rationality, it counts. This is just one trait of humans. Not satisfying Bayesian rationality is not sufficient to conclude that it is close to humans. Humans have many other traits, and from an agent-based perspective, they can also be endowed with different personalities.\n- LLM vs human; this point is currently very controversial in the literature.\n\nTherefore, the authors' experiments may only illustrate that this algorithm has a certain robustness and can have good effects even when agents do not satisfy Bayesian-rationality. But this cannot indicate that this algorithm is more realistic, nor can it be said to be an effect brought by LLM.\n\nIn addition, the main text does not provide the LLM's role-play settings, and the experiments do not involve human participation or human data. Therefore, I believe the experiments and their claims are mismatched.\n\n**3**\n\nThe abstract must be changed to a single paragraph."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y8RxXSjrqJ", "forum": "RtS4UqSmNt", "replyto": "RtS4UqSmNt", "signatures": ["ICLR.cc/2026/Conference/Submission20469/Reviewer_gvhy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20469/Reviewer_gvhy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916014862, "cdate": 1761916014862, "tmdate": 1762933911515, "mdate": 1762933911515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how a social planner can intervene on signal strength in the information cascade model to induce both altruistic and biased outcomes. The authors derive optimal policies for the planner in both cases, in both the setting where the social learning aspect is ignored and when the social learning aspect is taken into account. They then study these dynamics with LLMs posing as the agents, the social planner, and a belief oracle."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The main contributions of this work are theoretical results for intervening on signal strength and an interesting experimental setup to evaluate the theoretical results. I am not aware of this particular style of intervention, and I am excited to see clear and insightful theoretical work in this vein. The paper is also clearly written and laid-out."}, "weaknesses": {"value": "Some small recommendations:\n1. include more of the experimental setup in the main body. It would just help to clarify how exactly you set this up with prompting etc.\n2. the theorems may benefit from diagrams to show what the policies are in the different regimes. \n3. worth having one extra line about computation about $\\tilde{b}_i$ in the main body as otherwise it is quite mysterious to the reader since it is a crucial point."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OfqgSkFfEh", "forum": "RtS4UqSmNt", "replyto": "RtS4UqSmNt", "signatures": ["ICLR.cc/2026/Conference/Submission20469/Reviewer_n8ZY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20469/Reviewer_n8ZY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762239008878, "cdate": 1762239008878, "tmdate": 1762933910799, "mdate": 1762933910799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits sequential social learning (Bikhchandani et al.) and adds a centralized planner who, at each step, chooses the precision of the next agent’s private signal. Agents best respond given the public belief and the stated precision. The planner’s problem (altruistic vs. biased objective) is cast as an infinite‑horizon discounted MDP in the public belief state. A key theoretical contribution is proving that the altruistic optimal value function $V^*_A$ is convex in belief, which underpins a threshold‑type policy structure; the paper also reports LLM‑based simulations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The convexity proof of $V^*_A$ is neat.  It gives a clear policy structure and may be useful for other related problems, e.g., dynamic information acquisition, and sequential contract design."}, "weaknesses": {"value": "- The model is a little too restrictive.  It is not clear if their result can go beyond symmetric binary signal setting.\n- The LLM experiment require more cares.  Figure 1 is misleading, as the value of prior and posterior are self-reported by LLMs.  Similarly, they should validate the strength of the signal from the oracle agent, e.g., asking another LLM to guess q_i.  I do not feel comfortable comparing those values with Bayesian models.\n\n\n### Miner issue\n- The theorem statement can be clearer.  For instance, though Theorem 1 only works for $\\delta = 1$, Theorem 2 should hold for all discount factors $\\delta<1$. \n- Equation (28) should depend on prefix of the trajectory but $P^*_\\lambda$ seems to the probability of whole trajectory.\n- You may try to simplify the proof by using dynamic programming (Bellman operator) or coupling."}, "questions": {"value": "Instead of a symmetric binary signal and 01 loss, how is the result generalized to a general signal and a loss function?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EUKucCkM9z", "forum": "RtS4UqSmNt", "replyto": "RtS4UqSmNt", "signatures": ["ICLR.cc/2026/Conference/Submission20469/Reviewer_LWVF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20469/Reviewer_LWVF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240146150, "cdate": 1762240146150, "tmdate": 1762933910187, "mdate": 1762933910187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}