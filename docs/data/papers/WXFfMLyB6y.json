{"id": "WXFfMLyB6y", "number": 17709, "cdate": 1758279592420, "mdate": 1759897159167, "content": {"title": "Generalizable Coarse-to-Fine Robot Manipulation via Language-Aligned 3D Keypoints", "abstract": "Hierarchical coarse-to-fine policy, where a coarse branch predicts a region of interest to guide a fine-grained action predictor, has demonstrated significant potential in robotic 3D manipulation tasks by especially enhancing sample efficiency and enabling more precise manipulation.\nHowever, even augmented with pre-trained models, these hierarchical policies still suffer from generalization issues.\nTo enhance generalization to novel instructions and environment variations, we propose Coarse-to-fine Language-Aligned manipulation Policy (CLAP), a framework that integrates three key components: 1) task decomposition, 2) VLM fine-tuning for 3D keypoint prediction, and 3) 3D-aware representation.\nThrough comprehensive experiments in simulation and on a real robot, we demonstrate its superior generalization capability.\nSpecifically, on GemBench, a benchmark designed for evaluating generalization, our approach achieves a 12\\% higher average success rate than the SOTA method while using only 1/5 of the training trajectories.\nIn real-world experiments, our policy, trained on only 10 demonstrations, successfully generalizes to novel instructions and environments.", "tldr": "", "keywords": ["3D manipulation", "Imitation Learning", "Coarse-to-fine Policy"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8934dee6f271fe9dfc7d6d092b81edb1e50260d5.pdf", "supplementary_material": "/attachment/dc5ded7ef0bb457a02d87308c655b8c1d0c38638.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes CLAP, a hierarchical coarse-to-fine policy for robotic 3D manipulation that integrates a pre-trained VLM as a coarse task planner and a fine-grained action predictor. The method introduces task decomposition into step-wise language instructions and employs a sequential reasoning process for 3D keypoint prediction. Evaluated on the GemBench benchmark and real-world tasks, CLAP demonstrates superior generalization to novel instructions and object variations while using significantly fewer training trajectories than prior state-of-the-art methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The approach effectively addresses compositional generalization by decomposing tasks into reusable step-wise instructions, enabling skill recombination across tasks.\n2. The integration of pre-trained models with a structured reasoning pipeline enhances sample efficiency and spatial understanding, as validated through rigorous ablation studies.\n3. The method achieves notable performance gains in both simulation and real-world experiments, particularly on long-horizon and articulated object tasks."}, "weaknesses": {"value": "1. The computational overhead of the two-stage architecture and sequential VLM reasoning remains unquantified, raising concerns about real-time applicability.\n2. The reliance on a chain of reasoning steps introduces fragility; errors in object localization or instruction generation may propagate and compromise task execution.\n3. The method assumes tasks can be linearly decomposed into language instructions, which may not hold for tasks requiring non-sequential or implicit motor skills."}, "questions": {"value": "1. What is the inference latency of CLAP under real-world operational conditions, and how does it compare to baseline methods like RVT2?\n2. In the sequential reasoning process where the coarse task planner first localizes task-related objects before predicting step instructions and keypoints, how does the method perform when object localization fails? \n3. Can CLAP handle tasks that lack clear step-wise descriptions or require fine-grained motions not easily captured by language?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wzScOHUJSX", "forum": "WXFfMLyB6y", "replyto": "WXFfMLyB6y", "signatures": ["ICLR.cc/2026/Conference/Submission17709/Reviewer_zNT5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17709/Reviewer_zNT5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839257926, "cdate": 1761839257926, "tmdate": 1762927547126, "mdate": 1762927547126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CLAP, a hierarchical 3D manipulation policy to address the poor generalization of existing coarse-to-fine methods, especially for novel environments and skill compositions. CLAP features two innovations: 1. Coarse Task Planner: A fine-tuned VLM (Qwen 2.5 VL-3B) acts as the coarse branch. It decomposes tasks into step-wise language instructions $l_k$ and predicts a corresponding 3D keypoint $p{t_k}$ for each step, guided by a sequential reasoning process and an auxiliary object detection task. 2. Fine-grained Action Predictor: A separate module receives the specific step instruction $l_k$ and a \"3D-aware representation\" (built from SigLIP and DINOv2) centered at $p{t_k}$ to predict the final action. Experiments on GemBench show CLAP outperforms SOTA by 12% using only 1/5 of the data (20 demonstrations). Real-world tests also confirm strong generalization from only 10 demonstrations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The VLM is innovatively fine-tuned to be the coarse branch, directly outputting 3D keypoints. The \"sequential reasoning\" fine-tuning pipeline is a clever method for aligning language, 3D space, and planning.\n2. The method achieves strong results on GemBench (62.0% avg. success) with only 1/5 of the data (20 demos) and generalizes in the real world from only 10 demos, which is highly practical."}, "weaknesses": {"value": "1. **Extreme architectural complexity:** This paper relies on RVT2's canonical views, uses one VLM (Qwen) as the coarse planner, and uses two other powerful pre-trained models (SigLIP and DINOv2) for the fine-grained branch. This complexity makes reproduction difficult, and it's unclear if the performance gain comes from the novel architecture or simply from stacking 3+ powerful (and separate) pre-trained encoders.\n2. **Dependency on and lack of transparency about auxiliary data:** The method relies on fine-tuning on an \"auxiliary 3D object detection dataset\" and a \"language plan dataset\". The paper does not detail the source, scale, or creation cost of these datasets. If these decomposed steps require significant manual annotation, it would be a major, hidden data cost that undermines the claimed sample efficiency of 10-20 demonstrations.\n3. **Dependency on keyframes:** As the authors admit in the conclusion, the entire framework is tied to keyframe-based imitation learning. This makes it unsuitable for unstructured tasks where keyframes are difficult to define (e.g., wiping, stirring), limiting its generality.\n4. **Cascading errors:** This is a hierarchical system. If the coarse task planner (VLM) makes an error in the first stage—e.g., (a) generating the wrong step instruction $l_k$, or (b) predicting an inaccurate 3D keypoint $p_k$—the system will likely fail catastrophically. The paper does not discuss or evaluate the robustness to such cascading errors.\n5. **Insufficient comparation with highly related work:** The \"Related Work\" section fails to discuss (or differentiate from) several key recent works exploring an extremely similar problem space. Specifically, this paper needs a clear comparison with GravMAD (arxiv: 2409.20154) and DeCo (arxiv: 2505.00527). All three papers (including CLAP) are trying to solve the same core challenge: how to use foundation models and task decomposition to enable imitation learning models to zero-shot generalize to novel, long-horizon, compositional 3D manipulation tasks. This lack of comparison makes it difficult for reviewers to assess CLAP's unique contribution in this increasingly crowded and important research direction."}, "questions": {"value": "1. Please clarify the source and scale of the auxiliary data (the \"language plans\" and \"object position dataset\") required for VLM fine-tuning. Is this data auto-generated or does it require manual annotation? If manual, how does the effort (e.g., in person-hours) to create these datasets compare to collecting the 10-20 demonstrations?\n\n2. Can you elaborate on the justification for this complex architecture? Why is it necessary to use separate SigLIP and DINOv2 encoders for the fine-grained branch, rather than (for example) re-using the VLM's (Qwen's) own visual encoder? Were simpler, encoder-sharing setups tested?\n\n3. This framework is a cascade. How does the system perform if the coarse task planner makes an error in the first stage (e.g., predicting a keypoint $p_k$ with a 10cm offset, or generating the wrong step instruction $l_k$)? Does the fine-grained action predictor have any robustness to correct for moderate errors from the coarse planner, or does the system fail catastrophically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6kiDuv4cZb", "forum": "WXFfMLyB6y", "replyto": "WXFfMLyB6y", "signatures": ["ICLR.cc/2026/Conference/Submission17709/Reviewer_uaCQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17709/Reviewer_uaCQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916722840, "cdate": 1761916722840, "tmdate": 1762927546613, "mdate": 1762927546613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CLAP, a hierarchical coarse to fine policy for 3D robotic manipulation that aims to generalize to novel instructions and environment changes. The coarse task planner is a fine tuned VLM that first decomposes a task into step wise language instructions, then reasons about object locations and predicts a language aligned 3D keypoint used to crop observations. The fine grained branch fuses step instruction with a 3D aware visual representation from multi view RGB D and depth encoders to predict actions. In simulation on GemBench, CLAP reports an average success rate that is 12 percent higher than the previous best method while using only one fifth of the training trajectories, and in real robot tests it generalizes to new tasks and perturbations with only ten demonstrations per task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- CLAP augments the coarse branch into a task planner that performs step wise decomposition, object localization, and language aligned 3D keypoint prediction, then conditions the fine branch on the current step rather than the entire task. The two round inference protocol and the auxiliary 3D object detection task are thoughtful adaptations that better align VLMs to manipulation.\n- Strong empirical evidence spans GemBench with detailed multi level results and an ablation table that isolates the impact of language plans, last step memory, object reasoning, and pre trained encoders. Real robot experiments further support the claims."}, "weaknesses": {"value": "- The idea of using coarse-to-fine planning is not new, such as ReKep [1] , MakeADonut [2] and VoxPoser [3]. The technical novelty is limited and the contribution is unclear either.\n- The paper lacks sufficient details about the method, e.g., how are keypoints predicted; what are their distributions; how to convert from keypoints to robot actions. The finegrained action predictor in Figure 2 is never described in detail, making it hard to understand the proposed method. Therefore, the paper seems to be unpolished and not ready to be accepted.\n\n[1] Huang, Wenlong, et al. \"Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation.\" arXiv preprint arXiv:2409.01652 (2024).\n\n[2] You, Yang, et al. \"Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable Manipulation with Tools.\" IEEE Robotics and Automation Letters (2025).\n\n[3] Huang, Wenlong, et al. \"Voxposer: Composable 3d value maps for robotic manipulation with language models.\" arXiv preprint arXiv:2307.05973 (2023)."}, "questions": {"value": "- What fraction of your training set comes from language plans versus robot trajectories, and how were plans obtained and validated. Please report sensitivity to the amount and quality of plan data.\n- How robust is the planner to plan errors or mis localized objects. Do you have a recovery strategy or re planning mechanism during execution.\n- Can you provide equal trajectory comparisons to RVT2 or BridgeVLA and a study controlling for the number of demonstrations per variation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Yj4mMYviEf", "forum": "WXFfMLyB6y", "replyto": "WXFfMLyB6y", "signatures": ["ICLR.cc/2026/Conference/Submission17709/Reviewer_x1Wc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17709/Reviewer_x1Wc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964546680, "cdate": 1761964546680, "tmdate": 1762927546131, "mdate": 1762927546131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the generalization issues in the hierarchical coarse-to-fine policy, the authors propose to extend a prior work, RVT2, with an VLM for task decomposition and 3D keypoint prediction. Specifically, to enhance such capabilities on the VLM, several ideas for fine-tuning the VLM have been introduced, namely 1) task decomposition, 2) VLM fine-tuning for 3D keypoint prediction, and 3) 3D-aware representation. Experimentally, the superior performance and training sample efficiency have been validated in both simulation and real-world along with comparison of a wide range of strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The presentation is easy-to-follow with elaborative description of the approach.\n\n- An effective extension on top of RVT2: a sensible idea to add an VLM with enhanced generalizability of task compostionality and spatial awareness; To achieve this, the authors also propose of a set of pratical techniques, i.e., adding spatial reasoning, including previous sub-plans as inputs, 3D key-points prediction, etc.. \n\n- Superior training sample efficiency has been shown. Its performance surprisingly surpasses the SOTA methods while using only 1/5 of the training trajectories.\n\n- Comprehensive experimental study in simulation and real-world, demonstrating significant performance gain against a wide range of baselines."}, "weaknesses": {"value": "- Presentation seems too wordy. The writing style of the current draft reads like a technical report. I would suggest the authors to shorten the method part and make it more concise so that the readability can be further improved.\n\n- Limited novelty. Though the performance is encouraging, the technical novelty remains limited in terms of the desgin choice and tricks applied for fine-tuning the VLM. More understanding and analysis can be conducted in order to improve the technical quality of this work, e.g., what exactly is the data efficiency coming from? Is it from the greater task compositionality or the enhanced spatial awareness?\n\n- Lack of a explicit discussion about related works on system1&2 policies. The topic of this submission seems quite relevant to the idea of system1&2 policies, a discussion about the difference and connection would be highly recommended.\n\n- Following the issue above, the discussion of run-time efficiency is also missing. How is it compred to the original RVT2? To which extend does adding the VLM module sacrifice the run-time efficiency? \n\n- The fine-grained action predictor in Fig.2 is simplified, mismatched to the description in the main texts."}, "questions": {"value": "- Run-time efficiency: have the authors tried to use the same amount of training data during training as the other baselines for further improvement?\n\n- Redundant spatial information as input for the fine-grained action predictor: the inputs include multi-view images, depth and the 3D position embedding from pixel-wise 3D coordinates. It seems that the spatial information is quite redundant from these inputs, have the authors ablate this desgin choice?\n\n- Generalization concern for spatial awareness of the VLM after being fine-tuned on a spefic dataset: it's a popular idea to fine-tune an VLM for better in-context generalization, e.g., task compositionality and spatial awareness. I am wondering whether the fine-tuning causes performance degradation when testing on OOD data outside the training distribution.\n\n- Another question regarding the robustness mentioned above is about the hallucination of the VLM after being fine-tuned for greater spatial awareness. In the entropy column of table 3 [1], it is shown that spatial reasoning also helps on predictive performance on failure detection task but not necessarily on uncertainty estimation. It would be interesting and inspiring to know if this also happens when using the VLM as the task planner.\n\n[1] Zheng, Z., Feng, Q., Li, H., Knoll, A., & Feng, J. (2024). Evaluating uncertainty-based failure detection for closed-loop llm planners. arXiv preprint arXiv:2406.00430."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "20uvCdM7l3", "forum": "WXFfMLyB6y", "replyto": "WXFfMLyB6y", "signatures": ["ICLR.cc/2026/Conference/Submission17709/Reviewer_EPq1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17709/Reviewer_EPq1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972143510, "cdate": 1761972143510, "tmdate": 1762927545809, "mdate": 1762927545809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}