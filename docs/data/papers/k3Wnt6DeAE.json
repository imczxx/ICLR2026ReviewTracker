{"id": "k3Wnt6DeAE", "number": 19562, "cdate": 1758297280744, "mdate": 1759897032407, "content": {"title": "Equivariant Neural Networks for General Linear Symmetries on Lie Algebras", "abstract": "Encoding symmetries is a powerful inductive bias for improving the generalization of deep neural networks. However, most existing equivariant models are limited to simple symmetries like rotations, failing to address the broader class of general linear transformations, $\\mathrm{GL}(n)$, that appear in many scientific domains. We introduce \\textbf{Reductive Lie Neurons (ReLNs)}, a novel neural network architecture exactly equivariant to these general linear symmetries. ReLNs are designed to operate directly on a wide range of structured inputs, including general $n$-by-$n$ matrices.\nReLNs introduce a novel adjoint-invariant bilinear layer to achieve stable equivariance for both Lie-algebraic features and matrix-valued inputs, \\textit{without requiring redesign for each subgroup}. This architecture overcomes the limitations of prior equivariant networks that only apply to compact groups or simple vector data. We validate ReLNs' versatility across a spectrum of tasks: they outperform existing methods on algebraic benchmarks with $\\mathfrak{sl}(3)$ and $\\mathfrak{sp}(4)$ symmetries and achieve competitive results on a Lorentz-equivariant particle physics task. In 3D drone state estimation with geometric uncertinaty, ReLNs jointly process velocities and covariances, yielding significant improvements in trajectory accuracy. ReLNs provide a practical and general framework for learning with broad linear group symmetries on Lie algebras and matrix-valued data.", "tldr": "We introduce Reductive Lie Neurons, a general equivariant architecture that extends rotations to the full general linear group. This enables equivariant learning on matrix-valued geometric data, expanding the scope of geometric deep learning.", "keywords": ["Equivariant Neural Networks", "General Linear Group", "Reductive Lie Group", "Geometric Uncertainty"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/202e12ca15b0112c113e5446e9be349d81779b8b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a novel equivariant architecture framework \"Reductive Lie Neuron\" (ReLN) which generalizes the \"Lie Neuron\" architecture from Lin et al. 2024 from semisimple Lie groups to reductive Lie groups.\nThis is achieved by considering a more general and expressive bilinear form instead of the Killing form, which can be degenerate on reductive Lie algebras."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed idea is elegant and interesting, and can be potentially impactful in some of the applications described by the authors.\nThe manuscript is quite well written, although it provides little background on the advanced mathematical tools leveraged, which might make it less accessible to the general audience (see comments below)."}, "weaknesses": {"value": "While the paper is well motivated and mentions some exciting applications, I found the experimental validation less interesting and weaker.\nIndeed, the authors consider 4 tasks, 2 simple synthetic datasets with *semisimple* Lie Algebras in Sec 5.1, an apparently saturated Jet-Tagging benchmark with the reductive gl(n) Lie group in Sec 5.2, and (if I understood correctly) a SO(3) equivariant task in section 5.3.\n\nWhile these experiments seems good proof of concepts, I don't think they provide good arguments for the adoption of ReLN: in the first 2 tasks (Sec 5.1) ReLN simply matches Lie Neuron since the Lie algebra is semisimple, in the last one (Sec 5.3) the task is only SO(3) symmetric and the authors only consider an arguably poor SO(3) equivariant baseline. The Jet-Tagging task in Sec 5.2 seems to be the only one where reductive Lie Algebra equivariance is really necessary but the saturated benchmark doesn't show any strong benefits of ReLN with respect to less equivariant baselines.\n\nIn the conclusions, the authors claim the proposed method achieves SOTA results on benchmarks and deliver large gains in challenging drone state estimation task, but I am a bit more skeptical about these claims.\nThe authors suggest a wider array of physical systems, including dynamics of articulated robots and large scale sensor fusion as future applications to explore. These seems very interesting and exciting applications indeed, so I wonder why not benchmarking the proposed method on these relevant problems rather than the toy experiments considered in the manuscript.\n\nSee also questions below."}, "questions": {"value": "I think it might be wroth spending a couple of words to explain what it is a reductive Lie algebra, the Killing form and what it means for it to be degenerate or, at least, the authors could point to some definition provided later.\nFor example, in Sec 4.1, different constructions based on these terms are described but no definition of reductive Lie algebra or Killing form is provided yet.\nI am very familiar with the equivariance literature but not so much with reductive Lie groups, so I am not sure how to interpret exactly the equations in Definition 4.1.\n\nSince the method builds on top of Lie Neurons (Line et al 2024a), it might be worth giving a quick overview of it first.\n\n\nEq. 4: is $d_c$ somehow dependent on the input too? Otherwise, if that is a fixed learnable vector, that operation doesn't seem equivariant, right?\nSimilar question for the Max-Killing pooling: is $D_{n,c}$ depending on the input?\n\n\nLine 371: parameter matching is not really the most fair comparison. Why not doing a simple hyperparameter search and pick the best configuration for each model? E.g., from Table 4, it seems that the original LorentzNet had has 200K params, so why matching LorentzNet to the 84K params of ReLN rather than scaling ReLN to the 200K params of LorentzNet? \n\nI am not very familiar with the Jet-Tagging dataset, but the results in Table 4 suggest this benchmark is quite saturated as most models achieve the exact same accuracy and AUC. Even the two instances of LorentzNet have the exact same performance despite the different number of params. Can the authors comment on this? Is there a better dataset the authors could validate the model on?\n\n\nSec 5.3: I might have misunderstood something, but isn't this task just E(3) equivariant? Indeed, in Table 5, only SO(3) test-time augmentation is performed. Then, why should we care about GL(n) here? I understand that the covariance matrices live in the SPD(3) manifold, but the group acting on them here is just SO(3) (or O(3)), so one could build a very competitive baseline using most of the related literature on E(3)-equivariant methods. If this is the case, then, why do you only consider a tweaked Vector-Neuron architecture as a baseline (which moreover seems to be one of the few related works which can't handle adjoint equivariance). Am I missing something?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mr5pUrObx6", "forum": "k3Wnt6DeAE", "replyto": "k3Wnt6DeAE", "signatures": ["ICLR.cc/2026/Conference/Submission19562/Reviewer_psZr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19562/Reviewer_psZr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925775043, "cdate": 1761925775043, "tmdate": 1762931445415, "mdate": 1762931445415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new class of equivariant neural networks with inputs and outputs residing in the Lie algebra of GL(n) that are equivariant to the adjoint action of the group. Such adjoint equivariance is useful for problems involving matrix-valued data such as inertia and covariance, and can be extended to the usual left-action equivariance in some cases. While the basic formulation of adjoint-equivariant linear layers and nonlinearities has been established in a prior work [1], the method therein requires non-degeneracy of the Killing form on the Lie algebra and the contribution of this work is extending applicability to GL(n) and subgroups with reductive Lie algebra that may have degenerate Killing form. The key idea for this is leveraging the structure of reductive Lie algebras to augment the Killing form with a choice of Ad-invariant inner product on the center to resolve the nondegeneracy. With this, the authors generalize the designs of linear layer, nonlinearities, and pooling layers in [1]. The authors evaluate the proposed architecture on SO(3) adjoint equivariant Platonic solid classification task, a Sp(4) adjoint invariant synthetic regression task, SO(1, 3) left equivariant jet tagging task based on a new identification of embedding map that equates left action on data space and adjoint action on embedding space, and SO(3) adjoint equivariant drone state estimation with uncertainty represented by covariance matrix, and report improved performance mainly over non-equivariant baselines, vector neurons and Lie neurons.\n\n[1] Lin et al., Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras, ICML 2024."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper's use of augmented bilinear form to generalize Lie neurons to reductive Lie algebras is original and technically sound as far as I can confirm.\n\nS2. The writing and presentation is overall clear and easy to follow."}, "weaknesses": {"value": "W1. Among the four experiments presented, as far as I can understand there are no experiments that involves reductive Lie algebras with degenerate Killing form, which are the key targets of the construction given in the paper (please correct me if I am wrong). From this, I was not able to draw the conclusion that the proposed method actually solves the problem setup given in Section 4.1.\n\nW2. While the work extends the applicability of [1] to reductive Lie algebras, the extension might not be as broad as one may expect. One reason is that the construction cannot be applied to non-unimodular Lie groups [2, Corollary 8.31] including affine transformations (Line 39). Experiments and/or discussions on which specific groups and problems are made accessible based on this work, and why they are useful, seem necessary.\n\nW3. I am not sure why being able to process matrix-valued inputs (e.g., in geometric uncertainty processing in Section 5.3) is a unique advantage or contribution of this work, given that standard left-action equivariant architectures offer extensions to matrix- or tensor-inputs based on actions on product spaces [3, 4, 5]. The authors discuss these approaches in Lines 109-111 but I am not convinced that these types of architectural designs are substantially more limited than the proposed method. One specific reason is that, while extending these architectures to matrix-valued inputs is direct, extending the proposed adjoint equivariant architecture to left-action problems is not as straightforward and requires problem-specific handling (e.g., Section 5.2).\n\n[2] Knapp, Lie Groups: Beyond an Introduction, 2002.\n\n[3] Thomas et al., Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds, arXiv 2018.\n\n[4] Fuchs et al., SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks, NeurIPS 2020.\n\n[5] Finzi et al., A Practical Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary Matrix Groups, arXiv 2021."}, "questions": {"value": "I have no particular questions but would like to hear the authors' response on the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JsUuCVITIT", "forum": "k3Wnt6DeAE", "replyto": "k3Wnt6DeAE", "signatures": ["ICLR.cc/2026/Conference/Submission19562/Reviewer_X174"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19562/Reviewer_X174"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972263403, "cdate": 1761972263403, "tmdate": 1762931444817, "mdate": 1762931444817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a modification to Lie Neurons (Lin et al 2024a) to make them work for reductive Lie groups. Lie Neurons introduced a clever idea for constructing equivariant nonlinearities and pooling by using the inner product define on the Lie algebra, called the Killing form. Lie neurons worked well for \"semisimple\" Lie groups, which don't have nontrivial center (subgroup commuting with the group), but did not work for reductive groups )having nontrivial center) because the Killing form becomes degenerate (zero norm for center). The general linear group $GL(n) \\\\simeq SL(n) \\\\rtimes \\\\mathbb{R}\\\\backslash \\\\{0\\\\} $   is reductive and its Lie algebra is $\\\\mathfrak{gl}(n) = \\\\mathbb{R} I \\oplus \\\\mathfrak{sl}(n)$. They propose using a modified Killing form $\\tilde{B}$ which adds a nondegenerate inner product in the center of the group to the usual Killing form.\nThey use $\\tilde{B}$ instead of $B$ to define nonlineariries and other layer components similar to Lie Neurons. They vallidate their layer on a few problems and show strong improvement in one, the drone navigation problem."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The theoretical foundation seems solid. It resolved the Killing form degeneracy issue in a very simple way. \n2. ReLN usifies previous approaches as it is a slight generalization of Lie Neurons and reduces to it on semisimple Lie algebras. \n3. The drone navigation results show impressive improvement over the baseline."}, "weaknesses": {"value": "1. My main issue is contribution. Beyond defining $\\tilde{B}$, I don't seem to find any distinction between this work and Lie Neurons (Lin 2024a). Specifically, the Killing form on $\\\\mathfrak{gl}(n)$  is $B(X,Y) = 2n\\\\cdot \\\\mathrm{tr} (XY) - 2\\\\mathrm{tr}(X) \\\\mathrm{Y}$. They define $\\\\tilde{B}(X,Y) = \\\\mathrm{tr}(X) \\\\mathrm{Y} + B(X,Y)$. That's basically their main theoretical contribution. All the rest, including ReLN-ReLU nonlinearity and ReLN-Bracket seem to be identical to Lie Neurons, just using $\\tilde{B}$ instead of $B$. \n2. For the drone experiment, the VN baseline seems a bit strange. As vector neurons don't work for matrices and for the velocity+covariance case they do eigendecomposition. But compared to velocity alone, the results with covariance are an order of magnitude worse. So, the eigendecompositon proposal may be flawed. As I understand it, no one else has suggested that VN can be used this way, right? So maybe a naive baseline like SO(3) augmented ResNet would have been more competitive. In contrast, the velocity only results onf VN are not bad at all and pretty close to ReLN."}, "questions": {"value": "1. can you clarify the distinction between this work and Lin 2024a beyond $\\tilde{B}$? some of the contribution points seem too specific and don't give a big picture, beyond a slightly different application of Lie Neurons. \n2. In the top-tagging, the results are basically the same as some baselines. Is there an advantage in using ReLN? Could Lie Neuron be applied there too? \n3. did you consider stronger baselines for the drones? Could maybe combining using VN for the velocity with MLP for covariance help?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XGZXCfShO6", "forum": "k3Wnt6DeAE", "replyto": "k3Wnt6DeAE", "signatures": ["ICLR.cc/2026/Conference/Submission19562/Reviewer_V7Ta"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19562/Reviewer_V7Ta"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976753989, "cdate": 1761976753989, "tmdate": 1762931443931, "mdate": 1762931443931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper ‘Equivariant Neural Networks for General Linear Symmetries on Lie Algebras’ suggests an approach how to adopt Lie Neurons architecture (Lin et al., 2024) to the case of the tasks when the Killing form is degenerate (this happens if the corresponding Lie algebra is reductive). This problem of Lie Neurons is mentioned in the discussion section of their paper. The authors of the current paper call their modification of Lie Neurons architecture as Reductive Lie Neurons (ReLNs). ReLNs are equivariant with respect to the adjoint action of the general linear Lie group and, consequently, all its subgroups. ReLNs propose a learnable, non-degenerate, and Ad-invariant bilinear form that is effective for the reductive (but non-semisimple) algebra gl(n). The authors demonstrate the framework's versatility on algebraic benchmarks (semisimple case), a Lorentz-equivariant particle physics task (via an embedding into a reductive algebra), and a 3D drone state estimation problem (via an embedding into another reductive algebra)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1). The paper is generally well-written. I liked the introduction and Figures 1 and 2. They provide nice high-level overviews of the application landscape and the method's position in the field.\n\\\n2). Providing a practical architecture for exact GL(n) equivariance is an original contribution (although direct applications of full GL(n)-equivariance are questionable)."}, "weaknesses": {"value": "1). Theoretical and conceptual contribution of the work is moderate:\n\\\na). When the Lie algebra is semisimple, the proposed form $\\tilde{B}$ almost coincides with the Killing form $B$ considered in Lie Neurons (see Section C.1). But the semisimple Lie algebras are mostly interesting for applications, and even the authors' experiments in Sections 5.1.1 and 5.1.2 consider the tasks with semisimple Lie algebras, and in the experiment in Section 5.2, the authors artificially put the O(1,3)-equivariant task in the reductive gl(5). \n\\\nb). The proposed layers (Section 4.2) mostly duplicate the layers proposed in Lie Neurons, just changing $B$ on $\\tilde{B}$.\n\\\nc). The proofs of the main theorems (Proposition 4.1 and equivariance of the proposed mappings; the proofs are in Propositions B.1-B.3 and Appendix D)  seem to appear either almost trivial or almost line by line similar to those provided in the Lie Neurons paper.\n\n2). No public code is provided, which hinders verification of the reported results and adoption of the method.\n\n3). The current experiments 5.1.1 and 5.1.2 duplicate the ones provided in Lie Neurons (for semisimple Lie algebras). However, I believe the experiments are insufficient for the provided statements. They do not fully substantiate the need for the generalized gl(n) framework. Key benchmarks (Sections 5.1.1, 5.1.2) are performed on semisimple algebras where ReLNs and Lie Neurons are nearly equivalent. In Section 5.2, the Lorentz-equivariant task is (probably auxiliary) embedded into gl(5), but a comparison against Lie Neurons applied to the native semisimple algebra is missing, making it difficult to judge the necessity of the reductive extension.\n\n4). The authors mention only SO(n) equivariance in introduction and related work, however O(n), O(p,q), SO(p,q) are also often applied (the authors even use them in the top tagging experiment).\n\n5). The related work section omits a significant body of literature on equivariant networks based on Clifford (Geometric) algebras (see, for example, Clifford Group Equivariant Neural networks (CGENN, https://openreview.net/forum?id=n84bzMrGUD&noteId=sQG6abJbs8), Geometric Algebra Transformer (https://openreview.net/forum?id=M7r2CO4tJC), Clifford Group Equivariant Simplicial Message Passing Networks (https://openreview.net/forum?id=Zz594UBNOH&noteId=6jug7zZnrW), Generalized Lipschitz Groups Equivariant Neural Networks (https://openreview.net/forum?id=H0ySAzwu8k), and others). The paper CGENN can be added in Top Tagging experiment 5.2, they also consider this task.\n\n6). Minor points:\n\\\na). Consistency of notation: the authors use both gl(n) and gl(n,R) notion for the same Lie algebra (e.g. see lines 159 and 212).\n\\\nb). In the references, all the titles of the papers are lowercased, so sometimes ‘Lie’ appears as ‘lie’, etc.\n\\\nc). Formula (17) duplicates the proof of Proposition A.1."}, "questions": {"value": "1). What are the results for the original Lie Neurons model on the Top Tagging (5.2) and Drone State Estimation (5.3) tasks? A direct comparison is crucial for evaluating the improvement offered by the ReLN generalization of Lie Neurons.\n\n2). I did not fully understand the following point. The paper proves equivariance under the adjoint action ($X\\mapsto gXg^{-1}$) of the general linear group GL(n) and its subgroups. For orthogonal groups, the standard notion of equivariance is often defined by the left multiplication for vectors ($v\\mapsto gv$), i.e. we can either firstly multiply any vector by an orthogonal matrix and then apply the network, or firstly apply the network and then multiply the result by the same orthogonal matrix, and we need to get the same result. Could you please clarify the relationship between these two actions in your framework, especially for groups like O(n) and SO(n)? Does the group adjoint action become a left multiplication on the R^3 for the orthogonal groups? Please provide the details.\n\n3). The Killing form for gl(n) has the form $B(x,y)=2n \\cdot trace(XY)-2\\cdot trace(X)\\cdot trace(Y)$ (see e.g. Wiki page on the Killing form). The authors introduce the modified Killing form $\\tilde{B}=2n \\cdot trace(XY)-trace(X)\\cdot trace(Y)$ (without 2 in the second summand). Could you please specify how these 2 formulas are related? What was the motivation for the specific scaling in your definition? Any details would be possibly helpful for the readers.\n\n4). In Table 3, what is the precise meaning of \"Test Aug.\" and the reported values? Is this MSE?\n\n5). For the reader's convenience, could you add a brief table in the appendix summarizing the definitions of the main Lie groups and algebras considered in the work besides general linear (special linear, symplectic, etc.)?\n\n6). Can the ReLN architecture be generalized to complex Lie groups and algebras, which are also prevalent in applications like quantum mechanics?\n\n7). Please address comments, suggestions, and questions in my Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qP7GEttIxW", "forum": "k3Wnt6DeAE", "replyto": "k3Wnt6DeAE", "signatures": ["ICLR.cc/2026/Conference/Submission19562/Reviewer_ChwN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19562/Reviewer_ChwN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19562/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762083567531, "cdate": 1762083567531, "tmdate": 1762931443361, "mdate": 1762931443361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}