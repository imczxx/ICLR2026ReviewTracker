{"id": "6K08FPo2cf", "number": 293, "cdate": 1756734155772, "mdate": 1759898268780, "content": {"title": "Matting Anything 2:  Towards Video Matting for Anything", "abstract": "Video matting is a crucial task for many applications, but existing methods face significant limitations. They are often domain-specific, focusing primarily on human portraits, and rely on the mask of first frame that is challenging to acquire for transparent or intricate objects like fire or smoke. To address these challenges, we introduce Matting Anything 2 (MAM2), a versatile and robust video matting model that handles diverse objects using flexible user prompts such as points, boxes, or masks. We first propose Promptable Dual-mode Decoder (PDD), a effective structure that simultaneously predicts a segmentation mask and a corresponding high-quality trimap, leveraging trimap-based guidance to improve generalization. To tackle prediction instability for transparent objects across video frames, we further propose a Memory-Separable Siamese (MSS) mechanism. MSS employs a recurrent approach that isolates trimap prediction from potentially interfering mask memory, significantly enhancing temporal consistency. To validate our method's performance on diverse objects, we introduce the Natural Video Matting dataset, a new benchmark with substantially greater diversity. Extensive experiments show that MAM2 possesses exceptional matting accuracy and generalization capabilities. We believe MAM2 demonstrates a significant leap forward in creating a video matting method for anything.", "tldr": "", "keywords": ["Video Matting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ffb793da2721902e092733ee79b697797c1c5cd3.pdf", "supplementary_material": "/attachment/e730155445628f4f98577db17e816ae46162e870.zip"}, "replies": [{"content": {"summary": {"value": "This work introduces Matting Anything 2 known as MAM2, a prompt driven video matting framework that moves beyond human portrait centric settings by supporting points, boxes, and masks, by stabilizing trimap prediction for transparent and complex objects, and by reducing reliance on a first frame mask. The method adds a Promptable Dual-mode Decoder that predicts a segmentation mask and a trimap in one pass, and a Memory Separable Siamese mechanism that stabilizes trimap decoding for transparent or complex objects across time. The authors also introduce a Natural Video Matting benchmark with diverse non human portrait categories. Results report strong accuracy on both natural objects and human videos, and competitive image matting as well."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem motivation regarding portrait bias and reliance on a first frame mask for selection. The paper explains why transparent targets such as smoke or fire make mask prompting difficult and why lighter prompts like box or points are preferable for such cases.\n\n2. Architectural idea with practical value. PDD extends the SAM2 mask decoder to produce both a mask and a trimap in one pass, and it leverages the strong mask quality of SAM2 as a spatial prior. In practice this yields cleaner boundaries and a more stable unknown band, which improves the final alpha matte.\n\n3. Transparent object failure analysis and remedy. The paper identifies temporal collapse where unknown regions drift to foreground for later frames. MSS addresses this by running a second PDD pass to decode the trimap from memory free features using the first pass mask as a pseudo prompt. Parameters are shared between the passes.\n\n4. New test dataset (Natural Video Matting) for generalization."}, "weaknesses": {"value": "1. The mathematical specification of the pipeline is insufficient.\nThe paper lacks a complete equation level description of the forward computations, especially for the interaction between PDD and MSS.\n\n2. Figures do not explain the full system behavior. Figure 2 leaves ambiguity about how segmentation data and matting data are used across training iterations and stages. It is unclear how features from the MSS pathway connect to the PDD pathway, which parts are trainable at each stage, and whether user prompts for MSS and PDD are shared or distinct. The figure should be redrawn to show the end to end pipeline with data sources, feature flow, prompt flow, and trainable versus frozen modules.\n\n3. The boundary between the SAM2 decoder and the authors’ contributions is unclear.\nThe text and figures do not make it evident what is inherited from the original SAM2 mask decoder and what is newly introduced in this work. The paper should provide a module level figure that clearly labels inherited blocks and newly added blocks, together with explicit annotation of the mask output token, the trimap output token, and all entry points for user prompts.\n\n4. Why the method works is not analyzed.\nThe paper states that the proposed method fixes the failure cases but gives little analysis of the mechanism. In particular, the paper should explain why the Mask Augment Feature and the preserved Feature without Memory lead to stable trimap prediction, with diagnostic evidence or ablations.\n\n5. Experimental fairness and data accounting are not sufficiently documented.\nAccording to Appendix 1, models appear to be trained on different data sources. The manuscript should quantify the training data for each method in comparable units such as number of images, clips, and frames, and include experiments where all methods are trained on the same data to isolate the effect of the proposed design. Table 8 also indicates a parameter gap of nearly nine times between MAM2 and the strongest baseline. This capacity difference makes it difficult to attribute the gains in MAD and GRAD to the proposed architecture rather than to model size."}, "questions": {"value": "1. Figure 5 appears to report results only on the Natural Video Matting dataset. Could you add a companion figure that compares MAM2 with baselines on additional public video matting test sets?\n\n2. Table 2 shows MAM2 performance with box and point prompts. In Table 4 these entries appear in faint text, which suggests they may be out of scope or not directly comparable. Could you create a separate table summarizing results for models that use both prompts box and point under the same data and evaluation protocol, reporting MAD and GRAD, so the two prompt setting can be compared fairly with single prompt settings?\n\n3. Could you revise Figure 4 to also display the matted image together with the trimap for the same frames and prompts so readers can directly see how trimap quality translates into the visual result?\n\n4. There are some typographical errors. For example, Figure 7,8,9 caption contains “visualizaiotn” instead of “visualization”. Could you proofread the paper and update all figure texts and captions, ensuring correct spelling, consistent capitalization?\n\n5. Section 3.4 is difficult to read and follow. Could you provide a clearer rewrite during rebuttal?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PQxlvdb27C", "forum": "6K08FPo2cf", "replyto": "6K08FPo2cf", "signatures": ["ICLR.cc/2026/Conference/Submission293/Reviewer_TLqf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission293/Reviewer_TLqf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549445357, "cdate": 1761549445357, "tmdate": 1762915487353, "mdate": 1762915487353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a generic video matting algorithm for different objects including person, animal, fire, water etc. It proposes a two-branch network to conduct the segmentation and tri-map prediction tasks, respectively. It also proposes a new dataset named Natural Video Matting covering different object categories.\n\nThe generic video matting task is challenging, and the authors take an initial step towards it."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tThis work deals with general objects for video matting, which is more advanced than existing work mainly dealing with humans.\n-\tAccording to table 2, the proposed method (matting anything2) outperforms existing methods on the proposed new dataset and a human matting dataset."}, "weaknesses": {"value": "-\tThe model architecture is largely Segment Anything 2 (SAM2) with some additional components. The box and point prompt capability are directly from SAM2. The novelty of the proposed framework is limited.\n-\tThe evaluation dataset is small for general object categories. It only contains 50 clips.\n-\tIn supplementary section A, it shows the proposed method and existing method uses different datasets for training the model. The proposed method used more image matting data for training, which may pose an unfair comparison with existing methods. Can authors provide a variant that is trained with same data (like what Matanyone used) to give a fair comparison?"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qT5Es3K7c7", "forum": "6K08FPo2cf", "replyto": "6K08FPo2cf", "signatures": ["ICLR.cc/2026/Conference/Submission293/Reviewer_Uqg2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission293/Reviewer_Uqg2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929658889, "cdate": 1761929658889, "tmdate": 1762915487257, "mdate": 1762915487257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Matting Anything 2, a versatile video matting model designed to overcome the domain-specificity (e.g., human-centric) and restrictive first-frame mask requirements of existing methods. The core technical contributions are twofold. First, a Promptable Dual-mode Decoder (PDD) that jointly predicts segmentation masks and high-quality trimaps, leveraging trimap-based guidance for generalization. Second, a Memory-Separable Siamese (MSS) mechanism that recurrently isolates trimap prediction from interfering mask memory, crucially improving temporal consistency for challenging transparent objects. To validate these contributions, the authors introduce the new, diverse Natural Video Matting (NVM) dataset. Experiments demonstrate that MAM2 significantly outperforms state-of-the-art methods on both diverse natural scenes and human portraits, accepting flexible prompts like points or boxes."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper demonstrates compelling quantitative and qualitative results, significantly outperforming previous state-of-the-art methods.\n2. The paper well extends SAM2's promptable, generalist architecture to handle the distinct and more complex task of alpha matting. \n3. The paper is well-written, clearly organized, and easy to follow."}, "weaknesses": {"value": "1. The name Natural Video Matting is confusing. In matting literature, \"natural\" typically implies real-world, non-composited videos. Since NVM is synthetic (composited from assets), this name is a misnomer and should be revised to avoid ambiguity.\n2. All experiments are conducted exclusively on synthetic (composited) videos. This leaves a significant gap in evaluation, as performance on real-world videos that contain artifacts like complex lighting, sensor noise, and motion blur remains unproven. The matting \"anything\" claim is therefore not fully substantiated.\n3. The paper lacks a dedicated limitations. There is no discussion of potential failure cases."}, "questions": {"value": "The paper presents an extension of SAM 2 to the matting domain, and the quantitative and qualitative results shown are excellent. My primary question, however, concerns the validation scope. All experiments were conducted on synthetic (composited) videos. This raises a question about the method's true capability to video matting \"anything\". To fully substantiate the paper's strong claims, I recommend that the authors provide qualitative results (and quantitative results if possible) on real-world video clips, such as those sourced from YouTube, to demonstrate the model's robustness to non-synthetic artifacts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yKmsiaw4VG", "forum": "6K08FPo2cf", "replyto": "6K08FPo2cf", "signatures": ["ICLR.cc/2026/Conference/Submission293/Reviewer_Qe9K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission293/Reviewer_Qe9K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931395900, "cdate": 1761931395900, "tmdate": 1762915487116, "mdate": 1762915487116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of generalized video matting beyond human-centric domains. \nIt introduces Matting Anything 2, a robust model capable of handling diverse objects, including transparent ones, with flexible user prompts such as points, boxes, or masks. \nThe authors propose a Promptable Dual-mode Decoder (PDD) that jointly predicts segmentation masks and trimaps to enhance matting quality and generalization. \nTo address temporal instability for transparent objects, a Memory-Separable Siamese (MSS) mechanism is designed.\nExtensive experiments on diverse exiting benchmark and the newly proposed Natural Video Matting (NVM) dataset demonstrate that MAM2 achieves state-of-the-art accuracy and strong generalization to diverse, real-world scenes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is overall easy to follow and understand. It provides a clear motivation and explains key ideas effectively with well-designed supporting figures (e.g., Figure 3 and 4).\n\n2. The ablation studies are comprehensive and convincingly demonstrate the effectiveness of each proposed component.\n\n3. The authors conduct extensive evaluations across diverse tasks (image and video matting) and environments, which strongly support the generality and robustness of the proposed model."}, "weaknesses": {"value": "1. Although the model shows superior performance over baselines across multiple tasks, much of the improvement may stem from leveraging the powerful foundation model SAM2. The proposed method benefits greatly from SAM2’s strong generalization and semantic understanding, whereas most baselines are not based on such advanced foundation models. Therefore, a more in-depth comparison with other SAM1/SAM2-based matting models is essential. For the image matting task, paper [A], which also utilizes SAM, would be a particularly relevant and strong comparison.\n\n2. The Natural Video Matting (NVM) dataset is presented as one of the main contributions, but its description lacks sufficient detail. While brief statistics (in Table 1) and a few visual examples (in the supplementary material) are provided, the paper should offer a more detailed breakdown of the dataset composition—such as domain categories and their relative proportions—especially since it emphasizes dataset diversity as a key feature.\n\n3. Since the primary application of video matting lies in video editing, it would strengthen the paper if the authors demonstrated editing results using the generated alpha mattes, rather than only presenting matte outputs.\n\n[A] ZIM: Zero-Shot Image Matting for Anything, ICCV 2025"}, "questions": {"value": "Minor issues include a few typos (e.g., “iamge” in line 406) and missing citations (e.g., MEMatte in line 203 is mentioned without a proper reference). A careful proofread and citation check would improve the paper’s overall quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RhvqXXkBDL", "forum": "6K08FPo2cf", "replyto": "6K08FPo2cf", "signatures": ["ICLR.cc/2026/Conference/Submission293/Reviewer_Fzs3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission293/Reviewer_Fzs3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140644748, "cdate": 1762140644748, "tmdate": 1762915487014, "mdate": 1762915487014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}