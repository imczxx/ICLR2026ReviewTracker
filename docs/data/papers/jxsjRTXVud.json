{"id": "jxsjRTXVud", "number": 1715, "cdate": 1756910863641, "mdate": 1759898192638, "content": {"title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind", "abstract": "Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while humans are skilled in modeling their opponent's thoughts and opinions proactively and dynamically, current LLMs struggle with such Theory of Mind (ToM) reasoning, resulting in limited diversity and opponent awareness. To address this limitation, we introduce **Theory of Mind Augmented Persuader (ToMAP)**, a novel approach for building more flexible persuader agents by incorporating two theory of mind modules that enhance the persuader's awareness and analysis of the opponent's mental state. Specifically, we begin by prompting the persuader to consider possible objections to the target central claim, and then use a text encoder paired with a trained MLP classifier to predict the opponent’s current stance on these counterclaims. Our carefully designed reinforcement learning schema enables the persuader learns how to analyze opponent-related information and utilize it to generate more effective arguments. Experiments show that the ToMAP persuader, while containing only 3B parameters, outperforms much larger baselines, like GPT-4o, with a relative gain of 39.4% across multiple persuadee models and diverse corpora. Notably, ToMAP exhibits complex reasoning chains and reduced repetition during training, which leads to more diverse and effective arguments. The opponent-aware feature of ToMAP also makes it suitable for long conversations and enables it to employ more logical and opponent-aware strategies. These results underscore our method's effectiveness and highlight its potential for developing more persuasive language agents. We will release our code via GitHub.", "tldr": "We propose ToMAP, a persuader training framework that utilizes theory of mind modules to effectively model the opponents' mental state, which exhibits superior persuasiveness and effective strategies.", "keywords": ["persuasion", "theory of mind", "large language model", "reinforcement learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83ed5f1b0cb37eac885befc0bf077e72a97dbe5d.pdf", "supplementary_material": "/attachment/cde5fb6a3a50eb319af126a3ef73c2eb3e76ac7f.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Theory of Mind Augmented Persuader (ToMAP), a framework designed to enhance persuasive capabilities in large language models by modeling the opponent's mental state. ToMAP employs a text encoder and a trained MLP to predict the opponent's stance on counterclaims within a cognitive graph that represents interconnected beliefs and arguments. It features two key modules: a counterclaim predictor, which helps plan diverse and proactive arguments, and an opponent attitude predictor, which estimates the persuadee's level of agreement. A reinforcement learning schema enables the persuader to interpret and leverage these opponent-related insights effectively, producing more logical, opponent-aware, and contextually adaptive strategies. As a result, ToMAP generates richer reasoning chains, reduces repetition, and performs well in long, dynamic dialogues, outperforming much larger models such as GPT-4o despite using only 3B parameters.\nWhile the paper shows promising empirical results, the relationship with state-of-the-art is insufficient, and important information about the empirical evaluation is lacking."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well structured, with a clearly stated motivation and key message. The proposed methodology seems sound and fits naturally within a two-agent (persuader–opponent) framework. The reinforcement learning (RL) schema effectively enables the persuader to learn adaptive persuasive strategies based on the opponent's state. The design of the reward function goes beyond a single persuasion reward and incorporates auxiliary, heuristically driven components (including a repetition penalty). \n\t- The approach achieves state-of-the-art performance using a much smaller 3B-parameter persuader model, outperforming significantly larger models such as GPT-4o. \n\t- The paper includes a broad analysis, including ablation studies of the RL and Theory of Mind (ToM) modules, impact of auxiliary RL rewards and performance as the number of dialogue turns increases.\n\t- The approach is validated through a human evaluation, showing substantial inter-rater agreement (Cohen's κ = 0.7)."}, "weaknesses": {"value": "- The related work section is overly brief, and merely mentions different strands of research. It fails to effectively relate the proposed problem and approach to the state-of-the-art, and does not explicitly identify research gaps. As a result, it is not clear how the proposed ToMAP model contributes to the research field.\n- it is unclear why it is sufficient to model the persuadee as an agent. While there is a validation through a human evaluation, it is unclear what it entails: how many human experts are there? in which sense are they experts in what? how was their agreement? what was the setup for their evaluation?\n- The paper fails to clarify important aspects of the setup and empirical evaluation, such has how to measure content overlap between consecutive turns and why a (seemingly low?) value of 12% is a concern - is this improved by the proposed ToMAP model? \n- the definition in Eq. 7 is unclear - what does it mean to negate a natural language prompt?\n- (line 148) - \"First, as LLMs are being deployed into practical applications, they often act as persuadees.\" -> this broad (bold?) claim is not supported by any references. \n- the notion of persuasion and the agreement or disagreement with a stance are not sufficiently defined."}, "questions": {"value": "- what is the research gap?\n- how is the methodology different from close related work?\n- how is the human evaluation conducted with how many participants?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rbw6Es6Niy", "forum": "jxsjRTXVud", "replyto": "jxsjRTXVud", "signatures": ["ICLR.cc/2026/Conference/Submission1715/Reviewer_f5DR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1715/Reviewer_f5DR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928556867, "cdate": 1761928556867, "tmdate": 1762915866524, "mdate": 1762915866524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ToMAP, which incorporates mental theories to enhance its persuasiveness. Through two modulesone that predicts counterclaims and another that predicts the persuadee’s attitude toward those claims. ToMAP uses only 3B parameters and outperforms large models like GPT-4o on multiple datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Adding ToM into persuasion is something I haven’t seen much of, and I think it’s really innovative. It’s a fresh approach that feels like it could be the next step in making LLMs more human-like in how they reason.\n2. The experiments are thorough and convincing. They test ToMAP against a range of baselines and show how much better it performs.The ablation study proved that both Tom modules are important.\n3. It could be very useful in real-world scenarios and has excellent application prospects. The writing is fluent. The overall logic is clear, and the motivation, method, and experiment are all explained clearly."}, "weaknesses": {"value": "1.\tThere's too little human evaluation. Section 5.1 only had human judges evaluate 50 conversations, then said LLMs and human judgment were aligned, and that was it. 50 samples is far too few, and only the QWen-7B persuade was tested.\n2.\tThe long conversation experiment has problems. The paper mentions long conversations but skips details on how to avoid repetition within them. Figure 5 says that RL plateau even declines after 3 turns, but ToMAP keeps increasing. But you trained it using 3 turns (Table 5), so it's normal for the RL model to fail if it hasn't seen longer conversations."}, "questions": {"value": "1. How do you select counterclaims? Just prompts or auto?\n2. If a stronger SFT baseline is used (such as using all training data or training more epochs), will ToMAP still have such a significant advantage?\n3. Can human evaluation be scaled up? 50 samples are too few now"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ojhzft7eds", "forum": "jxsjRTXVud", "replyto": "jxsjRTXVud", "signatures": ["ICLR.cc/2026/Conference/Submission1715/Reviewer_DNns"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1715/Reviewer_DNns"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983520139, "cdate": 1761983520139, "tmdate": 1762915866154, "mdate": 1762915866154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ToMAP (Theory of Mind Augmented Persuader), an RL-trained persuader that augments an LLM with two opponent-modeling modules: (i) a counterclaim predictor that anticipates objections, and (ii) an opponent attitude predictor (BGE-M3 encoder + MLP) that estimates the persuadee’s stance on those counterclaims. The overall system optimizes a turn-local persuasion reward (normalized opinion-shift on a 5-point Likert scale) together with auxiliary penalties, trained with PPO. Empirical results across three datasets (CMV, Anthropic, args.me) and multiple persuadee models (Gemma, LLaMA, DeepSeek-R1, GPT-4o) demonstrate that ToMAP improves persuasion effectiveness, particularly in multi-turn dialogues. The model outperforms SFT and RL-only baselines, with added analysis of human-alignment (50-sample study) and strategy taxonomy showing more logical, opponent-aware argumentation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The reinforcement learning setup is carefully defined with detailed reward formulation, ToM module integration, and training pipeline transparency, which can be a good step for social LLMs\n2. Experiments span multiple datasets, persuasive contexts, and opponent models, showing generalizability.\n3. The strategy taxonomy and qualitative examples give a decent idea of how ToMAP produces more persuasive arguments."}, "weaknesses": {"value": "1. Though ablations exist, it remains partially unclear to me whether improvements stem from ToM features themselves or increased contextual conditioning capacity.\n2. The critic is noisy and prone to reward hacking; The RM, is an LLM and as the authors themselves discuss recent work show that LLMs are not adept at it.\n3. To claim RL+ToM does the improvement, the confounder of ToM needs to be removed to see if it is RL (i.e. supervision) that does the improvement or RL+ToM"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1eymWVJZlW", "forum": "jxsjRTXVud", "replyto": "jxsjRTXVud", "signatures": ["ICLR.cc/2026/Conference/Submission1715/Reviewer_Kh2V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1715/Reviewer_Kh2V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993572676, "cdate": 1761993572676, "tmdate": 1762915865398, "mdate": 1762915865398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a theory-of-mind inspired training framework for training smaller language models (LLMs) to be more effective in persuasive dialogues by introducing a counterclaim provider and an opponent attitude predictor. Then they do RL to train the persuader with these additional inputs. The authors demonstrate that their specialized 3B model outperforms models like GPT-4o in a synthetic, LLM-vs-LLM persuasion task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is overall well-written and clear. The proposed framework makes intuitive sense. I appreciate the authors’ relatively thorough ablation studies and the human-validation of the LLM judge model."}, "weaknesses": {"value": "Results overall are inconsistent and somewhat unconvincing. \n\n1)\tif we look at the results in Table 1, it very much seem to be the case that whether or not the proposed framework works depends on the persuade model; RL setting outperform ToMAP when LLaMa3.1 is the persuade; SFT seems to outperform both RL and ToMAP when Phi-4 is the persuade model; The simple averaging of scores in the \"Avg.\" column obscures these crucial inconsistencies and paints an overly optimistic picture. Additionally, aggregating results across different datasets and in-domain/out-of-domain persuadees in this manner is methodologically questionable.\n\n2)\tBaselines are somewhat unfair. The proposed method relies on complex, external modules and a specific training regimen to perform ToM-like reasoning. A fair zero-shot baseline would involve prompting the model to perform all of these steps (e.g., \"First, anticipate potential counterarguments. Second, estimate the opponent's stance on them. Third, formulate a response.\") within its context. Without such strong, prompt-engineered baselines, it is unclear if the performance gain comes from the proposed framework or simply from the fact that the baselines were not instructed to perform the same reasoning. This is arguably quite important, especially given the complexity of the proposed pipeline. \n\n3)\tAlso, an additional issue is, based on Figure 4, ToMAP seems to significantly increase the output token. This introduces a potential confounding variable: the performance gain might be an artifact of increased inference-time computation and planning rather than the specific ToM modules. The authors need to rule out the hypothesis that allowing the baseline models to generate similarly long chain-of-thought-style reasoning would also improve their performance.\n\n4)\tIt is unclear whether the proposed framework is general. The Opponent Attitude Predictor is trained on data from a single model (QWen-7B). Its ability to generalize and accurately predict the attitudes of unseen models is unclear; The entire ToMAP framework is only applied to one model family (QWen-3B). It is unclear if these techniques and findings would transfer to other architectures (e.g., Llama, Mistral) and model sizes.\n\nConceptual contributions and framing:\n\n5)\tThe use of the cognitive science term \"Theory of Mind\" is an overstatement. The implemented mechanism—generating a flat list of counterclaims and predicting scores—is a form of opponent modeling, but it lacks the recursive, nested, and rich mental state representation that genuine ToM entails. The authors should discuss this conceptual gap and reframe their contribution more accurately.\n\n6)\tThe entire experiment is conducted in a synthetic LLM-vs-LLM environment. There is no evidence to suggest that the strategies learned (e.g., increased use of \"Common Ground\") would be effective in persuading humans, who possess genuine beliefs, emotions, and cognitive biases not modeled by current LLMs. The paper's claims about developing more \"human-like\" persuaders are speculative.\n\n7)\tThe paper fails to engage with recent and highly relevant work on enhancing LLM ToM, particularly through interactive methods like asking clarifying questions. Discussing work such as https://arxiv.org/abs/2412.12175, https://arxiv.org/abs/2502.04485, and https://arxiv.org/abs/2502.00640 would strengthen the literature review and contextualize the paper's contributions.\n\nQuestions about evaluation: \n\n8)\tThe use of third-party \"human experts\" to assess a persuadee's attitude (line 373) is fundamentally flawed. Only the individual being persuaded can accurately report their internal mental state and attitude. A third-party observation is a proxy for perceived success, not a measure of actual attitude change. You can ask any researcher working on (human) persuation research and present your approach, and see their reactions.\n\n9)\tThe entire automated evaluation relies on a relatively small model (Qwen-7B) as the persuadee/judge. Smaller models are known to have stronger biases, inconsistencies, and may or may not serve as reliable evaluators for complex tasks like persuasion. The validation study is a good first step but it is done on only 50 samples and thus unlikely to cover the entire distribution. Overall, the reliance on this single, small model for all training and evaluation is still very much a concern.\n\n10)\tThe strategy usage in Figure 6 is not convincingly different between the base model and ToMAP. The distributions appear qualitatively similar, and one could even argue that ToMAP's distribution has lower entropy, potentially indicating a degree of mode collapse towards a few specific strategies rather than greater strategic diversity?\n\nIssues with Prompts and Presentation:\n\n11)\tI have some concerns regarding the prompt used: 1) Line 791-792: “2. Try your best to persuade Bob into believing your claim by proposing arguments with fine logic and elaboration” May I ask why “fine logic and elaboration” is included here? Could this constrain the model’s persuasion strategy, especially in a zero-shot setting? In line 824-825 “2. DO NOT include uncertified evidences or unverified information.” could you provide some rationale on including this?\n\n12)\tOverall, could you show one complete conversation of your proposed framework vs. baselines? Like Table 6 but one complete conversation with many turns. \n\n13)\tLots of formatting issues (e.g. line 94-95 citation format is wrong; Line 102 a space is missing; Line 266 please follow how Qwen team officially spell their models)"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7IdHJ1lhsm", "forum": "jxsjRTXVud", "replyto": "jxsjRTXVud", "signatures": ["ICLR.cc/2026/Conference/Submission1715/Reviewer_Eovx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1715/Reviewer_Eovx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996482528, "cdate": 1761996482528, "tmdate": 1762915864744, "mdate": 1762915864744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}