{"id": "eH9Wlahibz", "number": 24388, "cdate": 1758356372123, "mdate": 1759896768689, "content": {"title": "Improving Sparse-View 3DGS Generalization via Flat Minima Optimization", "abstract": "Recent advances in neural rendering have established 3D Gaussian Splatting (3DGS) as a highly efficient representation for novel view synthesis, enabling real-time training and rendering with strong fidelity. However, when supervision is limited to a sparse set of input views, 3DGS tends to overfit to the observed images, resulting in poor generalization to unseen viewpoints. We approach this challenge from the perspective of flat minima (FM) optimization, which seeks solutions that remain stable under small parameter perturbations and are thus more robust. Viewing Gaussian parameters as trainable weights, we adapt FM principles to the geometric and dynamic nature of 3DGS by introducing several key techniques. First, we propose a Scale-Adaptive Perturbation (SAP) scheme that scales perturbation magnitude according to each Gaussian’s anisotropy, preserving fine details while promoting robustness. Second, we adopt stochastic perturbation where each Gaussian is probabilistically perturbed or left unchanged, allowing perturbations while preventing oversmoothing of scene details. Third, we schedule the perturbation magnitude to increase gradually during training, avoiding excessive noise before Gaussians capture stable structure. Finally, we incorporate periodic reinitialization of non-positional parameters such as scale, rotation, and opacity, and Spherical Harmonics (SH) coefficients. preventing degenerate cases like elongated Gaussians and maintaining well-conditioned primitives throughout optimization. Together, these techniques form a lightweight framework that integrates seamlessly into existing 3DGS pipelines without architectural changes. Experiments on LLFF and Mip-NeRF360 demonstrate that our method consistently improves both quantitative metrics and perceptual quality under sparse-view supervision, producing reconstructions that are sharper, more stable, and better generalized to novel viewpoints.", "tldr": "We enhance sparse-view 3DGS generalization using flat-minima optimization via random position perturbations.", "keywords": ["Artificial Intelligence", "Computer Vision", "Novel View Synthesis", "3D Gaussian Splatting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b8260662421532ab081d36a7cf9a6b239f3efac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a flat-minima optimization framework for sparse-view 3DGS that injects scale-adaptive perturbations into Gaussian positions during training to reduce overfitting. It further improves novel view rendering qualities by mixing perturbed and unperturbed objectives, perturbation magnitude scheduling, and periodic re-initialization. It significantly improved the rendering quality of vanilla 3DGS under sparse-view settings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The integration of Flat Minima Optimization into vanilla 3DGS under sparse view settings to avoid overfitting is well motivated.\n- The method is clearly explained and easy to reproduce.\n- The ablation studies clearly demonstrate the effectiveness of each of the proposed components."}, "weaknesses": {"value": "- Limited quantitative gain: the main issue of this paper is that it's not convincing that the proposed method really outperforms the baseline method DropGaussian. In most experiments, it only achieves very marginal quantitative improvements. In some occations (e.g., 12-view NVS), the PSNR of DropGaussian is even higher.\n- Missing baselines: two strong baselines for sparse-view NVS are missing: MAtCha Gaussians (CVPR 2025) and Difix3D+ (CVPR 2025). They leverage monocular depth priors and diffusion priors to achieve high-quality NVS results, respectively. Comparing to these additional two baselines would provide a more comprehensive assessment of the effectiveness of the proposed method."}, "questions": {"value": "I recommend the authors to add qualitative/quantitative comparisons to the missing baselines MAtCha Gaussians and Difix3D+. I also recommend the authors to provide a side-by-side video comparison with all baselines to help better assess the rendering qualities.\n\nApart from the mentioned weaknesses, I have an additional question:\n- Can the proposed method (at least the probablistic Scale-Adaptive Perturbation part) be implemented as a general plug-in applicable to a wide range of sparse-view Gaussian Splatting pipelines? For instance, if incorporated into baselines such as DropGaussian, would it achieve a comparable relative quantitative improvement over those methods as it does when applied to vanilla 3DGS? Demonstrating consistent relative gains across diverse pipelines would make the contribution significantly more convincing, highlighting the method’s plug-and-play versatility and broad applicability.\n\nI would consider raising my score if the author could add the missing baselines and show a consistent improvement by plugging in the proposed approach into more baseline methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zUc3fYvXKX", "forum": "eH9Wlahibz", "replyto": "eH9Wlahibz", "signatures": ["ICLR.cc/2026/Conference/Submission24388/Reviewer_Hzaj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24388/Reviewer_Hzaj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761247837076, "cdate": 1761247837076, "tmdate": 1762943066460, "mdate": 1762943066460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Clarification on Overlap With Prior Noise-Injection Studies in 3DGS (Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting)"}, "comment": {"value": "I would like to note that our NeurIPS 2025 paper “Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting” (https://openreview.net/forum?id=GrPo8NTtzK) has already conducted a comprehensive analysis of noise injection applied to Gaussian parameters (opacity, scale, position, SH, etc.) as a means to reduce co-adaptation in 3DGS. The idea explored in this submission is therefore highly related to what we previously studied.\n\nSince the paper currently does not cite this line of research, I suggest adding the reference for proper attribution and to clarify what new insights this submission provides beyond existing work."}}, "id": "yPcxOSWBjA", "forum": "eH9Wlahibz", "replyto": "eH9Wlahibz", "signatures": ["~Kangjie_Chen2"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Kangjie_Chen2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24388/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762980051515, "cdate": 1762980051515, "tmdate": 1762980051515, "mdate": 1762980051515, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limited generalization of 3D Gaussian Splatting (3DGS) under sparse-view supervision.\n The authors reinterpret 3DGS optimization as a form of weight learning and propose a set of flat-minima-inspired perturbation strategies to promote robustness.\n Specifically, the method perturbs Gaussian parameters with magnitudes scaled by their anisotropy, applies stochastic perturbations to a random subset of Gaussians, schedules the perturbation strength to increase during training, and periodically reinitializes non-positional parameters to avoid degeneracy.\n Experiments on LLFF and Mip-NeRF360 show consistent improvements over existing baselines such as DropGaussian and CoR-GS."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Framing sparse-view overfitting as a sharp-minima problem provides a fresh and intuitive perspective.\n2. The approach is simple to implement, adds minimal computational cost, and integrates easily into existing 3DGS pipelines.\n3. Ablation studies and comparisons are clearly presented, isolating the contributions of each component."}, "weaknesses": {"value": "1. Despite the flat-minima motivation, the method essentially performs Gaussian noise injection—a well-known regularization technique. Similar stochastic or Bayesian formulations (e.g., 3DGS-MCMC) are not cited or compared.\n2. No analysis (e.g., curvature or sharpness metrics) is provided to demonstrate that the optimization indeed converges to flatter minima.\n3. Reported improvements are small and may fall within expected training variance, raising questions about the actual impact of the proposed components."}, "questions": {"value": "1. Can you provide evidence that the proposed method achieves flatter minima, such as curvature or Hessian-based analysis?\n2. How sensitive is performance to perturbation probability, noise scale, and reinitialization frequency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FibDnpvEjh", "forum": "eH9Wlahibz", "replyto": "eH9Wlahibz", "signatures": ["ICLR.cc/2026/Conference/Submission24388/Reviewer_B2SA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24388/Reviewer_B2SA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626724250, "cdate": 1761626724250, "tmdate": 1762943066249, "mdate": 1762943066249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles sparse-view 3D Gaussian Splatting (3DGS) overfitting by importing flat-minima (FM) optimization ideas and tailoring them to the geometry of Gaussians. The core method (1) perturbs Gaussian positions with a Scale-Adaptive Perturbation (SAP) whose noise matches each Gaussian’s anisotropy and size, (2) applies those perturbations stochastically (some Gaussians may have perturbations and some may not) per-Gaussian to avoid oversmoothing, (3) linearly schedules perturbation strength over training, and (4) periodically reinitializes non-positional parameters (scale, rotation, opacity, SH) to prevent degenerate elongated Gaussians. Notably, the method is lightweight and does not require any architectural changes to 3DGS. The method improves PSNR/SSIM and lowers LPIPS versus baselines. Ablations confirm position noise with anisotropic scaling, stochastic application, scheduling, and reinitialization each contribute to the gains. Qualitatively, results show sharper details and better geometric consistency in under-constrained regions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated approach: The connection between flat minima optimization and sparse-view generalization in 3DGS is intuitive and well-articulated. Viewing overfitting as convergence to sharp minima is a reasonable perspective.\n2. Comprehensive ablations: The paper includes thorough ablation studies examining each component (perturbation design, parameter choices, stochastic strategy, scheduling, reinitialization), providing evidence for design decisions.\n3. Practical and lightweight: The method integrates seamlessly into existing 3DGS pipelines without architectural changes or significant computational overhead.\n4. Consistent improvements: Results show steady gains across multiple datasets, view settings, and metrics (PSNR, SSIM, LPIPS), demonstrating robustness."}, "weaknesses": {"value": "1. The core ideas (perturbation-based optimization, parameter reinitialization) are well-established techniques. While the adaptation to 3DGS is reasonable, the conceptual contribution feels incremental.\n2. While consistent, the quantitative gains are relatively small. Some qualitative differences (eg. in Figure 2) are subtle.\n3. The paper doesn't provide rigorous analysis of why position perturbations specifically lead to flatter minima or how the proposed techniques relate to formal FM theory. No measurement of actual loss landscape sharpness before/after.\n4. The method introduces several hyperparameters. Limited discussion of sensitivity to these choices or guidance on setting them for new scenarios."}, "questions": {"value": "1. How does computational cost compare to baselines? While you mention it's lightweight, actual training time and memory comparisons would be helpful.\n2. Why perturb only positions and not jointly optimize with other parameters? Table 4 shows position is best, but have you tried learned perturbation schedules for different parameter types?\n3. Can this approach be combined with other sparse-view methods (depth priors, semantic features) for further improvements?\n4. What happens in extremely sparse settings (e.g., 2 views)? Are there failure cases where your method doesn't help?\n\n\nAdditionally, Figure 1 doesnt seem to be rendering on PDF viewers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uk0H4Q9pGm", "forum": "eH9Wlahibz", "replyto": "eH9Wlahibz", "signatures": ["ICLR.cc/2026/Conference/Submission24388/Reviewer_2A7R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24388/Reviewer_2A7R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762220559053, "cdate": 1762220559053, "tmdate": 1762943065943, "mdate": 1762943065943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel optimization framework for improving the generalization of 3D Gaussian Splatting (3DGS) in sparse-view novel view synthesis. Traditional 3DGS methods tend to overfit when trained on limited input images, leading to poor reconstruction quality in unseen viewpoints. The authors propose incorporating Flat Minima (FM) optimization—a concept from neural network training—into 3DGS to mitigate this issue. Specifically, they introduce a Scale-Adaptive Perturbation (SAP) scheme that applies probabilistic, geometry-aware perturbations to Gaussian positions, along with perturbation scheduling and periodic parameter reinitialization to improve robustness and prevent overfitting. Experiments on LLFF and Mip-NeRF360 datasets demonstrate that this method achieves consistent improvements in PSNR, SSIM, and LPIPS compared to existing 3DGS baselines (DropGaussian, CoR-GS, and DNGaussian). Ablation studies validate the importance of each proposed module, showing that the FM-inspired perturbations improve both fidelity and generalization, particularly under sparse-view settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. While this paper builds on existing FM optimization techniques for improving model generalization, extending parameter perturbation from traditional neural network weight training to gaussian splatting field fitting is entirely novel. The Scale-Adaptive Perturbation method, in particular, represents a unique application of perturbation.\n\n2. The paper clearly outlines each design decision in its perturbation scheme, with every section providing information relevant to understanding the methodology and results.\n\n3. The paper offers strong evidence for its conclusions through quantitative tables and rigorous ablative studies that motivate each design decision.\n\n4. The significance to the field of sparse view novel synthesis is clear: the paper delivers a gaussian splatting field fitting pipeline that improves both quantitative and qualitative results while opening a new direction for future methods—the incorporation of FM optimization techniques and ideas."}, "weaknesses": {"value": "1. Regarding presentation, Equation 4 does not clearly show how sampling covariance depends on the scale and rotation of a given Gaussian kernel, though this relationship is illustrated in the Scale Adaptive Perturbation section of Figure 1.\n2. Additionally, while the tables covering the ablation studies are informative, including visual ablative results similar to those in Figure 2 would strengthen the argument.\n3. In terms of soundness, one aspect requiring further investigation is the scope of experiments on perturbing different parameters. Although evidence suggests that perturbing position alone yields the best overall performance, the experiments and accompanying discussion are insufficient to prove this conclusively. Notably absent are perturbation experiments on elements such as spherical harmonic coefficients. The paper would benefit from expanding these experiments or acknowledging that further exploration may be necessary for future work."}, "questions": {"value": "My questions are primarily related to the concerns I had about your “Effect of Perturbing Different Parameters” section. I feel the section/ablative study indicates that applying perturbation to other parameters of gaussian splatting fields will not improve performance. It is not quite clear if this is a claim you intend to make or if you encourage further exploration in this area."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nMrTxb0LdD", "forum": "eH9Wlahibz", "replyto": "eH9Wlahibz", "signatures": ["ICLR.cc/2026/Conference/Submission24388/Reviewer_v3C8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24388/Reviewer_v3C8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762867192058, "cdate": 1762867192058, "tmdate": 1762943065755, "mdate": 1762943065755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}