{"id": "wtLyksjIdl", "number": 21387, "cdate": 1758317005299, "mdate": 1759896924679, "content": {"title": "Building Learning Context For Autonomous Agents Through Generative Optimization", "abstract": "Building intelligent agents that learn involves designing systems that can evolve their behavior based on experiences. While early approaches to large language models (LLMs) agent learning relied mostly on structured memory and in-context learning, they often led to behavioral instability, poor interpretability, and difficulty in control.\nRecent success in generative optimization, where an LLM is used as an optimizer, has shown the possibility of creating autonomous software agents. By separating behavior logic (workflow) and how that logic is updated (optimizer), the agent designer can exhibit more control over the agent.\nIn this work, we show the surprising fact that the agent learning problem is \\textit{under-specified} with the generative optimization framework. If we want an agent to learn the right behavior, we must set up the right context that will induce such behavior. \nWe investigate three types of software engineering problems that span data science, computer security, game playing, and question answering. We show that the original generative optimization framework can only learn robustly under one of the three settings. To address the issue, we propose to construct a meta-graph through templates to introduce the right learning context to an LLM optimizer. With this addition, we demonstrate that defining the right learning context enables agents to discover behaviors aligned with the designer's objectives. In particular, we show the first known result of using generative optimizers to learn executable programs that play Atari games, where the resulting agents achieve performance comparable to deep reinforcement learning while requiring 50%-90% less training time.", "tldr": "We show how to construct context-specific learning graphs that allow LLM agents to learn desirable behaviors", "keywords": ["Agent Learning", "Large Language Model Optimizer", "Reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a63374b2ac32218fd4ace17b6176ad2e8cd6462f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper is focusing on the application of Generative Optimization to address the under-specification problem in autonomous agent learning. It proposes a fix by adding template-based \"meta-graphs\" that guide the LLM optimizer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Main strength is in the introduction of a learning context meta-graph that guides an LLM, acting as an optimizer, to iteratively refine an agent's executable code (workflow) using rich, non-differentiable feedback."}, "weaknesses": {"value": "The primary goal is to build an optimized agent computational graph from repeatable workflow graphs (basically several standard agent execution steps: generate, rerank, summarize), which is identical to reconstructing/reorganizing the agent context on the given task. There is actually no significant or superior difference compared to other agent learning frameworks [1, 2, 3, 4]. This work could be more insightful by comparing with other agent learning frameworks.\n\nBatch learning or Episodic learning template, in essence, are two different ways to construct the problem feedback based on the generated workflow graph, one is parallel, the other is sequential. The paper provides these two different ways to construct the meta-graph, but in the experiments, but not provide any further investigation/discussion of the reasoning for designing these two types of graphs, nor they perform ablations on the performance of agent learning using two different graphs on the same problem.\n\n[1] Ma, X., Lin, C., Zhang, Y., Tresp, V., & Ma, Y. (2025). Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation. arXiv preprint arXiv:2506.09046.\n\n[2] Wei, A., Nie, A., Teixeira, T. S., Yadav, R., Lee, W., Wang, K., & Aiken, A. (2024). Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces. arXiv preprint arXiv:2410.15625.\n\n[3] Yuan, S., Chen, Z., Xi, Z., Ye, J., Du, Z., & Chen, J. (2025). Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training. arXiv preprint arXiv:2501.11425.\n\n[4] Yang, Y., Kang, S., Lee, J., Lee, D., Yun, S. Y., & Lee, K. (2025). Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback. arXiv preprint arXiv:2506.04287.\n\n\nMinor:\n\nIn abstract, “We investigate three types of software engineering problems …\", shouldn’t it be four types?\n\nDiagram needs to be replotted; they are currently not readable, see Fig. 1, Fig. 2, Figure A.4, etc."}, "questions": {"value": "As the paper suggested, this proposed framework can generalize to different domains. While in this work, the paper is focusing mainly on engineering problems. Can you expand the application of this framework to some open-ended tasks, such solving research problems?\n\nThe experiments conducted in this work are generally have a clear/explicit task objective. How can a generative computational graph be effectively structured and trained when the task objective is unknown or highly unstructured ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7qNJHt3q8p", "forum": "wtLyksjIdl", "replyto": "wtLyksjIdl", "signatures": ["ICLR.cc/2026/Conference/Submission21387/Reviewer_isEt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21387/Reviewer_isEt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761219162905, "cdate": 1761219162905, "tmdate": 1762941737736, "mdate": 1762941737736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that using an LLM as an optimizer over an LLM-based agent’s workflow parameters often fails because the learning context is under-specified. Therefore, it proposes specifying a meta-graph, which is constructed by inserting a workflow graph into one of three learning templates (interactive, batch, episodic) via operators so that the optimizer’s objective is aligned with what the agent designer actually wants. Comprehensive empirical validation across software engineering, reasoning, and control tasks, highlighting substantial performance and efficiency gains over deep RL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper well addresses the under-specification problem in existing LLM-based generative optimization frameworks.\n- The work is implemented and evaluated the framework across three distinct learning regimes, including interactive (MLAgentBench), batch (GSM8K, BBEH), and episodic (Atari).\n- The paper is well-organized and pedagogically clear."}, "weaknesses": {"value": "- There is no ablation that directly compares performance across identical workflows and feedback signals with and without the meta-graph structure.\n- Missing baselines which formalize LLM-as-optimizer paradigms [1]. They share the same optimization philosophy but differ in graph specification or update strategy, excluding them makes it difficult to attribute improvements uniquely to context specification.\n- Although the paper claims training-time savings, it does not specify how “training time” is measured. Moreover, the paper acknowledges that optimization is sometimes unstable, but does not quantify variance across runs.\n- Results rely on carefully crafted “staged” or “suggestive” feedback, yet the sensitivity of performance to phrasing or feedback noise is untested.\n\n---\n\n[1] Zhou, W., Ou, Y., Ding, S., et al. (2024). Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532."}, "questions": {"value": "- Did the authors conduct any ablation experiments where the workflow and feedback oracle were held constant and only the meta-graph template was varied?\n- How sensitive is performance to the phrasing or structure of feedback? Could the authors quantify robustness to prompt perturbations or randomized feedback?\n- The work mentions instability and potential “meta-overfitting” but provide only qualitative discussion. What mechanisms or criteria do you use to detect divergence or overfitting during generative optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LBBVKG7naw", "forum": "wtLyksjIdl", "replyto": "wtLyksjIdl", "signatures": ["ICLR.cc/2026/Conference/Submission21387/Reviewer_8Nik"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21387/Reviewer_8Nik"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720875608, "cdate": 1761720875608, "tmdate": 1762941737423, "mdate": 1762941737423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper frames agent learning as a generative optimization problem by representing the agent’s behavior and its update process as computational graphs (meta-graphs). It proposes explicit learning contexts (interactive, batch, episodic) to facilitate self-improvement of LLM-based agents across domains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Provides a unified and general framework for agent learning that works even without differentiability.\n* Demonstrates versatility across tasks (Atari, MLAgentBench, QA) with notable performance gains."}, "weaknesses": {"value": "* The method is overly conceptual and abstract, relying heavily on heuristics that vary across application domains (e.g., feedback design).\n* The framework has low reproducibility, as entire process of optimization heavily depends on LLMs.\n* Lack of comprison to baselines. There are several relevant baselines that automatively designing agent framwork like ADAS (Automated Design of Agentic Systems). It would be nice if authors clarify distinction of the proposed framework compared to such previous approach.\n* Writing is quite hard to follow. Especially in the method section, it would improve readability if the authors included more concrete examples and clearer references to illustrative figures when explaining the proposed framework."}, "questions": {"value": "* How much cost does the generative optimization take? It would be better if you can discuss more on optimization steps and costs depending on each task setup."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "O0TELSMtHr", "forum": "wtLyksjIdl", "replyto": "wtLyksjIdl", "signatures": ["ICLR.cc/2026/Conference/Submission21387/Reviewer_CKF7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21387/Reviewer_CKF7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921311625, "cdate": 1761921311625, "tmdate": 1762941737143, "mdate": 1762941737143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an improved method for LLM-based agent optimization by constructing a meta-graph via templates. This introduces the appropriate learning context to the LLM optimizer, addressing the under-specification of agent learning and enabling more robust behavior aligned with the designer’s objectives."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The results on Atari seem promising.\n- Experiments span three distinct domains (data science, computer security, and game playing)"}, "weaknesses": {"value": "- The presentation is confusing and difficult to follow sequentially. Key concepts are introduced without adequate explanation or forward references, forcing readers to jump ahead (or stay confused even after doing so).  \n  - \"Generative optimization\" is not a widely adopted term. It is under-explained and lacks clear references in abstract and introduction.\n  - It remains unclear what exactly is being optimized (e.g., what the agent’s parameters are and how they are represented in what search space).  \n  - The purpose and role of \"templates\" are ambiguous: why three of them are introduced, whether they are used separately or jointly, and how they relate to the learning setup.  \n  - The term \"meta-graph\" is inconsistently formatted (sometimes \"meta graph\") and poorly defined.  \n- Significant handcrafting undermines generality: different workflows are manually designed for each environment, and template selection appears domain-specific with no clear decision criteria.  \n- Baseline choices are questionable. Comparing to traditional RL methods (e.g., DQN, PPO) in Atari might be misleading given the fundamentally different paradigms. Stronger comparisons would be other LLM-based optimization frameworks (e.g., DSPy, TextGrad, OptoPrime), which are mentioned in related work but not evaluated against."}, "questions": {"value": "see above weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oP7sZHf0bB", "forum": "wtLyksjIdl", "replyto": "wtLyksjIdl", "signatures": ["ICLR.cc/2026/Conference/Submission21387/Reviewer_R7SN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21387/Reviewer_R7SN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982802194, "cdate": 1761982802194, "tmdate": 1762941736731, "mdate": 1762941736731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}