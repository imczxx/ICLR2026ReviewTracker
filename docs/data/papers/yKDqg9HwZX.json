{"id": "yKDqg9HwZX", "number": 9834, "cdate": 1758142797369, "mdate": 1763435564550, "content": {"title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction", "abstract": "Universal multimodal embedding models have achieved great success in capturing semantic relevance between queries and candidates. However, current methods either condense queries and candidates into a single vector, potentially limiting the expressiveness for fine-grained information, or produce too many vectors that are prohibitively expensive for multi-vector retrieval. In this work, we introduce MetaEmbed, a new framework for multimodal retrieval that rethinks how multimodal embeddings are constructed and interacted with at scale. During training, a fixed number of learnable Meta Tokens are appended to the input sequence. At test-time, their last-layer contextualized representations serve as compact yet expressive multi-vector embeddings. Through the proposed Matryoshka Multi-Vector Retrieval training, MetaEmbed learns to organize information by granularity across multiple vectors. As a result, we enable test-time scaling in multimodal retrieval where users can balance retrieval quality against efficiency demands by selecting the number of tokens used for indexing and retrieval interactions. Extensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and the Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed achieves state-of-the-art retrieval performance while scaling robustly to models with 32B parameters.", "tldr": "", "keywords": ["multimodal retrieval", "information retrieval"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/778754110b794085594c20d2febeda682fd7accf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces METAEMBED, a multimodal embedding learning framework designed for scalable late interaction. METAEMBED adds a small number of learnable Meta Tokens to multimodal inputs and trains them using a Matryoshka Multi-Vector Retrieval (MMR) strategy, which organizes embeddings hierarchically to enable test-time scaling. Specifically, MMR enables test-time scaling balancing retrieval accuracy and latency. \nThe authors evaluate METAEMBED on MMEB and ViDoRe v2 benchmarks across multiple vision-language backbones (e.g., Qwen2.5-VL, PaliGemma, and LLaMA-3.2-Vision), showing state-of-the-art retrieval performance and flexible inference efficiency across different VLM varying backbones and model sizes."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong performance: METAEMBED demonstrates strong performance across diverse domains and setups over baseline retrievers.\n2. General applicability across VLM backbones and model sizes: METAEMBED is evaluated on Qwen2.5-VL, PaliGemma, and LLaMA-3.2-Vision, show that it generalizes to different model architectures and parameter scales.\n3. Comprehensive analysis of METAEMBED: the authors provide comprehensive ablations such as the effectiveness of MMR component, efficiency analysis, analysis on different VLM backbones."}, "weaknesses": {"value": "1. Limited originality beyond adaptation of MRL: The main innovation lies in adapting Matryoshka Representation Learning (MRL) to multi-vector retrieval. While this adaptation is meaningful, much of the practical foundation (i.e., multimodal representation learning with Matryoshka nesting) derives from prior work (Cai et al. 2025).\n2. Insufficient analysis of efficiency in training: Although efficiency is discussed in the context of query encoding at test-time (Table 3). Comparing computational costs in training across different retriever methods can further strengthen the paper, as additional training costs incurred could be a trade-off for enabling flexible test-time compute.\n3. Empirical gaps in hyperparameter justification: the paper fixes the group sizes to be G=5 without ablation on group size for MRL. A sensitivity study on different group configurations would strengthen the argument for generality."}, "questions": {"value": "1. In Table 2, ColQwen2 should correspond to a 3B model rather than 2B.\n2. “P” in L182 likely refers to number of visual patch tokens and should be denoted.\n3. How do you empirically choose a group size of 5 and with these configurations for MRL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PSyCfy9f5X", "forum": "yKDqg9HwZX", "replyto": "yKDqg9HwZX", "signatures": ["ICLR.cc/2026/Conference/Submission9834/Reviewer_rgEq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9834/Reviewer_rgEq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761192223518, "cdate": 1761192223518, "tmdate": 1762921315374, "mdate": 1762921315374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces METAEMBED, a novel framework designed to enhance multimodal retrieval by addressing the key challenges of scalability, efficiency, and fine-grained retrieval accuracy. Existing methods either compress queries and candidates into a single vector, losing fine-grained information, or generate too many vectors, leading to computational inefficiencies. METAEMBED introduces Meta Tokens, which are small, learnable tokens appended to both queries and candidates, and their final contextualized representations serve as compact yet expressive multi-vector embeddings during retrieval. By employing a Matryoshka Multi-Vector Retrieval (MMR) mechanism, the model organizes embeddings into hierarchical groups, enabling flexible scaling at test-time where users can adjust retrieval quality against computational resources."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The introduction of Meta Tokens and the Matryoshka Multi-Vector Retrieval (MMR) approach provides a unique solution to the scalability and efficiency issues in multimodal retrieval, allowing flexible scaling at test-time.\n2. METAEMBED demonstrates remarkable scalability, allowing users to adjust retrieval quality based on computational constraints without retraining the model, a critical advancement for real-world deployment.\n3. The paper provides extensive evaluations across various model sizes and VLM architectures, showcasing METAEMBED's robustness and effectiveness in diverse scenarios, including challenging tasks like visual document retrieval and multimodal embedding."}, "weaknesses": {"value": "1. The design intuition of meta embedding is somewhat similar to the concept of query extension in the early information retrieval field. Could the authors further differentiate between these two design intuitions?\n2. The approach of using embedding with different granularities for retrieval is promising. Have the authors explored or statistically analyzed which granularity of embedding provides better discrimination? This could offer valuable guidance for future embedding extraction."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cU6s6g04pu", "forum": "yKDqg9HwZX", "replyto": "yKDqg9HwZX", "signatures": ["ICLR.cc/2026/Conference/Submission9834/Reviewer_g6en"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9834/Reviewer_g6en"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824810767, "cdate": 1761824810767, "tmdate": 1762921314764, "mdate": 1762921314764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework designed to resolve the critical trade-off between expressiveness and efficiency in multimodal retrieval by using a small set of learnable \"Meta Tokens\" to form a compact multi-vector representation. The central contribution is the Matryoshka Multi-Vector Retrieval (MMR) training objective, which organizes information into a coarse-to-fine structure. This enables a single model to be flexibly scaled at test-time to meet varying accuracy and efficiency demands without retraining.\nOverall, this is a quality paper with interesting findings that could have significant applications for the research community. Although some questions and concerns remain, as detailed below, I am willing to raise my score if these points are adequately addressed."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Novel Combination of Existing Concepts: novel synthesis of learnable tokens, late-interaction, and Matryoshka learning, resulting in the technically solid Matryoshka Multi-Vector Retrieval objective.\n\n- The experimental evaluation is thorough and convincing, demonstrating state-of-the-art performance on diverse benchmarks like MMEB and ViDoRe v2."}, "weaknesses": {"value": "- Lack of Interpretability and Qualitative Analysis: The \"Meta Tokens\" are treated as a black box. The paper offers no analysis or visualization to explain what these tokens learn, missing an opportunity to provide deeper scientific insight beyond a strong engineering result.\n\n- A key motivation presented in the introduction of the paper is solving the efficiency problem for multimodal-to-multimodal (universal) retrieval, yet no experiments on such a task are presented. This leaves a core claim of the paper unsubstantiated by empirical evidence. Specifically, as far as I know, the evaluation on the MMEB and ViDoRe v2 benchmarks lacks an image-to-image retrieval task (which is different from the visual document retrieval task or image+text retrieval task), which would be a key area for qualitative analysis."}, "questions": {"value": "- (Regarding the 2nd weakness) To better understand the mechanism behind METAEMBED, could you provide some qualitative analysis of the learned Meta Embeddings? For instance, what kinds of visual or textual concepts do different tokens appear to specialize in? Does the first token of an MMR-trained model consistently capture more global, summary-level information compared to later tokens, which might focus on finer details?\n\n-The choice of MMR group sizes {(1, 1), (2, 4), (4, 8), (8, 16), (16,64)} appears somewhat heuristic. How sensitive is the model's performance to this specific configuration? For example, what happens if a more linear scaling is used, and what was the process for selecting this particular set of nested budgets?\n\n-The split-(16,64) baseline in Table 5 performs worse than the single-last baseline. This is counter-intuitive, as one might expect a multi-vector representation, even a naive one, to retain more information. Does this imply that without explicit training tailored for late interaction, simply partitioning a representation is detrimental?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SKaUkKqgX3", "forum": "yKDqg9HwZX", "replyto": "yKDqg9HwZX", "signatures": ["ICLR.cc/2026/Conference/Submission9834/Reviewer_dVyq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9834/Reviewer_dVyq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900436530, "cdate": 1761900436530, "tmdate": 1762921314299, "mdate": 1762921314299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose MetaEmbed which uses grouped multimvector embeddings to scale the \"test-time compute\" used during retrieval. MetaEmbed introduces a tradeoff between resources (computation, memory), and multimodal retrieval quality. The approach is validated on both general multimodal retrieval tasks (MMEB) and document retrieval (ViDoRe)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The manuscript is clearly written and well structured.\n    \n- The introduction of a latency-accuracy tradeoff mechanism into multimodal retrieval is novel and well-motivated.\n    \n- The authors demonstrate the value of MetaEmbed across a good variety of multimodal embeddings tasks (both general and visual documents).\n    \n- The MMEB and ViDoRe results are mostly fair, with the majority of compared works having similar training distributions."}, "weaknesses": {"value": "- The training data has large distribution overlap with evaluation tasks (especially on key tasks, like ImageNet-1K, ViDORe). Therefore, it is unclear how well the model will generalize.\n    \n- It is unclear in tables 1 and 2 which models are trained on MMEB/ViDoRe (and therefore, strongly advantaged). For clarity, it would be valuable to specify which models are in-distribution vs out-of-distribution when reporting performance.\n    \n- Several relevant training settings are not included in appendix A.1, such as Optimizer, optimizer hyperparameters, weight decay, learning rate scheduling, was temperature fixed [1] or trained [2]?\n    \n- The analysis of latency is limited and potentially misleading. How is the index stored? A realistic setup would use an appropriate framework like FAISS [3]. Can the authors provide more details about their inference setup (table 3) and justify its relationship to a realistic deployment?\n    \n- A common use-case would be short text queries and a much larger candidate pool, in which case scoring latency may become significant.\n    \n- The device memory requirements for the best performing group settings are very significant, even for the small candidate pool size. This is both a constraint in itself, and can incur significant performance penalties if the index cannot fit into GPU memory.\n\nMinor:\n\n- The reproducibility statement says: “Implementation and compute details (optimizers, LoRA settings, group sizes, temperatures, hardware) are provided in Appendix A.1.” However, some of these details are found elsewhere, whereas others seem to be missing entirely. Ideally, these settings should all be in A.1, even if it results in some duplication from the main text.\n\n- Potentially relevant related work: [4] seems to corroborate the importance of VLM backbone choice for multimodal retrieval, and may be worth including in section 4.2."}, "questions": {"value": "Some questions can be found in the section above, in addition:\n\n- Can the authors provide a per-task breakdown of MMEB? Although averages over MMEB tasks are often reported, not all tasks in MMEB are equally useful when evaluating a model, therefore a per-task breakdown would be useful.\n   \n- How are the text queries templated into context? Are task-specific system prompts reused between training and evaluation?\n\n- A comparison between the late interaction approach and a single vector that controls for dimension would be interesting (potentially created by concatenating across the sequence dimension).\n\n[1] R. Meng et al., “VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and Visual Documents,” 2025, doi: 10.48550/arxiv.2507.04590.  \n[2] C. Jia et al., “Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision,” 2021, doi: 10.48550/arxiv.2102.05918.  \n[3] M. Douze et al., “THE FAISS LIBRARY,” IEEE transactions on big data, pp. 1–17, 2025, doi: 10.1109/TBDATA.2025.3618474.  \n[4] B. Schneider, F. Kerschbaum, and W. Chen, “ABC: Achieving Better Control of Visual Embeddings using VLLMs,” Transactions on Machine Learning Research, vol. 2025-, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iKCAHAdsqB", "forum": "yKDqg9HwZX", "replyto": "yKDqg9HwZX", "signatures": ["ICLR.cc/2026/Conference/Submission9834/Reviewer_L8o6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9834/Reviewer_L8o6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924165709, "cdate": 1761924165709, "tmdate": 1762921313959, "mdate": 1762921313959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal Revision and General Response to Reviewers"}, "comment": {"value": "Dear Reviewers and ACs, \n\nThank you for taking the time to review our paper! We are glad that all reviewers recognized the strengths of MetaEmbed and **gave positive initial feedback.** \n\nDuring the rebuttal, we focus on addressing concerns that appeared across reviews and update the submission accordingly **with the differences highlighted in BLUE**:\n\n- Generalization and clarity on training data / in-distribution vs out-of-distribution (reviewers L8o6, rgEq).\n  - We emphasize the explicit *OOD* subset in MMEB and reported MetaEmbed’s OOD performance under this stricter protocol. We also clarified that all models in Tables 1 and 2 are trained on the same mixture of MMEB-train + ViDoRe-train.\n- Sensitivity to MMR group sizes and justification of the chosen configuration (reviewers dVyq, rgEq).\n  - We explain the design principles behind the group schedule {(1,1), (2,4), (4,8), (8,16), (16,64)}: exponential growth on the query side and higher capacity on the candidate side. We also added preliminary sensitivity evidence: extending the schedule with an additional large group (32,128) does not improve validation performance but increases training and memory cost, indicating diminishing returns beyond our chosen configuration.\n- Efficiency and scalability considerations for practical deployment.\n  - We clarify the concrete inference setup for Table 3 (PyTorch tensor index + `torch.einsum` scoring), stated that these measurements are a conservative upper bound rather than an optimized production system, and discussed how practical deployments would rely on FAISS or multi-vector-specific indexes plus standard techniques to handle large candidate pools and GPU memory constraints.\n\nWe hope these revisions and clarifications address the shared concerns while strengthening the overall narrative of MetaEmbed as a practical, general, and extensible solution for scalable multimodal retrieval."}}, "id": "IrFRwlgPha", "forum": "yKDqg9HwZX", "replyto": "yKDqg9HwZX", "signatures": ["ICLR.cc/2026/Conference/Submission9834/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9834/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission9834/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763435680928, "cdate": 1763435680928, "tmdate": 1763435680928, "mdate": 1763435680928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}