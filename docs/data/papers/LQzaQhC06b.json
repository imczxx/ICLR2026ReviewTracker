{"id": "LQzaQhC06b", "number": 21975, "cdate": 1758324310150, "mdate": 1759896892924, "content": {"title": "Coded-Smoothing Module: Coding Theory Helps Generalization", "abstract": "We introduce the Coded-Smoothing module, which can be seamlessly integrated into standard training pipelines, both supervised and unsupervised, to regularize learning and improve generalization with minimal computational overhead. In addition, it can be incorporated into the inference pipeline to randomize the model and enhance robustness against adversarial perturbations.\nThe design of coded-smoothing is inspired by general coded computing, a paradigm originally developed to mitigate straggler and adversarial failures in distributed computing by processing linear combinations of the data rather than the raw inputs. Building on this principle, we adapt coded computing to machine learning by designing an efficient and effective regularization mechanism that encourages smoother representations and more generalizable solutions. Extensive experiments on both supervised and unsupervised tasks demonstrate that coded-smoothing consistently improves generalization and achieves state-of-the-art robustness against gradient-based adversarial attacks.", "tldr": "", "keywords": ["Generalization", "Overfitting", "Regularization", "Adversarial Robustness", "Coded Computing", "Covariate Shift", "Spline"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b5ff5c5eda34a5dca41903388516015bdf41364.pdf", "supplementary_material": "/attachment/31fffb7957f5f1c742aac343aab158e7d19e1b69.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a novel training method for improving model generalization, drawing on the concept of coded computing. The approach involves recoding the input data and using it as an auxiliary branch for regularization during training, thereby enhancing the model's generalization capability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed regularization method is both novel and supported by a comprehensive theoretical analysis.\n2.The proposed Coded Smoothing module is designed for flexible integration into both supervised and unsupervised training frameworks, and can be readily reproduced due to its straightforward implementation."}, "weaknesses": {"value": "1. The method description in this paper is thorough, but the experiment appears relatively weak. The classification experiments should be supplemented with results on real-world datasets. Even under computational constraints, validation on a subset such as ImageNet-100[1] should be considered as a minimum requirement.\n2. The generalization performance of the proposed method on OOD data remains insufficiently validated. It would be more convincing to include additional evaluations on specialized OOD benchmarks such as ImageNet-R and ImageNet-S.\n3. The comparison with baseline methods remains relatively limited, as only classical approaches including ERM and Mixup are included. It would be beneficial to incorporate more recent and advanced baselines to better demonstrate the superiority of the proposed method.\n4. The experimental evaluation of the generative model is currently insufficient, as quantitative metrics alone cannot fully capture the perceptual quality of generated samples. It is essential to include qualitative visualizations of the generated outputs. Furthermore, the analysis should at minimum demonstrate the improvement achieved over Mixup when applied in conjunction with WGAN.\n5. The data encoding and decoding process introduces dimensionality expansion to the input. Has there been any analysis on the computational efficiency of this approach, particularly regarding the regularization overhead during training and the additional operations required during inference?\n\n[1] Tian Y, Krishnan D, Isola P. Contrastive multiview coding[C]//European conference on computer vision. Cham: Springer International Publishing, 2020: 776-794."}, "questions": {"value": "I will consider adjusting my score based on the authors' response to these weakness points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4IsmQsuhfh", "forum": "LQzaQhC06b", "replyto": "LQzaQhC06b", "signatures": ["ICLR.cc/2026/Conference/Submission21975/Reviewer_fg2C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21975/Reviewer_fg2C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557031145, "cdate": 1761557031145, "tmdate": 1762942004809, "mdate": 1762942004809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a coded-smoothing module to enhance generalization in image classification.\nInspired by coded computing (Moradi et al., 2024), the authors design a regularization mechanism that promotes smoother feature representations.\nSpecifically, the module uses spline-based encoder and decoder functions: (1) the spline encoder is fitted to a batch of inputs x; (2) the encoded batch is processed by the main network f; and (3) the decoded outputs are compared with the originals to enforce consistency.\nThis process imposes a higher-order smoothness constraint on f, leading to better generalization.\nExperiments are conducted on supervised image classification under in-domain, adversarial, and covariate-shift settings, as well as an unsupervised image-generation task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a novel perspective by adapting coded computing ideas, originally from distributed and information theory, into the context of ML regularization.\n\n- The proposed coded-smoothing module is simple, lightweight, and applicable to both supervised and unsupervised settings.\n\n- The spline-based encoder/decoder enforces higher-order smoothness, providing an interesting theoretical link between coding theory and representation regularization."}, "weaknesses": {"value": "- Adversarial robustness results are weaker than MixUp. Under covariate shift, performance is also not better than Mixup.\n\n- Missing comparisons and discussions with Manifold Mixup and related methods that already smooth latent representations. Table 1 and 3 should include full results of Manifold Mixup.\n\n- Conceptual novelty is limited: the idea closely resembles Latent/Manifold Mixup and other interpolation-based regularizers.\n\n- The inference method uses batch-based encoding/decoding (rather than single-sample). This reduces practicality in scenarios where only one input comes at a time.\n\n- Notation and clarity issues: several functions (e.g., g1, g2) are undefined, making parts of the paper difficult to follow."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P7WvkSI4e4", "forum": "LQzaQhC06b", "replyto": "LQzaQhC06b", "signatures": ["ICLR.cc/2026/Conference/Submission21975/Reviewer_L9et"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21975/Reviewer_L9et"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662524075, "cdate": 1761662524075, "tmdate": 1762942004482, "mdate": 1762942004482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **Coded-Smoothing**, a new regularization module inspired by **coded computing theory**, which traditionally mitigates straggler and adversarial failures in distributed computing by operating on coded (linearly combined) data. The authors adapt this principle to deep learning, proposing a module that encourages **local smoothness** and **better generalization**, while also improving **adversarial robustness**.\n\nThe proposed module operates in three steps:  \n1. **Encoding** — combines a batch of inputs into coded samples via spline interpolation.  \n2. **Computation** — evaluates the model on the coded samples.  \n3. **Decoding** — reconstructs approximate outputs of the original samples and penalizes discrepancies to enforce smoothness.\n\nDuring inference, the authors propose **Randomized Coded Inference (RCI)**, which randomizes the input order before encoding to disrupt gradient-based adversarial attacks (e.g., FGSM, PGD).  \n\nExperiments on CIFAR-10/100, TinyImageNet, and GANs demonstrate that coded-smoothing improves generalization compared to ERM and Mixup, enhances adversarial robustness, and applies effectively in both supervised and unsupervised settings, all with minimal computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel conceptual link:** The paper establishes a creative and original connection between **coding theory** and **regularization in deep learning**, introducing a theoretically motivated way to enforce smoothness.\n\n- **Unified applicability:** The proposed coded-smoothing module works seamlessly for **both supervised and unsupervised** learning settings, unlike Mixup-style approaches that rely on label information.\n\n- **Strong empirical results:** Demonstrates consistent improvements in test accuracy and adversarial robustness across multiple datasets (CIFAR-10/100, TinyImageNet) and architectures.\n\n- **Minimal computational overhead:** The spline-based implementation adds negligible cost to training and inference, making the method practical for large-scale applications.\n\n- **Adversarial robustness:** The **Randomized Coded Inference (RCI)** strategy offers a simple yet effective defense against gradient-based adversarial attacks without requiring adversarial training.\n\n- **Theoretical grounding:** Provides analytical justification (Lemma 1) that links the number of coded samples and function smoothness to the approximation error, offering theoretical intuition for why the method works."}, "weaknesses": {"value": "- **Narrow comparative evaluation:** Experiments mainly compare against ERM and Mixup. Other strong baselines such as CutMix, Manifold Mixup, consistency regularization, or adversarial training are missing.\n\n- **Hyperparameter interpretability:** The effects of key hyperparameters (e.g., the weighting factor µ and the ratio of coded samples N/K) are not clearly analyzed or justified, and practical tuning guidance is lacking.\n\n- **Ablation study depth:** Although appendices mention ablation analyses, the main paper does not clearly quantify trade-offs or sensitivity regarding batch size, coded sample count, or the balance between smoothness and accuracy.\n\n- **Assumption of smoothness transfer:** It is assumed that spline-induced smoothness in the input domain translates to smoother representations in feature space, but this relationship is not empirically validated.\n\n- **Robustness evaluation limitations:** The reported adversarial robustness results do not appear to test **adaptive attacks** (e.g., expectation-over-transformation), which may overestimate the protection offered by RCI."}, "questions": {"value": "1. **Gradient flow and differentiability:**  \n   The paper provides a pseudo-code implementation of the coded-smoothing module but does not clarify how gradients flow through the encoder and decoder (spline fitting) steps.  \n   Are these spline operations treated as differentiable with respect to the network parameters, or are they non-trainable transformations through which gradients do not propagate?  \n   Have you observed any gradient stability issues due to the decoding approximation?\n\n2. **Fixed versus random coding points:**  \n   The encoding and decoding points (Chebyshev nodes) appear to be fixed throughout training.  \n   Have you experimented with randomizing or re-sampling these points during training, or toward the end of training, to act as an additional stochastic regularizer?  \n   Could dynamic or randomized coding points improve generalization or robustness?\n\n3. **Theoretical connection:**  \n   Lemma 1 provides intuition about the approximation error, but it remains unclear how the proposed regularization quantitatively affects model smoothness or generalization.  \n   Can you provide a more formal connection between coded-smoothing and a Lipschitz or higher-order smoothness bound on the function \\( f(\\cdot) \\)?\n\n4. **Adversarial robustness evaluation:**  \n   The reported results demonstrate strong performance under FGSM and PGD attacks.  \n   Have you tested the method against **adaptive attacks** that account for inference-time randomness (e.g., expectation-over-transformation)?  \n   How does the robustness change when such attacks are considered?\n\n5. **Hyperparameter sensitivity:**  \n   How sensitive is performance to the choice of key parameters such as the weighting factor \\( \\mu \\), the ratio \\( N/K \\), and batch size?  \n   Can you offer empirical or theoretical guidance on tuning these parameters?\n\n6. **Selective application:**  \n   The paper applies coded-smoothing to the full network.  \n   Have you explored applying it only to specific layers or blocks to trade off computational cost and regularization strength?\n\n7. **Comparison to other smoothness-based regularizers:**  \n   How does coded-smoothing relate to other smoothness-enforcing techniques like Jacobian regularization, spectral normalization, or consistency regularization?  \n   Could it be complementary to these methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7UiUeYhXxq", "forum": "LQzaQhC06b", "replyto": "LQzaQhC06b", "signatures": ["ICLR.cc/2026/Conference/Submission21975/Reviewer_caW3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21975/Reviewer_caW3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994301346, "cdate": 1761994301346, "tmdate": 1762942004004, "mdate": 1762942004004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a regularization method: coded-smoothing module to regularize deep networks by enforcing local smoothness: a batch is encoded into multiple coded samples via spline/Chebyshev points, passed through a chosen network block, and then decoded to reconstruct the original block outputs; a reconstruction loss is added to the task loss, and at inference a randomized coded inference (RCI) variant permutes batches to disrupt gradient-based attacks; experiments on CIFAR-10/100, TinyImageNet, WGAN-GP (CIFAR-10, CelebA), and distribution shift (CIFAR-10.1/10.2/10C) show modest in-distribution gains over ERM/mixup, improved GAN IS/FID, and notable robustness boosts under FGSM/PGD (though not competitive with strong adversarial training), supported by a lemma that bounds decoding error improving with function smoothness and larger code size."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The idea of reframing coded computing as a data/function-space regularizer is novel and goes beyond pairwise linearity such as mixup and SMOTE.\n* The presentation is clear, provides a clean algorithmic description, a formal error bound for the coded reconstruction, and ablations/sensitivity in appendices.\n* Clarity: the training objective is simple (one mixing coefficient μ) and the module integrates at arbitrary layers without label dependence.\n* Leads to generalization improvements and inference-time robustness via RCI without retraining or adversarial training."}, "weaknesses": {"value": "* The evaluation is limited to vision at CIFAR/TinyImageNet scale, with no ImageNet-1k or transformer/non-vision results, so cross-domain applicability remains uncertain.\n* The run time impact is not reported. The proposed module contains mixed component both within module, in loss function and in data space. Therefore it is important to understand the impact on module latency and complexity. \n* Visual messages in figures are often not clear. For example figure 1 (a) misses the core idea of code computing that enforcing closeness between decoded estimates and true outputs; and in figure 1(b) the decision boundary in both panels appear very similar."}, "questions": {"value": "* Scaling/generalization: how does the method perform on ImageNet-1k and on larger transformers (e.g., ViT/BERT) where sequence batching and attention may interact with coded batches?\n* Hyperparameter guidance: the method introduces various new hyperparameters, what robust default choices of N/K, μ, and spline/Chebyshev order work across datasets, or any recommended tuning strategy?\n* Compute overhead: what are the training/inference time and memory overheads versus ERM/mixup across N and batch sizes, and how does RCI affect latency?\n* Baseline breadth: how does the method compare with CutMix, AugMix, consistency regularization, and post-hoc smoothing/ensembling; any negative interactions?\n* Dropout comparison: when combined with (or compared to) dropout/DropBlock/stochastic depth, are the gains additive, redundant, or conflicting, and at which layers should each be applied?\n* Figure clarity: can you quantify the two-spirals boundary difference (e.g., curvature/total variation metrics, margin maps) to substantiate the qualitative claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RFtyT7P4Ay", "forum": "LQzaQhC06b", "replyto": "LQzaQhC06b", "signatures": ["ICLR.cc/2026/Conference/Submission21975/Reviewer_TRqB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21975/Reviewer_TRqB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998827923, "cdate": 1761998827923, "tmdate": 1762942003754, "mdate": 1762942003754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}