{"id": "88ZLp7xYxw", "number": 15559, "cdate": 1758252664442, "mdate": 1763753331064, "content": {"title": "Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI", "abstract": "Understanding how the brain encodes visual information is a central challenge in neuroscience and machine learning. A promising approach is to reconstruct visual stimuli—essentially images—from functional Magnetic Resonance Imaging (fMRI) signals. This involves two stages: transforming fMRI signals into a latent space and then using a pre-trained generative model to reconstruct images. The reconstruction quality depends on how similar the latent space is to the structure of neural activity and how well the generative model produces images from that space. Yet, it remains unclear which type of latent space best supports this transformation and how it should be organized to represent visual stimuli effectively.\n\nWe present two key findings. First, fMRI signals are more similar to the text space of a language model than to either a vision-based space or a joint text–image space. Second, text representations and the generative model should be adapted to capture the compositional nature of visual stimuli, including objects, their detailed attributes, and relationships. Building on these insights, we propose PRISM, a model that Projects fMRI sIgnals into a Structured text space as an interMediate representation for visual stimuli reconstruction. It includes an object-centric diffusion module that generates images by composing individual objects to reduce object detection errors, and an attribute–relationship search module that automatically identifies key attributes and relationships that best align with the neural activity.\n\nExtensive experiments on real-world datasets demonstrate that our framework outperforms existing methods, achieving up to an 8% reduction in perceptual loss. These results highlight the importance of using structured text as the intermediate space to bridge fMRI signals and image reconstruction.", "tldr": "We present PRISM, a framework to decode visual stimuli from fMRI with language model alignment", "keywords": ["Neuroscience", "Functional Magnetic Resonance Imaging", "Image reconstruction", "Reconstruction"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a773f848f84e5eb499e81b4f34c189008be28842.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper challenges the prevailing assumption in fMRI-to-image reconstruction that the optimal latent space must match the visual modality of the stimulus. Through a systematic comparison, the authors present a key finding: the text embedding space of a language model shows stronger alignment with fMRI signals than vision or joint vision-language spaces. Building on this insight, the paper proposes PRISM, a novel framework that uses a structured text space as the intermediate representation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's central discovery—that fMRI signals align better with a text-based latent space than a vision-based one—is a significant conceptual breakthrough. This challenges a fundamental assumption in the field and provides a new and compelling direction for bridging neuroscience and generative AI.\n\n2. The authors conduct extensive experiments on three distinct fMRI datasets (NSD, BOLD5000, GOD) and show that PRISM consistently outperforms multiple state-of-the-art methods across a wide range of metrics. The inclusion of an image-based QA task further strengthens the claim that the reconstructions are not only visually faithful but also semantically meaningful."}, "weaknesses": {"value": "1. The framework's core design assumes a fixed number of objects (m=2) per image. This is a significant constraint, as real-world scenes can contain one dominant object, or many. The neuroscientific justification based on cognitive load is an interesting hypothesis but also a simplification. This design choice may force the model to hallucinate a second object when only one is present or fail to capture the richness of a more complex scene, limiting its practical applicability and generalizability.\n\n2. The attribute-relationship search module is complex, involving an ɛ-greedy search guided by an LLM, constrained by CKA, and optimized for LPIPS. A critical baseline is missing: comparing PRISM's performance against a simpler approach where the structured descriptions are generated by a state-of-the-art vision-language model (e.g., GPT-4V) using a single, fixed, hand-crafted prompt. This would help isolate the contribution of the elaborate search mechanism itself.\n\n3. While the paper provides an ablation study in Table 8 showing m=2 is optimal, the analysis could be more thorough. For instance, does the optimal number of objects vary with image complexity or content? Providing examples where the m=4 model fails (e.g., by hallucinating objects) would make the argument for m=2 more concrete and visually intuitive.\n\n4. The case study in Table 6 demonstrates a convergence towards \"spatial\" keywords, which is an interesting result. However, the paper could provide a deeper neuroscientific interpretation of this finding. Why are spatial relationships more \"brain-aligned\" for reconstruction than, for example, functional or descriptive attributes? A more detailed discussion connecting these results to known properties of the visual cortex would enhance the paper's interdisciplinary value.\n\n5. The illustration in Figure 1 is not precise enough, making it difficult to intuitively grasp the authors' intended meaning by simply viewing the diagram. For instance, in the \"Attribute/Relationship Search\" section, the authors use only a single looping arrow to denote the complex iterative labeling process. Additionally, in the alignment between the \"Predicted Description\" and \"Structured Description,\" the sentence descriptions in both the left and right parts appear to be the same, which is confusing.\n\n6. While the paper employs extensive engineering optimizations to accurately decode objects and their relationships, it lacks sufficient neuroscientific interpretability. For instance, it does not provide a compelling explanation for its core finding: why brain signals map more strongly to language models than to vision models."}, "questions": {"value": "1. The authors should clearly state in the main text what the Encoder architecture is and which specific Large Language Model (LLM) is used for language decoding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GCkUxozmcK", "forum": "88ZLp7xYxw", "replyto": "88ZLp7xYxw", "signatures": ["ICLR.cc/2026/Conference/Submission15559/Reviewer_TY5P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15559/Reviewer_TY5P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558624382, "cdate": 1761558624382, "tmdate": 1762925834951, "mdate": 1762925834951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "I have finished reading the entire manuscript. This paper focuses on the task of reconstructing the visual images subjects once viewed from fMRI data. Unlike previous research, this paper did not choose the widely selected T2I models for reconstruction, such as SDXL, but instead opted for a class of object-centric T2I models. The authors believe that using object-centric generative models can better reconstruct information about the objects, their positions, and attributes within the visual scene.\n\nAt the methodological level, the authors use a pre-trained VLM to generate object-level structured descriptions for the images. To improve the accuracy of these structured descriptions, the authors designed a prompt optimization method. Subsequently, the authors employ a prompt tuning approach, using fMRI as input to an LLM to predict these structured descriptive languages. Finally, the predicted structured descriptions and the pre-trained object-centric generative model are utilized to reconstruct the seen image.\n\nThe paper's evaluation primarily utilized the NSD dataset, focusing on the accuracy of the synthesized images, and simultaneously included an ablation study of the method.\n\nIn my opinion, the innovation and contribution of this paper lie in the introduction of the object-centric generative model, which could enable more accurate reconstruction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The use of an object-centric generative model for more precise fMRI-to-Image reconstruction is novel.\n\n+ Evaluating the details of the reconstruction using a task similar to VQA (Visual Question Answering) is promising."}, "weaknesses": {"value": "+ Although the authors demonstrate through experiments that aligning fMRI to the text representation space can yield better results, this approach is still unusual or requires further explanation. Based on the setup of the NSD dataset, the fMRI signals used by the authors originate from the nsdgeneral ROI, which is almost entirely the visual cortex of the brain. Therefore, this part of the fMRI data should logically contain only visual information, making the practice of embedding it into a pure text space seem to require further justification.\n\n+ The authors should introduce some more recent baselines for comparison. MindEye2 was published at ICML 2024, and more methods have emerged over the past year or so; the authors should discuss and compare against them.\n\n+ I believe that more case studies should be presented in the experimental section, including the object-level structured descriptions and their corresponding reconstructed images. I think this content would help to better understand the performance and effectiveness of the method."}, "questions": {"value": "+ The baseline methods, such as MindEye and MindEye2, used more reconstruction evaluation metrics, including Alex(2), Alex(5), Eff, and SwAV. Why did the authors omit these evaluation metrics?\n\n+ Based on the results in Table 6, this keyword optimization strategy does not seem to bring about a substantial improvement in the keywords. I believe that further evaluation and explanation are needed here.\n\n+ In the experiment, was the number of object-level structured descriptions, $m$, set to 2? This is a guess I made based on the prompt design on page 21. If so, I am curious whether extracting more objects would help with the reconstruction.\n\n+ According to Formula 4, is the same fMRI input repeated $m$ times to generate structured descriptions for a single image? What is the significance of this design? Alternatively, why is a dedicated MLP set up for each structured description?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nZJCgC7qfO", "forum": "88ZLp7xYxw", "replyto": "88ZLp7xYxw", "signatures": ["ICLR.cc/2026/Conference/Submission15559/Reviewer_jrM3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15559/Reviewer_jrM3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638064937, "cdate": 1761638064937, "tmdate": 1762925834186, "mdate": 1762925834186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRISM, a new framework for reconstructing visual images from fMRI signals. The authors find that fMRI data aligns more closely with structured text representations than with vision-only or joint vision-language spaces. The proposed method first decodes fMRI signals into an object-centric text description, which details objects, their attributes, and their relationships. This structured text then guides a specialized generative model to create the image, a process designed to improve the compositional accuracy of the reconstruction. The paper reports that this approach outperforms existing methods on three standard fMRI datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths include proposing a novel approach that challenges the assumption of using vision-based representations, instead providing evidence for the effectiveness of a structured text space for fMRI decoding. The method introduces specific components, such as an object-centric generative module and an attribute-relationship search, to directly address the known problem of compositional accuracy in image reconstruction. The paper's claims are supported by comprehensive quantitative evaluations across three different public datasets, showing improved performance over existing baseline methods, and are further validated by targeted ablation studies."}, "weaknesses": {"value": "- The paper's central claim that fMRI signals align better with \"text space\" than \"vision space\"  rests on a specific comparison. The \"text space\" is generated using embeddings from image captions, which are already highly semantic, human-generated descriptions of the images. This is contrasted with the latent spaces of vision models (e.g., LDM, ResNet50). Consequently, the finding may be more precisely interpreted as fMRI signals aligning well with semantic, object-centric descriptions of images, rather than with language or text in a more general sense. \n\n- The ablation study in Table 5 evaluates the impact of the \"Attribute-Relationship Search\" module. While the full PRISM model outperforms the \"w/o AttOpt.+Bst\" variant (which uses the best-performing initial keyword) , the performance difference between these two is smaller than the difference between the \"best initial\" and \"worst initial\" keywords. This suggests that while the search module adds value, a significant portion of the performance might be achievable by simply selecting a good, spatially-focused keyword from the initial set."}, "questions": {"value": "- The paper demonstrates that fMRI signals align better with the T5 language model's text space than with the tested vision or joint-latent spaces. How dependent is this key finding on the specific choice of models? For instance, would this alignment hold if compared against different, perhaps more recent or semantically robust, vision-only representations?\n\n- The entire training process for the fMRI-to-text encoder relies on structured descriptions generated by a pre-trained Vision-Language Model (VLM). How do errors, omissions, or inherent biases in this VLM's descriptions (which serve as the \"ground truth\" text) propag\n\n- The object-centric diffusion module is designed to improve compositional accuracy. How does this method scale when reconstructing highly complex scenes containing many objects, significant overlap, or abstract relationships that are inherently difficult to capture in the structured text format?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7t0CngeyRi", "forum": "88ZLp7xYxw", "replyto": "88ZLp7xYxw", "signatures": ["ICLR.cc/2026/Conference/Submission15559/Reviewer_FGs4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15559/Reviewer_FGs4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718481960, "cdate": 1761718481960, "tmdate": 1762925833726, "mdate": 1762925833726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The reconstruction of visual stimuli (images) from functional Magnetic Resonance Imaging (fMRI) signals involves a two-stage pipeline: transforming fMRI signals into a latent space and then using a pre-trained generative model to create the final image. The quality of the final reconstruction is largely determined by the alignment between the latent space and the structure of the neural activity, as well as the generative model's efficiency in producing images from that space. The proposed work demonstrates that accurate visual stimuli reconstruction can be achieved using the text space of a Large Language Model (LLM) instead of image-based latent representations. Furthermore, the reconstruction quality is further improved by object-centric diffusion that generates images by composing individual objects and identifying object attributes and relationships within the neural activity captured by the fMRI signals. Experiments on real-world datasets demonstrated that the proposed PRISM framework outperformed existing methods in CLIP metric by (in absolute terms) by 0-4% and in Inception V3 metric by 1-3%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper makes an interesting observation that fMRI signals exhibit greater similarity to the text space of a language model than to either a vision-based space or a joint text-image space.\n\n2. The pipeline for generating images that captures object relationships in the text space is intuitive.\n\n3. The paper performs better than existing methods on many datasets, thus showing the importance of utilizing a structured text space as the an intermediate bridge between fMRI signals and image reconstruction."}, "weaknesses": {"value": "1. The experimental results need to be clarified. How is the result for MindEye2+SDXL achieved? The original paper itself shows worse performance. The details regarding which NSD subject is used need to be provided.\n\n2. Some other experimental comparisons need to be made:\n[1] Wang, Shizun, et al. “Mindbridge: A cross-subject brain decoding framework.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n[2] Gong, Zixuan, et al. “Mindtuner: Cross-subject visual decoding with visual fingerprint and semantic correction.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 13. 2025.\n\n3. Results on other text spaces such as RoBERTa should be provided."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rnDdJyXGjE", "forum": "88ZLp7xYxw", "replyto": "88ZLp7xYxw", "signatures": ["ICLR.cc/2026/Conference/Submission15559/Reviewer_M3K1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15559/Reviewer_M3K1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950120066, "cdate": 1761950120066, "tmdate": 1762925833194, "mdate": 1762925833194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}