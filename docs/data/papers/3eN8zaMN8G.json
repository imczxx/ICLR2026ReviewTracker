{"id": "3eN8zaMN8G", "number": 10030, "cdate": 1758157124110, "mdate": 1759897679703, "content": {"title": "DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas", "abstract": "Simulating human profiles by instilling personas into large language models (LLMs) is rapidly transforming research in personalization, social simulation, and human-AI alignment. However, most existing synthetic personas remain shallow and simplistic, capturing minimal attributes and failing to reflect the rich complexity and diversity of real human identities. We introduce DeepPersona, a scalable generative engine for synthesizing narrative-complete synthetic personas through a two-stage, taxonomy-guided method. First, we algorithmically construct the largest-ever human-attribute taxonomy, comprising over hundreds of hierarchically-organized attributes, by systematically mining thousands of real user-ChatGPT conversations. Second, we progressively sample attributes from this taxonomy, conditionally generating coherent and realistic personas, averaging hundreds of structured attributes and roughly 1 MB of narrative text, two orders of magnitude deeper than prior works. Intrinsic evaluations confirm significant improvements in attribute diversity (32% higher coverage) and profile uniqueness (44% greater) compared to state-of-the-art baselines. Extrinsically, our personas enhance GPT-4.1-mini’s personalized Q&A accuracy by 11.6% average on ten metrics, and substantially narrow (by 32%) the gap between simulated LLM ``citizens'' and authentic human responses in social surveys. DeepPersona thus provides a rigorous, scalable, and privacy-free platform for high-fidelity human simulation and personalized AI research.", "tldr": "", "keywords": ["Synthetic Data Generation", "Synthetic Personas", "Persona Generation", "Human Simulation", "LLM Personalization", "Social Simulation", "Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/707a1bdcaef1c0a9c946c4b5a09bde857870c215.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces DeepPersona, a two-stage generative engine that synthesizes detailed, diverse, and customizable synthetic persona data. The authors first construct the largest human-attribute taxonomy to date by mining and filtering self-disclosure content from human-LLM interactions. They then employ a progressive attribute sampling approach that iteratively selects diverse attributes and conditions a large language model to generate coherent values and narrative text."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The motivation is clear and important, pinpointing the problem of \"persona depth\" in previous persona generation approaches.\n- The method the authors use to extract is systematic and thoughtful. \n- Evaluation is done extensively in a multi-faceted manner, ranging from four different downstream tasks.\n- Experiments are conducted on many frontier AI models from different sources, further supporting the generality of this work.\n- Human experiments are included to complement the possible concerns regarding the instability of LLM judges.\n- Most importantly, this work provides a scalable platform for synthetic persona generation, which I think is a significant contribution to the community."}, "weaknesses": {"value": "I did not spot any significant weaknesses in this paper. One minor regret would be that qualitative examples are limited. It would be great to see qualitative comparisons between previous approaches and DeepPersona.\n\nAlso, this is minor, but there are some formatting issues on page 23. Please amend the overflow issue."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3Y6DR4uoPj", "forum": "3eN8zaMN8G", "replyto": "3eN8zaMN8G", "signatures": ["ICLR.cc/2026/Conference/Submission10030/Reviewer_rXfh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10030/Reviewer_rXfh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761695708967, "cdate": 1761695708967, "tmdate": 1762921437819, "mdate": 1762921437819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a taxonomy-guided framework that generates synthetic personas by sampling from a human-attribute tree. It addresses the deep and coherent persona generation from existing work. Experiments show higher personalization quality and closer alignment to human distributions on World Values Survey and Big Five benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The synthetic persona generation problem is interesting and timely.\n- They provide a large-scale taxonomy of human attributes, which is really beneficial for the literature.\n- The experiments are good, showing the advantages of the generated synthetic persona.\n- The authors provide a method to diversify selected attributes."}, "weaknesses": {"value": "- The method is naive. They did break the sampling procedure into two stages (sampling attributes from the taxonomy first and then sampling values from the given attributes), but they are heavily manually engineered.\n- Generally, it seems to be a neat paper and can bring benefits to the community, but the novelty is limited. It would be more appreciated if this paper were submitted to the benchmark and dataset tracks instead of the main tracks."}, "questions": {"value": "- Can we have a learnable way to learn the selector and generator so that it generates personas towards a chosen population? like these methods: PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning, ICML24 and Mixture-of-Personas Language Models for Population Simulation, ACL25.\n- Why do we set the ratio to 5 : 3 : 2 ratio? Will any other combination work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NQJCTqTu3c", "forum": "3eN8zaMN8G", "replyto": "3eN8zaMN8G", "signatures": ["ICLR.cc/2026/Conference/Submission10030/Reviewer_8nFh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10030/Reviewer_8nFh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977524446, "cdate": 1761977524446, "tmdate": 1762921437550, "mdate": 1762921437550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DEEPPERSONA, a two-stage generative framework for creating synthetic personas."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I cannot find any strength or contribution in the current manuscript of the paper."}, "weaknesses": {"value": "The writing is misleading and difficult to follow, suggesting that the paper is not yet in a finished state.\n\nIn the abstract, the authors claim to be “mining thousands of real user–ChatGPT conversations.” However, Section 3 shows that no new data were mined; instead, the work relies entirely on existing datasets (Puffin, prefeval_implicit_persona). This inconsistency significantly weakens the claimed novelty.\n\nAt the start of Section 3, several important concepts are introduced without any explanation. Terms such as “text mass Narr(P)” and “persona/attribute depth” appear multiple times but are never defined or justified. Similarly, the choice of parameters—such as enforcing k > 10²—is arbitrary and unsupported by analysis or intuition. The authors should explain what k represents, why that threshold was chosen, and how it affects outcomes.\n\nThe paper also claims to contribute a dataset or toolkit (Section 3.3), but none of these resources—toolkit, evaluation scripts, or datasets—are publicly accessible. Without open access, the community cannot verify the claims, replicate the results, or assess the contribution’s practical value. As presented, the work lacks transparency and reproducibility."}, "questions": {"value": "- Are the chosen datasets (Puffin, prefeval_implicit_persona, HiCUPID) demographically balanced?\n\n- How was GPT-4.1-mini’s classification validated? Was any human verification or inter-annotator agreement performed? Were disagreements between GPT-4.1-mini and human judgments analyzed or resolved? If there is human evaluation, what are their background?\n\n- Why is the 5:3:2 sampling ratio (near:middle:far attributes) considered optimal? No ablation study or justification is provided.\n\n- What criteria or heuristics determine the depth budget (k)? How sensitive are the results to this parameter?\n\n- How does the model prevent contradictions among attributes generated across different stages of progressive filling? Could the random traversal introduce bias or unrealistic attribute combinations that rarely occur in real human populations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PU6FaEA1vY", "forum": "3eN8zaMN8G", "replyto": "3eN8zaMN8G", "signatures": ["ICLR.cc/2026/Conference/Submission10030/Reviewer_ERkS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10030/Reviewer_ERkS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001690625, "cdate": 1762001690625, "tmdate": 1762921437289, "mdate": 1762921437289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}