{"id": "KFSv2egats", "number": 23464, "cdate": 1758344139635, "mdate": 1763235061166, "content": {"title": "Learning to Attribute with Attention", "abstract": "Given a sequence of tokens generated by a language model, we may want to identify the preceding tokens that *influence* the model to generate this sequence. Performing such *token attribution* is expensive; a common approach is to ablate preceding tokens and directly measure their effects. To reduce the cost of token attribution, we revisit attention weights as a heuristic for how a language model uses previous tokens. Naive approaches to attribute model behavior with attention (e.g., averaging attention weights across attention heads to estimate a token's influence) have been found to be unreliable. To attain faithful attributions, we propose treating the attention weights of different attention heads as *features*. This way, we can *learn* how to effectively leverage attention weights for attribution (using signal from ablations). Our resulting method, Attribution with Attention (AT2), reliably performs on par with approaches that involve many ablations, while being significantly more efficient.", "tldr": "We learn to pinpoint the in-context information that a language model uses when generating content, using attention weights as features.", "keywords": ["attribution", "generative models", "large language models"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b75537155fe347048818d31c90167c4e722fa4dc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "AT2 learns per-head coefficients over attention weights to predict how ablating prior tokens would change a model’s output, then uses those weights for fast attribution on new examples. Empirically, it matches example-specific ablation surrogates while beating gradient and average-attention baselines, and is far more efficient at inference time"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Simple, generalizable signal: Treats attention heads as features and learns how much to trust each head, aligning with evidence that heads differ functionally. \n\n* Strong efficiency: AT2 is >2× faster than one forward pass and ~8× faster than one backward pass; concrete timings are reported on Llama-3.1-8B/HotpotQA. \n\n* Competitive accuracy: Performs comparably to example-specific surrogate modeling and better than gradient/average-attention across context and thought attribution."}, "weaknesses": {"value": "* Although per-head learning helps, attention remains a debated proxy for explanation. Also AT2 assumes the effect of ablating sources is a linear, additive function and then learns a single, input-independent set of head coefficients to predict it from attention weights. This collapses inherently non-linear, context-dependent interactions between tokens and heads into fixed, first-order features.\n\n* Lacks thorough comparisons to contemporary attribution method like Oren Barkan, Yonatan Toib, Yehonatan Elisha, Jonathan Weill, and Noam Koenigstein. Llm ex- plainability via attributive masking learning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 9522–9537, 2024. Please see the recent works on attribution methods and compare with them how your method performs.\n\n* How AT2 behaves when applied to larger models above 10B and how the computation time varies."}, "questions": {"value": "same as above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jKiKB12NrX", "forum": "KFSv2egats", "replyto": "KFSv2egats", "signatures": ["ICLR.cc/2026/Conference/Submission23464/Reviewer_Y4X7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23464/Reviewer_Y4X7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627628189, "cdate": 1761627628189, "tmdate": 1762942672172, "mdate": 1762942672172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "qq4DrG7YhL", "forum": "KFSv2egats", "replyto": "KFSv2egats", "signatures": ["ICLR.cc/2026/Conference/Submission23464/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23464/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763235060264, "cdate": 1763235060264, "tmdate": 1763235060264, "mdate": 1763235060264, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AT2 (Learning to Attribute with Attention), a method for efficient and faithful token-level attribution in large language models. AT2 is motivated by the observation that attention heads vary in their usefulness for attribution—some correlate closely with causal influence, while others do not. \n\nThe method trains a global surrogate that learns head-specific coefficients to linearly weight attention distributions across layers, approximating the log-probability change induced by token ablations. Once trained, the surrogate provides attribution for new examples without performing additional ablations or gradient computations, achieving near-ablation faithfulness while being significantly faster. Experiments on context and thought attribution tasks using Phi-3.5-Mini, Llama-3.1-8B, and DeepSeek-R1-Qwen-7B demonstrate that AT2 generalizes across models and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear motivation and conceptual framing:** The paper clearly identifies that not all attention heads contribute equally to causal influence, and formulates attribution as learning to weight heads by their usefulness.\n    \n2. **Practical efficiency:** AT2 amortizes ablation-based attribution, providing faithful approximations with significant speedup.\n\n3. **Faithful yet simple formulation:** The linear surrogate is interpretable, easy to train, and applicable to existing transformer architectures without modification."}, "weaknesses": {"value": "1. **Head redundancy and insufficient analysis:** Many attention heads are highly correlated, yet the paper provides no analysis of how this redundancy affects the surrogate’s performance.\n\n2. **Lack of surrogate architecture analysis:** The paper demonstrates that the linear surrogate works but does not justify _why_ this specific structure is optimal compared to non-linear or hierarchical alternatives.\n\n3. **Architecture sensitivity untested:** AT2’s features depend directly on the target model’s attention topology (number of heads, layer depth, sparsity). The paper does not study whether the method generalizes across models with distinct attention configurations."}, "questions": {"value": "**Questions for the Authors**\n\n1.  How stable are the learned coefficients across random seeds or training subsets?  Do the same heads consistently emerge as useful for attribution?\n\n2. Have you considered comparing AT2 with attribution methods that use gradient-weighted attention features? It would be valuable to include comparisons with Attention × Grad variants such as Att×Att-Grad and Grad-SAM [1].\n\n3. How sensitive is AT2’s performance to correlated or redundant attention heads?\n\n4. How does AT2 behave under models with different attention architectures (e.g., sparse, grouped, or multi-query attention)?\n\n[1] Oren Barkan et al., Grad-sam: Explaining transformers via gradient self-attention maps, In CIKM, 2021\n\n**Additional Suggestions**\n\n1. The notation in Algorithm 1 could be clarified and made more consistent with the equations in the main text. This would improve readability and alignment between the pseudo-code and the mathematical formulation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v6noqyB4Ss", "forum": "KFSv2egats", "replyto": "KFSv2egats", "signatures": ["ICLR.cc/2026/Conference/Submission23464/Reviewer_FLah"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23464/Reviewer_FLah"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635509243, "cdate": 1761635509243, "tmdate": 1762942671896, "mdate": 1762942671896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AT2 first learns how much to trust each attention head, treating the heads like features, and fits a simple linear predictor that estimates how the model’s output would change if you removed earlier tokens. After this one-time training, it reuses the same learned weights to produce fast attributions on new inputs. In experiments, it performs about as well as per-example ablation surrogates and better than gradient or average-attention baselines, while being very efficient: it takes less than half the time of a single forward pass and roughly one-eighth the time of a backward pass on Llama-3.1-8B with HotpotQA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Treats attention heads as features and learns per-head trust; aligns with evidence that heads differ functionally. \n\nStrong efficiency. AT2 is >2× faster than one forward and ~8× faster than one backward pass on Llama-3.1-8B/HotpotQA (timings reported). \n\nCompetitive accuracy. Comparable to example-specific surrogate modeling (ESM) and better than gradient and average-attention across context and thought attribution."}, "weaknesses": {"value": "* Attention as explanation is debated. Prior work questions attention’s reliability, so per-head learning helps but doesn’t settle the proxy issue. \n\n* AT2 assumes additive effects of source ablations and learns a single, input-independent matrix θ; this can miss non-linear, context-dependent interactions between tokens and heads. \n\n* Empirical comparisons cover ESM, gradient, and average attention but several recent attribution methods are not included as baselines. \n\n* Experiments target ≤8B models but comparing the behavior and timings for larger models will increase the reliability of the method."}, "questions": {"value": "Please see the weakness as questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VaGbqWsm7A", "forum": "KFSv2egats", "replyto": "KFSv2egats", "signatures": ["ICLR.cc/2026/Conference/Submission23464/Reviewer_h1vT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23464/Reviewer_h1vT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775871773, "cdate": 1761775871773, "tmdate": 1762942671634, "mdate": 1762942671634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AT2, an amortized attribution method that learns global per-head attention weights using a small set of ablation-supervised examples, then applies these weights at test time to score source tokens/sentences. It evaluates on context attribution and “thought attribution,” comparing to example-specific surrogates, attention averaging, and gradient norms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The problem setup is very clear, the paper is well written.\n- Amortizing per-example surrogates with shared parameters is practical.\n- Has empirical gains over naïve baseline and has much lower test-time cost."}, "weaknesses": {"value": "- Train a higher-level (amortized) surrogate to mimic ablation effects using attention-head features feels like a straightforward application of standard amortization/linear-surrogate ideas. The trade-off (speed vs. small fidelity drop) is unsurprising and largely incremental.\n- A method can be simple, but then there should be more insight on why and how it works. At the moment, the paper reads as an engineering tweak rather than yielding interpretability insights.\n- Results mostly compare a few operating points, it’s hard to see a compelling “dominant region” where AT2 is the obvious choice. Maybe show some kind of compute-normalized Pareto, e.g., plotting performance vs. test-time FLOPs/latency across baselines."}, "questions": {"value": "- When AT2 fails vs. ESM, maybe analyze why and what happened (head misweighting? insufficient feature expressivity?).\n- Is it possible to plot some Pareto curve across methods, maybe at some fixed test-time budgets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MAuOqLgkfi", "forum": "KFSv2egats", "replyto": "KFSv2egats", "signatures": ["ICLR.cc/2026/Conference/Submission23464/Reviewer_AaoN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23464/Reviewer_AaoN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835128563, "cdate": 1761835128563, "tmdate": 1762942671292, "mdate": 1762942671292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new token attribution method AT2. It assigns a learnable coefficient to each attention head, and the attribution score for an input token is the sum of attention scores from different attention heads weighted by their coefficient. The method is evaluated on context attribution and thought attribution. The results show that AT2 outperforms other methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method is well-motivated and clearly presented.\nWhile the training is expensive, the cost of the attribution at inference time is negligible.\nThe experimental results show that the method is effective and efficient."}, "weaknesses": {"value": "The major problem I am concerned about is the rationalisation of measuring the attribution/influence 'ground truth' by ablating the input: 1) the corrupt sequence may change the semantic meaning and result in different attention behaviours, and 2) this method is good at seeing what one token does on its own, but it may struggle with teamwork, e.g., he influence of one token might be entirely dependent on the presence of another."}, "questions": {"value": "It would be easy to train different non-linear models after collecting the datasets of full vs ablated attention scores. Do you try different model designs, though the linear model has the best efficiency"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EQpsaiQGz5", "forum": "KFSv2egats", "replyto": "KFSv2egats", "signatures": ["ICLR.cc/2026/Conference/Submission23464/Reviewer_thPH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23464/Reviewer_thPH"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973007418, "cdate": 1761973007418, "tmdate": 1762942670894, "mdate": 1762942670894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}