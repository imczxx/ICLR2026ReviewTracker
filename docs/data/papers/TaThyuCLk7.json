{"id": "TaThyuCLk7", "number": 12376, "cdate": 1758207412356, "mdate": 1763708881150, "content": {"title": "DRDFL: Divide-and-conquer Collaboration for Efficient Ring-topology Decentralized Federated Learning", "abstract": "Federated learning traditionally relies on server-based architecture, which often incur high communication costs and suffer from single points of failure. To avoid these limitations, we explore Ring-topology Decentralized Federated Learning \n(RDFL), a fully decentralized paradigm that enables peer-to-peer training. However, the inherent challenge of data heterogeneity is further amplified in RDFL due to limited communication bandwidth cross clients and the sparse connectivity of the ring topology. In this paper, we propose the Divide-and-conquer collaboration RDFL framework (DRDFL), which captures underlying data patterns by jointly learning personalized and invariant knowledge through two complementary modules with distinct optimization objectives.\nSpecifically, each client trains a transferable *Learngene* module via adversarial optimization against a uniform label distribution to learn consensus knowledge, thereby mitigating label distribution skew induced by data heterogeneity.\nTo simultaneously alleviate feature distribution skew, a personalized *PersonaNet* module is introduced that models local features using a Gaussian mixture distribution and updates them based on the global class representation.\nClients only share lightweight *Learngene* and global representations with a directed neighbor, which guarantees flexible choices for resource efficiency and better convergence.\nExtensive experiments show that our method achieves superior performance in RDFL while reducing the communication cost to only 0.58 M, which is more than two orders of magnitude lower than the state-of-the-art baseline. This substantial reduction highlights the effectiveness of our approach in addressing data heterogeneity under stringent communication constraints.", "tldr": "", "keywords": ["Federated Learning", "Label distribution skew", "Feature distribution skew", "Learngene"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f125941f1c511733525605581147bacabe1630e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DRDFL, a divide-and-conquer framework for Ring-topology Decentralized Federated Learning. The method aims to tackle data heterogeneity and sparse communication challenges that arise in peer-to-peer FL systems without a central server. DRDFL introduces two modules: 1) a transferable Learngene module to encapsulate consensus knowledge addressing the label distribution skew. 2) a local PersonaNet module to mitigate feature distribution skew for local personalized feature modeling via Gaussian mixtures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Decentralized federated learning is a timely and important research direction, as it mitigates several inherent limitations of centralized FL most notably the single point of failure and heavy reliance on a central coordinator. Addressing data heterogeneity among clients is also a key open challenge in this area, and this work takes a step toward that goal.\n\n2) The paper is well written and is supported by good number of experiments."}, "weaknesses": {"value": "1) While the paper motivates the ring topology as a means to improve communication efficiency and remove reliance on a central server, the justification for specifically adopting a ring structure remains somewhat unconvincing. For instance, the introduction mentions vehicle-to-vehicle networks as a potential application. However, in such dynamic environments, client participation is highly transient where vehicles continuously enter and leave the network. It is therefore unclear how the proposed framework would ensure stable participation long enough to complete a full and have several communication cycles around the ring. In practice, decentralized systems could still achieve low communication cost through peer-to-peer communication within dynamic local neighborhoods rather than enforcing a fixed ring structure. Clarifying why a strict ring topology is chosen over more flexible peer-to-peer connectivity, and how DRDFL handles client disconnection in such dynamic scenarios, would strengthen the motivation and applicability of the approach.\n\n2) All reported experiments appear to assume 100% client participation in each communication round within the ring topology. It would be valuable to understand how DRDFL performs under partial client participation. For instance, with 50 total clients, one could randomly sample small subsets (e.g., 5 clients) to form temporary rings per round, with different subsets participating in subsequent iterations. \n\n3) The paper evaluates DRDFL under moderate non-IID settings, but it remains unclear how the method would perform in extreme heterogeneity scenarios, specifically where each client possesses a distinct set of class labels (i.e., no label overlap).\n\n4) How would DRDFL behave under a linear (path) topology, e.g., A → B → C → D, where no cycle exists? In a directed path, upstream clients (e.g., A) never receive updated Learngene or class statistics from downstream nodes, so consensus information cannot propagate back.\n\n5) The evaluation is restricted to ResNet-18 and small datasets, with all clients participating every round. This leaves an open question, how DRDFL scales to larger models and datasets. Expand the study with larger models and datasets. For example use ViT-B/16 and TinyImageNet.\n\n6) The proposed framework requires each client to communicate class statistics Learngene optimization. This will not have privacy implications behind sharing class distributions of clients?\n\n7) For fairness, all baselines and DRDFL should be compared under same computation cost. The authors should report figures where the x-axis is the computation cost, and accuracy on the y-axis.\n\n8) All experiments were performed under a homogeneous model architecture (ResNet-18) across clients, yet the comparison set omits decentralized model-averaging baselines that also use re-Weighted SoftMax (WSM) cross-entropy [1] during training mechanisms to mitigate data heterogeneity. Prior work such as DFML [2] have used such setup as a baseline. Compare DRDRL with WSM in decentralized FL setting.\n\n[1] Legate et al. \"Re-weighted softmax cross-entropy to control forgetting in federated learning\". PMLR, 2023.\n\n[2] Khalil et al. \"DFML: Decentralized Federated Mutual Learning.\" TMLR 2024."}, "questions": {"value": "Please address concerns in Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YYVacfSggs", "forum": "TaThyuCLk7", "replyto": "TaThyuCLk7", "signatures": ["ICLR.cc/2026/Conference/Submission12376/Reviewer_LsAK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12376/Reviewer_LsAK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760796068452, "cdate": 1760796068452, "tmdate": 1762923280690, "mdate": 1762923280690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Questions"}, "comment": {"value": "> CQ1:  Reasons for choosing ring topology.\n\nThank you for the insightful comment. We would like to emphasize that the proposed methodology is not intrinsically tied to a ring structure. As detailed in Appendix B.2.7, we have explicitly extended and validated our approach under alternative and dynamically varying topologies. The experimental results confirm that our method remains effective and feasible across these more flexible connectivity patterns, addressing the concern that a fixed topology may be impractical.\n\nThe primary reason for presenting our method within a ring topology in the main manuscript is its simplicity and representational clarity. As one of the most basic connected topologies, the ring provides a stringent and transparent testbed for evaluating the core innovation of our work: the efficient propagation and consolidation of “consensus knowledge’’ under minimal communication. Demonstrating strong performance in such a sparse and high-diameter topology offers compelling evidence that the method can generalize and even perform better in richer peer-to-peer connectivity settings.\n\nMoreover, as discussed in Appendix B.2.6, DRDFL naturally supports newly joined clients through an efficient initialization mechanism, ensuring seamless participation even under client churn. Likewise, transient disconnections do not disrupt the algorithm, as the *Learngene* is inherited only from direct neighbors rather than relying on the entire ring being intact. In practical scenarios such as vehicle-to-vehicle networks, the system can exploit latency buffers to detect departures and reconfigure the ring among active participants, enabling continuous operation despite dynamic membership.\n\n> CQ2: How sensitive is the method to the hyperparameters controlling the balance between generalization and personalization losses?\n\nThank you for raising this important point. We would like to clarify that the generalization loss and personalization loss are both fundamental components of our framework, and preliminary experiments showed that they operate at similar scales. Therefore, we adopted the same default hyperparameter values in the main experiments.\n\nTo empirically assess the sensitivity, we conducted a 3×3 grid search over the two loss weights $L_{{GL}} \\in$ $[0.5, 1, 2]$ and $L_{{PR}}$ $\\in [0.5, 1, 2]$ under the CIFAR-100 dataset with ( $\\beta$ = 0.1). Across all nine combinations, the performance remains highly stable, ranging from 91.42% to 92.86%, a fluctuation of **less than 1.5 percentage points**. This demonstrates that DRDFL is not sensitive to the precise choice of these hyperparameters, and a single default setting ($L_{GL} = L_{PR} =1$ ​) already achieves near-optimal performance without additional tuning. We will incorporate these results into the revised manuscript to ensure completeness and clarity.\n\n| $L_{GL}$ / $L_{PR}$ | 0.5  | 1|2\n| --------- | --------- | --------- |--------- |\n| 0.5|  91.71%| 92.09%|91.57%\n| 1|  92.77%|  92.86%|91.80%\n| 2|91.89%| 92.38%|91.42%"}}, "id": "FMMLuz9BUw", "forum": "TaThyuCLk7", "replyto": "TaThyuCLk7", "signatures": ["ICLR.cc/2026/Conference/Submission12376/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12376/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12376/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763707989308, "cdate": 1763707989308, "tmdate": 1763707989308, "mdate": 1763707989308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DRDFL, a novel framework for Decentralized Federated Learning (DFL) on a ring topology. The core idea is to tackle data heterogeneity by decoupling the model into two components: a personalized PersonaNet module and a consensus Learngene module. The PersonaNet is trained to capture client-specific feature distributions using a Gaussian mixture model. The Learngene, which is the only component communicated between clients, is trained to learn class-invariant, generalizable knowledge through an adversarial optimization process against a uniform label distribution. The authors claim that this \"divide-and-conquer\" approach achieves superior personalization and generalization performance with very low communication overhead (0.58M parameters) in a ring-topology setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed architecture, which combines a VAE-like structure with a dual-branch encoder for personalization (PersonaNet) and generalization (Learngene), is novel in the context of DFL. The specific technique of using an adversarial classifier trained towards a uniform distribution to enforce class-invariance in the Learngene module is a clever and interesting contribution.\n\n2. The paper addresses a highly relevant and challenging problem: efficient and effective DFL under sparse communication topologies (ring) and severe data heterogeneity. A method that can achieve strong performance with minimal communication cost would be a significant advancement for practical peer-to-peer applications like collaborative autonomous driving or edge IoT systems."}, "weaknesses": {"value": "1. The proposed DRDFL framework is exceedingly complex. It involves a VAE-like structure with two separate encoders, a decoder, a primary classifier, and an adversarial classifier. The optimization involves minimizing a combination of at least seven different loss terms: reconstruction loss ($L_{rec}$), classification loss on original and noised data ($L_{ce}$), two losses for PersonaNet ($L_{cls}, L_{log}$), and three losses for Learngene ($L_{kl}, L_{adv}, L^u_{adv}$). This complexity raises serious questions about the method's practicality, stability, and sensitivity to hyperparameter tuning. The ablation study in Table 5 is insufficient as it only removes entire loss groups (L_PR or L_GL), failing to justify the necessity of each individual component.\n\n2. The convergence analysis provided in Appendix A.2-A.3, while a good effort, feels generic and disconnected from the core novelties of the method. Specifically, Assumption 3, which bounds the parameter variation of the Learngene module ($||\\tilde{\\phi}^t - \\phi^t_m||^2_2 \\leq \\delta^2$), is a very strong and potentially unrealistic assumption. In a highly non-IID setting, the Learngene received from a neighbor with a drastically different data distribution could be very far from the client's current version, making $\\delta^2$ large and rendering the convergence bound vacuous. The analysis does not seem to capture the dynamics of the adversarial training or the VAE objective, which are central to the proposed method.\n\n3. The paper champions the ring topology for its communication efficiency but largely ignores its critical drawbacks. The primary issue is the slow information mixing time, which is linear in the number of clients ($O(M)$). Information from one client takes $M-1$ rounds to reach its other neighbor. This could severely hamper convergence speed in large-scale networks (e.g., M > 100), a limitation not explored in the experiments (which use M=20 and M=50). More importantly, the paper dismisses the critical issue of node failure as \"future work\". In a ring, a single disconnected client breaks the entire communication loop, halting the training process. This is a major practical failure point that undermines the viability of the proposed approach for real-world systems."}, "questions": {"value": "1. The system is a complex amalgamation of multiple techniques (VAE, GMM, adversarial learning, noisy reconstruction). Could the authors provide a more detailed ablation study to justify the inclusion of each component? For instance, what is the performance impact of removing the noisy reconstruction, or using only the KL-divergence for the Learngene without the adversarial component? How sensitive is the model to the relative weighting of the numerous loss terms?\n\n2. Regarding Assumption 3 in your convergence proof, how do you justify that the norm difference between the received and local Learngene module is bounded by a small constant $\\delta^2$, especially in the early stages of training or under extreme label skew where neighboring clients might have completely disjoint class sets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7AOCXAZR2X", "forum": "TaThyuCLk7", "replyto": "TaThyuCLk7", "signatures": ["ICLR.cc/2026/Conference/Submission12376/Reviewer_E5sm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12376/Reviewer_E5sm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925108487, "cdate": 1761925108487, "tmdate": 1762923280309, "mdate": 1762923280309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces DRDFL a framework for decentralized federated learning under ring topology communication. The method decomposes learning into two modules : i) Learngene that is trained adversarially under a uniform label distribution to capture global, class invariant knowledge helping with label skew; and ii) PersonaNet a personal network based on Gaussian mixture distributions to learn and preserve client specific features and alleviate feature distribution skew. Clients share Learngene parameters and Gaussian statistics with neighbors to enable peer to peer learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The separation of the personalization and generalization concerns by having two separate modules for the peer to peer learning setting is new.\n2. The use of adversarial training with uniform priors for Learngene and Gaussian mixture modeling for PersonaNet seems to effectively bridge the personalized representation learning with federated optimization.\n3. The experiments on multiple datasets show superior performance of the proposed algorithm."}, "weaknesses": {"value": "1. While the idea is novel for the peer to peer learning network, similar ideas have existed in the FL setups with access to a central server. \n2. The paper is somewhat hard to follow and the writing could better describe the intuition of each component and design choices, for example how the adversarial uniform constraints achieve global invariance, etc.\n3. If and how these two modules interact with each other is under discussed."}, "questions": {"value": "1. How sensitive is the method to the hyperparameters controlling the balance between generalization and personalization losses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kMecHcjneY", "forum": "TaThyuCLk7", "replyto": "TaThyuCLk7", "signatures": ["ICLR.cc/2026/Conference/Submission12376/Reviewer_tDu8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12376/Reviewer_tDu8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931593763, "cdate": 1761931593763, "tmdate": 1762923280041, "mdate": 1762923280041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors present DRDFL, a method for personalized, decentralized federated learning that aims to improve personal model performance via two modules. The paper decomposes representation learning into a private PersonaNet module to fix feature skew and a shared, adversarially-trained Learngene module to fix label skew. Their latents are combined for classification, while only the Learngene module and global Gaussian means and variances are communicated between clients. The paper compares their method against various SOTA baselines in the literature and across different heterogeneity measures. In terms of the paper’s strengths, they demonstrate that their method improves on or is comparable to the accuracy of other methods in most experiments. Furthermore, their method achieves these improvements with a much lower communication overhead between clients. However, the paper would benefit from a clearer motivation and definition of the personalized FL setting, as well as the clarification of minor details in the questions section."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Overall, the paper is well-written, thorough, and generally persuasive.\n\n\n* The proposed method demonstrates superior or comparable performance to existing baselines from the literature when evaluated on both global and local test accuracy across a range of heterogeneity conditions.\n\n\n* The approach achieves roughly two orders of magnitude lower communication cost in terms of transmitted parameters compared to other centralized and decentralized federated learning methods.\n\n\n* The authors also provide very extensive ablation studies and supplementary analyses that answered most questions that I had about the work. These include\n   * Alternative communication topologies\n   * Computational overhead of DRDFL\n   * Convergence behavior\n   * Robustness under gradient reconstruction attacks\n   * Effects of removing individual loss components\n   * Performance with an increased number of clients\n   * Scalability to newly joined clients"}, "weaknesses": {"value": "* The first claimed contribution of the paper is the delineation of heterogeneity into feature skew and label skew. However, this distinction is already well-established in the federated learning literature (e.g., [1]) and therefore should not be presented as a novel contribution.\n\n\n* The motivation for the personalized DFL objective is also insufficiently articulated. Additionally, it is unclear whether the Dirichlet-$\\beta$ and shard-based partitioning schemes used in the experiments meaningfully represent realistic personalization scenarios. From an intuitive perspective, assessing model personalization with the dirichlet partition over labels would encourage client models to learn solutions heavily biased in favor of the local label skew with poor generalization quality. While this limitation may reflect broader challenges within the personalized FL subfield and extend beyond the scope of this particular work, the paper would nonetheless benefit from a more explicit discussion of these conceptual issues."}, "questions": {"value": "* The personalization objective should be defined more clearly and introduced earlier in the paper. This objective represents a key distinction from works such as DFedAvg, where the aim is to jointly train a single global model. While the authors implicitly reference this objective through the use of Local-T and Global-T curves, it would strengthen the manuscript to articulate it explicitly in the early sections.\n\n\n* Instead of reporting “0.58 M of communication” as a standalone statistic in the abstract and contributions section, it would be more informative to contextualize this value by comparing it to state-of-the-art baselines from the start (e.g., indicating that the method transmits an order of magnitude fewer parameters). The communication cost is relative to the size of the backbone classifier, so the sole number is not very explanative on its own.\n\n\n* The manuscript should specify which backbone classifier is used across all methods, as this detail is necessary for interpreting performance and communication comparisons.\n\n\n* The qualitative analysis using Grad-CAM is an interesting direction. However, to substantiate the claims about what is learned by PersonaNet and Learngene, the authors should include at least a few more visual examples in the Appendix to better justify their interpretations. It is not clear whether the image presented is representative of the majority of the images.\n\n\n* Why is the work limited to the ring topology? It is unclear why the method cannot be used on any DFL topology.\n\n\nReferences\n\n[1] J. Pei, W. Liu, J. Li, L. Wang and C. Liu, \"A Review of Federated Learning Methods in Heterogeneous Scenarios,\" in IEEE Transactions on Consumer Electronics, vol. 70, no. 3, pp. 5983-5999, Aug. 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2CqPlNPI4g", "forum": "TaThyuCLk7", "replyto": "TaThyuCLk7", "signatures": ["ICLR.cc/2026/Conference/Submission12376/Reviewer_kvHE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12376/Reviewer_kvHE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957165148, "cdate": 1761957165148, "tmdate": 1762923279596, "mdate": 1762923279596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}