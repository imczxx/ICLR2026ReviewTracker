{"id": "OMjcX1Z6Uu", "number": 22558, "cdate": 1758332829085, "mdate": 1759896859649, "content": {"title": "Markovian Transformers for Informative Language Modeling", "abstract": "Chain-of-Thought (CoT) reasoning often fails to faithfully reflect a language model's underlying decision process. We address this by introducing a Markovian language model framework that can be understood as a reasoning autoencoder: it creates a text-based bottleneck where CoT serves as an intermediate representation, forcing the model to compress essential reasoning into interpretable text before making predictions. We train this system with a GRPO-style policy gradient algorithm using parallel sampling, a frozen baseline CoT', within-batch standardized advantages, and actor-reward (chain-rule) gradients. Our approach yields large gains on QA tasks (e.g., GSM8K: 20.7% to 54.5%; +33.8 pp; ARC-Challenge: 47.5% to 76.9%; +29.4 pp). Perturbation analyses across types and severities show consistently higher sensitivity to CoT edits (typically 52%--82% of cases favor Markovian), indicating stronger causal reliance on the CoT. Cross-model evaluation confirms that learned CoTs generalize across architectures, suggesting they capture transferable reasoning patterns rather than model-specific artifacts.", "tldr": "Markovian Transformers make Llama-3.1-8B emit causally necessary token-level chains of thought, boosting GSM8K accuracy by 34 pp and revealing self-explanatory reasoning that guides its own future token predictions.", "keywords": ["Markovian Transformers", "chain-of-thought reasoning", "language model interpretability", "causal reasoning", "reinforcement learning", "next-token prediction", "GSM8K", "large language models"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60c230b26804ba46435694175d7d5d6dd8df637e.pdf", "supplementary_material": "/attachment/864fec3317452b13e9ed5aa713b830709d17bda6.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Markovian Language Model framework. By training the system with GRPO-style policy gradient algorithm, the model yields large gains on QA tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed markovian transformers is interesting and I recognize its novelty. The paper did abundant experiments on various datasets, which makes the paper sound."}, "weaknesses": {"value": "W1: Though you mentioned what is the baseline in your manuscript, but I didn't understand what your baseline is quite well. Also, I want to look at the comparison between yours method/architecture between the model/training method we are now using today. For example, I didn't see the comparison to LLM post-trained on CoT by teacher-forcing and RL methods. It is hard for me to judge its significance."}, "questions": {"value": "Q1: In figure 1, while $o_1$ stands for question and $s$ stands for CoT, it is hard for me to understand what is the $o_i$ on the right. How do you get the $o_i$? If it is sampled according to $s_i$, then which part corresponds to the question?\n\nQ2: In figure 2, can you explain why it actually performs worse on MMLU?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UvmaUqL7BZ", "forum": "OMjcX1Z6Uu", "replyto": "OMjcX1Z6Uu", "signatures": ["ICLR.cc/2026/Conference/Submission22558/Reviewer_K7Fw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22558/Reviewer_K7Fw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873090133, "cdate": 1761873090133, "tmdate": 1762942277255, "mdate": 1762942277255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tries to improve the problem of chain of thought unfaithfulness, specifically when chain of thought isn't actually necessary for the model to reach its conclusions. The authors propose a Markovian framework wherein the model is blocked from attending to the original question during answer generation This forces all task-relevant information to be funneled through the chain of thought. To do this training, they use a GRPO-style algorithm and introduce further improvements like parallel sampling. After training, the models perform better on math reasoning benchmarks. The authors also use perturbation analyses on a Wikipedia text completion task to show that the chains of thought produced by their methods are more causally dependent than those produced by standard approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem (chain-of-thought unfaithfulness) is of broad interest to the field\n- The method design seems novel and creative. It also seemed tricky to implement and the authors developed training strategies to make it work."}, "weaknesses": {"value": "- Although the authors state that the approach improves performance on benchmarks, I don't think they compared to a baseline-- eg, what if you just did the training but didn't have the attention to the original question blocked?\n- It would also be nice to have some qualitative discussions of the chains of thought that result from this training procedure, whether through example transcripts or through some grading of readability."}, "questions": {"value": "- I'm not sure how to interpret figure 3. I'm assuming the x-axis ('sample') is number of tokens sample. Are there some examples of what these chain of thought transcripts look like? Are there parts of the transcript where the llama model has stated the answer?\n- I also would like more clarification to interpret the perturbation analyses. Is this analysis testing how well the model adheres to the exact continuation of the wikipedia text? If so, why not do the same perturbation analyses on the previous QA tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oCkjWmqGEZ", "forum": "OMjcX1Z6Uu", "replyto": "OMjcX1Z6Uu", "signatures": ["ICLR.cc/2026/Conference/Submission22558/Reviewer_V7a8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22558/Reviewer_V7a8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977288024, "cdate": 1761977288024, "tmdate": 1762942276943, "mdate": 1762942276943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Markovian Language Model (MLM) framework that enforces structural constraints on Chain-of-Thought (CoT) reasoning. The key insight is treating CoT as a \"reasoning autoencoder\" where the model must compress essential information into interpretable text that serves as a bottleneck between question and answer. By preventing the model from accessing the original question during answer generation (only seeing the CoT), the framework forces causal reliance on the CoT."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "It is a nice and elegant idea to introduce causal reliance by construction. The distinction between \"faithfulness\" and \"informativeness\" is pragmatic and operationalizable. The reinforcement learning formulation seems sound and there is good improvement after training. There is interesting cross-model generalization where learned CoTs in one architecture transfer to another architectures"}, "weaknesses": {"value": "The paper oscillates between two different stories: compression (Wikipedia experiment) and sufficiency (for QA experiment).\n\nThe experiments are a bit superficial, there is only one LLaMA model and no baselines such as SFT and GRPO (yet the appendix F anyway shows some Wikipedia results for other models, then why not report results on the QA dataset as well?). \nIt would be necessary to compare to other post-training baselines, especially on these datasets where any type of reinforcement learning. \nFurthermore, It is not clear what is the training data for the experiments in section 5.1, and there is no notion of uncertainty that is being reported (like std or confidence intervals). Appendix D.1 further discuss alternative reinforcement learning formulation, but it is not clear how and why the one in the main paper was chosen."}, "questions": {"value": "How does the approach compare to baselines finetuning methods on QA dataset?\nWhat was the training data in QA experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S0i9noSoYO", "forum": "OMjcX1Z6Uu", "replyto": "OMjcX1Z6Uu", "signatures": ["ICLR.cc/2026/Conference/Submission22558/Reviewer_w9RC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22558/Reviewer_w9RC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991952814, "cdate": 1761991952814, "tmdate": 1762942276677, "mdate": 1762942276677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Markovian Language Model (MLM) framework that enforces a causal dependency between a model’s Chain-of-Thought (CoT) and its final prediction. The key idea is a “reasoning autoencoder” architecture that introduces a text-based bottleneck: the model must first generate a CoT, and only that CoT (not the original question) is used to produce the answer. The model is trained using a GRPO-style policy gradient algorithm, where the reward depends on how informative the generated CoT is for answer prediction. Empirical results show large improvements on reasoning benchmarks and higher perturbation sensitivity to CoT edits, suggesting the CoTs have become ''load-bearing.''"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of enforcing a Markovian structure to make CoTs causally essential is novel, conceptually elegant, and well-motivated.\n\n- The introduction of informativeness as a learning objective is interesting and moves beyond traditional notions of faithfulness or interpretability.\n\n- The formalisation of the Markovian LM and integration of actor–reward gradients (where the reward depends on the same model parameters) are technically sound and well-presented.\n\n- Empirical results show strong and consistent improvements on reasoning benchmarks.\n\n- The perturbation sensitivity and cross-model transfer analyses go beyond accuracy metrics, probing whether the model is actually using CoTs."}, "weaknesses": {"value": "- It is unclear which components of the method (Markovian bottleneck, actor–reward coupling, within-batch normalisation, or reward design) are responsible for the observed gains. The paper should include controlled ablations to isolate these effects.\n\n- The informativeness criterion works well for deductive or mathematical reasoning tasks where the CoT captures logical steps. However, for non-deductive or knowledge-grounded tasks (e.g., MMLU, factual QA), informativeness alone may be insufficient.  Additionally, the authors claim that forcing the model to predict the answer only from CoT improves informativeness. Still, there is no baseline from which the model predicts (Question + CoT) under the same RL training. This comparison is critical to verify that the performance gains truly stem from the Markovian constraint.\n\n- There is no comparison against recent process-supervised or fine-tuned CoT methods such as STaR. This makes it difficult to judge whether the proposed Markovian constraint provides a real advantage over established reasoning-enhancement techniques."}, "questions": {"value": "Missing citations: \n\n1. Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning EMNLP Paul et.al. 2024\n2. Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations ICML Ferreira et. al. 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "00vC2EXTiB", "forum": "OMjcX1Z6Uu", "replyto": "OMjcX1Z6Uu", "signatures": ["ICLR.cc/2026/Conference/Submission22558/Reviewer_Rwtw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22558/Reviewer_Rwtw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762269022708, "cdate": 1762269022708, "tmdate": 1762942276446, "mdate": 1762942276446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}