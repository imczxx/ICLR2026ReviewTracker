{"id": "Vr0VniKoen", "number": 13274, "cdate": 1758215892136, "mdate": 1763313109220, "content": {"title": "MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models", "abstract": "Large Language Models (LLMs) acquire emergent reasoning capabilities when fine-tuned in an online setting with simple rule-based rewards. Recent studies, however, indicate that success in this regard is conditioned on the latent solvability of tasks in the base LLM: RL can only amplify answers to which the base model already assigns non-negligible probabilities. This work investigates the emergence of chemical reasoning capabilities and what these prerequisites mean for chemistry. We identify two necessary conditions for RL-based chemical reasoning: 1) Symbolic competence, and 2) Latent domain knowledge. We propose MiST: a set of mid-stage training techniques to satisfy these, including data-mixing with SMILES-aware preprocessing and continued pre-training on a rich data mixture of 2.9B tokens. These steps raise the latent-solvability score on IUPAC to SMILES translation by 2x and enable RL to lift top-1 accuracy on reaction prediction from 4.1% to 25.2% on challenging chemical tasks, while producing faithful reasoning traces. Our results define clear prerequisites for chemical reasoning training and highlight the broader role of mid-stage pre-training in unlocking reasoning capabilities.", "tldr": "LLMs need certain requisites before RL fine-tuning to succeed at chemical reasoning. Pretraining targeting symbolic competence and latent domain knowledge are key to unlocking reasoning abilities", "keywords": ["LLM", "reasoning", "chemistry", "science", "ai4science", "reinforcement learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/795c5eaa9157240e41e12a8477cde711b108ffca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work tackles the problem of understanding what enables LLMs to perform in the chemical reasoning domain and proposes a diagnostic framework, as well as a mid-stage dataset. The authors argue that two prerequisites are necessary for reasoning in the chemical domain: (1) symbolic competence (ability to read/write valid SMILES), and (2) latent chemical knowledge (distinguishing correct and incorrect statements about molecules). They propose diagnostic metrics (SCS and CCS) to measure these prerequisites, create a 2.9B-token chemistry corpus through preprocessing, and demonstrate improvements across multiple chemistry tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Novel diagnostic approach:** The SCS/CCS metrics attempt to quantify a priori readiness for RL training rather than just measuring end performance. This predictive framing—can we assess whether RL will work before expensive training?—is valuable and timely.\n- **Systematic investigation of prerequisites:** Unlike prior work that simply applies RL to chemistry, this paper explicitly decomposes what's needed (symbolic competence + domain knowledge) and attempts to measure each component separately. The conceptual framework could guide future domain adaptation work.\n- **Detailed data preprocessing pipeline:** The multi-step approach is more sophisticated than simply scraping chemistry text.  \n- **Honest about contradictions:** Unlike many papers, the authors show results where MiST hurts performance (CMG 58.6→1.2%) rather than cherry-picking successes. This transparency is valuable for understanding when the approach fails."}, "weaknesses": {"value": "- **Inadequate statistical validation:** No error bars, confidence intervals, or significance tests are reported. Single-run experiments on one backbone (Qwen-2.5-3B) prevent generalization claims and drastically limit the insight of this work.\n- **Missing baselines:** Only NatureLM is compared (appendix only); recent models like Intern-S1-mini [1] and ether0 [2] are ignored.\n- **Incomplete ablations:** No compute-matched SFT+RL baseline without MiST across all tasks—essential to isolate MiST's contribution. \n- **Underspecified protocols:** Rollouts, generation settings (temperature, top-p), and test set sizes are unreported, preventing reproducibility.\n\n- **Performance collapses contradict the thesis**\n  - CMG paradox: 58.6% (base) → 1.2% (MiST) → 34.8% (SFT)—a catastrophic collapse that directly contradicts MiST benefits.  \n  - I2S reasoning failure: Post-SFT, reasoning helps (34.5→68.2%); post-RL, reasoning hurts (68.2→67.5%). Authors claim “capability already present” (L291), but this suggests memorization, not reasoning acquisition.  \n  - RxN strange results: MiST+SFT for RxN barely exceeds a 10% random baseline; SFT dataset alone destroys performance, is there then any value in having it for this task. No Abalation really shows this.\n\n\n- **Conceptual contradiction:** L105 dismisses perplexity as “not meaningful,” yet SCS/CCS compute Cohen’s *d* on token log-probabilities—literally perplexity components. \n\n- **Unvalidated metrics:**  \n  - No predictive validation: SCS/CCS thresholds are never shown to correlate with downstream RL success.  \n  - Arbitrary design choices: Corruption rate ρ=0.2 is unablated; why not 0.1 or 0.3?  \n  - Prompt dependence unexamined: SCS/CCS use a specific prompt (“The molecule represented...”) but prompt sensitivity is never tested—different phrasings could yield different scores.  \n  - CCS ambiguity: Sentence-swapping may test textual coherence, not chemical knowledge. No inter-model consistency is tested. No degree of information swapping is examined.\n\n- **Presentation quality:** Wrong template (NeurIPS, not ICLR), broken appendix indexing, inaccessible Figshare links, duplicate citations. Missing specifications: templates (L212–218), system prompts (L162), context lengths (mentioned but absent from configs).\n\n- **Unsubstantiated claims:** Claims MiST necessity despite published work suggesting otherwise (e.g., [2], L295–301). “Competent scientific assistant” claim (L311–319) is not demonstrated.\n\n- **Fundamental issue:** The thesis—that MiST creates necessary prerequisites for RL—is undermined by (1) CMG collapse showing MiST can destroy capabilities, (2) unvalidated diagnostics contradicting stated principles, (4) insufficient ablations, and (3) insufficient distinction from simpler alternatives (larger models, more SFT data). Without statistical rigor, compute-matched controls, explanation of failures, or validation that diagnostics predict success, contributions remain speculative.\n\n### Additional References\n\n - [1] Lei Bai, Zhongrui Cai, Yuhang Cao, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, *et al.* **Intern-S1: A Scientific Multimodal Foundation Model.** *arXiv preprint arXiv:2508.15763*, 2025.  \n\n - [2] Siddharth M. Narayanan, James D. Braza, Ryan-Rhys Griffiths, Albert Bou, Geemi Wellawatte, Mayk Caldas Ramos, Ludovico Mitchener, Samuel G. Rodriques, and Andrew D. White. **Training a Scientific Reasoning Model for Chemistry.** *arXiv preprint arXiv:2506.17238*, 2025."}, "questions": {"value": "- Can you provide results from 3–5 runs with error bars? Are any improvements statistically significant (t-tests or similar)?\n- Please provide SFT+RL (no MiST) with matched training compute for all tasks. How does this compare to MiST+SFT+RL?\n- Why does MiST catastrophically reduce CMG accuracy from 58.6% to 1.2%? This directly contradicts your thesis. Does MiST sometimes harm performance?\n- Have you tested the influence of randomized SMILES strings on SCS? What is the influence of different prompts or base models? How is CCS influenced by the degree of information changed? \n- How were SCS/CCS thresholds determined? Show correlation plots between these scores and downstream task accuracy.\n- Why does RL reduce the benefit of reasoning on I2S? Doesn’t this suggest models are memorizing rather than learning to reason?\n- Have you tested on other base models (7B, different families)? Do the prerequisites hold across architectures?\n- What’s the effect of formatting rewards? Do they dilute task-specific learning? Provide ablations removing different reward components.\n- What system prompts were used for base, SFT, and RL models? Why no system prompt for fine-tuning (noted in appendix)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OZonDfQJ64", "forum": "Vr0VniKoen", "replyto": "Vr0VniKoen", "signatures": ["ICLR.cc/2026/Conference/Submission13274/Reviewer_wQXW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13274/Reviewer_wQXW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725711181, "cdate": 1761725711181, "tmdate": 1762923951901, "mdate": 1762923951901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors argue that RL-based training of chemical reasoning models can only be successful if the base model already possesses (a) symbolic competences — i.e., the ability to read and understand chemistry-specific notation such as SMILES strings — and (b) domain-specific chemical knowledge. For both (a) and (b), the authors propose evaluation metrics based on the model’s token log-likelihood. Additionally, the authors employ a training pipeline consisting of domain-specific pretraining (MIST training), supervised fine-tuning (SFT), and reinforcement learning (RL). Across these training stages, the introduced metrics for (a) and (b), as well as top-1 accuracy across three different subtask categories, are reported, with the RL-trained models generally performing best."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**(S1 - relevance, novelty) - Relevance of the raised research question.** I share the authors’ view of RL as an amplifier for knowledge that is already present but latent within the base model. It is almost a general assumption that RL (given sufficient compute budget) will succeed if the base model is sufficiently capable. However, it remains unclear what exactly constitutes a “good base model”. This is precisely the research question addressed here: “What pretraining and prerequisites must an LLM satisfy so that RL can reliably unlock chemical reasoning?” In this sense, research in this direction is highly relevant, and a well-founded answer with accompanying insights would represent a novel contribution.\n\n**(S2 - relevance, novelty) - Measures for symbolic competence and chemical knowledge.** Focusing on the two skills (a) and (b) appears well motivated based on the introduction. I appreciate the proposed metrics and believe they reflect, to some extent, the base model’s competencies that the authors argue are necessary (for minor weaknesses, see questions). Reading Section 3 — and considering the rationale in (S1) — these metrics seem both relevant and interesting. I was particularly curious to see how well they correlate with the model’s learning speed during the RL stages. (Unfortunately, this was never shown - see weaknesses.)\n\n**(S3 - quality, clarity) - Clear problem setting, well embedded in the current research landscape.** Sections 1, 2, and 3 are remarkably well written and clearly structured. Sections 1 and 2 are easy to read and follow, providing a solid overview of the research scope, how the raised question fits into related work, and why it is important to pursue. Section 3 builds upon this foundation by introducing the metrics for the previously discussed competencies (a) and (b).\n\nWith this level of clarity, narrative flow, and writing quality — combined with a relevant and interesting research question that includes quantitative measures of base model competencies — the first part of the manuscript, Sections 1 – 3, stands out as the best I have read during this review process, ..."}, "weaknesses": {"value": "... however, the second part of the manuscript (from Section 4 onward) feels like a completely separate work. The storytelling flow breaks at several points — in fact, the raised research questions are never answered, and the experiment is unsuitable. Several text passages lose their focus; for example, Section 4 describes in great technical detail how to derive flattened text (which is not particularly interesting and could be moved to the appendix) instead of providing details about the training procedure and the aforementioned toolbox. Overall, the manuscript still appears to be in a draft stage rather than a polished submission (e.g., see formatting issues in l228f).\n\n**(W1 - relevance) - Experiments do not help to answer the research question.** The chosen experiment is not suitable to address the stated research question. To answer it properly, different base models with varying measures for (a) and (b) would need to be RL-trained, and the resulting training behavior would then need to be correlated with these measures. This way, the authors might have been able to identify thresholds for (a) and (b) representing the necessary competencies for successful RL training. However, the authors do not mention such thresholds in their analysis. Therefore, the only conclusion that can be drawn from the experiment is that all three training stages help improve model performance — a finding that is of limited novelty and has been demonstrated before, e.g., in [1].\n\n**(W2 - quality, significance) - Missing error bars.** All results are presented without error bars or statistical tests. Consequently, it remains unclear to what extent the reported performance gains might have occurred by chance.\n\n**(W3 - quality) - Cluttered equations.** The included equations appear cluttered, and the notation is inconsistent. For example, in Equation (2), the authors should avoid using full-word variable names such as “corrupt” or “canon” and instead adhere to standard mathematical notation."}, "questions": {"value": "* **Token log-likelihood extraction:** Is there a reason the authors chose token log-likelihood instead of perplexity? While both metrics are conceptually similar, the latter appears to be the canonical choice for this type of evaluation.\n\n* **Symbolic competence score:** I assume that for this metric, valid and corrupted SMILES are sampled or generated for each new dataset. Would this measure then remain comparable across datasets? Would it be possible to define a static global set of valid and corrupted SMILES to allow for better cross-dataset comparison? Or is it important to measure symbolic competence specifically within certain chemical subspaces? Does the metric vary across such subdomains or with molecular complexity?\n\n* **Latent chemical knowledge / latently solvable:** These terms are used frequently throughout the manuscript. While their choice is appropriate, I found myself reflecting on their precise meaning. A brief introductory explanation early on would have been helpful.\n\n* **Latent chemical knowledge metric:** I understand why the symbolic competence approach is reused here, but I am unsure whether it truly captures the required competence. Wouldn’t a massively overfitted model achieve a high score on this metric without actually being able to associate new samples in the RL stage with underlying chemical knowledge?\n\n### Minor comment\n[1] and [2] are missing from the related work section, despite being state-of-the-art chemical reasoning models.\n\n### References\n* [1] Narayanan. *Training a Scientific Reasoning Model for Chemistry.*\n* [2] Zhao. *MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs.*\n\n### General comment\nThe raised research question is highly interesting, and research in this direction is much needed. The introduction and motivation of this question are exceptionally well written. However, from Section 4 onward, the manuscript feels rushed and, frankly, not yet ready for publication. Nevertheless, I see strong potential in this work. With suitable experiments and a more polished presentation, I believe it could become a valuable contribution to one of the upcoming ML conferences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IQly7Vh0jV", "forum": "Vr0VniKoen", "replyto": "Vr0VniKoen", "signatures": ["ICLR.cc/2026/Conference/Submission13274/Reviewer_oqMf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13274/Reviewer_oqMf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952824494, "cdate": 1761952824494, "tmdate": 1762923951606, "mdate": 1762923951606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two criteria to validate whether an LLM has the potential to perform chemical reasoning after further RL training. They construct the CPT and SFT training corpus to increase these two criteria and achieve better reasoning performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea of constructing criteria to determine whether a model has the potential for reasoning training is interesting and creative. Although not adequate, their experiments provide preliminary evidence that the criteria they proposed are effective to some extent."}, "weaknesses": {"value": "1. The paper is ill-organized, to the point of significantly hindering readability.\n    * Their citations have severe problems. When I try to look up one of their citation, 'ChemLLM', neither the title, the authors, nor the ArXiv ID matched what was listed in their paper. I then searched using the given ArXiv ID and found that the title and authors were completely different, and clearly inconsistent with the intended citation context. I was therefore unable to locate the paper they referenced, which raises serious doubts about the correctness of other citations in the paper as well.\n    * The references to sections and tables are confusing and chaotic. The reference in line 51 refers to section 1, while itself is in section 1. The reference in line 182 seems unnecessary. The two references in line 184 are either inappropriate (Use the header of Table 4 as the task list instead of constructing a new table) or invalid (They refer to the Appendix while not providing any appendix). etc.\n\n2. The overall content of the paper is not substantial enough to justify a full-length paper.\n\n    The manuscript includes repetitive discussions and excessive implementation details that would be better suited for the appendix. The experimental evaluation is quite limited and insufficient to convincingly validate the proposed core innovation. Additional datasets, baselines, and ablation studies are needed to strengthen the results. In general, the work feels more like an ongoing research project to me, with significant room for further development and refinement.\n\n3. The paper overclaims many contributions while providing insufficient validation for its core contribution.\n    * In the Introduction, the authors claim to have proposed several elements that, in my view, do not constitute genuine contributions. For instance, they state that they “propose a representative set of tasks in chemistry that are suitable for reasoning,” yet they do not actually formalize or construct a new benchmark or task set. They also claim to have built a new pretraining corpus, but both the data scale and the construction methodology are not particularly attractive or novel.\n    * In contrast, I believe the true innovation of this work lies in the idea of constructing criteria to determine whether a model has the potential for reasoning-oriented training. This idea itself is both interesting and valuable, and if supported by sufficient experimental evidence, it could easily justify a full paper. However, despite claiming to have conducted extensive experiments (as mentioned in lines 41–42 and 52–53 of the Introduction), the experimental section does not actually provide results that substantiate these claims.\n\n        In my opinion, if the authors could rigorously validate their claim (for example, by demonstrating that the proposed prerequisite is indeed correlated with reasoning performance, and could serve as an indicator; or ideally, that there exists a threshold beyond which reasoning ability begins to emerge), this paper would become a very interesting and valuable piece of work."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gVwTbUPJpl", "forum": "Vr0VniKoen", "replyto": "Vr0VniKoen", "signatures": ["ICLR.cc/2026/Conference/Submission13274/Reviewer_udTx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13274/Reviewer_udTx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982719531, "cdate": 1761982719531, "tmdate": 1762923951311, "mdate": 1762923951311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MiST within a multi-stage pipeline to develop chemical reasoning in LLMs. To enhance the chemical reasoning abilities, MiST applies continued pre-training on a rich 2.9B token chemistry corpus, followed by Supervised Fine-Tuning (SFT). These stages increase the latent solvability score dramatically. Crucially, the final post-training stage, employing Reinforcement Learning (RLVR), amplifies these emergent capabilities, significantly improving the performance of multiple chemistry tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Robust Data Construction from Diverse Sources. \n\nThe model benefits from a high-quality 2.9B token corpus for continued pre-training (MiST), drawing extensively from Diverse sources such as ChemRxiv + S2ORC and PubChem synthetic data. Additional datasets including both instruction-following dataset and reasoning traces from DeepSeek-R1 are also collected for enhancing the chemical reasoning abilities.\n\n2. Effective Multi-Stage Training Pipeline. \n\nThe training incorporates continued pre-training (MiST) for general chemistry knowledge, Supervised Fine-Tuning (SFT) using instruction datasets (like SmolInstruct) and reasoning traces distilled from DeepSeek-R1, and RLVR for specific task post-training amplification.\n\n3. Significant Performance Improvement. \n\nThis holistic methodology raises the latent solvability of tasks and successfully amplifies capabilities, improving the performance of LLM on multiple chemistry reasoning tasks."}, "weaknesses": {"value": "1. No experiments to show improvement in terms of SMILES generation\n\nIt's unclear whether after the extensive training with data including SMILES strings, LLM can successfully generate accurate SMILES strings while retaining good reasoning abilities now. Although the paper includes CCS and SCS scores for comparison, it is not explicit enough about the improvement in the ability to generate reasonable and correct SMILES strings. \n\n2. No experiments to show improvement in terms of reasoning abilities\n\nAlthough on 4 reasoning tasks, the trained LLM shows significant improvement in metrics compared with base model. But it is very expected and simply fine-tuning the LLM with multi-source data could also achieve similar or even higher performance based on the metrics reported in LlaSMol paper. Therefore, with only these top-1 accuracy, it is unclear whether the reasoning ability of LLM is truly improved after the extensive multi-stage training."}, "questions": {"value": "1. is it possible to include an additional table to extensively test the ability to generate correct SMILES strings with more metrics, like whether the generated SMILES strings are grammarly correct now? At what percentage, different LLM models can output correct SMILES strings and after the training, the trained LLM can output the correct SMILES strings? Is it possible to have some specific tasks like translating SMILES correspond to the same molecule to specifically test the understanding ability of LLM towards SMILES strings after the training?\n\n2. Can the authors show multiple reasoning examples from the trained LLM to demonstrate the improved reasoning abilities? It will also be great if more quantitative analysis based on human expert can be conducted to demonstrate the effectiveness of reasoning outputs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xf32E3zg17", "forum": "Vr0VniKoen", "replyto": "Vr0VniKoen", "signatures": ["ICLR.cc/2026/Conference/Submission13274/Reviewer_uEhF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13274/Reviewer_uEhF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13274/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987562754, "cdate": 1761987562754, "tmdate": 1762923949902, "mdate": 1762923949902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their constructive and very detailed feedback. The updated manuscript has substantially improved over the time after the first submission, and thanks to the reviews. We have now implemented several revisions to address the main concerns from the reviewers and ultimately give a better and more focused shape to the manuscript.\n\nThe paper now has a stronger focus on diagnostic metrics as predictive tools, that can help assess the readiness of base models for RL training on chemical tasks. MiST is now positioned as a method that can help satisfy such readiness, rather than a core contribution itself. In this vein, we have updated the manuscript with several more experiments that directly show the value of measures like SCS at predicting future success, and others that help clarify the construction of the metric, justifying decisions that at first seemed ad-hoc. Finally, we conducted a case study that further showcases the predictive power of this approach.\n\nHere we summarize some of the main additions and fixes we have introduced in this new version of the manuscript. The changes have been highlighted in blue in the new version to facilitate revision.\n\n- Added Section 5.1: Formulation and validation of SCS:\nWe analyze the effect of varying the corruption rate in the formulation of SCS, and find that 0.2 is enough to cause differentiation between the models.\n- Added Section 5.2: SCS as a predictive framework:\nCorrelation analysis between pre-RL SCS against post-RL performance. We show ρ >0.6 cross-task performances, and show a clear success signal post-RL when evaluated in-task (Figure 4). We also include a case-study, where we determine the best large base model for RL training from a pool of open-access models. The model (Mistral-24B) matches the one used as base for ether0 (Narayanan 2025), retrospectively confirming the optimality of this choice, despite it having lower benchmark scores than similarly sized or even larger LLMs.\n- Multi-scale validation: All experiments have now also been scaled to 7B, showing consistent observations across scales.\n- Additional baselines: We now include ChemLLM-7B as a new baseline in Table 2, showing even our 3B models overperform it.\n\nAdditionally we have improved the presentation and organization of the paper, combining the manuscript and appendix into a single document, which addresses the broken appendix references. Dataset processing details have been mostly moved to the appendix, and Sections 4 and 5 have been largely revised to match the flow of the first Sections. Other formatting issues as noted by reviewers have also been fixed, as well as improved mathematical notation and revised citations.\nAs a summary, the new version has shifted the tone into a prescriptive framework where we show that SCS and SCS-like measures can predict success of RL on specific domains, chemistry in our case, with actionable thresholds that can be computed on small scale but that translate into much larger models. MiST finds a role as an enabler of such readiness, to optimize models against SCS while preventing over-fitting to scientific notation.\nWe believe the work has substantially improved and we thank the reviewers again for their constructive comments and in-depth revisions. We hope the strengths the reviewers had originally identified are now even stronger in the new paper, and we are open to further enriching discussion and revision!"}}, "id": "YKRzZc539n", "forum": "Vr0VniKoen", "replyto": "Vr0VniKoen", "signatures": ["ICLR.cc/2026/Conference/Submission13274/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13274/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission13274/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763330525054, "cdate": 1763330525054, "tmdate": 1763330525054, "mdate": 1763330525054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}