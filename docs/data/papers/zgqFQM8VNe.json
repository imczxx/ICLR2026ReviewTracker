{"id": "zgqFQM8VNe", "number": 18458, "cdate": 1758288010669, "mdate": 1759897102241, "content": {"title": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask", "abstract": "Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. \nAlthough world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. \nWe introduce **ConsisDrive**, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. \nOur framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks \nto ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, \nthereby preserving object identity consistency; \nand (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. \nBy integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset.", "tldr": "We present a identity-preserving world model that generates realistic multi-view driving videos with superior fine-grained temporal consistency.", "keywords": ["Temporal Consistency", "Video Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f85c1f16361999a28c51597c130c7af6ba43931.pdf", "supplementary_material": "/attachment/a043bea19aad7dc0b331fa8b1acab1612486e55d.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents ConsisDrive, a novel video diffusion model that incorporates instance-specific attention to improve instance consistency in autonomous driving scenarios. To address identity drift, the method introduces an instance global identity condition embedding that represents all instances in the scene. For each instance, specialized attention is applied to patches corresponding to the instance mask, and the output is fused with the standard attention output through gated addition."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on addressing the important problem of instance consistency in driving scene generation.\n2. Experimental results demonstrate that ConsisDrive achieves strong performance on key metrics."}, "weaknesses": {"value": "1. This method adopts full attention, which is very costing\n2. Insufficient literature review: The paper does not includerecent driving world models published in late 2024 and early 2025. I list some of them: [UniMLVG](https://arxiv.org/abs/2412.04842), [UniScene](https://arxiv.org/abs/2412.05435), [DrivingSphere](https://arxiv.org/abs/2411.11252), and [DiVE](https://arxiv.org/abs/2409.01595). Notably, UniMLVG and UniScene are open-source, and UniMLVG reports better FVD performance.\n3. The current results do not conclusively demonstrate that instance mask attention is the primary contributor to improved instance consistency. The improvement is mainly shown in the quantitative metrics. While Figure 3 suggests the complete pipeline improves instance-level consistency, the compared baselines differ from ConsisDrive in multiple dimensions, including model architecture, pretrained models, and conditioning inputs. The authors should isolate the contribution of the proposed attention in the qualitative results. Additionally, I recommend that the authors provide some challenging cases, such as vehicles that temporarily disappear and later reappear in the scene."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zi4OKnepGx", "forum": "zgqFQM8VNe", "replyto": "zgqFQM8VNe", "signatures": ["ICLR.cc/2026/Conference/Submission18458/Reviewer_V1rg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18458/Reviewer_V1rg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761492300415, "cdate": 1761492300415, "tmdate": 1762928155654, "mdate": 1762928155654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ConsisDrive, an identity-preserving world model for driving scene video generation, which aims to address the \"identity drift\" problem. The core contribution lies in the introduction of two instance-aware mechanisms: Instance-Masked Attention (IMA) and Instance-Masked Loss (IML). IMA utilizes identity masks and trajectory masks to enforce visual tokens to exclusively interact with features of their corresponding instances across both spatial and temporal dimensions, thereby ensuring the consistency of instance attributes. IML employs a probabilistic dynamic masking strategy to adaptively emphasize supervision on foreground regions, reducing interference from background noise. Experimental results demonstrate that ConsisDrive achieves state-of-the-art video generation quality (lower FID/FVD) on the nuScenes dataset and shows significant performance improvements on downstream perception tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed IMA presents a simple yet effective solution by integrating instance-level identity conditioning and cross-frame propagation into the Transformer's 3D self-attention mechanism via instance identity masks and instance trajectory masks.\n2. The evaluation is comprehensive. Beyond standard video generation metrics (FID, FVD), the paper thoroughly assesses the utility of the generated data through downstream tasks, including perception and multi-object tracking.\n3. The video results in the supplementary materials are impressive and highly realistic."}, "weaknesses": {"value": "1. The necessity and advantages of injecting instance attributes (category, size, tracking ID) as a global condition Ginto the attention mechanism via the Instance Identity Mask, compared to traditional conditioning approaches in diffusion models, require deeper discussion and justification.\n2. It needs a clearer rationale for why encoding these instance attributes into a global embedding Gand interacting via the Identity Mask Mk,m+iis superior to alternative conditioning strategies, such as injecting them directly as tokens into the value V or using an additional cross-attention layer.\n3. The probabilistic dynamic masking strategy (with probability α) in IML is crucial for balancing foreground and global reconstruction. However, there is a lack of ablation studies on the choice or impact of α."}, "questions": {"value": "Please refer the above Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BTuVAxvC6I", "forum": "zgqFQM8VNe", "replyto": "zgqFQM8VNe", "signatures": ["ICLR.cc/2026/Conference/Submission18458/Reviewer_JFL4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18458/Reviewer_JFL4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637526155, "cdate": 1761637526155, "tmdate": 1762928155056, "mdate": 1762928155056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a good solution for improving instance-level temporal consistency in autonomous driving scene video generation. It identifies an important problem of high-cost real data collection in autonomous driving and demonstrates good performances through experiments. However, its lack of original methodological contributions makes it insufficient acceptance. The core ideas are adaptations or combinations of existing techniques to a specific scene, rather than novel frameworks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "S1. This work identifies instance identity drift (including category shifts, color inconsistencies, foreground dilution) as a serious issue for driving-oriented synthetic data, and provides solutions to the unique demands of driving scenes.\n\nS2. The experimental results do show advantages over current approaches. Evaluations on downstream tasks are included, providing an important insight that incorporating synthetic data helps to improve performances on downstream tasks. Also, ablations verify the importance of each module.\n\nS3. The paper is easy to follow.\n\nS4. The authors provide sufficient materials for reproducibility, which fosters the research community."}, "weaknesses": {"value": "W1. Limited Novelty. I would say that this is a great engineering work with a reasonable pipeline and should produce good results, but lacks significant distinction between this work and previous works. For example, CineMaster proposes using 3D depth box and class labels to achieve semantic layout control with ControlNet, which is quite similar to this work, in my opinion. Also, the authors claim that they propose instance-masked attention and instance-masked loss. However, neither are ground-breaking in computer vision. 1) Instance-masked attention only uses masked attention, which is a common practice in computer vision (e.g., Mask2Former), and the authors only ADAPT it to the autonomous driving scene. 2) Instance-masked loss is designed to force the models to focus on certain areas, whose idea is similar to Focal Loss, which have been used in many classic computer vision tasks.\n\nW2. Not So Comprehensive Experimental Setup. Although the authors present experimental results on multiple downstream tasks, the authors mainly train and test on the nuScenes dataset (on the video generation task, the main claim), which raises doubts about the generalization ability of the framework, especially lacking of cross-dataset validation, even in the supplementary materials. I would suggest that the authors benchmark on other autonomous video generation datasets (e.g., on private dataset in the supplementary materials and report the results)."}, "questions": {"value": "Q1. Open Sora 2.0 and ControlNet seem not to be the best option, have the authors tried other SOTA foundation models like Wan (2.1/2.2)? The reviewer thinks that Wan 2.1 has relatively good performances regarding identity preserving.\n\nQ2. Please report the computational cost of the framework and comparative methods, e.g., training epochs, total training time, inference latency, FLOPS (I know that the authors mention it in Appendix E, but I wonder what exactly it is).\n\nQ3. In Fig. 2(c), The text (Frame t and Frame t+1) is oriented vertically to the right, but the image is oriented vertically to the left.\n\nQ4. In the second and third parts of quantitative analysis subsection, the authors mainly use the generated videos as the sole or additional training data except for Table 2 (correct me if I’m mistaken), I wonder what the results will be if the authors directly generate videos from the validation set and conduct experiments on the MOT task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Not Applicable"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m8BQqWdrmQ", "forum": "zgqFQM8VNe", "replyto": "zgqFQM8VNe", "signatures": ["ICLR.cc/2026/Conference/Submission18458/Reviewer_6KzT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18458/Reviewer_6KzT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920418564, "cdate": 1761920418564, "tmdate": 1762928154373, "mdate": 1762928154373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ConsisDrive, a driving world model designed to address identity drift—a common failure mode in generative video models where objects change appearance or category across frames.  It achieves state-of-the-art results in FID (3.88) and FVD (37.23), and shows significant improvements in downstream tasks like perception (NDS) and multi-object tracking (IDS), demonstrating strong instance-level temporal consistency."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clearly identifies and addresses “identity drift,” a critical yet understudied issue in driving video generation.\n- Instance-Masked Attention effectively enforces instance-level consistency by leveraging identity and trajectory masks.\n- Instance-Masked Loss adaptively balances foreground and background supervision, improving fidelity for small objects."}, "weaknesses": {"value": "The paper lacks comparisons with several recent SOTA methods, particularly **InstaDrive** [1], which also focuses on the quality of instance-level generation. Including such comparisons would better contextualize the proposed method’s performance and highlight its relative strengths or limitations in generating high-fidelity instances.\n\n[1] Yang Z, Guo X, Ding C, et al. InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025: 25410-25420."}, "questions": {"value": "- How does the model handle severe occlusions or interactions between instances that may challenge the instance masking mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oPtuzHMJ6k", "forum": "zgqFQM8VNe", "replyto": "zgqFQM8VNe", "signatures": ["ICLR.cc/2026/Conference/Submission18458/Reviewer_4E4z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18458/Reviewer_4E4z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007596147, "cdate": 1762007596147, "tmdate": 1762928153878, "mdate": 1762928153878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}