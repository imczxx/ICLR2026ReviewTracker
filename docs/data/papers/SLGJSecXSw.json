{"id": "SLGJSecXSw", "number": 13018, "cdate": 1758212738304, "mdate": 1759897470570, "content": {"title": "FPDou: Mastering DouDizhu with Fictitious Play", "abstract": "DouDizhu is a challenging three-player imperfect-information game involving competition and cooperation. Despite strong performance, existing methods are primarily developed with reinforcement learning (RL) without closely examining the stationary assumption. Specifically, DouDizhu's three-player nature entails algorithms to approximate Nash equilibria, but existing methods typically update/learn all players' strategies simultaneously. This creates a non-stationary environment that impedes RL-based best-response learning and hinders convergence to Nash equilibria. Inspired by Generalized Weakened Fictitious Play (GWFP), we propose FPDou. More specifically, to ease the use of GWFP, we adopt a perfect-training-imperfect-execution paradigm: we treat the two Peasants as one player by sharing information during training, which converts DouDizhu into a two-player zero-sum game amenable to GWFP’s analysis. To mitigate the training-execution gap, we introduce a regularization term to penalize the policy discrepancy between perfect and imperfect information. To make learning efficient, we design a practical implementation that consolidates RL and supervised learning into a single step, eliminating the need to train two separate networks. To address non-stationarity, we alternate on-policy/off-policy updates. This not only preserves stationarity for $\\epsilon$-best-response learning but also enhances sample efficiency by using data for both sides. FPDou achieves a new state of the art: it uses a 3$\\times$ smaller model without handcrafted features, outperforms DouZero and PerfectDou in both win rate and score, and ranks first among 452 bots on the Botzone platform. The anonymous demo and code are provided for reproducibility.", "tldr": "A theoretically plausible and empirically effective algorithm for Doudizhu game.", "keywords": ["DouDizhu", "deep reinforcement learning", "fictitious play"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cd8dad50c9fa539fe4c348e0b60b72823b5f0700.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present FPDou, which achieves SOTA performance in DouDizhu using an extension of generalized weakened fictitious self play. In training, DouDizhu is treated as a two-player zero-sum game where the two peasants are treated as a team sharing perfect information. To extend the game to 3 players at deployment with full imperfect information, partial-observation feature extractors are trained for the peasants to have similar features as the perfect information encoders. These imperfect information features are then used in deployment. FPDou outperforms agents from prior work in skill."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents an impressive and difficult to achieve empirical result, attaining SOTA performance in DouDizhu."}, "weaknesses": {"value": "While the main empirical result is very impressive, there are several issues in my opinion with the communication of the method. Revising the paper with clearer language would help address many of my concerns.\n\n1. The presentation of the proposed approach is unclear. In line 203, it's claimed that a mixture of deep RL and supervised learning is used to learn a $\\epsilon$-best response as well as the average policy, yet this supervised learning component of the optimization is never mentioned again. Where in Algorithm 1 or the loss functions in section 4 does the method ensure that an average policy is learned? How is the average-policy supervised learning facilitated, and what loss is used? The theoretical explanation in section 3.2 follows, but it isn't clear how the process in the last paragraph of section 3.2 is actually implemented, which seems to be a key detail.\n\n2. The proposed approach to make partially observed features similar to fully observed features is a heuristic. The paper relies on an L2 feature regularize to “recover” the imperfect-info policy. This is a concise solution but somewhat ad-hoc. Unless there is a properly specific to DouDizhu that allows it, there’s no guarantee that an NE with imperfect-information is similar to an NE with teammate-shared perfect information. It would improve the paper if the authors could please clarify any convergence implications or limitations of using perfect-info training and imperfect-info execution.\n   \n\nI believe a missing limitation is that because the peasant policies are trained to only work with each other, they may not effectively cooperate well in ad-hoc peasant teams with other players/policies. (This limitation itself is not a weakness)\n\nAlso see Questions, which concern unclear aspects of the method."}, "questions": {"value": "a) Unless I am confused, the distinction between on-policy and off-policy seems to misuse vocabulary. According to Algorithm 1, the off-policy Q-learning method is used in all stages, always drawing data from an (off-policy) replay buffer $\\mathcal{D}$. To my understanding, at no point is \"on-policy\" RL ever actually used. Would a more appropriate terminology be something like off-policy learning with staggered opponent freezing?\n\nb) The replay buffer size of 100,000 seems very small for keeping historical data from all past best responses. How are you ejecting data from this replay buffer? Are you using reservoir sampling like in NFSP [1]? How does this replay buffer ensure that you can produce an average policy over a long training history across many days?\n\nc) Section E.4: What is \"Fraction of off-policy data in each batch: 0.5\"? I don't see explicit mention of this elsewhere in the paper. Are there two sources of data from which batches are constructed rather than the single replay buffer used in line 14 of Algorithm 1?\n\n[1] Heinrich, J., & Silver, D. (2016). Deep Reinforcement Learning from Self-Play in Imperfect-Information Games."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KPADwZeDyi", "forum": "SLGJSecXSw", "replyto": "SLGJSecXSw", "signatures": ["ICLR.cc/2026/Conference/Submission13018/Reviewer_pnAj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13018/Reviewer_pnAj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878677694, "cdate": 1761878677694, "tmdate": 1762923757873, "mdate": 1762923757873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an algorithm for learning to play DouDizhu. It is an amalgamation of ideas for building a new SOTA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper does an extensive evaluation of the proposed algorithm. There many tasteful practical choices made, from observing that the reward is discrete and using distributional RL to balancing training with win percentage."}, "weaknesses": {"value": "To start with text itself, there are lots of choice of words that do not make reading the paper easier. \n- 038 surely advancements have not been introduced to the game itself\n- 050 \"this reduces DouDizhu to two-plahyer zero-sum\" I think that a noun is missing\n- The appendix conflates the literature on extensive form games and game theory. In particular (@ 899) game theory does not assume perfect recall. \n- The paper refers to sequence form strategies as strategies. \n- At this level, introducing Kuhn's theorem is odd and it is not clear why realization-equivalent (I think it should be equivalence) is introduced.\n- The derivation of the GWFP (both in appendix and the main text) takes a lot of space just to make the point that sampling from the average sequence form policy is equivalent to first sampling a sequence form and then sampling from that sequence form policy.\n- 1278 the sum diverges and does not converge to zero. \n- The paper if fixated on being theoretically correct and grounded in GWFP but conveniently ignores that the perfect training imperfect execution (PTIE) paradigm is clearly not safe. The paper that introduced PTIE claims that it is an extension of centralized training, distributed execution but this is clearly not the case, the value net (the central element of training) is not used in execution but the Q functions training with PTIE are distilled into the average policy.\n- Going to the contributions of the paper, the only substantial contribution seems to be the off-policy, on-policy flags introduced in the loop\n   -  GWFP has been used for this game, so has PTIE, and learning sequence form averages from replay bufer (cf PSRO)\n- Twice, the paper claims that policy churns helps the exploration but policy churn is omnipresent everywhere so what makes DouDizhu special?"}, "questions": {"value": "Figure 12 a is the sampling strategy used to retrain a network from scratch? If so how long did the training take, how does the choice of temperature affects the WP?\nCan you explain ADP again?\nTable 2: Can the error be estimated with bootstrap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kQYoXdtg3r", "forum": "SLGJSecXSw", "replyto": "SLGJSecXSw", "signatures": ["ICLR.cc/2026/Conference/Submission13018/Reviewer_EC6D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13018/Reviewer_EC6D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976454301, "cdate": 1761976454301, "tmdate": 1762923757492, "mdate": 1762923757492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FPDou, a new RL framework designed to master the three-player imperfect-information card game DouDizhu. The paper adapts Generalized Weakened Fictitious Play to a deep RL setting, addressing the non-stationarity issue of multi-agent training. The authors convert the three-player game into a two-player zero-sum formulation by treating the two Peasants as a unified agent, and also apply alternative on-policy/off-policy updates and distributional Q-networks. Empirically, FPDou achieves SOTA performace with smaller models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of the paper is overall well-motivated. The analysis of instability in simultaneous self-play is interesting, and the proposed solution effectively addresses this issue under the given setting, providing valuable insights for the research community.\n- The empirical performance is strong — the paper achieves SOTA results using a smaller model, while maintaining a reasonable training cost.\n- The writing is clear and easy to follow."}, "weaknesses": {"value": "- The main obstacle to applying GWFP to the DouDizhu problem is its two-player zero-sum game setting, which the paper addresses by merging the peasants and adding a regularization term. However, simply aligning the latent representations with or without perfect information does not eliminate the need for perfect information and lacks a sound rationale. Therefore, FPDou is unlikely to satisfy the PTIE framework, since prior related work used only perfect information during policy evaluation.\n- The explanation of the off-policy component of the framework is unclear. If a fixed opponent is required, why is off-policy learning necessary? Which algorithm is used for the updates? How does off-policy learning ensure the stability of the model?\n- The paper sets a 0.5 win-rate threshold to ensure the ε-best response, which still introduces a degree of heuristics. There is also a potential risk that, in some iterations, the model may never reach a 0.5 win rate, thereby blocking training. This affects the method's generalizability. Although the authors mention an automated threshold adjustment process in the appendix, there is no noticeable difference in performance, which is somewhat counterintuitive and warrants further explanation.\n- I still have concerns regarding the generalizability of the paper, considering that it solely focuses on DouDizhu and many design choices and findings are problem-specific. For instance, are there existing works combining GWFP with RL in other games? How can other multi-agent games be generally transformed into two-player zero-sum settings? Which components of FPDou’s design could provide insights or inspiration for researchers working on different tasks?"}, "questions": {"value": "- More information on addressing the training-execution gap should be provided, such as visualizations of the regularization term during training or more effective methods to prevent agent cheating.\n- The motivation and methodology for off-policy learning need to be further explained.\n- Related work on applying GWFP to other games, as well as more generalizable takeaways, needs to be added."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sTO8lNwrP1", "forum": "SLGJSecXSw", "replyto": "SLGJSecXSw", "signatures": ["ICLR.cc/2026/Conference/Submission13018/Reviewer_UK6c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13018/Reviewer_UK6c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123610376, "cdate": 1762123610376, "tmdate": 1762923757173, "mdate": 1762923757173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}