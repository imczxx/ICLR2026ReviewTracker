{"id": "ksCYO3GRUY", "number": 23128, "cdate": 1758339937399, "mdate": 1759896831198, "content": {"title": "MetaOCDN: A Cognition-Inspired Meta Optimized Complementary Dual Networks for Online Continual Concept Drift Adaptation", "abstract": "The *Complementary Learning Systems* (CLS) theory points that humans can continuously and efficiently adapt to new tasks through the collaboration between the hippocampus and the neocortex: the former rapidly encodes new knowledge, while the latter extracts structured knowledge by abstract learning. Their synergy enables humans not only to quickly learn new tasks in the short term but also to transfer acquired knowledge across different tasks. Inspired by this theory, we address the challenge of streaming data mining under open environment with concept drift by proposing a cognition-inspired meta optimized complementary dual networks architecture (MetaOCDN), which consists of the Adaptive Fine Tuning Network (AFT-Net) and the Meta Representation Network (MRN-Net). AFT-Net is similar to the hippocampus, selectively fine-tunes key layers based on gradient variations to achieve rapid adaptation to novel concepts; MRN-Net is similar to the neocortex, we design self-supervised duality loss to continuously enhance its deep representation capability, thereby improving generalization to unknown distributions; furthermore, we design MAML-based multi-scale knowledge distillation strategy to facilitate dynamic information flow and knowledge transfer between the two networks. In summary, MetaOCDN provides a brain-inspired collaborative architecture that integrates the rapid responsiveness of AFT-Net with the abstract generalization capacity of MRN-Net, and enhances their interaction through knowledge distillation, thereby achieving a dynamic balance between fast adaptation and stable generalization in non-stationary data streams with concept drift. Extensive experiments demonstrate that MetaOCDN consistently outperforms state-of-the-art baselines across various drift scenarios.", "tldr": "", "keywords": ["open environment", "concept drift", "streaming data"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60464362b724b5f97b1e4a24ffc79f3518edcac9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of **concept drift** in streaming data mining. Inspired by the **Complementary Learning Systems (CLS)** theory in the human brain, the authors propose a dual-network architecture called **MetaOCDN**. The architecture consists of two components:  \n\n1. **Adaptive Fine-Tuning Network (AFT-Net):** Modeled after the hippocampus, it quickly adapts to new data distributions using a **gradient-aware selective fine-tuning** strategy.  \n2. **Meta-Representation Network (MRN-Net):** Modeled after the neocortex, it extracts stable, structured knowledge from historical data via **self-supervised learning**.  \n\nThe authors claim that these two networks collaborate through **MAML-based knowledge distillation**, achieving **rapid adaptation (plasticity)** while maintaining **generalization (stability)**, and outperforming existing methods across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Strong empirical results:** The paper's most prominent strength is its significant performance improvement over **17 baseline methods** across **9 diverse datasets** (including classification and regression), as shown in Table 1. Its **average rank (Avg Rank)** is 2.55, demonstrating a clear advantage.  \n\n2. **Excellent ablation studies:** Section 5.2 is particularly well-executed. Through ablation experiments, the authors clearly show that AFT-Net's **gradient-aware fine-tuning** (Figures 5 and 6a) and MRN-Net's **stable anchor mechanism** (Figure 6b) are both critical to its high performance.  \n\n3. **Theoretical support:** The paper provides theoretical analysis in Section 4, including **Theorem 1** (demonstrating the superiority of selective fine-tuning) and **sublinear regret bounds** (proving convergence efficiency).  \n\n4. **Candid discussion of limitations:** In Section 5.1, the authors openly discuss the model’s failures on the **Hyperplane** (incremental drift) and **Kddcup99** (discrete features) datasets, providing logical explanations for these shortcomings."}, "weaknesses": {"value": "1. **Presentation is confusing:** As noted under \"Presentation Quality,\" the authors’ heavy reliance on CLS theory (as a narrative device) **obfuscates a clear understanding of the technical contribution**.  \n\n2. **Limited conceptual novelty:** The paper’s main weakness lies in its core inspiration. The CLS-inspired dual-network (fast/slow) architecture has been explored in the **continual learning literature** (e.g., DualNet). The authors should clarify in the related work section how their approach **differs from these prior CLS-inspired models**.  \n\n3. **Limitations of the strategy:** As acknowledged by the authors, AFT-Net's **gradient-aware** strategy fails in cases of very slow or subtle **incremental drift** (e.g., the Hyperplane dataset). This suggests that the strategy **relies on sufficiently strong gradient signals**, which limits its general applicability."}, "questions": {"value": "1. The core idea of a CLS-inspired fast/slow dual-network already exists in prior work (e.g., DualNet). Could the authors more clearly explain how your contribution—such as the **specific implementations of AFT-Net and MRN-Net**—differs from these existing CLS-inspired architectures, and what the **key distinctions and advantages** are?  \n\n2. Given that the **gradient-aware** strategy fails under subtle incremental drift (e.g., Hyperplane), does this imply a fundamental limitation? Have you considered potential solutions, perhaps by introducing a mechanism that does **not entirely rely on strong gradient signals** to complement AFT-Net’s activation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "A9nl7Jkf10", "forum": "ksCYO3GRUY", "replyto": "ksCYO3GRUY", "signatures": ["ICLR.cc/2026/Conference/Submission23128/Reviewer_WAmY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23128/Reviewer_WAmY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902683076, "cdate": 1761902683076, "tmdate": 1762942523506, "mdate": 1762942523506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MetaOCDN, a novel approach for online concept drift adaptation inspired by the Complementary Learning Systems (CLS) theory from neuroscience. The authors draw parallels between the human brain's hippocampus-neocortex collaboration and a dual-network architecture for streaming data. Specifically, MetaOCDN consists of two components: (1) an Adaptive Fine Tuning Network (AFT-Net) that simulates the hippocampus for rapid adaptation to new concepts through gradient-aware selective fine-tuning, and (2) a Meta Representation Network (MRN-Net) that simulates the neocortex for extracting structured knowledge using self-supervised duality loss. The authors further design a MAML-based multi-scale knowledge distillation strategy to facilitate knowledge transfer between these networks. The paper includes theoretical analysis showing sublinear regret bounds and extensive experimental validation across multiple classification and regression tasks with concept drift, demonstrating consistent improvements over state-of-the-art baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper makes a compelling connection between neuroscience (CLS theory) and machine learning for concept drift adaptation, providing a biologically inspired framework that addresses a critical challenge in streaming data analysis.\n\n2. The gradient-aware selective fine-tuning strategy represents a significant contribution. The authors' insight that different layers respond differently to concept drift (demonstrated in Figure 2) leads to a more efficient adaptation mechanism than full fine-tuning."}, "weaknesses": {"value": "1. While the paper acknowledges poor performance on the Hyperplane dataset (incremental drift), it lacks sufficient analysis of why this occurs and how the method could be adapted for this important drift type. The explanation that \"the AFT-Net tends to freeze more layers\" is insufficient without concrete evidence or proposed solutions.\n\n2. The paper doesn't provide a detailed analysis of the computational and memory overhead introduced by maintaining two networks and the knowledge distillation process. This is crucial for real-world deployment considerations.\n\n3. There's limited discussion about the sensitivity of the method to key hyperparameters (e.g., β for balancing similarity/difference loss, the drift-aware threshold parameters). Understanding how these parameters affect performance would strengthen practical applicability.\n\n4. While the paper compares with many baselines, it's unclear if the most recent state-of-the-art methods specifically designed for concept drift (from the last 6-12 months) are included in the comparison.\n\n5. The paper lacks discussion about practical constraints such as memory requirements for storing historical samples, computational constraints in real-time applications, and how the approach scales to high-dimensional data."}, "questions": {"value": "- Theorem 1's claim that \"full fine-tuning yields a non-zero loss at all times\" seems overly strong. While the proof shows full fine-tuning may not reach zero loss due to approximation error, there are scenarios where full fine-tuning could converge to zero loss on the current distribution (though it would likely forget previous knowledge). This distinction should be clarified.\n\n- There's insufficient analysis of failure cases beyond the incremental drift issue.\n\n- More visualization of how the gradient-aware selective fine-tuning actually operates on different drift types would strengthen the claims.\n\n- Technical sections are not easy to follow. It is suggested to improve the writing logic and add some figures to explain the method more clearly. \n\n- Many grammatical issues exist (e.g., \"we analysis\" instead of \"we analyze\" on page 2).\n\n- The connection between neuroscience concepts and technical implementation could be strengthened in the methodology section.\n\n- The related work section can be improved from the following aspects. Some recent papers on concept drift from 2024 (beyond those cited) might be missing. The connection to continual learning literature could be expanded, as concept drift shares significant overlap with continual learning challenges. The paper could better position itself relative to meta-learning approaches specifically designed for concept drift adaptation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dB0dxOQvWi", "forum": "ksCYO3GRUY", "replyto": "ksCYO3GRUY", "signatures": ["ICLR.cc/2026/Conference/Submission23128/Reviewer_dxkh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23128/Reviewer_dxkh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952067315, "cdate": 1761952067315, "tmdate": 1762942523174, "mdate": 1762942523174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a cognition-inspired meta-learning framework called MetaOCDN to address the problem of online continual learning under concept drift in open environments. The method draws on the human brain's Complementary Learning Systems (CLS) theory, analogizing the collaborative mechanism between the hippocampus and neocortex as two sub-networks: AFT-Net, which is responsible for rapidly adapting to new concept changes, and MRN-Net, which extracts stable structured knowledge from historical samples. These two networks interact through MAML-based multi-scale knowledge distillation. Extensive experiments show that MetaOCDN outperforms existing mainstream methods in both classification and regression tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.\tThe paper is highly innovative, combining the CLS theory from cognitive science to design the AFT-Net and MRN-Net dual-network architecture, and proposing the gradient-aware selective fine-tuning strategy.\n\n2.\tThe theoretical analysis of the paper is rigorous: it provides proofs for the convergence and regret bound of the selective fine-tuning strategy, enhancing the theoretical depth.\n\n3.\tThe paper validates the effectiveness of the MetaOCDN model on multiple standard datasets with different types of drift, and designs ablation experiments and convergence analysis, making the results convincing."}, "weaknesses": {"value": "1.A brief explanation of the P_t (X，y) function in Section 3 is recommended. Additionally, the formulas in Sections 3.2-4.2 are dense, so it is suggested to include some intuitive illustrations to aid understanding.\n\n2.Some details in the paper need attention. In Table 1, the ETtml column should indicate the second-best results, consistent with the other columns.\n\n3.The paper proposes using the gradient-aware selective fine-tuning strategy to improve computational efficiency, but the computational complexity of MetaOCDN is not analyzed in detail in the paper."}, "questions": {"value": "1. Learning the data stream should concern both the trade-off of accuracy and computational complexity. So, the computational complexity of the proposed learning method is required.\n\n2. There are hyperparameters in the paper, like $\\beta$, the author should explain how to establish the hyperparameter, and a parameter sensitivity analysis is needed.\n\n3. Since the proposed method is designed based on MAML, please explain the difference between the proposed method and traditional multi-task learning method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "QwQ2nxX76d", "forum": "ksCYO3GRUY", "replyto": "ksCYO3GRUY", "signatures": ["ICLR.cc/2026/Conference/Submission23128/Reviewer_djQt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23128/Reviewer_djQt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990128260, "cdate": 1761990128260, "tmdate": 1762942522866, "mdate": 1762942522866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MetaOCDN is a framework for online learning under concept drift, where data distributions change over time. Authors draw inspiration from the human brain which learns using fast adaptation via the hippocampus and stable representations via the neocortex. The authors propose two specialized networks AFTnet and MRNnet. AFTnet adapts to new data using a gradiant aware selective fine tuning method that updates key layers affected by the drift. MRNnet learns generalizable representations using historical samples using a self supervised duality loss to improve stability and feature extraction over time. Both the networks interact through a MAML based knowledge distillation framework"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- the motivation for the method is well explained, with the 2 networks mimicking hippocampus and neocortex functionality from brain\n- comprehensive evaluation across both classification and regression drift benchmarks with ablation studies"}, "weaknesses": {"value": "- \n- The paper does not benchmark against recent large-scale online continual or meta-learning approaches [1]. Also does not compare against CLS theory based continual learning frameworks [2]\n- Most of the methods in comparison employ one network, so how to fairly compare against this method that utilized 2 networks and distillation approach?\n- The MAML-based bi-level optimization and multi-scale distillation can be computationally heavy for high-dimensional streams;?\n- Need more details about the replay buffer. How many samples, how does it affect the learning?\n- Results depend heavily on the gradient sensitivity threshold and layer-freezing heuristic. Need more details on drift-aware threshold and its sensitivity. \n\n\n[1] When Meta-Learning Meets Online and Continual Learning: A Survey\n[2] Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System"}, "questions": {"value": "- The authors argue that the full online gradient descent is O(d) complexity and that freezing layers improves response efficiency, but they do not provide analysis of computing per-layer gradients every step, maintaining the history matrix, and running the selection rule\n- Lemma 1 uses d_orth but it is not defined anywhere.\n- Line 242 - what are these multi-scale units and why we split it like that?\n-  Why is sparsity done layer-wise and not at neuron-level?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r4yowGxlgB", "forum": "ksCYO3GRUY", "replyto": "ksCYO3GRUY", "signatures": ["ICLR.cc/2026/Conference/Submission23128/Reviewer_B8o5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23128/Reviewer_B8o5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995940667, "cdate": 1761995940667, "tmdate": 1762942522543, "mdate": 1762942522543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}