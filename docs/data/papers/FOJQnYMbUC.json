{"id": "FOJQnYMbUC", "number": 5581, "cdate": 1757921187984, "mdate": 1759897966697, "content": {"title": "MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts", "abstract": "The current paradigm for reasoning in large language models (LLMs) involves models \"thinking out loud\" via a sequence of tokens, known as chain-of-thought (CoT). This approach, while effective, has several significant drawbacks. Firstly, inference requires autoregressive generation of often thousands of CoT tokens, which is slow and computationally expensive. Secondly, it constrains reasoning to the discrete space of tokens, creating an information bottleneck across reasoning steps. Thirdly, it fundamentally entangles reasoning with token generation, forcing LLMs to \"think while speaking,\" which causes potentially short-sighted reasoning. In light of these limitations, we re-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our approach, rather than autoregressively generating tokens, we model reasoning as a hidden Markov chain of continuous, high-dimensional \"thoughts\". Each reasoning step involves a transition of the internal thoughts, where explicit reasoning steps (which may consist of hundreds of tokens) serve as observable variables, which are windows to peek into the implicit thoughts. Since this latent process is incompatible with standard supervised learning, we further propose a two-phase variational training scheme. Our experiments on three benchmarks demonstrate that MARCOS outperforms existing continuous reasoning methods and, for the first time, achieves performance comparable to token-based CoT, even surpassing it by $4.7$\\% on GSM8K with up to $15.7\\times$ speedup in inference. Beyond this, MARCOS offers additional advantages, such as step-level instead of token-level control over stochasticity, opening significant opportunities for reinforcement learning and reasoning in LLMs.", "tldr": "", "keywords": ["Latent Reasoning", "Markov Chain", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9a3b62f4f14e73e0fd33745e260f1d294b90cc18.pdf", "supplementary_material": "/attachment/336437e04108f22c9208bab94449c9ff39369ac3.zip"}, "replies": [{"content": {"summary": {"value": "MARCOS is a new method for continuous (latent) reasoning, as opposed to token-based chain-of-thought (CoT). It reasons in a latent space of neurons (deep and shallow) for K steps, with iterative transitions of continuous thoughts with incorporated randomness. It reports results superior to CoT, and other continuous reasoning methods in a 'from-scratch' training setting (see weaknesses)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The novelty of the method is a major strength. It is quite different from other continuous reasoning methods proposed, such as iCoT,  Coconut or CoLaR: as said above it reasons in a latent space of neurons (deep and shallow) for K steps, rather than being autoregressively similar to token generation (while using continuous vectors). Results in the setting provided are good (but see weaknesses)."}, "weaknesses": {"value": "The GSM8k accuracy numbers seem very low? I mean for all methods, including the baselines. Even in the COCONUT paper (which is relatively old by now) they reached 43% for the baseline, and 34% for COCONUT, although I understand those aren’t based on text (equations)? Shouldn’t you start with modern Qwen baselines that are much better than this?\nI think 3B models with CoT should be in the 80-90% range (not ~13%)?  It also looks very strange that GSM8k doesn’t improve from 0.5B -> 3B?\n\nWhen you say trained from scratch, do you mean with only the GSM8k data, or with pretraining data? The whole point of, and performance gains from CoT, I believe are because it takes advantage of strong pretraining data, which includes human thoughts. I believe this is why it’s hard to match with continuous thoughts, unless we figure out how to match that with a similar principle or idea…  Therefore, not comparing to standard methods of training here seems to be not a good comparison.\n\nSmall things:\n\nVery first word of first sentence of intro:  “large” -> “Large”\n\n“leaving little room for thoughtful planning beforehand” – I think CoT has been shown to provide good planning no? And the citations are pre-CoT (1993)?\n\nGenerally the method description is hard to follow (which is always hard on something fundamentally different, but still, extra care must be made to make it super clear I think)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Vl7iAhpYzy", "forum": "FOJQnYMbUC", "replyto": "FOJQnYMbUC", "signatures": ["ICLR.cc/2026/Conference/Submission5581/Reviewer_yNrV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5581/Reviewer_yNrV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758939323, "cdate": 1761758939323, "tmdate": 1762918145688, "mdate": 1762918145688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Reasponse [1/2]"}, "comment": {"value": "Dear AC and all reviewers,\n\nThank you all for taking the time reviewing our paper! We are particularly encouraged by the reviewers’ recognition of the following strengths of our paper:\n- **Novelty**: “novel architecture” (Reviewer ifwg), “theoretical setup is clean” (Reviewer PWgj), “novel paradigm, novel blend of ideas” (Reviewer 3HPJ), “novelty is a major strength” (Reviewer yNrV).\n \n- **Effectiveness**: “good piece of analysis and promising direction for interpretable control” (Reviewer ifwg), “results are favorable” (Reviewer PWgj), “efficiency gains, significant speedups” (Reviewer 3HPJ), “ablations are comprehensive” (Reviewer 3HPJ), “results are good” (Reviewer yNrV).\n \n- **Significance**: “well-motivated, identifies the limitations of autoregressive, token-by-token CoT reasoning” (Reviewer ifwg), “a valuable research direction” (Reviewer ifwg), “further headroom” (Reviewer PWgj).\n\nWe’ve noticed that although all reviewers think the proposed method (MARCOS) is novel, effective, and significant, 3 out of 4 reviewers gave the score \"2\" as the overall rating. The most critical argument is that `we did not pretrain MARCOS to compare it with stronger baselines that are pretrained on massive data`. Another problem is that `in our ablation, removing the sparsity loss can cause dramatic decrease in performance, so it looks like other parts of the model are not necessary`. In this general response, we will try to explain why we do not have results of pretrained models, and why sparsity is so crucial in the training of MARCOS.\n\n**Common Response 1: Compare MARCOS with Pretrained Baselines**\n\nMARCOS has a very different structure from existing reasoning models. Unlike traditional CoT, where LLMs reason by progressively generating discrete tokens, MARCOS thinks in a high-dimensional continuous space. To do so, we disentangle `thinking` and `speaking` processes in traditional LLMs, where MARCOS first thinks in continuous space and then translates the latent reasoning step to natural language. Consequently, MARCOS can think freely without being restricted by the information bottleneck caused by discretization. Meanwhile, `speaking` becomes optional. Like a real human, MARCOS can think silently, and only decode intermediate thoughts when asked to. This leads to up to 15x speed up and higher reasoning accuracy simultaneously compared with traditional LLMs as shown in Table 1.\n\nThough these significant structural innovations can bring exciting benefits, they also make MARCOS incompatible with the existing LLM structure. The main difference is that MARCOS is learning a stochastic high-dimensional continuous mapping, instead of mappings between discrete tokens. We have tried to initialize MARCOS with weights from pretrained LLMs, such as Qwen-2.5, but found that training got stuck quickly at local minimums. For other baselines compared in the paper, they are essentially compressing the CoT tokens into continuous representations, making it possible to gradually transform a token-based LLM to them, which is not the case for MARCOS. As a result, **we train MARCOS from scratch** on medium-sized datasets as a verification of the concept, which has shonw the potential of MARCOS.\n\nWe fully agree that large-scale pre-training is needed to show that MARCOS also works well on scale, and could compete with SOTA token-based LLMs in practice. In fact, we have been working on the pre-training of MARCOS since a month ago, aiming to train it on at least 1T tokens, which will take about 4 more months given our current computation power. \n\nBut to address these concerns, inspired by Reviewer 3HPJ, we supplement an experiment by initializing the Understander and Speaker in our MARCOS using the pretrained Qwen2.5-0.5B. The baselines are also initialized with this checkpoint. It is important to note that the results of baselines can vary due to differences in the backbone used in different papers. For example, in [1], the accuracy of Coconut on GSM8K is 34%, while in [2], after switching to Llama3.2, the accuracy drops to 23%. As shown in the table below, our results are similar to those in [2], confirming that our implementation is correct. Furthermore, under these conditions, the performance of our model is still significantly superior!\n| | GSM8K|SVAMP|MultiArith|\n|-|-|-|-|\n|iCoT-SI |13.9|47.3|30.5|\n|coconut|23.6|50.8|46.7|\n|CoLaR|17.2|40.3|48.3|\n|MARCOS|24.1|48.5|49.2|\n|MARCOS(text)|26.6|57.2|48.7|\n\nAs a result, we believe MARCOS opens up a new paradigm for complex reasoning. which should be of interest to researchers and engineers working on LLM reasoning and structure. \n\n[1]Training Large Language Models to Reason in a Continuous Latent Space\n\n[2]Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains"}}, "id": "8Z3YMG4rHt", "forum": "FOJQnYMbUC", "replyto": "FOJQnYMbUC", "signatures": ["ICLR.cc/2026/Conference/Submission5581/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5581/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5581/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763425807970, "cdate": 1763425807970, "tmdate": 1763427150514, "mdate": 1763427150514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MARCOS (Markov Chain of Continuous Thoughts), a novel paradigm for reasoning in LLMs that models the reasoning process as a hidden Markov chain over continuous representations rather than discrete token sequences. The key innovation is separating transitions between continuous thought states, or the \"thinking\" process from \"speaking\" (translating thoughts to natural language), with explicit modeling of randomness via variational modules. A variational module introduces a step-level random variable to control stochasticity and allow interpretable knobs over depth, verbosity, and format. The method is evaluated on mathematical reasoning benchmarks (GSM8K, SVAMP, MultiArith), achieving performance comparable to or exceeding token-based Chain-of-Thought with significant speedups."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The explicit separation of thinking and speaking tokens with use of variational modeling of randomness is a novel blend of ideas, and the $R_k$ module gives step-level control of randomness.\n2. MARCOS has efficiency gains, reporting significant speedups, and show that non-autoregressive speaking still holds up reasonably.\n3. Ablations are comprehensive and show how each component matters."}, "weaknesses": {"value": "1. The main limitation is that all models were trained from scratch on the GSM8k-Aug dataset, making it difficult to quantify how MARCOS would perform against a pretrained LLM. It is unclear whether this paradigm is compatible with large-scale pretraining or if it only works when trained from scratch on a specific task.\n2. The evaluation is confined to arithmetic word problems. These tasks are highly structured and the steps of reasoning are well-defined. Moreover, the mathematical reasoning tasks tested on are also relatively simple tasks, so MARCOS does not establish competitiveness on harder tasks such as MATH and AIME.\n3. The approach depends heavily on $R_k$ sparsity, leading to model collapse without it, yet provides limited theoretical justification for why this bias yields stable, disentangled \"factors\" of reasoning."}, "questions": {"value": "1. The approach currently uses a fixed number of reasoning steps, K=3. How would a dynamic K be potentially implemented as this is crucial for problems requiring more reasoning steps?\n2. Have you tried mixture-of-Gaussians or flows for $R_k$​? If not, what evidence indicates the Gaussian prior with nonlinear mapping suffices to capture multi-modal path distributions?\n3. The train from scratch evaluation makes it difficult to compare against the standard practice of finetuning large pretrained models . Have you experimented with initializing the \"Understander\" and \"Speaker\" modules from a pretrained model (e.g., Qwen2.5-0.5B) and only training the \"Thinker\" and variational components? If so, how did this impact performance and training stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c4hMNt5SUM", "forum": "FOJQnYMbUC", "replyto": "FOJQnYMbUC", "signatures": ["ICLR.cc/2026/Conference/Submission5581/Reviewer_3HPJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5581/Reviewer_3HPJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938058383, "cdate": 1761938058383, "tmdate": 1762918145280, "mdate": 1762918145280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper decouples an LM’s internal and external thought processes by casting them as the latent and observed variables of a conditional Hidden Markov Model (cHMM). The authors use supervised CoT training to learn weights that maximize data likelihood via a summed ELBO—one term per supervised time step in the cHMM. They compare favorably to other supervised CoT methods, including Coconut and CoLaR."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "* The theoretical setup is clean and helps justify what might otherwise feel like an unusual number of training components.\n* Results are favorable relative to CoLaR, which is already a strong supervised baseline.\n* All results are pre-RL, so there may be further headroom."}, "weaknesses": {"value": "* Parts of the presentation are unclear, with some important details missing (see Question).\n* I would prefer more evaluations in place of Figures 4–6, which focus on interpretability that seems tangential. In particular, please investigate why the sparsity penalty is so crucial—it feels like an outlier in an otherwise elegant design. Would an entropy-based penalty perform similarly?"}, "questions": {"value": "What exactly drives the reported 15.7× inference speedup? Even if intermediate text is not always emitted, generation still proceeds stepwise, so I would expect comparable latency unless speaking is largely bypassed. From the appendix I see K=3, but how many tokens did CoT-SFT produce on average? Are latent steps more stepwise-efficient than CoT tokens, or is the speedup primarily from answer-only decoding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "w7kGB5H1Z6", "forum": "FOJQnYMbUC", "replyto": "FOJQnYMbUC", "signatures": ["ICLR.cc/2026/Conference/Submission5581/Reviewer_PWgj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5581/Reviewer_PWgj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991514645, "cdate": 1761991514645, "tmdate": 1762918145063, "mdate": 1762918145063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MARCOS, a new reasoning paradigm for large language models. The goal is to replace token-based Chain-of-Thought (CoT) with a continuous-space model. The authors propose modeling reasoning as a \"conditional hidden Markov model (cHMM)\" where latent, high-dimensional \"thoughts\" transition independently from the \"speaking\" process of generating tokens. The model is trained with a two-phase variational scheme. The authors claim this approach achieves performance comparable to or exceeding token-based CoT and other continuous reasoning methods, while being significantly faster."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-Motivated Problem: The paper correctly identifies the limitations of autoregressive, token-by-token CoT reasoning: it is slow, entangles reasoning with token generation, and creates an information bottleneck. The goal of decoupling thinking and speaking is a valuable research direction.\n\n2. Novel Architecture: The core idea of a dual-component latent space ($Neu^{deep}$ and $Neu^{shallow}$) and the use of a variational framework to manage randomness ($R_k$) at the step level rather than the token level is interesting.\n\n3. Analysis of Randomness: The analysis in Section 4.4, which attempts to show how different dimensions of the random variable $R_k$ control distinct properties like sentence length and format, is a good piece of analysis and a promising direction for interpretable control."}, "weaknesses": {"value": "1. Inverted scaling in Table 1:\nA critical issue is the evidence of inverted scaling in Table 1. For nearly all continuous baselines (Coconut, CoLaR, CODI), the 3B parameter models perform significantly and anomalously worse than their 0.5B counterparts. This inverted scaling is not expected and suggests a deep issue with the training or implementation of these larger baselines, rendering them invalid as good-faith comparison points.\n\n2. Baseline performance:\n The reported scores for baselines like Coconut seem significantly lower than those reported in their original papers. \n\n3. The paper introduces a large-scale, complex framework (cHMM, randomness encoders/predictors, $Neu^{deep/shallow}$) without adequately justifying why this specific formalism is necessary. It is not clear why a simpler recurrent state-space model would not suffice. The paper is heavy on new jargon, making it difficult to position this work relative to the vast existing literature on VAEs, latent variable models, and structured prediction."}, "questions": {"value": "1. In Table 1 ablations, without sparsity loss, the model performance drops dramatically. It makes me wonder the necessity of ELBO formulation of the problem. Also it's unclear whether there exists the issue of posterior collapse. To me, the model's success is an artifact of sparsity prior rather than variational framework itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ugCXDTkOIE", "forum": "FOJQnYMbUC", "replyto": "FOJQnYMbUC", "signatures": ["ICLR.cc/2026/Conference/Submission5581/Reviewer_ifwg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5581/Reviewer_ifwg"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission5581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762496857951, "cdate": 1762496857951, "tmdate": 1762918144622, "mdate": 1762918144622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}