{"id": "URtz3JhoWh", "number": 23345, "cdate": 1758342458319, "mdate": 1759896819759, "content": {"title": "LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?", "abstract": "Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address\nthese issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing human; (3)\nplanned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 32 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81st percentile, a strong result that nonetheless falls short of top human contestant performance, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. We have made the code of our benchmark accessible at https://anonymous.4open.science/r/LiveOIBench-25F9/.", "tldr": "Use Informatic Olympiads to benchmark LLMs with human contestant performance", "keywords": ["Coding Benchmark", "Competitive Coding", "Test-time Scaling", "Informatic Olympiads"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/964bbeece533847cc029bbb900f0e97c5956847b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LiveOIBench, a comprehensive benchmark designed to evaluate the coding and algorithmic reasoning capabilities of large language models (LLMs). The benchmark addresses several critical limitations of existing coding datasets, such as test case insufficiency, lack of exceptionally difficult problems, and reliance on external APIs that hinder reproducibility. Furthermore, the benchmark integrates official human contestant data, enabling direct comparisons through metrics like human percentiles and ELO ratings. The authors' evaluation of 32 models reveals that even top performers like GPT-5, while achieving an impressive 81st percentile, still lag behind elite human contestants, highlighting substantial room for progress in LLM reasoning capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents a much-needed, high-quality benchmark for evaluating the performance of frontier large language models (LLMs) on competitive programming–style (Informatics Olympiad) problems, which demand deep algorithmic reasoning and structured problem solving.\n* While prior works such as OI-Bench and LiveCodeBench have explored similar domains, the paper clearly acknowledges these efforts and articulates how LiveOIBench advances beyond them through improved data curation, richer evaluation metrics, and stronger reproducibility.\n* A key distinguishing feature is that LiveOIBench is “live” — it continuously incorporates newly released Olympiad contests, ensuring ongoing relevance and contamination-free updates. Moreover, it uniquely benchmarks LLMs directly against human contestant performance, offering an interpretable and intuitive comparison framework\n* The breadth and diversity of the dataset — spanning 403 problems from 72 contests across 14 Olympiads — significantly enhances the benchmark’s robustness and difficulty spectrum, making it a valuable resource for the community.\n* The evaluation is comprehensive, covering 32 leading proprietary and open-weight models, and complemented by detailed analyses of algorithmic categories (e.g., dynamic programming, graph theory) and reasoning behavior."}, "weaknesses": {"value": "While this is an excellent paper with a significant contribution, there are several areas where it could be strengthened to improve its clarity, reproducibility, and the impact of its conclusions. Listed a few below: \n\n* While the paper includes ablations across different Pass@k values and reasoning budgets, it does not examine the sensitivity of results to prompt structure or instruction phrasing. Given the well-known prompt sensitivity of LLMs, an analysis using alternative templates (e.g., explicit “think step-by-step” or language-specific wrappers) would strengthen the conclusions about model robustness and fairness under different interaction styles.\n* Will be good if authors can add generation parameters such as temperature and top_p in the draft. Explicitly listing these values (even in the appendix) would enhance transparency.\n* While the paper mentions that the “full problem statement is presented to the models,” it does not provide the exact prompt wrapper or instruction format used to elicit code generation (e.g., whether the prompt included directives like “write code in C++ that reads from stdin”). Adding this will enhance the readability. \n* The paper does not explicitly mention which programming language the models were instructed to generate code in. I am assuming the outputs were in C++ (correct me if I am wrong), since Olympiads such as IOI and JOI traditionally restrict submissions to that language. However, it would be interesting to see whether the models were allowed to generate solutions in other languages such as Python or Java, as contests like USACO permit multiple languages. Add this detail will help. \n* Also, evaluating cross-language performance could provide valuable insight into the models’ adaptability and coding generalization abilities. Authors may consider adding this in the draft. \n* The reasoning-trace study (Sec. 5.2) classifies model behavior into five categories using GPT-OSS-120B itself, but there is no error analysis or validation performed on how accurately GPT-OSS-120B-High classifies the reasoning traces into these buckets. Authors may consider adding this in the draft. While it would be more robust to involve multiple LLMs for this classification task and compute an inter-annotator agreement metric (such as Cohen’s κ) to assess consistency, even a smaller-scale manual validation or qualitative error analysis would strengthen the credibility of the findings. For instance, the paper could include an example reasoning trace generated by one of the LLMs on a particular problem and then illustrate how GPT-OSS-120B-High assigns it to the five reasoning categories, making the analysis more transparent and interpretable."}, "questions": {"value": "I encourage the authors to address the specific weaknesses raised above, as clarifications or additional evidence on these points could substantially strengthen the paper. In particular, I would appreciate responses or additional details on:\n\n1. What programming languages were allowed, and what prompt template was used (include the wrapper text such as ‘write C++ code that reads from stdin’)?\n\n2. What were the decoding hyper parameters used during evaluation?\n\n3. Clarify the update window and whether a hidden, rotating holdout will be kept for future releases? \n\n4. Will be great if authors can conduct qualitative analyses with multiple prompt variations to evaluate the stability and robustness of model performance under different prompting conditions, and report the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fqXGkWKEW6", "forum": "URtz3JhoWh", "replyto": "URtz3JhoWh", "signatures": ["ICLR.cc/2026/Conference/Submission23345/Reviewer_RVy3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23345/Reviewer_RVy3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699233181, "cdate": 1761699233181, "tmdate": 1762942617891, "mdate": 1762942617891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LiveOIBench, a competitive programming benchmark derived from Informatics Olympiad contests. It comprises 403 problems with detailed subtask rubrics and expert curated private test cases, supporting offline evaluation. The authors benchmark 32 models and find that GPT-5 achieves the best average performance yet still exhibits a marginal gap compared to top-tier human contestants. They also observe that thinking models outperform non-thinking variants and that higher-performing models allocate more tokens to analysis rather than implementation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper accurately identifies limitations of current code benchmarks and constructs a high-quality benchmark that addresses those issues.\n\n- LiveOIBench contains 400+ competitive problems, which is large-scale and high-difficulty relative to existing datasets and can provide a valuable evaluation framework for future works.\n\n- The paper conducts substantial experiments and surfaces several clear empirical insights."}, "weaknesses": {"value": "- Because all problems come from real OI contests, newer models may have been trained on these problems and their solutions, which could compromise the reliability of the experimental results due to potential data contamination.\n\n- There may be mismatches or missing mappings between contestants and their Codeforces profiles, which could affect the accuracy of Codeforces Elo computations and percentile estimates."}, "questions": {"value": "1. The paper states (around line 200) that 40 tasks were sampled to check PDF-to-Markdown conversion accuracy. What were the results, and how do you ensure high conversion quality for the remaining tasks?\n\n2. The paper does not specify the programming languages used during evaluation. Since contestants may use a range of languages, what is the impact of language choice on the reported metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fLPmOC3m0c", "forum": "URtz3JhoWh", "replyto": "URtz3JhoWh", "signatures": ["ICLR.cc/2026/Conference/Submission23345/Reviewer_faYZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23345/Reviewer_faYZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850564082, "cdate": 1761850564082, "tmdate": 1762942617481, "mdate": 1762942617481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper pulls together a high quality dataset to test the reasoning ability of frontier LLMs on difficult coding problems and compares their performance to top human reasoners. It does extensive work to setup a website that will pull new challenging problems in from future coding contests so that a fresher dataset is always available to counter the contamination that happens as older test sets and their solutions spread on the internet."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A very challenging dataset of coding infomatic style problems is made, with a nice website with a leaderboard to compare various LLMs performance. The dataset is contamination resistant by being designed to be updated with new problems from future contests."}, "weaknesses": {"value": "I would have liked to see more baselines for other model providers like Anthropic or X.AI"}, "questions": {"value": "Can you add baselines for the other major closed models?  I think the performance of more frontier models on the dataset would make the paper more valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CxNdC1KB2q", "forum": "URtz3JhoWh", "replyto": "URtz3JhoWh", "signatures": ["ICLR.cc/2026/Conference/Submission23345/Reviewer_rxTf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23345/Reviewer_rxTf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150190968, "cdate": 1762150190968, "tmdate": 1762942616799, "mdate": 1762942616799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LiveOIBench, a benchmark of 403 Olympiad-level coding problems from 72 contests to evaluate LLMs’ algorithmic reasoning. It features expert-written test cases, subtask rubrics, and human contestant data. Evaluations of 32 models show GPT-5 reaches the 82nd human percentile, still below top human coders. The benchmark is comprehensive, reproducible, and contamination-free, offering a new standard for assessing LLM coding ability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Expert-curated Olympiad problems with rich subtasks and reliable private tests.\n2. Direct comparison to human contestants enables meaningful percentile evaluation.\n3. Comprehensive, reproducible benchmark revealing detailed reasoning and algorithmic insights."}, "weaknesses": {"value": "1. The analysis and conclusion are not deep, providing limited new insights beyond benchmark construction.\n2. The paper mainly focuses on dataset creation and evaluation, making it more suitable for a dataset or benchmark track rather than the main ICLR track."}, "questions": {"value": "1. How do you ensure the fairness and consistency of problem difficulty and judging standards across the 14 Olympiads (e.g., NOI vs. ICPC-style tasks), given their distinct styles and scoring rubrics?\n2. Could you elaborate on why C++ appears to yield better outcomes in your experiments—do models genuinely reason differently in C++, or is this advantage primarily due to compilation or runtime efficiency effects?\n3. Have you analyzed whether model size, reasoning token allocation, or RL/distillation strategies correlate more strongly with success on algorithmic tasks like dynamic programming?\n4. Given the benchmark’s reliance on Olympiad data, what measures are in place to detect or mitigate data leakage, especially since some contest problems or close variants might exist in public training corpora used by large models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LAgWlFjRFm", "forum": "URtz3JhoWh", "replyto": "URtz3JhoWh", "signatures": ["ICLR.cc/2026/Conference/Submission23345/Reviewer_Xz3c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23345/Reviewer_Xz3c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177248670, "cdate": 1762177248670, "tmdate": 1762942616013, "mdate": 1762942616013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### Paper Summary  \nThis paper presents LiveOIBench, a competitive coding benchmark designed to evaluate large language models (LLMs)’ algorithmic reasoning and programming capabilities. It curates 403 high-quality tasks from 72 official Informatics Olympiad competitions (2023–2025), complemented by abundant official test cases. The benchmark evaluates 32 LLMs across closed-source/open-source and thinking-enabled/non-thinking-enabled categories, using human-centric metrics to compare models against human competitors.  \n\n### Core Contributions  \n1. Constructs a large-scale, reproducible benchmark leveraging recent, authoritative Olympiad tasks, addressing limitations of synthetic or outdated coding benchmarks.  \n2. Introduces multi-dimensional metrics (e.g., Human Percentile, Codeforces ELO) for intuitive human-model performance comparisons, advancing beyond simplistic pass/fail indicators.  \n3. Provides granular insights into model strengths/weaknesses via subtask breakdowns and error mode analysis (e.g., compilation vs. logic errors).  \n4. Validates performance hierarchies across model types, highlighting the value of structured thinking for complex algorithmic tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Subtasks with Scoring Rubrics** By leveraging the \"subtask breakdown\" and \"hierarchical scoring rubrics\" inherent in informatics olympiad problems, the paper breaks down the model's \"overall accuracy rate\" into \"score performance across various ability modules.\" This enables the acquisition of nuanced insights into \"which specific types of tasks the model excels at and which it struggles with,\" rather than merely providing a vague conclusion about \"the model's performance level.\"\n* **Continuous Updates.** The Continuous Updates of LiveOIBench are achieved through five core mechanisms: dynamic task injection, user feedback-driven iteration, versioned management, scoring rule calibration, and delayed release strategy. Its primary goals are to maintain the benchmark’s timeliness, discriminative power, and fairness, while combating data contamination and model overfitting. This design paradigm aligns closely with cutting-edge dynamic benchmarks such as LiveBench and LiveCodeBench, reflecting the latest trends in the current AI evaluation field."}, "weaknesses": {"value": "* **Overclaim** The authors claim that LiveOIBench is `the first comprehensive competitive coding benchmark constructed directly from Informatics Olympiads tasks`—a claim that is not justified given the existence of concurrent work.\n* **Insufficient Justification** This paper does not do enough to mitigate risks of data contamination, as template-based contamination has not been assessed to a reasonable extent."}, "questions": {"value": "Whether large language models (LLMs) reason based on memorization remains highly controversial. Most benchmarks attempt to address contamination issues from diverse perspectives. Unlike mathematical reasoning, the reviewer argues that informatics olympiad tasks may share similar algorithmic templates while having different surface forms. Is a Serendipity Metric—designed to calibrate template similarities—necessary to make the benchmark more contamination-free?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ixS2DcGIKa", "forum": "URtz3JhoWh", "replyto": "URtz3JhoWh", "signatures": ["ICLR.cc/2026/Conference/Submission23345/Reviewer_bPmJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23345/Reviewer_bPmJ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762616332129, "cdate": 1762616332129, "tmdate": 1762942615625, "mdate": 1762942615625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}