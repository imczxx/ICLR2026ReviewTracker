{"id": "DzvPiqh23f", "number": 4671, "cdate": 1757741639521, "mdate": 1763753789209, "content": {"title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation", "abstract": "Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20$\\times$ beyond teacher's capability, avoiding common issues such as over-exposure and error-accumuation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, \nequivalent to 99.9\\% of the maximum span supported by our base model’s position embedding and more than 50x longer than that of our baseline model.  Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms  baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-pp.github.io.", "tldr": "autoregressive long video generation", "keywords": ["long video generation", "diffusion model", "autoregressive video generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60997ed5e2812db1a114f26987e6faa73c174a9b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Self-Forcing++, a training framework for minute-scale autoregressive video generation that mitigates long-horizon degradation without long-video teachers or datasets. It identifies a dual train–test mismatch: models train with dense teacher supervision on short clips but must sustain much longer rollouts at inference, causing compounding errors (motion stalling, exposure drift).  \nSelf-Forcing++ rolls the student out far beyond the teacher’s horizon, re-injects diffusion noise (Backward Noise Initialization) to preserve temporal context, and performs Extended Distribution Matching Distillation by uniformly sampling K-length windows (teacher horizon) from the long rollout for student–teacher alignment. Training consistently uses a rolling KV cache, eliminating cache mismatches and overlapping-frame recomputation. An optional GRPO stage with an optical-flow smoothness reward further enhances temporal continuity.  \nExperiments show substantial gains over CausVid, Self-Forcing, SkyReels-V2, and MAGI-1 on 50/75/100s tasks, especially in motion dynamics and a proposed Visual Stability metric that addresses VBench’s long-video bias. With increased compute, the model scales to 4m15s while maintaining quality."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper clearly diagnoses the core barrier to long-horizon video generation as a dual train–inference mismatch in temporal horizon and supervision density. The overall pipeline is communicated cleanly, with a step-by-step flow (long self-rollout, backward noise re-injection, sliding-window distillation, rolling KV cache). The failure modes of prior work, including motion collapse, exposure drift, and cache-state mismatches, are explained convincingly and tied to design choices.\n2.\tThe method achieves strong, consistent gains over baselines (CausVid, Self-Forcing, SkyReels-V2, MAGI-1) on comparable 50–100s tasks, especially in motion dynamics and exposure/visual stability. With increased training budget, and without long-video data or teachers, it scales to 4m15s (near positional embedding limits) while maintaining coherent motion and stable exposure. \n3.\tThe authors identify and analyze a bias in VBench that can favor over-exposed or degraded frames, which are common failure modes in prior systems. They propose Visual Stability, an evaluation protocol leveraging a strong video MLLM and human verification to better capture long-horizon degradation and exposure issues, thereby improving the reliability of benchmarking for long video generation."}, "weaknesses": {"value": "1.\tThe paper should provide a clearer accounting of computational resources for both training and inference, such as FLOPs, GPU hours, and memory, to enable fair cost–quality comparisons with baselines.\n2.\tWhile the paper includes ablations (attention window, noisy-KV, GRPO), it would be even stronger with finer analyses of key design choices, e.g., alternative window sampling distributions, different noise schedules/intensities for backward initialization, and systematic sweeps over K/N ratios.\n3.\tThere are minor punctuation and typography issues. For example, in the Introduction’s first paragraph the last sentence ends with a comma instead of a period, and around line 202 the phrase \"by $\\Theta$\" is followed by an extra period. A careful proofreading pass to fix such punctuation/formatting inconsistencies would improve presentation quality."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hXPOjF24L6", "forum": "DzvPiqh23f", "replyto": "DzvPiqh23f", "signatures": ["ICLR.cc/2026/Conference/Submission4671/Reviewer_hYZq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4671/Reviewer_hYZq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761458554730, "cdate": 1761458554730, "tmdate": 1762917504934, "mdate": 1762917504934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Self-Forcing++, a method that achieves a landmark breakthrough in the stability of long-horizon autoregressive video generation. It directly confronts the critical issue of quality degradation that occurs when models distilled from short-video teachers are tasked with generating minute-scale sequences. The core contribution is an elegant and highly effective training strategy that requires no long-video data. The method involves prompting the student model to generate long, error-accumulated rollouts, and then using the short-horizon teacher to perform \"segment-wise correction\" on randomly sampled clips from these rollouts. This process effectively teaches the model to recover from its own extrapolation errors.\nEmpirically, the results are groundbreaking. The method extends high-fidelity video generation from a baseline of ~10 seconds to over four minutes, a feat previously considered out of reach. It demonstrates an unprecedented ability to maintain local visual quality, motion dynamics, and temporal coherence over these extended durations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's most significant and undeniable contribution is its success in mechanically extending the stable generation horizon of autoregressive models by an order of magnitude. The ability to produce minute-scale videos without collapsing is a remarkable engineering feat that directly addresses a major bottleneck in the field. This establishes a new, albeit signal-level, state-of-the-art for generation duration.\n2. The conceptual reframing of the long-video problem is the paper's most elegant intellectual contribution. The core insight—that a short-horizon teacher can be repurposed as a \"local error corrector\" for a student's long, degraded rollouts—is a highly non-obvious and clever idea. This formulation provides a novel, data-efficient paradigm for tackling extrapolation challenges.\n3. A key practical finding of this work is the demonstrated correlation between the training budget and the achievable horizon of stable generation (Figure 6). This provides the community with a clear, albeit potentially brute-force, recipe for progress: longer, stable videos can be achieved by investing more computation into the self-correction training loop. This is a valuable and actionable result.\n4. The authors should be credited for their critical examination of existing evaluation protocols. Identifying and attempting to remedy the biases of VBench when applied to long-form video demonstrates a level of scientific rigor and thoughtfulness that is often missing. While their proposed metric is deeply flawed, the motivation to improve evaluation standards is in itself a positive contribution to the community's discourse."}, "weaknesses": {"value": "1. Architectural Constraint: A Bounded Information Horizon via the KV Cache.\nThe model's autoregressive generation relies on a fixed-size rolling KV cache of length L. This mechanism architecturally imposes a strict Markov assumption on the generative process. The conditional probability of generating the current latent state x_t is not conditioned on the entire history x_{<t}, but is instead approximated by p(x_t | x_{t-L:t-1}). Consequently, the model's information horizon is strictly bounded by L. Any semantic dependency or causal relationship requiring the integration of information over a temporal span greater than L (e.g., long-term entity consistency, narrative causality) cannot be modeled, as the requisite information has been discarded from the context window. This is a fundamental architectural limitation, not a matter of training scale.\n2. Objective Function Constraint: Optimization of Local, not Global, Properties.\nThe proposed Self-Forcing++ training objective is formulated as an expectation over uniformly sampled, short, contiguous segments of length K (where K is the teacher's horizon). The loss function is a sum of local distribution matching objectives, each minimizing the KL divergence between the student's and teacher's distributions within that local window: KL(p_θ(x_i:i+K) || p_T(x_i:i+K)). This objective function provides a powerful supervisory signal for enforcing local properties—such as texture fidelity, motion dynamics, and exposure consistency—that are observable within a segment of length K. However, it provides no direct supervisory signal for dependencies between non-overlapping segments, W_i and W_j, where j > i+K. The model is therefore trained to be an expert local generator, but it is not explicitly optimized for global semantic coherence. The observed long-term stability is an emergent property of chaining together high-fidelity local segments, not a result of learning a true global model of the video distribution."}, "questions": {"value": "1. On the Inherent Trade-offs of the Global-to-Local Distillation: Your approach involves distilling knowledge from a bidirectional, full-attention teacher into a causal, autoregressive student. This global-to-local translation necessarily involves trade-offs. Could you comment on what capabilities are inherently lost in this process? For example, a bidirectional model can generate \"perfectly looping\" videos or effects where the beginning is influenced by the end. Are such temporally non-causal phenomena fundamentally beyond the reach of your framework, and do you see this as a necessary price for achieving scalability and streaming capabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nimghCnXfv", "forum": "DzvPiqh23f", "replyto": "DzvPiqh23f", "signatures": ["ICLR.cc/2026/Conference/Submission4671/Reviewer_PrTo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4671/Reviewer_PrTo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642666915, "cdate": 1761642666915, "tmdate": 1762917504698, "mdate": 1762917504698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an algorithm to improve the length extrapolation capability of Self-Forcing-style autoregressive video diffusion models. In Self Forcing, due to the limitation that the teacher model is only trained on short duration, the student perform short-duration rollout (e.g. 5s) and receives supervision from the teacher trained on the same length. Here, the student model performs extended-duration rollout"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper demonstrates strong empirical results with multi-minute-long generation with no significant error accumulation. This is achieved without supervision from long videos by leveraging teacher feedback from short individual video segments generating by the student via self-rollout. \n- The experiments are very comprehensive. The paper not only demonstrates the advantage over prior work (e.g., Self Forcing) but also compares with alternative strategies (noisy kv or local attention) that alleviate error accumulation. The paper also identifies an important issue in existing benchmark (VBench) and proposes a better benchmarking."}, "weaknesses": {"value": "- The description of \"Backwards Noise Initialization\" in Section 3.2 is very unclear and confusing. \n  - Is the \"Backwards Noise Initialization\" term comes from \"backward simulation\" in DMD2? It seems to be the case when I read the sentence \"they used this for... circumvent the need for real training data\". But in Fig. 2, it seems to refer to injecting noise to student output before computing fake/real scores. The noise injection is employed in the original DMD and all follow-up works, is unrelated to \"circumvent real training data\", and should not be an original contribution.\n  - Does t represent noise level or frame index? Sometime it seems to refer to noise level (x_t = (1 − σ_t)x_0 + σ_tϵ) but sometime it seems to refer to the frame index (N clean frames and clean trajectory {x^S_t}_{t=1}^N)\n  - Is there still a fake score network with separate parameters, like in DMD? If so, in eq. 2 the fake score s^S_\\theta should not be denoted with parameter theta - theta is the parameter of the student.\n  - What is S in x_t^S (L198)?\n- The way Figure 2 compares CausVid/SF/SF++ is confusing. The second row of frames of CausVid (frames with independent noise levels) are the input to the **student** model. But the second row of frames of the SF/SF++ appears to be the input to the **teacher** model and the fake score network. The input of the student model for SF/SF++ is not shown and the output of the student model is illustrated in the first row. CausVid should also have the corresponding \"Backward Noise Initialization\" step but it's subsumed in the \"DMD Alignment\" process. In short, the frames depicted in the same row, which are supposed to be side-to-side comparisons, do not correspond to things at the same stage.\n- Would be great if the authors can discuss the relationship with APT2 (Lin et al. 2025), which also uses a similar long video training method to improve long video generation ability."}, "questions": {"value": "- What is the exact thing that is scaled in the \"Training Budget Scaling\" section (4.4)? Is it the batch size, number of iterations, or per-iteration rollout-duration. Could you provide detailed experiment config for each (1x, 4x, 8x, 20x, 25x) training setup?\n- Have you compared the proposed strategy (rollout an entire, long video then uniformly sample a single segment, e.g., rollout 1 min then sample 5s) versus an alternative strategy that rollout one segment at a time, compute loss for it and then rollout another segment (e.g., rollout 5s, compute loss and backprop, then rollout 5s)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HLT5Wnc7mA", "forum": "DzvPiqh23f", "replyto": "DzvPiqh23f", "signatures": ["ICLR.cc/2026/Conference/Submission4671/Reviewer_avME"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4671/Reviewer_avME"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050968909, "cdate": 1762050968909, "tmdate": 1762917504443, "mdate": 1762917504443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}