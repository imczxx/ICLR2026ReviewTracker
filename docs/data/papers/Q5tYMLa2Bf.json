{"id": "Q5tYMLa2Bf", "number": 16365, "cdate": 1758263759363, "mdate": 1759897245529, "content": {"title": "CAMO: Category-Agnostic 3D Motion Transfer from Monocular 2D Videos", "abstract": "Motion transfer from 2D videos to 3D assets is a challenging problem, due to inherent pose ambiguities and diverse object shapes, often requiring category-specific parametric templates.  We propose CAMO, a category-agnostic framework that transfers motion to diverse target meshes directly from monocular 2D videos without relying on predefined templates or explicit 3D supervision. The core of CAMO is a morphology-parameterized articulated 3D Gaussian splatting model combined with dense semantic correspondences to jointly adapt shape and pose through optimization. This approach effectively alleviates shape-pose ambiguities, enabling visually faithful motion transfer for diverse categories. Experimental results demonstrate superior motion accuracy, efficiency, and visual coherence compared to existing methods, significantly advancing motion transfer in varied object categories and casual video scenarios.", "tldr": "", "keywords": ["motion transfer", "3D Gaussian splatting", "differentiable rendering"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b50b6886590bab95a72357e71caf4da8c39d7ff3.pdf", "supplementary_material": "/attachment/4a20dc651e3a5487e0656d3326349aa2d54e4a53.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces CAMO, a category-agnostic 3D motion transfer framework that maps motions from a monocular 2D video onto arbitrary 3D targets—without using category templates or reconstructing source 3D meshes. CAMO represents the target as an articulated 3D Gaussian Splatting model driven by an LBS-based kinematic chain. It learns morphology-adaptive parameters (bone lengths, scale, offsets) to handle shape differences and uses dense 2D–3D semantic correspondences to reduce pose ambiguity. The total loss combines differentiable rendering, keypoint, and regularization terms. Experiments on DT4D and Mixamo show state-of-the-art performance in motion accuracy and visual realism, outperforming template-based and reconstruction-to-retarget baselines, while maintaining fast optimization (<10 min per sequence)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Category-agnostic design: Avoids SMPL/SMAL or per-class priors; directly optimizes on the target with only monocular source supervision. This reduces error cascade from reconstruct→retarget pipelines.\n2. Clarity: The paper is well-presented, easy to follow.\n3. Clear empirical gains: SOTA PMD/FID across datasets; ablations isolate the contributions of rendering loss, shape param., and keypoints; efficient optimization on commodity hardware."}, "weaknesses": {"value": "1. While image-space supervision is attractive, robustness under heavy occlusion or fast motion isn’t systematically quantified; correspondence quality in such regimes is unclear.\n2. Real-world setup relies on a render-and-compare camera initialization. The method’s sensitivity to poor initial camera guesses or to calibration drift is not analyzed.\n3. For experiments, the authors should provide more visualization results. The demo's examples can not convince me for robust results.\n4. About comparison, I'm curious about the comparison with a simple pipeline: using video generation/editing model to get edited object, and then apply dynamic 3D generation model for animation. Can CAMO outperform it in all evaluation aspects?"}, "questions": {"value": "Please see weakness. If all my concerns are well conducted, I'll consider raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C09ExuCAh5", "forum": "Q5tYMLa2Bf", "replyto": "Q5tYMLa2Bf", "signatures": ["ICLR.cc/2026/Conference/Submission16365/Reviewer_S7f1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16365/Reviewer_S7f1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568422686, "cdate": 1761568422686, "tmdate": 1762926491753, "mdate": 1762926491753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CAMO, a template-free, category-agnostic 2D→3D motion transfer method. It avoids reconstruct-then-retarget pipelines by optimizing the target asset directly in image space using (i) an articulated 3D Gaussian representation with morphology-adaptive parameters (bone lengths, global scale, local Gaussian offsets) and (ii) dense 2D to 3D semantic correspondences for disambiguation. Results show lower PMD/FID vs composite baselines (SPT+, NPR+) and Transfer4D across Mixamo & DT4D, plus qualitative real-world demos."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Sound and principled design.\nThe optimization objective (photometric + SSIM + semantic + temporal regularization) is coherent and mathematically well-founded.\nThe choice of directly optimizing in image space with an articulated 3D Gaussian structure is both elegant and technically solid.\n\nClear handling of 3D lifting.\nThe method performs implicit 3D lifting by analysis-by-synthesis, guided by differentiable rendering and dense 2D to 3D correspondences.\nThis eliminates explicit 3D supervision and is novel for the community.\n\nThorough ablations and visualizations.\nThe paper includes extensive quantitative and qualitative analyses (Tables 1–6, Figs. 5–14) demonstrating the necessity of morphology parameters and semantic losses. Failure cases and challenging cases are all presented and analyzed well."}, "weaknesses": {"value": "1. Dependence on rigging quality and pre-processing.\nThe approach assumes access to well-rigged target meshes. Auto-rigging tools (e.g., UniRig, MagicArticulate) introduce noticeable artifacts when bone topology mismatches occur. Is there any way to address this?\n\n2. Temporal scalability and long-sequence degradation.\nThe time-conditioned MLP cannot effectively model long (>600 frame) motion sequences; it gradually drifts or repeats poses. It's better to give some visualised results.\n\n3. Absence of physical realism metrics.\nEvaluation focuses on FID and PMD, but omits motion stability or contact-based metrics (e.g., foot-skating, interpenetration). Without such analysis, claims of realistic motion transfer remain visually but not physically validated.\n\n4. Computation and convergence analysis are limited.\nAlthough the authors report that optimization takes <10 minutes per sequence on an RTX 4090, there is no detailed runtime or convergence study across different mesh complexities or sequence lengths."}, "questions": {"value": "CAMO delivers a robust, well-implemented, and innovative approach to category-agnostic 2D to 3D motion transfer.\nThe technical contribution and experimental validation are solid and can be a good contribution to the community.\nThe paper meets ICLR’s bar for novelty and soundness and is likely to stimulate follow-up work in differentiable 3D motion learning.\nI strongly suggest that the authors open-source the source for reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aaW0fBFcqo", "forum": "Q5tYMLa2Bf", "replyto": "Q5tYMLa2Bf", "signatures": ["ICLR.cc/2026/Conference/Submission16365/Reviewer_gp2U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16365/Reviewer_gp2U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954167980, "cdate": 1761954167980, "tmdate": 1762926491303, "mdate": 1762926491303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a category agnostic method for transferring motion from a 2d monocular video to a 3d target mesh. the method optimizes the pose of a target 3D model, represented by an articulated 3dgs framework, directly in the 2d observation space. The core components are a morphology-adaptive shape parameterization，which includes learnable bone lengths, global scale, and local offsets，and a dense semantic correspondence loss. This correspondence, derived from a pretrained feature extractor, aligns the 3d target with the 2d source."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, structured, and easy to comprehend.\n2. The quality of the results look good.\n3. The methods handle category agnostic motion transfer without parametric models like SMPL and SMAL.\n4. The use of articulated 3DGS for differentiable rendering , the leveraging of foundation models for robust semantic understanding , and the carefully designed morphology parameterization come together to form a reasonable and coherent framework."}, "weaknesses": {"value": "1. Motion is parameterized by an MLP conditioned on a sinusoidal time embedding (Appendix A.2). This architecture is known to struggle with representing very long, complex, or non-cyclic motions, which is confirmed by the paper's own analysis (Appendix A.4) showing degradation on longer sequences.\n2. The method is still vulnerable to fundamental 2D-to-3D ambiguities. The paper's failure cases (Fig. 14) show it can confuse left/right limbs or fail during severe self-occlusion, problems that persist despite the semantic correspondence loss.\n3. Missing references, there are some works with similar settings:\n[1] PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling\n[2] Puppeteer: Rig and Animate Your 3D Models"}, "questions": {"value": "1. Have you analyzed when these ambiguities are most likely to occur? For instance, does the left-right confusion happen most often from specific camera angles where the 2D projection is maximally ambiguous?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "82bte0q79V", "forum": "Q5tYMLa2Bf", "replyto": "Q5tYMLa2Bf", "signatures": ["ICLR.cc/2026/Conference/Submission16365/Reviewer_2Hny"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16365/Reviewer_2Hny"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000083977, "cdate": 1762000083977, "tmdate": 1762926490807, "mdate": 1762926490807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CAMO, a category-agnostic framework for transferring articulated 3D motion directly from monocular 2D videos to arbitrary 3D target meshes, without relying on category-specific templates or explicit 3D supervision.\nThe method builds upon articulated 3D Gaussian Splatting (3DGS) and introduces:\n\nMorphology-adaptive parameterization (learnable bone lengths, local Gaussian offsets, and global scaling).\n\nDense 2D–3D semantic correspondence based on foundation features.\n\nJoint optimization of pose and shape with photometric and semantic losses.\n\nExperiments on Mixamo, DeformingThings4D, and real-world videos show clear improvements over baselines like SPT+, NPR+, and Transfer4D."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Category-agnostic capability.\nThe paper convincingly demonstrates that CAMO generalizes to both humanoids and non-humanoid animals, addressing the typical limitation of category-specific template models.\n\nClear ablations and metrics.\nThe quantitative improvements on PMD and FID are substantial (up to 85% improvement on non-quadruped categories). The ablation study effectively supports the importance of morphology parameterization and semantic keypoint supervision."}, "weaknesses": {"value": "**Evaluation is largely self-contained and lacks cross-domain tests.**\nWhile the results on Mixamo and DT4D are consistent, these datasets are synthetic and often aligned in topology.\nReal-world results (Fig. 6) are qualitative only, without any perceptual or human study evaluation.\nThere’s no analysis of failure cases (e.g., severe occlusions, topology mismatch, or multi-actor scenes).\n\n**Limited discussion on theoretical implications and generalization.**\nThe “morphology-parameterized” model is empirically motivated, but the paper does not analyze why this representation improves optimization stability or disentangles shape/pose effectively.\n\n**No discussion on identifiability or optimization convergence issues under only 2D supervision.**\n\n**Writing and clarity issues.**\nThe text contains numerous redundancies and long sentences; several sections (e.g., Sec. 3.1–3.3) are nearly copied from AnyMo.\nFigures could be more informative — e.g., Fig. 2 is conceptually overloaded but lacks a clear depiction of data flow or loss supervision points."}, "questions": {"value": "How robust is CAMO under non-articulated or highly deformable motions (e.g., cloth, jellyfish, smoke-like motions)?\n\nHow is the orientation-sensitive feature extractor trained or selected? Is it frozen or fine-tuned during optimization?\n\nDoes the method handle camera motion explicitly, or does it assume static background and known intrinsic/extrinsic parameters?\n\nHow does the method scale to longer sequences (e.g., 1K+ frames)? Is optimization stable or prone to drift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "axmrBy2cHF", "forum": "Q5tYMLa2Bf", "replyto": "Q5tYMLa2Bf", "signatures": ["ICLR.cc/2026/Conference/Submission16365/Reviewer_JyNN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16365/Reviewer_JyNN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038443547, "cdate": 1762038443547, "tmdate": 1762926490323, "mdate": 1762926490323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}