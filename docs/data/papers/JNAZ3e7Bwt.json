{"id": "JNAZ3e7Bwt", "number": 13638, "cdate": 1758220262339, "mdate": 1759897423147, "content": {"title": "Continuously Augmented Discrete Diffusion model for Categorical Generative Modeling", "abstract": "Standard discrete diffusion models treat all unobserved states the same way, typically mapping them to an absorbing [MASK] token. This creates an \"information void\" where global semantic information that may be inferred for the masked tokens from the unmasked tokens is not directly passed from one denoising step to another.  We introduce **Continuously Augmented Discrete Diffusion (CADD)**, a framework that augments the discrete state space with a paired diffusion in a continuous latent space. This yields graded, gradually corrupted states in which masked tokens are represented by noisy yet informative latent vectors rather than information voids. At each reverse step, CADD uses the continuous latent as a semantic hint to guide discrete denoising. The design is clean and compatible with existing discrete diffusion training. At sampling time, the strength and estimator of the continuous latent vector enables a controlled trade-off between mode-coverage (diversity-oriented) and mode-seeking (context-localization-oriented). Empirically, we demonstrate CADD improves generative quality over mask-based diffusion across text generation, image synthesis, and code modeling, with consistent gains on both qualitative and quantitative metrics against strong discrete baselines.", "tldr": "We present a discrete diffusion model paired with continuous diffusion in the latent space, which improves the categorical data modeling in text, image and code domain.", "keywords": ["Diffusion", "Language Modeling", "Code generation", "Image generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/417993656cfa4a5c2818ff4eb1a968ec6620be7b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles a core problem in discrete diffusion models: all states collapse to a single absorbing state without retaining meaningful information. Inspired by the continuous diffusion model, the authors propose continuously augmented discrete diffusion (CADD), a hybrid framework that augments the standard masking process paired diffusion process in a continuous latent space. CADD utilizes a noisy continuous vector as a semantic condition to guide the discrete desnoising step. Experimental results across three distinct modalities demonstrate the effectiveness and versatility of CADD."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea that combines the discrete diffusion process with the continuous semantic latent to resolve the information void problem is novel and seamlessly integrated into the training and sampling pipeline. Furthermore, the lower bound of the objective is well-justified.\n\n- The paper's motivation and core concept are clearly presented, and the visualizations in Figures 1 and 2 are also clear.\n\n- The effectiveness of CADD was validated across three distinct tasks, consistently demonstrating improvement over conventional discrete diffusion models. The paired continuous latent vector effectively resolves the 'information void' problem, which is clearly reflected in the scalable generation quality as sampling steps increase."}, "weaknesses": {"value": "- While compatible with MDM training, the CADD framework is inherently more complex. It requires managing two parallel diffusion samplers and fusing their representations at every step.\n\n- The authors opt to use only the standard discrete cross-entropy loss for simplicity, omitting the continuous MSE loss that would naturally arise from the ELBO. While this works well empirically, the paper could be strengthened by either (a) showing that adding the MSE loss provides no benefit (justifying its omission) or (b) exploring the full loss term, which might lead to even better performance.\n\n- While CADD shows superior capabilities with large sampling steps, the performance seems to degrade sharply with a few-step sampling, probably due to the mode covering problem of the continuous semantic latent. This could amplify the inefficiency of the discrete diffusion models, which already suffer from large NFEs."}, "questions": {"value": "- Modern visual generative models mostly utilize vector-quantized (VQ) visual tokens for image synthesis with complex conditioning (such as text prompts). As this paper primarily searches for unconditional visual generation with discrete pixel space, could CADD be impacted by the complex VQ tokens and conditioning?\n\n- Masked Generative Models are a family of discrete diffusion models with an absorbing state, while they show superior efficiency [1][2]. Can the CADD framework be applied as a drop-in augmentation to MaskGIT-style models? If so, what is the expected trade-off in terms of sample quality/diversity?\n\n[1] Maskgit: Masked generative image transformer\n[2] An Image is Worth 32 Tokens for Reconstruction and Generation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical concerns raised."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QAenM616an", "forum": "JNAZ3e7Bwt", "replyto": "JNAZ3e7Bwt", "signatures": ["ICLR.cc/2026/Conference/Submission13638/Reviewer_LxZ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13638/Reviewer_LxZ1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832624014, "cdate": 1761832624014, "tmdate": 1762924217151, "mdate": 1762924217151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is motivated by the information loss when transforming clean tokens into mask tokens in MDMs. To address this issue, CADD augments each token with an informative continuous latent vector, which can provide more information on the original tokens. I find this idea is quite novel and can be viewed as the most important contribution of this paper.\nThe authors discuss the training and sampling method of CADD and validate it on image, text, and code generation tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- I find the idea of augmenting MDMs with continuous latent vectors to provide semantic information is both novel and well-motivated.\n- The empirical evidence is strong to support the effectiveness of the CADD method.\n- The paper presents a thorough derivation for training objective (ELBO) and sampling method, making the methodology rather practical."}, "weaknesses": {"value": "- The authors use a learnable network $w_\\theta$ which takes sequence $x$ as input and outputs the latent vector $z$. However, in my opinion, a more straightforward and robust way is directly assigning a learnable latent vector for each token, and all the parameters forms a \"codebook\". Will this parameterization works in practice?\n- In line 5 of Algorithm 1, the $w_\\theta$ takes the entire $x$ as input, and in line 10 of Algorithm 2, the $w_\\theta$ takes a single token $x^i$ as input. Which one is the correct expression?\n- The authors should discuss how they implement the  $w_\\theta$ in practice. Moreover, as they introduce an additional model, a comparison on memory and computation time should be provided.\n- I suggest that the authors to provide more experimental results. For example, the test perplexity and zero-shot perplexity can be a more  common benchmark for unconditional text generation. I am also interested in how the FID score on CIFAR10 scales on different choices of NFEs?"}, "questions": {"value": "- The current methodology of discrete diffusion almost converges to masked diffusion models. However, regarding the information void issue of MDMs, can this issue be alleviated by discrete diffusion with a uniform distribution noise?\n- How do you compute your NFE? (e.g. in Algorithm 2)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0BT7VODYd3", "forum": "JNAZ3e7Bwt", "replyto": "JNAZ3e7Bwt", "signatures": ["ICLR.cc/2026/Conference/Submission13638/Reviewer_xGtH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13638/Reviewer_xGtH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920545142, "cdate": 1761920545142, "tmdate": 1762924216640, "mdate": 1762924216640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Continuously Augmented Discrete Diffusion (CADD), a framework which combines the strengths of Discrete Diffusion Models and Continuous Diffusion Models. CADD retains the discrete masking strategy and uses continuous latent vectors as the tokens get masked, to maintain semantic information. In essence, two diffusion processes happen at the same time. Experimental results show that CADD models consistently generate higher quality samples over the baselines in three tasks: text, image and code generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well written, complete and easy to follow.\n* It bridges a known gap and is conceptually novel. This specific combination of discrete diffusion with continuous latent vectors when the tokens are masked had not been previously explored.\n* The idea is technically sound and the formulation of CADD is coherent. The factorization q(x_t​,z_t​ | x_0​)=q(x_t ​|x_0​)q(z_t​|x_t​,x_0​) provides a clear way to combine the discrete and continuous variables. \n* There is strong empirical validation. Results are presented for 3 different tasks, and CADD offers significant improvement over the baselines."}, "weaknesses": {"value": "* The discrete and continuous reverse process are derived in Section 4, but in the experiments the continuous MSE term is not used. This means that the continuous part is never directly trained to match the distribution, just guided. Although z_t appears to carry meaningful information, this makes me question how including the MSE would impact z_t and overall performance.\n* CADD maintains an extra latent vector per masked token, when compared to standard Discrete Diffusion Models. The authors should discuss whether this results in extra computational or memory cost.\n* In the text generation experiments, the authors do not compare CADD against any continuous/hybrid diffusion model, such as Duo."}, "questions": {"value": "1. What happens if you include the MSE loss? How does this affect z_t and performance?\n2. Does CADD incur extra computational cost vs discrete diffusion models? If so, is it significant?\n3. Did the authors test any alternative schedules for the continuous process?\n4. In the main experiments for text generation, why isn’t Duo a baseline?\n5. It could be interesting to include interpretability analysis regarding z_t, to visualize examples where the latent hints help disambiguate the tokens."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VlMw4D3Mv0", "forum": "JNAZ3e7Bwt", "replyto": "JNAZ3e7Bwt", "signatures": ["ICLR.cc/2026/Conference/Submission13638/Reviewer_oUtt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13638/Reviewer_oUtt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937553231, "cdate": 1761937553231, "tmdate": 1762924216199, "mdate": 1762924216199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Continuously Augmented Discrete Diffusion (CADD), a framework that combines Masked Discrete Diffusion (MDM) with a parallel continuous diffusion process in the embedding space. The key innovation is to replace the \"information void\" of a [MASK] token with a noisy but semantically informative continuous latent vector. This provides \"soft hints\" during the denoising process, leading to more coherent and higher-quality generation across text, image, and code modalities. The method is shown to be effective, scalable, and simple to implement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper target at solving a limitation of existing MDMs, which is the \"information void\" created by the absorbing [MASK] state. The idea of augmenting the discrete space with a continuous one to preserve graded semantic information is both intuitive and powerful.\n\nThe paper provides extensive experiments across multiple modalities (text, image, code) and scales (from 168M to 7B parameters). Consistent gains are observed over baselines.\n\nThe paper is well-written."}, "weaknesses": {"value": "Time cost of model running can be analyzed. For example, a time cost and FLOPs comparison between CADD and the baselines can be helpful for the readers to understand the performance-efficiency trade-off.\n\nThe experiment comparison is presented mainly with statistical result. It lacks illustrative example to show how the latent space parameter helps improve the generation performance.\n\nCode is not released for reproducibility checking."}, "questions": {"value": "What is the inference-time latency of CADD (with K=1) compared to a standard MDM baseline when generating a sequence of fixed length? Is the overhead primarily from the extra continuous state operations or from the architectural changes?\n\nThe continuous latent z_t is initialized from a standard Gaussian prior at step T. Have you experimented with more informative priors, and if so, what was the effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TVxzBHNsNh", "forum": "JNAZ3e7Bwt", "replyto": "JNAZ3e7Bwt", "signatures": ["ICLR.cc/2026/Conference/Submission13638/Reviewer_7PKE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13638/Reviewer_7PKE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000076784, "cdate": 1762000076784, "tmdate": 1762924214758, "mdate": 1762924214758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}