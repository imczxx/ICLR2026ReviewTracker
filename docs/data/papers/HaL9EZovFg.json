{"id": "HaL9EZovFg", "number": 3850, "cdate": 1757554605349, "mdate": 1759898066370, "content": {"title": "XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models", "abstract": "Omni-modal large language models (OLLMs) aim to unify audio, vision, and text understanding within a single framework. While existing benchmarks have advanced multimodal evaluation, it remains unclear whether OLLMs achieve modality-invariant reasoning or inherit modality-specific biases. We introduce \\textbf{XModBench}, a large-scale tri-modal benchmark explicitly designed to measure cross-modal consistency. XModBench contains 60K multiple-choice questions across five task families and systematically covers all six cross-modality directions, enabling diagnosis of task competence, modality disparity, and directional imbalance. Experiments show that even the strongest model, Gemini 2.5 Pro, (i) struggles with spatial and temporal reasoning, achieving less than 60% accuracy, (ii) suffers from modality disparities, with performance dropping by over {20 points} on average when audio inputs replace text, and (iii) exhibits directional imbalance, with a {9-point gap} when using vision as context versus using text as context.\nThe findings suggest that OLLMs fall short of modality-invariant reasoning, and XModBench provides a fundamental diagnostic tool for evaluating and improving their overall cross-modal competence.", "tldr": "", "keywords": ["Omni-modal Benchmark", "Cross-modal consistency"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1615f4df5b158f411ab4ddcbbe1bf50be99fd14.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents XModBench, a large-scale tri-modal benchmark for evaluating cross-modal consistency in omni-language models (OLLMs) that handle text, vision, and audio.\nUnlike previous multimodal benchmarks, XModBench focuses on whether models maintain consistent reasoning across modalities.\nIt includes 60K multiple-choice questions spanning five task families and six modality directions, and introduces diagnostic metrics for task competence, modality disparity, and directional imbalance.\nExperiments on leading OLLMs (e.g., Gemini 2.5 Pro, Qwen2.5-Omni) show that current models still lack modality-invariant reasoning, especially in spatial, temporal, and audio-related tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths\n\nNovel Focus: Targets an important but underexplored problem — evaluating cross-modal consistency rather than just multimodal performance.\n\nComprehensive Benchmark Design: Covers six modality directions and five task families, providing a balanced and systematic tri-modal evaluation.\n\nInsightful Diagnostics: Introduces clear metrics (modality disparity and directional imbalance) that reveal hidden biases and asymmetries in current OLLMs."}, "weaknesses": {"value": "This paper compares model performance differences across modalities for the same question. However, it does not discuss whether such differences are caused by information loss during modality conversion — for example, when a video or audio question is converted into text, the textual description cannot fully capture the visual or auditory content.\n\nThe paper does not explore whether specific training strategies or data construction methods could help mitigate these shortcomings in cross-modal consistency."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CFkDVH67n0", "forum": "HaL9EZovFg", "replyto": "HaL9EZovFg", "signatures": ["ICLR.cc/2026/Conference/Submission3850/Reviewer_DEkV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3850/Reviewer_DEkV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761607175797, "cdate": 1761607175797, "tmdate": 1762917066090, "mdate": 1762917066090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces XModBench, a large-scale, tri-modal (audio, vision, text) benchmark designed to evaluate omni-modal large language models (OLLMs). The core objective is to move beyond task-specific accuracy and measure cross-modal consistency—the ability of a model to produce consistent answers when the same semantic content is presented in different modalities. XModBench comprises 60,828 multiple-choice questions systematically covering five task families (perception, spatial reasoning, temporal reasoning, linguistic understanding, and external knowledge) and all six possible cross-modal directions between the three modalities. The authors use this benchmark to evaluate a range of OLLMs, including the Gemini series and several open-source models. The key findings demonstrate that even state-of-the-art models like Gemini 2.5 Pro lack true modality-invariant reasoning, exhibiting significant performance drops on spatial/temporal tasks, major disparities when inputs are switched (e.g., text vs. audio), and directional imbalances (e.g., V->T vs. T->V)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a crucial question: are OLLMs truly modality-invariant? It moves evaluation beyond simple accuracy on multimodal tasks and proposes a novel, principled method for measuring cross-modal consistency. The design, which permutes modalities for the same semantic question, is the paper's core strength.\n\n\n\n2. The benchmark is comprehensive. It contains over 60K questions , covers 5 diverse task families (from perception to external knowledge) , and 17 subtasks. This breadth ensures that the findings are not artifacts of a single domain.\n\n\n\n3. High-Quality Curation. The authors detail a rigorous data curation and verification process. The explicit use of \"human in-the-loop verification\" and multiple rounds of testing by annotators addresses major concerns about the quality and ambiguity of web-sourced or generated data .\n\n4. Actionable Diagnostics. The paper doesn't just rank models. It provides specific, interpretable diagnostic metrics—modality disparity and directional imbalance —that allow researchers to pinpoint where and how their models are failing. The failure case analysis in Section 4.5 and Figure 6 reinforces this with qualitative examples"}, "weaknesses": {"value": "1. Accessibility and Cost: The benchmark's primary strength—its scale—is also a potential weakness for adoption. Evaluating a model on 60,828 question-answer pairs, many of which involve multiple modalities, appears to be a computationally expensive process. The paper does not mention the availability of a smaller, standardized \"lite\" subset for researchers with limited compute. Furthermore, no information is provided on the practical costs of evaluation, such as total token usage for API-based models (like the Gemini series) or GPU hours for open-source models. This omission could be a significant barrier to widespread adoption and reproducibility.\n\n2. Limited Analysis of SOTA Performance: The paper's results clearly establish Gemini 2.5 Pro as the top-performing model, yet one that still has significant flaws. However, the analysis is largely limited to reporting these scores and failures. The paper would be strengthened by a deeper discussion hypothesizing why this model performs so much better on average than its open-source counterparts. Is it its training data, a specific architectural choice, or better-aligned encoders? A more in-depth analysis of the causes of SOTA performance (and its limitations) would be more impactful than just documenting the performance itself. Could be helpful if we could analyze how it was trained even with a guess."}, "questions": {"value": "Given the impressive scale of the benchmark, have the authors considered releasing a standardized \"lite\" subset? A smaller, balanced subset would significantly lower the barrier to entry, allowing for more rapid experimentation and broader adoption by the research community.\n\nCould the authors provide an estimation of the computational cost to run the full XModBench evaluation? Specifically, what is the approximate token usage (input and output) for evaluating an API-based model like Gemini, and what are the estimated GPU-hours for an open-source model?\n\nThe performance of Gemini 2.5 Pro is a key data point. While its failures on spatial/temporal tasks are clear , its overall superiority is also evident. Do the authors have any insights or hypotheses as to why this model demonstrates relatively better cross-modal consistency and overall competence compared to the other models tested?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The authors use web-sourced data without providing ethic statements."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k6gvIioNlW", "forum": "HaL9EZovFg", "replyto": "HaL9EZovFg", "signatures": ["ICLR.cc/2026/Conference/Submission3850/Reviewer_a37s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3850/Reviewer_a37s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749194130, "cdate": 1761749194130, "tmdate": 1762917065655, "mdate": 1762917065655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces XModBench, a large-scale tri-modal benchmark (audio, vision, text) designed to evaluate cross-modal consistency in omni-modal large language models (OLLMs). The dataset contains 60K multiple-choice QA pairs rendered in six modality directions (e.g., audio→text, image→audio), covering five task categories: perception, spatial reasoning, temporal reasoning, linguistic understanding, and external knowledge. The authors benchmark several frontier models (e.g., Gemini 2.5 Pro) and report three key findings: (i) OLLMs still struggle with spatial/temporal reasoning, (ii) performance drops significantly when audio replaces text, and (iii) models show strong directional imbalance (e.g., text→vision vs. vision→text). The benchmark aims to serve as a diagnostic tool for measuring modality-invariant reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper explicitly targets cross-modal consistency, a dimension often ignored in existing multimodal benchmarks.\n\nEach question instance is rendered across all six modality mappings, enabling controlled comparison and directional analysis.\n\nLarge scale and broad coverage.\nThe dataset includes >60K QA samples spanning 17 subtasks, with balanced modality construction.\n\nRelevance to current model trends.\nAs many new models claim “omni-modality,” this benchmark fills a timely evaluation gap."}, "weaknesses": {"value": "Table 2 is overloaded and hard to interpret.\nThe key conclusions (e.g., text→image > audio→text; Gemini has lowest variance) are meaningful, but the table is dense, lacks focused analysis, and could be split into smaller tables aligned with each main claim.\n\nInteresting modality-swap results but no deeper investigation.\nThe paper observes asymmetric performance (e.g., vision→text vs. text→vision) but does not analyze why. For example:\n– Do any models use interleaved multimodal training data?\n– Do models with such data show smaller swap gaps?\n\nDataset quality control is unclear.\nThe benchmark claims 60K samples but does not report human validation, error rate, or annotation quality checks.\n\nNo discussion of answer-option bias.\nSome modalities may allow shortcut guessing (e.g., lexical cues in text choices). There is no “noise-input” baseline to rule this out (e.g., Gemini with shuffled / blank modality input).\n\nLack of analysis for <25% performance cases.\nSeveral settings score worse than random guessing (25% for 4-choice MCQ), but the paper does not explain whether this is due to instruction following, noisy inputs, or poor distractor design."}, "questions": {"value": "Dataset quality\nHave you conducted human verification on a subset of the 60K samples? If so, what is the estimated annotation error rate?\n\nDistractor bias\nCan models guess correct answers without context? Please provide “no-input” or “noise-input” baselines to quantify answer-option bias.\n\nModality swap analysis\nDo any evaluated models train on interleaved multimodal corpora (e.g., narrated video, audiocaps)? If yes, do they exhibit smaller directional gaps?\n\nTable 2 clarity\nWould you consider splitting Table 2 into multiple focused tables (e.g., task competence, disparity, imbalance) to improve readability?\n\nBelow-random performance\nFor conditions where models score <25%, what is the failure mode? Instruction refusal? Systematic misalignment? Poor distractor construction?\n\nBenchmark extensibility\nDo you plan to release tools for adding new modality pairs (e.g., text↔3D, audio↔video)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rOIpzy6qso", "forum": "HaL9EZovFg", "replyto": "HaL9EZovFg", "signatures": ["ICLR.cc/2026/Conference/Submission3850/Reviewer_3qp2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3850/Reviewer_3qp2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933946680, "cdate": 1761933946680, "tmdate": 1762917065387, "mdate": 1762917065387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces XModBench, a large-scale tri-modal benchmark specifically designed to measure cross-modal consistency in Omni-modal Large Language Models (OLLMs) by systematically covering six cross-modality directions for audio, vision, and text. XModBench comprises over 60,000 multiple-choice questions across five task families and 17 subtasks, enabling diagnostic assessment of task competence, modality disparity, and directional imbalance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensive Diagnostic Scope: XModBench provides a large-scale, systematically balanced tri-modal QA benchmark, covering all six modality permutations (audio, vision, text) for both the context and candidate answers. The benchmark covers five diverse task families (perception, spatial, temporal, linguistic, external knowledge), each with multiple subtasks.\n\n- Detailed Empirical Analysis: The authors conduct a detailed empirical analysis of cutting-edge OLLMs, including a performance breakdown by task and modality configuration (Table 2). This analysis effectively identifies the significant lack of capability or competence in current OLLMs within the audio domain."}, "weaknesses": {"value": "- XModBench primarily focuses on isolated cross-modal alignment (e.g., T→V, V→T) and fails to cover true mixed tri-modal capabilities (e.g., Image+Vision+Audio→Text/Image). This combined modality reasoning is arguably the critical differentiator separating OLLMs from Multimodal Large Language Models (MLLMs) and specialized speech models.\n\n-  The data curation shows an over-reliance on GPT-5 as the primary question generation tool. However, the quality assurance (QA) or filtering process for this synthetic data is not clearly elaborated. This risks labeling XModBench as a 'silver' dataset rather than a 'gold' standard, where prioritizing data quality over sheer quantity is paramount.\n\n-  Contextualization against MLLMs: The analysis lacks comparison against the performance of MLLMs focused on traditional ASR or image-to-text (I2T) tasks, such as Qwen-VL or Intern-VL. It remains unclear whether OLLMs maintain a performance advantage in these specific subdomains when compared to these more focused MLLMs."}, "questions": {"value": "same as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xWfbLQYkuG", "forum": "HaL9EZovFg", "replyto": "HaL9EZovFg", "signatures": ["ICLR.cc/2026/Conference/Submission3850/Reviewer_2qMR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3850/Reviewer_2qMR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986019605, "cdate": 1761986019605, "tmdate": 1762917064146, "mdate": 1762917064146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}