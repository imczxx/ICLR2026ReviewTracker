{"id": "bMINsPQpME", "number": 7882, "cdate": 1758040732371, "mdate": 1763607873307, "content": {"title": "Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in Vision-Language Models", "abstract": "Spatial reasoning ability is crucial for Vision Language Models (VLMs) to support real-world applications in diverse domains including robotics, augmented reality, and autonomous navigation. Unfortunately, existing benchmarks are inadequate in assessing spatial reasoning ability, especially the \\emph{intrinsic-dynamic} spatial reasoning which is a fundamental aspect of human spatial cognition. In this paper, we propose a unified benchmark, \\textbf{Spatial-DISE}, based on a cognitively grounded taxonomy that categorizes tasks into four fundamental quadrants: \\textbf{I}ntrinsic-\\textbf{S}tatic, Intrinsic-\\textbf{D}ynamic, \\textbf{E}xtrinsic-Static, and Extrinsic-Dynamic spatial reasoning. Moreover, to address the issue of data scarcity, we develop a scalable and automated pipeline to generate diverse and verifiable spatial reasoning questions, resulting in a new \\textbf{Spatial-DISE} dataset that includes Spatial-DISE Bench (559 evaluation VQA pairs) and Spatial-DISE-12K (12K+ training VQA pairs). Our comprehensive evaluation across 28 state-of-the-art VLMs reveals that, current VLMs have a large and consistent gap to human competence, especially on multi-step multi-view spatial reasoning. Spatial-DISE offers a robust framework, valuable dataset, and clear direction for future research toward human-like spatial intelligence. Benchmark, dataset, and code will be publicly released.", "tldr": "We created Spatial-DISE, a new benchmark to test dynamic cognition spatial reasoning. We found top VLM models fail at cognitive skills like mental simulation, not perception, showing a universal gap to human performance.", "keywords": ["Spatial Reasoning", "Vision-Language Models", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/99ce8771b794a260460d69e31a0a95f9dff6302e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Spatial-DISE, a new benchmark and dataset designed to evaluate the spatial reasoning capabilities of VLMs, along with the data generation pipeline. The authors argue that existing benchmarks are inadequate, often focusing on static scenarios and lacking a systematic, cognitively-grounded framework. To address this, they propose the DISE taxonomy, which categorizes spatial reasoning tasks into four quadrants based on two dimensions: Intrinsic vs. Extrinsic and Static vs. Dynamic. Experiment results show that current models struggle in mental simulation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive empirical evaluation. The paper presents an extensive and rigorous evaluation across 28 distinct Vision-Language Models. The selection of models is commendable, covering a wide spectrum of architectures including proprietary APIs, open-source foundation models, and specialized models fine-tuned for reasoning. The experiments with fine-tuning in Section 4.3 are interesting, where performance gains on in-domain tasks come at the cost of generalization to other spatial benchmarks.\n2. Cognitively grounded and unified taxonomy, which provides a comprehensive analysis framework in analysing spatial reasoning performance of VLMs. \n3. The paper is well-written, with rich error analysis."}, "weaknesses": {"value": "1. Fail to disentangle perceptual confounders from reasoning deficits. While the paper attributes model failures primarily to reasoning, as far as I'm concerned, the analysis may not fully account for more fundamental perceptual challenges that act as confounding variables. The evaluation format often presents a complex multi-panel image containing both the question pattern and several graphical options. A model's failure could stem from an inability to correctly understanding this layout which is a visual parsing issue, not a spatial reasoning deficit (which I believe is not really the perceptual error and comprehension error in L447. Please correct me if I'm wrong). This potential confounder weakens several key conclusions:\n* The claim about the relationship between static and dynamic reasoning could be affected.\n* The \"catastrophic forgetting\" observed in Section 4.3 could be reinterpreted. The performance drop on other benchmarks might not be a loss of reasoning ability, but rather a failure to generalize to different visual formats and layouts on the perception side. \n2. Concerns in task categorization. While the DISE taxonomy is well-founded, the mapping of specific tasks to its quadrants can be ambiguous. A task may not be \"cognitively pure\" and could be solvable via multiple reasoning strategies that cross quadrant boundaries. For example, 3D Shape Finding is classified as Intrinsic-Static, implying a logical deduction about a fixed object's properties. However, a viable and intuitive strategy to solve this task involves mentally rotating the cube (an Intrinsic-Dynamic process) to build a complete 3D mental model before identifying the missing face. This ambiguity raises concern regarding the diagnostic clarity of the benchmark. If a model fails this task, it is unclear whether the failure lies in static deduction or dynamic mental simulation, which in turn limits the precision of the paper's conclusions about specific cognitive weaknesses.\n3. Lack of deeper analysis on model architecture. The evaluation is comprehensive in its breadth of models but could be deeper in its analysis of architectural influences. The paper groups models by family or purpose (e.g., \"reasoning models\") but does not delve into how specific architectural choices (e.g., type of vision encoder, cross-attention mechanism, size of the vision model vs. language model) might correlate with performance on the different DISE quadrants. (minor comments that are not factored into my score fyi)"}, "questions": {"value": "See as above in weaknesses. I'm happy to adjust the scores if the author can address the three concerns in the weaknesses. \n\nTypo: missing . in L433"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nTL8BL2N72", "forum": "bMINsPQpME", "replyto": "bMINsPQpME", "signatures": ["ICLR.cc/2026/Conference/Submission7882/Reviewer_YgFh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7882/Reviewer_YgFh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761387050986, "cdate": 1761387050986, "tmdate": 1762919915836, "mdate": 1762919915836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Spatial-DISE, a unified benchmark for evaluating spatial reasoning in vision-language models (VLMs). It introduces a cognitively inspired taxonomy, DISE (Intrinsic/Extrinsic × Static/Dynamic), to categorize spatial reasoning tasks. Using a three-stage pipeline combining real-world, synthetic, and human-validated samples, the authors construct a diverse dataset and evaluate 28 VLMs against human performance (76.8%). Results reveal large performance gaps, especially in dynamic and extrinsic reasoning. A fine-tuning study shows that models can improve on Spatial-DISE but suffer from catastrophic forgetting. The paper also offers a cognitive error taxonomy analyzing model failures.\n\nMain Contributions\n- A cognitive taxonomy (DISE) for structuring spatial reasoning evaluation.\n- A large, human-verified benchmark dataset combining real, synthetic, and controlled samples.\n- A comprehensive evaluation of 28 leading VLMs with human baselines.\n- Error taxonomy and analysis revealing systematic weaknesses in spatial reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear Cognitive Taxonomy\n- The proposed DISE framework (Intrinsic/Extrinsic × Static/Dynamic) is cognitively grounded, offering interpretability which is good for benchmark design.\n- This taxonomy provides a systematic structure that unifies fragmented spatial reasoning benchmarks.\n\n2. Cognitive Error Analysis\nThe error taxonomy (Perceptual / Comprehension / Reasoning errors) and the subtypes (Rule Application, Mental Simulation, Holistic–Local Failure) are both novel and psychologically informed.\n\n3. Human Benchmarking\nThey include 54 human participants to establish a strong baseline."}, "weaknesses": {"value": "1. Fine-tuning and Generalization Analysis Could Be Deeper\n- The fine-tuning experiments (Qwen2.5-VL, SpaceOm) show interesting trends but lack representation analysis — e.g., why forgetting occurs, or what cognitive dimension the gains are concentrated in.\n\n2. Visual Accessibility\n- Figures are a bit too dense (figure 1, 2, 3) and may not fully communicate the DISE framework or task intuitions clearly to non-specialists."}, "questions": {"value": "1. Beyond taxonomy and scale, what new reasoning capabilities does this benchmark test?\n2. Is there evidence of cross-domain transfer? For example, does training on Intrinsic tasks improve performance on Extrinsic reasoning, or are these dimensions fully independent?\n\nI think overall it's a good benchmark. I would consider raising my score if the questions and weaknesses are well-addressed."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "It seems that many human annotators were involved in this study, and the responsible research practices should be further elaborated and reviewed."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DL3GqZMOMz", "forum": "bMINsPQpME", "replyto": "bMINsPQpME", "signatures": ["ICLR.cc/2026/Conference/Submission7882/Reviewer_oTiT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7882/Reviewer_oTiT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988940020, "cdate": 1761988940020, "tmdate": 1762919914969, "mdate": 1762919914969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes two semi-synthetic datasets (Spatial-DISE Bench and Spatial-DISE-12K) to evaluate spatial reasoning in Vision-Language Models (VLMs), with a special emphasis on intrinsic and dynamic tasks that are missing from previous benchmarks. The datasets are created using a new data generation framework based on the graphics software Blender to generate complex 3D scenarios that require spatial reasoning to solve. The work also offers a comprehensive evaluation of 28 state-of-the-art (SotA) VLMs on the proposed data showing that current models perform much worse than human subjects.\n\nDespite some good contributions (i.e., evaluation study), I am really not sure that creating new datasets and the entire data generation pipeline (which are claimed to be key contributions) was at all needed in the first place. Given the way the problem is framed (see Table 1), one could combine pre-existing benchmarks to obtain all combinations of domains, sources, DISE tasks and large scale, completely bypassing the need to create a new data generation pipeline and new datasets.\n\nThe motivation behind the need for spatial reasoning in VLMs is also unconvincing in the current presentation. I am not saying it should not be done, but right now it is unclear why the community should be interested.\n\nBecause the above issues pertain to key contributions and the main motivation of the paper, I am leaning towards rejection. I am, however, open to clarifications from the authors in case I am misunderstanding those aforementioned aspects, in which case I would be willing to update my score."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tAdoption of taxonomy to classify spatial tasks from the cognitive sciences.\n-\tComprehensive evaluation of 28 SotA VLMs.\n-\tHuman quality control of created datasets.\n-\tHuman performances (54 participants) collected and included in the evaluation as a baseline.\n-\tNew data generation framework based on Blender that can generate complex spatial reasoning scenarios."}, "weaknesses": {"value": "-\tSeveral critical parts motivating this work are unclear or questionable, which makes me seriously concerned as to whether this work addresses a genuine gap in the literature (more below).\n-\tMany important details in the methodology (dataset creation, error analysis) are unclear or missing (more below).\n-\tAny discussion about potential limitations of the work is completely missing."}, "questions": {"value": "**Questions to authors**\n\n-\t[**critical**] Why was it necessary to create Spatial-DISE datasets? Can’t we combine all existing benchmarks referenced in Table 1 to get a large-scale meta-benchmark that covers all domains and DISE tasks?\n-\t[**critical**] Why VLMs capability for sophisticated dynamic spatial reasoning is important beyond the fact that humans can do it? Can you give some detailed examples and elaborate on their importance?\n-\t[**critical**] The last paragraph in Section 2 discusses latest work that also covered most or all DISE tasks, but mentions verifiability as an important distinctive factor. Can you elaborate on the importance of verifiability and why it was so critical that it warranted creating a new data generation pipeline?\n-\tWhy was there a need to create two datasets (Spatial-DISE Bench and Spatial-DISE-12K) and not just one? Do they serve different purposes?\n-\tSection 3.2 Stage 1 mentions 1180 VQA pairs, but then Spatial-DISE Bench (which is 53% “wild” data) has only 559 samples? Where does the 559 come from?\n-\tWere all 12,000 generated VQA pairs manually verified by humans?\n-\tCan the proposed data generation framework generate only 3D scenarios? Can it generate 2D examples?\n-\tIs the error analysis done with Doubao-1.6-thinking free from any potential mistakes? That is, can the model make mistakes (e.g., miscategorise other models’ errors) and mislead the error analysis?\n-\tIn Section 5, why only a subsample (200) of incorrect responses was used and not 100%?\n\n**Additional feedback**\n\n-\tI do not think that claiming the DISE taxonomy as novel is accurate as it was derived from (Maier, 1996; Uttal et al., 2013). Authors should make it clear that this taxonomy has been adopted from previous work. For example, first contribution in line 98 calls it novel.\n-\tCalling non-synthetic data “wild” seems very uncommon and quite confusing. Using a more common term “real data” or “real-world data” would resolve the confusion. If you keep using an uncommon term, I would suggest to explain it early in the text.\n-\tMost of the code in the algorithm listings (Algorithms 1-5) is very hard to interpret as the code mostly consists of function names that are not explained anywhere. Are those functions built-in procedures in Blender?\n-\tThe paper uses phrases like “mental rotation”, “mental transformation” and alike throughout. I am not sure the word “mental” is the right choice as it generally refers to the mind, which I do not think VLMs have. Words like “simulation” or “hypothesis” might be more accurate.\n-\tVerifiability is being mentioned very often throughout the text, but its importance to this work is never properly explained.\n-\tLine 68: “have limited scopes” -> “are limited in scope”.\n-\tLine 69: “multi-steps” -> “multi-step”.\n-\tLine 83: “The Spatial-DISE” -> “Spatial-DISE”.\n-\tLines 89-90: mentions “Spatial-DISE Bench” for the first time without explaining what it is.\n-\tI do not think the first paragraph of Section 3 is at all needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VoXBrPhWA4", "forum": "bMINsPQpME", "replyto": "bMINsPQpME", "signatures": ["ICLR.cc/2026/Conference/Submission7882/Reviewer_o491"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7882/Reviewer_o491"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762511463226, "cdate": 1762511463226, "tmdate": 1762919914460, "mdate": 1762919914460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Spatial-DISE, a new benchmark for evaluating spatial reasoning in VLMs. The work leverages a $2\\times2$ cognitive taxonomy (Intrinsic/Extrinsic vs. Static/Dynamic) to structure the benchmark. A key contribution is a scalable Blender-based pipeline for generating a 12K-pair training dataset and a 559-pair evaluation bench. The authors perform a comprehensive study on 28 VLMs, finding that current models perform poorly, especially on dynamic tasks, and that failures are rooted in reasoning (e.g., rule application, mental simulation) rather than perception."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The adoption of the DISE cognitive taxonomy provides a structured and theoretically-grounded framework for evaluating spatial reasoning, moving beyond ad-hoc task collections.\n\n2. The Blender pipeline is a valuable engineering contribution, enabling the verifiable and programmatic generation of complex 3D spatial reasoning tasks, which are difficult to source at scale.\n\n3. The empirical study is extensive, testing 28 SOTA models. The resulting error analysis (Section 5) provides a clear insight: the primary bottleneck for VLMs is cognitive reasoning, not visual perception."}, "weaknesses": {"value": "1. **Marginal Novelty in a Crowded Field**: The paper's own related work (Table 1) demonstrates this is an extremely crowded and concurrent field (e.g., BSA, OmniSpatial, BLINK, SPACE). I believe it is necessary to demonstrate that this dataset is fundamentally distinct from existing ones and possesses intrinsic value.\n\n2. **Dataset Utility is Questionable**: The paper presents the Spatial-DISE-12K dataset as a major contribution for future training. However, the paper's own finetuning analysis (Section 4.3, Table 3)  demonstrates this dataset may be flawed. While SFT on Qwen2.5-VL-7B improves performance on the in-domain Spatial-DISE benchmark (+23.6pp), the SpaceOm model shows \"catastrophic forgetting\" on a general benchmark like CVBench (a -32.4pp drop) . This strongly suggests the 12K dataset lacks diversity and induces severe overfitting to the specific generative patterns of the pipeline.\n\n3. **Limited Evaluation**: The model set omits several state-of-the-art commercial VLMs (e.g., GPT o3/5, Google Gemini 2.5 pro), which weakens the headline claim about “current VLMs.” They have more robust and powerful ability."}, "questions": {"value": "I am curious about the true value of the dataset, as I strongly suspect that it may merely overfit to its own format.\n\nIf a base model (e.g., Qwen-VL) could be fine-tuned on this dataset and subsequently demonstrate performance gains on other benchmarks (such as VSI-Bench, OmniSpatial and SPACE), I would be much more inclined to recognize the dataset’s contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u82gEBFACN", "forum": "bMINsPQpME", "replyto": "bMINsPQpME", "signatures": ["ICLR.cc/2026/Conference/Submission7882/Reviewer_v2Mn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7882/Reviewer_v2Mn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762703637042, "cdate": 1762703637042, "tmdate": 1762919914033, "mdate": 1762919914033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "## General Response\n\nWe sincerely thank all reviewers for the careful reading and many constructive comments.\n\n### General Questions：\n\n**1. On contribution and novelty.**\n\nWe agree that spatial reasoning for VLMs is a highly active and concurrent area, and we do not claim that Spatial-DISE is the first benchmark of its kind. In the revision we will clarify that our contribution is **not** a brand-new taxonomy, but a **cognitively grounded framing and dataset package that is explicitly complementary to existing work**:\n\n- We **adopt** the Intrinsic/Extrinsic × Static/Dynamic (DISE) taxonomy from cognitive science and use it as a *unifying lens* to systematically re-locate prior datasets and expose under-served regions, especially dynamic and 3D abilities.\n- Based on this lens, we design a **DISE-balanced evaluation suite (Spatial-DISE Bench)** and a paired **training-scale synthetic dataset (Spatial-DISE-12K)** with a seed-controlled, verifiable pipeline that targets precisely those under-represented abilities.\n- We position Spatial-DISE-12K explicitly as a **training resource that complements scarce real-world spatial data**, rather than as a replacement for existing real or mixed benchmarks such as BLINK, OmniSpatial, SPACE, or VSIBench.\n\nIn this sense, Spatial-DISE is novel not because it introduces “yet another” task collection, but because it couples an established cognitive framework with a **training-ready, verifiable dataset** deliberately focused on the gaps that this framework reveals.\n\n**2. On fine-tuning, external benchmarks, and the earlier performance drops.**\n\nReviewers also questioned whether Spatial-DISE-12K truly provides useful training signal beyond its own format, and whether the previously reported large performance drops on external benchmarks indicate that the dataset encourages over-specialization.\n\nAfter receiving the reviews, we revisited and extended our experiments with a more conservative and standard fine-tuning setup (smaller learning rate, stronger regularization, and validation-based early stopping):\n\n- For **SpaceOm**, fine-tuning on Spatial-DISE-12K now yields **consistent improvements** on Spatial-DISE *and* on all evaluated external benchmarks (CVBench, SAT, SPACE, OmniSpatial, VSIBench-MCQ), instead of the strong degradation previously reported on CVBench.\n- For **Qwen2.5-VL-7B**, fine-tuning *only* on Spatial-DISE-12K substantially improves performance not just on Spatial-DISE, but also on multiple independent spatial benchmarks, including those with very different visual and linguistic styles (e.g., strong gains on OmniSpatial and VSIBench-MCQ).\n- We additionally compare against several strong commercial VLMs on Spatial-DISE and show that a Qwen2.5-VL-7B model fine-tuned on Spatial-DISE-12K can **substantially outperform** these systems on our benchmark, further supporting the dataset’s practical value as a training resource.\n\nTaken together, these updated results indicate that the earlier large drops were due to an **overly aggressive fine-tuning recipe**, rather than an inherent flaw in Spatial-DISE-12K. Under a standard configuration, the dataset provides **generalizable spatial reasoning signal** that transfers across heterogeneous benchmarks, which directly addresses the shared concerns about its utility and risk of overfitting.\n\n### Summary of changes:\n\nWe've made the revisions to the main paper according to all reviewers' comments. The main revisions are summarized as follows:\n\n1. We have updated the results in Table 2, added more commercial models, and updated SFT results.\n2. We have updated the results in Table 3, added two more OOD benchmark tests results.\n3. We have moved original Figure 3 to Appendix as Figure 1 already shown the dataset curation pipeline.\n4. We have removed first paragraph in section 3 as it was a redundant information.\n5. We have added two transfer heatmap for illustration new analysis results.\n6. We have updated section 4.3 for latest analysis on SFT results.\n7. We have added an limitation section in the end.\n\n### Updates ongoing:\n\n- [ ]  Listings in Appendix.\n- [ ]  Detailed training setup in Appendix.\n- [ ]  Tasks-wise accuracy in Appendix."}}, "id": "gGBHok6cMc", "forum": "bMINsPQpME", "replyto": "bMINsPQpME", "signatures": ["ICLR.cc/2026/Conference/Submission7882/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7882/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission7882/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763606400380, "cdate": 1763606400380, "tmdate": 1763606400380, "mdate": 1763606400380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}