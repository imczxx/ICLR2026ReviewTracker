{"id": "OFk9jdUgdc", "number": 2344, "cdate": 1757061148942, "mdate": 1763111201189, "content": {"title": "Toward Generalizable Deblurring: Leveraging Massive Blur Priors with Linear Attention for Real-World Scenarios", "abstract": "Image deblurring has advanced rapidly with deep learning, yet most methods exhibit poor generalization beyond their training datasets, with performance dropping significantly in real-world scenarios. Our analysis shows this limitation stems from two factors: datasets face an inherent trade-off between realism and coverage of diverse blur patterns, and algorithmic designs remain restrictive, as pixel-wise losses drive models toward local detail recovery while overlooking structural and semantic consistency, whereas diffusion-based approaches, though perceptually strong, still fail to generalize when trained on narrow datasets with simplistic strategies. Through systematic investigation, we identify blur pattern diversity as the decisive factor for robust generalization and propose Blur Pattern Pretraining (BBP), which acquires blur priors from simulation datasets and transfers them through joint fine-tuning on real data. We further introduce Motion and Semantic Guidance (MoSeG) to strengthen blur priors under severe degradation, and integrate it into GLOWDeblur, a Generalizable reaLwOrld lightWeight Deblur model that combines convolution-based pre-reconstruction & domain alignment module with a lightweight diffusion backbone. Extensive experiments on six widely-used benchmarks and two real-world datasets validate our approach, confirming the importance of blur priors for robust generalization and demonstrating that the lightweight design of GLOWDeblur ensures practicality in real-world applications.", "tldr": "We address dataset bias in image deblurring with Blur Pattern Pretraining (BBP) and Motion and Semantic Guidance (MoSeG), enabling a lightweight diffusion model to generalize robustly across benchmarks and real-world data.", "keywords": ["linear attention; image deblur; diffusion model; blur pattern"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/041633f1c1a5ab374a2a999c575d5596d4a813fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a generalizable approach to real-world image deblurring. Observing that existing models suffer significant performance degradation on unseen data, the authors identify the core issue as the lack of diversity in blur patterns within training datasets, as well as an overreliance on pixel-wise losses that ignore structural and semantic consistency. To address this, they introduce a Blur Pattern Pretraining (BBP) strategy, which first learns blur priors from simulated datasets and then performs joint fine-tuning on real-captured data. They also present GLOWDeblur, a lightweight diffusion-based deblurring model that incorporates motion and semantic guidance to enhance restoration under severe blur. Experimental results across multiple benchmark and real-world datasets demonstrate the methodâ€™s superior generalization capabilities, highlighting the importance of blur priors and architectural design for practical deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  Precise Problem Definition with Rigorous Evidence: The paper's motivation is clear and highly persuasive. It moves beyond the conventional discussion of \"realism\" by skillfully combining Table 1 (performance degradation) and Figure 3 (imbalanced pattern distributions). This robustly proves that the root cause of generalization failure is the biased and imbalanced distribution of \"blur patterns\" in the training data. \n\n2.  Shifting Research Focus with a Systematic Strategy: Based on this key insight, the paper successfully shifts the research focus from pursuing \"realism\" to pursuing \"blur pattern diversity.\" It proposes a systematic data strategy to address this issue. It first learns blur priors from simulation datasets with broad pattern coverage and then jointly fine-tunes on real-world datasets to align the distribution.\n\n3. Superior Visual Results: This strategy directly targets the core of the problem, and its effectiveness is intuitively validated by the superior visual results."}, "weaknesses": {"value": "1. Weak Methodological Innovation and Unaddressed Concerns: The paper's novelty is limited, primarily relying on stacking existing modules like MoG and SeG. These additions also introduce significant concerns: the accuracy of MoG's motion estimation is unverified and risks misguiding the restoration. Furthermore, the usage of SeG is ambiguous; if a VLM is required at test time, it introduces an unfair external annotation, and its robustness on low-quality, poorly-described images is unexplored.\n\n2. Contradictory Metrics and Fidelity Concerns: The experimental results show a clear contradiction. The method excels in No-Reference (NR-IQA) metrics like MANIQA, yet it significantly lags behind in traditional Full-Reference (FR-IQA) metrics such as PSNR and SSIM. This strongly suggests that the model sacrifices fidelity for perceptual quality, likely suffering from the common \"hallucination\" problem in diffusion models (generating plausible but inaccurate details). The critical omission of the LPIPS metric fails to resolve these fidelity concerns.\n\n3. Misleading \"Lightweight\" Claim: The paper's claim of being \"lightweight\" is misleading. It selectively highlights optimizations in the diffusion core while ignoring the total system complexity (including the UNet, MoG, and potentially a large VLM for SeG). The lack of FLOPs or actual inference time figures, combined with the mention of high-end training hardware (8x A800 80G GPUs), makes the claims of efficiency highly questionable."}, "questions": {"value": "1.  Is the SeG module required to run during the inference stage? If so, does this imply the model relies on a large external VLM to achieve its advantage, leading to an unfair comparison and contradicting the \"lightweight\" claim? \n\n2.  Is the motion estimation within the MoG module trained end-to-end, or does it use a fixed pre-trained model? If the former, are there specific loss functions or visualizations to prove it generates correct motion guidance? If the latter, how is its accuracy and applicability to your dataset ensured? \n\n3.  How do the authors explain the significant gap where the model lags in PSNR/SSIM but excels in NR-IQA metrics? Does this imply the model suffers from the common \"hallucination\" issue (deviating from ground truth)? Why is the critical LPIPS metric missing to validate the perceptual fidelity against the ground truth? \n\n4.  To validate the \"lightweight\" claim, what are the average inference time, the number of the parameters, and the FLOPs for the proposed method? \n\n5.  Could you please detail the collection and filtering criteria for the self-created RWBlur400 dataset? Does this dataset have corresponding ground truth? If not, is relying solely on NR-IQA metrics sufficient to claim robust real-world performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b9wo7kyxbN", "forum": "OFk9jdUgdc", "replyto": "OFk9jdUgdc", "signatures": ["ICLR.cc/2026/Conference/Submission2344/Reviewer_npVR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2344/Reviewer_npVR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761450076032, "cdate": 1761450076032, "tmdate": 1762916200989, "mdate": 1762916200989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We respectfully request the withdrawal of this submission. Following extensive internal deliberation, the authors have decided to further develop and substantially extend the scope of the work, with the intention of submitting a revised manuscript to a more appropriate venue. All co-authors unanimously support this withdrawal request. We thank the program committee for their time, consideration, and efforts in handling our submission."}}, "id": "zLRNpPhhdm", "forum": "OFk9jdUgdc", "replyto": "OFk9jdUgdc", "signatures": ["ICLR.cc/2026/Conference/Submission2344/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2344/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763111199869, "cdate": 1763111199869, "tmdate": 1763111199869, "mdate": 1763111199869, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. This paper discussed the importance of training deblurring methods with diverse blur patterns, and proposed a training strategy that first pre-trains the model on diverse synthetic blur pattern then real blurred imags. \n2. A novel deblurring architecture consists of lighweight diffusion model is also proposed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Performance shows that the proposed training strategy is helpful.\n2. The lightweight diffusion architecture is novel, which is a good tradeoff between generalization and efficiency,"}, "weaknesses": {"value": "1. I think the paper should specifies that it is proposed to handling motion blur instead of general blurs.\n2. The training pipeline is complex and hard to reproduce.\n3. No explicit inference time evaluation."}, "questions": {"value": "1. In line 144 and 129, I guess BBP is not the abbreviation of Blur pattern pretraining\n2. How to get the blur pattern statistics shown in Table 3.\n3. Were captions generated on degraded images or sharp images? If it is the latter, how to get correct captions during inference?\n4. Is such an aggressive encoder able to reconstruct original images?\n5. I know authors claimed that mix-training is sub-optimal, but I wonder the performance of training the model with just one step using all datasets mentioned in this paper.\n6. In the owl of Figure 6, I wonder if the feather on the wing is really the reconstructed details or just hallucination. If it is hallucination, then it is dangerous to use it in camera.\n7. How were other compared methods trained, on the same dataset or simply gran the pre-trained ckpt?\n8. If the architecture is one of the contribution, then even without the BPP it should performs better than other methods. I would like to see more results about this. \n9. Any evaluations of inference time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qHdAtws3RK", "forum": "OFk9jdUgdc", "replyto": "OFk9jdUgdc", "signatures": ["ICLR.cc/2026/Conference/Submission2344/Reviewer_joud"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2344/Reviewer_joud"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761611911093, "cdate": 1761611911093, "tmdate": 1762916200563, "mdate": 1762916200563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limited generalization of image deblurring models in real-world scenario. It identifies main challenges from blur diversity-realism tradeoff and proposes BPP to learn diverse blur priors from simulated data, transferring them to real domains via joint fine-tuning. To further improve robustness, the authors introduce Motion and Semantic Guidance (MoSeG), which leverages motion cues and semantic context to enhance restoration quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Writing: The paper is well-organized and clearly written, making it easy to follow.\n\n2. Logical Idea: The integration of motion-related and semantic-related information for deblurring is intuitive and well-motivated, representing a logical extension of existing approaches. The overall framework design is cohesive, and experiments demonstrate promising results."}, "weaknesses": {"value": "1. Motion Guidance: The proposed motion modeling appears limited to 2D directional blur, whereas real-world blur often includes depth-axis motion components. Consequently, BPP may fail to capture full 3D motion complexity, reducing its applicability to realistic scenarios. Moreover, since motion trajectories require paired sharp images to be computed, the motion guidance component can only be trained on synthetic datasets, potentially restricting its generalization to real-world data.\n\n2. Semantic Guidance: Under conditions of severe blur, the pretrained VLM~(QwenVL) model used for semantic extraction may produce inaccurate or unreliable outputs, weakening its contribution to deblurring quality.\n\n3. Typo: BBP -> BPP"}, "questions": {"value": "1. Could the authors provide a comparison with models trained on all major datasets (GoPro, HIDE, REDS, RealBlur, etc.) to more convincingly demonstrate the effectiveness of their approach?\n\n2. What is the patch size or spatial scale used for motion estimation in motion guidance, and how sensitive is the performance to this choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TefdGLoOP4", "forum": "OFk9jdUgdc", "replyto": "OFk9jdUgdc", "signatures": ["ICLR.cc/2026/Conference/Submission2344/Reviewer_b7dR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2344/Reviewer_b7dR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807200378, "cdate": 1761807200378, "tmdate": 1762916200301, "mdate": 1762916200301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the generalization problem in image deblurring caused by the biases in deblurring datasets. The authors identified the characteristics in the current datasets and investigated the generalization performance using the existing deblurring method, Restormer. Then, this paper proposes Blur Pattern Pretraining to use a simulation-based dataset, GSBlur, to pretrain the deblurring models, thus improving the generalization ability. Furthermore, the deblurring performance is enhanced by the proposed GLOWDeblur model, which consists of several auxiliary tasks like motion estimation, text-guided diffusion."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-motivated. The generalization problem in the image deblurring task is crucial and has not been fully explored.\n- The characteristic analysis of the current deblurring datasets contributes to the deblurring community.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "- There are concerns for the BBP in tackling the generalization problem: BBP relies on GSBlur, a larger existing simulation dataset. Thus, the improvement may be due to 1) GSBlur is a larger training dataset and 2) GSBlur covers the blur characteristics of each test set, rather than generalizing to new blur characteristics.\n- GLOWDeblur significantly degrades the PSNR and SSIM in most cases in Table 3. However, these are major metrics in the image deblurring task. This method may severely affect the pixel-level similarity between the output and the ground truth, which is not a satisfactory deblurring result.\n- Many auxiliary tasks (motion estimation, text-guided diffusion generation) are equipped into the model. This raises the concern of the efficiency compared with other methods. But this is not mentioned in the paper."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BoH5n96HP3", "forum": "OFk9jdUgdc", "replyto": "OFk9jdUgdc", "signatures": ["ICLR.cc/2026/Conference/Submission2344/Reviewer_CQv5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2344/Reviewer_CQv5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814090363, "cdate": 1761814090363, "tmdate": 1762916200021, "mdate": 1762916200021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}