{"id": "puocvrFZRl", "number": 15013, "cdate": 1758246864027, "mdate": 1759897335569, "content": {"title": "Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models", "abstract": "Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph interactions and associated text attributes, are prevalent in real-world applications. Existing methods, such as Graph Neural Networks (GNNs) and Large Language Models (LLMs), mostly focus on static TAGs. Extending these existing methods to DyTAGs is challenging as they largely neglect the *recent-global temporal semantics*: the recent semantic dependencies among interaction texts and the global semantic evolution of nodes over time. Furthermore, applying LLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To tackle these challenges, we propose $\\underline{Dy}$namic $\\underline{G}$lobal-$\\underline{R}$ecent $\\underline{A}$daptive $\\underline{S}$emantic $\\underline{P}$rocessing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to efficiently and effectively reason on DyTAGs. Specifically, we first design a node-centric implicit reasoning method together with a sliding window mechanism to efficiently capture recent temporal semantics. In addition, to capture global semantic dynamics of nodes, we leverage explicit reasoning with tailored prompts and an RNN-like chain structure to infer long-term semantics. Lastly, we intricately integrate the recent and global temporal semantics as well as the dynamic graph structural information using updating and merging layers. Extensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority, achieving up to 34\\% improvement in Hit@10 for destination node retrieval task. Besides, DyGRASP exhibits strong generalization across different temporal GNNs and LLMs.", "tldr": "This paper propose DyGRASP, which captures the recent-global semantics inherent in dynamic text-attribute graphs with large language models.", "keywords": ["Dynamic text-attributed graph", "graph learning", "large language model"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86d57b18aff64d428e3cd35f7e996971536a256c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the challenge of Dynamic Text-Attribute Graphs (DyTAGs), which feature time-evolving graph interactions and associated text attributes that are common in real-world applications. Existing methods, like Graph Neural Networks (GNNs) and Large Language Models (LLMs), mainly focus on static Text-Attribute Graphs (TAGs). Extending these approaches to DyTAGs is difficult because they fail to account for recent-global temporal semantics, which include both the recent semantic dependencies among interaction texts and the global semantic evolution of nodes over time. Additionally, applying LLMs to the vast and evolving text data in DyTAGs introduces efficiency problems.\n\nTo solve these challenges, the authors propose a novel method called Dynamic Global-Recent Adaptive Semantic Processing (DyGRASP). DyGRASP combines LLMs and temporal GNNs to reason on DyTAGs in an efficient and effective manner."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. DyGRASP effectively combines both recent and global temporal semantics by utilizing sliding window mechanisms for short-term dependencies and RNN-like structures for long-term dependencies. This dual approach improves the model's ability to capture evolving patterns in DyTAGs.\n\n2. The combination of LLMs for semantic processing and temporal GNNs for structural reasoning is an effective solution for addressing both the textual and graph-based challenges in DyTAGs. The model’s design optimizes the use of LLMs to handle the evolving text efficiently, solving a major bottleneck in current systems. The idea of implicit and explicit reasoning paths may provide some insights to the researchers in this field.\n\n3. This paper provides a comprehensive experimental result to verify its effectiveness."}, "weaknesses": {"value": "1. While the integration of LLMs and temporal GNNs is innovative, it also increases the overall complexity of the model. This dual integration might result in higher computational costs and more difficult tuning for large-scale applications.\n\n2. The paper highlights the efficiency of DyGRASP in handling evolving texts, but it would benefit from a more in-depth discussion of the computational trade-offs involved in its design, particularly when applied to large-scale DyTAGs with vast amounts of dynamic text."}, "questions": {"value": "Authors may consider providing more analysis about the efficiency of the framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qnFWlcux8i", "forum": "puocvrFZRl", "replyto": "puocvrFZRl", "signatures": ["ICLR.cc/2026/Conference/Submission15013/Reviewer_jwom"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15013/Reviewer_jwom"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760946872887, "cdate": 1760946872887, "tmdate": 1762925343767, "mdate": 1762925343767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an innovative framework named DyGRASP for the semantic understanding and reasoning problems of dynamic text attribute graphs. Targeting the insufficient semantic understanding ability of traditional temporal graph neural networks and the low efficiency of large language models, the authors propose a sliding window mechanism to efficiently capture recent temporal semantics, and use LLM to summarize the chain-structure long-range semantics of nodes for global information modeling. Experimental results on 4 benchmark datasets showcase its performance superiority and the generalization ability to various LLMs and temporal GNNs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper is well-written, with clear and engaging prose. The figures are well-crafted and greatly facilitate the understanding of the proposed framework's technical details.\n\nS2. Addressing the targeted challenges, namely, LLM efficiency and the modeling of global semantic evolution in dynamic text, is essential to the DyTAG framework. Each component of the proposed design is carefully motivated and well justified.\n\nS3. Extensive experimental results are provided, including the transductive & inductive performance, ablation and hyper-parameter study, as well as efficiency study, providing a comprehensive understanding of the effectiveness of the proposed framework."}, "weaknesses": {"value": "W1. Several case studies are provided to better illustrate the reasoning process of the proposed framework, such as the summarization results generated by large language models.\n\nW2. The authors employ a uniform partition strategy for global semantic summarization; however, since user intent may fluctuate over different time periods, this approach could be considered a technical limitation of the framework."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RshBtDb4N8", "forum": "puocvrFZRl", "replyto": "puocvrFZRl", "signatures": ["ICLR.cc/2026/Conference/Submission15013/Reviewer_amvQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15013/Reviewer_amvQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761455200741, "cdate": 1761455200741, "tmdate": 1762925343310, "mdate": 1762925343310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DyGRASP tackles Dynamic Text-Attributed Graphs by combining large language model reasoning with temporal GNNs. It captures recent semantics via node-centric implicit reasoning and sliding windows, and global dynamics via explicit, prompt-guided summaries chained over time. Merging these signals with graph structure yields O(|E|) efficiency and accuracy, boosting Hit@10 by up to 34% and generalizing across LLMs and temporal GNN backbones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The research topic is interesting and practically relevant, particularly the node retrieval task.\n\n2. Efficiency is a critical issue, especially when employing LLMs as encoders."}, "weaknesses": {"value": "1. There are multiple prior works on dynamic text-attributed graphs; however, the paper’s motivation is framed against static TAGs, which mispositions the contribution.\n\n2. The related work is outdated—particularly for Temporal GNNs. The most recent citations end in 2023; please include the latest works.\n\n3. The description of the “global reasoning chain” is unclear; stating it is “RNN-like” is insufficient and difficult to follow.\n\n4. Although several dynamic TAG methods are cited, the paper does not compare against them in the evaluation.\n\n**Major Concern:**\n\n**(1) Empirically, DyGRASP (DyGFormer) with global and recent modules improves performance by only ~2% over DyGFormer alone in Tables 1–2, yet removing “global and recent” in Fig. 4 causes a >10% drop.**"}, "questions": {"value": "1. What is the training pipeline for the node-retrieval task—end-to-end or two-stage?\n\n2. What is the rationale for “sorting by timestamp” in node-centric implicit reasoning?\n\n3. In Eq. (2), the formulation seems to capture local rather than global semantics; please clarify how global semantics are obtained.\n\n4. What do “gathering” and “aggregating” features mean in $MP_{G}(\\cdot)$, which typically refers to the TGNNs? What is the difference between $MP_{G}(\\cdot)$ and TGNNs' neighbor aggregation?\n\n5. Could you please compare the proposed method with existing dynamic text-attributed graph approaches?\n\n6. Could you please evaluate the effectiveness of the sliding window?\n\n**Major Question:**\n\n**(1) Empirically, DyGRASP (DyGFormer) with both global and recent modules improves performance by only ~2% over DyGFormer alone in Tables 1–2, yet removing “global and recent modules” in Fig. 4 causes a >10% drop. Could you please explain this discrepancy?**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FA2kjYjc9Q", "forum": "puocvrFZRl", "replyto": "puocvrFZRl", "signatures": ["ICLR.cc/2026/Conference/Submission15013/Reviewer_tZWv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15013/Reviewer_tZWv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878771639, "cdate": 1761878771639, "tmdate": 1762925342864, "mdate": 1762925342864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DyGRASS, a framework designed to address semantic comprehension and reasoning in dynamic text-attributed graphs. The authors design a sliding window strategy that effectively captures short-term temporal patterns. Additionally, they employ an LLM to distill long-range, chain-structured semantic information at the node level, enabling comprehensive global semantic representation learning. Evaluations conducted on four benchmark datasets demonstrate the model's outstanding performance and its adaptability to different LLM and temporal GNN backbones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The inefficiency of LLMs and the limited semantic modeling capability of GNNs represent two major challenges in modeling DyTAGs. This paper offers an effective and well-founded solution to address these issues.\n\n2. The technical design of the proposed framework is well-motivated, and its effectiveness has been empirically validated through experiments.\n\n3. Comprehensive experimental results are provided, demonstrating the superior performance and efficiency of the proposed framework."}, "weaknesses": {"value": "1. To better validate the efficiency of the proposed DyGRASS, a comparison of inference time and token consumption with other baseline models should be provided.\n\n2. Case studies illustrating the global summarization results of LLMs are recommended, along with an analysis of how global semantic information enhances prediction performance."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CKjHsJK03U", "forum": "puocvrFZRl", "replyto": "puocvrFZRl", "signatures": ["ICLR.cc/2026/Conference/Submission15013/Reviewer_hKYq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15013/Reviewer_hKYq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918507122, "cdate": 1761918507122, "tmdate": 1762925342229, "mdate": 1762925342229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}