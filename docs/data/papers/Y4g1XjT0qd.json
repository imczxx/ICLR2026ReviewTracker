{"id": "Y4g1XjT0qd", "number": 14904, "cdate": 1758245379871, "mdate": 1759897342356, "content": {"title": "FCLoRA: Low-Rank Adaptation with Fine-Grained Component Injection", "abstract": "In recent years, low-rank adaptation (LoRA) has emerged as a significant paradigm, which freezes the pre-trained weights and introduces small, learnable adapters instead of fine-tuning the full set of parameters. In this work, we uncover several key insights regarding to the $\\textit{singular}$ components of the network parameters based on Singular Value Decomposition (SVD). Firstly, the dominant singular components with large singular values in pre-trained network parameters can be effectively reused during fine-tuning, whereas the fine-grained components with smaller singular values are more task-specific and require substantial adaptation. Secondly, the growth of singular values in the LoRA adapter leads to the forgetting of pre-trained knowledge $-$ a well-known issue called $\\textit{catastrophic forgetting}$. Building upon these observations, we propose $\\textbf{FCLoRA}$, which injects learnable fine-grained singular components to the pre-trained model. By employing parameterized SVD and restricting the singular values to an appropriate range, $\\textbf{FCLoRA}$ can effectively adapt to new tasks by learning in the fine-grained singular domain and alleviates the catastrophic forgetting problem. We conduct extensive experiments and demonstrate that $\\textbf{FCLoRA}$ not only improves performance but also effectively retains pre-trained knowledge.", "tldr": "", "keywords": ["Large Language Models", "Low-Rank Adaptation", "Catastrophic Forgetting"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b536bcad428396b70fa24ee00339cb66b21e5190.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes FCLoRA, a parameter-efficient fine-tuning framework that enhances LoRA by injecting fine-grained singular components into pretrained models using a parameterized SVD formulation.  The authors analyze how different singular value ranges in pretrained weights correspond to transferable versus task-specific knowledge."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a principled analysis of the relationship between singular value spectra, task transferability, and forgetting, offering new geometric insight into PEFT.\n2. The parameterized SVD with bounded singular values is simple yet effective, easily integrable into existing LoRA pipelines.\n3. Directly mitigates catastrophic forgetting — a well-known but underexplored problem in PEFT research."}, "weaknesses": {"value": "1. I found that the conclusions on the principle subspace are somehow contradict to those in PISSA. Can the author explain the reason?\n2. The paper introduces an upper bound on singular values to prevent catastrophic forgetting. However, LoRA’s empirical success partially relies on allowing singular value amplification for better expressivity. How do you justify that a static bound does not suppress task-specific adaptation capacity, especially for highly anisotropic pretrained representations?\n3. How does FCLoRA differ in principle from AdaLoRA or PiSSA, which also modulate the singular spectrum during training? Is the key contribution the explicit upper bound, or does the fine-grained injection scheme lead to qualitatively different subspace evolution?\n4. Since SVD is required for each adapter, how does the computational overhead scale with model size? Are there efficient approximations to make FCLoRA practical for industrial-scale models?\n5. Could the authors provide quantitative evidence that this stabilization directly correlates with reduced forgetting, beyond task accuracy—e.g., via spectral entropy or principal angle evolution?"}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DpmZBkrprI", "forum": "Y4g1XjT0qd", "replyto": "Y4g1XjT0qd", "signatures": ["ICLR.cc/2026/Conference/Submission14904/Reviewer_5YNq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14904/Reviewer_5YNq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761193689152, "cdate": 1761193689152, "tmdate": 1762925248851, "mdate": 1762925248851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FCLoRA, a modification of the Low-Rank Adaptation (LoRA) method for parameter-efficient fine-tuning. The motivation stems from two main observations derived from Singular Value Decomposition (SVD) analysis of network parameters: 1) Dominant singular components (large singular values) of pre-trained weights are reusable, while fine-grained components (small singular values) are more task-specific and need adaptation. 2) The growth of singular values in the LoRA adapter ($\\Delta W$) during fine-tuning can lead to catastrophic forgetting of pre-trained knowledge, linked theoretically to a decrease in the prior probability term in MAP estimation ($p (\\theta|\\mathcal{D}_A) $). FCLoRA addresses these by injecting a learnable low-rank update $\\Delta W$ formulated using parameterized SVD ($\\Delta W = U\\Sigma V^\\top$). Critically, it restricts the learnable singular values $\\sigma_n \\in \\Sigma$ to be within an \"appropriate range\" by clipping them below a pre-defined upper bound $\\bar{\\sigma}$ (e.g., a quantile $\\sigma^{(q)}$ of $W_0$'s singular values). This aims to focus learning on the fine-grained domain and prevent excessive singular value growth, thereby improving adaptation while mitigating forgetting. Experiments on NLU, QA, and commonsense reasoning tasks are presented."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a reasonable motivation based on SVD analysis, distinguishing the roles of dominant vs. fine-grained components and linking singular value growth in adapters to catastrophic forgetting via a theoretical argument (Theorem 3.1). Figure 1 visually supports these claims.\n\n2. The core idea of using parameterized SVD and explicitly constraining the magnitude of learnable singular values ($\\Sigma$) with an upper bound $\\bar{\\sigma}$ is a direct and intuitive way to implement the paper's motivation of focusing on \"fine-grained\" adaptation and preventing excessive deviation from the pre-trained state."}, "weaknesses": {"value": "1. Limited Novelty and Distinction from Prior Work: The use of parameterized SVD in LoRA variants is not new (e.g., AdaLoRA). Several recent works also leverage SVD components for LoRA initialization or modification, such as PISSA, MiLoRA, LoRA-XS, and SORSA. FCLoRA's specific contribution lies in clipping the learnable singular values $\\Sigma$ to an upper bound $\\bar{\\sigma}$. The paper fails to sufficiently articulate why this specific mechanism is significantly novel or superior compared to these closely related methods (e.g., MiLoRA which also focuses on smaller components, or AdaLoRA which prunes components). Experimental comparisons against PISSA and MiLoRA are notably absent despite their relevance.\n\n\n2. Crucial Hyperparameter $\\bar{\\sigma}$ Underexplored: The choice of the upper bound $\\bar{\\sigma}$ is critical to FCLoRA's definition of \"fine-grained\" injection. How should $\\bar{\\sigma}$ be chosen? The paper suggests using a quantile $\\sigma^{(q)}$ of $W_0$'s singular values but provides little guidance or analysis on which quantile works best, how sensitive the results are to this choice, or whether it needs tuning per task/model. The ablation study in Appendix O.2 shows sensitivity but lacks a principled selection method. This ambiguity makes the method difficult to apply reliably.\n\n3. Insufficient Validation of Forgetting Mitigation: The primary claim regarding catastrophic forgetting mitigation relies heavily on Theorem 3.1 (based on Laplace approximation) and experiments measuring performance degradation on the pre-training task after single-task fine-tuning (Section 5.3, Figure 3). While indicative, this does not adequately simulate or evaluate performance in a proper continual learning setting with a sequence of distinct downstream tasks. Standard CL benchmarks and metrics (like backward transfer or average accuracy over a sequence) are needed to robustly validate the claims about mitigating forgetting in practice.\n\n4. Computational Aspects: Parameterized SVD requires learning $U, \\Sigma, V$ and potentially enforcing orthogonality via regularization $R(U,V)$ with coefficient $\\gamma$. How does the computational cost and optimization stability compare to standard LoRA (learning $A, B$)? The paper claims negligible overhead but provides limited analysis (Appendix P shows increased training/inference time)."}, "questions": {"value": "1. Can the authors clearly differentiate FCLoRA's contribution from related works like PISSA, MiLoRA, AdaLoRA, and SORSA, both conceptually and empirically (e.g., through direct experimental comparisons)?\n\n2. How should the crucial hyperparameter $\\bar{\\sigma}$ be set in practice? Can the authors provide guidelines or further sensitivity analysis across different models and task types? Is the quantile approach generally applicable?\n\n3. To substantiate the claim of mitigating catastrophic forgetting, could the authors evaluate FCLoRA on standard continual learning benchmarks involving task sequences and report metrics like average accuracy and backward transfer, comparing against relevant CL baselines?\n\n4. Can the authors provide more details on the practical training stability and convergence speed of learning parameterized SVD components ($U, \\Sigma, V$ with orthogonality constraints) compared to standard LoRA ($A, B$)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nKHzNObfoC", "forum": "Y4g1XjT0qd", "replyto": "Y4g1XjT0qd", "signatures": ["ICLR.cc/2026/Conference/Submission14904/Reviewer_54EY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14904/Reviewer_54EY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566224919, "cdate": 1761566224919, "tmdate": 1762925248366, "mdate": 1762925248366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FCLoRA, a LoRA variant that uses a parameterized SVD of the adapter and focuses the learnable part on fine-grained singular components, while keeping dominant components within a bounded range. The motivation has two parts. First, an empirical SVD analysis suggests that large singular components of pre-trained weights can often be reused across downstream tasks, while smaller components are more task-specific. Second, during fine-tuning, the singular values of a LoRA adapter can grow without control, and the authors link this growth to catastrophic forgetting via a MAP style argument. FCLoRA injects a low rank update through SVD and clips the learnable singular values using a quantile of the spectrum of the pre-trained weight. The goal is to let the model adapt to new tasks without erasing the pre-trained prior. Experiments on NLU, QA and several reasoning tasks show small but mostly consistent gains over common LoRA baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I like the attempt to tell a fully spectral story. The paper explains that dominant components can be reused, that fine grained components need to be adapted, and that uncontrolled growth of singular values weakens the pre-trained prior. This is intuitive and the figures support it. The method itself is conceptually light, it is just LoRA with an SVD front and a clipping rule for the singular values. The experiments cover several models and tasks, which shows that the idea is at least practical."}, "weaknesses": {"value": "The main problem is limited novelty compared with recent SVD based LoRA work. Learning U, Sigma and V for LoRA, or choosing and bounding the singular values, is already present in several papers. This work does not fully isolate what FCLoRA adds beyond saying that the goal is to mitigate forgetting.\n\nA second problem is that the crucial hyperparameter, the quantile of the pre-trained spectrum that defines the upper bound on Sigma, is under explained. The paper simply says to use a quantile of the singular values, but it does not show how sensitive the results are to this choice, whether different models or tasks need different quantiles, or whether per layer and global quantiles behave differently. At the moment this looks like a hand tuned knob.\n\nA third problem is that the evidence for forgetting is weak. The central story of the paper is catastrophic forgetting, but the experiments mainly check how much performance on the pre-training task is lost after fine-tuning a single task. This is not the same as showing mitigation on a real continual learning sequence with several distinct downstream tasks and with standard CL metrics such as average accuracy and backward transfer. Without this, it is hard to give credit for the claimed mitigation.\n\nFinally, the computational aspect of learning a parameterized SVD, including possible orthogonality regularization, is not quantified. There are no numbers for wall clock time, peak memory, or extra parameters, compared to standard LoRA or to another SVD style baseline."}, "questions": {"value": "1. Can you run at least one sequential or continual experiment, for example a chain of four or five NLU tasks, and show that FCLoRA forgets slower than standard LoRA and at least one recent SVD LoRA baseline?\n2. Can you provide a sensitivity analysis of the clipping quantile, for example per layer versus global, and quantiles such as 0.6, 0.7, 0.8 and 0.9, on two different model and task pairs?\n3. Can you report training time, peak memory and number of extra parameters for FCLoRA, for standard LoRA, and for one SVD LoRA baseline, on the same hardware and with the same sequence length?\n4. Can you clarify in a short ablation in which way FCLoRA differs from AdaLoRA or MiLoRA, other than the forgetting motivation, for example by replacing your clipping rule with their allocation rule?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CmDR8mmt25", "forum": "Y4g1XjT0qd", "replyto": "Y4g1XjT0qd", "signatures": ["ICLR.cc/2026/Conference/Submission14904/Reviewer_KCpv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14904/Reviewer_KCpv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803960867, "cdate": 1761803960867, "tmdate": 1762925247764, "mdate": 1762925247764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FCLoRA, a variant of LoRA fine-tuning that injects \"fine-grained\" singular components into a frozen pre-trained model via a parametrized SVD. FCLoRA treats the adapter adjustment weights as $U\\Sigma V^\\top$ and clips each learnable singular value $\\sigma_n$ to lie below a fixed threshold $\\bar{\\sigma}$, which is some q-th quantile of the original extracted singular values. As part of the objective function, the paper applies an orthogonality regularizer to $U$ and $V$. The main claim of the paper is that by focusing learning on smaller (or a range of) singular directions, FCLoRA adapts to new tasks while preserving the pre-trained structure, thus reducing catastrophic forgetting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- A clear intuition that the dominant singular vectors of pre-trained weights should remain fixed while only finer components adapt. Then, the idea of parametrizing the adapter by SVD with slipping is a plausible way to realize this idea. \n- Forgetting mitigation is demonstrably effective, though there is a drop in the pre-training accuracy. \n- The authors do include an ablation table comparing variants of LoRA w./w.o. SVD parametrization, orthogonality constraint, and clipping. This is a good sign that the authors tried to analyze the components of their method."}, "weaknesses": {"value": "- Several recent methods already (i) initialize/operate in SVD space, (ii) target minor components (milora), or (iii) decompose weight updates (dora). Here, the primary new element is clipping $\\Sigma$ plus an interpretation of it as fine-grained injection. The paper should more rigorously contrast their clipping to: MiLoRA's only minor-component editing, AdaLoRA's rank budgeting, and LoRA^2's multi-scale SVD, with controlled ablations where only clipping differs. \n- The theory is suggestive, but not decisive. The bound is loose and non-constructive wrt. an optimal threshold $\\bar{\\sigma}$; it motivates clipping but does not characterize when clipping is both necessary and sufficient. Spectral-norm growth study is cited to argue a typical increase in the norm, not the link to adapter-only updates, and optimizer types & batch size in these tasks are not quantified. \n- Ablations are insufficient. Although Table 5 contrasts four variants, there are still mixed changes. The effects of each component (clipping threshold, orthogonalization, and SVD parametrization) are not cleanly disentangled. For example, there is no run with parametrized SVD without clipping, or clipping without orthogonalziation. This makes it harder to know which design choices actually drive the results. \n- Note on the clipping threshold. The choice of $\\bar{\\sigma}$ is only briefly described as a fixed quantile of the pretrained singular values, and then used without justification. The main text offers no guidance on how it was chosen or how results would vary. Appendix O claims to study it, but those results & explanations need to be in the main paper. In practical usage, one would want to know if performance changes sharply or if fine-tuning on a certain set yields significantly different settings. Without this, a proper description of what is what, the method's robustness is unclear. It appears that, across models and tasks, this hyperparameter setting varies, adding another dimension of complexity. \n- Forgetting is tested only on Bookcorpus with minimal detail on the setup, and just for text. The analysis lacks diversity across pre-training corpora or modalities. \n- Despite \"no overhead\" claims, the appendix shows slower training due to orthogonal regularization. This contradicts the efficiency claim and should be reported clearly. \n- **One other important note:** While the proposed spectral clipping intuitively restricts representational flexibility, the method consistently outperforms all baselines, including the unconstrained ones and across all datasets. This is counterintuitive. The paper should analyze why a capacity-limiting constraint leads to higher downstream accuracy. Is it because of uneven hyperparameter tuning or something else? For this reason, the empirical claims seem unconvincing."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RSdK0g6229", "forum": "Y4g1XjT0qd", "replyto": "Y4g1XjT0qd", "signatures": ["ICLR.cc/2026/Conference/Submission14904/Reviewer_MCxD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14904/Reviewer_MCxD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814563145, "cdate": 1761814563145, "tmdate": 1762925246941, "mdate": 1762925246941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response #3 -  Mitigation of catastrophic forgetting"}, "comment": {"value": "Since Theorem 3.1 characterizes how the growth of singular values leads to forgetting, it shows that FCLoRA can mitigate catastrophic forgetting. In addition to the analyses in Figure 3 and Table 5, we have already conducted further experiments on catastrophic forgetting for RoBERTa and LLaMA in Appendix N, titled 'Further experiments on mitigating catastrophic forgetting'.\n\nTo further address the reviewers’ concerns, we additionally conducted more experiments on catastrophic forgetting. Note that for DeBERTa, issues in the publicly released pre-trained weights related to the pre-training head prevent running this evaluation. For the C4-en dataset, we sampled the top 10k test examples for measurement.\n\n1. RoBERTa + OpenWebText, Accuracy ($\\uparrow$)\n\n| Model       | SST2$_{finetune}$ | SST2$_{pretrain}$ | CoLA$_{finetune}$ | CoLA$_{pretrain}$ | QNLI$_{finetune}$ | QNLI$_{pretrain}$ | RTE$_{finetune}$ | RTE$_{pretrain}$ | MRPC$_{finetune}$ | MRP$_{pretrain}$ | \n|-|--|-|--|-|--|--|-|--|-|--|\n| Pre-Trained | -         | 68.21    | -         | 68.21    | -         | 68.21    | -        | 68.21    | -          | 68.21     |\n| LoRA        | 94.8      | 54.33    | 64.49     | 63.67    | 92.73     | 62.88    | 80.39    | 53.47    | 89.05      | 18.36     |\n| FCLoRA      | **95.37** | **65.67**| **64.79** | **64.31**| **93.09** | **63.59**| **83.15**| **62.69**| **90.32**  | **47.27** |\n\n\n2. RoBERTa + STORIES, Accuracy ($\\uparrow$)\n\n\n| Model       | SST2$_{finetune}$ | SST2$_{pretrain}$ | CoLA$_{finetune}$ | CoLA$_{pretrain}$ | QNLI$_{finetune}$ | QNLI$_{pretrain}$ | RTE$_{finetune}$ | RTE$_{pretrain}$ | MRPC$_{finetune}$ | MRP$_{pretrain}$ | \n|--|---|--|--|--|--|--|---|---|---|---|\n| Pre-Trained | -         | 69.49| -         | 69.49| -         | 69.49| -        | 69.49| -          | 69.49 |\n| LoRA        | 94.8      | 40.13    | 64.49     | 61.87    | 92.73     | 62.89    | 80.39    | 45.90    | 89.05      | 8.42      |\n| FCLoRA      | **95.37** | **60.38**    | **64.79** | **64.45**| **93.09** | **63.91**| **83.15**| **60.31**    | **90.32**  | **41.12** |\n   \n    \n3. LLaMA + PG19, Accuracy ($\\uparrow$) / PPL ($\\downarrow$)\n\n| Model       | LLaMA-7B Acc$_{finetune}$ | LLaMA-7B PPL$_{pretrain}$ | LLaMA-2-7B Acc$_{finetune}$ | LLaMA-2-7B PPL$_{pretrain}$ |\n|---|---|-|---|-------|\n| Pre-Trained | - | 6.66 | - | 6.61 |\n| LoRA        | 74.7 | 8.69 | 77.6 | 11.36 |\n| FCLoRA      | **78.6** | **7.78** | **81.3** | **8.92** |\n\n4. LLaMA + C4-en, Accuracy ($\\uparrow$) / PPL ($\\downarrow$)\n\n| Model       | LLaMA-7B Acc$_{finetune}$ | LLaMA-7B PPL$_{pretrain}$ | LLaMA-2-7B Acc$_{finetune}$ | LLaMA-2-7B PPL$_{pretrain}$ |\n|---|--|----|----|---|\n| Pre-Trained | - | 6.66 | - | 6.61 |\n| LoRA        | 74.7 | 8.69 | 77.6 | 11.36 |\n| FCLoRA      | **78.6** | **7.78** | **81.3** | **8.92** |\n\n\n- Continual Learning on GLUEwith order: SST-2 -> CoLA -> RTE -> MRPC\n\n\n| Model     | SST-2$_{finetune}$ | SST-2$_{pretrain}$ | CoLA$_{finetune}$ | CoLA$_{pretrain}$ | RTE$_{finetune}$ | RTE$_{pretrain}$ | MRPC$_{finetune}$ | MRPC$_{pretrain}$ |\n|-----------|------------|-----------|-----------|----------|----------|---------|-----------|----------|\n| Pre-train | -          | 61.64     | -         | 61.64    | -        | 61.64   | -         | 61.64    |\n| LoRA      | 94.50      | 45.15     | 59.74     | 48.51    | 71.00    | 40.47   | 88.97     | 33.38    |\n| MiLoRA    | 94.76      | 48.64     | 59.77     | 52.64    | 71.00    | 47.33   | 88.24     | 29.77    |\n| FCLoRA    | **95.37**  | **51.76** | **60.77** | **54.04**| **79.18**| **52.44**| **89.30** | **45.08**|\n\n- Although our main paper focuses on a single round of fine-tuning, we also evaluated FCLoRA in a continual learning setting where multiple tasks are learned sequentially. Specifically, we trained four GLUE tasks in sequence. During this process, we kept the pre-trained weights frozen and loaded only the LoRA weights from the previous task before training the next one. Compared to LoRA and MiLoRA, FCLoRA exhibited significantly slower forgetting.\n\n\n- Continual learning on Benchmark Task\n\n| Model  | Order-1        | Order-2        | Order-3        | avg            |\n|--------|-----------------|----------------|-----------------|----------------|\n| O-LoRA | 66.28 / -14.91  | 68.35 / -12.54 | 71.06 / -7.80   | 68.75 / -11.75 |\n| FCLoRA | **75.75 / -4.80** | **77.32 / -2.14** | **77.12 / -2.40** | **76.73 / -3.11** |\n\n To evaluate our model in a benchmark continual learning setting, we followed the experimental framework of O-LoRA [1]. We used the publicly released original code from the O-LoRA repository and measured both Average Accuracy (AA) and Backward Transfer (BWT). Following the original paper, AA was computed as the average accuracy over all tasks after completing the final task.\n \n \n> [1] Wang, Xiao, et al. \"Orthogonal subspace learning for language model continual learning.\" Findings of the Association for Computational Linguistics: EMNLP 2023. 2023."}}, "id": "Uxi38IdA7q", "forum": "Y4g1XjT0qd", "replyto": "Y4g1XjT0qd", "signatures": ["ICLR.cc/2026/Conference/Submission14904/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14904/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission14904/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763722436499, "cdate": 1763722436499, "tmdate": 1763722986787, "mdate": 1763722986787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response #2 -  Detailed explanation of $\\bar{\\sigma}$"}, "comment": {"value": "Setting $\\bar{\\sigma}$ as a quantile of the singular values of the pre-trained weights is a natural design choice, as it inherently accounts for the spectrum distribution across different models and layers. In practice, the spectrum distribution varies depending on the model size and the type of layer. For example, both across different models and within the same model, the largest singular value of the pre-trained weights can differ significantly.\n\n- In LLaMA-7B, the first query layer has a largest singular value of 32.21, whereas in RoBERTa the first query layer has 10.20.\n- Within the same model, the first query layer of LLaMA-7B has a singular value of 32.21, while the last query layer has 15.49.\n- Within the same model, the first query layer of LLaMA-7B has 32.21, whereas the first value layer has 4.38.\n\nThus, assigning a global constant value is not suitable for scalability across models. Moreover, since fine-grained components should consider their relative position within each singular spectrum, our quantile-based design is appropriate. Naturally, the choice of quantile varies depending on the task or model and can be configured as a hyperparameter.\n\nHowever, since we understand the reviewers’ concerns, we conducted several experiments regarding $\\bar{\\sigma}$.\n\n\n1. Ablation study on learnable $\\bar{\\sigma}$\n\n| Model           | CoLA     | RTE      | MRPC     | STS-B     |\n|-----------------|----------|----------|----------|-----------|\n| Learnable sigma | 64.21    | 82.43    | 90.28    | 91.10     |\n| FCLoRA          | **64.79**| **83.15**| **90.32**| **91.22** |\n\nThis experiment evaluates the performance when sigma_bar is learnable. We observed that using a fixed constant yields better performance than allowing sigma_bar to be learnable.\n\n\n2. Ablation study on global or layer-wise quantile\n\n\n| RoBERTa   | CoLA     | RTE      | MRPC     | STS-B     |\n|-----|----|------|----|--|\n| Descending ($\\sigma^{(4)}\\rightarrow\\sigma^{(1)}$)   | 63.03    | 83.03    | 89.87    | 90.84     |\n| Ascending ($\\sigma^{(1)}\\rightarrow\\sigma^{(4)}$)    | 63.40    | 80.99    | 88.64    | 91.06     |\n| FCLoRA (Global)     | **64.79**| **83.15**| **90.32**| **91.22** |\n \nWhile the original FCLoRA uses a uniform quantile across all layers, we evaluated the effect of varying the quantile on a layer-wise basis. In the descending setting, the quantile decreases as the layer index increases, dividing all layers into four groups. For example, in a 32-layer architecture with 'Descending', the top eight layers use $\\bar{\\sigma}=\\sigma^{(4)}$, the next eight layers use $\\bar{\\sigma}=\\sigma^{(3)}$, and so on. The ascending setting applies the reverse order. Although the optimal strategy may vary across tasks, using a single consistent quantile across all layers consistently yielded the most stable and strong performance.\n\n\n3. Sensitivity study for quantile on performance and mitigation of catastrophic forgetting: \n\n| Task | Metric        | 0.1     | 0.3     | 0.5     | 0.7     | 0.9     |\n|------|---------------|---------|---------|---------|---------|---------|\n| CoLA | Acc$_{finetune}$  | 64.17   | 64.60  | **64.79** | 63.32  | 62.59  |\n|      | Acc$_{pretrain}$  | 42.76   | 47.03  | **54.78** | 50.47  | 52.28  |\n| MRPC | Acc$_{finetune}$  | 89.95   | 89.71  | **90.32** | 89.95  | 89.62  |\n|      | Acc$_{pretrain}$  | 7.41    | 25.80  | 32.00  | 31.21  | **42.25** |\n| STSB | Acc$_{finetune}$  | **91.09** | 91.04  | 91.08  | 91.02  | 91.02  |\n|      | Acc$_{pretrain}$  | 47.51   | **51.34** | 45.99  | 50.92  | 51.29  |\n| RTE  | Acc$_{finetune}$  | 80.14   | 80.99  | 81.71  | 81.95  | **83.03** |\n|      | Acc$_{pretrain}$  | 28.29   | 41.25  | 39.50  | **48.45** | 45.57  |\n\n| Model       | 0.1            | 0.3            | 0.5            | 0.7            | 0.9            |\n|-------------|----------------|----------------|----------------|----------------|----------------|\n| SQuAD1 $r=4$  | **88.1**/93.8  | 87.8/93.8  | **88.1**/**93.9**      | 88.0/**93.9**      | 88.0/93.8      |\n| SQuAD1 $r=8$  | 88.0/94.1  | 87.8/93.8  | **88.6**/**94.3** | 88.0/93.9      | 87.9/93.8      |\n\n\n\nWe measured the quantile-dependent performance variation for NLU and QA tasks, and additionally evaluated sensitivity to catastrophic forgetting for NLU tasks. (For DeBERTa, issues in the publicly released model related to the pre-training head prevented us from conducting experiments to directly measure pre-training performance. [link](https://github.com/microsoft/DeBERTa/issues/74)) The results show that when the quantile is too small and $\\bar{\\sigma}$ becomes excessively large, pre-training performance degrades significantly. This effect is particularly strong for MRPC and RTE when the quantile is 0.1, where performance drops sharply. In contrast, when the quantile increases to an appropriate range and sigma_bar becomes sufficiently small, we find that both pre-training retention and downstream fine-tuning performance can be simultaneously achieved."}}, "id": "3pq5DeO0uQ", "forum": "Y4g1XjT0qd", "replyto": "Y4g1XjT0qd", "signatures": ["ICLR.cc/2026/Conference/Submission14904/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14904/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission14904/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763722782341, "cdate": 1763722782341, "tmdate": 1763722782341, "mdate": 1763722782341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response #1 - Novelty of FCLoRA compared to SVD-based model"}, "comment": {"value": "Our methodology is an efficient approach rigorously devised from empirical evidence for adaptation across the singular value groups of the pre-trained weight (Figure 1) and the theoretical foundation for forgetting (Theorem 3.1). We explained the works referenced by the reviewers (PiSSA, MiLoRA, AdaLoRA) in Related Work Section 2.2, included SVD-based models as baselines in Table 1 to Table 3 of the main text to empirically demonstrate the effectiveness of our method in adaptation, and further showed in Section 5.1 that our approach addresses the limitations of Explicit SVD-based LoRA. In Appendix N.2, we also demonstrated that our method mitigates forgetting compared to parameterized SVD-based LoRA (AdaLoRA, SorSA).\n\n\nMoreover, contrary to the reviewers' statement, the use of parameterized SVD in AdaLoRA and LoRA$^2$ was intended for rank allocation rather than singular value analysis. SorSA applies parameterized SVD on top of PiSSA's initialization scheme. Therefore, none of these methods address the magnitude of singular values *during* fine-tuning. Spectral initialization methods, including PiSSA and MiLoRA, only propose initializing adapters based on standard SVD theory and do not consider the evolution of the spectrum throughout fine-tuning. Consequently, as shown in Figure 3, the spectral norm of the adapter grows rapidly during fine-tuning. We have explicitly discussed all such points in the related work and the appendix.\n\n\nNevertheless, considering the remaining concerns raised by the reviewers, we conducted additional experiments.\n\n1. Ablation study for integrating FCLoRA with existing models\n\n| Method            |  CoLA | QNLI  | RTE   | MRPC  | STS-B |\n|-------------------|-------|-------|-------|-------|-------|\n| LoRA              | 64.49 | 92.73 | 80.39 | 89.05 | 90.87 |\n| MiLoRA            | 64.31 | 92.96 | **81.35** | **89.30** | 90.06 |\n| MiLoRA + FCLoRA   | **64.57** | **93.11** | 81.23 | **89.30** | **91.00** |\n| AdaLoRA           | 61.37 | 92.54 | 81.11 | 89.05 | 90.62 |\n| AdaLoRA + FCLoRA  | **64.30** | **93.16** | **82.67** | **89.78** | **91.02** |\n| FCLoRA            | **64.79** | 93.09 | **83.15** | **90.32** | **91.22** |\n\nOur method generates fine-grained components by clipping singular values according to the spectrum of the pre-trained weights under a fixed rank. The proposed approach can be integrated with models using parameterized SVD such as AdaLoRA and LoRA$^2$, as well as spectral initialization methods such as PiSSA and MiLoRA. We evaluated the performance by integrating our method with promising approaches like AdaLoRA and MiLoRA. In this ablation, both the backbone models and FCLoRA used the best hyperparameters identified for each model. As shown in the table below, integrating our method leads to substantial performance improvements in most cases. This indicates that existing models still have room to utilize their limited parameters more effectively and that learning fine-grained components contributes significantly to improving adaptation.\n\n2. More comparisons on baselines \n\n| Model       | PEFT   | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA | Avg. |\n|-------------|--------|-------|------|------|-----------|------------|-------|-------|------|------|\n| ChatGPT     | -      | 73.1  | 85.4 | 68.5 | 78.5      | 66.1       | 89.8  | 79.9  | 74.8 | 77.0 |\n| LLaMA2-7B   | LoRA   | 69.8  | 79.9 | 79.5 | 83.6      | 82.6       | 79.8  | 64.7  | 81.0 | 77.6 |\n|             | PiSSA  | 67.6  | 78.1 | 78.4 | 76.6      | 78.0       | 75.8  | 60.2  | 75.6 | 73.8 |\n|             | MiLoRA | 67.6  | 83.8 | 80.1 | 88.2      | 82.0       | 82.8  | 68.8  | 80.6 | 79.2 |\n|             | FCLoRA | 73.2  | 82.9 | 79.8 | 91.9      | 83.0       | 85.2  | 71.6  | 82.6 | **81.3** |\n\n| QA      | SQuADv1.1 (0.08%) | SQuADv1.1 (0.65%) |\n|---------|-------------------|-------------------|\n| AdaLoRA | 87.2/93.4         | 87.6/93.7         |\n| MiLoRA  | 87.1/93.3         | 85.5/92.0         |\n| PiSSA   | 87.2/93.2         | 85.6/92.1         |\n| FCLoRA  | **88.1/93.9**         | **88.9/94.3**         |\n\n\nWe additionally report results for commonsense reasoning and question answering to further validate the performance of FCLoRA. For commonsense reasoning, the PiSSA and MiLoRA results are taken from the MiLoRA paper, and for QA we tuned the models using the learning rate configured for AdaLoRA. The results confirm that our method continues to outperform baselines across both tasks."}}, "id": "1cQmn7WDjf", "forum": "Y4g1XjT0qd", "replyto": "Y4g1XjT0qd", "signatures": ["ICLR.cc/2026/Conference/Submission14904/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14904/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission14904/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763723016903, "cdate": 1763723016903, "tmdate": 1763723016903, "mdate": 1763723016903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}