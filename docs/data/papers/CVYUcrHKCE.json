{"id": "CVYUcrHKCE", "number": 10632, "cdate": 1758178155691, "mdate": 1762922276772, "content": {"title": "Schema-Refiner: Synergizing Knowledge Graphs and LLMs for Proactive Schema Refinement in Text-to-SQL", "abstract": "Text-to-SQL broadens data access but often underperforms in real-world databases. We trace a key cause to lexical–-schema ambiguity, which manifests as homonymy, synonymy, and irregular naming that obscure the mapping between user utterances and schema elements, leading models astray. Prior work primarily deploys interactive, downstream fixes (e.g., robust decoding or clarification), which do not resolve the schema’s intrinsic ambiguity. To mitigate this challenge, we propose Schema-Refiner, a neuro-symbolic framework that (i) builds a schema knowledge graph, (ii) applies community detection to recover column-level context, (iii) uses a large language model to infer canonical semantics, and (iv) synthesizes CREATE VIEW statements, exposing a standardized, disambiguated logical schema layer for direct consumption by downstream Text-to-SQL models. This layer leaves the database untouched; a rule-based rewriter maps queries over the views into equivalent SQL on the original schema. To evaluate robustness to lexical–schema ambiguity and the effectiveness of our approach, we construct Amb-Spider by injecting ambiguities into Spider with human-verified annotations. Across multiple state-of-the-art Text-to-SQL systems, Amb-Spider consistently reduces execution accuracy. When paired with Schema-Refiner, these systems better detect ambiguities and regain a large share of the lost accuracy.", "tldr": "We introduce a framework that automatically cleans up and standardizes ambiguous database schemas, making Text-to-SQL systems more robust and accurate.", "keywords": ["Text-to-SQL", "Schema Ambiguity", "Schema Refinement", "Neuro-Symbolic Methods", "Knowledge Graph"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3dd57ab1cf415fa6813fa398a23e1d497d33c4cb.pdf", "supplementary_material": "/attachment/8c742ca3778b568bd181fdcebde6009752d01c11.zip"}, "replies": [{"content": {"summary": {"value": "The paper attempts to address the challenge of linking text to schema elements in the Text-to-SQL task.  They focus on hononymy (different columns in the schema with similar names), synonymy, and uninformative column names.  They propose a pipeline based on first infering a column context based on column-values, and other columns of the schema represented as a knowledge graphs, getting canonical names of the columns using an LLM, and then based on some logic creating views in terms of informative, un-ambigous column names.  Queries are generated on the view, and programmatically transformed to SQL on the actual physical schema."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The idea of creating a virtual clean schema as a view makes a lot of sense in practice.\n\n* The experimental results show significant gains with their proposed strategy of creating virtual schema\n\n* The problem tackled in the paper is of great practical importance."}, "weaknesses": {"value": "* W1 Important details needed to assess the worth of their proposed claims are missing.  The paper mostly sketches out the top-level idea, but does not explain the working of each step of their idea. \n     * W1(a)  A crucial missing part is on the creation of views.  After the tests in Equations (6)--(8) are conducted, on what set are the views created and how?   \n     * W1 (b) To which knowledge graph is the schema mapped?  \n     * W1 (c) How eactly is the community discovery algorithm run? The paper mentions a proper noun (Leiden) as a the community discovery algorithm without any citation.\n\nW2 The paper defines homonymy etc purely in terms of column names. However, an absolute column name does not make sense.  Shouldn't all your definitions prefix each column name with the name of its containing table.  Generic column names like \"name\" and \"id\" are often repeated across tables in a schema, and they are not to be treated stand-alone.\n\nW3 The experiments are not reproducible.  Paper does not provide details of how exactly they obtained the ambiguous schema starting from the Spider schema.\n\nW4 The topic of ambiguous or noisy column names has been studied before, and the paper cites those papers, however their discussion of the related work is perfunctionary and does not present nuanced analysis of prior work and technically how their approach differs.  Related work is too generic and high-level and reads like it is LLM-generated.\n\nW5  The paper crucially relies on the LLM to get canonical schema names based on column-context and sampled values.  How will this help for numerical columns or with private niche domain specific schema?  Also, please note that almost all SOTA Text-to-SQL systems attach sampled values with column names when inputing the schema to the LLM for text-to-sql generation.\n\nW6 The paper does not consider SOTA text-to-sql systems in its evaluations.\n\nW7 The paper does not empircally compare with any of the prior work on mitigating effect of ambiguous schema."}, "questions": {"value": "Please address the concerns raised in the weak points above.  Other minor concerns below.\n\nIn line 187, the set of candidate column pairs for synonymy detction seems needless large.\n\nOn what basis are the three thresholds chosen? Would these work for schema where table names differ from each other only by a small suffix (example date range).  Please see the Spider V2 dataset schema.\n\nIn Figure 1, it is not at all clear how LLM helps identify that the salaries are monthly, not annual.  How is the full name extracted from the given schema?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Cnu8Rm6iAD", "forum": "CVYUcrHKCE", "replyto": "CVYUcrHKCE", "signatures": ["ICLR.cc/2026/Conference/Submission10632/Reviewer_wJE5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10632/Reviewer_wJE5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761049618223, "cdate": 1761049618223, "tmdate": 1762921888304, "mdate": 1762921888304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "YRpuiIwM3A", "forum": "CVYUcrHKCE", "replyto": "CVYUcrHKCE", "signatures": ["ICLR.cc/2026/Conference/Submission10632/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10632/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762922275697, "cdate": 1762922275697, "tmdate": 1762922275697, "mdate": 1762922275697, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses lexical-schema ambiguity (homonymy, synonymy, irregular naming) in Text-to-SQL by building a schema knowledge graph, using Leiden community detection to extract context, employing LLMs to infer canonical names, and generating CREATE VIEW statements for a clean logical layer. They introduce Amb-Spider, a benchmark with controlled ambiguity injection, and show that state-of-the-art Text-to-SQL models degrade 3-25% on ambiguous schemas but recover 45-75% of lost performance with their refinement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Schema ambiguity does hurt Text-to-SQL systems. The 20+ percentage point drops on DIN-SQL/DAIL-SQL validate this matters.\n2. CREATE VIEW layer is pragmatic—no database modification, queries auto-rewritten back to original schema."}, "weaknesses": {"value": "1. Synthetic benchmark when real ones exist: Amb-Spider artificially injects ambiguities into the clean Spider dataset, but Spider 2.0 and BIRD already contain real enterprise-level schema messiness. Testing on synthetic data doesn't validate whether the approach works on actual production databases. The paper should evaluate on Spider 2.0/BIRD to demonstrate real-world effectiveness.\n\n2. Insufficient literature review: Schema graphs have been standard in Text-to-SQL (RAT-SQL, BRIDGE, LGESQL). The graph construction is similar to RAT-SQL. The paper needs to properly situate contributions within existing work and clearly articulate what's fundamentally new beyond applying 2024 LLMs to established techniques.\n\n3. Outdated technical design: The pipeline uses Leiden (2019 structural algorithm) when modern LLM embeddings capture both structure and semantics. More critically, the system uses embeddings + threshold grid search (testing 64 threshold combinations) for classification when direct LLM prompting (\"are these columns the same concept?\") would be simpler, require no tuning, and provide better contextual reasoning at comparable cost."}, "questions": {"value": "1. Can you provide results on Spider 2.0 or BIRD to validate the approach works on actual enterprise databases rather than synthetic ambiguity?\n\n2. Direct LLM classification: What's the performance and cost comparison between your embedding + threshold approach versus directly asking GPT-4 to classify ambiguities? Include ablation study showing whether the added complexity provides meaningful benefit.\n\n3. Documentation baseline: For databases with existing metadata (even if incomplete), how does Schema-Refiner compare to simply providing that documentation to the Text-to-SQL model? What percentage of real databases lack sufficient documentation to make your approach necessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "doEOmyxCUE", "forum": "CVYUcrHKCE", "replyto": "CVYUcrHKCE", "signatures": ["ICLR.cc/2026/Conference/Submission10632/Reviewer_wutB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10632/Reviewer_wutB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713880689, "cdate": 1761713880689, "tmdate": 1762921887939, "mdate": 1762921887939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of lexical–schema ambiguity in Text-to-SQL tasks, where ambiguous or inconsistent column/table names lead to reduced model accuracy. The authors propose Schema-Refiner, a neuro-symbolic pipeline that proactively refines database schemas before query generation. It builds a schema knowledge graph, applies community detection, uses an LLM to infer canonical names, and generates CREATE VIEW statements to create a cleaner logical schema layer. A new benchmark, Amb-Spider, is introduced by injecting ambiguity into Spider to test this setting. Experiments show that ambiguity significantly degrades Text-to-SQL performance and that Schema-Refiner partially recovers the lost accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Clear motivation and problem importance. Lexical–schema ambiguity is a real and underexplored issue for Text-to-SQL systems, and the paper articulates this motivation clearly.\n- Well-structured and interpretable system design. \n- The pipeline is easy to follow and combines knowledge graphs, community detection, and LLM-based canonicalization into a cohesive workflow. The non-destructive CREATE VIEW strategy is practical."}, "weaknesses": {"value": "- Limited novelty and technical depth.\nWhile the integration of different components is thoughtful, the method itself feels incremental. It primarily combines existing techniques—graph construction, LLM inference, and view generation—without introducing new learning formulations or algorithms. The contribution is mainly system-engineering oriented rather than algorithmic, which falls short of ICLR’s emphasis on methodological innovation.\n- Narrow and synthetic evaluation.\nThe evaluation focuses on demonstrating that ambiguity harms Text-to-SQL and that Schema-Refiner can mitigate this harm. However, there is no comparison to current state-of-the-art models under realistic or noisy conditions, and the benchmark (Amb-Spider) is synthetically generated by LLMs rather than derived from real-world databases. As a result, the practical and external validity of the findings is limited.\n- Generalizability and scalability concerns.\nThe pipeline appears ad-hoc and domain-specific, relying on multiple hand-tuned thresholds and prompting heuristics. Its performance on large-scale, multilingual, or cross-domain schemas is unclear. The dependence on repeated LLM inference also raises scalability and cost concerns.\n- No analysis of efficiency or deployment overhead.\nSchema refinement involves multiple LLM calls and graph operations, but the paper does not provide runtime or computational cost analysis. The absence of efficiency metrics weakens the practical argument for adoption."}, "questions": {"value": "The paper presents a meaningful exploration of schema ambiguity and a well-motivated system design. However, its novelty and generalizability are limited, and the evaluation does not convincingly establish that Schema-Refiner represents a substantive advance over existing approaches."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CV1uSqbeq6", "forum": "CVYUcrHKCE", "replyto": "CVYUcrHKCE", "signatures": ["ICLR.cc/2026/Conference/Submission10632/Reviewer_WNY2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10632/Reviewer_WNY2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136691568, "cdate": 1762136691568, "tmdate": 1762921887562, "mdate": 1762921887562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on lexical–schema ambiguity in Text-to-SQL systems, which arises from homonymy, synonymy, and irregular table/column naming and degrades accuracy in real-world databases. The authors propose Schema-Refiner, a neuro-symbolic framework that builds a schema-based knowledge graph, performs community detection, queries an LLM to infer canonical field names and semantics, and automatically generates CREATE VIEW statements to expose a clean schema layer for downstream NL2SQL models. They also create Amb-Spider, an injected-ambiguity benchmark derived from Spider. Experiments show large accuracy drops under ambiguity and partial recovery when using the proposed refinement layer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper tries to address a real challenge, schema ambiguity, in Text-to-SQL. This reflects the real-life scenarios in many enterprise or open databases.\n\nS2. The view creation essentially builds a semantic layer on top of the ambiguous schema, which provides standard/clean contextual information for Text-to-SQL.\n\nS3. The newly created benchmark demonstrates non-trivial performance degradation with introduced noise."}, "weaknesses": {"value": "W1. The paper has limited novelty. The proposed solution essentially combines a set of off-the-shelf components (KG, LLM prompting, and view-based abstraction). As a result, the contribution reads more as a system integration effort.\n\nW2. Synthetic ambiguity injection may not reflect realistic enterprise schema drift. It is unclear whether Amb-Spider aligns with real production workloads. Moreover, the approach has not been evaluated on large/complex enterprise schemas with thousands of tables/columns, where schema noise, denormalization, and missing relational structure (pk/fk) are common, raising concerns about scalability and practical applicability.\n\nW3. The proposed method heavily relies on LLM heuristics with insufficient ablations to isolation the value of each component (e.g., prompt design, KG integration vs. table context alone). Key design choices such as similarity thresholds and ambiguity definitions feel heuristic rather than grounded in principled semantic criteria.\n\nW4. The synthetic dataset could be overfitting to Spider training distributions via semantics learned from dataset itself. Moreover, Spider dataset is relatively  small and structurally simple compared to enterprise database settings. The evaluation focuses primarily on execution accuracy."}, "questions": {"value": "Q1. How realistic is Amb-Spider compared to real corporate schema? Any human-in-the-loop/SME validation?\n\nQ2. How often do canonical names diverge from true business semantics even on clean Spider?\n\nQ3. Did you evaluate simple baselines (e.g., camelCase → words, prefix/suffix normalization, column-embedding clustering)? For example, NameGuess introduce a suite of techniques to introduce noise to database schema.\n\nQ4. What is the computational cost? LLM inference for every column can be expensive.\n\nQ5. Can the system gracefully handle mixed semantic fields or JSON column payloads in modern warehouses?\n\nQ6. Does this generalize to enterprise schemas with thousands of tables, weak foreign key use, or buried semantic constraints?\n\nQ7. Is data probing feasible in regulated or realistic settings where direct data sampling is restricted or the query workload is under control?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mcSH7XaJ44", "forum": "CVYUcrHKCE", "replyto": "CVYUcrHKCE", "signatures": ["ICLR.cc/2026/Conference/Submission10632/Reviewer_bVhx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10632/Reviewer_bVhx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152538553, "cdate": 1762152538553, "tmdate": 1762921887141, "mdate": 1762921887141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}