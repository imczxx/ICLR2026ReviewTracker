{"id": "VT3tTfXrDi", "number": 5918, "cdate": 1757946350846, "mdate": 1763718664542, "content": {"title": "ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation", "abstract": "While recent generative models synthesize high-quality visual content, they still struggle with generating rare or fine-grained concepts.\nTo address this challenge, we explore the usage of Retrieval-Augmented Generation (RAG) for image generation, and introduce ImageRAG, a training-free method for rare concept generation. \nUsing a Vision Language Model (VLM), ImageRAG identifies generation gaps between an input prompt and a generated image dynamically, retrieves relevant images, and uses them as context to guide the generation process. \nPrior approaches that use retrieved images require training models specifically for retrieval-based generation. In contrast, ImageRAG leverages existing image conditioning models, and does not require RAG-specific training.\nWe demonstrate our approach is highly adaptable through evaluation over different backbones, including models trained to receive image inputs and models augmented with a post-training image-prompt adapter. \nThrough extensive quantitative, qualitative, and subjective evaluation, we show that incorporating retrieved references consistently improves the generation abilities of rare and fine-grained concepts across three datasets and three generative models.", "tldr": "Rare concept image generation using dynamically retrieved image references.", "keywords": ["RAG", "image generation", "rare-concept generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/41f7895da558db20bd60fdad1c28880d5da3cdea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to address the recent generative models problems in generating rare or fine-grained concepts. ImageRAG framework is introduced as a training-free method for rare concept generation. ImageRAG identifies generation gaps with the guidance from a Vision Language Model. The VLM retrieves relevant images while ImageRAG uses them as context to guide the generation process."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of applying RAG in image generation is up-to-date.\n\n2. The method illustration is clear.\n\n3. Writing is easy to follow."}, "weaknesses": {"value": "1. As the most relevant work, RealRAG[A] is only mentioned in the related work section, while ignoring in the other sections, especially for the introduction and experiments.  \n\na. Line-090: \"previous works employing image retrieval for better image generation, train models specifically for the task, hindering wide applicability.\" The reviewer believe RealRAG does not train models specifically for task. \n\nb. In the experiments, only several base models are included for comparison, more methods in this area should be compared, especially for RealRAG[A].\n\n[A] Lyu Y, Zheng X, Jiang L, et al. RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning[C]//Forty-second International Conference on Machine Learning.\n\n2. In the method, CoT is used for identifying challenging concepts. Could the author provide more discussion regarding this part? Is there a  trade-off between the CoT length and the generation performance? Is there any over thinking problem in this scenarios?"}, "questions": {"value": "### **1. On Related Work and Comparison with RealRAG**\n\n**a. Conceptual Differences**\n\n* In this paper, *RealRAG* (Lyu et al., ICML 2024) is mentioned only briefly in the related work section but not discussed elsewhere.\n  Could the author elaborate on how the method differs conceptually from RealRAG, especially given that RealRAG also employs retrieval-augmented generation but does **not** require task-specific model retraining?\n* What are the main methodological distinctions between ImageRAG and RealRAG in terms of:\n\n  * Retrieval mechanisms and\n  * Integration with the generation process?\n\n**b. Experimental Comparison**\n\n* In the experiments, only a few baseline models are included. Could the authors clarify why RealRAG and other recent retrieval-based image generation methods were excluded from the comparison?\n* Have the authors evaluated the framework under the same experimental conditions as RealRAG (e.g., dataset, retrieval backbone, evaluation metrics) to ensure a fair comparison?\n\n---\n\n### **2. On Chain-of-Thought (CoT) Reasoning for Concept Identification**\n\n* The authors mention that CoT reasoning is used to identify challenging or ambiguous concepts. Could the authors expand on how the CoT process interacts with the retrieval or generation stages?\n* Is there a trade-off between **CoT length** and **generation quality** (e.g., longer CoTs improving reasoning but reducing visual fidelity or inference speed)?\n* Have the authors observed any “overthinking” effects — cases where extended reasoning sequences introduce redundancy or semantic drift — and if so, how do the authors mitigate them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Aph5lgkIPx", "forum": "VT3tTfXrDi", "replyto": "VT3tTfXrDi", "signatures": ["ICLR.cc/2026/Conference/Submission5918/Reviewer_vzAE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5918/Reviewer_vzAE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761561631984, "cdate": 1761561631984, "tmdate": 1762918349704, "mdate": 1762918349704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose a image Retrieval-Augmented Generation (RAG) framework for image generation named ImageRAG. ImageRAG using VLM to retrieves relevant images to enhance custom image generation. Through extensive quantitative, qualitative, and subjective evaluation, paper show that incorporating retrieved references consistently improves the generation abilities of rare and fine-grained concepts."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper proposes a novel pipeline designed for image retrieval-augmented generation (RAG), which can retrieve relevant images to enhance the generative performance of diffusion models.\n2. The proposed framework automates the common engineering practice of improving generation quality through reference images, demonstrating strong practical value.\n3. The paper introduces an efficient approach for leveraging image knowledge database, which offers valuable insights for the development of multimodal large models.\n4. The paper is clear and easy to follow."}, "weaknesses": {"value": "1. If the target concept to be generated does not exist in the retrieval dataset, is there a possibility that irrelevant images may be selected, thereby potentially affecting the generation quality adversely? The paper should include corresponding quantitative analyses.  \n2. [Retrieval-Augmented Diffusion Models] has proposed a similar concept. The paper should provide a detailed comparison with this work with same base model and dataset, and analyze advantages of ImageRAG.  \n3. Will the diversity of generated images be constrained by the retrieval dataset, leading to visually similar outputs for same prompts? The paper should include relevant analyses to clarify this."}, "questions": {"value": "SDXL+IPA have different image and text input interface, and cannot use the prompt template in Figure 3. How dose the ImageRAG handle models like this? Are the images correctly associated with the corresponding text snippets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dNBRW8xQEu", "forum": "VT3tTfXrDi", "replyto": "VT3tTfXrDi", "signatures": ["ICLR.cc/2026/Conference/Submission5918/Reviewer_1GfW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5918/Reviewer_1GfW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618696728, "cdate": 1761618696728, "tmdate": 1762918349387, "mdate": 1762918349387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RAG into text-to-image generation process for some rare or fine-grained concepts. This approach is training-free and model agnostic, and can operate with off-the-shelf image-conditioning interfaces. Image RAG contains four steps: 1/ rough image generation from a text prompt; 2/ use VLM to identify the missing concepts in a CoT process,  3/ retrieve reference images corresponding to the missing concepts from an external dataset; 4/ regenerate the image using these reference images by a generative model. The authors evaluated the method using three different base models, including SDXL, FLUX, and OmniGen, on three benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The adaptation of RAG to text-to-image generation is inspiring. It focuses on \"polish\" the generated image at inference time rather than retraining.\n\n- The formulation of CoT-based retrieval is promising, which is effective for identifying the missing visual elements.\n\n- The evaluation is comprehensive, including qualitative comparisons, user study, and ablation studies. It is good to see that failure cases are included in this paper, which help the reader to better understand the limitation of this paper."}, "weaknesses": {"value": "- The applicable scenario is limited. ImageRAG suits most for the scenario where rare or even weird concepts exist, such as \"a boston bull\", but the successfulness greatly depends on the coverage of the retrieval dataset (e.g. LAION-350K subset). It would be better to see that this method can handle well with the complex and lengthy prompts that are usually more customer-driven. \n\n- No report or measurement of the latency and computational cost of this multi-step pipeline, which is even more important than the metrics measuring image quality. We should always consider the latency for real-time or interactive use cases.\n\n- The technical novelty is limited. Even though the concept of ImageRAG is good, each component in this framework is individually normal. And the success of ImageRAG heavily rely on the performance of the submodule such as CLIP, GPT-4o and T2I, etc. \n\n- The performance improvements in CLIP or DINO metrics are in general very limited (<0.01 absolute)."}, "questions": {"value": "- Since the limitations are listed in this paper, is there any solution or strategy to improve them?\n\n- Is there any failure cases that VLM misidentify the gaps between text prompts and rough images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5y6bMHyzAq", "forum": "VT3tTfXrDi", "replyto": "VT3tTfXrDi", "signatures": ["ICLR.cc/2026/Conference/Submission5918/Reviewer_fN6i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5918/Reviewer_fN6i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898589542, "cdate": 1761898589542, "tmdate": 1762918349030, "mdate": 1762918349030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ImageRAG, a training-free retrieval-augmented generation method that enhances text-to-image models by dynamically identifying generation gaps using a vision-language model and retrieving relevant reference images to guide the synthesis of rare or fine-grained concepts. The approach is model-agnostic, demonstrated through evaluations on diverse backbones like OmniGen, SDXL, and FLUX, showing improved alignment with prompts without requiring additional training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method's adaptability is a key strength, as it seamlessly integrates with existing image-conditioning models like IP-Adapter and OminiControl, enabling broad applicability across different architectures. Extensive experiments, including quantitative metrics, qualitative examples, and human studies, robustly validate that ImageRAG consistently improves rare concept generation, with users preferring it over baselines in text alignment and visual quality."}, "weaknesses": {"value": "1. The method's efficacy is highly dependent on the retrieval dataset; if it lacks relevant images (e.g., specializing in birds when generating dogs), performance may not improve, as illustrated in the retrieval data limitations.\n\n2. It relies on the VLM's accuracy for gap identification; errors in concept detection (e.g., false positives in alignment checks) could lead to missed enhancements, though the paper notes robustness issues with some VLMs.\n\n3. While the paper notes VLM API calls add 10–30 seconds, it lacks comparisons to baseline T2I inference latency and does not explore optimizations (e.g., cached embeddings for small datasets) for real-time use cases.\n\n4. Although diversity is mentioned, the paper does not quantify how ImageRAG impacts generation diversity across models (e.g., SDXL’s low diversity vs. OmniGen’s high diversity) or whether retrieval introduces bias toward reference image styles."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DQUXVPPXtu", "forum": "VT3tTfXrDi", "replyto": "VT3tTfXrDi", "signatures": ["ICLR.cc/2026/Conference/Submission5918/Reviewer_bsqR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5918/Reviewer_bsqR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5918/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930952295, "cdate": 1761930952295, "tmdate": 1762918347918, "mdate": 1762918347918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}