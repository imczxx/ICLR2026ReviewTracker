{"id": "rjs5K6JMWp", "number": 12645, "cdate": 1758209223200, "mdate": 1759897496464, "content": {"title": "Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning", "abstract": "Reinforcement learning (RL) has played an important role in improving the reasoning ability of large language models (LLMs). Some studies apply RL directly to \\textit{smaller} base models (known as zero-RL) and also achieve notable progress. However, in this paper, we show that using only 920 examples, a simple distillation based on the base model can clearly outperform zero-RL, which typically requires much more data and computational cost. By analyzing the token frequency in model outputs, we find that the distilled model shows more flexible reasoning. It uses anthropomorphic tokens and logical connectors much more often than the zero-RL model. Further analysis reveals that distillation enhances the presence of two advanced cognitive behaviors: Multi-Perspective Thinking or Attempting and Metacognitive Awareness. Frequent occurrences of these two advanced cognitive behaviors give rise to flexible reasoning, which is essential for solving complex reasoning problems.", "tldr": "We find that distillation with a small number of samples can endow the base model with flexible reasoning patterns, which are crucial for tackling complex reasoning tasks.", "keywords": ["LLM", "reasoning", "distillation", "zero-RL"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4637cd61d3160c158ff393d6f3b9e6cb1450c140.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work discuss the performance gap between zero-RL models and distilled models.  Authors adopt the popular open-sourcing zero-RL models compared to their own fine-tuned distilled model, and observe that distilled model significantly outperforms the zero-RL models. Authors consider that this performance gap is attributed to different token distributions; is that, the distilled model is more likely to think like human, containing more anthropomorphic tokens and logical tokens. Further analysis shows that even forbiddening these distinctive tokens, the distilled model can still perform comparable to zero-RL models. Authors also analyze the advanced cognitive behaviors, i.e. multi-perspective thinking or attempting and metacognitive awareness, and find that the distilled model consistently perform better."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work studies an interesting topic of comparing distillation and zero-RL training.\n\nAuthors conduct in-depth analysis to explore why distillation is better to zero-RL training.\n\nGood writing work."}, "weaknesses": {"value": "1. Insufficient evaluations. This work only evaluates on 32B models. More evaluations should be conducted on smaller size models and other model series, like Qwen-2.5/3-7B/8B/14B or LLaMA models. All experiments conducted in this work are based on Qwen2.5-32B, which cannot reflect the generaliablity of the analysis.\n\n2. I am a bit confused on the motivation of the work. It is good to reveal that the distilled model is better to zero-RL models for Qwen2.5-32B series models. But, does it commonly stand for most models, distillation methods and zero-RL methods? I do not catch any new techniques or insightful observations from the work.\n\n3. How about the effects of the amount of training data and different teacher models?"}, "questions": {"value": "See weakness.\n\nLine 411: Wwhen -> When"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qi10Jk80UL", "forum": "rjs5K6JMWp", "replyto": "rjs5K6JMWp", "signatures": ["ICLR.cc/2026/Conference/Submission12645/Reviewer_mG5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12645/Reviewer_mG5M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568926734, "cdate": 1761568926734, "tmdate": 1762923486518, "mdate": 1762923486518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines whether small-scale reasoning distillation can outperform zero-shot Reinforcement Learning (RL) and explores the underlying reasons. The authors conducted a controlled experiment using the Qwen 2.5 32B model, comparing its distillation with a teacher against three distinct RL strategies. They then analyzed two significant linguistic patterns: anthropomorphic patterns and logical connectors and used them to explain why distillation fosters these while zero-shot RL does not. Finally, the paper discusses potential contributing factors such as reward hacking and overfitting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The framing of the paper is clear where the authors talk about a hypothesis (can distillation with limited samples outperform zero-RL) and then plan experiments in the direction to showcase their findings. \n- The authors study the reasons behind it and found linguistic patterns and behaviours to justify the hypothesis. \n- The experiments are controlled and the same model is used to test the hypothesis by training it with distillation vs zero-RL. \n- The paper is readable and the conclusion is clear."}, "weaknesses": {"value": "1. I think the paper central idea is known already to the community and that's why there is nothing new to get from the paper. It is well known that zero-RL is either quite hard to start (faces a cold start problem for small to mid sized models where the right answers is not presented in the rollouts) or quite expensive if started without SFT (Deepseek R1 paper mentions this briefly where they mentions zero-RL works but it would be better to warm it up with some samples, otherwise quite expensive at the start). The paper reaches to the exact same conclusion with Qwen 32B model (lets say a medium sized model) and there’s not a lot of takeaways. \n2. If the authors wanted to go into the proposed hypothesis in detail, the experiments could have been designed better. A lot of different models with varied sizes could have been tested to show the scaling nature of the hypothesis, different sample sizes of distilled samples could have been used (100, 250, 500, …) and not just 920 directly and so on. This seems incomplete with the authors trying to make sense of the experiments by showing some linguistic phenomenon which is also not backed and seems like something would support the hypothesis no matter the final results. \n3. GPT 4o as a teacher did not work while Deepseek R1 led to great performance. This means that the quality of data is very important and not necessarily means distillation is better than zero RL. Also the distillation data sample size per query is 10-16K tokens which involves backtracking, self reflection, improvement and so on. The same phenomenon in RL is more externally and I think unless the distillation is restricted or generated from a non-thinking model, this comparison is not fair and hence the conclusion of distillation > zero-RL is not clear. \n4. Fix some typos like \"consine\", \"scheduler\", \"3s hours\", .."}, "questions": {"value": "1. Can the authors provide a controlled experiment with limited tokens from distillation or samples generated from a non-thinking teacher and show that they reached to the same conclusion of distillation > zero-RL?\n2. Can authors run experiments with different size of the dataset (100, 250, 500,..) and plot the curves to show when distillation gets better than RL? \n3. Can authors run the banning tokens experiments with not just banning the tokens but rather rewriting it with some other LLMs and distilling with it? Banning a token reduces the prob of the entire sentence and hence breaks the flow and this way you can show style vs strategy difference? Are the tokens important or the style of that without those tokens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IAD45Nja6N", "forum": "rjs5K6JMWp", "replyto": "rjs5K6JMWp", "signatures": ["ICLR.cc/2026/Conference/Submission12645/Reviewer_hGih"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12645/Reviewer_hGih"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687051392, "cdate": 1761687051392, "tmdate": 1762923486109, "mdate": 1762923486109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether distillation can outperform zero-RL for enhancing reasoning in smaller LLMs (<32B parameters). The authors fine-tune Qwen2.5-32B on only 920 AIME problems with responses generated by DeepSeek R1, and show this distilled model outperforms three state-of-the-art zero-RL models on mathematical reasoning benchmarks. Through linguistic analysis, they attribute this success to \"flexible reasoning\" characterized by increased use of anthropomorphic tokens (e.g., \"wait,\" \"maybe\") and logical connectors (e.g., \"alternatively,\" \"but\"). They further identify two \"advanced cognitive behaviors\": Multi-Perspective Thinking and Metacognitive Awareness, that appear more frequently in distilled model outputs and correlate with performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Empirical Contribution: The core finding that 920 distilled examples can match or exceed zero-RL models trained on 10-50× more data is practically valuable and challenges current assumptions about the necessity of expensive RL training for smaller models.\n\nTeacher Model Ablation (Table 13): This is a strong experiment showing that distilling from GPT-4o (which lacks flexible reasoning patterns) provides minimal benefit while distilling from QwQ-32B and DeepSeek R1 works well. This supports the claim that the teacher's reasoning style, not just correctness, matters.\n\nComprehensive Evaluation: The paper includes multiple challenging benchmarks (AIME 2024/2025, HMMT, GPQA, MATH500) with careful attention to reproducibility details - fixed temperatures, multiple runs, detailed prompt templates, and unbiased Pass@k estimation.\n\nToken Restriction Experiment: The ablation preventing generation of distinctive tokens (Table 3) is clever and provides evidence that these patterns matter for performance, even if the model attempts workarounds."}, "weaknesses": {"value": "The core comparison is unfair and the paper's framing is a bit misleading. The title and abstract claim distillation \"outperforms\" zero-RL, but: Distillation uses DeepSeek R1, which itself required massive computational resources and RL training to develop. This is equivalent to comparing \"learning from an expert's pre-computed solutions\" versus \"solving problems from scratch\". The paper should compare total computational budgets including teacher training costs, not just student training. The authors acknowledge samples aren't \"directly comparable\" (lines 156-157) but still make superiority claims throughout. What you've actually shown is that transfer learning from an expensive teacher outperforms training from scratch with limited compute.\n\nThe \"advanced cognitive behaviors\" are standard problem-solving strategies people apply in complex problem solving. The contribution is showing distillation transfers advanced cognitive behaviors while zero-RL doesn't, which is descriptive rather than explanatory. The paper doesn't reveal why zero-RL fails to discover these patterns or how to induce them without distillation.\n\nLLM-as-Judge Reliability: The cognitive behavior analysis (Section 4.2, Figure 4) relies entirely on GPT-4o judgments. The author should justify with human validation and robustness checks to show its reliability.\n\nNarrow experimental scope: Single base model (Qwen2.5-32B) and single model family. Training on AIME (1983-2023), testing on AIME (2024-2025) creates distribution matching advantage. No cross-model family validation (Llama, Mistral, etc.)"}, "questions": {"value": "1. Can you provide a complete accounting of computational costs including DeepSeek R1's training? How does \"920 examples of distillation + R1's training cost\" compare to \"zero-RL from scratch\"?\n\n2. Have you tried distillation followed by RL? Does this combination outperform either alone? This would directly test your hypothesis that distillation provides a better starting point for RL.\n\n3. Can you have human annotators validate the GPT-4o cognitive behavior counts on a subset (e.g., 50 problems)? What is the inter-rater agreement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RtUofmRODv", "forum": "rjs5K6JMWp", "replyto": "rjs5K6JMWp", "signatures": ["ICLR.cc/2026/Conference/Submission12645/Reviewer_Hxhi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12645/Reviewer_Hxhi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856113459, "cdate": 1761856113459, "tmdate": 1762923484980, "mdate": 1762923484980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates potential reasons on why distillation on a small amount of examples can outperform zero-RL for improving reasoning abilities of smaller language models (32B). The authors demonstrate that distilling from DeepSeek R1 onto Qwen2.5-32B-base substantially outperforms state-of-the-art zero-RL models across multiple challenging benchmarks. They find that distilled models exhibit more flexible reasoning patterns, characterized by higher frequency of anthropomorphic tokens and logical connectors."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a practically relevant question about distillation versus zero-RL for smaller models, showing that distillation can outperform zero-RL in some scenarios. The findings have direct implications for practitioners.\n- Token frequency analysis quantifies stylistic differences between approaches. The conceptualization of \"Multi-Perspective Thinking\" and \"Metacognitive Awareness\" offers a framework for understanding machine reasoning, and the token-restriction experiment demonstrates causality.\n- The paper provides sufficient detail including training configurations, prompt templates, and appendices for reproducibility."}, "weaknesses": {"value": "- The success of distillation depends on access to a superior teacher model (DeepSeek R1) with existing reasoning capability. Table 13 shows distilling from GPT-4o yields poor results. This prerequisite limits the method's scope to scenarios where such expert teachers exist, yet receives insufficient emphasis in the main text.\n-  Several confounding factors complicate interpretation. The 920 AIME problems represent extremely difficult competition mathematics from a single domain—is the gain from flexible reasoning style, problem difficulty, or both? The distilled model also generates significantly longer responses (Table 1). How much improvement stems from extended thinking (longer CoT) versus the specific cognitive structure? Experiments with simpler problems or controlled response lengths could disentangle these effects."}, "questions": {"value": "- How does performance change as the number of distillation examples varies (e.g., 100, 500, 920)? \n- Figures 1-3: The font size is too small. The authors might want to increase font size for token labels in later versions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5WGtgYn4iv", "forum": "rjs5K6JMWp", "replyto": "rjs5K6JMWp", "signatures": ["ICLR.cc/2026/Conference/Submission12645/Reviewer_5K35"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12645/Reviewer_5K35"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955979074, "cdate": 1761955979074, "tmdate": 1762923484306, "mdate": 1762923484306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}