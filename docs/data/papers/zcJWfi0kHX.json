{"id": "zcJWfi0kHX", "number": 9852, "cdate": 1758143948638, "mdate": 1759897691780, "content": {"title": "Estimation and Clustering in Finite Mixture Models: Bayesian Optimization as an Alternative to EM", "abstract": "We address the problem of maximum likelihood estimation (MLE) for finite mixtures of elliptically distributed components, a setting that extends beyond the classical Gaussian mixture model. Standard approaches such as the Expectation–Maximization (EM) algorithm are widely used in practice but are known to suffer from local optima and typically require strong assumptions (e.g., Gaussianity) to guarantee convergence. In this work, we use the Bayesian Optimization (BO) framework for computing the MLE of general elliptical mixture models. We establish that the estimates obtained via BO converge to the true MLE, providing asymptotic *global* convergence guarantees, in contrast to EM. Furthermore, we show that, when the MLE is consistent, the clustering error rate achieved by BO converges to the optimal misclassification rate. Our results demonstrate that BO offers a practical, flexible, and theoretically sound alternative to EM for likelihood-based inference in mixture models, particularly in complex and/or non-Gaussian elliptical families where EM is difficult to implement and/or analyze. Experiments on synthetic and real data sets confirm the effectiveness and practical applicability of BO as an alternative to EM.", "tldr": "This paper proposes using Bayesian Optimization instead of EM for maximum likelihood estimation in finite mixtures of elliptical distributions.", "keywords": ["Clustering", "Mixture Model", "Bayesian Optimization", "EM"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf031bc6a812a1fa53869e4a6ff1d3b9e1945207.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new approach for parameter estimation in elliptical finite mixture models by replacing the traditional Expectation–Maximization (EM) algorithm with Bayesian Optimization (BO). The idea is to train a Gaussian Process (GP) model that takes as input the mixture model parameters and provides as output the log-likelihood of the model for a given dataset.  At each iteration the trained GP model is used to select new candidate parameters using a GP-UCB acquisition function and the new training point is added to the GP training set, \nthus GP training is repeated with the augmented training set. The authors provide theoretical results such as a proof that the proposed BO framework achieves global convergence to the maximum likelihood estimate (MLE). Experiments are performed on two synthetic datasets involving 2d Student’s t and skewed Student’s t mixtures of two components."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. A new approach is presented for mixture model training based on Bayesian Optimization.\nS2. It is shown that the method converges to the MLE."}, "weaknesses": {"value": "W1. The method is theoretically sound, but difficult to apply even for moderate values of d, since the number of parameters is of d^2 order.\nW2. There are several hyperparameters to be set.\nW3. There are several unclear points regarding method implementation (see questions below)\nW4. Experiments using only two synthetic datasets of very low dimension (2d) and with only two Student-t components are not sufficient."}, "questions": {"value": "Q1. There is no detail on how steps (4) and (8) of Algorithm 1 are implemented and what is their complexity.\nQ2. In line 78 it is mentioned that experiments on real datasets have been conducted, but I cannot find those results in the paper.\nQ3. In addition to the omission of experiments on real data, the reported experimental evaluation is completely insufficient \n(two datasets with two component mixtures on two-dimensional data).\nQ4. What is the execution time of the method compared to SMM? How is the SMM initialized? Is it executed only once or from several initializations keeping the solution with best likelihood?\nQ5. It seems that priors \\pi_i are not included in the parameters.\nQ6. What is the execution time to run the method? \nQ7. It seems expensive to apply the method even for moderate values of input dimension d (e.g d=10).\nQ7. How did you select the number of iterations T and other method hyperparameters?\nQ8. Only the comparison with SMM is meaningful in the experiments.\nQ9. What about clustering performance? NMI values of the obtained solutions with respect to the ground truth solution should be provided.\nQ10. It would add value to the paper if plots of the 2d datasets were provided.   \nQ11. How do you select the initial training points and specify their number?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9FtRHfoI06", "forum": "zcJWfi0kHX", "replyto": "zcJWfi0kHX", "signatures": ["ICLR.cc/2026/Conference/Submission9852/Reviewer_dACm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9852/Reviewer_dACm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649453242, "cdate": 1761649453242, "tmdate": 1762921324519, "mdate": 1762921324519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes using Bayesian Optimization (BO) to learn the parameters of a mixture model. Since BO is a black box optimization method, it comes with a theoretical guarantee that the global optimum will be found with an infinite number of steps. Experiments show that with a finite number of steps, the approach often outperforms EM-GMM (when the data is not generated by a GMM)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Optimizing the log likelihood of a mixture model is a fundamental problem in ML and the approach provided here is original. It is certainly interesting to see how well BO will work in this setting."}, "weaknesses": {"value": "The paper claims to establish global optimality guarantees, but these guarantees only hold when the number of evaluations T goes to infinity. I believe that this weakens the claim that  the paper provides the \" first global convergence\nguarantees for a practically implementable algorithm in this setting\". Allowing an infinite number of evaluations is not practically implementable. \n\nIn practice, we expect the number of evaluations to grow exponentially with the dimensionality of the data, and indeed all the experiments in the paper are with two dimensional data. For such a setting, even simple grid search followed by a few iterations of EM might work. \n\nThe experiments make no mention of run times or the initialization procedure for EM. Presumably by trying more random restarts the EM results will improve and this is also true for the BO approach. So a fair comparison should take into account run times as well.\n\nFinally, the experimental results indeed show consistent failures of EM-GMM but this is not particularly surprising since the data was not generated by a GMM. The EM-SMM results are mostly comparable to those of BO except for a few datapoints in figures 2b,2c."}, "questions": {"value": "What are the runtimes?\n\nHave you tried different initialisation strategies for EM?\n\nHow does BO perform in higher dimensions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9zDKipnv5q", "forum": "zcJWfi0kHX", "replyto": "zcJWfi0kHX", "signatures": ["ICLR.cc/2026/Conference/Submission9852/Reviewer_8NNN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9852/Reviewer_8NNN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804464748, "cdate": 1761804464748, "tmdate": 1762921324185, "mdate": 1762921324185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the use of Bayesian optimization (BO) for computing the maximum likelihood estimate (MLE) of general elliptical mixture models, given that standard alternatives such as EM are known to suffer from local optima. It establishes that the BO estimates converge to the true MLE, which gives asymptotic global convergence guarantees.Furthermore, under certain technical assumptions, the clustering error achieved by BO converges to the optimal misclassification error. An experimental study on synthetic and real datasets confirms the superior performance of BO compared to EM."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Even though the main idea and the corresponding algorithm are quite simple, the authors are able to show global convergence properties, which the main competitors (EM) generally lack for arbitrary elliptical distributions. This could be significant in situations where EM gets stuck in local optima.\n- The paper is well written and the exposition is detailed while the theory seems sound (even though I did not check all proofs in detail).\n- The experimental study shows that BO achieves the most consistent performance across a wide range of parameterizations, unlike EM which can heavily depend on the exact parameters."}, "weaknesses": {"value": "- To the best of my understanding, the paper did not address the question of execution time and/or sample complexity. It is true that EM may get stuck to local minima - but how does BO compare to EM in terms of execution time? Actually, as the experimental figures show, EM can often achieve very good performance and comparable to BO. It is then natural to ask how the two frameworks compare in terms of total execution time. Obviously, if BO has a huge budget, it should be able to perform better than a gradient-based method that can get stuck in local minima. But if the extra time cost is very high, this could be an important consideration.\n- The authors experimented with Student's T mixture model. I was not clear why they did not use more choices. The Gaussian mixture model is presumably less challenging? But what about other models following the elliptical distribution? I feel a more diverse set of mixture models would paint a more accurate picture of the relative performance of the two frameworks. The results on Student's T mixture model are definitely interesting and encouraging, but they could have ben complemented by results from other distributions, too."}, "questions": {"value": "- Can the authors elaborate on the execution time of BO, and how it compares to EM? They already mention various regret bounds in Section 2, but these are mostly asymptotic. What about the total execution time, especially with respect to EM, especially in the challenging settings/parameterizations? \n- Perhaps the authors could also include convergence plots showing how BO reaches the MLE over time?\n- In the cases where EM performed poorly, does EM benefit from multiple runs with different random initialisations? I was wondering whether simple workarounds might help improve EM's performance (irrespective of how BO performs).\n- Why have the authors only experimented with Student's T mixture model? Why not with Gaussian mixture models and/or other elliptical distributions? I feel this would give a more accurate picture of the relative strength of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SXLjPWkXvx", "forum": "zcJWfi0kHX", "replyto": "zcJWfi0kHX", "signatures": ["ICLR.cc/2026/Conference/Submission9852/Reviewer_1HXx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9852/Reviewer_1HXx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216664437, "cdate": 1762216664437, "tmdate": 1762921323831, "mdate": 1762921323831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the problem of finding the parameters of a mixture\nmodel. Contrary to most existing methods, the contribution provides a\nglobal optimum by leveraging the Bayesian optimization framework. An\napplication of this framework to mixture is described along with\ntheoretical guaranties. Toy examples assess the efficiency of the\nproposed method."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Most of methods for the estimation of the parameters of a mixture\nmodel are limited to local maxima and may be trapped into arbitrarly\nbad extrema. It thus of primary importance to study methods able to\nprovide theoretical guaranties on the quality of the output (here a\nglobal maximum). This is even more interesting for mixtures of\ndistributions other than Gaussians which are more difficult to process\nwith the classical Expectation-Maximization algorithm.\n\nThe document is very well written and pedagogical."}, "weaknesses": {"value": "The experiments are rather disappointing. The two synthetic datasets\nare toy examples (only two components mixtures). More over, contrary\nto the affirmation in the introduction, only synthetic experiments are\nshown and no real datasets are presented.\n\nMoreover, this omission is highly problematic: on such an important\nmatter, whether it is a LLM hallucination or an oversight, it shows a\nlack of seriousness and respect for the reader. The authors are visibly\nunable to seriously proofread their article for this kind of mistakes...\n\nMisc remarks:\n- Please increase the size of figures\n- Put Table 1&2 in the body, examples should not be second-class\n  content"}, "questions": {"value": "What is the point of the paragraph \"Expert knowledge\" ? There is not\nmention of it anywhere.\n\nWhere are the real datasets experiments ? You mention it in the\nabstract, in the introduction, and at the beginning of the\nexperimental section but there are no results displayed.\n\nBayesian Optimization is known to be subject to the cold-start\nproblem. Are you affected and how do you deal with this issue ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5F4Nqm7fut", "forum": "zcJWfi0kHX", "replyto": "zcJWfi0kHX", "signatures": ["ICLR.cc/2026/Conference/Submission9852/Reviewer_LKcJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9852/Reviewer_LKcJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762276163246, "cdate": 1762276163246, "tmdate": 1762921323513, "mdate": 1762921323513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}