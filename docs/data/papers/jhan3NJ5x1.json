{"id": "jhan3NJ5x1", "number": 4165, "cdate": 1757617720414, "mdate": 1763725698252, "content": {"title": "Physics-Guided Motion Loss for Video Generation Model", "abstract": "Current video diffusion models generate visually compelling content but often violate \nbasic laws of physics, producing subtle artifacts like rubber-sheet deformations and \ninconsistent object motion. We introduce a frequency-domain physics prior that improves \nmotion plausibility without modifying model architectures. Our method decomposes common \nrigid motions (translation, rotation, scaling) into lightweight spectral losses, \nrequiring only 2.7% of frequency coefficients while preserving 97%+ of spectral energy. \nApplied to Open-Sora, MVDIT, and Hunyuan, our approach improves both motion accuracy and action recognition by ~11\\% on average on OpenVID-1M (relative), while maintaining visual quality. User studies show 74--83% preference for our physics-enhanced videos. It also reduces warping error by 22--37% (depending on the backbone) and improves temporal consistency scores. These results indicate that simple, global spectral cues are an effective drop-in regularizer for physically plausible motion in video diffusion.", "tldr": "", "keywords": ["Video generation", "Diffusion model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a7c6e0e967cec1df42675ec0f9003da99cdfd85.pdf", "supplementary_material": "/attachment/61e5db89c3e0dcf46f52dfd534527c73711f043e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a frequency-domain physics-guided framework to improve the physically plausible motion in video diffusion. This paper formulates rigid motions (translation, rotation, and scaling) within a unified spectral SIM(2) framework, and proposes corresponding differentiable frequency-domain losses. Empirically, the proposed method improves motion accuracy and temporal coherence across multiple backbones (Open-Sora, MVDIT, Hunyuan)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper explores an important problem in video generation.\n- The SIM(2)-based spectral derivation unifies translation, rotation, and scaling within a mathematically sound framework.\n- The loss is architecture-agnostic and can be inserted into any diffusion model without modifying the backbone.\n- Evaluation spans three major video diffusion systems and includes multiple metrics."}, "weaknesses": {"value": "- The innovation lies mainly in unifying them under the SIM(2) formulation.\n- The method only addresses translation, rotation, and scaling, which limits applicability to real-world complex scenes.\n- There are some related physics-constrained video generation works, such as [a], which should also be discussed. Also, except for comparing with the baseline models, it should compare with some related works.\n- [b] is a comprehensive physics generation benchmark designed to evaluate physical commonsense correctness in T2V generation. To validate the effectiveness of the proposed method, it is suggested to apply [b].\n- Since the proposed method is plugged into the baseline models, it should report the generation times.\n- How to set the temperature parameter τ? And how sensitive it is to τ?\n\n\n[a] MOTIONCRAFT: Physics-based Zero-Shot Video Generation\n[b] Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation"}, "questions": {"value": "See the detailed comments in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jwLoWU0o4p", "forum": "jhan3NJ5x1", "replyto": "jhan3NJ5x1", "signatures": ["ICLR.cc/2026/Conference/Submission4165/Reviewer_2TB5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4165/Reviewer_2TB5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761452918948, "cdate": 1761452918948, "tmdate": 1762917208026, "mdate": 1762917208026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a physics guided motion loss for video generation models, designed to enhance motion plausibility and easily integrate with any existing video diffusion models. The loss regularizes fundamental physical motions (rotation, translation, and scaling) in the frequency domain, where these motions exhibit simple and easily detectable patterns. Both qualitative video results and quantitative evaluations demonstrate the effectiveness of the proposed loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It is novel and interesting to regularize basic global physical motions in the frequency domain, a simple yet effective approach that can be easily integrated into any video generation model."}, "weaknesses": {"value": "1. The applicable physics motion patterns are limited to rotation, translation and scaling.\n2. My understanding is that the method is primarily effective for videos containing a single dominant motion and cannot handle scenarios involving multiple objects moving differently or simultaneous camera and object motion."}, "questions": {"value": "1. If a video contains both camera motion and object motion, for example, when the camera is rotating or zooming while an object translates across the scene, can the proposed method still capture both types of motion?\n\n2. I am also curious how the method performs on static scenes with only camera motion compared to models that explicitly use camera motion as control signals."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YUFYJ73nbb", "forum": "jhan3NJ5x1", "replyto": "jhan3NJ5x1", "signatures": ["ICLR.cc/2026/Conference/Submission4165/Reviewer_ffbp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4165/Reviewer_ffbp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881346049, "cdate": 1761881346049, "tmdate": 1762917207763, "mdate": 1762917207763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a frequency-based, physics-informed approach to enhance motion quality and physical plausibility in video diffusion models without degrading visual fidelity or text alignment.\nRather than operating purely in pixel or latent space, the method injects a low-pass frequency-domain constraint that regularizes temporal dynamics and enforces physically consistent motion patterns.\nThe approach is applied as an auxiliary frequency-domain loss during training, compatible with existing architectures such as Open-Sora, MVDIT, and Hunyuan , by training a LoRA. It requires no modification to the diffusion backbone and adds only moderate computational cost.\nMain contributions are:\n-  A physics-guided frequency-domain regularization for video diffusion training that improves temporal motion realism.\n-  Efficient low-pass truncation scheme reducing computational cost. Differentiable frequency-domain least-squares loss integrated seamlessly into standard diffusion training loops.\n-  Extensive empirical validation on multiple video diffusion models."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The paper introduces a novel frequency-domain regularization for video diffusion models that leverages spectral signatures of translation, rotation, and scaling to guide learning without altering model architecture. The strengths are:\n-  The idea of combining classical ideas from Fourier analysis and the SIM(2) motion group with modern video diffusion models, demonstrating a creative synthesis of physics-based priors and deep generative modeling.\n-  The authors provide a thorough derivation connecting basic physical motions (translation, rotation, scaling) to spectral signatures, with attention to windowing, interpolation errors, and numerical stability. The breakdown of translational, rotational, and scaling motion losses, along with adaptive weighting, is logically organized and explained with intuitive interpretations.\n-  Results are reported on multiple video diffusion backbones and evaluated on diverse metrics. The experiments are comprehensive (no LoRA, +LoRA with other losses, +LoRA with proposed loss), and quantitative gains are consistent."}, "weaknesses": {"value": "-  Although the theory is solid, as a paper in the video generation field, its presentation lacks some intuitive visualizations, such as visual demonstrations of spectral changes, and the qualitative evaluation is relatively limited;\n-  In the Abstract, “regularizer” is written as “regular- izer,” which looks like a copy-paste error;\n-  On the first page, in the “four groups” listing, why only (i) is bolded;\n-  As an important demonstration, the supplementary video is of poor asthetic quality and needs improvement.\n\nOverall, the paper has no significant issues in theory or experiments, but there are some minor presentation problems."}, "questions": {"value": "-  Does this method support multiple object motions? (e.g. if a single apple is cut in half and the two halves split away.)\n-  Does this method support the color, illumination, or texture change of an object within a video? If so, how does this loss reduce color flickering like the train in Appendix Fig. 4 ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ega5RtREcI", "forum": "jhan3NJ5x1", "replyto": "jhan3NJ5x1", "signatures": ["ICLR.cc/2026/Conference/Submission4165/Reviewer_1ggH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4165/Reviewer_1ggH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160819234, "cdate": 1762160819234, "tmdate": 1762917207213, "mdate": 1762917207213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}