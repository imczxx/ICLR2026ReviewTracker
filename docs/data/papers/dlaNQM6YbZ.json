{"id": "dlaNQM6YbZ", "number": 14643, "cdate": 1758240770787, "mdate": 1763175212754, "content": {"title": "The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks", "abstract": "Benchmarks shape scientific conclusions about model capabilities and steer model development. This creates a feedback loop: stronger benchmarks drive better models, and better models demand more discriminative benchmarks. Ensuring benchmark reliability is therefore essential for trustworthy evaluation and meaningful progress. In this work, we study benchmark reliability from a \\emph{distributional} perspective and introduce benchmark harmony, which measures \\textit{how uniformly a model's performance is distributed across the subdomains of a benchmark}. We posit that high harmony is a desirable benchmark property, indicating that the aggregate metric reflects uniform competence across subdomains. Across 19 multiple-choice benchmarks and five model families, we map each benchmark onto a mean-variance plane of harmony computed across models, where high mean and low variance signal more reliable evaluation. Our analysis shows that less harmonious benchmarks can give misleading results, since overall accuracy may be disproportionately influenced by specific subdomains. For instance, \\emph{ARC-Easy} is overwhelmed by questions on \\emph{Biological Concepts}, overshadowing other critical subdomains such as Geography, Physics, Chemistry, and Environmental Science. By recommending that harmony should be reported alongside accuracy, we reframe evaluation from \nsimple performance averages to a more robust, distributionally reliable measurement of performance.", "tldr": "", "keywords": ["Benchmark reliability", "meta-evaluation of benchmarks", "evaluation reliability", "diagnostic evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a6ce100f6baf9de52b628ca4b2a418b83caf0c6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper suggest benchmark harmony as a new metric to complement performance metrics like accuracy. A model's harmony score on a benchmark is a measure of variance between the performance on different (semantically meaningful) subsets of the benchmark. For a given benchmark, the authors consider the mean and variance of harmony accross models. They argue that high mean harmony and low harmony variance indicate more reliable evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Considering attributes of benchmarks beyond accuracy seems generally useful.\n- The proposed metric seems reasonably informative and relatively lightweight to compute\n- The writing is clear and easy to follow."}, "weaknesses": {"value": "- Some of the formulas used in the paper have large number of moving parts without strong apparent reason. It is not clear, whether results strongly depend on details in the used parameters, which would enable cherry-picking results. \n    - For example, why not simply define harmony as the (weighted) variance of subset performances? Similarly, what is the intuition behind the formula used for pruning? \n- It seems like harmony scores could become very misleading whenever the clustering picks up on aspects of question difficulty. As an extreme example, if a valid benchmark was partioned into its easier and harder half, harmony would depend on model quality in a U-shape, potentially yielding both low mean harmony and decently high harmony variance. \n- While Figure 3 shows that ground-truth harmony and harmony based on the proposed clustering correlate on two datasets, it is unclear whether this remains true accross benchmarks. In addition, the regression coefficient is decently off from one in one of the two examples, suggesting that harmony measured by the proposed method is not really comparable between benchmarks. \n    - More broadly, the motivation for the specific validation setup (for example, only 4 MMLU subdomains) is a bit unclear. \n- Overall, it remains a bit unclear what reporting harmony offers compared to directly reporting subdomain scores, especially when there are only a few subdomains (as in the examples from Figure 3)"}, "questions": {"value": "- Figure 5: If I understand correctly, you prune low-harmony benchmarks more aggressively. How does the experiment ensure that the larger differences in accuracy for low-harmony benchmarks are not simply explained by pruning more data points?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "khjIT0AQIO", "forum": "dlaNQM6YbZ", "replyto": "dlaNQM6YbZ", "signatures": ["ICLR.cc/2026/Conference/Submission14643/Reviewer_85MX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14643/Reviewer_85MX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761206133644, "cdate": 1761206133644, "tmdate": 1762925017442, "mdate": 1762925017442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study benchmark reliability from a distributional perspective and introduce benchmark HARMONY, which measures how uniformly a model’s performance is distributed across the subdomains of a benchmark. The authors, however, represent HARMONY as essentially a measure of benchmark quality, positing that for a given model set, HARMONY is an independently informative measure, tracking uniform competence across discovered subdomains."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The topic the authors select for study, measures of benchmark reliability, is timely, and more research in this area is definitely needed.\n* The authors have provided a pretty extensive set of experimental results; I particularly appreciate their inclusion of the fully open Olmo2 models and their relatively large benchmark selection.\n* Although their focus in the main paper on MCQA comparisons may be problematic in the specifics, in general, I think that focus improves the rigor of the paper and the trustworthiness of the measure.\n* The description of the measure is quite thorough and detailed, which helps the reader understand the core contributions easily.\n* The controlled pruning experiments demonstrate that low-harmony benchmarks are fragile."}, "weaknesses": {"value": "* The paper's core methodology relies on 'predictive similarity' - clustering questions by the similarity of models' output probability distributions. However, \nin MCQA settings, these distributions are heavily concentrated on answer tokens (A/B/C/D or short answer phrases). It is not clear to me that the spectral clustering approach described by the authors would lead to meaningful or interpretable clusters in this setting. The conditional output probabilities generated by models would be (A) different for every model and (B) heavily concentrated on a small set of tokens.\n* The authors' mean-variance plane interpretation of benchmark reliability does not control for benchmark size; this has the potential to be a significant confound, as larger benchmarks can be expected to exhibit more stable variance characteristics. I am not convinced from the current analysis that HARMONY measure they would be independently informative when comparing benchmarks which are identical in size.\n* I have a doubt about the derivation of the clusters. In Section 2.3, we have -- \"we sweep 2 ≤ k ≤ 20 and select the value maximizing the silhouette score\". This means that k is data-driven and likely varies across benchmarks. Although the Harmony formula includes size weights: wi = |Ai|/|B|, larger benchmarks will tend to yield more stable cluster size estimates and lower sampling variance in wi. I cannot find in the paper a report on how many clusters each benchmark yielded, whether k correlated with benchmark size, whether k correlated with HARMONY.\n* HARMONY, as a measure of a *benchmark's* reliability, is potentially confounded by the fact that benchmark HARMONY is aggregated over a model set. Not all models are intended to be generalists, and not all benchmarks are intended to measure generalist capability. Furthermore, there are practical concerns. The temptation on the part of researchers would be to compare HARMONY across studies, but this will almost never be possible, because new models will come out and the model set and the HARMONY scores will no longer be directly comparable. There is also a risk of researchers cherry picking model sets to make benchmarks appear more or less harmonious, depending on their goals.\n* In Figure 2, the \"Benchmark Size\" field is not defined in the caption and is difficult to interpret from visual inspection, it's hard to figure out which benchmarks correspond to which sizes."}, "questions": {"value": "* In the main plot in Figure 2, what is the strength of correlation between harmony mean and harmony variance? From visual inspection it looks like it would be a pretty strong negative correlation, but I'm curious to know the exact value.\n* Why did the authors not conduct asystematic analysis of cluster characteristics, including an examination of whether cluster count or size distribution correlates with HARMONY?\n* Can the authors show that HARMONY clusters correspond to semantic categories? Specifically: (1) qualitative examples showing cluster coherence, (2) comparison of discovered clusters against ground-truth domain labels where available (beyond the 2 validation cases), and (3) ablation showing that predictive similarity outperforms random clustering of equal size and number?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zTaZQWbWlI", "forum": "dlaNQM6YbZ", "replyto": "dlaNQM6YbZ", "signatures": ["ICLR.cc/2026/Conference/Submission14643/Reviewer_5RAY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14643/Reviewer_5RAY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761229572481, "cdate": 1761229572481, "tmdate": 1762925017122, "mdate": 1762925017122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HARMONY, an entropy-based metric that measures how uniformly a model’s performance is distributed across subdomains of a benchmark. It aims to reveal when aggregate accuracy obscures uneven competency."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important and under-discussed issue in benchmark-based evaluation: the degree to which average scores mask uneven performance across subdomains.\n- The authors present a sufficiently large-scale empirical study across many model families and benchmarks.\n- Overall, the depth of analysis and in particular how confounding factors influence the findings was great. The authors made a significant effort to provide the reader with all necessary information to understand their methodology, along with all necessary information to arrive at genuine and robust interpretations of the findings (e.g., by reporting statistical significance in plots)."}, "weaknesses": {"value": "- The paper does not clearly define what “benchmark reliability” means and uses the term in ways that overlap with concepts such as validity without clarifying the distinction. \n- Relatedly, The paper mentions external valdity in the related work section but validity more broadly is even more relevant in this context in my opinion, see for example [1] and [2] (but also many others, many of Hannah Wallach’s work is relevant here!)\n- Section 2.3 is hard to follow. Given that the partition induction & clustering are key parts of the approach, I would kindly ask the authors to provide some more context (and intuition) why they took the design decisions they took. Some open questions to me are: Why should the similarity be model-aware? Doesn't that mean that HARMONY of a benchmark changes depending which models are used to assess HARMONY? Isn't this easily gameable by developers? Why did the authors think that spectral clustering was preferable over other clustering methods? Why did they set 2<=k<=20 and decided that maximizing the silhouette score was a prudent choice? Adding some explanation what they hope this expresses would help the reader's understanding.\n- Some citations are missing throughout the text, e.g., general statements like the first sentence in line 032 which should be supported by a reference (as an example, a potential citation here could be [3]). Same with the second sentence that follows right after. Overall, there are a couple of instances like this in the paper where I recommend adding more references.\n- I would have liked to see more of a discussion how HARMONY scores should affect benchmark design or what the practical takeaway for benchmark designers should be. I.e., is the authors' standpoint that it's better to not have any general benchmarks that span multiple domains in the first place? I.e., should all benchmarks be specialized? This also seems to cut into a lot of validity discussions, i.e., it's much harder to design a valid benchmark for a broad abstract construct (e.g., \"undergraduate-level knowledge\" like MMLU than for a very specific domain (e.g., fundamentals of astronomy) and I would expect that HARMONY for the former would be much harder to achieve than for the latter.\n\nSmall nits that affected presentation score:\n- The circle sizes in the legend in Figure 2 are visually almost indistinguishable.\n\n\n[1] https://arxiv.org/abs/2505.10573\n[2] https://arxiv.org/pdf/2412.01934\n[3] https://dl.acm.org/doi/pdf/10.1145/3708359.3712152?casa_token=dsjgmGDH8AUAAAAA:CvbRJT-NGxwBc9vr3RegWENQzOPbfHXz78JCBN1UlsuiPIPTenGWWcYB2XS0FRVEFjpFk24ZpHDg"}, "questions": {"value": "See weaknesses.\n\nI'm generally willing to updating my score if the points in the weaknesses section (in particular on reliability definition and the difference to validity and Section 2.3 questions) are clarified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VHMYXTWOeR", "forum": "dlaNQM6YbZ", "replyto": "dlaNQM6YbZ", "signatures": ["ICLR.cc/2026/Conference/Submission14643/Reviewer_EZ1V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14643/Reviewer_EZ1V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055539348, "cdate": 1762055539348, "tmdate": 1762925016712, "mdate": 1762925016712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Benchmarks critically shape model development, but their reliability is poorly understood. To this end, the authors introduced HARMONY, a metric quantifying how uniformly a model's performance is distributed across semantic subdomains of a benchmark. They partitioned benchmarks using \"predictive similarity\", a model-aware clustering based on KL divergence of model probability distributions. They then computed HARMONY (normalized Shannon entropy) for 36 models across 5 families on 19 MCQA benchmarks, positioning each benchmark in a mean-variance plane.\n\nThe paper's key contributions include the HARMONY metric, which measures performance uniformity across subdomains where higher values indicate more reliable benchmarks, and intorducing predictive similarity, a novel model-aware partitioning approach based on divergence of model logits. Testing 19 benchmarks, fragility evidence shows that low-HARMONY benchmarks have unstable aggregate scores under pruning while high-HARMONY benchmarks remain stable. They also test for model family specific patterns: Qwen and Llama show negative correlation between model size and HARMONY, while Gemma and OLMo show positive correlation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The problem motivation is timely and important: benchmarks critically shape model development, are needed for governance mechanisms, and audit methods are genuinely needed. \n\n* The scale and comprehensiveness of the empirical study is thorough, evaluating 36 models across 5 families on 19 benchmarks with detailed ablations and cross-model analysis in the appendices. \n\n* The authors design a synthetic benchmark (RedundantQA) with clean ground-truth partitions (paraphrases vs. distractors), enabling controlled validation of the similarity metric without confounds. \n\n* The pruning experiments provide concrete evidence that low-HARMONY benchmarks are genuinely fragile and that their aggregate scores shift significantly when rebalancing, while high-HARMONY benchmarks remain stable. This is a great finding!\n\nI like the paper,  but I have a few uncertainties that I would need to have sufficiently resolved before increasing my scores."}, "weaknesses": {"value": "## Weakness 1\n\nThe paper defines HARMONY using normalized Shannon entropy but never justifies this choice over alternative uniformity measures (e.g., Gini coefficient, Rényi entropy, coefficient of variation). Appendix B compares variants of KL divergence, but not fundamentally different approaches to measuring performance uniformity. Can you please comment on why Shannon entropy is specifically the right measure for \"uniformity of competence\"? Is HARMONY more robust than other measures to gaming by benchmark designers?\n\n## Wekaness 2\n\nWhile theoretically grounded (app B), it's unclear whether the symmetrized KL divergence approach to model-aware similarity is genuinely novel or how it compares to existing model-based similarity metrics in the literature. The paper could be strengthend by making the novelty claim more explicitly justified.\n\n## Weakness 3\n\nThe main text does not clearly specify how model-specific partitions are unified across 36 models to produce the mean-variance plane in Figure 2. It should explicitly state whether HARMONY is computed per-model then aggregated, or if a reference partition is used.\nCan you comment on whether you compute 36 separate partitions (one per model) and then compute HARMONY for each or is a single reference partition used across all models?\n\n## Weakness 4\n\nThe paper does not address whether the clustering procedure generalizes to benchmarks with overlapping/ambiguous semantic categories beyond the clean cases tested (RedundantQA, MMLU) with clearly separable domains. How does your method perform on benchmarks where categories have significant semantic overlap or ambiguous boundaries? \n\n## Weakness 5\n\nThe paper asserts uniform performance indicates \"broad competence\" (2.2) but provides no rigorous justification for why this is desirable beyond empirical stability in Figure 5. Some legitimate task domains may have inherently heterogeneous difficulty. Also, the paper does not seem to distinguish between low HARMONY caused by poor benchmark design and low HARMONY reflecting genuine domain heterogeneity (e.g., some subdomains are objectively harder). \n\nTo give a concrete example: This MCQA dataset https://arxiv.org/abs/2502.16051 can be used to evaluate LM decision-making in clinical mental healthcare settings. The dataset is split across task categories (due to day-to-day clinical decision-making composing different tasks) that vary significantly in complexity and nature. Some categories even have no true answer (due to the inherent ambiguities even to human domain experts). Thus, model performances vary drastically across categories due to different task natures and conflicts with moral ambiguity/model alignment methods. This dataset (if used as a benchmark) would probabyl get a low Harmony score and I am not sure if that is a good thing and whether this problem will increase with increasingly more nuanced and domain-specific benchmarks. (I'm also happy to hear arguments how such benchmarks would be bad to strengthen the Harmony justificaiton)\n\nEssentially, I think Harmony measures away to link accuracy/performance increase to a somewhat linear increase in performance. Why should we expect uniform performance across subdomains to be desirable? Aren't some domains legitimately harder than others and wouldn't enforcing harmony erase meaningful task heterogeneity? How do you distinguish between low HARMONY that indicates a design flaw versus low HARMONY that reflects legitimate variation in task difficulty across subdomains? \n\n## Weakness 6\n\nBy computing model-specific partitions, HARMONY seems to measure whether a particular model has learned semantic structure, not whether the benchmark itself is well-designed. This could conflate model alignment with benchmark audit, potentially undermining the claim to benchmark reliability assessment. If HARMONY is model-specific, how is this a benchmark audit rather than a model audit? Couldn't a randomly initialized model coincidentally achieve high HARMONY on arbitrary clusters, implying the benchmark is \"reliable\"? Also, to what extent can a model's predictive similarity be manipulated to inflate harmony?\n\n## Weakness 7 \n\nThe paper recommends \"report HARMONY alongside accuracy\" but provides no concrete thresholds, decision rules, or guidance for how practitioners should act on this information (e.g., when to disqualify benchmarks, how to weight accuracy vs. harmony tradeoffs). What are the practical decision rules? Should benchmarks with HARMONY < 0.5 be excluded? How should practitioners trade off accuracy improvements against harmony decreases (Figure 5)?\n\n## Weakness 8 \n\nAppendix H demonstrates hierarchical multi-dimensional evaluation but provides no principled criterion for when to stop recursively partitioning (e.g., you could partition MMLU Biology down to single-question granularity). This could undermine the generality of the framework. At what point will using HARMONY lead to evaluate benchmarks just to calculating HARMONY of increasingly narrow subcategories and thus loosing its value to make a statement about an entire benchmark?"}, "questions": {"value": "I've embedded my questions into the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9tq7VP8KiW", "forum": "dlaNQM6YbZ", "replyto": "dlaNQM6YbZ", "signatures": ["ICLR.cc/2026/Conference/Submission14643/Reviewer_F1zp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14643/Reviewer_F1zp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142893297, "cdate": 1762142893297, "tmdate": 1762925016382, "mdate": 1762925016382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}