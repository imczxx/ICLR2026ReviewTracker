{"id": "hQ4Ph3TNYK", "number": 15568, "cdate": 1758252735534, "mdate": 1759897298595, "content": {"title": "SG-Gaze: Structurally and Geometrically Consistent Representation Learning for Generalizable 3D Gaze Estimation", "abstract": "Learning accurate and generalizable 3D gaze representations remains challenging due to the lack of a unified and physically meaningful representation. Existing methods rely on either appearance features or simplified geometric modeling, but fail to jointly capture geometric and structural consistency. \nThey exhibit poor cross-domain generalization and typically require large-scale multiview datasets to mitigate viewpoint variation, yet still struggle with domain shifts between controlled and in-the-wild settings.   \nTo address these issues, we propose $\\textbf{SG-Gaze}$, a dual-branch framework that learns a $\\textbf{S}$tructurally and $\\textbf{G}$eometrically Consistent $\\textbf{R}$epresentation $\\textbf{(SGR)}$ for gaze estimation. \nThe analytical branch embeds features into a geodesically aligned spherical manifold for interpretable regression, while the model-guided branch reconstructs 3D eyeball structure under weak 2D edge supervision. Through adversarial training, the resulting SGR is simultaneously appearance discriminative, structurally faithful, and geometrically consistent.\nTo further improve robustness, we introduce View-Consistent Regularization, which augments training SGR with synthetic view perturbations and enforces rotation-equivariant consistency across gaze vectors and structural projections. This reduces reliance on costly multiview data and narrows cross-domain gaps.  \nExtensive experiments on synthetic and real-world datasets show that SG-Gaze achieves state-of-the-art accuracy and strong cross-domain generalization in 12 challenging transfer scenarios. Our work demonstrates the importance of unifying structurally and geometrically consistent representation with equivariant regularization, providing broader insight into building more interpretable and generalizable models.", "tldr": "A dual-branch framework (SG-Gaze) learns structurally and geometrically consistent gaze representations under view transformations through adversarial alignment, achieving state-of-the-art accuracy and strong cross-domain generalization.", "keywords": ["3D Gaze Estimation", "Gaze Representation Learning", "Eye Model Reconstruction", "Geometric and Structural Consistency"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6cfd8d8d1814bf62f915e457e0f0e15894e8ad66.pdf", "supplementary_material": "/attachment/7bf1946893190ce535ac423a0df36fe73b06dc69.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SG-Gaze, a dual-branch framework that learns a Structurally and Geometrically Consistent Representation (SGR) for 3D gaze estimation. By jointly parsing structural cues and reconstructing the 3D eyeball, and by incorporating several training and regularization techniques, the method aims to achieve strong structural fidelity, geometric consistency, and cross-domain robustness. Experiments demonstrate competitive or state-of-the-art performance, especially under cross-domain evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of jointly learning structural and geometric representations for generalizable 3D gaze estimation is conceptually sound and aligns with the field’s push toward physically grounded representations.\n\n2. The paper is well organized, and the method design is clearly presented with consistent motivation and logical flow.\n\n3. The visualizations and figure design are of high quality, helping convey both the architecture and qualitative outcomes effectively.\n\n4. The experimental section is rich and comprehensive, including detailed visual analyses and clear improvements on cross-domain benchmarks."}, "weaknesses": {"value": "1. **Over-Complex Architecture with Limited Theoretical Grounding.**\nThe proposed dual-branch design (AGE + MGR + adversarial alignment + VCR) appears over-engineered relative to the observed gains. While each component is individually reasonable, their combination lacks a clear theoretical justification or ablation-driven insight into necessity and interaction.\n\n2. **Limited Scalability and Real-World Applicability.**\nThe framework relies on pseudo-labeled 2D edge supervision and synthetic viewpoint perturbations, which may not generalize well to real-world, low-quality, or dynamic eye-tracking data. The reported gains could stem from dataset-specific biases rather than genuine cross-domain robustness.\n\n3. **Ambiguous Physical Interpretability Claims.**\nAlthough the paper emphasizes “physically meaningful” representations, these claims are not empirically or anatomically validated. The spherical manifold embedding and eyeball reconstruction components are not directly linked to measurable physiological parameters, weakening the interpretability argument."}, "questions": {"value": "1. Could the authors clarify how each component (AGE, MGR, adversarial alignment, and VCR) contributes individually to performance? Specifically, which part is most critical for achieving cross-domain robustness, and have the authors explored whether a simpler variant  yields comparable results?\n\n2. The paper claims that the proposed representation is “physically meaningful.” How is this physical interpretability quantitatively validated? For instance, do the reconstructed eyeball geometries or spherical manifold embeddings correlate with measurable anatomical parameters (e.g., corneal radius or eyeball center offset)? If not, could the authors elaborate on what exactly is meant by “physically interpretable” in this context?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Wpv2LM78Tq", "forum": "hQ4Ph3TNYK", "replyto": "hQ4Ph3TNYK", "signatures": ["ICLR.cc/2026/Conference/Submission15568/Reviewer_X9m8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15568/Reviewer_X9m8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575373782, "cdate": 1761575373782, "tmdate": 1762925842738, "mdate": 1762925842738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a dual branch framework for generalizable 3D gaze estimation that learns a \"structurally and geometrically consistent \nrepresentation (SGR). In Analytical Gaze Estimation (AGE) branch, it projects features onto a spherical manifold for geometrically interpretable regression, and a Model-Guided Reconstruction branch recovers eyeball structure under weak 2d edge supervision.\nA VCR module enforces rotation-equivariant consistency between feature-space and physical space transformations.\nThrough this design, the propsed framework SG-Gaze achieves interpretable, physically grounded and domain robust gaze prediction."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The idea of enforcing structural and geometric consistency in gaze representation is reasonable. \nI find it interesting how the paper grounds its formulation in physical intuition (e.g., spherical topology, rotation equivariance) rather than just black-box regression; it makes the whole approach feel interpretable and principled.\n\nThe dual-branch design (AGE + MGR) has its own well-explained benefits, analytical fitting, and weakly supervised reconstruction, which complement each other nicely, and the adversarial alignment between them seems sound and reasonable.\n\nOverall, the experiments are convincing: consistent SOTA gains across 12 transfer setups, good qualitative results, and lightweight architecture — the method feels both scientifically sound and practically useful."}, "weaknesses": {"value": "- While the paper emphasizes physical interpretability, some modules (especially the feature-space rotation) remain a bit abstract and vague.\nIt is unclear how well this learned mapping truly corresponds to real 3D rotations in practice.\n\n- The weak 2D edge supervision in MGR is interesting but might limit applicability in real-world scenarios where reliable iris/pupil segmentation is not available or noisy."}, "questions": {"value": "How is the Isomap projection handled during training and inference. Is it precomputed, differentiable, or updated online? It’s not clear how scalable or stable this step is when features shift during training.\n\nThe feature-space rotation is an indeed interesting idea, but how strongly does it correlate with actual 3D rotations in physical space? Some qualitative evidence (e.g., rotating SGR and visualizing gaze shift) would really strengthen the claim.\n\nSince the MGR branch assumes a spherical eyeball and a simple pinhole camera, how robust is SG-Gaze under real-world optics like corneal refraction or off-axis cameras in AR/VR devices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4PgUpArVBN", "forum": "hQ4Ph3TNYK", "replyto": "hQ4Ph3TNYK", "signatures": ["ICLR.cc/2026/Conference/Submission15568/Reviewer_z7g4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15568/Reviewer_z7g4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837671820, "cdate": 1761837671820, "tmdate": 1762925842186, "mdate": 1762925842186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SG-Gaze, a dual-branch framework combining Analytical Gaze Estimation and Model-Guided Reconstruction to learn structurally and geometrically consistent representations for 3D gaze estimation. The method includes View-Consistent Regularisation (VCR) to improve cross-domain generalisation. The authors report improvements of up to 38.61% on cross-dataset transfer tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly articulates the limitations of existing appearance-based and model-based methods and proposes a unified framework to address both.\n2. The paper includes extensive experiments across multiple datasets (UnityEyes, TEyeD, LPW) with 12 cross-domain transfer scenarios and thorough ablation studies.\n3. The method achieves competitive or state-of-the-art performance on most cross-domain transfer tasks, demonstrating improved generalisation.\n4. Table 4 and Figure 5 provide good insights into the contribution of each component (AGE, MGR, VCR).\n5. The paper addresses few-shot learning scenarios and analyses sensitivity to edge point sparsity, which are important for real-world deployment."}, "weaknesses": {"value": "The main components are largely borrowed from recent prior work. The AGE branch uses Isomap projection and spherical fitting directly from AGG (Bao & Lu, 2024). The MGR branch uses a 3D eyeball reconstruction similar to De2Gaze (Xiao et al., 2025) and other model-based methods. The VCR component is inspired by 3DGazeNet (Ververas et al., 2024), essentially applying synthetic rotations as data augmentation. The primary contribution appears to be combining these existing techniques rather than introducing fundamentally new methods. The \"structurally and geometrically consistent representation\" is more of a conceptual framing than a concrete technical innovation.\n\nThere are several technical aspects that I could not fully follow:\n- The abstract and introduction prominently mention \"adversarial training\" and an \"adversarial alignment module\" (Fig 1d, Fig 3d). However, Section 3.5 (Loss Functions) contains no adversarial loss term. The discriminator architecture and training procedure are never described.\n- Figure 1 mentions \"Max(F∩G)\" and \"Max(F∩S)\" but this notation is never explained.\n- Section 3 states SGR must satisfy three constraints, but these are informally described. No clear mathematical formulation of what SGR actually is as a representation. The two branches seem to produce separate outputs rather than a unified representation.\n- Using an MLP to approximate Isomap (Appendix B.2) seems like a hack to avoid computational cost. This introduces approximation error that is never quantified. Why not use other dimensionality reduction methods that are differentiable by design?\n\nThere is some within-domain performance degradation. Table 1 shows VGG-16+SG-Gaze performs significantly worse within-domain. Table 3 shows adding semantic supervision increases 2D gaze error. The paper acknowledges SG-Gaze introduces \"stronger inductive bias\" that may \"trade off fine-grained fitting,\" but this is a significant limitation not adequately addressed.\n\nThere is a large gap with De2Gaze. In Table 3 SG-Gaze achieves 0.94° vs De2Gaze's 0.54° on within-domain evaluation. This gap suggests the method may be sacrificing accuracy for generalisation. Given that De2Gaze also uses eyeball reconstruction, why is there such a large difference?\n\nTable 2 compares against methods from 2017-2024, but within-domain comparison (Table 3) uses older baselines. There is no comparison with more recent transformer-based approaches in cross-domain setting. What precisely is the \"Transformer-based\" method in Table 3?\n\nFact 2 is imprecise. \"The eyeball is a physical sphere\" - the eyeball is approximately spherical but has significant deviations (corneal bulge, non-rigid deformation).\n\nThe paper acknowledges (Appendix C.2) that ignoring the kappa angle between optical and visual axes affects accuracy. This is a fundamental limitation that undermines claims about physical faithfulness."}, "questions": {"value": "Besides responding generally to the weaknesses listed above, some specific questions are:\n\n1. Where is the adversarial loss? How does the adversarial discriminator work?\n2. Why does semantic supervision hurt 2D gaze accuracy within-domain?\n3. Why is there a 74% accuracy gap with De2Gaze within-domain (Table 3)?\n4. What exactly is SGR as a representation? Is it the concatenation of features from both branches?\n5. How sensitive are results to Isomap approximation error from the IP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zcCSSA6NHc", "forum": "hQ4Ph3TNYK", "replyto": "hQ4Ph3TNYK", "signatures": ["ICLR.cc/2026/Conference/Submission15568/Reviewer_Nzjy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15568/Reviewer_Nzjy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919859309, "cdate": 1761919859309, "tmdate": 1762925841736, "mdate": 1762925841736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SG-Gaze, a dual-branch framework for 3D gaze estimation that combines appearance-based features with geometric and structural consistency. It introduces a Structurally and Geometrically Consistent Representation (SGR), learned through two branches: Analytical Gaze Estimation (AGE), which projects features onto a spherical manifold, and Model-Guided Reconstruction (MGR), which reconstructs 3D eyeball structures with weak 2D edge supervision. Additionally, View-Consistent Regularization (VCR) enhances generalization across different viewpoints. Experiments show that SG-Gaze outperforms state-of-the-art methods in accuracy and cross-domain generalization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces a novel framework that effectively learns structurally and geometrically consistent representations. The integration of both appearance features and geometric modeling is a valuable and innovative approach for gaze estimation.\n    \n2. The experimental analysis is thorough and well-conducted. The proposed framework consistently outperforms existing state-of-the-art methods, demonstrating its superiority in within- and cross-  datasets."}, "weaknesses": {"value": "1. The paper lacks clarity and is difficult to follow. Significant improvements in the overall presentation and readability are recommended.\n    \n    - The notation is not well-explained throughout the paper. For instance, the symbol \"F\" in Line 150 is not defined.\n        \n    - Fig. 3 is challenging to understand based on the description in the main text, as the notation and naming conventions are inconsistent between the figure and the text, making it hard to interpret the visual content.\n        \n    - The concept of the structurally and geometrically consistent representation is not sufficiently explained. The mechanism by which the structure and geometry are made consistent remains unclear.\n        \n    - Sections 3.2 and 3.3 lack sufficient detail. The notations used are not fully explained, and the rationale and theoretical background behind the operations in these sections are not provided, making it hard to grasp the approach.\n        \n    - The formation and construction of the structural and geometrical features are not clearly defined, leaving questions about how these features are derived and integrated.\n        \n2. The applicability of the proposed method needs further clarification. The experiments are based on eye images captured by head-mounted devices, which provide high-resolution images. However, the method's effectiveness on images captured by non-head-mounted devices, which typically have much lower resolution, is unclear. Can this method be applied to such lower-resolution images?"}, "questions": {"value": "The readability of the paper could be significantly improved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CwGyIuT8Ld", "forum": "hQ4Ph3TNYK", "replyto": "hQ4Ph3TNYK", "signatures": ["ICLR.cc/2026/Conference/Submission15568/Reviewer_ZVUT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15568/Reviewer_ZVUT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071489120, "cdate": 1762071489120, "tmdate": 1762925841244, "mdate": 1762925841244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}