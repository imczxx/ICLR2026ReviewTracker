{"id": "XYmLpiIgZa", "number": 22683, "cdate": 1758334429115, "mdate": 1763668196079, "content": {"title": "Explicit and Effectively Symmetric Schemes for Neural SDEs", "abstract": "Backpropagation through (neural) SDE solvers is traditionally approached in two ways: discretise-then-optimise, which offers accurate gradients but incurs prohibitive memory costs due to storing the full computational graph (even when mitigated by checkpointing); and optimise-then-discretise, which achieves constant memory cost by solving an auxiliary backward SDE, but suffers from slower evaluation and gradient approximation errors. Algebraically reversible solvers promise both memory efficiency and gradient accuracy, yet existing methods such as the Reversible Heun scheme are often unstable under complex models and large step sizes. We address these limitations by introducing a novel class of stable, near-reversible Runge–Kutta schemes for neural SDEs. These Explicit and Effectively Symmetric (EES) schemes retain the benefits of reversible solvers while overcoming their instability, enabling memory-efficient training without severe restrictions on step size or model complexity. Through numerical experiments, we demonstrate the superior stability and reliability of our schemes, establishing them as a practical foundation for scalable and accurate training of neural SDEs.", "tldr": "We introduce a novel class of stable, near-reversible Runge–Kutta schemes for neural SDEs", "keywords": ["Neural SDEs", "GenAI", "numerical solvers"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9cf8e58ed153ef6578e53bd955580ff1f050acfa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Constructing reversible solvers for SDEs is an important problem in Neural SDEs due to computational problems with both optimize-then-discretize and discretize-then-optimize approaches. Existing reversible solvers such as ALF and Reversible Heun have a small stability region, making them unsuitable for many applications. The author construct an RDE version of the EES scheme, a type of RK scheme, and show that it has a comparable stability region as well-established RK schemes. Numerical experiments are provided where EES_R outperforms Reversible Heun."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Good writing with a good level of rigor. The introduction of the related works makes the method a very natural continuation.\n- EES_R seems to inherit all the good properties of EES.\n- EES_R has good performance on two test equations."}, "weaknesses": {"value": "- Lack of novel contribution: I feel that EES_R is an application of the result from Redmann & Riedel to EES.\n- The experiment with SDE-GAN from [1] is missing, which makes it a bit hard to fully appreciate that EES_R is a strict improvement over Rev Heun.\n[1] Kidger, Patrick, et al. \"Efficient and accurate gradients for neural sdes.\" Advances in Neural Information Processing Systems 34 (2021): 18747-18761."}, "questions": {"value": "- In Figure 1, is it possible to plot the stability region of Reversible Heun? It is unclear to me what the metric to compare the stability region of Rev Heun vs EES_R.\n- I'm guessing that one (albeit very small) drawback of EES_R vs Reversible Heun is simply the extra computation/memory to evaluate the drift and diffusion term. Maybe the difference is only a constant depending on the order of EES. Is that true?\n- It is not clear to me what the domains in the two experiments are and how much they overlap with the stability region of EES and Reversible Heun.\n- I would like to understand more about the near-symmetry property. It is fine if near-symmetry is enough for practical problems, but is it possible to construct pathological test equations such that near-symmetry is not enough to guarantee the accuracy of the inverse solution is bounded? What are the properties of these pathological cases such that they differ from the typical average-case practical problems?\n- Can you be more precise about the \"high-volatility dynamics\" mentioned on line 392?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IlOobkXuBx", "forum": "XYmLpiIgZa", "replyto": "XYmLpiIgZa", "signatures": ["ICLR.cc/2026/Conference/Submission22683/Reviewer_VyCj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22683/Reviewer_VyCj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761617408289, "cdate": 1761617408289, "tmdate": 1762942335192, "mdate": 1762942335192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “Explicit and Effectively Symmetric Schemes for Neural SDEs” introduces a new family of Explicit and Effectively Symmetric (EES) Runge–Kutta solvers for Neural Stochastic Differential Equations (Neural SDEs). These methods aim to combine the stability and memory efficiency of reversible solvers with the simplicity and computational efficiency of explicit schemes.\n\nBuilding upon recent advances in rough path theory, the authors generalize EES methods (originally designed for ODEs) to rough differential equations (RDEs). They derive theoretical results showing that EES schemes maintain high stability comparable to classical RK3/RK4 solvers and demonstrate mean-square stability for stochastic systems. Two experiments (a high-volatility Ornstein–Uhlenbeck process and a geometric Brownian motion calibration) show that the proposed EESR(2,5) method is significantly more stable and achieves faster, more reliable training than the commonly used Reversible Heun scheme."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novelty & Contribution – Extends the concept of near-reversible explicit solvers (EES) to Neural SDEs/RDEs, bridging rough path theory and practical neural training.\n\nTheoretical Rigor – Provides clear derivations of convergence and stability properties, including mean-square stability analysis.\n\nPractical Relevance – Demonstrates tangible improvements in training stability on challenging stochastic models where existing reversible methods fail.\n\nClarity of Design – The algorithmic formulation for backpropagation (Algorithm 1) is well structured and easily integrable into autodiff frameworks.\n\nBalanced Discussion – Acknowledges trade-offs (extra evaluations per step vs. stability) and outlines realistic future extensions."}, "weaknesses": {"value": "Stability under Non-smooth Neural Coefficients:\nThe stability analysis assumes the drift and diffusion coefficients f,g are sufficiently smooth (bounded derivatives, Lipschitz).\n→ How would the EESR scheme behave if the neural network coefficients are non-smooth or poorly regularized, as often happens in real-world Neural SDE training?\nWould the mean-square stability guarantee still approximately hold, or could the method exhibit spurious oscillations?\n\nAdaptive Step Size:\nCould an adaptive step-size EESR be designed to dynamically maintain stability without sacrificing explicitness?\n\nComputational Cost:\nGiven that EESR(2,5) requires three function evaluations per step (vs. one in Reversible Heun), how does this scale in large neural SDEs or diffusion models?\n\nStochastic Gradient Interaction:\nWhen EESR is used inside a stochastic training loop (mini-batch + noisy gradients), is its near-reversibility still advantageous, or do stochastic gradient perturbations break that property?\n\nEmpirical Testing Scope:\nThe benchmarks focus on low-dimensional dynamics. How would EESR perform on high-dimensional latent Neural SDEs or in score-based diffusion models with neural drift and diffusion?"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7kPkxsW2Jf", "forum": "XYmLpiIgZa", "replyto": "XYmLpiIgZa", "signatures": ["ICLR.cc/2026/Conference/Submission22683/Reviewer_tYYW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22683/Reviewer_tYYW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637093121, "cdate": 1761637093121, "tmdate": 1762942334911, "mdate": 1762942334911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce explicit and effectively symmetric Runge-Kutta schemes to overcome the numerical stability issues of Reversible Heun methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is very well written and provides a great introduction and background to the problem. Tackling stability issues in algebraically reversible solvers to overcome limitations in the other two alternative methods is is timely, and the construction is plausible and well-motivated."}, "weaknesses": {"value": "Analysis is solely focused on comparisons between reversible Heun and EES(2,5). The scope of the theoretical contributions are limited to stability analysis of EES for stochastic SDE where analysis of ODEs has been shown in prior work. Without broader baselines or diffusion/practical Neural-SDE tasks, it’s hard to gauge practical impact."}, "questions": {"value": "**Q1. Practical evidence of overcoming Reversible Heun failure modes.** The authors cite [1] in the context of the limitations of Reversible Heun, but do not provide empirical evidence that their EES(2,5) techniques have overcome these limitations. The work would greatly benefit from including a comparison on the applications in [1] and a comparison to the Euler integration used in [1].\n\n[1] Qinsheng Zhang and Yongxin Chen. Path integral sampler: a stochastic control approach for sampling. arXiv preprint arXiv:2111.15141, 2021.\n\n**Q2. Missing reversible/non-reversible baselines.** The work would greatly improve with comparisons to the ALF and the McCallum & Foster method.\n\n**Q3. Relation to recent algebraically reversible RK solvers.** The authors should include a discussion of highly related recent work [2] specifically developing algebraically reversible explicit RK schemes, and should comment on the differences of their work.\n\n[2] Zander W. Blasingame and Chen Liu, REX: Reversible Solvers for Diffusion Models. arXiv preprint arXiv:2502.08834v1, 2025\n\n**Q4. Practical applications.** The work would greatly improve with numerical results comparing their technique to others on problems of high relevance to ICLR including diffusion/application based Neural-SDEs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h1xiQEMkBY", "forum": "XYmLpiIgZa", "replyto": "XYmLpiIgZa", "signatures": ["ICLR.cc/2026/Conference/Submission22683/Reviewer_G7my"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22683/Reviewer_G7my"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957859085, "cdate": 1761957859085, "tmdate": 1762942334667, "mdate": 1762942334667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the Explicit and Effectively Symmetric (EES) schemes proposed by [Shmelev05], which is a Runge-Kutta scheme for ODEs, to neural SDEs and RDEs. The method is motivated by the problem of backpropagation through SDE solvers. There are 3 approaches: (1) stores the full forward graph (\"discretise–then–optimise\"), which is accurate by expensive; (2) integrates an adjoint/backward SDE (\"optimise–then–discretise\"), which is cheap by inaccurate; (3) algebraically reversible solvers, which promise both accuracy and cheap compute, but are often unstable (e.g., Reversible Heun). The paper provides a memory‑efficient backprop algorithm through explicit RK solvers using a reverse step and in‑place stage‑wise autodiff (Algorithm 1), and provides stability analysis which shows that the method has similar stability to the classical RK3/RK4 methods. The paper also conducts experiments that compare the proposed EESR(2,5) scheme against Reversible Heun on two neural SDE tasks: (1) a high‑volatility 2D OU process and (2) a GBM calibrated to option prices—reporting. For both tasks, EESR reports lower losses and fewer training epochs (Figs. 3–4)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is mathematically rigorous and novel. Extending EES schemes from ODEs to the RDE/SDE is natural but non‑trivial, and the high stability is demonstrated both theoretically and experimentally, which solves the posed question of algebraically reversible SDE solvers being unstable. The two training case studies are well-chosen."}, "weaknesses": {"value": "My main concern is that the baseline is narrow. The comparisons focus almost exclusively on Reversible Heun. The comparison with other common baselines would help. Also, including performance measures like wall‑clock time, function‑evaluation counts, and memory profiles, would substantiate the claimed practical advantages. Moreover, both tasks (2D OU, 1D GBM) are low-dimensional and fairly structured; results on higher‑dimensional NSDEs, stiff drifts, or non‑Gaussian rough drivers would better test generality."}, "questions": {"value": "- Can you quantify gradient error by comparing EESR‑based gradients to a “reference” gradient obtained by storing the forward graph at a much smaller step size?\n- Section 5 notes that when stability is not an issue, Reversible Heun uses fewer evaluations per step. Can you characterise regimes where Heun is preferable, and perhaps propose a hybrid scheme that switches to EESR only when instability is detected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YcVsQs9XgX", "forum": "XYmLpiIgZa", "replyto": "XYmLpiIgZa", "signatures": ["ICLR.cc/2026/Conference/Submission22683/Reviewer_F8uU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22683/Reviewer_F8uU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22683/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991895094, "cdate": 1761991895094, "tmdate": 1762942334456, "mdate": 1762942334456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}