{"id": "argTvCxHf9", "number": 11096, "cdate": 1758189127554, "mdate": 1759897609055, "content": {"title": "TRIDENT: Three-Dimensional Data Copyright Infringement Detection in LLMs", "abstract": "Large Language Models (LLMs) are trained on large datasets that may contain copyrighted material, leading to risks of data infringement. Existing detection methods usually work at the word level or rely on a single feature, such as lexical similarity or surface overlap. However, LLMs can reproduce copyrighted content through semantic rewriting or logical transfer, which makes these methods less effective. Therefore, we propose TRIDENT, a \\textbf{T}h\\textbf{R}ee-D\\textbf{I}mensional Method for \\textbf{D}ata Copyright Infringem\\textbf{ENT} Detection in LLMs. TRIDENT combines three dimensional features: surface features, semantic relevance, and quality assessment, tackling both explicit replication and implicit infringement situations. Specifically, it utilizes a statistics-based method to provide interpretable significance verification, and a learning-based method to enable efficient automated detection. Comparison with the state-of-the-art method on GPT2-XL and Deepseek-7B show that TRIDENT reduces the false positive rate from $44.85\\%$ to $0.25\\%$ and increases the true positive rate from $14.65\\%$ to $99.7\\%$, achieving an AUC close to $99.9\\%$.", "tldr": "", "keywords": ["Large Language Models（LLMs）；Copyright infringement detection；"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a30d94dcaa001f25b91acfa332dfa795251016c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces TRIDENT, a method for detecting whether an LLM has been exposed to copyright-protected content during training. Unlike prior approaches that rely on a single signal (e.g., word-overlap), TRIDENT combines three complementary dimensions: (1) surface-level similarity (word-overlap metrics), (2) semantic-level similarity (for capturing conceptual memorization), and (3) quality-level signals (e.g., lexical diversity) that reflect the creativity of generated text. These dimensions are used to extract features from a Reference model (not exposed to the copyrighted set) and a Target model (which is exposed to copyrighted data), and are then fed into two detectors: a trained Random Forest classifier and a more classical one using a Difference-in-Differences framework. On GPT-2-XL and DeepSeek-7B, both variants outperform recent detection baselines and achieve near-perfect AUC scores on a dataset of English novels collected by the authors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Detecting LLM training data is not entirely new, but it’s definitely an important and timely topic, especially regarding copyright-content detection. For that reason, trying to improve over current detectors, which are still not perfect, is a well-motivated goal.\n\n- The idea of combining the three different dimensions is, to my knowledge, novel and I see it as the main contribution of the work.\n\n- I also think the paper does mostly a good job in explaining the proposed method. I have some minor clarification questions, which I develop in the following sections."}, "weaknesses": {"value": "- The method depends on a paired Reference vs. Target model comparison. In black-box models like GPT-5, even thought they sometimes allow fine-tuning, TRIDENT could only be used to test the presence of newly released books. With the current setup it remains unclear if it could be possible to construct such copyright-clean version of the reference model if we were interested in detecting an older book, like Harry Potter.\n\n- Perplexity may be a good membership inference indicator but it also limits the scope of models that are able to run TRIDENT, due to requirement for token-level probability access.\n\n- In Figure 5, the ROUGE-L distributions are almost perfect, and  Section 6.1 reports the models were fine-tuned for 10 epochs. After so much repeated exposure, memorization is unsurprising, but this setup does not seem comparable to how models are trained and deployed in practice."}, "questions": {"value": "- Picking up on the weakness regarding generalization to black-box models: can TRIDENT be applied to a fully black-box LLM such as GPT-5?\n\n- Is it necessary to include all the features in the RF classifier? As results show, some are highly correlated with each other, which may indicate redundancy.\n\n- What specific benefit does PCA provide over simpler weight aggregation?\n\n- Did the fine-tuning hurt the other model capabilities in any way? Since the models train for so many epochs on these books could they \nhave lost performance on other tasks? It would be great to see some experiments on this.\n\n- What is the detection performance with different levels of exposure of the books? Per example, if the models only train for 1 epoch.\n\n- I don’t see any references about possible release of code and the public-domain portion of the data. Will those be available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s5GS3qtnFo", "forum": "argTvCxHf9", "replyto": "argTvCxHf9", "signatures": ["ICLR.cc/2026/Conference/Submission11096/Reviewer_9ZP5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11096/Reviewer_9ZP5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760819033821, "cdate": 1760819033821, "tmdate": 1762922273424, "mdate": 1762922273424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework for detecting copyrighted content memorized by fine-tuned LLMs.\nA reference model is first obtained by fine-tuning a base model on public data. For each candidate sequence suspected of being copyrighted, both the reference and target models are prompted with a prefix of the sequence, and their generated continuations are collected.\nThe approach then computes a range of similarity metrics between the generated and target sequences, capturing surface-level, semantic, and quality-based similarities.\nTwo inference methods are proposed to determine whether a sequence was seen during training:\n- Integrated-Metrics Statistical Inference (IMSI): aggregates all similarity metrics, applies a difference-in-differences adjustment, and computes a final membership score using an optimal threshold.\n- Machine Learning Direct Discrimination (MLDD): trains a random forest classifier on the metric features to predict membership directly.\nThe framework is evaluated against existing membership inference attacks (MIAs) for LLMs, with additional ablations on sequence and prefix length, and an analysis of how each similarity type contributes to detection performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Detecting (copyright-protected) LLM training data is a timely and important problem\n- The paper considers a broad range of similarity metrics for inferring membership"}, "weaknesses": {"value": "The main issues I find with this work is (a) the lack of clarity about the evaluation dataset, which makes it difficult to evaluate and ultimately trust the results (see below) and (b) the limited novelty, the method being an ensemble of existing similarity metrics. \n\nDataset:\n- Previous work [1,2,3] has shown attacks against LLMs to be very sensitive to a distribution shift between member and non-member data and has proposed the use of model-less baselines, e.g., a simple Bag of Words classifier, to provide a measure of the distribution shift between member and non-member data. Adding a proper model-less baseline would provide a sound basis for the experimental setup and the reported results.\n- Details are lacking about where the data is collected from or how the dataset is constructed.\n- It is not clear what the MLDD model is trained vs evaluated on. Is the same data used for both? If so, the high performance metrics would seem misleading. \n- IMSI is not explained clearly: In section 4.2, delta_pub is computed by taking the expectation of S(x) over records in D_pub. D_pub is never defined, nor do the authors explain what is contained in it. In 4.3, D_val is used to find an optimal threshold. D_val is also never defined.\n- The method is only evaluated on one dataset (that is not public) and two models. This makes it difficult to know if the results will generalize to other datasets or are overfit to the specific dataset. Including the method’s performance on more (publicly accessible) datasets would strengthen the findings.\n\nNovelty:\n- The method does not seem particularly novel, being mostly an ensemble of existing similarity metrics without introducing a new modeling concept, theoretical insight, or attack formulation.\n- The two proposed inference methods (IMSI and MLDD) are also relatively standard statistical and machine-learning approaches, applied without significant innovation.\n\nMinor:\n- TRIDENT is misspelled (“THRIDENT”) in Section 4 (line 224)\n- New notations are introduced and never used again (e.g., H_0, H_1). This makes the paper difficult to read at times.\n- The composite score weighting (Section 4.1) could be explained more clearly. I assume “class” and “category” refer to dimensions, but it would be helpful to state this clearly.\n\n[1] Duan, Michael, et al. \"Do Membership Inference Attacks Work on Large Language Models?.\" First Conference on Language Modeling. 2024\n[2] Meeus, Matthieu, et al. \"Sok: Membership inference attacks on llms are rushing nowhere (and how to fix it).\" 2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). IEEE, 2025.\n[3] Das, Debeshee, et al. \"Blind baselines beat membership inference attacks for foundation models.\" 2025 IEEE Security and Privacy Workshops (SPW). IEEE, 2025."}, "questions": {"value": "- Where is the dataset collected from?\n- How many samples does the dataset contain, and how many of them are used for fine-tuning?\n- What is the exact data split between training, validation, and test sets for the MLDD model? Was the model evaluated on unseen data? If not, why? \n- What is contained in D_pub and D_val?\n- In the captions of tables 1 and 2, it says “N=2000”. N is never defined, from the rest of the paper it seems to denote sample size, but in Section 6.1 (Experimental setup), it says that the dataset consists of 100 samples. Could you clarify what N denotes and how many samples are used for training/evaluating your method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vgtOZRz33t", "forum": "argTvCxHf9", "replyto": "argTvCxHf9", "signatures": ["ICLR.cc/2026/Conference/Submission11096/Reviewer_1WM4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11096/Reviewer_1WM4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806550310, "cdate": 1761806550310, "tmdate": 1762922273046, "mdate": 1762922273046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses copyright infringement in LLMs and proposes TRIDENT, built on the observation that copyrighted data in an LLM is reflected in surface features, semantic relevance, and generation quality. Specifically, the method comprises two detection methods: IMSI and MLDD. Extensive experiments demonstrate its superiority over other detection approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well organized and easy to follow. \n2. This paper integrates various metrics (e.g., BLEU and Perplexity) to build the detection approaches."}, "weaknesses": {"value": "1. The technical contribution is a bit weak in this paper. The three-dimensional quantification framework incorporates well-studied metrics and classifies them into three dimensions: SLD, SCD, and QAD. To this end, the authors reorganize them for two detection approaches: IMSI and MLDD. I think authors should explicitly discuss the challenges when building these two approaches. \n2. The method description is unclear. In Section 4.1, IMSI requires intra-class weights and inter-category weights, but there is no further discussion about how these weights are set. In the introduction, the authors state that \"train a classifier for fully automated detection\" for MLDD. However, I cannot see any details about training in Section 4.2.\n3. The authors train reference and target models from a base model, which is impractical because these two models do not exist at the same time (i.e., one is trained by the copyrighted materials, while the other is trained by the non-copyrighted materials). Recent works on copyright protection use the pretrained base models (e.g., LLaMA and Qwen) and highlight their copyright infringement issues. How about when we want to report copyright infringement issues involving the proposed detection approaches for open-source models?"}, "questions": {"value": "**See Weaknesses**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RFaDKuTtEy", "forum": "argTvCxHf9", "replyto": "argTvCxHf9", "signatures": ["ICLR.cc/2026/Conference/Submission11096/Reviewer_JS5V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11096/Reviewer_JS5V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015377854, "cdate": 1762015377854, "tmdate": 1762922272709, "mdate": 1762922272709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for detecting LLM copyright infringement based on an aggregation of multiple metrics. The approach relies on an ensemble of metrics including BLEU, ROUGE, BERTScore, and Perplexity, as well as differences between a target model and a reference model. These features are then fed into a classifier to detect potential copyright infringement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The topic of copyright infringement detection is important and relevant for the LLM community.\n\nThe idea of combining different metrics into an ensemble seems well-motivated for this type of problem."}, "weaknesses": {"value": "The approach assumes access to both a reference model finetuned on non-copyrighted data and another model trained on both copyrighted and non-copyrighted data. This assumption does not seem very practical.\n\nThe experiments are limited to finetuning, where models are trained for 10 epochs. This seems quite high, and overfitting is likely at that point. In practice, finetuning would typically be done for fewer epochs. It would strengthen the paper if the authors could show results comparing their method and baselines when finetuning for only 1 epoch.\n\nIt is unclear whether the proposed method could be extended to pretraining data detection.\n\nNone of the baselines make use of reference information as the proposed method does, meaning they operate with less information. It would be useful to compare the proposed approach against a \"reference\"-type of baseline such as the one named \"Small\" from [1], but instead of using a smaller model as a reference as in [1] the authors can use the reference model from their paper.\n\n\nReferences\n\n[1] https://www.usenix.org/system/files/sec21-carlini-extracting.pdf"}, "questions": {"value": "If I understand this correctly, the novels are split into smaller chunks for detection. Is each chunk treated as a separate sample? How large is the detection set after splitting (i.e., the total number of samples, not just the number of novels)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iwOKDBWGgS", "forum": "argTvCxHf9", "replyto": "argTvCxHf9", "signatures": ["ICLR.cc/2026/Conference/Submission11096/Reviewer_9Yuv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11096/Reviewer_9Yuv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762100572148, "cdate": 1762100572148, "tmdate": 1762922272199, "mdate": 1762922272199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}