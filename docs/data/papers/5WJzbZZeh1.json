{"id": "5WJzbZZeh1", "number": 1516, "cdate": 1756888802340, "mdate": 1759898205150, "content": {"title": "AIGID-RFT: Reinforcement Fine-Tuning Multimodal LLMs for AI-Generated Image Detection", "abstract": "The rapid progress of generative artificial intelligence has made AI-generated image (AIGI) detection increasingly critical for digital forensics and trustworthy media. Existing AIGI detectors are effective on raw images but lack robustness against post-processing operations. Meanwhile, multimodal large language models (MLLMs) have demonstrated strong general capabilities, but their direct application to AIGI detection remains limited. To address these challenges, we propose AIGID-RFT, a novel MLLM-based AI-generated image detector. Unlike prior methods that rely on supervised fine-tuning, we adopt reinforcement learning as the post-training paradigm and design verifiable rewards tailored for the AIGI detection task, thereby unlocking the intrinsic potential of MLLMs. Furthermore, we carefully design a Cross Layer Forensic Adapter, which is integrated in parallel with the vision encoder to effectively exploit multi-level visual features for enhanced detection performance. Our method requires only binary labels for training, eliminating the need for costly text annotations. Extensive experiments demonstrate that our method significantly outperforms existing AIGI detectors under diverse post-processing operations that simulate real-world scenarios.", "tldr": "", "keywords": ["Multimedia Forensics", "MLLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/588bc4d19809dee8d3f719220d136f525b1ed076.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AIGID-RFT, a novel MLLM-based AI-generated image (AIGI) detector that leverages reinforcement learning with verifiable rewards (GRPO) and a Cross Layer Forensic Adapter (CLFA). The method enhances robustness to real-world post-processing operations - particularly JPEG compression (QF=50) - by integrating CLFA in parallel across intermediate layers of the vision encoder. Experiments on AIGIBench and LOKI show that AIGID-RFT achieves state-of-the-art performance, especially under degradation, while requiring only binary \"real / fake\" labels for training.\n\n[This review was utilising LLMs for typos and grammar/wording improvements, as well as literature research.]"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) Reproducible Setup: The use of public datasets (AIGIBench), open models (Qwen2.5-VL-Instruct 3B and 7B), fixed random seed (42), and full implementation details.\n2) Novel methodological integration: Combining reinforcement learning with MLLMs for AIGI detection is underexplored; this work fills an important gap.\n3) Strong evaluation design: Includes ablations, robustness tests across degradations, generalization to unseen generators (e.g., DALLE-3), and comparison to 11 SOTA detection methods.\n4) Clear motivation and alignment with real-world needs (JPEG compression), which remains a major challenge in practice.\n5) Regulatory and societal relevance: With regulations like the EU AI Act mandating transparency for AI-generated content, robust detection methods are becoming legally necessary. This work addresses a critical infrastructure need as synthetic media becomes ubiquitous across journalism, legal proceedings, and public discourse."}, "weaknesses": {"value": "While the paper is technically strong, it falls short in contextualising its contribution relative to recent peer-reviewed SOTA methods (published before July 24, 2025, see last Q/A in https://iclr.cc/Conferences/2026/ReviewerGuide):\n1) Missing comparison with CO-SPY (CVPR 2025): a recently released, highly relevant work that also targets generalization and robustness in AIGI detection by combining semantic and artifact-based features. The ICLR policy requires authors to cite and compare with any peer-reviewed paper published before July 24, 2025. This comparison is essential for evaluating the novelty and impact of AIGID-RFT.\n2) No discussion or comparison with existing forensic adapters, such as Cui et al., \"Forensics Adapter: Adapting CLIP for Generalizable Face Forgery Detection\" (CVPR 2025), which introduces adapter-based fine-tuning in visual encoders.\n3) While comparisons of CLFA vs Direct Additive Fusion (Appendix C), different model sizes (Appendix D) and prompts (think vs no-think, Appendix E) were studied, a direct ablation of the vision encoder + CLFA alone vs. the full MLLM pipeline would help clarify whether the performance gain comes from the adapter or is due to broader model capabilities.\n4) While inference speed and scalability are acknowledged briefly, a more nuanced analysis (e.g., latency comparison with lightweight CNNs, or an accuracy/compute metric for all tested models) would strengthen the practical impact statement.\n5) The comparison to the base MLLMs is done using zero-shot performance. A comparison to the models capabilities using few-shot prompts (see also theoretical discussions in https://arxiv.org/html/2507.16003v1), would also raise the quality of your new methodology."}, "questions": {"value": "To help clarify the contribution and improve the paper’s scientific rigor and actuality, please address:\n1) How does AIGID-RFT compare quantitatively to CO-SPY (CVPR 2025)\n2) Could you include an ablation that evaluates only the vision encoder + CLFA (without the LLM)? This would help isolate whether the gains come from the adapter itself or depend on full MLLM integration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PjMldIrcQS", "forum": "5WJzbZZeh1", "replyto": "5WJzbZZeh1", "signatures": ["ICLR.cc/2026/Conference/Submission1516/Reviewer_pa3X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1516/Reviewer_pa3X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761412784175, "cdate": 1761412784175, "tmdate": 1762915789006, "mdate": 1762915789006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AIGID-RFT, an AIGC Detection method that integrates reinforcement learning and MLLM features. On one hand, it aims to avoid overfitting caused by supervised learning through reinforcement learning. On the other hand, it leverages pre-trained knowledge from MLLMs to fuse features and form more generalizable forensic features. Experiments on AIGIBench demonstrate its effectiveness compared to several existing algorithms."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Proposes an AIGC Detection model that combines reinforcement learning and MLLM pre-trained features.\n- Demonstrates effectiveness against SOTA (State-of-the-Art) methods on the AIGIBench and LOKI datasets."}, "weaknesses": {"value": "**Unclear motivation explanation:**\n\nFirst, why is reinforcement learning adopted, and what are its core advantages over the supervised learning used in existing works? The phenomenon described in lines 038–047 and Figure 1 shows that existing works suffer severe performance degradation when detecting JPEG-compressed data. However, what is the connection between this phenomenon and the use of reinforcement learning? Can reinforcement learning learn to resist interference from JPEG or other post-processing during training? Additionally, does the use of reinforcement learning heavily depend on the initially provided network architecture and weights (e.g., why Qwen2.5-VL-7B is selected)? And why are MLLMs used? It is suggested that the authors first provide a more sufficient explanation of the solution's motivation.\n\n**Weak originality in the method:**\n\n(1) The Cross Layer Forensic Adapter (CLFA) proposed in Section 3.2 is merely an MLP-based feature fusion module. The Up/Down-projection and LayerNorm techniques involved are not original. Furthermore, what is the connection between this adapter and \"forensics\"? It does not include sufficient improvements tailored to the AIGC Detection task, making it difficult to be regarded as an independent core innovation.\n\n(2) The reinforcement learning-based training stage in Section 3.3 directly adopts GRPO without additional modifications.\n\n**Insufficient experimental analysis:**\n\n(1) The results in Table 1 show poor detection performance for Deepfake (face-swap) images. The paper attributes this to \"significant differences between such face-swap images and fully generated images in the training set.\" If face-swap data is center-cropped, can a significant performance improvement be achieved? It is suggested that the paper conduct a more in-depth discussion on this point.\n\n(2) Lines 356–364 only describe the results in Table 2 without analysis. For example, why does GPT-4o achieve significantly better performance than other MLLMs? Additionally, these MLLMs initially only achieve an accuracy close to random guessing (~0.5), so why is it claimed that MLLMs are beneficial for the AIGC Detection task?\n\n(3) Based on the results in Table 3, why does the proposed method experience a greater performance drop in Fake Accuracy compared to Real Accuracy under JPEG compression? And why can compressed Real images be detected more accurately?\n\n(4) Why do failure cases like those in Figure 3 occur? Is it due to incorrect semantic judgment by the MLLM, or the inability to extract valid forensic traces from visual signals?\n\n(5) Line 416 mentions, \"We observe that SFT tends to overfit to images in the training set, whereas GRPO promotes better generalization.\" Why can GRPO avoid overfitting caused by supervised learning? If other methods (e.g., adversarial training) are adopted, can they achieve similar detection performance to GRPO-based reinforcement learning?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "M6JRMEVywS", "forum": "5WJzbZZeh1", "replyto": "5WJzbZZeh1", "signatures": ["ICLR.cc/2026/Conference/Submission1516/Reviewer_tuqj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1516/Reviewer_tuqj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575133357, "cdate": 1761575133357, "tmdate": 1762915788873, "mdate": 1762915788873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address the robustness of current AI-generated image detectors against post-processing operations by using multimodal large language models. A MLLM-based AI-generated image detector termed AIGID-RFT is proposed by introducing reinforcement learning. Furthermore, a Cross Layer Forensic Adapter is designed with the vision encoder to exploit multi-level visual features for enhanced detection performance. Experiments demonstrate the effectiveness of proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper leverages the MLLM for AI-generated image detection task, which is interesting.\n2. The proposed reinforcement learning with MLLM for the task is interesting.\n3. The robustness is actually a critical issue in this area.\n4. The visualization example in Figure 3 is practical since it provides some explainable reasoning process.\n5. The authors conduct extensive experiments on mainstream MLLM and other AI-generated image detector baselines."}, "weaknesses": {"value": "1. Although the robustness issue the authors aim to solve is critical in this area and the results in Figure. 1 supports this, I am still confused how the authors solve this issue in this paper, more explanations on this are needed.\n2. There are already some similar Adapter or LoRA technologies, the authors should explain the difference between their CLFA and these existing ones.\n3. There are some more powerful multimodal foundation models being proposed, such as Qwen3-VL, did the authors consider to extend their methods to other powerful recent foundation models?\n4. Why choose GRPO for reinforcement learning instread of DPO/PPO? What is the connection between GRPO and CLFA?\n5. The authors claim that using reinforcement learning with MLLM for ai-generated detection is unexplored, which I can hardly agree. There are some related work such as [1,2,3]. More discussions and comparions are needed.\n\n[1] Huang, Tai-Ming, et al. \"ThinkFake: Reasoning in Multimodal Large Language Models for AI-Generated Image Detection.\" arXiv preprint arXiv:2509.19841 (2025).\n[2] Ji, Yikun, et al. \"Towards Explainable Fake Image Detection with Multi-Modal Large Language Models.\" arXiv preprint arXiv:2504.14245 (2025).\n[3] Xu, Zhipei, et al. \"AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection.\" arXiv preprint arXiv:2505.15173 (2025)."}, "questions": {"value": "Please refer to the weakness part. My major concerns lie in the novelty and method design parts. So I currently lean towards negative ratings, if the authors could address the former issues properly during rebuttal, I will change my ratings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tCknY3NETu", "forum": "5WJzbZZeh1", "replyto": "5WJzbZZeh1", "signatures": ["ICLR.cc/2026/Conference/Submission1516/Reviewer_hXs7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1516/Reviewer_hXs7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881491657, "cdate": 1761881491657, "tmdate": 1762915788756, "mdate": 1762915788756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AIGID-RFT, a novel approach for detecting AI-generated images using multimodal large language models (MLLMs) fine-tuned with reinforcement learning. The authors note that while current AI-generated image detectors work well on unmodified images, they are fragile against common post-processing(e.g., resizing, compression, filters..). On the other hand, multimodal LLMs have shown impressive general capabilities, but naively applying them to image forensics does not work well. AIGID-RFT addresses this by using reinforcement learning as a post-training multimodal model rather than standard supervised fine-tuning. They develop verifiable reward signals to guide the model toward correct real/fake predictions, and introduce a Cross Layer Forensic Adapter module that is inserted into multiple layers of the model’s visual encoder to capture multi-scale visual features crucial for detecting artifacts. Through an RL training process, the MLLM learns to better recognize AI-generated images and avoid being misled by post-processing tricks. This approach demonstrates how combining the strengths of large multimodal models with RL fine-tuning can improve the deepfake image detection."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I am not expert in this field, but to the best of my knowledge, it is first paper to employ RL post-training for ai generated image detection. Good significance. \n- This paper is well written,organized and easy to follow\n- Shows strong performance on extensive amounts of experiments. Components are well explained."}, "weaknesses": {"value": "- Lack of inference time comparison with other methods\n- Limited literature survey. Since adoption of multimodal model for AIGID is relatively new, it would be better to introduce similar works like https://arxiv.org/pdf/2504.14245. I am not expert in this field but I've seen some similar works before."}, "questions": {"value": "- Is text annotation necessary for SFT method? What if we make label for SFT with simple text like <answer> fake <answer>? \n- I think choice of benchmark(AIGIbench, Li et al 2025) is little bit concerning since it is less known benchmark. What is your reasoning behind not using traditional benchmark"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zvfTDECw82", "forum": "5WJzbZZeh1", "replyto": "5WJzbZZeh1", "signatures": ["ICLR.cc/2026/Conference/Submission1516/Reviewer_dDMg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1516/Reviewer_dDMg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975846398, "cdate": 1761975846398, "tmdate": 1762915788623, "mdate": 1762915788623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}