{"id": "wDHaTrs8UT", "number": 22595, "cdate": 1758333252116, "mdate": 1759896857792, "content": {"title": "GDC: From Brittle Optimality to Robust Satisfiability  via Riemannian Risk Geometry", "abstract": "Standard reinforcement learning (RL) often yields brittle policies that fail under hard safety constraints. We propose **Geodesic Duality Control (GDC)**, which adapts an agent’s risk posture endogenously by re-weighting the Bellman target using local geometric cues of the value function—specifically, the gradient magnitude $||\\nabla V||$ and a curvature surrogate $\\kappa$. To accommodate piecewise-smooth neural critics, we formulate a Sub-Riemannian / generalized-gradient treatment and provide practical, numerically stable curvature surrogates. Our main theoretical result shows that, under explicit regularity and stochastic-model assumptions, GDC induces a curvature-decreasing learning dynamic:\n\n$$\n\\frac{d\\kappa}{dt} < 0,\n$$\n\nwhich in turn increases a quantifiable safety margin $\\Delta_s$. We validate the mechanism with proof-of-concept experiments—including a hard-boundary safety environment (*Optimal-Trap*), targeted ablations, and a computational-cost study on Humanoid-Safety—to confirm the intended geometric risk posture. We do not claim broad empirical superiority on all benchmarks; rather, the paper’s primary contribution is theoretical, with key components validated empirically.", "tldr": "We propose Geodesic Duality Control (GDC), a reinforcement learning method that adjusts the Bellman target based on geometric curvature to improve policy safety margins while maintaining performance.", "keywords": ["Differential Geometry", "Risk-Sensitive Control", "Robustness"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60994ae845d3b9bcf95a8cbd7ccaa7e046977903.pdf", "supplementary_material": "/attachment/ac048a0826b0efc5f9c4095a3d07292124ac97a9.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Geodesic Duality Control (GDC), which adapts an agent’s risk posture endogenously by re-weighting the Bellman target using local geometric cues of the value function, aiming at enhancing safe RL."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper promotes safe RL, which is an important research topic."}, "weaknesses": {"value": "1. I find it difficult to follow the mathematical notation, which makes it challenging to verify the correctness of the theoretical results. For example, immediately after equation (2.2), the meanings of k and κ are not defined. Are they mappings or scalar parameters?\n\n2. In equation (3.2), $C_{\\tau}$ depends on policy/update stability. What is the definition of stability in this context, and how is $C_{\\tau}$ obtained?\n\n3. The paper mentions that \"safety boundaries may move over time.\" Does this mean the safety conditions and constraints are time-varying?\n\n4. What is the formal definition of safety margins? It would be helpful if the paper provided concrete examples to illustrate this concept.\n\n5. Assumption 4.1 relies on a predictor. How can such a predictor be realized in practice?\n\n6. The experimental results show that the proposed approach outperforms the compared RL frameworks; however, these are only standard baselines that were published many years ago. The paper shall compare with safe RL approaches. In recent years, significant efforts have been devoted to safe RL, such as the work in [1-6]. Additionally, the paper lacks a comprehensive literature review of related work. For these reasons, I cannot assess whether the proposed method outperforms existing safe RL frameworks.\n\n**References**\n\n[1] Wang, Z., & Mahmoudian, N. (2025). Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model. arXiv preprint arXiv:2508.09971.\n\n[2] Banerjee, A., Rahmani, K., Biswas, J., & Dillig, I. (2024). Dynamic model predictive shielding for provably safe reinforcement learning. Advances in Neural Information Processing Systems, 37, 100131-100159.\n\n[3] Cai, Yihao, Yanbing Mao, Lui Sha, Hongpeng Cao, and Marco Caccamo. \"Runtime Learning Machine.\" ACM Transactions on Cyber-Physical Systems (2025).\n\n[4] Phan, D. T., Grosu, R., Jansen, N., Paoletti, N., Smolka, S. A., & Stoller, S. D. (2020). Neural Simplex architecture. In NASA Formal Methods: 12th International Symposium, Moffett Field, CA, USA, May 11–15, 2020.\n\n[5] Cheng, R., Orosz, G., Murray, R. M., & Burdick, J. W. (2019, July). End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. In Proceedings of the AAAI conference on artificial intelligence.\n\n[6] Hongpeng Cao, Yanbing Mao, Lui Sha, and Marco Caccamo. Physics-regulated deep reinforcement learning: Invariant embeddings. In The Twelfth International Conference on Learning Representations, 2024."}, "questions": {"value": "My questions are also included in the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SArC8f8H3R", "forum": "wDHaTrs8UT", "replyto": "wDHaTrs8UT", "signatures": ["ICLR.cc/2026/Conference/Submission22595/Reviewer_K7HT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22595/Reviewer_K7HT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760829490906, "cdate": 1760829490906, "tmdate": 1762942295856, "mdate": 1762942295856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a geometry-aware Bellman update that incorporates local geometric properties of the critic (such as steepness and concavity) into policy evaluation and improvement. The method adaptively adjusts how much this geometric information influences the update, with the goal of reducing safety risk. Experiments on humanoid control and navigation tasks in Safety-Gymnasium indicate that the method can reduce safety cost while achieving reward comparable to existing safe RL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces the idea of using local geometric information of the reward critic (e.g., steepness and concavity) to assess risk and to modify the Bellman update. This is a novel perspective compared to standard safe RL approaches that explicitly learn a separate cost critic.\n\n2. The method shows promising results on Safety-Gymnasium benchmarks, suggesting that it can reduce safety cost while keeping reward competitive, without relying on an explicit cost critic."}, "weaknesses": {"value": "1. The paper does not provide training curves. This is important for evaluating stability and convergence robustness in RL.\n\n2. No qualitative evidence (e.g., videos) is provided to compare behaviors between baselines and the proposed method. For safety-focused work, qualitative behavior differences are important.\n\n3. It is unclear why geometric properties of the reward critic (steepness and concavity) should reflect safety risk. In general, reward and cost are different signals. The paper does not convincingly argue why shaping with these geometric terms should guarantee safer behavior.\n\n4. Problem formulation is underspecified. In the preliminaries, the paper states that the objective is to maximize expected return. However, the experiments clearly optimize for both high reward and low cost, which corresponds to a Constrained MDP setting. The role of cost in Algorithm 1 and Section 2.4 is not clearly defined.\n\n5. The paper lacks a Related Work / Prior Work discussion, which makes it difficult to assess originality relative to existing safe RL methods.\n\n6. The theoretical analysis does not provide a formal guarantee or even a clear argument for why the proposed update should reduce cost while preserving reward. In particular, there is no proof that the method solves a constrained RL problem or enforces a cost threshold."}, "questions": {"value": "1. Please give a concrete example showing how high curvature or non-smoothness of (Q(s,a)) implies unsafe behavior.\n2. Why do you multiply $\\sigma(s,a;Q)$ by $\\min(0, R(s,a))$? What is the intuition or theoretical motivation for this specific gating?\n3. You argue that the smoothness term $\\sigma(s,a;Q)$ measures safety risk. But in Safety-Gymnasium, cost is often not aligned with reward (for example in `PointGoal` and `CarGoal`, the reward encourages reaching the goal, while the cost penalizes hitting hazards). In that case, $\\sigma(s,a;Q)$ only reflects the reward landscape, not the cost landscape. Why should penalizing steepness/concavity of the reward critic reduce cost?\n4. In Algorithm 1, line 9: why is $\\kappa(s', a')$ adjusted using the Lagrange multiplier on the cost? Please explain the mathematical reasoning behind subtracting that term, and how it connects to constrained optimization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aLoMCEkDYX", "forum": "wDHaTrs8UT", "replyto": "wDHaTrs8UT", "signatures": ["ICLR.cc/2026/Conference/Submission22595/Reviewer_16w9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22595/Reviewer_16w9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644376577, "cdate": 1761644376577, "tmdate": 1762942295613, "mdate": 1762942295613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The authors define an internal risk using the gradient and curvature of the critic, and solve the constrained RL problem by minimizing this risk.\n- Rather than strictly satisfying global constraints, the goal is to obtain a piecewise-smooth critic."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The authors derive the conditions required for the proposed GDC operator to converge to a fixed point and prove convergence under these conditions."}, "weaknesses": {"value": "- The overall presentation is remarkably unclear and reader-unfriendly.\n- Before describing the proposed method, the authors provide neither an explanation of the mathematical objective (i.e., the specific type of RL agent being targeted) nor any justification for that objective.\n    - For example, there should be a statement like: \"We aim to satisfy constraints while keeping the critic's gradient and curvature below certain thresholds. This objective is motivated by the need to prevent the following issues…\"\n    - Without such foundational context, the paper jumps straight into methods, making the flow extremely difficult to follow.\n- The following points lack sufficient explanation (new concepts should be introduced sequentially for clarity in a good paper):\n    - Line 93: No definition of local risk profile.\n    - Equation 2.1: No justification for using $c \\cdot \\max(0, \\cdot)$ in the risk definition.\n    - Line 119: No explanation of the penalty mechanism.\n    - Line 142: No description of the mollified critic.\n    - Figure 1: Text is too small to read.\n    - Line 148: Lanczos cost is neither explained nor cited.\n    - Line 151: HVP oracle is neither defined nor cited.\n    - Line 157: No justification for the implementation checklist requirements.\n    - Line 167: cost is used without prior definition.\n    - Line 169: $C_{\\text{target}}$ is undefined.\n    - Line 175: temperature is introduced without definition.\n    - Line 202: No reason provided for controller calibration.\n    - Line 262: failure boundary, $V_{\\max}$, and $\\text{Unc}(t)$ are all undefined.\n    - Proposition 4.1: failure is not defined.\n    - Line 281: Scheduling of $c$ is highly heuristic.\n- Too many new hyperparameters are introduced, requiring extensive heuristic tuning, which significantly increases the practical time needed to obtain a preferable RL agent.\n- The constrained RL algorithms used as baselines in the experiments are quite outdated despite being presented as state-of-the-art."}, "questions": {"value": "- Section D.1 suggests that Q is defined in a continuous action space, but the preliminary section implies it is discrete and finite. Which is correct?\n    - The rest of the paper should be consistently aligned with one definition.\n- In Equation 2.7, as $\\kappa_0$ increases, the sigmoid approaches 0, preventing the application of penalties. It seems $\\kappa_0$ does not serve its intended role. what is the issue here?\n- Proposition 4.1 introduces the concept of safety distance. Is the proposed method applicable only to collision avoidance tasks?\n- In Figure 2 (a, b), why does GDC outperform other constrained RL methods?\n    - Theoretically, GDC must minimize internal risk in addition to satisfying constraints, so its performance should be at most as good as the optimal policy of the original constrained RL problem.\n    - The observed better performance suggests that either the baseline algorithms are not state-of-the-art or there are issues with their training configurations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AHq37UIgXt", "forum": "wDHaTrs8UT", "replyto": "wDHaTrs8UT", "signatures": ["ICLR.cc/2026/Conference/Submission22595/Reviewer_1S9K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22595/Reviewer_1S9K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993127751, "cdate": 1761993127751, "tmdate": 1762942295251, "mdate": 1762942295251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General clarification and overview of planned revisions"}, "comment": {"value": "We thank all reviewers for their detailed and thoughtful feedback.  \nHere we provide a general clarification for all reviewers and outline the main\nrevisions we are planning. In subsequent comments we will address\nreviewer-specific questions point by point.\n\n**Goal and main idea.**  \nOur goal is *not* to propose yet another heuristic safe-RL algorithm, but to\ndevelop a **geometry-based theoretical framework** for safe RL and to validate\nits key mechanisms empirically. Concretely, we (i) define a local geometric risk\nsignal κ(s,a) derived from the critic’s gradient and curvature, (ii) analyze the\nresulting **Geodesic Duality Control (GDC) Bellman operator** and prove\ncontraction and safety-margin properties, and (iii) instantiate these ideas in\npractice via numerically stable curvature surrogates and an adaptive risk\ncontroller.\n\nSeveral reviewers noted that this overall story was hard to see from the\ncurrent presentation. We acknowledge that the exposition is too dense and that\nimportant definitions are scattered.\n\n**Clarifying problem formulation and notation.**  \nIn a revision we will add a short subsection “Problem Setup and Objective”\nbefore Section 2, explicitly stating the MDP, the safety signal, and the\nprecise objective (maximize return while keeping long-run violation rate below\na target $C_{\\text{target}}$). We will also introduce a unified notation and\nconcept table collecting the definitions of local risk κ(s,a), local risk\nprofile, safety boundary and safety margin, $C_{\\text{target}}$, $V_{\\max}$,\n$\\mathrm{Unc}(t)$, the failure event, etc. This directly addresses the concerns\nabout missing or delayed definitions (especially R2 and R4).\n\n**From geometry to safety.**  \nA central question (especially from R3) is why the local geometry of the\nreward-based critic should reflect safety risk. We will add an intuitive 2D\nexample and visualization (based on the Optimal-Trap environment) showing how\nhazards induce sharp “cliffs” in Q, so that large gradient norm / negative\ncurvature aligns with empirical failure regions. We will also clarify how we\npenalize violations in the reward used to train the critic, so that catastrophic\nevents correspond to abrupt drops in Q, and discuss how this interacts with\npotential reward–cost misalignment in Safety-Gymnasium.\n\n**Accessibility of the mathematical formalism.**  \nWe agree that introducing the sub-Riemannian formalism and generalized\ngradients too early makes the paper difficult to read (R1, R2, R4). We will\nrestructure the theory sections to start from an informal Euclidean picture and\nto state each main theorem in plain language first, moving some technical\ndetails and proofs to the appendix. The goal is to keep the theoretical\nguarantees unchanged while making the narrative easier to follow for RL\npractitioners.\n\n**Implementation details and hyperparameters.**  \nReviewers also raised concerns about undefined quantities such as the\nmollified critic, the Lanczos cost, and the adaptive parameter κ₀, as well as\nthe number of hyperparameters. We will clarify that GDC introduces only two new\nscalar hyperparameters beyond SAC (curvature weight and adaptation rate),\ndescribe which values are shared across tasks, and move the implementation\nchecklist from the appendix into the main text. We will also better explain the\ncontrol intuition behind the κ₀ update rule and its relation to constrained RL\n(Lagrangian) methods.\n\n**Experiments and baselines.**  \nWe agree that the current experimental section is limited in scope and that we\nshould more clearly situate GDC with respect to recent safe-RL baselines\n(R1, R2, R3, R4). We have started running additional experiments with newer\nbaselines (from the repository suggested by R1 and works cited by R4), adding\ntraining curves and qualitative trajectory visualizations. Preliminary results\nindicate that our main safety–performance trade-off conclusions remain\nunchanged. We will include these additional results and an expanded related-work\ndiscussion in a revised version.\n\nWe hope this high-level clarification helps contextualize our work and\naddresses some of the main concerns about clarity and positioning. We are\ngrateful for the reviewers’ efforts and will follow up with detailed,\nreviewer-specific responses.\n\nAt a high level, the paper asks how we can control safety directly through the\nshape of the value function, instead of by adding hard constraints or training\na separate cost critic. We show that local geometric cues — how steep and\ncurved \\(Q\\) is around the current state–action — encode how close the policy is\nto a safety boundary. GDC turns these cues into a principled Bellman update\nthat expands safety margins while preserving contraction. This yields a general\nrecipe for building safe variants of standard RL methods whenever a\ndifferentiable critic is available."}}, "id": "Wd99bnM3ze", "forum": "wDHaTrs8UT", "replyto": "wDHaTrs8UT", "signatures": ["ICLR.cc/2026/Conference/Submission22595/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22595/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22595/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763046928266, "cdate": 1763046928266, "tmdate": 1763046928266, "mdate": 1763046928266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The study proposes a theoretical and algorithmic framework for safe reinforcement learning, interpreting risk as a local geometric property of the critic (value function).  Instead of external constraints or global cost budgets, GDC computes a geometric risk metric that combines the gradient norm and a curvature surrogate. Experiments demonstrate improved safety-return trade-offs compared to SAC, PCPO, and FOCOPS, with empirical validation of the theoretical safety margin bounds."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Frames risk as an endogenous geometric signal.\n\n2. Provides formal proofs for contraction (Theorem 3.3), Lipschitz properties (Lemma 3.1), and dynamic safety margins (Proposition 4.1).\n\n3. Empirically supports theoretical safety-margin predictions (Fig. 4)."}, "weaknesses": {"value": "1. The Sub-Riemannian formalism, while elegant, may be inaccessible to most RL practitioners.\n\n2. Some proofs rely on strong regularity assumptions (bounded spectral norms, smooth mollifiers) that may not strictly hold in deep RL.\n\n3. Experiments focus on a few safety environments; lacks evaluation on large-scale settings.\n\n4. Comparisons omit newer safe-RL baselines, see here for new baselines: https://github.com/chauncygu/Safe-Reinforcement-Learning-Baselines\n\n5. While adaptive control helps, other hyperparameters (such as k and $\\eta$) still require task-specific tuning, possibly limiting generality."}, "questions": {"value": "1. How sensitive is GDC to inaccuracies in curvature estimation, especially under noisy gradients?\n\n2. Could GDC be integrated with model-based or offline RL frameworks where Q is not directly differentiable?\n\n3. What are the differences compared with the two papers? These also consider state safety. [1] https://proceedings.mlr.press/v164/liu22c/liu22c.pdf [2] https://ieeexplore.ieee.org/document/10616119"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b86pfDfbyG", "forum": "wDHaTrs8UT", "replyto": "wDHaTrs8UT", "signatures": ["ICLR.cc/2026/Conference/Submission22595/Reviewer_AaXj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22595/Reviewer_AaXj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049328909, "cdate": 1762049328909, "tmdate": 1762942294889, "mdate": 1762942294889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}