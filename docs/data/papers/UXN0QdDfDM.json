{"id": "UXN0QdDfDM", "number": 16815, "cdate": 1758269030965, "mdate": 1759897217962, "content": {"title": "Direct Preference Optimization for Dynamical System Modeling", "abstract": "Accurately predicting complex dynamic systems is crucial for scientific research and engineering practice, which is widely used in weather forecasting and fluid dynamics, \\textit{etc}. However, \\textit{the off-the-shelf} deep methods that rely only on numerical metrics, which often fail to capture \\textbf{rare} events and ignore \\textbf{human needs} for physical consistency and interpretability. With this in mind, this paper proposes a human-machine collaborative dynamical system prediction framework~\\method{} that combines numerical accuracy with human preference scores. First, we pre-train the base model by minimizing the expectation risk to achieve a reliable convergence landscape. Then, we ideally plug in a diverse sampling strategy for generating different candidate predictions and adopt human-trusted metrics to select high (low)-quality prediction pairs to train a preference model. Finally, we jointly optimize the fixed preference objective with the pre-trained prediction model to improve both numerical accuracy and human perceptible quality. We provide theoretical analysis shows that this process can be seen as a bi-level optimization or game problem under certain conditions and can converge to an equilibrium solution. Experimental results demonstrate that \\method{} not only effectively reduces overall risks in various dynamic system scenarios, including numerical weather forecasting and fluid vortex simulation, but also significantly outperforms existing SOTA methods in visual consistency and capturing extreme events.", "tldr": "", "keywords": ["AI for Physics"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f39cd3a1459cd60db4271bce296bcf031f00c907.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes the PRIMS framework, which first pretrains forecasting models and then aligns them with human-preferred or trusted metrics. The framework is evaluated on benchmarks in fluid dynamics and numerical weather forecasting, and can serve as a plug-and-play enhancement module for a variety of predictive models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written, with clear explanations and a well-structured presentation.\n\n2. The proposed method is novel and highly flexible. By fine-tuning models using Direct Preference Optimization (DPO), it effectively addresses the challenge of aligning model outputs with non-differentiable or difficult-to-optimize expert criteria.\n\n3. The authors conduct extensive experiments, demonstrating consistent and reasonably strong results across multiple benchmarks."}, "weaknesses": {"value": "The paper claims that the proposed framework enhances visual perceptual consistency and attention to extreme events. While these goals are important, I find the supporting evidence for these claims somewhat unconvincing.\n\n1. The improvement in visual perceptual consistency appears to be a byproduct of better performance resulting from optimization with respect to human-trusted metrics, rather than a direct consequence of the proposed framework. It is not entirely clear how the optimization process itself inherently contributes to perceptual consistency, and further clarification on this conceptual link would be valuable.\n\n2. The model is evaluated on the SEVIR dataset, which, to my understanding, is designed to assess a model’s ability to capture extreme weather events. It would be helpful if the authors could provide additional context on this dataset—particularly regarding its composition and the setup of training and testing. If the model is trained specifically on data containing extreme events, how does this setting differ from standard datasets, and how does it substantiate the claim of superior performance when predicting real extreme events?"}, "questions": {"value": "See comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "icyf2DsW4J", "forum": "UXN0QdDfDM", "replyto": "UXN0QdDfDM", "signatures": ["ICLR.cc/2026/Conference/Submission16815/Reviewer_Mook"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16815/Reviewer_Mook"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761612254587, "cdate": 1761612254587, "tmdate": 1762926843641, "mdate": 1762926843641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework to integrate human-trusted metrics into the scientific prediction framework."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and clearly presents the framework. The experiments are comprehensive."}, "weaknesses": {"value": "Some inconsistencies exist. E.g., the perline noise vs. the Gaussian noise. The theoretical proof is missing."}, "questions": {"value": "1.\tWhat is the Perlin noise in Fig. 1? In you Eq. (4), it seems that you use Gaussian noise.\n2.\tThe author is suggested to give a toy example to reveal what human-trusted metric improves what performances, e.g., extreme event predictions. \n3.\tThe author should mention the limitations and future work. \n4.\tIn the abstract, the author mentioned the theoretical proofs to a stable equilibrium. However, I couldn’t find it in the main part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ERkh8v1ZGE", "forum": "UXN0QdDfDM", "replyto": "UXN0QdDfDM", "signatures": ["ICLR.cc/2026/Conference/Submission16815/Reviewer_jDus"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16815/Reviewer_jDus"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793956035, "cdate": 1761793956035, "tmdate": 1762926843299, "mdate": 1762926843299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRISM, a human-machine collaborative framework for improving spatiotemporal forecasting in dynamical systems. The authors argue that traditional pixel-wise training objectives like MSE fail to capture rare or high-frequency physical events, and often produce over-smoothed, perceptually unrealistic results. PRISM addresses this by incorporating a learned preference model trained from human-trusted proxy metrics. This model is used to guide fine-tuning of the base forecasting model via direct preference optimization, aiming to balance numerical accuracy with physical realism. The authors also release a new benchmark dataset (HPSci) with human preference annotations, and demonstrate improvements on several dynamical system tasks including fire simulation, fluid convection, and extreme weather forecasting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The motivation is clear and grounded in real limitations of current forecasting models, especially regarding smoothness and failure to model rare or structurally important events. The overall pipeline is well-structured: pretrain with MSE, generate perturbed samples, learn a preference oracle, and fine-tune with DPO. This general framework is easy to follow\n\n2. The experiments are thorough, with comparisons across multiple backbones (ConvNets, Transformers, Neural Operators, GNNs) and tasks."}, "weaknesses": {"value": "1. The method borrows heavily from language model alignment (e.g., DPO in ChatGPT), but the analogy to physical systems is not entirely convincing. In NLP, humans can directly compare two responses. In dynamical systems, \"human preference\" is often just a proxy for certain physical heuristics (SSIM, energy spectrum, etc), which may not actually reflect expert judgment or be consistent across tasks.\n\n2. The paper relies on perturbation (e.g., Gaussian noise, embedding swaps) to generate preference pairs, but it is unclear whether these perturbed samples still preserve the physical integrity of the system. In many cases, adding noise can lead to completely unphysical outputs, making pairwise ranking unreliable.\n\n3. There is no serious discussion about how transferable the learned preference model is across different types of systems. Preferences learned on Rayleigh–Bénard convection may not be meaningful for fire propagation or extreme weather, since the evaluation criteria differ.\n\n4. The choice of proxy metrics (e.g., SSIM, CSI) is taken as given, but they have known flaws. SSIM penalizes high-frequency changes even if they are physically correct. CSI only focuses on event detection but ignores structural evolution.\n\n5. Some ablation results are missing. For instance, what happens if you skip the preference fine-tuning and just train with noisy samples? Or what if the preference model is replaced with a simpler ranking function?"}, "questions": {"value": "1. Your setup is quite inspired by the reward modeling used in LLMs. But in those settings, humans directly rank the responses. In your case, how confident are you that metrics like SSIM or CSI are really aligned with human judgment of \"better\" or \"more realistic\"? Do you have any evidence beyond citing that they are \"widely used\"?\n\n2. The perturbation module is critical to generating diverse samples. But how do you ensure these perturbations do not produce physically invalid outputs? Wouldn't this make the preference labels unreliable?\n\n3. Have you tested whether a preference model trained on one domain (e.g., fire) works well on another (e.g., convection)? If not, would you expect to need retraining for each task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "C26zRwFvwK", "forum": "UXN0QdDfDM", "replyto": "UXN0QdDfDM", "signatures": ["ICLR.cc/2026/Conference/Submission16815/Reviewer_ygcU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16815/Reviewer_ygcU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917093165, "cdate": 1761917093165, "tmdate": 1762926842899, "mdate": 1762926842899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PRISM, a framework that applies Direct Preference Optimization (DPO) to dynamical system modeling. The authors argue that traditional pixel-wise metrics (e.g., MSE) produce overly smooth predictions that fail to capture extreme events and physical realism. The proposed approach involves: (1) pre-training a foundation model with MSE loss, (2) generating diverse prediction samples through perturbation of the inputs, (3) training a preference model using physics-based metrics as proxies for human judgment, and (4) jointly optimizing the foundation model using both MSE and DPO losses. The paper demonstrates improvements on physics-based metrics across multiple benchmarks including fire simulation, Rayleigh-Bénard convection, and weather forecasting."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. **Important problem**: The paper addresses a real limitation in dynamical system modeling, namely, that standard MSE optimization produces overly smooth predictions that may miss extreme events and lack physical plausibility.\n2. **Comprehensive experiments**: The evaluation covers diverse domains (fluid dynamics, fire spread, weather) and tests across 11 different backbone architectures.\n3. **Results**: The method shows improvements across most models and metrics, suggesting the approach has practical utility.\n4. **Easy to use**: The framework can be applied to various existing models without architectural modifications."}, "weaknesses": {"value": "1. **Lack of clarity around proposed dataset and human involvement**: The paper proposes \"a novel human-machine collaborative framework\". The paper also claims to use \"human preferences,\" \"crowdsourced annotations,\" and \"complex, often non-differentiable human judgments.\" In practice, as far as I can tell, the \"HPSci benchmark\" does not appear to be a new dataset with actual human annotations, but rather consists of existing datasets (BLASTNet, PDEBench, SEVIR) augmented with standard metrics such as SSIM, CSI, and turbulent kinetic energy spectra. This interpretation only became clear after a careful reading of the paper, and I'm still not sure what was actually done. Also, the metrics used in the paper are in fact differentiable, contradicting the claim of \"non-differentiable human judgments.\"\n2. **Overclaimed novelty**: The core methodology appears to be a straightforward application of DPO (Rafailov et al., 2024) to the domain of dynamical systems modeling. The paper claims \"Novel Methodology\" (page 3, line 108) and frames itself as a methodological contribution, but in practice there appear to be no modifications or extensions to the DPO algorithm itself. Given the lack of methodological innovation, and the apparent lack of actual human involvement (see previous point), this paper seems like a proof of concept more than anything. Such a study is still interesting in my opinion, but I think the paper in its current form oversells the actual contribution.\n3. **Missing baselines**: An obvious baseline is direct multi-objective optimization, that is, incorporating the physics metric directly in the loss function, e.g., `L = L_MSE + λ·L_SSIM`.\n4. **Missing theoretical analysis**: The abstract and introduction (page 1, line 21) claim \"bi-level optimization problem converging to a stable equilibrium\" and reference \"game theory approach.\" However, the \"theoretical grounding\" claimed in the abstract is entirely missing\n5. **Perturbation strategies underspecified**: The paper claims: \"we generate a diverse set of candidate predictions by perturbing inputs or replacing discrete embeddings.\" The input perturbation is described in Section 3.3.1, but the replacement of discrete embeddings is not described anywhere."}, "questions": {"value": "1. Did any actual humans annotate preferences in HPSci? If so, please describe the annotation protocol. If not, please clarify that the dataset uses only automated metrics.\n2. What is the core methodological contribution, beyond applying DPO in a dynamical systems setting?\n3. Can you clarify the claim that metrics are \"non-differentiable\"? SSIM, CSI, and energy spectrum comparisons all have defined gradients. If you mean something else, please specify.\n4. Can you provide the promised theoretical analysis (convergence guarantees, stability) or remove these claims?\n7. Why DPO? Given that your preference signal comes from differentiable metrics, what specifically does the DPO framework provide that direct gradient-based optimization of those metrics does not?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wxIkduWf8r", "forum": "UXN0QdDfDM", "replyto": "UXN0QdDfDM", "signatures": ["ICLR.cc/2026/Conference/Submission16815/Reviewer_fRvu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16815/Reviewer_fRvu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16815/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935799802, "cdate": 1761935799802, "tmdate": 1762926842556, "mdate": 1762926842556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}