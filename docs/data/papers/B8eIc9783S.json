{"id": "B8eIc9783S", "number": 7969, "cdate": 1758046733587, "mdate": 1763134932158, "content": {"title": "One-Prompt Strikes Back: Sparse Mixture of Experts for Prompt-based Continual Learning", "abstract": "Prompt-based methods have recently gained prominence in Continual Learning (CL) due to their strong performance and memory efficiency. A prevalent strategy in this paradigm assigns a dedicated subset of prompts to each task, which, while effective, incurs substantial computational overhead and causes memory requirements to scale linearly with the number of tasks. Conversely, approaches employing a single shared prompt across tasks offer greater efficiency but often suffer from degraded performance due to knowledge interference. To reconcile this trade-off, we propose **SMoPE**, a novel framework that integrates the benefits of both task-specific and shared prompt strategies. Inspired by recent findings on the relationship between Prefix Tuning and Mixture of Experts (MoE), SMoPE organizes a shared prompt into multiple \"prompt experts\" within a sparse MoE architecture. For each input, only a select subset of relevant experts is activated, effectively mitigating interference. To facilitate expert selection, we introduce a prompt-attention score aggregation mechanism that computes a unified proxy score for each expert, enabling dynamic and sparse activation. Additionally, we propose an adaptive noise mechanism to encourage balanced expert utilization while preserving knowledge from prior tasks. To further enhance expert specialization, we design a prototype-based loss function that leverages prefix keys as implicit memory representations. Extensive experiments across multiple CL benchmarks demonstrate that SMoPE consistently outperforms task-specific prompt methods and achieves performance competitive with state-of-the-art approaches, all while significantly reducing parameter counts and computational costs.", "tldr": "", "keywords": ["Continual Learning", "Prefix Tuning", "Mixture of Experts"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5009a33d52d111ead033a7eef35dc1ea8c37b712.pdf", "supplementary_material": "/attachment/24efe8ccc3cc57f9b218628e0e3f8341f17b888e.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SMoPE, a framework for continual learning that combines the efficiency of shared prompts with the adaptability of task-specific prompts.SMoPE structures a single shared prompt into multiple “prompt experts” within a sparse mixture-of-experts (MoE) architecture, dynamically activating only the most relevant subset to reduce knowledge interference. It introduces a prompt-attention score aggregation mechanism for efficient expert selection, an adaptive noise mechanism to balance expert utilization, and a prototype-based loss that uses prefix keys as implicit task memory. Experiments on standard benchmarks such as ImageNet-R, CIFAR-100, and CUB-200 show that SMoPE outperforms task-specific prompt methods while cutting computational costs by up to 50%. Overall, SMoPE demonstrates that structured sparsity in shared prompts can effectively mitigate forgetting and achieve state-of-the-art continual learning performance with low computational costs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes SMoPE, an approach that integrates a sparse mixture of experts architecture into the prefix tuning framework, featuring a prompt-attention score aggregation mechanism for efficient and dynamic expert selection. \n2. This paper introduces an adaptive noise mechanism to encourage balanced expert utilization without overwriting prior knowledge, and a prototype-based loss that leverages prefix keys as implicit memory. \n3. This paper demonstrates that SMoPE achieves state-of-the-art results on multiple CL benchmarks, while using significantly fewer parameters and reducing computation compared to existing prompt-based methods."}, "weaknesses": {"value": "1. The proposed method integrates multiple components, including a sparse mixture of experts architecture built on prefix tuning, an adaptive noise mechanism, prefix keys as prototypes, and task-adaptive prediction. However, the paper does not include an ablation study to analyze the individual contribution of each component. This omission makes it difficult to assess which parts of the system are most critical to its overall performance.\n2. The rationale behind the unified proxy score that aggregates individual scores in Equation (7) is not clearly explained. Without a theoretical or empirical justification for this formulation, it is unclear whether the aggregation is mathematically or conceptually sound.\n3. The experimental evaluation is not sufficiently comprehensive. The results are primarily reported for 10-split settings, leaving out potentially informative scenarios such as 5-split, 20-split, and 50-split evaluations. Including these would strengthen the empirical validation and demonstrate the method’s robustness across different data partitioning conditions."}, "questions": {"value": "1. Could the authors provide additional ablation studies to clarify the contribution of each component (sparse moe, adaptive noise mechanism, prefix keys as prototypes, and task-adaptive prediction) to the overall performance?\n\n2. Would it be possible to include more experimental results under different data-split settings (such as 5-split, 20-split, and 50-split) to demonstrate the robustness of the proposed approach? Now only 5-split, 10-split, 20-split for ImageNetR are included in the Appendix.\n\n3. Could the authors elaborate on the motivation and theoretical justification for the unified proxy score in Equation (7)? In particular, how was the aggregation of individual scores determined to be appropriate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XTnBJVJftU", "forum": "B8eIc9783S", "replyto": "B8eIc9783S", "signatures": ["ICLR.cc/2026/Conference/Submission7969/Reviewer_L3Zd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7969/Reviewer_L3Zd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448959261, "cdate": 1761448959261, "tmdate": 1762919982096, "mdate": 1762919982096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to Reviewers' Comments"}, "comment": {"value": "Dear Area Chairs and Reviewers,\n\nWe would like to express our sincere gratitude to the reviewers for their time, effort, and constructive feedback in evaluating our submission. Their insightful comments have greatly contributed to enhancing the clarity and completeness of our work. We are particularly encouraged by the positive assessments provided, which highlight the following key aspects of our contribution:\n\n- **Contribution**:\n    - **Clear Problem Formulation**: The paper is recognized for addressing a well-defined and important problem: reconciling the efficiency benefits of a shared prompt with the performance advantages of task-specific prompts in continual learning (Reviewer 3X9D). The proposed methodology was also acknowledged as valuable (Reviewer QtZY).\n    - **Efficacy in Mitigating Catastrophic Forgetting**: Our proposed method, SMoPE, was commended for achieving state-of-the-art performance on multiple continual learning benchmarks, notably using a single shared prompt (Reviewers Uup3, QtZY, L3Zd).\n    - **Computational Efficiency**: A key advantage of SMoPE is its superior efficiency. It demonstrates strong performance with a significant reduction in both parameter count and computational overhead compared to existing prompt-based methods. Specifically, SMoPE reduces trainable parameters and FLOPs by up to 50% relative to task-specific baselines (Reviewers Uup3, L3Zd).\n    - **Innovative Adaptive Noise Mechanism**: The introduction of an adaptive noise mechanism was recognized as a novel contribution that facilitates balanced expert utilization without overwriting prior knowledge (Reviewer L3Zd).\n    - **Novel Use of Prototype-Based Loss**: The concept of using prefix keys as implicit memory through a prototype-based loss was identified as an intriguing and valuable contribution (Reviewers 3X9D, L3Zd).\n    - **Novelty in Sparse Expert Activation**: The core idea of interpreting prefix-tuning within the Mixture of Experts framework and employing sparse activation was seen as both novel and compelling (Reviewers QtZY, 3X9D). This includes a noteworthy prompt-attention score aggregation mechanism for dynamic and efficient expert selection (Reviewer L3Zd).\n\n- **Soundness**: \n    - **Methodological Soundness**: The integration of sparse expert routing, adaptive noise, and a prototype-based loss was found to be an effective approach for balancing expert utilization and mitigating catastrophic forgetting (Reviewer Uup3).\n    - **Rigorous Experimentation**: The experimental validation was commended for its thoroughness, demonstrating robust performance against relevant baselines across diverse datasets and pre-training paradigms (Reviewer 3X9D).\n    - **Insightful Ablation Studies**: The ablation study was acknowledged for providing valuable insights into the contributions of individual components, supported by clear quantitative analysis (Reviewer QtZY).\n    \n- **Presentation**: The manuscript was noted for being well-written and easy to follow (Reviewer 3X9D).\n\nWe deeply appreciate the reviewers' positive feedback and thoughtful suggestions. We will address each reviewer's specific concerns in our detailed responses below and describe the corresponding revisions made based on their valuable recommendations.\n\nBest regards,\n\nAuthors"}}, "id": "5NgQCJfEGA", "forum": "B8eIc9783S", "replyto": "B8eIc9783S", "signatures": ["ICLR.cc/2026/Conference/Submission7969/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7969/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7969/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763139997112, "cdate": 1763139997112, "tmdate": 1763139997112, "mdate": 1763139997112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SMoPE, a framework for continual learning that combines the efficiency of shared prompts with the adaptability of task-specific prompts.SMoPE structures a single shared prompt into multiple “prompt experts” within a sparse mixture-of-experts (MoE) architecture, dynamically activating only the most relevant subset to reduce knowledge interference. It introduces a prompt-attention score aggregation mechanism for efficient expert selection, an adaptive noise mechanism to balance expert utilization, and a prototype-based loss that uses prefix keys as implicit task memory. Experiments on standard benchmarks such as ImageNet-R, CIFAR-100, and CUB-200 show that SMoPE outperforms task-specific prompt methods while cutting computational costs by up to 50%. Overall, SMoPE demonstrates that structured sparsity in shared prompts can effectively mitigate forgetting and achieve state-of-the-art continual learning performance with low computational costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes SMoPE, an approach that integrates a sparse mixture of experts architecture into the prefix tuning framework, featuring a prompt-attention score aggregation mechanism for efficient and dynamic expert selection. \n2. This paper introduces an adaptive noise mechanism to encourage balanced expert utilization without overwriting prior knowledge, and a prototype-based loss that leverages prefix keys as implicit memory. \n3. This paper demonstrates that SMoPE achieves state-of-the-art results on multiple CL benchmarks, while using significantly fewer parameters and reducing computation compared to existing prompt-based methods."}, "weaknesses": {"value": "1. The proposed method integrates multiple components, including a sparse mixture of experts architecture built on prefix tuning, an adaptive noise mechanism, prefix keys as prototypes, and task-adaptive prediction. However, the paper does not include an ablation study to analyze the individual contribution of each component. This omission makes it difficult to assess which parts of the system are most critical to its overall performance.\n2. The rationale behind the unified proxy score that aggregates individual scores in Equation (7) is not clearly explained. Without a theoretical or empirical justification for this formulation, it is unclear whether the aggregation is mathematically or conceptually sound.\n3. The experimental evaluation is not sufficiently comprehensive. The results are primarily reported for 10-split settings, leaving out potentially informative scenarios such as 5-split, 20-split, and 50-split evaluations. Including these would strengthen the empirical validation and demonstrate the method’s robustness across different data partitioning conditions."}, "questions": {"value": "1. Could the authors provide additional ablation studies to clarify the contribution of each component (sparse moe, adaptive noise mechanism, prefix keys as prototypes, and task-adaptive prediction) to the overall performance?\n\n2. Would it be possible to include more experimental results under different data-split settings (such as 5-split, 20-split, and 50-split) to demonstrate the robustness of the proposed approach? Now only 5-split, 10-split, 20-split for ImageNetR are included in the Appendix.\n\n3. Could the authors elaborate on the motivation and theoretical justification for the unified proxy score in Equation (7)? In particular, how was the aggregation of individual scores determined to be appropriate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XTnBJVJftU", "forum": "B8eIc9783S", "replyto": "B8eIc9783S", "signatures": ["ICLR.cc/2026/Conference/Submission7969/Reviewer_L3Zd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7969/Reviewer_L3Zd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448959261, "cdate": 1761448959261, "tmdate": 1763263941283, "mdate": 1763263941283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the trade-off between efficiency and performance in prompt-based continual learning. Current methods either use task-specific prompts, which suffer from scaling issues and inference overhead, or a single shared prompt, which leads to performance degradation due to interference. The authors propose SMOPE, a method that structures a single shared prompt as a sparse Mixture of Experts. It uses a novel prompt-attention score aggregation mechanism to efficiently select a small subset of \"prompt experts\" for each input, mitigating interference. Additionally, an adaptive noise mechanism is introduced to balance expert utilization while preserving prior knowledge, and a prototype-based loss leverages prefix keys as implicit memory to enhance specialization. The work effectively combines ideas from MoE and prefix tuning into the single-prompt CL setting, showing strong results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-   The paper addresses a clear and important problem: reconciling the efficiency benefits of a single shared prompt with the performance advantages of task-specific prompts in continual learning.\n-   The core idea of interpreting the single prefix prompt pool as a Mixture of Experts and applying sparse activation is novel and interesting, building cleverly upon recent interpretations linking prefix tuning and MoE structures.\n-   The experiments are thorough and the presentation is easy to follow, demonstrating strong performance against relevant baselines across multiple datasets and pre-training paradigms."}, "weaknesses": {"value": "-   The prototype loss using prefix keys is an intriguing idea, but its relative contribution compared to the benefits gained from sparse activation and adaptive noise seems modest according to the ablation study. It's unclear how crucial this component is for mitigating forgetting versus simply enabling better expert specialization.\n-   The method introduces several components and associated hyperparameters, including the number of selected experts K, the noise level epsilon, and the loss weights alpha_router and alpha_proto. While some sensitivity analyses are provided, the interplay between these hyperparameters and the overall robustness to their selection could be explored further.\n-   The central claim is that sparse activation mitigates interference. While plausible and supported by the overall performance gains, the paper lacks direct analysis or visualization showing how expert activation patterns evolve across tasks and whether overlap is indeed reduced compared to a dense single-prompt baseline like OVOR."}, "questions": {"value": "-   Based on the ablation study in Table 4, the prototype loss seems to provide the final increment of performance. How critical is this component for preventing forgetting of past tasks, as opposed to simply improving expert routing for the current task? Could you provide analysis comparing forgetting metrics with and without this loss?\n-   Could you provide more details on the selection process for the number of experts K and the loss weighting hyperparameters? How sensitive is the method's performance to variations in K, alpha_router, and alpha_proto, especially concerning their relative balance?\n-   Would it be possible to visualize expert activation frequencies or provide statistics showing how expert usage changes across the sequence of tasks? This could offer more direct evidence for the claims about reduced interference and knowledge reuse."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qNuacl6spS", "forum": "B8eIc9783S", "replyto": "B8eIc9783S", "signatures": ["ICLR.cc/2026/Conference/Submission7969/Reviewer_3X9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7969/Reviewer_3X9D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726171572, "cdate": 1761726171572, "tmdate": 1762919981698, "mdate": 1762919981698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SMoPE, a framework for continual learning with prompt that employs a sparse mixture of prompt experts architecture within the prefix-tuning paradigm. By structuring a single shared prompt as multiple prompt experts and using sparse MoE routing, the method aims to mitigate catastrophic forgetting and knowledge interference. Experiments on continual learning benchmarks, including ImageNet-R, CIFAR-100, and CUB-200, demonstrate that SMoPE achieves superior performance to existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors carefully derive how prefix tuning maps onto a multi-gate MoE, resulting in the novel idea of sparse activation of prompt experts.\n2. The empirical results show that SMoPE can outperform both single-prompt and task-specific prompt baselines. The ablation study offers in-depth insight into which components matter, with clear quantitative impacts."}, "weaknesses": {"value": "1. The literature review, though broad, primarily references earlier or closely related prompt-learning studies but omits several recent and highly relevant advances [1][2]. Incorporating these works into the related work section and including them in the comparative evaluation would strengthen the contextual foundation of the paper.\n2. While the proposed method is valuable, individual contributions such as the adaptive noise mechanism, prototype-based loss[3], and prompt score aggregation[2] extend or increment existing ideas. The presented extensions are incremental in nature, and although their integration is beneficial, the distinctiveness of each mechanism could be more forcefully argued.\n\n3. The results in Table 6 indicate that SMoPE underperforms compared to existing methods as the number of tasks increases. The authors also acknowledge that interference may persist when task numbers grow. Providing additional analyses, such as accuracy curves across incremental steps, would offer a more comprehensive understanding of the method’s behavior and limitations.\n\n[1] Roy A, Moulick R, Verma V K, et al. Convolutional prompting meets language models for continual learning[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024: 23616-23626.\n\n[2]Gao Z, Cen J, Chang X. Consistent prompting for rehearsal-free continual learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 28463-28473.\n\n[3]Zhu F, Zhang X Y, Wang C, et al. Prototype augmentation and self-supervision for incremental learning[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 5871-5880."}, "questions": {"value": "Please refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ryyuNHrpD8", "forum": "B8eIc9783S", "replyto": "B8eIc9783S", "signatures": ["ICLR.cc/2026/Conference/Submission7969/Reviewer_QtZY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7969/Reviewer_QtZY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819606930, "cdate": 1761819606930, "tmdate": 1762919981267, "mdate": 1762919981267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SMoPE, a method that integrates a sparse Mixture of Experts (MoE) architecture into prefix tuning for prompt-based continual learning. The core idea structuring a single shared prompt as multiple “prompt experts” and activating only a sparse subset per input is intuitively appealing and aims to reconcile the trade-off between parameter efficiency and performance degradation due to interference. While the paper reports strong empirical results across several benchmarks, I have some concerns regarding the novelty, methodological rigor, and experimental fairness that limit the contribution’s significance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  SMoPE achieves SOTA results on multiple continual learning benchmarks while using only a single shared prompt.\n2. By activating only K out of Np prompt experts per input, SMoPE reduces both trainable parameters and FLOPs by up to 50% compared to task-specific baselines.\n3. The combination of sparse expert routing, adaptive noise, and prototype-based loss successfully balances expert utilization and preserves past knowledge, reducing catastrophic forgetting."}, "weaknesses": {"value": "1. The connection between prefix tuning and MoE is leveraged as the main motivation. However, SMoPE’s key components, prompt-attention score aggregation, adaptive noise for load balancing, and a prototype loss using prefix keysbare largely adaptations of existing techniques (e.g., sparse routing in MoE, load-balancing noise, and prototype-based memory). The score aggregation mechanism simplifies computation but lacks a compelling theoretical justification beyond sample complexity equivalence (which is shown only under strong identifiability assumptions). The method feels more like an engineering combination than a conceptual advance.\n2. The paper claims superiority over task-specific prompt methods like HiDe-Prompt and NoRGa. However, it explicitly adopts a corrected implementation from Feng et al. (2025). While this correction may be valid, its use without independent verification or ablation (e.g., showing performance with and without the fix) undermines the fairness of the comparison. If the corrected baselines underperform due to implementation conservatism, SMoPE’s gains may be overstated.\n3. SMoPE relies on a fixed-size shared prompt (Np = 25) regardless of the number of tasks (up to 20 in experiments). While results are strong in this regime, the method offers no mechanism to expand capacity as the task sequence grows indefinitely, a fundamental requirement for practical continual learning.\n4. The assumption of \"strong identifiability\" in the supplementary materials is overly idealized and difficult to strictly meet in practical neural network experts."}, "questions": {"value": "Pls see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bNmEFBbZ01", "forum": "B8eIc9783S", "replyto": "B8eIc9783S", "signatures": ["ICLR.cc/2026/Conference/Submission7969/Reviewer_Uup3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7969/Reviewer_Uup3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901524422, "cdate": 1761901524422, "tmdate": 1762919980872, "mdate": 1762919980872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}