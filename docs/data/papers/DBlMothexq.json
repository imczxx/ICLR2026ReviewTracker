{"id": "DBlMothexq", "number": 3417, "cdate": 1757421447311, "mdate": 1759898090772, "content": {"title": "Metropolis-Hastings Discrete Diffusion: Reward-Guided Sampling by Exploring the Clean Data Manifold", "abstract": "Discrete diffusion models have recently emerged as a powerful class of generative models for discrete data, showing effectiveness across diverse scientific domains, such as chemistry and biology. In these fields, the notion of data quality is often well defined, for example drug-likeness in molecules, which makes reward-based guidance at inference time crucial. While reward guidance has been extensively studied for continuous diffusion models, existing approaches are either inapplicable to discrete diffusion due to their reliance on reward gradients, or ineffective because they lack local search. Some methods based on intermediate rewards are applicable to discrete diffusion but tend to underperform, since intermediate rewards are noisy due to the non-smooth nature of reward functions used in scientific domains. To address this, we propose Metropolis-Hastings Discrete Diffusion (MHDD), a method that performs effective test-time reward-guided sampling for discrete diffusion models, enabling local search without relying on intermediate rewards. The key idea is to construct a Markov chain of clean samples with the target distribution as its stationary distribution. We achieve this using the Metropolis–Hastings algorithm. However, directly applying it to discrete diffusion is infeasible due to the intractable acceptance probability. To address this, we design the proposal distribution by sequentially applying the forward and backward processes, which makes the acceptance probability tractable. Experiments on molecule and biological sequence generation with four different reward functions demonstrate that our method consistently outperforms prior approaches that rely on intermediate rewards.", "tldr": "", "keywords": ["Discrete Diffusion", "Generative AI", "Biological Sequence Design"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa608413a3ea1078fbf53deb1ba68e8d2c6a4963.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Metropolis–Hastings Discrete Diffusion (MHDD): a training‑free, test‑time sampler that forms a Markov chain over clean samples $x_0$ to optimize a reward‑weighted distribution. At each iteration, MHDD samples a noise level, draws noisy samples $x_t$ via the forward process, runs an M-step reverse from $x_t$ to propose $x_0’$, and accepts with a reward‑only rule."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- MHDD applies uniformly across masked vs uniform discrete diffusion and discrete‑time vs CTMC variants with low implementation overhead. \n- The paper argues persuasively against noisy intermediate rewards in scientific domains. \n- On three datasets and multiple rewards, MHDD tops baselines in mean reward."}, "weaknesses": {"value": "- Eq. (3) is derived in App. B by conditioning on a fixed $x_t$, while Sec. 4.2 defines a forward–backward proposal whose marginal is a mixture over $t$ and $x_t$ (See question 4).\n- Table 2 reports point estimates only, no seeds, dispersion, or statistical significance. \n- The diagnostics for mixing / convergence are absent. No acceptance rates, burn‑in traces, or sensitivity to $t_{\\mathrm{lo}},t_{\\mathrm{hi}}$. \n- Figure 2 has no legend.\n- Code is not yet provided."}, "questions": {"value": "## 1.\nYour sampler proposes by partially noising then running an M-step reverse (Table 4 shows M=5 and M=10 for MPRA), which seems intrinsically local in Hamming space. If the chain starts in a low‑reward mode separated by low‑density regions under $p_{\\text{pre}}$, it may rarely traverse to a distant high‑reward mode without a very large t (near 1). In other words, MHDD appears to share BoN’s limitation noted in Sec. 4 (lack of trajectory guidance).\n\nCould you argue why MHDD has better mixing properties then BoN? For instance, can you report cross‑mode jump rates, acceptance vs. t, effective sample size, or ablate $t_{\\mathrm{lo}},t_{\\mathrm{hi}}$? \n\n## 2. \nConceptually, one could use intermediate rewards only to design the proposal (e.g., an SMC‑like local search to generate candidates), then apply an MH correction. Have you explored such “guided‑proposal + MH correction” hybrids?\n\n## 3. \nThere’s a complementary line of work on discrete diffusion that diffuses in a continuous space and decodes at the end (learned embedding or simplex relaxations). Recent Malliavin‑calculus–based methods allow conditioning on non‑differentiable terminal‑time rewards (Pidstrigach et al., 2025).\nDo you expect combining a continuous relaxation with Malliavin‑based terminal guidance to work for your scientific rewards? Can you comment on that approach? \n\n## 4.\nIn Algorithm 1, the proposal seen by the chain on clean states is the marginal mixture\n$ \\mathbb E [p_{\\mathrm{rev}}(x^\\prime_0 \\mid x_t)]$ where $p_{\\mathrm{rev}}(\\cdot\\mid x_t)$ is implemented by an M-step reverse process and the expectation is on $t\\sim U[t_{\\text{lo}},t_{\\text{hi}}]$ and $x_t\\sim p_t(\\cdot | x_0)$. By contrast, Sec. 4.2 writes $q(x^\\prime_0\\mid x_0):=p_t(x_t\\mid x_0)p_{\\mathrm{rev}}(x^\\prime_0 \\mid x_t)$ (Eq. (2)) and App. B derives the reward‑only acceptance $A=\\min(1,\\exp(\\Delta r/\\beta))$ (Eq. (3)) by conditioning on a fixed sampled $x_t$ so that the same $x_t$ appears in numerator and denominator. \n\nIn a standard MH update on the marginal state space, however, the reverse term involves a fresh draw $x^\\prime_t \\sim p_t(\\cdot\\mid x^\\prime_0)$, generally different from the $x_t$ used to propose $x^\\prime_0$. In that case, the cancellation with $p_{\\text{pre}}$ does not automatically follow.\n\n- What is the actual state space of the chain? Is the Markov chain defined on the augmented state $(x_0,t,x_t)$? if so, are $t$ and any reverse‑path randomness included so that the same auxiliaries appear in both directions of the Hastings ratio? \n- Under what condition is Eq. (3) valid? Do you assume reversibility of the marginal proposal w.r.t. the pre-trained prior? \n- What bias should we expect if the reverse kernel only approximates the posterior?\n\n## 5.\nApp. C says: “Since S samples are generated, we scale the NFE by S for MHDD” (p. 16), but the exact per‑method accounting is unclear. For MHDD does the number of reverse steps $M$ depend on the sampled $t$? That would make the number of NFEs random. \n\n*Conditioning Diffusions Using Malliavin Calculus*. Jakiw Pidstrigach, Elizabeth L. Baker, Carles Domingo-Enrich, George Deligiannidis, Nikolas Nüsken. (2025). ICML"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PhcQRGe9zP", "forum": "DBlMothexq", "replyto": "DBlMothexq", "signatures": ["ICLR.cc/2026/Conference/Submission3417/Reviewer_7t7b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3417/Reviewer_7t7b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760389465, "cdate": 1761760389465, "tmdate": 1762916714267, "mdate": 1762916714267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper suggests a method to generate reward-oriented samples. Compared with previous baselines, authors suggest to only rely on clean sample reward, getting target distribution by metropolis hasting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea is interesting, since the authors point out that on some scientific cases, process reward model is easy to make mistakes.  \nThe suggested MHDD is easy to implement but effective."}, "weaknesses": {"value": "- Convergence: while metropolis hastings can converge to some distributions in the end, it’s unclear that MHDD will finally converge to the target distribution $p_{\\beta}(x_0)$. The chain may finally converge to a bad distribution or remain confined to limited regions. \n- Efficiency: The claimed efficiency improvement over Best-of-N sampling is not clearly demonstrated under compute-matched conditions. The authors claim that the method is compared under the same model NFE. To me, it should be a curve where x-axis is the computation and y-axis is the performance. Could the authors show this kind of diagram so that readers know the performance comparison under different computation resources?\n- Initialization: The method may heavily depend on the choice of the initial sample $x_0$; poor initialization could lead to slow convergence rate or suboptimal results. In this way, authors should repeat the experiments many times to have the error bar. Authors can also consider (not mandatory) compare the differences when sampling good $x_0$, bad $x_0$, or random $x_0$.\n- Diversity: Since proposals are local noise–denoise perturbations, generated samples might stay close to the starting region, potentially limiting coverage and sample diversity. Could the authors show the divergence measure, on generated samples, compared with baselines?\nUniform diffusion:"}, "questions": {"value": "- Could SGDD experiment on USM? Authors say SGDD is based on uniform diffusion, so only experiment it on SEDD-U, but how about USM?\n- SMC and SVDD seem to fail at all experimental datasets, which is very different from my sense but still interesting. Since those are actually effective methods in many cases, do the authors have any ideas why they can be so worse than BoN?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "odpjeN1eIS", "forum": "DBlMothexq", "replyto": "DBlMothexq", "signatures": ["ICLR.cc/2026/Conference/Submission3417/Reviewer_1zUF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3417/Reviewer_1zUF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779484533, "cdate": 1761779484533, "tmdate": 1762916713935, "mdate": 1762916713935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the challenge of reward-guided sampling for discrete diffusion models in scientific domains (e.g., chemistry and biology), where discrete diffusion models have shown promise in generating discrete data such as molecules and DNA sequences. However, existing reward-guided approaches face critical limitations: gradient-based methods designed for continuous diffusion are inapplicable to discrete spaces, while methods relying on intermediate rewards suffer from noise due to the non-smooth nature of reward functions in scientific fields (e.g., a single character change in a SMILES string can invalidate a molecule and collapse its reward to zero). \n\nTo address these issues, the authors propose Metropolis-Hastings Discrete Diffusion (MHDD), a training-free method that enables effective reward-guided sampling without relying on intermediate rewards. The core idea is to construct a Markov chain of clean samples using the Metropolis-Hastings algorithm, with the target reward-weighted distribution as its stationary distribution. A key technical innovation is the design of a proposal distribution via sequential forward (corrupting clean samples) and backward (denoising noisy samples) processes, which makes the otherwise intractable acceptance probability in the Metropolis-Hastings algorithm tractable. \n\nExperiments on molecule generation (QM9, ZINC250K datasets) with three reward functions (QED for drug-likeness, ring count, synthetic accessibility SA) and biological sequence generation (MPRA dataset) with HepG2 enhancer activity reward demonstrate that MHDD consistently outperforms prior methods (e.g., Best-of-N, SMC, SVDD, SGDD) across all discrete diffusion frameworks (masked diffusion MDM, uniform state diffusion USM, score-based CTMC diffusion SEDD-M/SEDD-U)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The major contribution is that this paper breaks the reliance of reward-guided sampling in discrete diffusion on intermediate rewards. It combines the Metropolis-Hastings algorithm with the forward-backward processes of discrete diffusion to construct a Markov chain of clean samples. The design of the forward-backward proposal distribution cleverly solves the intractable acceptance probability problem when applying Metropolis-Hastings to diffusion models, presenting novelty in terms of methodology."}, "weaknesses": {"value": "1.\tLow Burn-In Efficiency and Unquantified Time Overhead: Requires discarding the first half of the Markov chain (burn-in period) to ensure convergence. For example, in some datasets, when the total number of iterations K is 6390, 3195 iterations are discarded during burn-in. However, the paper does not quantify the additional time overhead caused by the burn-in period and K iterations. Time is a critical indicator in practical applications such as rapid drug screening; the lack of this information makes it impossible to evaluate the practical efficiency of the method and compare its time competitiveness with methods like Best-of-N (BoN) and SMC.\n2.\tFixed Proposal Distribution Hyperparameters and No Time Sensitivity Analysis: Relies on manually set hyperparameters (e.g., tlo=0.5, thi=0.8, M=5/10) for the proposal distribution. There is no analysis of how these hyperparameters affect sampling time—for instance, whether increasing M significantly prolongs the time per iteration, or if adjusting tlo/thi can shorten time while maintaining reward performance. Additionally, there is no adaptive hyperparameter adjustment strategy, resulting in insufficient flexibility and a lack of exploration into the trade-off between time and performance.\n3.\tLack of Time Overhead Comparison with Other Methods: Although the total NFE is controlled to ensure fairness in experiments, NFE is not entirely equivalent to actual time overhead (the computational complexity of single-step NFE may vary across methods). No actual time consumption comparison between MHDD and methods like BoN, SMC, and SVDD is provided, making it impossible to fully judge MHDD's comprehensive advantages in the \"performance-time\" dimension."}, "questions": {"value": "1.\tThe paper notes that the burn-in period requires discarding the first half of the Markov chain (e.g., 3195 iterations discarded when K=6390 for certain datasets) to ensure convergence, but it does not quantify the additional time overhead caused by the burn-in period and K iterations. Given that time is critical for practical applications like rapid drug screening, can you supplement experimental data to quantify the time consumed by the burn-in period and K iterations? Additionally, can you compare MHDD's total time overhead with that of methods like Best-of-N (BoN) and SMC in the same experimental tasks to clarify its time competitiveness?\n2.\tThe proposal distribution of MHDD relies on manually set hyperparameters such as tlo=0.5, thi=0.8, and M=5/10, yet there is no analysis of how these hyperparameters impact sampling time. For example, does increasing M significantly extend the time per iteration? Can adjusting tlo or thi reduce sampling time while maintaining reward performance? Moreover, since there is no adaptive hyperparameter adjustment strategy currently, do you have plans to explore such strategies to balance time overhead and performance flexibility?\n3.\tWhile the paper controls the total Number of Function Evaluations (NFE) to ensure experimental fairness, NFE does not fully equate to actual time overhead—single-step NFE may differ in computational complexity across methods (e.g., MHDD's forward-backward process vs. SMC's particle resampling). Can you provide specific data on the actual time consumption of MHDD versus methods like BoN, SMC, and SVDD under the same total NFE? This would help fully assess MHDD's comprehensive advantages in the \"performance-time\" dimension."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zez2ol9sDQ", "forum": "DBlMothexq", "replyto": "DBlMothexq", "signatures": ["ICLR.cc/2026/Conference/Submission3417/Reviewer_gv3v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3417/Reviewer_gv3v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855856590, "cdate": 1761855856590, "tmdate": 1762916713592, "mdate": 1762916713592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MHDD, a reward-guided sampler for discrete diffusion models that avoids noisy intermediate rewards by doing Metropolis-Hastings on the clean data manifold. The key idea is a forward-backward, starting from a clean sample, applying the forward corruption to a random time, then running a short reverse denoising to propose a new clean sample. This makes MH acceptance ratio tractable and (under their construction) depend on reward differences. Experiments on molecules and DNA sequence show improvements over baselines. The method is claimed to be generally applicable to masked and uniform discrete diffusion and to deliver stronger guidance when process rewards are unreliable."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. By constructing proposals with forward–reverse diffusion, the acceptance rule eliminates intractable terms and depends on rewards. This motivation matches domain facts. \n\n2. Model-agnostic across discrete diffusion families. makes the work broader than SGDD, which targets uniform settings. \n\n3. Simple acceptance rule and reusable chain can be computationally light useful for batch design."}, "weaknesses": {"value": "1. The acceptance simplification assumes proposal factors that make the MH ratio tractable. In practice distribution is approximated by a learned reverse model. This introduces modeling bias that can break detailed balance and stationarity guarantees. Analyses quantifying bias under approximate reverse kernels (e.g., pseudo-marginal MH or noisy MH analyses) and diagnostics of reversibility violations can make claims more grounded. \n\n2. Where did p_{pre} go in acceptance? The paper defines the target but ends with an acceptance depending only on r. It's questionable whether the proposal construction implicitly incorporates the prior so it cancels, and under which conditions. Explicit derivation showing cancellation with learned reverse dynamics, and an ablation demonstrating that removing the forward–backward construction changes the implied target (e.g., acceptance starts to need p_{pre} terms) are needed. \n\n3. Mixing/burn-in is not quantified, and no autocorrelation, effective sample size, acceptance-rate curves, or sensitivity studies are provided. For MH chains on combinatorial spaces, these are essential. \n\n4. The paper scales NFE by the number of drawn chain samples, but the practical cost per improved sample vs. baselines isn’t explored. Computation studies are not included, which can be important to show practical efficacy.\n\n5. Some baselines are missing. Planning-augmented discrete diffusion and path-planning for masked diffusion methods are not compared. These explicitly do local search in discrete diffusion and represent trajectory-level guidance similar to MHDD’s local moves. \n\n6. Comparison with training-based alignment for discrete diffusion method is missing. They are competitor paradigms for reward-guided generation, and adding side-by-side budget-matched comparisons can better justify the advantages. \n\n7. Metrics commonly used other than reward, such as diversity, are missing, which are important to ensure MH steps don’t reduce diversity or exploit reward quirks. Current metrics are insufficient alone.\n\n8. Ablations are very thin. No ablation studies are provided to show the contribution of key component. The method depends on designed sampling, short reverse length, chain length, etc., which should be studied."}, "questions": {"value": "1. How sensitive is performance to narrower vs. wider time windows? \n\n2. Do chains collapse to narrow modes?\n\n3. Under equal wall-clock, how does MHDD compare to baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A8uJsbJuKs", "forum": "DBlMothexq", "replyto": "DBlMothexq", "signatures": ["ICLR.cc/2026/Conference/Submission3417/Reviewer_a6b4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3417/Reviewer_a6b4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3417/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974781753, "cdate": 1761974781753, "tmdate": 1762916713205, "mdate": 1762916713205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}