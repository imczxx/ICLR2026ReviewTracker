{"id": "XVx4CYKmxQ", "number": 4587, "cdate": 1757715784774, "mdate": 1759898024587, "content": {"title": "INTERPRETABLE COMPACT CATEGORICAL FEATURES ENCODING FOR SUPERVISED LEARNING", "abstract": "In supervised learning, encoding techniques for continuous features are well studied. Despite the fact that categorical features often carry significant, if not a dominant portion of, feature information in a supervised learning problem the prevailing encoding techniques today are still one-hot encoding and target encoding in real world applications after decades. One hot encoding is known for its curse of dimensionality issue, especially when categorical features are of high cardinality and/or sparse. Such problem not only increases the problem size but may also introduce instability for numerical solvers. Target encoding is often used for categorical data with high cardinality. It is considered an ad-hoc approach that works well in many but not all cases. There is continued effort to study efficient methods to reduce dimension and retain interpretability. However, so far new algorithms developed by the research community are either too complicated or do not exhibit better performance than existing approaches. Our goal is to introduce an algorithm to address such practical issue. In this paper, we propose a polynomial  algorithm called Interpretable Compact Categorical Feature Encoding for Supervised Learning (ICFESL) to compress categorical features by collapsing categorical feature levels. Our encoding ensures no information loss for the regression problem and minimum information loss for the classification case. We prove that applying K-means clustering to the OLS or MLE coefficients yields optimal clustering schemas for the dimension reduction problem. We test our algorithm on larger real world datasets in both regression and classification problems and the results show that its performance is comparable or superior to existing methods.", "tldr": "Interpretable categorical feature encoding", "keywords": ["Supervised Learning", "Categorical Feature Encoding", "Compression", "Embedding"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/59c29089416d7e513eb3602e008396d488452c0b.pdf", "supplementary_material": "/attachment/414497af132fb63c6a6ef6ceb9528b012800a452.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ICFESL, a polynomial-time algorithm for compressing high-cardinality categorical features by clustering their levels based on the coefficients from a preliminary OLS or Logit model. The core idea is to use K-means on these coefficients, weighted by class frequencies or the derivative of the logistic function, to find an optimal grouping that minimizes information loss. The authors provide theoretical guarantees for this approach under an orthogonality assumption of one-hot encoded vectors. The method is evaluated on multiple datasets for both regression and classification tasks, demonstrating performance comparable or superior to one-hot and target encoding while significantly reducing dimensionality and maintaining interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Interpretability**: The method directly compresses the original feature levels, preserving the meaning of the features, which is a significant advantage over black-box embedding methods.\n\n**Theoretical Grounding**: Provides proofs of optimality under specified conditions, which is rare for practical encoding schemes.\n\n**Practicality and Scalability**: The algorithm has polynomial time complexity and is demonstrated on real-world, high-cardinality datasets, showing its potential for industrial use."}, "weaknesses": {"value": "**Reliance on Strong Assumptions**: The optimality guarantees depend on the orthogonality of OHE vectors, which is an idealized condition. The practical workaround (Hamming clustering) is mentioned but not deeply evaluated.\n\n**Hyper-parameter Sensitivity**: The performance depends on p-value thresholds and the choice of stopping criterion, which requires tuning and may not be fully automated.\n\n**Limited Baseline Comparison**: While compared to OHE and target encoding, a comparison with other advanced methods like feature hashing or SOTA encoding methods would strengthen the empirical validation."}, "questions": {"value": "1. How sensitive is the algorithm's performance to the violation of the orthogonality assumption?\n\n2. Could you provide an ablation study showing the performance gain/loss with and without the Hamming distance pre-processing step?\n\n3. The p-value threshold is a key hyper-parameter. Did you explore automated ways to set it, perhaps based on the desired level of compression, rather than relying on a decision plot?\n\n4. Have you considered applying this method to tree-based models like LightGBM or CatBoost directly, which have their own built-in mechanisms for handling categorical features? How would ICFESL complement or compete with these native methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qwiyNGWjS8", "forum": "XVx4CYKmxQ", "replyto": "XVx4CYKmxQ", "signatures": ["ICLR.cc/2026/Conference/Submission4587/Reviewer_BN3p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4587/Reviewer_BN3p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4587/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761096479844, "cdate": 1761096479844, "tmdate": 1762917457229, "mdate": 1762917457229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### Summary\n\nThe paper addresses encoding high-cardinality categorical features. One-hot encoding explodes dimensionality, target encoding is ad-hoc. The core  Fit OLS on one-hot encoded data, then cluster resulting coefficient values using K-means.\n\n### Main contributions:\n\nFor regression: proves clustering is lossless under orthogonality assumption across all feature levels. The weighted average is the exact OLS solution for the collapsed problem.\nFor classification: no exact solution - minimizes prediction squared error instead of likelihood. Admits \"measurable information loss.\"\nAlgorithm is polynomial O(|P|mn²) with automatic K selection via inertia stopping criterion. Requires tuning p-value threshold manually via decision plots.\n\nThe authors test on 5 datasets with OLS/Logit, XGBoost, TabNet. Results are okay. Sometimes their method performs better than target encoding, often it is comparable, and occasionally worse. They show that the dimension of the problem reduces but interpretability of actual clusters are never validated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clear and easy to understand. The theoretical claims and proves seem valid.\n- The algorithm presented is polynomial time O(|P|mn²) is reasonable. Not trying to solve some NP-hard problem with exponential search like prior work (GRASP). However, the work doesn't directly compare with GRASP.\n- Experiments across model types & datasets are good to have."}, "weaknesses": {"value": "- My biggest issue with the paper is that there is no validation of interpretability despite it being the core claim. The authors assume that categorical clustering preserves interpretability. While I agree with it on surface, I don't think it is always true. The clustering algorithm can make meaningless uninterpretable groups across feature levels which have a similar coefficient. I would have liked to see analysis on what kind of clusters does their algorithm produce or does the algorithm improve clustering or interpretability compared to other sensible baselines.\n   - What's needed: Show actual clusters, compare to natural hierarchies, or measure cluster coherence. Report interpretability metrics across baselines.\n    - Embedding features in vector space is a strong baseline and there are papers that try to interpret those features. A comparison to such methods would be nice as well. \n-  More directly comparable baselines are needed to assess the strengths of the algorithm. Some suggestions:\n    - CatBoost with native categorical handling - directly addresses the same problem, widely used\n    - Entity embeddings - standard deep learning approach for categoricals\n    - GRASP (Carrizosa et al., 2021) - most directly comparable prior work\n  The excuse that GRASP solves a \"different problem\" (single feature vs. all features) is weak since ICFESL also processes features independently (Algorithm 1, lines 280-294). The real issue is GRASP is expensive, but a limited comparison on smaller features would still be valuable.\nWithout these comparisons, we can't assess if coefficient clustering is competitive with other principled approaches."}, "questions": {"value": "1. Can you provide concrete examples of clusters from your experiments? For instance, for the US AQI dataset where \"Local Site Name\" goes from 1,033 levels to 103 clusters, what do clusters 1, 5, and 10 actually contain? Do the grouped monitoring sites share geographic proximity, pollution sources, or other interpretable attributes?\n2. What exactly happens to categorical levels filtered out by the p-value threshold? Looking at Table 2, Fraud UW goes from 59 OHE features to 34 ICFESL features. Are the remaining 25 dropped entirely, kept as individual levels, or something else?\n3. Can you quantify what happens when orthogonality is violated? Even a simple simulation would help: generate correlated categorical features, apply your method, measure the actual vs. theoretical information loss. How robust is the \"lossless\" claim in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZH2RSHpe7s", "forum": "XVx4CYKmxQ", "replyto": "XVx4CYKmxQ", "signatures": ["ICLR.cc/2026/Conference/Submission4587/Reviewer_BXkH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4587/Reviewer_BXkH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4587/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847678156, "cdate": 1761847678156, "tmdate": 1762917456894, "mdate": 1762917456894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method named ICFESL (Interpretable Compact Categorical Feature Encoding for Supervised Learning), a polynomial-time algorithm for encoding categorical features by clustering feature levels based on OLS/MLE coefficients. The approach addresses the dimensionality curse of one-hot encoding (OHE) and the ad-hoc nature of target encoding. The authors prove that applying K-means clustering to regression coefficients (weighted by observation counts) or classification coefficients (weighted by logistic derivatives) yields optimal clustering schemas. The method is evaluated on five real-world datasets using OLS, XGBoost, and TabNet models, demonstrating comparable or superior performance to existing encoding methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses real problem: High-cardinality categorical encoding is genuinely challenging in practice\n- Polynomial complexity: More tractable than GRASP-based approaches (exponential)\n- Automatic cluster number selection: Unlike methods requiring pre-specified K, uses stopping criteria to determine cluster counts\n- Interpretability focus: Unlike many neural embedding approaches, the proposed one maintains coefficient interpretability"}, "weaknesses": {"value": "- Limited novelty: The core idea of clustering coefficients is relatively straightforward; applying K-means to regression coefficients is not a significant algorithmic innovation.\n- Incremental improvement: Results show ICFESL is comparable to target encoding in most cases, with only marginal improvements in select scenarios.\n- Theoretical-practical gap: The orthogonality assumption is acknowledged to \"rarely hold by default\" yet no systematic study of when violations matter\n-  Weak baselines: No comparison with learned embeddings (entity embeddings, CatBoost's ordered target encoding variants). No comparison with recent categorical encoding methods from the literature\nHamming distance clustering is presented as a baseline but is actually a preprocessing step for ICFESL\n- Limited experimental scope: only 5 datasets (3 classification, 2 regression)\n- No datasets with truly high cardinality (US AQI has 1033 unique values but after filtering may be much smaller)\n- Missing analysis: No runtime comparisons with other methods. No study of performance scalability with cardinality. No ablation study on the impact of Hamming distance preprocessing"}, "questions": {"value": "- Can you provide empirical analysis of how often the assumption X^T_{i,j}X_{s,t} = 0 holds in your datasets before and after Hamming clustering?\n- Why does ICFESL underperform target encoding on XGBoost for most datasets? Can you explain this pattern?\n- Is Hamming clustering mandatory or optional?\n- What is the \"Min Obs\" parameter and how do you set it?\n- Can you show results on datasets with higher cardinality?  let's say about 100k levels"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GeMbfKWl3i", "forum": "XVx4CYKmxQ", "replyto": "XVx4CYKmxQ", "signatures": ["ICLR.cc/2026/Conference/Submission4587/Reviewer_xbgW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4587/Reviewer_xbgW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4587/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762237952052, "cdate": 1762237952052, "tmdate": 1762917456331, "mdate": 1762917456331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}