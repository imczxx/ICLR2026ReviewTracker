{"id": "nCE7Sli461", "number": 4669, "cdate": 1757740572024, "mdate": 1763512477308, "content": {"title": "Why High-rank Neural Networks Generalize?: An Algebraic Framework with RKHSs", "abstract": "We derive a new Rademacher complexity bound for deep neural networks using Koopman operators, group representations, and reproducing kernel Hilbert spaces (RKHSs).\nThe proposed bound describes why the models with high-rank weight matrices generalize well.\nAlthough there are existing bounds that attempt to describe this phenomenon, these existing bounds can be applied to limited types of models.\nWe introduce an algebraic representation of neural networks and a kernel function to construct an RKHS to derive a bound\nfor a wider range of realistic models.\nThis work paves the way for the Koopman-based theory for Rademacher complexity bounds to be valid for more practical situations.", "tldr": "We derive a new Rademacher complexity bound that describes why the models with high-rank weight matrices generalize well, which is valid for a wide range of models.", "keywords": ["Generalization bound", "Deep neural network", "Koopman operator", "Reproducing kernel Hilbert space"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d06b7aea448d63e604bf5d54dcd76efee640f7bc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a Rademacher complexity bound for deep NNs derived using Koopman operators, group representations and RKHSs. Distinct from other bounds, the bound includes the determinant of the weight matrices in its denominator, and is therefore tighter than comparable bounds in the case of high-rank weight-matrices."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper builds on previous work on Rademacher complexity bounds via Koopman operators, removing restrictions which limited the applicability of the previously derived bounds.\n2. The resulting bounds scale much better than comparable bounds when the weight matrices are high-rank.\n3. The experimental results appear to back up the claims make in the paper."}, "weaknesses": {"value": "My main problem with this paper is readability. While I understand that the material is intrinsically difficult, it would nevertheless be helpful if more time was spent systematically introducing the various notations/definitions etc that are used. In particular, I found section 5 to be quite heavy going, and section 5.3 in particular was extremely difficult to parse. Perhaps the authors could include a table (even if it is located in the appendices?) presenting the various notations/definitions (and their inter-connections) in a more systematic way so that the reader isn't left constantly scanning back and forward through paragraphs of dense definitions/notations interleaved with prose to remind themselves what something means when it is eventually used?\n\nMore generally it seems to me that this is a somewhat incremental improvement on the previous work in Hashimoto et al, but I am open to argument in this regard as this is open to interpretation.\n\nSpecific points and questions:\n\n- Lemma 2.5: it seems to me that this result effectively rules out applying the method to ReLU networks as it blows up in the relevant limit. Can you see any way around this?\n- Example 3.3: is the limit in line 205(ish) due to $p_{c,x}(y)$ approaching the Dirac-delta as $c \\to \\infty$?\n- Theorem 4.3: is the supremum here actually bounded? The determinant cannot be zero for the invertible case, but it can be arbitrarily small, so the supremum can presumably be arbitrarily large. Also I assume $A_l$ here is the Koopman operator?\n- Theorem 5.1: it is confusing to use $\\alpha (f_l)$ here before introducing it (in remark 5.3). Also what role does this factor play? Could it just be replaced by $1$ for clarity in the theorem?\n- Section 5.3: the first paragraph here is far to mathematically dense and needs to be expanded for clarity. Also it strikes me that many of these are essentially geometric constructs, so perhaps a diagram could help?"}, "questions": {"value": "See weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gxgKGFhdG5", "forum": "nCE7Sli461", "replyto": "nCE7Sli461", "signatures": ["ICLR.cc/2026/Conference/Submission4669/Reviewer_X6tr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4669/Reviewer_X6tr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621423945, "cdate": 1761621423945, "tmdate": 1762917504860, "mdate": 1762917504860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To all the reviewers"}, "comment": {"value": "Thank you bery much for your constructive comments. We carefully addressed your comments and modified the manuscript. The modified parts are colored in red. \n\nWe sincerely apologize that there was an error in Example 3.3, as Reviewer y36o pointed out.\nWe corrected this error and modified the manuscript.\nWe appreciate Reviewer y36o for pointing out the error, which improves the quality of the paper.\nWe emphasize that these modifications do not affect the main contribution and main message of this paper.\nPlease see the answer for Reviewer y36o for more details."}}, "id": "zZ8AmzyNGX", "forum": "nCE7Sli461", "replyto": "nCE7Sli461", "signatures": ["ICLR.cc/2026/Conference/Submission4669/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4669/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4669/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763510585394, "cdate": 1763510585394, "tmdate": 1763512594730, "mdate": 1763512594730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new Koopman-operator-based Rademacher complexity bound to explain why neural networks with high-rank weight matrices can generalize well—an observation not captured by existing norm-based or compression-based generalization theories.\n\nThe key idea is to construct a reproducing kernel Hilbert space (RKHS) on the parameter space of neural networks using group representations and Koopman operators, enabling a generalization bound applicable to realistic models with bounded input domains and nonsmooth activations (e.g., tanh, sigmoid, Leaky ReLU)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Addresses a known theoretical gap:\nMost generalization bounds (norm-, compression-, or PAC-Bayes–based) explain why low-rank networks generalize, but fail for empirically observed high-rank networks. This work provides a principled explanation via Koopman-based operator theory.\n\nMathematical generality:\nThe approach unifies algebraic, operator-theoretic, and kernel methods, supporting both bounded and nonsmooth activations—something prior Koopman-based bounds (Hashimoto et al., 2024) could not handle.\n\nElegant use of RKHS:\nBy defining an RKHS on the parameter space, the authors convert the non-linear deep network mapping into a linear setting where standard Rademacher complexity tools apply — a conceptually clean and general construction.\n\nPractical implications validated by experiments:\nThe three plots in Figure 1 show consistent improvement in generalization across synthetic regression, MNIST dense nets, and LeNet CNNs when regularization terms derived from the bound are added.\n\nConnections to high-level mathematical structures:\nThe use of group representations (affine, Heisenberg) and operator algebras (Schur’s Lemma, von Neumann double commutant theorem) to describe neural networks is novel and could inspire deeper theoretical analyses of neural architectures."}, "weaknesses": {"value": "Dependence on bounded activation assumptions:\nThe proofs assume Koopman operator boundedness (Assumption 2.2), which fails for ReLU (derivative = 0 on half-line).\nThe authors acknowledge this limitation (Sec. 7) and suggest exploring weighted Koopman operators, but current results exclude the most popular activations.\n\n\nInterpretability of constants:\nThe bounds depend on abstract constants that are not easy to estimate in practice. Consequently, the bound is more qualitative than quantitative.\n\nComparative evaluation:\nWhile it compares against the 2024 Koopman bound, other modern generalization frameworks (e.g., PAC-Bayes, spectral norms, neural tangent kernel analyses) are not benchmarked.\n\nNo discussion of sample complexity scaling:\nThe 1/sqrt(S) dependence mirrors classical Rademacher bounds, but no new scaling exponents are derived."}, "questions": {"value": "1) Can your framework extend to ReLU networks via non-smooth or distributional Koopman operators?\n\n2) How sensitive is the determinant-based bound to weight scaling (e.g., if |det (W)| >> 1 due to scaling, not diversity)?\n\n3) In practical networks, rank deficiency often correlates with overfitting. Does your theory predict the same phenomenon in the inverse direction?\n\n4) How does the bound behave for randomly initialized networks versus trained ones?\n\n5) Could the RKHS kernel you define (Eq. 3.2) be computed empirically for small models to verify the bound numerically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TIPAnDRBzb", "forum": "nCE7Sli461", "replyto": "nCE7Sli461", "signatures": ["ICLR.cc/2026/Conference/Submission4669/Reviewer_DC2X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4669/Reviewer_DC2X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954578914, "cdate": 1761954578914, "tmdate": 1762917504552, "mdate": 1762917504552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper develops a new Rademacher complexity bound for deep neural networks by combining Koopman operator theory, group representations, and RKHS methods. Unlike traditional norm- or rank-based bounds, the proposed framework explains why high-rank networks can still generalize well. The authors construct a kernel on the parameter space and prove an isometric correspondence between this Koopman-RKHS and the function space of the network. Overall, the work provides a mathematically elegant perspective on neural network generalization, though some aspects of proofs raise some concerns about their validity."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper offers a novel and mathematically rigorous framework that unifies Koopman operator theory, RKHS analysis, and group representations for studying neural network generalization. It extends existing Koopman-based approaches to handle nonsmooth activations and bounded domains, broadening their practical relevance."}, "weaknesses": {"value": "The main weakness is a couple of issues that I found in proofs."}, "questions": {"value": "Example 3.3 introduces $p_{c,x}$ which is  Gaussian centered at $x$ whose variance goes to zero, as $c$ goes to infinity. \n\nThis function satisfies \n$||p_{c,x}||_{1}=1$ \n\nbut not \n\n$$||p_{c,x}||_{2}=1$$ \n\nas claimed in paper. \nThe latter claim, i.e. \n$$||p_{c,x}||_{2}=1$$ \nis then used in the proof of Theorem 4.3 where the dependence on $c$ vanishes (which I doubt is correct)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bTwbgrfiPl", "forum": "nCE7Sli461", "replyto": "nCE7Sli461", "signatures": ["ICLR.cc/2026/Conference/Submission4669/Reviewer_y36o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4669/Reviewer_y36o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762087101817, "cdate": 1762087101817, "tmdate": 1762917504303, "mdate": 1762917504303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new bound on the Rademacher complexity of deep neural networks, with the goal of better accounting for the observed good generalization of neural networks with high-rank weight matrices.  The paper improves upon earlier work by Hashimoto et al. 2024 that introduced an approach based on Koopman operators and RKHSs to represent and reason about neural networks.  The main improvement is the analysis of models in a different RKHS than in prior work, thus accounting for a larger and more practical model class, for example including non-smooth activation functions.  A small-scale experiment on MNIST digit classification shows that a regularizer derived from the bound improves on one derived from the earlier bound."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Expanding the reach of existing complexity bounds to a more realistic model class is a very important contribution.\n\n- The style of Koopman-based bounds appears to differ from much of the other existing work studying generalization in neural networks (besides the cited Hashimoto et al. 2024), and so may be complementary to other results.\n\n- The inclusion of empirical evidence, while limited, is helpful."}, "weaknesses": {"value": "- The main weakness is with the presentation.  I fear that in its current form, the paper will be accessible only to an very narrow segment of the ICLR audience.  While I am admittedly not an expert in the recent theoretical ML literature, I believe the paper should be able to make its results and implications much clearer to most of the ICLR community.  By the end of the paper, it is not clear to me in what way exactly the paper improves over Hashimoto et al. 2024, and how this improvement was achieved.  It is possible that parts of my summary above are incorrect as a result (please let me know if so!).  I would have expected a clear statement of the Hashimoto et al. 2024 bound and the new bound, both mathematically and intuitively, and ideally also other existing bounds for comparison.  I also would have expected a more intuitive \"walk-through\" of the key steps.  There are two paragraphs devoted to introducing RKHSs -- a broadly familiar concept, I believe -- while there are many other more obscure concepts and notation that are relied on but not explained.  While they cannot all be defined for the novice reader, some intuitive explanations of the ideas would go a long way.\n\n- A second possible and more minor weakness (which may be not a weakness at all, and rather due to my own misunderstanding -- again, please let me know if so!) is with the empirical results.  I am not able to understand why the bound was not compared directly to the prior bound (e.g. in Fig. 1(a)) but only as a form of regularization (in Fig. 1(b, c)).  I also wonder why there are no empirical results with more practical, larger datasets and models (say, ImageNet).  It is reasonable for a theoretical paper to limit itself to small experimental settings, if needed.  But it should be stated what are the limitations that prevent scaling up the experiments."}, "questions": {"value": "My remaining questions are mainly about low-level details:\n\n- Regarding these sentences:  \"... phenomena in which models with high-rank weight matrices generalize well have been empirically observed (Goldblum et al., 2020).  Since the norm-based and compression-based bounds focus only on the low-rank and nearly low-rank cases, they cannot describe these phenomena.\"  The second sentence need not be true if the high-rank matrices in Goldblum et al. 2020 are in fact nearly low-rank.  I guess that is not the case, but could you please confirm?\n\n- There is some imprecise language that would be good to clean up.  Examples:\n  - \"It describes how the model can fit unseen data\" -- not clear what \"It\" refers to.\n  - \"This bound is described by the ratio of the norm to the determinant of the weight matrix\" -- since there are multiple layers and multiple weight matrices, this can't be precise.\n  - \"the generalization bound is described by the Rademacher complexity\" -- unclear what it means for the Rademacher complexity to \"describe\" a bound.\n  - \"Our framework fills the gap between the Koopman-based analysis of generalization bounds and practical situations\" -- I think this is a bit stronger than intended (i.e. the paper does not fully fill the gap).\n\n- Typo:  \"The Hilbert space H to which the modes belong...\" --> \"... the models belong\"\n\n- In Fig. 1(a), I have a hard time seeing the color gradient.  Perhaps a different gradient would be visually clearer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oroF7d2LUA", "forum": "nCE7Sli461", "replyto": "nCE7Sli461", "signatures": ["ICLR.cc/2026/Conference/Submission4669/Reviewer_JANi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4669/Reviewer_JANi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762315935804, "cdate": 1762315935804, "tmdate": 1762917503917, "mdate": 1762917503917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}