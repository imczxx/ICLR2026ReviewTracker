{"id": "cMpOvMuyYa", "number": 18077, "cdate": 1758283595661, "mdate": 1759897134863, "content": {"title": "Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning", "abstract": "Long-horizon goal-conditioned tasks pose fundamental challenges for reinforcement learning (RL), particularly when goals are distant and rewards are sparse. While hierarchical and graph-based methods offer partial solutions, their reliance on conventional hindsight relabeling often fails to correct subgoal infeasibility, leading to inefficient high-level planning. To address this, we propose Strict Subgoal Execution (SSE), a graph-based hierarchical RL framework that integrates Frontier Experience Replay (FER) to separate unreachable from admissible subgoals and streamline high-level decision making. FER delineates the reachability frontier using failure and partial-success transitions, which identifies unreliable subgoals, increases subgoal reliability, and reduces unnecessary high-level decisions. Additionally, SSE employs a decoupled exploration policy to cover underexplored regions of the goal space and a path refinement that adjusts edge costs using observed low-level failures. Experimental results across diverse long-horizon benchmarks show that SSE consistently outperforms existing goal-conditioned and hierarchical RL methods in both efficiency and success rate.", "tldr": "We propose a new hierarchical RL framework (SSE) that, instead of relabeling subgoal failures as successes, treats them as terminal failures with zero reward, dramatically improving the high-level planner's reliability and efficiency.", "keywords": ["goal-conditioned reinforcement learning", "hierarchical reinforcement learning", "sparse reward", "long-horizon tasks", "graph-based policy learning", "subgoal planning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce7ede8bb230c070c4fd013b6a49ba88b86bb477.pdf", "supplementary_material": "/attachment/a08f8016b9502ae8e856a6036ffe76d9f1f24399.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed a novel method for hierarchical reinforcement learning, which leverages a frontier-based and failure-aware meta-policy replay buffer to ensure that goals that are reachable and at the frontier of experience are sampled. The method keeps track of this information with a graph-based encoding of the nodes in the environment. To ensure proper exploration, they leverage an exploration policy that is aware of all grid cells in 2D/3D space, and samples goals (at least partially) from these cells. \n\nThe main contribution are the novel replay buffer, and the method for having the graph be aware of what paths will fail, and having exploration be decoupled for goal space coverage."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- A key strength of their method is that it learns transitions between goals, enabling more efficient planning (as implied by Figure 1). It appears that they are effectively learning a high-level policy.  \n- The method achieves strong training performance compared to baseline approaches.  \n- The evaluations demonstrate that each component of the algorithm plays an essential role in achieving good performance in the environment.  \n- The experiments clearly illustrate the benefits of the proposed components (e.g., Figures 2 and 3).  \n- The overall presentation is clear and well-explained."}, "weaknesses": {"value": "- \"We assume the existence of a mapping φ such that φ(s) ∈ G, allowing the agent to infer goal progress from the current state.\"\n  This seems like a strong assumption. Is this mapping learned, or predefined? Clarifying this would help understand how generalizable the approach is.\n\n- The algorithm appears to involve substantial hand-crafting, which may limit its applicability to more complex or continuous environments. For instance, sampling from a grid-based estimator may not scale beyond grid-world settings.  \n  Similarly, assuming that subgoal reachability can be determined by $||\\phi\\left(s_{t^{\\prime}}\\right)-\\tilde{g}_t||<\\lambda$ is a strong and potentially unrealistic assumption.  Using this grid discretization for failure awareness also seems restrictive."}, "questions": {"value": "- how is $\\lambda$ set? I'm surprised there's no ablation on this.\n- the Strict Subgoal Execution (SSE) framework updates the high-level policy with positive returns only when the low level successfully reaches the assigned subgoal. This seems like the same high-level ideas as [1]. Could you contrast to this work? This seems like a graph-based version of that idea?\n- when comparing to methods like HIRO, what steps do you take to ensure that HIRO can use similar assumptions as this method, e.g. access to grid discretization scheme?\n- isn't having exploration be decoupled from goal space coverage a common strategy in this graph-based planning setting, e.g. [2], which you did not cite.\n\n[1] Self-Imitation Learning\n[2] Successor feature landmarks for long-horizon goal-conditioned reinforcement learning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0jX0L5JHi4", "forum": "cMpOvMuyYa", "replyto": "cMpOvMuyYa", "signatures": ["ICLR.cc/2026/Conference/Submission18077/Reviewer_dYta"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18077/Reviewer_dYta"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778627723, "cdate": 1761778627723, "tmdate": 1762927857758, "mdate": 1762927857758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Strict Subgoal Execution (SSE), a hierarchical reinforcement learning framework designed for long-horizon, goal-conditioned tasks with sparse rewards. SSE incorporates Frontier Experience Replay (FER) to identify and filter out unreachable subgoals, improving the reliability and efficiency of high-level planning. It also uses a decoupled exploration policy to better explore the goal space and a path refinement mechanism to adjust edge costs based on low-level failures. Experiments on multiple benchmarks demonstrate that SSE achieves higher efficiency and success rates than existing goal-conditioned and hierarchical RL methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The manuscript is clearly written, with figures that effectively elucidate the proposed methodology and equations that comprehensively convey its technical details.\n\n2. The experimental design is well structured, incorporating appropriate selections of baselines. The ablation study provides a thorough examination of the contribution of each component and offers a detailed analysis of the method’s sensitivity to hyperparameters."}, "weaknesses": {"value": "1. To make a stronger case for the method's generality, the paper should include results from a broader set of environments. Specifically, in line with the existing HRL works, the authors may want to evaluate the method on Pusher, AntFall, AntGather and Ant4Rooms, in addition to the relatively simpler maze-based tasks. This would provide a better understanding of how well SSE adapts to various state spaces and task complexities. Including additional tasks would also help demonstrate whether the method can avoid unstable regions and enforce subgoal completion.\n\n2. The use of only five random seeds in the comparative study may be insufficient to establish statistical significance. Increasing the number of seeds would lead to more reliable and robust experimental conclusions."}, "questions": {"value": "Would it be possible to include comparative studies on a broader range of environments, using a larger number of random seeds as suggested?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GlzO9LM2vz", "forum": "cMpOvMuyYa", "replyto": "cMpOvMuyYa", "signatures": ["ICLR.cc/2026/Conference/Submission18077/Reviewer_BjYj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18077/Reviewer_BjYj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832836848, "cdate": 1761832836848, "tmdate": 1762927857370, "mdate": 1762927857370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper has proposed a novel goal-conditioned hierarchical reinforcement learning approach, called SSE, to achieve reliable long-horizon planning. The proposed approach is evaluated in a set of simulated navigation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•  The paper addresses an important problem in hierarchical reinforcement learning — unreliable subgoal execution — which is crucial for long-horizon tasks.\n\n•  The introduction of Frontier Experience Replay (FER) is conceptually clear and provides a principled way to delineate reachable and unreachable subgoals, improving training stability."}, "weaknesses": {"value": "•  The proposed framework assumes that the goal space is known and low-dimensional, which may not hold for complex real-world manipulation or visual tasks where the goal representation itself is high-dimensional and uncertain.\n\n•  The technical novelty is moderate — SSE combines known components (graph-based HRL, experience replay, path cost reweighting) rather than introducing fundamentally new learning principles.\n\n•  All experiments are conducted in simulators; there is no validation in real-world robotic systems, limiting the practical credibility of the claimed reliability."}, "questions": {"value": "1.\tDoes the proposed method assume that the goal space is low-dimensional? This may not be true for complex manipulation tasks.\n\n2.\tIs the proposed method applicable to real-world tasks? Only conducting experiments in simulators is not convincing enough."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fQqxaMMtHK", "forum": "cMpOvMuyYa", "replyto": "cMpOvMuyYa", "signatures": ["ICLR.cc/2026/Conference/Submission18077/Reviewer_kJYV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18077/Reviewer_kJYV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900769513, "cdate": 1761900769513, "tmdate": 1762927857019, "mdate": 1762927857019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper mentioned that in long-horizon, sparse-reward tasks, high-level policies often choose subgoals that the low-level controller can’t reliably reach. When HER is applied at the high level, failures get relabeled. The authors propose Frontier Experience Replay (FER), which stores three kinds of high-level transitions—success, stop-on-failure (zero-return, early termination), and partial success to the last reliably reached waypoint. They introduce a decoupled exploration policy that prioritizes under-explored goal-space regions (simple grid-density estimator) alongside an ε-greedy high-level policy to improve coverage. The authors add failure-aware path refinement that inflates edge costs in high-failure regions of the goal graph, nudging Dijkstra planning away from unstable corridors. SSE substantially outperforms HRL (HIRO, HRAC) and graph-based methods (HIGL, DHRL, NGTE, PIG, BEAG) on success rate and often on learning speed"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strong empirical results on a diverse suite, including tasks that require implicit sequencing\nAblation coverage is thoughtful: removing FER or replacing with HER largely breaks performance on harder tasks"}, "weaknesses": {"value": "The density estimator and failure statistics hinge on a grid. This is fine for 2D/3D but will be problematic in higher dimensions and for goals that include orientation or other factors.\nNo guarantees or formal properties regarding convergence or bias introduced by early termination/FER."}, "questions": {"value": "This is a strong empirical paper with a simple, well-motivated idea that addresses a real failure mode in hierarchical goal-conditioned RL. Have you thought about how to extend to analyze theoretically with HER?\nHow often does path refinement genuinely alter the planned path (e.g., fraction of episodes where the refined path differs from shortest path)?\nhave you tried replacing grid-based novelty with learned density?if so, do the gains persist?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3dlS5pVlrg", "forum": "cMpOvMuyYa", "replyto": "cMpOvMuyYa", "signatures": ["ICLR.cc/2026/Conference/Submission18077/Reviewer_cF88"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18077/Reviewer_cF88"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957124700, "cdate": 1761957124700, "tmdate": 1762927856576, "mdate": 1762927856576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}