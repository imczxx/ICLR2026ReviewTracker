{"id": "Te9wGKtakV", "number": 17127, "cdate": 1758272504875, "mdate": 1759897194906, "content": {"title": "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations", "abstract": "We present \\textsc{PersonaConvBench}, a large-scale benchmark for evaluating personalized reasoning and generation in multi-turn conversations with large language models (LLMs). \nUnlike existing work that focuses on personalization or conversational structure in isolation, \\textsc{PersonaConvBench} tightly integrates both, offering three core tasks: sentence classification, impact regression, and user-centric text generation, covering 10 diverse Reddit-based domains. \nThis design enables systematic analysis of how personalized conversational context can shape LLM outputs in realistic, multi-user conversational scenarios. \nWe systematically benchmark several commercial and open-source LLMs under a unified prompting setup, and observe that incorporating personalized conversational history yields substantial performance boosts—e.g.,  \nachieving a 198\\% relative gain over the best non-conversational baseline in sentiment classification.\nBy releasing \\textsc{PersonaConvBench} with comprehensive evaluations and codes, we aim to facilitate research on LLMs that can adapt to individuals’ conversational styles, track long-term context, and generate more contextually rich and engaging responses.", "tldr": "", "keywords": ["Benchmark", "Personalize", "Conversation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d75463df91a38330a86038e18361e90fbffe328b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PersonaConvBench, a comprehensive benchmark designed to evaluate personalized reasoning and generation in multi-turn conversations with large language models (LLMs). Built upon Reddit posts and comments, PersonaConvBench leverages users’ interaction histories as personalized signals to predict new comments. The benchmark consists of 19,215 posts and over 111,239 conversations from 3,878 users, spanning 10 diverse Reddit-based domains. PersonaConvBench defines three core tasks:\n- Sentiment Classification: Binary prediction of the polarity of user replies.\n- Impact Forecasting: Regression-based prediction of community feedback scores.\n- Personalized Text Generation: Generation of user-specific follow-up responses.\n\nExperimental results demonstrate that incorporating personalized conversational history significantly improves the performance of state-of-the-art LLMs, including GPT-4.1, GPT-4o-mini, Claude-3.5, Llama-3.3, and DeepSeek-R1."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark’s focus on evaluating LLM personalization using users’ past interaction histories is both well-motivated and highly realistic.\n- The proposed dataset is large-scale and diverse, spanning 10 domains and encompassing varied conversation styles. Its thoughtful construction incorporates temporal constraints and a graph-based representation of conversations.\n- Extensive experiments yield strong empirical results: leveraging users’ past interaction histories and dialog context consistently improves performance across all tasks, models, and domains."}, "weaknesses": {"value": "- Evaluation Metrics: The evaluation of Personalized Text Generation primarily relies on n-gram overlap metrics and SBERT scores, using only a single reference response. Given the open-ended nature of dialog, there may be multiple valid responses for a given context, making these metrics potentially insufficient for capturing the full range of appropriate outputs. Additionally, the absence of human evaluation limits the assessment of response quality and relevance.\n- Research Findings: The results indicate that incorporating dialog context and user interaction history improves the prediction of user responses. However, this outcome is somewhat expected, as removing dialog context or substituting interaction history with that of other users constitute relatively weak baselines. Therefore, the reported improvements are not particularly surprising."}, "questions": {"value": "How many examples are used in the in-context learning setting? \nGiven that some user might have long interaction history, will there be context limitation when doing few-shot learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oa67nutErG", "forum": "Te9wGKtakV", "replyto": "Te9wGKtakV", "signatures": ["ICLR.cc/2026/Conference/Submission17127/Reviewer_i9Nf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17127/Reviewer_i9Nf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718290096, "cdate": 1761718290096, "tmdate": 1762927123567, "mdate": 1762927123567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a personalized benchmark dataset curated from Reddit posts. The authors construct a graph to capture the relationships between posts and their corresponding replies across conversation turns. In addition, they propose three benchmark tasks: sentiment classification, Reddit upvote prediction, and response generation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper presents a multi-turn personalized dialogue benchmark derived from Reddit posts."}, "weaknesses": {"value": "1. The paper provides limited ablation studies to support its experimental findings.\n\n2. The dataset curation process based on Reddit data is not particularly novel.\n\n3. Although the paper emphasizes conversational personalization, there is little evidence of incorporating personalization signals beyond dialogue history in the response generation process."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oeVPQ695r0", "forum": "Te9wGKtakV", "replyto": "Te9wGKtakV", "signatures": ["ICLR.cc/2026/Conference/Submission17127/Reviewer_iQSj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17127/Reviewer_iQSj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836563635, "cdate": 1761836563635, "tmdate": 1762927123191, "mdate": 1762927123191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PersonaConvBench introduces a large-scale, Reddit-derived benchmark that integrates personalization and multi-turn conversational structure across 10 domains and three tasks—sentiment classification, impact (score) regression, and user-specific next-text generation. The authors provide ~19k posts, evaluate multiple LLMs, showing large performance gains from personalized history."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Extensive, real-world dataset spans 10 Reddit domains—19,215 posts, ~111,239 conversations, 3,878 users, providing scale and diversity for robust evaluation of personalized conversational models. \n+ Novel formulation combines graph-structured multi-user, multi-turn conversations with three tasks—sentiment classification, impact regression, and user-centric next-text generation—plus standardized in-context prompting and evaluation protocols. \n+ Comprehensive LLM benchmarks reveal large personalization gains."}, "weaknesses": {"value": "- The paper measures personalization mostly via performance deltas (P-Conv vs P-NonConv) and paired t-tests, rather than a direct “degree of personalization” metric or richer human judgments.\n- Heavy Reddit preprocessing (Nu, Nr, Np thresholds) and class-imbalance filtering (initial ~11:1 skew reduced to ~5:1) retained only ~6k sentiment posts. Removal of deleted/short posts create selection bias toward highly active users, reducing representativeness and real-world robustness.\n- Experiments run GPT-4.1, GPT-4o-mini, Claude-3.5, LLaMA3.3, DeepSeek-R1 but omit Qwen-family and reasoning-mode evaluations. Greedy, zero-shot decoding may understate reasoning gains. \n- Generation evaluation relies on automatic metrics (ROUGE, BLEU, METEOR, SBERT) without reported human evaluation; these metrics can miss conversational quality, personalization nuance, and pragmatic appropriateness."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ElkPOF8NWG", "forum": "Te9wGKtakV", "replyto": "Te9wGKtakV", "signatures": ["ICLR.cc/2026/Conference/Submission17127/Reviewer_3gNT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17127/Reviewer_3gNT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841540495, "cdate": 1761841540495, "tmdate": 1762927122812, "mdate": 1762927122812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PERSONACONVBENCH, a large-scale benchmark that evaluates how LLMs perform personalized reasoning and generation in multi-turn conversations. It integrates personalization and dialogue context across ten domains and three tasks—classification, regression, and text generation—showing that using user-specific conversation history improves model performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper constructs the first benchmark that jointly models personalization and multi-turn dialogue, enabling systematic evaluation of LLMs’ ability to adapt to user-specific styles and evolving conversational context.\nBy representing multi-user conversations as directed temporal graphs, the benchmark captures realistic branching, temporal ordering, and inter-user dependencies—allowing for fine-grained personalization and contextual reasoning that go beyond flat dialogue datasets."}, "weaknesses": {"value": "Problem formulation lacks clarity:\nThe notation is underspecified — in particular, while Cu​ (the user trajectory set) is later defined, the meaning of f is not clearly introduced where it first appears. This makes it difficult to precisely understand what constitutes the model input.\n\nAmbiguity in task setup and visibility scope:\nIt is unclear whether the model has access to all users’ conversational trajectories or only those of the participants in the current dialogue. In real conversations, a user A replying to B might also draw on prior interactions with other users (e.g., C). The paper does not explicitly explain whether such cross-thread context is included, how it is implemented. If implemented, whether temporal constraints are also enforced in such cross-thread context (i.e., that a reply at time t can only use information from ≤ t – 1). Without a clear temporal or visibility restriction, the need for a graph-based formulation is weakened.\n\nLine 218: The phrase “conditioned on the conversational trajectory and the user’s trajectory set” is conceptually ambiguous. Are these two distinct inputs to the model, or do they represent different levels of abstraction of the same information? My understanding is that the conversational trajectory refers to an abstract notion, while the user’s trajectory set denotes the concrete collection of conversations associated with a specific user. If they are indeed separate inputs, please clarify their respective definitions, roles, and how they differ in practice.\n\nEvaluation limitations:\nFor the dialogue generation task, it appears that each message has only one reference reply as ground truth. Metrics such as ROUGE are thus poorly suited to capture the diversity and open-endedness of conversational responses, limiting the reliability of quantitative evaluation."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7wUm9pn0Aq", "forum": "Te9wGKtakV", "replyto": "Te9wGKtakV", "signatures": ["ICLR.cc/2026/Conference/Submission17127/Reviewer_Ux4W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17127/Reviewer_Ux4W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000864857, "cdate": 1762000864857, "tmdate": 1762927122206, "mdate": 1762927122206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}