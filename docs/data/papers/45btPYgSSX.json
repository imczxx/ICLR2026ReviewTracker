{"id": "45btPYgSSX", "number": 16178, "cdate": 1758261051235, "mdate": 1759897256248, "content": {"title": "Rewriting Pre-Training Data Boosts LLM Performance in Math and Code", "abstract": "The performance of large language models (LLMs) in program synthesis and mathematical reasoning is fundamentally limited by the quality of their pre-training corpora.  \nWe introduce two openly licensed pre-training datasets, released under the Llama 3.3 Community License, that significantly enhance LLM performance by systematically rewriting public data. SwallowCode ($\\approx$16.1 billion tokens) refines Python snippets from The-Stack-v2 through a novel four-stage pipeline: syntax validation, pylint-based style filtering, and a two-stage LLM rewriting process that enforces style conformity and transforms snippets into self-contained, algorithmically efficient examples. Unlike prior methods that rely on exclusionary filtering or limited transformations, our transform-and-retain approach refines low-quality code, maximizing data utility.\nSwallowMath ($\\approx$2.3 billion tokens) enhances Finemath-4+ by removing boilerplate, restoring context, and reformatting solutions into concise, step-by-step explanations. Within a fixed 50 billion token training budget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1 by +17.0 on HumanEval and +16.1 on HumanEval+ compared to Stack-Edu, surpassing the baseline model's code generation capabilities. Similarly, substituting SwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies confirm that each pipeline stage contributes incrementally, with rewriting yielding the largest gains.\nBy releasing datasets, prompts, checkpoints, and pipeline code, we ensure reproducibility and provide a transferable transform-and-retain methodology that can be adapted to other base models and LLM rewriting setups.", "tldr": "An LLM rewriting method (transform-and-retain) refines pre-training corpora. Instantiated as SwallowCode/SwallowMath, it improves code/math performance within a fixed budget, demonstrating general utility across base models and code/math domains.", "keywords": ["dataset", "pretraining", "code", "llm"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/28e3ba6ffa903360acd726f27b66ff88b4459436.pdf", "supplementary_material": "/attachment/32194a20c0e8e3927e209c7e664751bdbac27946.zip"}, "replies": [{"content": {"summary": {"value": "This paper refines Python snippets from The-Stack-v2 through a novel four-stage pipeline: syntax validation, pylint-based style filtering, and a two-stage LLM rewriting process that enforces style conformity and transforms snippets into self-contained, algorithmically efficient examples. Experiments show that continued pretraining using the resulting dataset improves performance of LLMs on several benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is clear and easy to follow\n2. They provided the code for the data processing pipeline, improving the reproducibility of the work"}, "weaknesses": {"value": "1. The novelty seems limited. Using LLMs to rewrite data for continued pretraining has been adopted by previous works such as Qwen2.5-Math. The proposed method seems more like minor engineering tricks than major algorithmic novelty.\n2. The proposed method was only compared to coarse pretraining datasets such as Stack v1 and Stack v2. Comparison between SwallowMath and other more carefully filtered and processed datasets such as OpenWebMath [1], Lemma [2], and MathCoder2 [3] should be considered as well.\n\n3. The code only contains Python snippets. It is unclear whether it can improve the models’ coding abilities of other programming languages.\n\n[1] Paster, Keiran, et al. \"Openwebmath: An open dataset of high-quality mathematical web text.\" arXiv preprint arXiv:2310.06786 (2023).\n\n[2] Azerbayev, Zhangir, et al. \"Llemma: An open language model for mathematics.\" arXiv preprint arXiv:2310.10631 (2023).\n\n[3] Lu, Zimu, et al. \"Mathcoder2: Better math reasoning from continued pretraining on model-translated mathematical code.\" arXiv preprint arXiv:2410.08196 (2024)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JT4UVVDnlT", "forum": "45btPYgSSX", "replyto": "45btPYgSSX", "signatures": ["ICLR.cc/2026/Conference/Submission16178/Reviewer_NDvj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16178/Reviewer_NDvj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760859399110, "cdate": 1760859399110, "tmdate": 1762926341880, "mdate": 1762926341880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Transform-and-retain rewriting of public corpora using a two-stage LLM pipeline, rather than exclusionary filtering. For code: Style-Guided Code Rewriting (SGCR) followed by Self-Contained Optimization Rewriting (SCOR). For math: rewrite Finemath-4+ to remove boilerplate, restore context, and produce concise step-by-step solutions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Thoughtful analysis of why synthetic-from-scratch may underperform due to diversity issues.\n- Decontamination checks and cross-model validation (Qwen2-7B) improve credibility.\n- Open release of data, prompts, and checkpoints enhances community value and reproducibility."}, "weaknesses": {"value": "- generality claims are suggestive but not demonstrated across languages or larger scales.\n- no analysis of downstream generalization outside HumanEval/+.\n- No quantitative quality checks on rewritten outputs (e.g., compile/run rate, test pass rate, semantic drift)—risk of introducing hallucinated correctness."}, "questions": {"value": "- What proportion of rewritten code compiles and runs? Any automated test execution stats on a held-out suite?\n- For SCOR, how often does algorithmic “optimization” reduce correctness (e.g., edge cases)? Any spot-audit or unit-test sampling?\n- Any evidence the gains persist or improve at larger pre-training budgets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iEewYdJgRc", "forum": "45btPYgSSX", "replyto": "45btPYgSSX", "signatures": ["ICLR.cc/2026/Conference/Submission16178/Reviewer_NJeE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16178/Reviewer_NJeE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761376722305, "cdate": 1761376722305, "tmdate": 1762926341131, "mdate": 1762926341131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SwallowCode and SwallowMath, two openly licensed datasets created by systematically rewriting existing public corpora using a multi-stage LLM-driven pipeline. The approach combines filtering (syntax and linter-based) with two-stage rewriting (SGCR for style, SCOR for self-containment and optimization) to enhance data quality. Continual pre-training of models like Llama-3.1-8B and Qwen2-7B on these datasets shows significant gains on code and math benchmarks, outperforming existing datasets like Stack-Edu and Finemath-4+."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* This paper releases two open source datasets under permissive licenses, supporting community reuse and extension.\n\n\n* The ablation studies isolate the impact of each pipeline stage, demonstrating clear and interpretable improvements."}, "weaknesses": {"value": "* The success and ceiling of this method are fundamentally limited by the capabilities and potential biases of the powerful LLM. Rewriting relies on Llama-3.3-70B-Instruct, which may introduce its own stylistic or semantic biases.\n\n\n\n* While the paper claims rewriting enforces \"algorithmic efficiency,\" it is not clear how the paper quantifies or measures this specific gain. Providing concrete metrics or examples of complexity improvements would strengthen this claim."}, "questions": {"value": "The SwallowCode pipeline utilizes a two-stage LLM rewriting process (one for style, one for efficiency/self-containment). Why was this split necessary? Did the authors attempt a comparative study using a single-stage LLM prompt designed to achieve all stylistic and functional improvements simultaneously, and if so, what was the performance degradation compared to the more complex two-stage approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7JZ71NqwDk", "forum": "45btPYgSSX", "replyto": "45btPYgSSX", "signatures": ["ICLR.cc/2026/Conference/Submission16178/Reviewer_bMSq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16178/Reviewer_bMSq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834534885, "cdate": 1761834534885, "tmdate": 1762926340461, "mdate": 1762926340461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript introduces two novel openly licensed datasets — SwallowCode and SwallowMath — designed to enhance LLM performance in code synthesis and mathematical reasoning. Both datasets apply a transform-and-retain methodology: instead of filtering out low-quality samples, the authors employ LLM-driven rewriting to refine existing corpora. Extensive ablations show significant gains on HumanEval, HumanEval+, GSM8K, and MATH benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method of rewriting is well-motivated with sufficient ablation studies of different data-filtering methods.\n\nThe improvements are substantial and consistent across multiple benchmarks (HumanEval, HumanEval+, GSM8K, MATH), with comprehensive ablation studies demonstrating that each pipeline stage contributes incrementally.\n\nThe paper is well-written and easy to follow."}, "weaknesses": {"value": "The novelty is somewhat limited. The paper reads more as an engineering work in improving pre-training performance with careful experimental design rather than introducing new ideas. The author should try to clarify the unique contribution and insight of this paper to academic community."}, "questions": {"value": "Are there other LLM rewriting techniques to compare with the proposed method? The comparison could include quality and training results (if applicable).\n\nAnother question is whether the proposed rewriting method is limited by the rewriting LLM, meaning the dataset would need frequent updates.\n\nCould the method be applied iteratively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vTzvUN3Te5", "forum": "45btPYgSSX", "replyto": "45btPYgSSX", "signatures": ["ICLR.cc/2026/Conference/Submission16178/Reviewer_C3n8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16178/Reviewer_C3n8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111399809, "cdate": 1762111399809, "tmdate": 1762926339880, "mdate": 1762926339880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}