{"id": "HjLC5fEWcI", "number": 8114, "cdate": 1758064078692, "mdate": 1763105208894, "content": {"title": "Towards WhiteBox Generative Models: Scaling Non-Parametric Sampling with Representation", "abstract": "Scaling and architectural advances have produced strikingly photorealistic image generative model, yet their mechanisms remain opaque. Rather than advancing scaling, we strip away complicated engineering trick and propose a simple, non-parametric conditional generative model. Our design is grounded in three principles of natural images—(i) spatial non-stationarity, (ii) low-level regularities, and (iii) high-level semantics—and defines each pixel’s distribution from its local context window. Despite its minimal architecture and no training, the model produces high-fidelity MNIST samples and visually compelling CIFAR-10 images. This combination of simplicity and strong empirical performance points toward a minimal theory of natural-image structure. The model’s white-box nature also allows us to have a mechanistic understanding how the model generalize and generate diverse images. We study it by analyzing how is each pixel generated by tracing every generated pixel back to its source images. These analysis reveal a simple, compositional procedure for \"part-whole generalization.\" These findings suggest a hypothesis for how large neural network generative model learn to generalize.", "tldr": "The paper presents a training-free, transparent image generation model that leverages simple natural image principles to produce high-quality samples and reveal how generative models generalize.", "keywords": ["Interpretability", "non-parametric model", "generative visual model", "n-gram", "representation learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7c2ee9f460615086e362d3d92285f25612477c89.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces an interpretable retrieval-based image generation system. \nThe synthetic image is being generated one pixel after another by taking the central pixel from the distribution of most similar patches contained in the given image dataset. \nThe proposed similarity metric takes into account both low-level and high-level patch statistics.\nNotably, high-level statistics are obtained from the pretrained SimCLR neural network.\n\nAuthors show visual results om MNIST and CIFAR 10 and compute common metrics such as Inception Score and FID."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Overall, this paper introduces an original idea of non-parametric generation at small scale. The method may be useful in education purposes."}, "weaknesses": {"value": "1. Relying on a pretrained SimCLR model to get high-level statistics seems contradicting the goal of the paper: building a  theory of natural-image structure (line 457). Since SimCLR is not interpretable, one of the components of the distance metric turns out not interpretable as well. However, no interpretable high-level statistics are proposed.\n2. In Tab.1 FID of an ablated model, not including low-level statistics, it almost twice as better comparing with the full proposed model. This suggests that the diversity of generated images is higher for the ablated model. However, there is no discussion of this fact in the paper.\n3. There are no results of Efros–Leung model in Tab. 1, in contrary to what is said in line 264.\n4. Class-conditional model is not evaluated in Tab. 1 either, despite the claims in line 269.\n5. The presented results are overly simplistic. However, I believe they can me made more interesting by incorporating hierarchical generation: from coarse scale image to high-resolution ones."}, "questions": {"value": "I ask the authors to address the weaknesses listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xpPqvvxWjo", "forum": "HjLC5fEWcI", "replyto": "HjLC5fEWcI", "signatures": ["ICLR.cc/2026/Conference/Submission8114/Reviewer_zC2Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8114/Reviewer_zC2Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760801065239, "cdate": 1760801065239, "tmdate": 1762920093749, "mdate": 1762920093749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely thank the reviewers and area chair for their time and thoughtful feedback on our manuscript. After careful consideration, we have decided to withdraw the submission. Our work aims to explore a qualitatively different route to understanding generative models and image structure, prioritizing understanding over optimizing conventional benchmark-driven objectives. We believe this conceptual direction may be better suited to a different venue and therefore choose to withdraw at this time."}}, "id": "tGJLY6bMZC", "forum": "HjLC5fEWcI", "replyto": "HjLC5fEWcI", "signatures": ["ICLR.cc/2026/Conference/Submission8114/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8114/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763105208083, "cdate": 1763105208083, "tmdate": 1763105208083, "mdate": 1763105208083, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a simple method to sample images that relies on simple rules about position and local statistics in both pixel space and embedding space. It extends a prior method that worked on patterns, not images, so that it can now sample images from simple datasets. The paper conducts analysis and ablations on the proposed method, enabled by its white box and non parametric nature, and claims that the method goes beyond simple memorization of the dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The method is conceptually simple and transparent, combining positional cues with pixel level and embedding space similarities.\n- The non parametric design enables ablations and analyses that are difficult with black box generators.\n- The work is an incremental extension that pushes pattern generation ideas toward simple images.\n- The approach has potential as an auxiliary mechanism to augment generation by interpolation with a strong neural baseline, analogous to infini-gram for LLMs."}, "weaknesses": {"value": "- The paper overclaims about natural images and neural network generalization. The evidence is limited to MNIST and CIFAR and a positional distance approach that appears tailored to centered digits or simple objects.\n- The three stated principles are hand wavy and are not shown to be exhaustive or derived in a principled way and emprically only evaluated on simple datasets.\n- The patch filling mechanics are unclear because with maximally overlapped patches and center placement early patches are often half unfilled, the paper does not state how the distances in pixel space and SSL space are computed for these patches. Also, near the end the last pixel cannot be centered so either centering is violated or padding is used without specification.\n- Calling d_SSL a global signal is overstated because the SimCLR backbone is applied to local patches.\n- Memorization remains a concern because DCR or an equivalent metric is not reported, the failure case in Figure 5 suggests a possible correlation between low DCR and visually plausible images while high DCR samples are poor, and Section 3.2, Figure 6, and Section 3.3 do not rule out reuse of common local patches in MNIST and CIFAR.\n- Writing and presentation issues:\n  - Line 39: current \"highlight\" to correct \"highlighting\".\n  - Lines 67-68: current \"We performance ablation study to show the crucial role of each of three principles ...\" -> correct \"We conduct an ablation study to show the crucial role of each of the three principles ...\".\n  - Line 71: current \"for mechanistically understand\" -> correct \"for mechanistically understanding\".\n  - Lines 54-56:  The sentence \"This method excels in stationary datasets like textures; however, it a great success in sythesizing low-level regularities such as textures.\" is not clear.\n  - Line 269:  \"Table 1\" does not have class-conditioned. If it is referring to Table 2 then it should be clear outperform is in terms of entropy (this is discussed in another section but I am not sure what Table 1 outperforming here is referring to)"}, "questions": {"value": "- What evidence supports the claim in Lines 33-34 that generation quality improvements exhibit diminishing returns with scaling?\n- How are distances computed when many pixels in a patch are unfilled, is any padding used, and how is the center handled near the end of sampling, for both pixel space and SSL distances?\n- Can you report DCR or an equivalent memorization metric, add controls to rule out reuse of common local patches in MNIST and CIFAR, and test whether low DCR samples correspond to visually plausible images while high DCR samples are poor (e.g., FID of low DCR samples and FID of high DCR samples)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6zqMlg7zoR", "forum": "HjLC5fEWcI", "replyto": "HjLC5fEWcI", "signatures": ["ICLR.cc/2026/Conference/Submission8114/Reviewer_xe56"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8114/Reviewer_xe56"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761523862709, "cdate": 1761523862709, "tmdate": 1762920093435, "mdate": 1762920093435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the author explores non-parametric image generation by sampling each pixel from conditional distribution built from similar patches from real images. Specifically, the author encourages low-level regularities, spatial non-sationarity, and high-level semantics of the sampled pixels. The author tested their model on MNIST and CIFAR-10 generation, and show that this whitebox model provide transparency  in the generation process, allowing full tracing to the source for each generated pixel."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper shows that basic image coherence can emerge from directly resampling and recombining local patches from real images, without any training. The result is limited in scope but offers a small empirical reminder of how much structure already exists in the data itself.\n- The examples in Figure 2 help illustrate the effect of each similarity component (low-level, positional, semantic), although the analysis remains mostly qualitative.\n- The visualization of where each generated pixel originated from offers a clear and intuitive way to understand how the model constructs images, even if the insight is primarily descriptive.\n- The approach requires no optimization or learned parameters, making it easy to reproduce and inexpensive to run compared to modern generative models."}, "weaknesses": {"value": "- The three “principles” described in L45-46  (spatial non-stationarity, low level regularities, and high-level semantics) appear to function mainly as hand-crafted inductive biases rather than theoretically grounded components.\n\n- Experiments are restricted to MNIST and CIFAR-10, which are small and relatively low-resolution datasets. The method is only demonstrated on MNIST and CIFAR-10. While MNIST results are mostly coherent, CIFAR-10 samples appear weak in Figure 3. It is unclear whether the approach would work on more complex, high-resolution natural images.\n\n- The method is computationally heavy due to pixel-wise nearest-neighbor search, making it impractical for larger datasets.\n\n- Notation is confusing: The mathematics would benefit from clearer and more consistent indexed notation.\n  - For example, section 2.2 $I_{real}$ as the set of real images. The same symbol $I$ is reused for the dataset, individual image.\n  - Similarly, the authors use $\\omega'$ and  $\\omega$ for real and synthetic image patches, but $I^{(i)}$ and $I'$ for real and synthetic images, which is inconsistent index style.\n\n- $R_{SSD}$, $R_{loc}$, $R_{SSL}$ appear to be chosen heuristically. For SSD and SSL, they include all patches within $(1+\\epsilon)$ times the minimum distance, but the paper does not specify how $\\epsilon$ is set or analyze sensitivity.\n\n- The suggestion that this method could help understand modern generative models is unconvincing. The approach is purely non-parametric and relies on explicit data retrieval, while models like diffusion or transformers are parametric and synthesize from learned latent spaces. The mechanisms differ too fundamentally for this method to offer meaningful insight.\n\n- Unclear or inconsistent writing.\n  - L55–56: “This method excels in stationary datasets like textures; however, it a great success in synthesizing low-level regularities such as textures.”Grammatically incorrect and redundant; the sentence should be clarified and revised for readability.\n  - L269: Mentions a “class-conditional model” in Table 1, but the table and text do not explain what this model is or what its results represent. The reference is confusing and incomplete.\n  - Figure 2: The left image includes a patch that appears roughly four times larger than the others in the bottom-right corner. It is unclear whether this is intentional, an artifact of visualization, or part of the synthesis result. The caption should clarify this."}, "questions": {"value": "- How are the thresholds chosen in practice? What is the impact of varying the thresholds?\n- How large is the candidate patch pool on average, and how does this affect runtime and sample quality?\n- What is the result by the “class-conditional model” in Table 1 (L269)? \n- Could the authors clarify whether the “three principles” correspond to established concepts in natural image statistics, or if they are newly proposed heuristics?\n- Have the authors tried the method on higher-resolution or real-world datasets to test scalability beyond MNIST and CIFAR-10?\n- What is the practical utility of this approach, given its high computational cost? Since the method requires extensive nearest-neighbor searches for every pixel, how feasible is it to apply beyond small datasets or low-resolution images?\n- How does the choice of patch size affect performance and image quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hPdWW1mhpu", "forum": "HjLC5fEWcI", "replyto": "HjLC5fEWcI", "signatures": ["ICLR.cc/2026/Conference/Submission8114/Reviewer_JUXT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8114/Reviewer_JUXT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857628281, "cdate": 1761857628281, "tmdate": 1762920092927, "mdate": 1762920092927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a simple and training-free probablistic model that can still create realistic iamges. It shows how small local details and overall high-level semantics work together during generation. The proposed model can generate images similar to those generated with neural networks on CIFAR10 and MNIST."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n\n1. The authors fomulate the 3-stage non-parametric process to uncover the mystery of generation, which is an interesting design.\n\n2. The writeup is clear and easy to follow.\n\n3. The ablation study shows how each components contribute to the final generation."}, "weaknesses": {"value": "Weaknesses\n\n1. Limited experiments. The authors only tested on CIFAR10 and MNIST, where the image is simple and low-resolution. Whether the method can scale or not still remains unclear. \n\n2. The motivation of the paper is weak. The method built upon some discovered properties of visual clue or image. However, it's not very interesting due to two facts, \n\n   - i). the properties are not surprised and have been incoprated in many advanced models explicitly or implicitly. Very typical case is the ResNet with CNN, \n\n   - ii). The generation is not impressive, however introduces additional cost during gerenation, which isn't discussed in the paper. \n\nUnclear to me how will this work guide the field towards better image generation. Like how will this affect how we build models, understand failures, etc."}, "questions": {"value": "1. How can this work be combined with modern models?\n\n2. What about the cases where the model collapses?\n\n3. Any human preferences check?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G9Dy3F7QEp", "forum": "HjLC5fEWcI", "replyto": "HjLC5fEWcI", "signatures": ["ICLR.cc/2026/Conference/Submission8114/Reviewer_dfKL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8114/Reviewer_dfKL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867583947, "cdate": 1761867583947, "tmdate": 1762920092641, "mdate": 1762920092641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}