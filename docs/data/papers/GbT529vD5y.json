{"id": "GbT529vD5y", "number": 5369, "cdate": 1757904763605, "mdate": 1759897979503, "content": {"title": "From Basis to Basis: Gaussian Particle Representation for Interpretable PDE Operators", "abstract": "Learning PDE dynamics for fluids increasingly relies on neural operators and Transformer-based models, yet these approaches often lack interpretability and struggle with localized, high-frequency structures while incurring quadratic cost in spatial samples. We propose to represent fields with a \\emph{Gaussian basis}, where learned atoms carry explicit geometry (centers, anisotropic scales, weights) and form a compact, mesh-agnostic, directly visualizable state. Building on this representation, we introduce a \\emph{Gaussian Particle Operator} that acts \\emph{in modal space}: learned \\emph{Gaussian modal windows} perform a Petrov--Galerkin measurement, a \\emph{PG Gaussian Attention} effects global cross-scale coupling. This basis-to-basis design is resolution-agnostic and achieves near-linear complexity in $N$ for fixed modal budget, supporting irregular geometries and seamless 2D$\\to$3D extension. On standard PDE benchmarks and real datasets, our method attains state-of-the-art–competitive accuracy while providing intrinsic interpretability.", "tldr": "", "keywords": ["PDE", "Operator Learning", "Fluid Dynamics"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8252b188690f1dc09d102443e33eb516044572fd.pdf", "supplementary_material": "/attachment/0ec8416681756ff56f57e3d886f1189d91a6e50d.zip"}, "replies": [{"content": {"summary": {"value": "Radial basis (with learnable diagonal covariance) representation followed by an Attention-based operator."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Center alignment loss is good to impose a meaning in Gaussian particle positions.\n- Evaluation on a realistic dataset, e.g., ERA5.\n- Universal approximation theorem."}, "weaknesses": {"value": "- Figure 1: Since non-diagonal covariance has not been considered, rotated Gaussians are impossible.\n- Section 3.2: I can’t find any explicit connections between the PG method. The described idea is 1) computing the coefficient for the Gaussian representation, 2) applying attention several times to the coefficients, and 3) reconstruction.\n- Park and Sandberg took a similar approach for Lemma 1. It is necessary to indicate what is new and what is brought from existing literature. (Also Theorem 3.2.)\n- It is necessary to include and discuss [PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh Representations, Kang et al., ICLR 2025] and references therein."}, "questions": {"value": "Questions\n- Line 97: How has frequency bias been addressed by GPO?\n- How were out-of-distribution (OOD) cases defined?\n- How were $\\sigma_{min}$ and $\\sigma_{max}$ chosen?\n- Ablation study for alignment loss?\n- \"GPO’s cost grows nearly linearly with N\" -> could you provide a table? Can’t find in Table 4 (Line 462—463).\n- Line 134: “computationally, the map ~ is local.” In my understanding, it is necessary to evaluate all Gaussians for $\\tilde{x}$. Thus, I would say conceptually local, but computationally global. Please correct me if I am wrong.\n- The authors mentioned interpretability and visualizability as two of the advantages of GPO. I agree with \"visualizability.\" But I am wondering what kind of meaningful interpretation is possible, other than that from visualizability.\n- Line 179: “at each location j, G weighted coefficients $z_j$ are derived from particles.” Why $z_j$ coefficients? (Line 206 also)\n- Eq. 11 isn’t the tilde notation for query points?\n\nSuggestions\n- Line 108: G overlaps: Gaussian and the number of that.\n- Eq. 3-6 notations can be improved.\n- Line 180: The PG acronym was defined multiple times.\n- Eq. 10 notation can be improved, e.g., $(x-\\mu)^T \\Sigma^{-1}(x-\\mu)$\n- Line 119: $a_j := a(x_j)$\n- Eq. 20 typo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nDrliqHyZu", "forum": "GbT529vD5y", "replyto": "GbT529vD5y", "signatures": ["ICLR.cc/2026/Conference/Submission5369/Reviewer_whp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5369/Reviewer_whp9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761166676754, "cdate": 1761166676754, "tmdate": 1762918027268, "mdate": 1762918027268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces the Gaussian Particle Operator (GPO), a neural operator framework that represents PDE fields via a set of learned Gaussian particles with explicit geometric parameters The model combines this representation with a Petrov–Galerkin Attention mechanism that acts in modal space, achieving resolution-agnostic, close to linear complexity. The paper provides a detailed mathematical formulation, expressivity proofs, and experiments on several PDE benchmarks, including Navier–Stokes (2D/3D) and large-scale datasets (ERA5, CARRA).\n\nOverall, the paper is technically sound and ambitious, aiming to bridge interpretability, scalability, and performance in operator learning. It highlights several appealing properties: explicit interpretability of latent states, mesh- and resolution-agnostic design, and theoretically motivated PG attention."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a novel representation of PDE fields as learnable Gaussian mixtures. While Gaussian kernels have been used in kernel neural operators and multigrid models, this work is the first to make them the primary latent state and to combine them with a Petrov–Galerkin attention formulation.\n\n- The connection between neural operator learning and Petrov–Galerkin projection is insightful and contributes to interpretability and theoretical grounding.\n\n- The paper is mathematically rigorous: derivations of the Gaussian basis, PG operator formulation, and expressivity theorems are thorough and clearly presented.\n\n- The scalability analysis is detailed (complexity reduction) \n\n- The ablation studies convincingly isolate the contributions of the Gaussian field and PG Gaussian Attention.\n\n- The paper is clearly written and well organized, with a logical flow from representation → operator → experiments.\n\n- Its design is promising for large-scale geophysical or climate modeling tasks where interpretability and mesh-agnostic scalability are key.\n\n- The visualization of Gaussian particles and modal activations provides rare interpretability in operator learning."}, "weaknesses": {"value": "1) Missing comparisons to prior Gaussian-kernel operator works\n\nThe use of Gaussian kernels in PDE learning is not entirely novel; several recent operator frameworks rely on Gaussian or RBF kernels as integral components [1][2][3][4][5]\n\nWhile M²NO and MGNO are briefly mentioned in the related-work section, no experimental or architectural comparison is made.\nThe authors should clarify how GPO differs (e.g., explicit particle state vs. fixed Gaussian smoothing) and whether it outperforms or scales better than these models.\n\n2) Limited diversity of baselines\n\nGPO is compared only with spectral (FNO, LSM) and Transformer-based operators.\n\nState-of-the-are convolutional methods, such as the Convolutional Neural Operator [6] and Generalized Multi-Scale PDE model [7], are missing. These methods are known to perform extremely well on advection-dominated problems.\n\nGraph-based operators (e.g., Graph Kernel NO [8], Geometry-Aware Operator Transformer [9]) are natural and scalable baselines for irregular/non-uniform domains and should be included for completeness.\n\n3) Experimental scope\n\nAll experiments are on smooth, fluid-like datasets. \n\nIt remains unclear how GPO performs on multi-scale turbulent data (see [10] for instance), or on elliptic PDEs (e.g., Poisson, Helmholtz) that test spatial conditioning rather than temporal dynamics.\n\nThe datasets (ERA5 and CARRA) are not fully described in the text. Details such as data size, temporal horizon, and number of rollout steps are missing; they should be reported in the appendix.\n\nWhile NS2D has per-epoch timing/memory (Table 4), analogous runtime/memory/horizon details for NS3D, ERA5, CARRA are not reported. Given the scalability claim, those numbers would be compelling.\n\nThe paper claims “improved spectral fidelity and stable rollouts”, but there is no explicit quantitative evaluation of rollout stability (e.g., error vs. time-step curves).\n\n4) Performance gaps\n\nGPO outperforms baselines only on ERA5 (and partly on CARRA) but not on NS2D/3D (Table 1).\nSince the model’s main strengths are interpretability and scalability rather than accuracy, stronger baselines are needed to demonstrate clear trade-offs. It would be useful to examine how the model scales with an increasing number of training samples and larger model sizes.\n\n5) Visualization insufficiency\n\nFigure 3 and 5 provide only one in-distribution and one OOD visualization. More qualitative comparisons on all datasets (NS3D, ERA5, CARRA) would make interpretability claims more convincing.\n\n____\n\n[1] Chen, Y., & Zhang, L. (2024). Dynamic Gaussian Graph Operator (DGGO): Gaussian kernel integral operators for function-space learning\n\n[2] Wang, Y., Liu, H., & Li, Z. (2024). Kernel Neural Operator (KNO): Learning nonlinear operators with deep kernel integral mappings\n\n[3] Xu, R., Chen, H., & Anandkumar, A. (2024). Operator Learning with Gaussian Processes\n\n[4] Li, Z., Lai, Z., Zhang, X., & Wang, W. (2024). M²NO: Multiresolution Operator Learning with Multiwavelet-Based Algebraic Multigrid.\n\n[5] He, J., Liu, X., & Xu, J. (2024). MGNO: Efficient Parameterization of Linear Operators via Multigrid.\n\n[6] Raonic, B., Molinaro, R., De Ryck, T., Rohner, T., Bartolucci, F., Alaifari, R., ... & de Bézenac, E. (2023). Convolutional neural operators for robust and accurate learning of PDEs. Advances in Neural Information Processing Systems, 36, 77187-77200.\n\n[7] Gupta, J. K., & Brandstetter, J. (2022). Towards multi-spatiotemporal-scale generalized pde modeling. arXiv preprint arXiv:2209.15616.\n\n[8] Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., & Anandkumar, A. (2020). Neural operator: Graph kernel network for partial differential equations. arXiv preprint arXiv:2003.03485.\n\n[9] Wen, S., Kumbhat, A., Lingsch, L., Mousavi, S., Zhao, Y., Chandrashekar, P., & Mishra, S. (2025). Geometry aware operator transformer as an efficient and accurate neural surrogate for pdes on arbitrary domains. arXiv preprint arXiv:2505.18781.\n\n[10] Herde, M., Raonic, B., Rohner, T., Käppeli, R., Molinaro, R., de Bézenac, E., & Mishra, S. (2024). Poseidon: Efficient foundation models for pdes. Advances in Neural Information Processing Systems, 37, 72525-72624."}, "questions": {"value": "1) How does the proposed Gaussian particle representation behave when modeling fields with sharp gradients or discontinuities? Does the model sufficiently capture such features?\n\n2) Could the authors provide quantitative evidence for the claimed rollout stability, e.g., error over multi-step horizons?\n\n3) What is the computational footprint (GPU memory, runtime, etc) for the large-scale ERA5 and CARRA experiments compared to NS2D/NS3D?\n\n4) Can the authors clarify the training setup for ERA5 and CARRA (dataset size, time resolution, number of rollouts at inference, and prediction horizon)?\n\nThe authors should also review the Weaknesses section for additional (implicit) questions and points raised."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "/"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qxj4zVNKFb", "forum": "GbT529vD5y", "replyto": "GbT529vD5y", "signatures": ["ICLR.cc/2026/Conference/Submission5369/Reviewer_btKh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5369/Reviewer_btKh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560647412, "cdate": 1761560647412, "tmdate": 1762918026805, "mdate": 1762918026805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Gaussian Particle Operator (GPO), a neural operator framework that represents physical fields using interpretable Gaussian particles (with centers, scales, and weights) and applies Petrov–Galerkin (PG) Gaussian Attention for efficient basis-to-basis coupling. The method aims to provide interpretability and near-linear complexity compared with conventional Transformer-based operators. Experiments on PDE benchmarks (2D/3D Navier–Stokes, ERA5, CARRA) show that GPO achieves accuracy competitive with state-of-the-art operators such as FNO, LSM, and Galerkin Transformer, while offering a more interpretable representation of flow structures."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The use of Gaussian particles as latent bases for neural operators provides an interpretable and geometrically meaningful representation of the field.\n\n- The design of PG Gaussian Attention links variational numerical methods and attention mechanisms, which is conceptually appealing.\n\n- The computational analysis shows near-linear scaling with respect to the number of spatial points, which is beneficial for high-resolution or 3D applications."}, "weaknesses": {"value": "- Marginal performance improvement: Despite the interesting design, GPO’s accuracy gains are minor. For example, on NS2D/NS3D, FNO—with fewer parameters—performs better or comparably, suggesting limited practical benefit.\n\n- Outdated or insufficient baselines: The comparisons rely on older models (FNO 2021, LSM 2023). Recent linear-complexity Transformers such as\n[arXiv:2310.01082] and [arXiv:2502.16249] (and many others) demonstrate superior efficiency and accuracy, and should be included for a fair evaluation.\n\n- Incomplete ERA5 evaluation: Only t and u variables are reported, while ERA5 provides many other physically relevant quantities (v, z, humidity, etc.). Reporting all would give a clearer picture of generalization.\n\n- Lack of strong empirical motivation: The claimed interpretability advantage is qualitative; there is no quantitative metric showing that the Gaussian particle representation improves diagnostic insight or prediction stability.\n\n- Limited novelty in practice: The Gaussian-basis idea resembles radial-basis or kernel expansions, and the theoretical expressivity results repeat well-known universal-approximation arguments [arxiv.org/abs/2412.05994]. The overall innovation feels incremental."}, "questions": {"value": "- In the proposed Petrov–Galerkin formulation, the trial and test spaces should ideally form a dual pair to ensure stability. How is this duality theoretically justified in your model? The paper defines Gaussian trial functions and Gaussian modal windows as test functions, but it is unclear whether they constitute a stable trial–test pairing. Please provide a formal argument or empirical verification demonstrating that the PG structure is indeed valid rather than only heuristic.\n\n- The introduction highlights the issue of frequency bias in Transformer-based neural operators, yet there is no theoretical or empirical evidence showing that your Gaussian representation mitigates it. Can you provide quantitative results demonstrating improved handling of high-frequency components? For example, experiments on high–Reynolds-number Navier–Stokes equations or high-frequency Burgers/Kelvin–Helmholtz problems would directly support this claim.\n\n- The paper frequently mentions interpretable Gaussian particles and physically meaningful modal couplings, but there is no quantitative experiment or metric assessing interpretability. How do you define physical interpretability in measurable terms? Beyond visualization, can you show that the Gaussian particle features (e.g., centers, scales, or weights) correlate with physical quantities such as vorticity or coherent structures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DNoct4YxcF", "forum": "GbT529vD5y", "replyto": "GbT529vD5y", "signatures": ["ICLR.cc/2026/Conference/Submission5369/Reviewer_7G4p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5369/Reviewer_7G4p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832119079, "cdate": 1761832119079, "tmdate": 1762918026536, "mdate": 1762918026536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Gaussian Particle Operator (GPO), a neural operator for learning PDE dynamics using learnable Gaussian basis functions with explicit geometric parameters (centers $\\mu$, anisotropic scales $\\sigma$, mixture weights $w$) that are visualizable and mesh-agnostic. GPO employs Petrov-Galerkin attention in modal space: Gaussian windows aggregate $N$ spatial locations into $G \\ll N$ modal tokens, perform $G \\times G$ self-attention, then scatter back to spatial locations, achieving linear complexity. The method has nice guarantees and seems to perform competitively with established methods on benchmark problems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The method is well-formulated, with relevant theoretical proofs (e.g., universal approximation) grounding the model design . It demonstrates competitive performance against established neural operator baselines on the tested problems .\n- The interpretability of the representation is a key strength. Figures 3 and 4 nicely shows the learned particles aligning with physical structures like fronts and filaments. \n- The bottleneck design of performing the compression into $G$ modal windows is a smart choice to reduce the computational cost and seems to play favorably on the front of training costs / memory cost of the model during training and inference as shown in Table 4."}, "weaknesses": {"value": "- The selection of benchmarks could be strengthened. To fully evaluate the method's efficacy against the current state-of-the-art and ensure fair comparisons, it would be beneficial to test it on more recent, standardized benchmarks. The datasets from 'TheWell', for example, would be an excellent candidate for this.\n- While the paper claims that rollout stability is a strength, these claims are not substantiated. The paper provides no details on the rollout windows evaluated. How long can the method evolve auto-regressively before it diverges due to accumulated error?\n- While the paper claims that this methodology should is able to handle spectral bias that plagues neural operator methods, I believe there's more empirical results required. A convincing demonstration would include an analysis of the error spectrum, such as L2 error across different spectral buckets or a RASPD plot, to quantitatively show an advantage over baselines like FNO.\n- I think the claim of tackling irregular geometries isn't well tested. As I understand it, the CARRA dataset still has a regular grid structure with masking. Would it be possible to demonstrate the performance on an unstructured mesh problem such as those found in GeoFNO?"}, "questions": {"value": "Please address the weaknesses. Additionally, I have the following questions:\n- Could you perhaps discuss about the seamless capability of the model to adapt from 2D to 3D? Would this also mean that the problem should be able to adapt from information on a 2D version of the problem to a 3D one. That is NS2D can be built upon to NS3D? \n- The encoder is an MLP that operates point-wise. This is counter-intuitive, as particle parameters ($\\sigma$) seem to depend on neighborhood information. Did you experiment with replacing this with a neighborhood-aware encoder, such as a CNN?\n- Have you studied how the model's performance scales with overall model size (total parameter count)? For example, standard Transformers exhibit well-defined scaling laws. Have you analyzed how GPO's performance scales when you simultaneously increase other parameters like model width and depth"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YnYx0gmcTF", "forum": "GbT529vD5y", "replyto": "GbT529vD5y", "signatures": ["ICLR.cc/2026/Conference/Submission5369/Reviewer_BpAa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5369/Reviewer_BpAa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973399732, "cdate": 1761973399732, "tmdate": 1762918026191, "mdate": 1762918026191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}