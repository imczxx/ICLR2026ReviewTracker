{"id": "lZrZgZ9wIu", "number": 16520, "cdate": 1758265529898, "mdate": 1759897235691, "content": {"title": "Conversion of sparse Artificial Neural Network to sparse Spiking Neural Network can save up to 99% of energy", "abstract": "Artificial Neural Networks (ANNs) are becoming increasingly important but face the challenge of the large scale and high energy consumption. Dynamic Sparse Training (DST) aims to reduce the memory and energy consumption of ANNs by learning sparse network topologies, which ultimately results in structural connection sparsity. Meanwhile, Spiking Neural Networks (SNNs) have attracted increasing attention due to their biological plausibility and event-driven nature, which ultimately results in temporal sparsity. To bypass the difficulty of directly training SNNs, converting pre-trained ANNs to SNNs (ANN2SNN) is becoming a popular approach to obtain high-performance SNNs. Here for the first time, we investigated the advantage of dynamically spare trained ANNs for conversion into sparse SNNs. By adopting Cannistraci-Hebb Training (CHT), a state-of-the-art brain-inspired DST family that resembles synaptic turnover during neuronal connectivity learning in brain circuits, we examined the extent to which connectivity sparsity impacts the accuracy and energy efficiency of SNNs across different conversion approaches. The results show that sparse SNNs can achieve accuracy comparable to or even surpassing that of dense SNNs. Moreover, sparse SNNs can reduce energy consumption by up to 99% compared with dense SNNs. Furthermore, driven by the interest in understanding the physical dynamic interactions between firing rate and accuracy in SNNs, we systematically analyzed the temporal relationship between the saturation of firing rate and accuracy in SNNs.  Our results reveal a significant time lag where firing rate saturation precedes accuracy saturation. We also demonstrate that the magnitude of this time lag is significantly different between sparse and dense networks, where the average time lag of sparse SNNs is higher than that of dense SNNs. By combining the structural sparsity of DST and temporal sparsity of SNNs, we make a step forward to the brain-like computational network architecture with high performance and energy efficiency.", "tldr": "", "keywords": ["Spiking neural networks", "ANN-to-SNN conversion dynamic sparse training", "Cannistraci-Hebb Training", "sustainable AI", "energy-efficient architectures"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/84514c24f298d76804f9b5c2c77ed71b09f01645.pdf", "supplementary_material": "/attachment/e99ebea0cfe80bd3573952ecec914cb073249bcf.zip"}, "replies": [{"content": {"summary": {"value": "This work combines ANN-SNN Conversion framework with a Dynamic Sparse Training (DST) scheme named Cannistraci-Hebb Training (CHT) to achieve low-power SNNs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. This work explores the feasibility of obtaining low-power SNNs from the perspective of conversion learning rather than SNN direct training (or referred to as STBP training)."}, "weaknesses": {"value": "1. This work only shows the energy-saving ratio in Table 2, but does not simultaneously display the inference accuracy under the corresponding sparsity level. In addition, this work only presents two small-scale static datasets (CIFAR-10/100) and convolutional network structures.\n\n2. The so-called energy saving in this work are compared to vanilla pretrained ANN models. This work did not compare the inference accuracy and energy consumption with a series of important works based on STBP sparse training.\n\n3. The sparse SNN obtained based on ANN-SNN Conversion requires a significant amount of time-steps in the inference phase, as evidenced by the time-steps listed in Table 2. In comparison, the SNN obtained from STBP sparse training usually only requires no more than 8 time-steps, which is also a limitation of this work.\n\n4. The sparse training in this work was conducted during the ANN stage and is not directly related to SNN, which raises concerns about the contribution of this work to the SNN community.\n\n5. It is obvious that the layout of the figures, tables and formulas in this work needs further optimization."}, "questions": {"value": "See Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vl68UpMtes", "forum": "lZrZgZ9wIu", "replyto": "lZrZgZ9wIu", "signatures": ["ICLR.cc/2026/Conference/Submission16520/Reviewer_3cJ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16520/Reviewer_3cJ8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477903474, "cdate": 1761477903474, "tmdate": 1762926608751, "mdate": 1762926608751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether dynamically sparse artificial neural networks (ANNs), trained using the Cannistraci-Hebb Training (CHT) algorithm, can improve the performance and energy efficiency of spiking neural networks (SNNs) when converted through existing ANN-to-SNN conversion methods. The authors report that converting dynamically sparse ANNs to sparse SNNs maintains comparable accuracy to dense baselines while achieving substantial theoretical energy reductions (up to 99%)"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality.\nPoses a concrete and timely question at the intersection of dynamic sparsity and ANN to SNN conversion: do topology-sparse ANNs yield accuracy/energy benefits after conversion when topology is preserved?\n\nQuality.\nProvides a clear experimental pipeline: train sparse ANNs via CHT, freeze topology, convert with three representative methods, then evaluate accuracy vs. time steps and theorized energy. \n\nClarity.\nThe paper is structured and readable (method figures, tables, and definitions are straightforward).\n\nSignificance.\nIf validated on hardware and broader settings, the claim that sparse-topology conversion preserves accuracy while dramatically reducing energy would matter for energy-aware neuromorphic deployment."}, "weaknesses": {"value": "1. Novelty appears incremental: The study combines two well-established components, namely dynamically sparse training (DST)-based sparsity and standard ANN-to-SNN conversion techniques, and focuses mainly on evaluating their combined effect rather than introducing a new methodological contribution.\n\n2. Energy claims are theoretical and hinge on strong assumptions; no hardware validation. The headline “up to 99% energy reduction” is derived from a spike-count/FLOP model plus constants (EMAC/EAC) and the assumption that sparse hardware gives linear speed/energy benefits w.r.t. link sparsity. There is no measurement on neuromorphic or sparse-compute hardware. Consequently, the core claim remains speculative without device-level latency/energy evidence or even cycle-accurate simulators."}, "questions": {"value": "1. Hardware validation: Can you provide on-device latency and energy results for one platform to substantiate the 99% savings, and report how close real savings are to the theoretical model?\n2. Scope expansion: Do results persist on larger datasets (e.g., Tiny-ImageNet/ImageNet-subset) and deeper/backbone variants (e.g., ResNet/transformers) to support broad claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RHkxD2DxSQ", "forum": "lZrZgZ9wIu", "replyto": "lZrZgZ9wIu", "signatures": ["ICLR.cc/2026/Conference/Submission16520/Reviewer_nyYp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16520/Reviewer_nyYp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839855000, "cdate": 1761839855000, "tmdate": 1762926608330, "mdate": 1762926608330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores a novel angle for ANN-to-SNN conversion: using dynamically sparsely trained ANNs as the source models. The authors employ Cannistraci-Hebb Training (CHT), a brain-inspired sparse training algorithm, to introduce sparsity into the ANN before conversion. The central claim is that this approach can produce sparse SNNs that achieve high energy efficiency (up to 99% savings compared to dense SNNs) while maintaining accuracy, representing a step towards more brain-like efficient computing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The core idea of investigating dynamically sparse ANNs for conversion is innovative and represents a fresh contribution to the ANN2SNN field.\n\nThe focus on sparsity is well-aligned with the fundamental advantages of SNNs for energy-efficient, event-driven computation.\n\nThe method demonstrates good performance and significant energy savings on MLP (Multi-Layer Perceptron) architectures, validating the potential of the approach on simpler networks."}, "weaknesses": {"value": "The title, \"CONVERSION OF SPARSE ARTIFICIAL NEURAL NETWORK TO SPARSE SPIKING NEURAL NETWORK CAN SAVE UP TO 99% OF ENERGY,\" is potentially misleading. It suggests a 99% saving over a baseline that is not clearly specified, likely leading readers to assume it's compared to a sparse ANN. The abstract clarifies it's versus a dense SNN, but the title remains overly broad and risks overstating the finding.\n\nThe experiments are conducted on small datasets. The paper's impact would be significantly greater with validation on larger, more complex datasets (e.g., ImageNet or its subsets).\n\nTable 2 reports sparsity and energy but crucially ​​omits the accuracy/performance​​ of the converted models. This makes it impossible to evaluate the true trade-off between efficiency and accuracy, which is the central claim of the work.\n\nA key component of the method, CHT, is based on a preprint that has not undergone peer review. This reliance weakens the methodological foundation of the paper, as the core algorithm's efficacy and claims are not yet independently verified.\n\nThe discussion and conclusion frame the work as a significant step towards brain-like architecture. However, simply converting a sparsely trained ANN to an SNN is a relatively indirect contribution to neuromorphic computing. The narrative should be tempered to more accurately reflect the specific contribution: an efficient conversion pipeline leveraging sparse training, rather than a fundamental advance in brain-like computing."}, "questions": {"value": "What are the accuracy results corresponding to the models in Table 2? Without these, the claim of high efficiency is incomplete. How much accuracy is sacrificed for the gained sparsity and energy savings?\n\nCan the demonstrated benefits of this conversion approach scale to larger, modern datasets and architectures (e.g., deep convolutional networks)? What are the potential challenges?\n\nGiven that CHT itself is not a contribution of this paper and is not yet peer-reviewed, to what extent are the observed benefits specific to CHT versus being a general property of any high-quality sparse training method? Could similar results be achieved with other established sparse training techniques?\n\nThe 99% energy saving is compared to a dense SNN. What is the energy saving compared to a sparse SNN converted from a standard (non-CHT) pruned ANN? This would better isolate the contribution of the dynamic sparse training method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CYgd4tZTg3", "forum": "lZrZgZ9wIu", "replyto": "lZrZgZ9wIu", "signatures": ["ICLR.cc/2026/Conference/Submission16520/Reviewer_FQAr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16520/Reviewer_FQAr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893334291, "cdate": 1761893334291, "tmdate": 1762926607863, "mdate": 1762926607863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel and promising approach that combines Dynamic Sparse Training with ANN-to-SNN conversion. The authors employ Cannistraci-Hebb Training to train highly sparse ANNs and successfully convert them into sparse SNNs. The results demonstrate that these sparse SNNs can achieve accuracy comparable to or even surpassing their dense counterparts, while achieving a remarkable theoretical energy reduction of up to 99%. Furthermore, the paper is the first to reveal the phenomenon of a time lag between the saturation of firing rate and accuracy in SNNs, and finds a significant difference in this lag between sparse and dense networks, providing new insights into the underlying mechanisms of SNNs. Overall, this is an important work that synergizes the advantages of structural sparsity and temporal sparsity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.This is a study on converting dynamically sparsely trained ANNs into SNNs, while prior work has mostly focused on converting dense networks.\n\n2.The authors validate their findings across two different network architectures (MLP and VGG-16), two datasets (CIFAR-10/100), and three representative conversion methods."}, "weaknesses": {"value": "1.The experiments are conducted solely on traditional CNNs like MLP and VGG-16. Currently, Transformer architectures have become mainstream in fields such as computer vision. To demonstrate the generalizability and state-of-the-art relevance of the proposed method, the authors should include experimental results on converting sparsely trained Transformer models from ANN to SNN.\n\n2.All experiments are performed on the relatively small CIFAR-10 and CIFAR-100 datasets. The absence of validation on large-scale, more challenging real-world datasets like ImageNet raises concerns about the generalization capability of the conclusions in complex scenarios and diminishes the practical value of the method."}, "questions": {"value": "1.Regarding the relatively small energy improvement (only 19%) for VGG-16 under the AEC method, the paper attributes it to the sparse SNN requiring a longer inference time T. Could the authors analyze why, under the AEC method, the 50%-sparse VGG-16 requires a longer T to reach peak accuracy? Does this suggest a potential incompatibility between certain conversion methods and sparse topologies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "J2kMVOMHvL", "forum": "lZrZgZ9wIu", "replyto": "lZrZgZ9wIu", "signatures": ["ICLR.cc/2026/Conference/Submission16520/Reviewer_NFnN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16520/Reviewer_NFnN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987765024, "cdate": 1761987765024, "tmdate": 1762926607460, "mdate": 1762926607460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}