{"id": "cPCGB402ff", "number": 20392, "cdate": 1758305450233, "mdate": 1759896980054, "content": {"title": "SportR: A Benchmark for Multimodal Large Language Model Reasoning in Sports", "abstract": "Artificial Intelligence brings powerful new tools to sports, from automated officiating to tactical analysis, but these applications all depend on a core reasoning capability. \nDeeply understanding sports requires an intricate blend of fine-grained visual perception and rule-based reasoning—a challenge that pushes the limits of current multimodal models. \nTo succeed, models must master three critical capabilities: perceiving nuanced visual details, applying abstract sport rule knowledge, and grounding that knowledge in specific visual evidence.\nCurrent sports benchmarks either cover single sports or lack the detailed reasoning chains and precise visual grounding needed to robustly evaluate these core capabilities in a multi-sport context. \nTo address this gap, we introduce SportR, the first multi-sports large-scale benchmark designed to train and evaluate MLLMs on the fundamental reasoning required for sports intelligence. \nOur benchmark provides a dataset of 5,017 images and 2,101 videos. To enable granular evaluation, we structure our benchmark around a progressive hierarchy of question-answer (QA) pairs designed to probe reasoning at increasing depths—from simple infraction identification to complex penalty prediction. \nFor the most advanced tasks requiring multi-step reasoning, such as determining penalties or explaining tactics, we provide 7,118 high-quality, human-authored Chain-of-Thought (CoT) annotations.\nIn addition, our benchmark incorporates both image and video modalities and provides manual bounding box annotations to directly test visual grounding in the image part. Extensive experiments demonstrate the profound difficulty of our benchmark. State-of-the-art baseline models perform poorly on our most challenging tasks. While training on our data via Supervised Fine-Tuning and Reinforcement Learning improves these scores, they remain relatively low, highlighting a significant gap in current model capabilities. SportR presents a new challenge for the community, providing a critical resource to drive future research in multimodal sports reasoning. The dataset is available at https://anonymous.4open.science/r/ICLR2026-E071/.", "tldr": "", "keywords": ["Multimodal Large Language Models", "Sports Understanding", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c346d36bc3523bf0ba7e8e790e084207c01e7269.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "(I tend to write shorter reviews and the length of the review does not reflect the quality of paper or the time spend on reviewing it).\n\nNote that I am not an expert in dataset creation, nor action benchmarks. I am reviewing this from a PoV of a deep learning research. \n\nThis paper introduces SportR, a large-scale, multi-sport benchmark designed to train and evaluate the fine-grained, rule-based reasoning of Multimodal Large Language Models (MLLMs). The authors argue that existing benchmarks lack the detailed reasoning chains and precise visual grounding needed to assess deep sports understanding. To address this gap, SportR provides a dataset of 5,017 images and 2,101 videos covering five sports, 50 foul types, and 12 tactics. The primary contributions are: (1) a progressive question-answering hierarchy complemented by 7,118 high-quality, human-authored Chain-of-Thought (CoT) annotations for complex tasks like penalty prediction; (2) the introduction of a novel explicit visual grounding task, the first for a multi-sport benchmark, which requires models to output precise bounding box coordinates for infractions ; and (3) extensive experiments showing that while state-of-the-art models perform poorly, training on SportR (via SFT and RL) yields significant gains and even demonstrates cross-modal generalization from image-based training to video task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The dataset is solid along with the wide range of benchmarking across various current models. I am sure the benchmark will have a lot of utility for the community."}, "weaknesses": {"value": "While I am not an expert, I see a few relevant papers that are missing from RW like. \n\nhttps://mrsalehi.github.io/action-atlas/\n\nhttps://ego-exo4d-data.org/\n\nAdding these things and on going general skill set understanding benchmarking to the background will help the readers contextualize the work better."}, "questions": {"value": "See above. I defer to other reviewers for more questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WCOgAkS2xL", "forum": "cPCGB402ff", "replyto": "cPCGB402ff", "signatures": ["ICLR.cc/2026/Conference/Submission20392/Reviewer_m3qH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20392/Reviewer_m3qH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760733064222, "cdate": 1760733064222, "tmdate": 1762933841639, "mdate": 1762933841639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SportR, a new multi-sports benchmark designed to evaluate the reasoning capabilities required for sports intelligence. \nIt provides a dataset of images and videos with question-answer pairs and detailed reasoning chains to test a model's ability to perceive visual details, apply sports rules, and ground knowledge in visual evidence.\nExperiments show that current state-of-the-art models struggle significantly with the benchmark's most challenging tasks, highlighting a major gap in multimodal reasoning capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper features a progressive hierarchy of questions that systematically test reasoning depth—from simple identification to complex, multi-step tasks like penalty prediction.\n\n- The benchmark consists of 7,118 human-authored Chain-of-Thought (CoT) annotations for the most complex tasks, providing models with explicit examples of the required reasoning process.\n\n- Extensive experiments show that state-of-the-art models perform poorly on SportR's most difficult tasks."}, "weaknesses": {"value": "- The author states that the paper is \"the first large-scale, multi-sport benchmark specifically designed to evaluate core reasoning capabilities.\" However, I know that there are actually some existing sports-domain datasets and benchmarks, such as \"Sportsu\". Therefore, what are the key differences between the dataset proposed in this paper and the existing datasets? Is it merely the addition of Chain-of-Thought (CoT) and grounding annotations?\n\n- As shown in Table 1 and Table 2, after SFT and SFT+RL, the model's performance significantly improves, even surpassing the zero-shot results of Gemini-2.5. This raises a question: is the model memorizing specific patterns from the training data, or has it acquired the ability to generalize to new scenarios (for example, a new match/game)? Furthermore, if a simple SFT+RL pipeline can enable the model to achieve a performance score of over 80, does this suggest that the task might not be challenging enough?\n\n- There seems to be no mention of the video attributes, such as duration, resolution, and FPS, which are crucial for video understanding tasks."}, "questions": {"value": "See the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Aeno4tw1rV", "forum": "cPCGB402ff", "replyto": "cPCGB402ff", "signatures": ["ICLR.cc/2026/Conference/Submission20392/Reviewer_7hxG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20392/Reviewer_7hxG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761292768274, "cdate": 1761292768274, "tmdate": 1762933841134, "mdate": 1762933841134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SportR, a new large-scale multimodal benchmark designed to evaluate and train multimodal large language models on fine-grained, rule-based reasoning in sports. The benchmark covers both images and videos across five major sports, providing over 7,000 human-authored Chain-of-Thought (CoT) rationales and 20,000 structured question-answer pairs. It introduces hierarchical QA tasks that range from infraction identification to penalty prediction and visual grounding. The authors also evaluate a range of open and closed source MLLMs (e.g., GPT-5, Gemini 2.5 Pro, Qwen2.5-VL) and show that the benchmark is both challenging and effective as a training resource."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Human-authored CoT rationales: The manual, expert-driven CoT annotations enhance the dataset’s reliability and interpretability, avoiding the noise of model-generated explanations. The listed procedures and training for annotators are rigorous and well justified.\n2. Comprehensive multimodal design: Covering both image and video modalities allows for evaluation of both spatial and temporal reasoning.\n3. Hierarchical QA framework: The progressive question design (from simple classification to reasoning and grounding) is well thought out and probes different layers of model understanding.\n4. Strong empirical evaluation: The authors present clear baselines across leading proprietary and open-source MLLMs and demonstrate meaningful improvements from fine-tuning and reinforcement learning."}, "weaknesses": {"value": "1. Limited ablation and error analysis: The paper would benefit from deeper analysis of failure cases or qualitative examples that illustrate where models still fall short and potential reasons why. The authors successfully show that their tasks are difficult, but they don't provide insights into why the tasks are difficult for current MLLMs.\n2. Visual Localization True Difficultly: For the visual grounding tasks that require predicting bounding box coordinates, it is unclear whether the low accuracy stems from the model’s inherent difficulty in producing precise coordinates or from the underlying reasoning complexity of the task itself. It would be informative to know how performance changes if the prompt focuses on spatial relations instead of direct coordinate prediction, for example, asking “near which body part is the foul occurring?” rather than requesting explicit bounding boxes. Additionally, could the authors report bounding box prediction accuracy for simpler localization tasks that do not involve reasoning, such as identifying the player’s position on the field?\n3. LLM Evaluator Reliability: It remains unclear how reliable and stable LLM-as-a-judge evaluation is in this setting. How often do frontier models disagree with each other when serving as evaluators? Even for a small subset of samples, it would be helpful to report how frequently the LLM’s judgments diverge from expert human annotations. While perfect alignment is not expected, providing this comparison would give readers a sense of the evaluation’s variance and help contextualize the reported results with an appropriate error margin."}, "questions": {"value": "1. How consistent were the CoT rationales across annotators? Even for two athletes with similar levels of experience, it seems likely their thought processes would heavily diverge. How important is this for training data quality?\n2. How sensitive are the results to the LLM-as-judge choice? Would human evaluation yield similar model rankings?\n3. Is there any evidence that models trained on SportR improve performance on non-sports multimodal reasoning benchmarks? This would support the idea that valuable skills are gained from learning this task, rather than being a case of sports related tasks not being common in existing training datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vCQlrO8cSx", "forum": "cPCGB402ff", "replyto": "cPCGB402ff", "signatures": ["ICLR.cc/2026/Conference/Submission20392/Reviewer_1rJ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20392/Reviewer_1rJ3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804484432, "cdate": 1761804484432, "tmdate": 1762933840431, "mdate": 1762933840431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SportR, a large-scale multimodal benchmark for evaluating and training MLLMs on fine-grained sports reasoning. The benchmark comprises 5,017 images and 2,101 videos across five sports (basketball, soccer, table tennis, badminton, American football), covering 50 foul types and 12 tactics. The key innovation is 7,118 fully human-annotated Chain-of-Thought (CoT) rationales and a novel visual grounding task requiring precise bounding box prediction. Experiments show that state-of-the-art models struggle significantly, with visual grounding IoU scores below 7% for baseline models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly articulates the gap between current sports benchmarks - either single-sport with detailed annotations OR multi-sport without fine-grained reasoning chains. SportR addresses both limitations simultaneously.\n\n- Fully human-authored CoT by 16 domain experts (including 2 NCAA Division I athletes) with rigorous quality control is a significant strength.\n\n- The coordinate-based grounding evaluation (Q5) is a novel visual grounding task."}, "weaknesses": {"value": "- Missing human evaluation: No human agreement studies or inter-annotator reliability metrics for the evaluation itself\n\n- Visual grounding (Q5) only applies to SportsImage, not SportsVideo, and no temporal grounding annotations at all.\n\n- The evaluation use close source LLM as judge, the paper acknowledges self-preference bias but doesn't adequately address it."}, "questions": {"value": "- What specific instructions were given to annotators for drawing bounding boxes? How was consensus reached on box boundaries? Examples in Fig. 18 show a single box - what about fouls involving multiple players?\n\n- For Q4 (free-form explanation), how is semantic similarity measured by LLM judges? What's the correlation with human judgment?\n\n- What's the split ratio? How many images/videos in test set? How was stratification across sports/foul types ensured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RYEATDSGEC", "forum": "cPCGB402ff", "replyto": "cPCGB402ff", "signatures": ["ICLR.cc/2026/Conference/Submission20392/Reviewer_AVw9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20392/Reviewer_AVw9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163771280, "cdate": 1762163771280, "tmdate": 1762933839681, "mdate": 1762933839681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}