{"id": "OKGcbsGMqc", "number": 20721, "cdate": 1758309385619, "mdate": 1759896961900, "content": {"title": "Nostra: Enabling Robust Robot Imitation via Multimodal Latent Imagination", "abstract": "Similar to humans, robots benefit from multiple sensing modalities when performing complex manipulation tasks. Current behavior cloning (BC) policies typically fuse learned observation embeddings from multimodal inputs before decoding them into actions. This approach suffers from two key limitations: 1) it requires all modalities to be present and in-distribution at test time, otherwise corrupting the latent state and leading to fragile execution; and 2) naive fusion across all inputs hinders learning from large-scale heterogeneous datasets, where only a subset of modalities may be informative at different phases of a task. We introduce Nostra, a multimodal state-space model that learns a modular per-modality latent representation, enabling flexible action prediction with or without specific inputs. BC-Nostra improves robustness to unseen noise by using KL divergence between inferred and imagined multimodal latents as a noise measure, and by employing latent imagination to predict action trajectories over arbitrary horizons. On a suite of MuJoCo-based tasks, BC-Nostra fits expert demonstrations up to six input modalities (multi-view RGB, depth, and proprioception), achieving over 20% higher performance under noisy evaluation. Furthermore, Nostra adaptively down-weights non-informative inputs, facilitating effective co-training on large heterogeneous robotics datasets with O(10k) demonstrations spanning diverse tasks and visual conditions. Finally, we demonstrate real-world deployment, where BC-Nostra achieves up to a 40% performance gain under camera occlusions on multiple manipulation tasks.", "tldr": "", "keywords": ["robotics", "multimodality", "state-space models", "robustness"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1a1225525b068646706744aa2d4923f8c76fe9f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces NOSTRA, a multimodal state-space model for visuomotor policy learning that enhances robustness in robot imitation learning. Unlike standard BC models that concatenate multimodal features (e.g., RGB, depth, proprioception) into a single latent vector, NOSTRA learns modular per-modality stochastic latent spaces. These latent subspaces allow the policy to adaptively handle missing or noisy inputs by performing multimodal latent imagination.\n\nThe authors further propose AdaMLI, a test-time mechanism that uses per-modality KL divergence between inferred and imagined latents to detect noise and selectively switch between open-loop and closed-loop execution.Comprehensive experiments on 12 simulated manipulation tasks (RoboMimic, MimicGen, MimicLabs) and four real-robot tasks demonstrate robustness improvements. The results show that structured, modular latent spaces coupled with adaptive latent imagination enable resilient visuomotor control under multimodal uncertainty."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method has some level of conceptual innovation. The introduction of per-modality latent subspaces disentangles sensor-specific information, preventing contamination from noisy modalities, which is an improvement over monolithic latent fusion. The proposed latent imagination mechanism (MLI) extends the notion of open-loop predictive world modeling to multimodal BC settings, allowing action inference when certain modalities are unavailable. AdaMLI provides a principled yet practical metric for test-time noise adaptation without retraining or auxiliary supervision.\n\n- Robust empirical validation / Good experimental results: Extensive simulation experiments (Tables 1–5) show consistent and gains over baselines (BC-LSTM, Diffusion Policy, Diffusion Forcing, BC-RSSM). Table 2 and 6 convincingly demonstrate resilience to RGB, depth, and proprioceptive noise as well as real-robot occlusion. Ablations (e.g., with/without MLI, with/without modularity) clearly isolate the contributions of each component.\n\n- Broader and practical impact: Demonstrated robustness in real-robot scenarios with only 30 human demonstrations shows real-world applicability. Training remains data-efficient and uses accessible architectures (ResNet18, GRU).\n\n- The paper is well-written and easy to read. The tables and figures are well-designed and helpful to understand the paper."}, "weaknesses": {"value": "- Incremental and limited novelty in core modeling: While well-executed, the main contribution lies in architectural modularization and the KL-based adaptation heuristic; conceptually, it extends RSSM and variational BC methods rather than introducing a fundamentally new learning paradigm.\n\n- Limited theoretical grounding for AdaMLI: The KL-divergence-based noise metric is intuitive but lacks a formal justification or sensitivity analysis. For example, false positives from high-variance but informative modalities could degrade stability.\n\n- Evaluation scope and generality, minor weakness: Experiments are confined to table-top manipulation; extending to locomotion or force-control tasks would better validate multimodal scalability. All real-robot experiments use a single robot platform (Franka Panda) — it remains unclear if the learned latent structure generalizes across hardware.\n\n- Limited ablation on hyperparameters: The effect of β, γ, and KL thresholds (fo, fc) on policy stability or exploration is not systematically analyzed."}, "questions": {"value": "- Could AdaMLI’s switching mechanism oscillate under mild noise? How stable is it across timesteps and modalities?\n\n- How does the modular latent space scale with the number of modalities (e.g., 10+) — does performance degrade due to GRU bottlenecks?\n\n- Could the per-modality KL divergence be used as an intrinsic exploration signal for self-supervised data collection?\n\n- - Did you try pre-training NOSTRA as a world model (predicting observations) before behavior cloning? Would that improve latent consistency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4QaYcPxFDV", "forum": "OKGcbsGMqc", "replyto": "OKGcbsGMqc", "signatures": ["ICLR.cc/2026/Conference/Submission20721/Reviewer_z4L7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20721/Reviewer_z4L7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760747165893, "cdate": 1760747165893, "tmdate": 1762934123181, "mdate": 1762934123181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "NOSTRA is a novel behaviour cloning approach that makes robot policies more reliable when using multiple sensors, like different cameras, depth, and proprioception. Instead of mixing all sensor information together, it learns a separate latent state for each sensor, so when one sensor becomes noisy or blocked, the model can simply ignore that sensor and use its internal memory to continue acting smoothly. This “latent imagination” idea comes from Dreamer, but here it is applied per-sensor. A simple KL-based rule is used to automatically detect when a sensor becomes unreliable and switch it off. In experiments, this works well—the policy stays stable under occlusions and noise, and also learns better from datasets where some inputs are unhelpful. However, the math in the paper does not fully match the implementation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of this paper is its clear and effective design choice of separating the latent representation by modality, which allows the policy to selectively ignore unreliable sensors and remain stable even when some inputs are noisy, occluded, or uninformative. This per-modality structure makes the overall approach intuitive and easy to reason about, and the paper demonstrates that it leads to robust performance improvements in practice. The experiments are solid and comprehensive, covering multiple simulated benchmarks, heterogeneous datasets, and real robot evaluations, all showing consistent gains. Additionally, the paper is well written, clearly structured, and easy to follow, making the core ideas and implementation details accessible."}, "weaknesses": {"value": "A limitation of the paper is that the mathematical formulation does not fully match the implementation. The method is framed as a joint generative model with an observation likelihood, but this likelihood is effectively removed by assuming infinite variance, which weakens the theoretical justification of the stated ELBO objective. As a result, the actual training loss functions more as a behavior cloning objective with a KL regularizer, rather than a principled variational bound. In addition, the KL-based modality switching mechanism relies on hand-tuned threshold values, and the paper does not analyze how sensitive the performance is to these choices. Finally, the paper does not provide capacity-controlled ablations to separate the benefits of modular latent factorization from the possibility that the model simply has greater latent capacity; controlling for total parameter count or latent dimensionality would be necessary to isolate the architectural contribution."}, "questions": {"value": "1. Mismatch between model and objective.\n   The method is presented as a joint generative model that includes an observation\n   likelihood term p(o_t | z_t), but in practice this likelihood is removed by assuming\n   infinite variance. This makes it unclear what objective is actually being optimized,\n   and whether the final loss should still be interpreted as an ELBO.\n\n2. Why present a joint model if observations are never reconstructed?\n   Since p(o_t | z_t) does not contribute to the training loss, the justification for\n   including it in the generative model is unclear. It may be more precise to present\n   the method directly as a conditional policy p(a_{1:T} | o_{1:T}) with a latent\n   bottleneck, instead of a joint model that is not trained as such.\n\n3. Initialization of the latent prior is unspecified.\n   Appendix B introduces a prior p(z_0), but the training objective does not include\n   a KL(q(z_0) || p(z_0)) term. The paper should clarify how z_0 is initialized and\n   whether any prior over z_0 is actually used in practice during training or inference.\n\n4. The KL regularizer is not the ELBO KL.\n   The loss uses a stop-gradient KL mixture:\n       γ KL(q || stop(p)) + (1 - γ) KL(stop(q) || p)\n   This breaks the variational interpretation of the ELBO. The paper should explain\n   the motivation behind this surrogate objective and clarify that the optimized loss\n   is no longer a strict ELBO.\n\n5. Heuristic modality switching without sensitivity analysis.\n   The KL-based modality switching relies on hand-tuned thresholds and hyperparameters.\n   The paper should provide sensitivity analysis to demonstrate robustness to these\n   choices and to validate the stability of the switching mechanism.\n\n6. Uncontrolled model capacity differences.\n   Because each modality has its own latent vector, the total latent capacity may be\n   larger than in single-latent baselines. The paper should include capacity-controlled\n   comparisons (e.g., matching total latent dimensionality or parameter count) to ensure\n   that improvements are due to the modular design rather than simply increased capacity.\n\nOverall:\n   The paper would score significantly better if these theoretical issues were clarified\n   and linked more directly to the actual training objective and implementation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DPoCxmIlbt", "forum": "OKGcbsGMqc", "replyto": "OKGcbsGMqc", "signatures": ["ICLR.cc/2026/Conference/Submission20721/Reviewer_62T8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20721/Reviewer_62T8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537066779, "cdate": 1761537066779, "tmdate": 1762934122576, "mdate": 1762934122576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors present Nostra: a latent variable model for training imitation learning model from datasets of heterogenous modalities. The problem itself is quite pertinent in the era where large robot models are trained with datasets with many different, somewhat mismatching modalities. The method is justified well, with a structured model that models the seen latents closed loop, and the unseen latents as an open loop dynamics model. Through this prior forcing setup, the model is able to learn from the available modalities, while learning to ignore noisy or not-present modalities. In experiments on simulated tasks, the authors show that the method outperform standard behavior cloning algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The algorithm itself is well justified – the training objective shown in equation 2 foilows naturally from the modeling assumptions set up earlier, and it is great to see that with minimal hyperparameter tuning, this objective leads to good objectives for training.\n2. On the benchmarks shown in the work, the authors outperform standard behavior cloning algorithms on different manipulation tasks.\n3. More impressively, the model is able to robustly predict sensible actions in absence of important modalities, such as visual input.\n4. In table 3, the algorithm also shows adaptive behavior when different modalities have different information content."}, "weaknesses": {"value": "1. Spending a bit more time and space towards analyzing and understanding the real robot results would make this work much stronger in practice. Especially in understand how this method performs with longer-horizon uncertainties that come from different dynamics of the robots vs. reality would be interesting to see.\n2. The primary evaluations are done on simulated robotic tasks, which while impressive can be less than informative on real robot tasks."}, "questions": {"value": "1. What are the training stability implications of training a model with ELBO on such small amount of data and large amount of modalities (20 demonstrations in the real world)?\n2. Currently, the latent transition models are all-to-all, but are there situations where it may be helpful to inject the dependencies as priors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WISPr8iDL9", "forum": "OKGcbsGMqc", "replyto": "OKGcbsGMqc", "signatures": ["ICLR.cc/2026/Conference/Submission20721/Reviewer_x29x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20721/Reviewer_x29x"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147884612, "cdate": 1762147884612, "tmdate": 1762934121904, "mdate": 1762934121904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an approach for robot learning using behavior cloning. The architecture is a state-space model with per-modality latents. The model is evaluated on simulated robotic manipulation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies an important problem (robot control with multi-modal inputs) and presents an interesting approach (state-space models with modality latents)"}, "weaknesses": {"value": "- The motivation for the proposed approach is a bit unclear and would be good to clarify. The paper mentions integrating different modalities, training with subsets of modalities, and noisy modalities which are all related but are not put in perspective.\n- Empirical results are overall limited. The approach is evaluated in fairly simple simulated settings that make it a bit hard to judge the importance of the results and how replicable the findings are across different settings.\n- The proposed approach contains a number of components and is a bit complex relative to the demonstrated gains over simpler methods. It would be good to discuss the relative tradeoffs further."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GBsuTQWRak", "forum": "OKGcbsGMqc", "replyto": "OKGcbsGMqc", "signatures": ["ICLR.cc/2026/Conference/Submission20721/Reviewer_if5D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20721/Reviewer_if5D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762222300553, "cdate": 1762222300553, "tmdate": 1762934121335, "mdate": 1762934121335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}