{"id": "iLWxh1RSEd", "number": 6734, "cdate": 1757993893058, "mdate": 1759897898035, "content": {"title": "ReviveEdit: Robust Sequential Editing via Dominant Subspace Preservation", "abstract": "Sequential knowledge editing in large language models often causes catastrophic collapse of the model’s general abilities, particularly for parameter-modifying methods. Existing approaches attempt to mitigate this issue with heuristic constraints, but they lack a principled understanding of the underlying failure mechanism and overlook the structured impact of edits on model parameters. In this work, we conduct a spectral analysis and identify a key failure mechanism: the progressive corruption of the dominant singular subspace of weight matrices, a low-rank subspace that we show is both crucial for encoding general abilities and highly sensitive to perturbations. Based on this insight, we propose REVIVE, a novel plug-and-play framework that prevents model collapse by explicitly preserving this dominant subspace. REVIVE projects any given update onto the singular vector basis of the original weight matrix and removes all components that would interfere with the protected subspace. This allows new knowledge to be integrated through less critical directions without damaging the model’s core structure. Extensive experiments show that REVIVE substantially outperforms existing methods, maintaining high editing efficacy and preserving general capabilities even under extreme sequences of up to 20, 000 edits.", "tldr": "We propose an dominant subspace protection identification method to ensure the model stability and the success of editing in long sequence editing.", "keywords": ["Sequential Model Editing", "Large Language Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cba7e6d788d66d3fdf04a0afc483c98991df3f41.pdf", "supplementary_material": "/attachment/24ff32ebabf3e7fdbe4be296b6704e1ef00c69a1.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the problem of performance degradation in large language models (LLMs) during sequential knowledge editing (SME).\nThe authors perform a singular value decomposition (SVD) on the model’s parameter matrices and observe that a model’s general capabilities are largely concentrated within its dominant singular subspace. They argue that continuous edits progressively distort this subspace, ultimately leading to model collapse.\nTo address this issue, the paper proposes ReviveEdit, a method that, during each edit, performs projection and filtering operations to constrain updates only to the low-energy directions of the parameter space. This procedure preserves the integrity of the dominant singular subspace and prevents degradation of general abilities.\nExperiments on GPT-J, LLaMA3, and other models, using datasets such as COUNTERFACT, ZSRE, and GLUE, demonstrate that ReviveEdit achieves higher editing success rates and stronger stability compared to prior methods. Notably, even after 20,000 sequential edits, the model retains approximately 86% of its downstream task performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Strengths:**\n\nThe experiments are **comprehensive and well-designed**.\nThey cover multiple models and tasks, including long-horizon sequential editing, and the results are stable and clearly reported. This demonstrates that the authors have invested substantial effort in the experimental evaluation.\n\nThe work also has **clear engineering value**.\nThe proposed method is simple, modular, and can be easily integrated with other model editing frameworks, making it practically useful for improving model robustness in large-scale applications."}, "weaknesses": {"value": "**Weakness:**\n\nThe core idea of this paper — preserving dominant directions in the parameter matrix’s feature or singular subspace — is not novel.\nEarlier works such as Delta-Edit (2024) and O-Edit (2025) have already proposed highly similar motivations from different perspectives, namely low-rank perturbation (Delta Projection) and orthogonal subspace regularization (gradient-space orthogonality).\n\nThe notion of “protecting the dominant subspace” has also been well established in prior research, including studies on low-rank fine-tuning, model compression, and weight perturbation analysis.\nAs a result, this paper’s theoretical contribution is limited, representing more of a formal restatement or spectral reinterpretation of existing ideas rather than a genuinely new conceptual advance.\n\n**Therefore, despite the solid empirical validation, this work does not meet the originality threshold expected for ICLR acceptance.**\n\n[1] O-EDIT: ORTHOGONAL SUBSPACE EDITING FOR LAN GUAGE MODEL SEQUENTIAL EDITING \n\n[2] DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise"}, "questions": {"value": "See **Weankess**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6ijvxV3q1v", "forum": "iLWxh1RSEd", "replyto": "iLWxh1RSEd", "signatures": ["ICLR.cc/2026/Conference/Submission6734/Reviewer_qf6z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6734/Reviewer_qf6z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6734/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760689177605, "cdate": 1760689177605, "tmdate": 1762919020819, "mdate": 1762919020819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes REVIVEEDIT, a framework for robust sequential model editing that prevents model collapse by preserving the dominant singular subspace of parameter matrices. Through spectral analysis, the authors argue that catastrophic degradation during sequential edits stems from the corruption of high-energy singular components that encode general abilities. REVIVEEDIT mitigates this by projecting updates onto the singular vector basis and removing directions that interfere with dominant subspaces.\n\nExperiments across GPT2-XL, GPT-J, and LLaMA3 on COUNTERFACT and ZSRE show substantial gains in both editing efficacy and general ability preservation, even after tens of thousands of sequential edits."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThe paper provides an insightful spectral explanation of why sequential editing leads to degradation, connecting weight structure and general ability loss.\n2.\tREVIVEEDIT is plug-and-play and compatible with existing editing frameworks such as MEMIT and AlphaEdit.\n3.\tStrong empirical validation across multiple models and baselines, including large-scale tests."}, "weaknesses": {"value": "1.\tLimited novelty compared to prior work –The method’s core idea—preserving or constraining updates within structured low-rank subspaces—bears strong resemblance to previous works such as PRUNE and AlphaEdit, which also regulate parameter updates through rank or null-space constraints. The new contribution (dominant subspace preservation via SVD) can be seen as an incremental extension of these ideas rather than a fundamentally new paradigm.\n2.\tThe use of SVD projection and component filtering is technically straightforward. The novelty mainly lies in the empirical finding that high-singular-value directions encode general abilities, but this is somewhat intuitive and overlaps with insights from AlphaEdit.\n3.\tComputational feasibility not fully addressed. Performing SVD for all large matrices is costly; no discussion of efficiency or scalability to very large LLMs (e.g., 70B parameters) is provided.\n4.\tThe argument that dominant subspace corruption is the cause of collapse remains empirical; a stronger theoretical guarantee or causal analysis is missing."}, "questions": {"value": "1.\tDoes REVIVEEDIT require recomputing SVD after each batch of edits, or is it fixed once? How does that affect computational efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bQqYE0pP67", "forum": "iLWxh1RSEd", "replyto": "iLWxh1RSEd", "signatures": ["ICLR.cc/2026/Conference/Submission6734/Reviewer_CBPd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6734/Reviewer_CBPd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6734/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761467162927, "cdate": 1761467162927, "tmdate": 1762919020236, "mdate": 1762919020236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers sequential knowledge editing in LLMs and argues that catastrophic degradation after long edit sequences arises from corruption of the dominant singular subspace of weight matrices. The authors propose REVIVE, a plug-and-play mechanism that i) decomposes each update $\\Delta W$ in the SVD basis of the original weight $W$ and ii) filters out components that involve the top singular directions using an energy threshold $\\tau$. Experiments demonstrate substantial gains across multiple editors and model families, while maintaining robustness for sequences of edits up to 20k."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a coherent explanation that deterioration of general abilities is linked to the dominant singular subspace.\n\n2. REVIVE can be applied to existing editors directly, while acknowledging some added compute and storage for SVD-based filtering.\n\n3. The authors conducted extensive experiments to validate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The core idea—preserving a knowledge subspace—is conceptually close to strands in continual learning and to AlphaEdit in model editing (though AlphaEdit emphasizes feature subspaces while REVIVE emphasizes parameter subspaces). \n\n2. Post-hoc projection may be suboptimal. If the edit intrinsically lies within the top singular subspace, filtering may prevent achieving the desired update.  It would be more principled to include constraint in the solution (similar to AlphaEdit) or even in the optimization (i.e., in finding the target output)."}, "questions": {"value": "1. Are there concrete cases where the desired edit demonstrably lies in the top-energy directions? \n2. With thousands of edits, does the projected subspace become saturated?\n3. Some parameter-preserving baselines should be considered. Recent SimIE is also plug-and-play and reports strong performance, and should be considered as a baseline.\n4. Fig. 4 suggests that editing performance decreases suddenly, while low-rank subspace similarity decays more gradually (Fig. 3). What mechanism explains this discrepancy? \n5. Could you add an ablation that solves the edit with the constraint (e.g., constrained LS / projected gradient) and compares it to post-hoc projection?\n\n[1]. Aging with grace: Lifelong model editing with discrete key-value adaptors, NeurIPS 2023.\n\n[2]. Towards lifelong model editing via simulating ideal editor, ICML 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4ZT8TwkQMo", "forum": "iLWxh1RSEd", "replyto": "iLWxh1RSEd", "signatures": ["ICLR.cc/2026/Conference/Submission6734/Reviewer_x4j5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6734/Reviewer_x4j5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6734/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562238720, "cdate": 1761562238720, "tmdate": 1762919019749, "mdate": 1762919019749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of catastrophic collapse, where LLMs’ general abilities degrade significantly during sequential model editing.  The authors conduct a spectral analysis and find that these general abilities are concentrated in the dominant singular subspace of weight matrices. Based on this insight, they propose REVIVE, a plug-and-play framework that modifies the updates generated by any existing parameter-modifying method (like MEMIT or RECT). This forces the edit to utilize only the less critical directions of the weight matrix, thereby preserving the model's core knowledge. Extensive experiments on GPT2-XL, GPT-J, and LLaMA3-8B show that REVIVE improves the stability and performance of baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The plug-and-play design is a significant practical advantage, as it allows for the direct enhancement of existing editing methods rather than requiring a new editor.\n\n2.The proposed methodology is well-motivated by the initial spectral analysis, which provides a clear explanation for the model collapse phenomenon, i.e., the corruption of the dominant singular subspace during the editing process."}, "weaknesses": {"value": "1.The analysis and method are focused on FFN layers. However, recent work has also explored editing other components, such as attention layers. The authors do not discuss how REVIVE would apply to these new methods.\n\n2.I have concerns about the effectiveness of the method's heavy reliance on SVD.  On larger models, the dimensionality of the weight matrix will be very high. It is not clear that the SVD can be computed efficiently at that scale.\n\n3.The method employs a hard projection that completely nullifies all update components in the dominant subspace. This might prevent the model from successfully editing certain facts that require a minor change to this core subspace."}, "questions": {"value": "1.While the spectral analysis is novel, the approach of projecting updates is similar to those like AlphaEdit. In addition to empirical results, the authors should provide a more detailed conceptual comparison and discussion.\n\n2.The method relies on a static dominant subspace derived from the original model weights. As a significant amount of new knowledge (potentially including foundational facts) is integrated over thousands of edits, is it guaranteed that this initial subspace still accurately represents the model's general abilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j6XDQPwk2w", "forum": "iLWxh1RSEd", "replyto": "iLWxh1RSEd", "signatures": ["ICLR.cc/2026/Conference/Submission6734/Reviewer_jfow"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6734/Reviewer_jfow"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6734/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982294900, "cdate": 1761982294900, "tmdate": 1762963060016, "mdate": 1762963060016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}