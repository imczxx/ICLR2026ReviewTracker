{"id": "7pvJoB4aKO", "number": 13324, "cdate": 1758216499186, "mdate": 1759897444821, "content": {"title": "Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs", "abstract": "Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of **Knowledge Purification**, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.", "tldr": "", "keywords": ["knowledge distillation", "large language model", "LLM routing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/adb4e5c1064319d0dffd13456fd69d9088c9c028.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical problem in multi-teacher knowledge distillation (KD) for Large Language Models (LLMs): the performance degradation that occurs as the number of teachers increases, attributed to knowledge conflicts. The authors introduce the concept of Knowledge Purification, which aims to consolidate the rationales from multiple teachers into a single and coherent rationale. They propose five purification methods in total, Knowledge Aggregation using an LLM to synthesize rationales, three LLM routing approaches (Plackett-Luce Ranking, PLM Classifier, and Similarity-based Router) which select the best single teacher's rationale based on the input question, and RL-based Teacher Selection that uses the student's performance as a reward to choose a teacher. Extensive experiments on commonsense and biomedical reasoning tasks show that routing-based and RL-based methods significantly outperform baselines like TinyLLM, and good generalization to out-of-domain datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The proposal of the concept of knowledge purification\n* Five methods are proposed for a comparative analysis across multiple dimensions (performance, CMV, out-of-domain generalization, etc.), and the analysis (e.g., Table 2) shows the trade-offs of each method\n* The proposed methods consistently outperform baselines, as well as the out-of-domain generalization"}, "weaknesses": {"value": "* The experiments are limited to four selected teacher models\n* Though the overall performance gain is clear, why certain methods work better needs more explanation. It would be helpful if the authors could provide more analyses of pros and cons of each method\n* Excluding the TwT baseline (in Appendix C.3) is reasonable but not fully convincing"}, "questions": {"value": "1. The performance of knowledge aggregation is relatively weak. Have you investigated why a powerful LLM fails to synthesize a good consolidated rationale? Is the issue the aggregation prompt or the task is inherently difficult?\n2. Since the router-based methods perform well and they only need the input question, could it be used to select the teacher per query during the data generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6Ta2ljwzdT", "forum": "7pvJoB4aKO", "replyto": "7pvJoB4aKO", "signatures": ["ICLR.cc/2026/Conference/Submission13324/Reviewer_Tdsi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13324/Reviewer_Tdsi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761204463459, "cdate": 1761204463459, "tmdate": 1762923985262, "mdate": 1762923985262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses multi-teacher knowledge distillation (KD), focusing on the challenges of knowledge conflict (i.e.divergent rationales among teachers) and computational cost when aggregating many teacher models. The authors propose Knowledge Purification, a concept for consolidating the rationales from multiple teacher LLMs into a single rationale to use for distillation. They design five purification methods based on aggregation, routing, and RL-based teacher selection styles."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The introduction of Knowledge Purification reframes multi-teacher distillation from the perspective of rationale integration rather than mere logit or feature averaging, addressing an important problem of multi-teacher KD.\n\n- The paper proposes five distinct purification strategies (aggregation, routing, and RL-based).\n\n- The paper is well written and structured."}, "weaknesses": {"value": "- The experimental evaluation is limited to commonsense and biomedical reasoning datasets. To establish the generality of the proposed approach, it should be extended to a broader range of tasks such as mathematical reasoning, coding, and instruction following.\n\n- Although the paper emphasizes improved efficiency, it does not provide quantitative evidence (e.g., training time, GPU hours) compared to existing multi-teacher methods like TinyLLM or TwT.\n\n- The baselines are limited to step-by-step distillation and TinyLLM, omitting state-of-the-art knowledge distillation approaches such as ABKD, MiniLLM, DistillM, or CKA-KD, which could challenge the claimed improvements."}, "questions": {"value": "See weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9TJXmxXERu", "forum": "7pvJoB4aKO", "replyto": "7pvJoB4aKO", "signatures": ["ICLR.cc/2026/Conference/Submission13324/Reviewer_Wb1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13324/Reviewer_Wb1Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824379678, "cdate": 1761824379678, "tmdate": 1762923984378, "mdate": 1762923984378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical challenge in multi-teacher knowledge distillation (MTKD) for Large Language Models (LLMs): the performance degradation caused by knowledge conflicts among the rationales provided by multiple teacher models. The authors identify that simply increasing the number of teachers in frameworks like TinyLLM does not monotonically improve student performance, often harming it due to conflicting or hallucinated reasoning paths. To solve this, the authors introduce the concept of \"Knowledge Purification\" (KP), which aims to consolidate the rationales from multiple teachers into a single, coherent rationale before distillation. This process mitigates conflicts and provides the student model with a unified source of knowledge. Through extensive experiments on commonsense and biomedical reasoning tasks, the paper demonstrates that KP methods, particularly the Similarity-based Router and RL-based Teacher Selection, consistently outperform strong baselines like TinyLLM and Distilling-Step-by-Step."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of \"Knowledge Purification\" is a direct, intuitive, and novel solution to the clearly identified problem of knowledge conflict in MTKD.\n2. The proposal of five methods from different families (aggregation, routing, RL) provides a thorough exploration of the solution space. This allows for a nuanced comparison of trade-offs between performance, computational cost, and transferability."}, "weaknesses": {"value": "1. As acknowledged in the limitations, the study is constrained to a ensemble of only four teacher LLMs. A critical question remains: how do these methods scale to 10, 20, or even more teachers? While the results with 4 teachers are promising, the effectiveness and computational overhead of, for example, the RL-based method or the PL ranking with a much larger pool of teachers is unexplored.\n2. The study is exclusively validated on multiple-choice question answering tasks. While this is a standard benchmark for reasoning, the generality of Knowledge Purification to other NLP tasks like open-ended generation, summarization, or translation is not established.\n3. While the routers offer excellent inference-time efficiency, the cost of training them is non-trivial (requiring a \"public set\" and 5000 training epochs). A discussion on the trade-off between the cost of training a router versus the cost of repeatedly sampling from all teachers for distillation across multiple tasks would be beneficial."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iCV9j5WABI", "forum": "7pvJoB4aKO", "replyto": "7pvJoB4aKO", "signatures": ["ICLR.cc/2026/Conference/Submission13324/Reviewer_Bt3j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13324/Reviewer_Bt3j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997553364, "cdate": 1761997553364, "tmdate": 1762923983921, "mdate": 1762923983921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes knowledge purification, which consolidates rationales from multiple teacher LLMs into a single rationale to address knowledge conflicts in distillation. Five methods are introduced: knowledge aggregation, LLM routing (Plackett-Luce ranking, PLM classifier, similarity-based router), and RL-based teacher selection. Experiments on commonsense and biomedical reasoning tasks show some performance gains, but the improvements are modest and lack groundbreaking insights."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) The focus on knowledge conflicts in multi-teacher distillation is attractive.\n\n(2) Experiments cover multiple datasets and student models, providing a broad assessment. \n\n(3) The five purification approaches offer varied perspectives, from simple aggregation to learned routing. Experimental results directly show the performance of the methods."}, "weaknesses": {"value": "(1) The core idea of rationale consolidation resembles prior work on knowledge fusion and ensemble distillation. Methods like Plackett-Luce ranking and similarity-based routing are direct adaptations from the existing literature, showing limited substantial innovation. The paper only provide an overall view of the different methods.\n\n(2) Methods like RL-based selection and aggregation involve significant complexity and does not present an obvious theoretical advantage through classical methods.\n\n(3) While the experimental results show some improvements, the gains are often marginal (e.g., ~1â€“3% accuracy boosts in Table 1)."}, "questions": {"value": "1. How does knowledge purification fundamentally differ from applying ensemble methods (e.g., weighted averaging, majority voting or averaging student model) to the teacher rationales? \n\n2. Why CMV was preferred over the information-theoretic metrics like Jensen-Shannon Divergence (JSD), which could quantify the divergence between teacher rationales?\n\n3. Based on the experiments, which methods should we choose when facing the situation of multiple teacher distillation (under different scenarios)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZUBwD4QylW", "forum": "7pvJoB4aKO", "replyto": "7pvJoB4aKO", "signatures": ["ICLR.cc/2026/Conference/Submission13324/Reviewer_EKzP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13324/Reviewer_EKzP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762839811782, "cdate": 1762839811782, "tmdate": 1762923983610, "mdate": 1762923983610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}