{"id": "Eu89qBtufl", "number": 10772, "cdate": 1758181591018, "mdate": 1759897630182, "content": {"title": "Weight Sharing for Graph Structured Data", "abstract": "Weight sharing is a key principle of machine learning.\nWhile well established for regular domains such as images, extending weight sharing to graphs remains challenging due to their inherent irregularity.\nWe address this gap with a novel weight-sharing paradigm that indexes weights directly by graph invariants, i.e., functions preserved under node permutations.\nThis formulation enables systematic reuse of parameters across structurally equivalent subgraphs, providing a principled mechanism for permutation-aware learning.\nTo demonstrate the practicality of the approach, we introduce ShareGNNs, a new family of permutation-invariant graph neural networks that instantiate\ninvariant-based weight sharing in a simple encoder-decoder design.\nWe prove that the expressivity of ShareGNNs is lower-bounded by the discriminative power of the chosen invariant, allowing dynamic control of complexity.\nExperiments on subgraph counting, synthetic, and real-world benchmarks show that ShareGNNs achieve competitive performance on graph-level classification and regression tasks while using only one message-passing layer.\nMoreover, we discuss how the approach enhances interpretability and transferability.", "tldr": "We introduce weight sharing for irregular data and provide an instatiation called ShareGNN which is applied to graph classification and regression.", "keywords": ["weight sharing", "graph neural networks", "message passing", "graph invariants"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b124986cb5c5b0815e80af1bfa5dc0c7f281c6c1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ShareGNNs, a novel family of permutation-invariant graph neural networks that realize invariant-based weight sharing by indexing weights through graph invariants preserved under node permutations. The approach enables systematic parameter reuse across structurally equivalent subgraphs, offering a principled and efficient mechanism for permutation-aware learning with controllable expressivity. Experimental results on synthetic and real-world benchmarks demonstrate that ShareGNNs achieve competitive performance with minimal message-passing depth, highlighting both the practicality and interpretability of the proposed paradigm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The writing is good.\n- The weight sharing on node/node pair is quite novel.\n- The performance on substructure counting is superior."}, "weaknesses": {"value": "- The expressive ability, scalability, and transferability are limited.\n- High computation cost on shortest distance computation.\n- The performance on real-world datasets is limited."}, "questions": {"value": "* The model design relies on node pairs observed in the training set. Consequently, if crucial node pairs are absent during training, generalization and overall effectiveness may be compromised.\n* The computational complexity of computing pairwise distances is underestimated. It is not merely $O(n^2)$, but rather $O(n^2) \\times O(\\text{SP})$, where $O(\\text{SP})$ denotes the complexity of the shortest path algorithm used.\n* Report the actual preprocessing time and compare it against the training time to assess overall efficiency.\n* In implementation, the model constrains the maximum pairwise distance, which inherently restricts long-range interactions and conflicts with the second stated design objective. Moreover, this truncation can exclude important structural patterns in large graphs, raising concerns about scalability.\n* Since all node-pair weights are learned, the model ignores the original edge connectivity of the graph, which may weaken its ability to leverage inherent structural information.\n* Although an edge-labeled variant is proposed, it sacrifices the ability to capture long-range interactions, potentially explaining the weak performance on the ZINC dataset.\n* The baselines used for substructure counting are outdated. Compare with recent SOTAs, such as Graph as Point Set (ICML 2024).\n* Given that node labels already encode 1-WL information, it is unclear why the model underperforms compared to the WL kernel.\n* The substructure counting task leverages pattern-label information, which gives this model an advantage, making the comparison with baselines potentially unfair.\n* What is the performance on large graphs? \n* What is the performance with various model depth?\n* A sensitivity analysis on the number of weight-sharing heads is needed.\n* Finally, the authors should discuss whether the proposed approach can be extended to node classification and link prediction tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d69hGlcdx3", "forum": "Eu89qBtufl", "replyto": "Eu89qBtufl", "signatures": ["ICLR.cc/2026/Conference/Submission10772/Reviewer_NgwU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10772/Reviewer_NgwU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760790930863, "cdate": 1760790930863, "tmdate": 1762921984221, "mdate": 1762921984221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"invariant-based weight sharing,\" a novel paradigm for graph neural networks where learnable weights are indexed directly by graph invariants (e.g., node labels, pairwise distances) rather than being fixed or computed from node features. This approach is designed to enable principled parameter reuse across structurally equivalent regions. The authors instantiate this concept in a new model, ShareGNN, which uses an encoder-decoder architecture to achieve global, transformer-like message passing in a single layer. The paper provides theoretical analysis linking the model's expressivity to the discriminative power of the chosen invariants and reports competitive experimental results on subgraph counting, synthetic benchmarks, and real-world graph classification and regression tasks, highlighting the efficacy of its shallow architecture."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of \"invariant-based weight sharing\" presents a interesting paradigm for parameterizing graph neural networks.\n\n2. The proposed ShareGNN model offers a potential solution to the over-smoothing issues of deep GNNs.\n\n3. The paper supports its claims with a comprehensive evaluation, including theoretical analysis of expressivity and empirical results across a diverse set of benchmarks."}, "weaknesses": {"value": "1. The method is not scalable. It relies on $O(n^3)$ preprocessing (all-pairs shortest paths) and an $O(n^2)$ layer computation. These costs are dismissed as \"negligible\" only because the experiments are confined to extremely small graphs (e.g., avg. 30 nodes). It lacks any experiments on medium or large benchmarks (like OGB).\n\n2. The core mechanism appears to be a combination of existing ideas: it uses R-GCN-style relational indexing (where the \"relation\" is a tuple of invariants) to parameterize a Graph Transformer-style global message passing layer.\n\n3. The main theoretical results (Propositions 1, 2, and 3) merely prove permutation equivariance and invariance. This is a fundamental design requirement for any GNN and not a novel contribution of this specific architecture."}, "questions": {"value": "The ablation studies in Appendix D.5 (e.g., Figures 13 & 18) are very insightful. They show that over 80% of the model's parameters—specifically, the weights corresponding to infrequent structural invariants—can be pruned with almost no loss in performance. Does this not strongly suggest that the proposed parameterization scheme is massively over-parameterized by design, and that the vast majority of learned weights are ultimately unnecessary for the task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3MW0KdgD5b", "forum": "Eu89qBtufl", "replyto": "Eu89qBtufl", "signatures": ["ICLR.cc/2026/Conference/Submission10772/Reviewer_LeQ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10772/Reviewer_LeQ3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620387419, "cdate": 1761620387419, "tmdate": 1762921983389, "mdate": 1762921983389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel Graph Neural Network (GNN) architecture called ShareGNN, whose core idea is to index parameter weights through graph invariants, enabling sharing and generalization across node pairs. The paper demonstrates a certain degree of novelty."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) Introducing graph invariants as a key to parameter sharing is an interesting viewpoint.\n(2) The concept is clearly explained and well-organized, logically connecting the core idea to a new architecture (ShareGNN) and theoretical proofs."}, "weaknesses": {"value": "(1) Although the invariant-based formulation is conceptually interesting, its empirical validation remains modest. Most benchmarks are standard small- to mid-scale datasets. Large-scale or high-density graphs (where O(n²) cost dominates) are not convincingly analyzed.\n(2) The authors claim comparable asymptotic complexity to Graph Transformers but with better parameter efficiency. However, no direct runtime or memory comparison with Graph Transformers is reported. In practice, the O(n²) operations for all node pairs could still be prohibitive for large graphs, unless sparsity or truncation effects are explicitly quantified.\n(3) There is limited exploration of how invariant choices (e.g., WL depth, pattern size, distance) affect performance and cost. It remains unclear which invariants are most influential and whether combinations always yield improvements."}, "questions": {"value": "See in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZQTCszWU45", "forum": "Eu89qBtufl", "replyto": "Eu89qBtufl", "signatures": ["ICLR.cc/2026/Conference/Submission10772/Reviewer_W22x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10772/Reviewer_W22x"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720030733, "cdate": 1761720030733, "tmdate": 1762921983036, "mdate": 1762921983036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ShareGNN, a GNN architecture that leverages parameter sharing based on node labels, substructure patterns, etc’. The authors provide some  theoretical results on permutation invariance and expressiveness and evaluate the method on several graph benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written and easy to follow, with well-structured explanations of the model and its motivation.\n-  The proposed approach is simple and conceptually intuitive, making it straightforward to implement and analyze."}, "weaknesses": {"value": "While the idea of exploring new forms of parameter sharing in GNNs is interesting, I find that the paper does not convincingly demonstrate meaningful advantages over existing architectures, either theoretically or empirically. The authores state five main advantages of ShareGNN:  Adaptivity, permutation awareness, expressiveness, Long-range interactions and Transferability and interpretability.  I have some concerns with regards to each of these.\n\n- Adaptivity: The authors highlight that ShareGNN dynamically adapts to graphs of varying sizes. However, this property is common to almost all GNNs, and it is not clear in what sense ShareGNN is more adaptive. In fact, by assigning parameters to multiple label or pattern types, the model may introduce more parameters per layer than a standard MPNN.\n\n- Permutation awareness: This is a standard property of most GNN architectures, so it does not appear to be a distinguishing feature.\n\n- Expressiveness: The expressivity discussion is unconvincing. The paper essentially shows that if one provides ShareGNN with expressive functions as input, it can compute them — which is true but not particularly insightful. There is no clear theoretical or empirical evidence of meaningfull enhanced expressivity beyond the expressivity of the functions used to define the parameter sharing shceme.\n\n- Long-range interactions: The claim that ShareGNN can propagate information across arbitrary node pairs in a single layer applies equally to most transformer-based GNNs. The paper does not clearly establish how ShareGNN achieves this more effectively.\n\n- Transferability and interpretability: This is the one claimed advantage that could be interesting, but the paper does not provide experiments to substantiate it. Demonstrations on transfer tasks or interpretability analyses would have strengthened this point.\nAdditionally, the experimental section is relatively weak. the aouthors state “standardized evaluation protocols are often missing in graph learning” , but in fact, widely used benchmarks such as ZINC and OGB provide well-defined splits and evaluation pipelines. The paper also uses smaller, less reliable benchmarks and compares primarily to outdated baselines and the performance of ShareGNN on them is modest.\n\nFinally, the novelty claim is somewhat overstated. Parameter sharing in GNNs has been directly explored in prior works such as in [1,2]. Additionally, standard MPNN variants like GIN and GCN, can be thought of as “parameter sharing for GNNS” as they already employ shared weights across nodes. In some ShareGNN configurations (e.g., with WL labels), the model may even have more parameters than a typical MPNN without clear gains in expressivity. \n\n\n[1] Maron et al’. Invariant and equivariant graph networks. 2018.\n\n[2]  Maron et al’.  Provably powerful graph networks. 2019."}, "questions": {"value": "- Include stronger experimental results — for example, on more modern OGB benchmarks — and provide comparisons to relevant baselines, especially recent substructure-counting GNNs (e.g., [3,4]) as well as imroved results on Zinc.\n\n- Provide clearer theoretical and emperical results that demonstrate a non-trivial improvement in expressivity, beyond the ability to compute the pre-defined functions used by ShareGNN.\n\n- Discuss related weight-sharing architectures in greater depth and position ShareGNN more precisely within this context.\nInclude interpretability or transfer experiments to support the claimed advantages.\n\n[3] Bao et al’. Homomorphism counts as structural encodings for graph learning. 2024.\n\n[4] Bouritsas et al’. Improving graph neural network expressivity via subgraph isomorphism counting. 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RnJqJBeqRJ", "forum": "Eu89qBtufl", "replyto": "Eu89qBtufl", "signatures": ["ICLR.cc/2026/Conference/Submission10772/Reviewer_fZtg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10772/Reviewer_fZtg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752933457, "cdate": 1761752933457, "tmdate": 1762921982394, "mdate": 1762921982394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}