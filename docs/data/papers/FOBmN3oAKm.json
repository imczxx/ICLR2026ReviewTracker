{"id": "FOBmN3oAKm", "number": 4785, "cdate": 1757767014491, "mdate": 1759898013149, "content": {"title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling", "abstract": "Existing post-training techniques for large language models are broadly categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking demonstration data but can lead to problematic generalization as a form of behavior cloning. Conversely, RFT can significantly enhance a model's performance but is prone to learn unexpected behaviors, and its performance is sensitive to the initial policy. In this paper, we propose a unified view of these methods and introduce Prefix-RFT, a hybrid approach that synergizes learning from both demonstration and exploration. Using mathematical reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is both simple and effective. Not only does it surpass the performance of standalone SFT and RFT, but it also outperforms parallel mixed-policy RFT methods. A key advantage is its seamless integration into existing open-source frameworks, requiring only minimal modifications to the standard RFT pipeline. Our analysis highlights the complementary nature of SFT and RFT, validating that Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore, ablation studies confirm the method's robustness to variations in the quality and quantity of demonstration data. We hope this work offers a new perspective on LLM post-training, suggesting that a unified paradigm that judiciously integrates demonstration and exploration could be a promising direction for future research.", "tldr": "We propose a hybrid approach to blend supervised and reinforcement fine-tuning.", "keywords": ["LLM Post-training", "Reinforcment Fint-Tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e22294b9152cf4a3694d6472741cf98ea02c0346.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Prefix-RFT, a hybrid approach that blends SFT and RL for LLM post-training. In particular, the method randomly samples a prefix from each expert demonstration and lets the actor model generate the continuation. This hybrid sequence, including off-policy prefix and on-policy continuation, is then used as a rollout sample in the RFT update. To ensure stability, the method applies entropy-based clipping on off-policy prefix tokens and uses a cosine decay scheduler to shorten prefix length over time, creating a curriculum from SFT to RFT. The experiments exhibit superior performance compared to SFT, RFT, and SFT-then-RFT baselines on math and reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic of bridging SFT and RL for better generalized LLM reasoning is significant. \n\n2. Experiments show promising results compared to baselines. \n\n3. Extensive analyses are provided to support the validity of the method."}, "weaknesses": {"value": "1. The presentation can be improved, especially the notations for introducing the method. For example, in Eq. (1), I don't understand what $\\sum_{y_t^i\\sim\\pi_{\\theta_{old}}}$ means, but to guess it might be the sum over $t$. In Eq. (2), again, the subscript is confusing, and it is hard to tell whether the prefix tokens from demonstrations contribute to the gradient from the formula. \n\n2. The paper combines tricks like entropy-based clipping, which can bring notable improvement to RFT on its own [1]. It is questionable whether the promising results come from the tricks or the proposed hybrid approach of Prefix-RFT. \n\n3. There are too many tunable configurations, including entropy clipping ratio, prefix sampling decay scheduler, etc, and they do not seem to be robust. For example, Figure 4 shows that simply changing the clipping ratio to 50% hugely degrades the performance from 50% to <45%. Figure 5 shows that the sampling schedule can also significantly affect the results. These non-robust configurations can bring a huge burden to practitioners.\n\n[1] Wang, Shenzhi, et al. \"Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning.\" arXiv preprint arXiv:2506.01939 (2025)."}, "questions": {"value": "1. What is the principle behind the cosine decay scheduler? Can it be any decaying scheduler, e.g., linear?\n\n2. Is it possible to conduct additional experiments on RFT with entropy-clipping for ablation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fY59UHrknc", "forum": "FOBmN3oAKm", "replyto": "FOBmN3oAKm", "signatures": ["ICLR.cc/2026/Conference/Submission4785/Reviewer_k1KL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4785/Reviewer_k1KL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760941126688, "cdate": 1760941126688, "tmdate": 1762917575037, "mdate": 1762917575037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a finetuning method for language models that combines supervised finetuning (SFT) with reinforcement finetuning (RFT). The proposed method, called Prefix-RFT, initializes the on-policy rollouts of RFT from prefixes of existing demonstrations. The idea is to guide RFT toward exploring more favorable outputs, while maintaining the goal-oriented objective of RFT. Empirical evaluations over math reasoning datasets show that Prefix-RFT outperforms SFT, RFT, and several other finetuning approaches."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The presentation of Prefix-RFT is clear. More broadly, I find the paper to be well-written and relatively easy to follow.\n\n2. How to best combine RFT with demonstrations is an active and important area of research, toward identifying robust language model finetuning guidelines. \n\n3. The empirical analysis in Section 5 helps shed light on the mechanisms underlying Prefix-RFT and its relation to SFT and RFT."}, "weaknesses": {"value": "1. Unfortunately, it seems that the main technical innovation of Prefix-RFT is not new. Aside from the arguably concurrent UFT paper [1], there is an even earlier work proposing to use demonstration prefixes to seed rollouts in policy gradient [2]. These methods are nearly identical, differing mostly in their technical details (e.g., amount of non-prefixed rollouts used in a batch and entropy-based clipping). While the paper does briefly mention [1], it is missing a reference to [2]. Given the vast similarity between the method proposed in [2] and Prefix-RFT, I believe it is necessary to clearly discuss the relation between these two works. \n\n2. In light of [2], the main contribution of this paper is to show that the existing idea of using prefixes of demonstrations, along with a few other heuristics (e.g., entropy clipping) can work well for math reasoning datasets. This can be a worthy contribution if the method is shown to perform significantly better than alternatives. However, since the paper does not report standard deviation across random seeds (or any other measure of statistical significance), it is difficult to assess how robust are the performance gains reported in Table 1.\n\n\n\nReview Summary and Recommendation\n---\n\nOverall, the idea behind Prefix-RFT is intuitive and clearly presented. However, since the core component of Prefix-RFT is not new, I find the contributions of this paper beyond existing works to be rather incremental. I therefore believe that it falls on the borderline. My current rating tends toward rejection, yet I am willing reconsider my assessment if the authors address the relation of Prefix-RFT to [2] and provide evidence that the gaps in performance in Table 1 are significant (e.g., what is the standard deviation of these results across random seeds for training?).\n\n\nAdditional (More Minor) Comments\n---\n\n- Typo in line 36: \"it's\"  should probably be \"It is\".\n\n- Typo in line 56: Period after reference to Liu et al. 2025d should probably be a comma.\n\n\n\n[1] Liu, M., Farina, G., & Ozdaglar, A. (2025). UFT: Unifying Supervised and Reinforcement Fine-Tuning. arXiv preprint arXiv:2505.16984.\n\n[2] Chang, J. D., Zhan, W., Oertell, O., Brantley, K., Misra, D., Lee, J. D., & Sun, W. (2024). Dataset reset policy optimization for rlhf. arXiv preprint arXiv:2404.08495."}, "questions": {"value": "--"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NxNzisVWCp", "forum": "FOBmN3oAKm", "replyto": "FOBmN3oAKm", "signatures": ["ICLR.cc/2026/Conference/Submission4785/Reviewer_Q8Mt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4785/Reviewer_Q8Mt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667200659, "cdate": 1761667200659, "tmdate": 1762917574666, "mdate": 1762917574666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Prefix Reinforcement Fine-Tuning (Pre-FT), a hybrid post-training method that leverages offline prefixes to guide exploration during reinforcement fine-tuning (RFT). By incorporating prefix guidance, Pre-FT expands an LLM’s knowledge boundary beyond that of RFT while mitigating the overfitting and memorization issues observed in supervised fine-tuning (SFT). Experimental results show that Pre-FT outperforms SFT, RFT, and other mixed-policy RFT approaches, and is robust to variations in both the quality and quantity of demonstration data."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Empirical results show that Pre-FT consistently outperforms SFT, RFT, and other mixed-policy RFT methods.\n- Pre-FT demonstrates strong robustness across varying quantities and qualities of demonstration data.\n- The paper is well-written."}, "weaknesses": {"value": "- **Lack of novelty and incremental contribution:** My main concern is that the use of off-policy data to enhance model capabilities has already been explored in several prior works [1, 2, 3]. Moreover, Pre-FT closely resembles UFT [3], which also applies the SFT loss to prefixes and uses RFT for partial continuations. The additional heuristics, such as entropy-based clipping and the cosine decay scheduler, appear too incremental to me.\n- **Pass@1 does not reflect the reasoning capabilities of Pre-FT:** The paper reports only Pass@1 to evaluate the effectiveness of Pre-FT and other approaches. However, Pass@1 alone does not adequately capture whether Pre-FT expands reasoning capabilities beyond SFT and RFT methods [5].\n## References\n[1] Learning to Reason under Off-Policy Guidance. arxiv 2504.14945.\n\n[2] Learning what reinforcement learning can’t: Interleaved online fine-tuning for hardest questions. arXiv:2506.07527v1.\n\n[3] Uft: Unifying supervised and reinforcement fine-tuning. arXiv:2505.16984.\n\n[4] Safety Alignment Should Be Made More Than Just a Few Tokens Deep. ICLR 2025 Oral.\n\n[5] Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?. AI4Math@ICML25 Oral."}, "questions": {"value": "- Why does Pre-FT, by using prefix tokens as additional guidance, help mitigate the language mixing problem in RFT? Are there any experiments demonstrating that Pre-FT exhibits less language mixing compared to RFT?\n- It would be interesting to investigate whether RFT can mitigate the shallow alignment problem in safety alignment [4]. Specifically, by using non-refusal prefixes during Pre-FT fine-tuning, can Pre-FT achieve deeper alignment on later tokens?\n- How does Pre-FT perform compared to SFT and RFT models when evaluated using Pass@$k$ with a large $k$ sampling budget?\n- What is the effect of freezing the prefix (i.e., not updating it) and training only on the partial continuations, or alternatively, updating all tokens in the prefix? How do these variants compare to Pre-FT?\n- While the paper claims that the gradients from offline demonstrations can be significantly larger than those from RFT, empirical evidence supporting this phenomenon is necessary — especially given that the number of offline prefix tokens is much smaller than the number of continuation tokens used for training. The paper should further explain why significantly larger gradients from offline demonstrations could lead to an unstable training process..\n- Why not assign an additional weight to the offline prefix tokens instead of relying on entropy-based clipping?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XIIYlbMXnF", "forum": "FOBmN3oAKm", "replyto": "FOBmN3oAKm", "signatures": ["ICLR.cc/2026/Conference/Submission4785/Reviewer_SL41"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4785/Reviewer_SL41"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829680567, "cdate": 1761829680567, "tmdate": 1762917574246, "mdate": 1762917574246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Prefix-RFT, a hybrid post-training method that integrates supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT) by sampling a demonstration prefix and letting the current policy complete it; the stitched sequence is then optimized with a PPO-style objective so imitation (prefix) and exploration (continuation) are updated in one pass. It frames SFT and RFT under a unified “log-probability gradient” view and adds two practical stabilizers—entropy-based clipping that backpropagates only through high-uncertainty demo tokens and a cosine-decay schedule for prefix length—to prevent dominance of off-policy tokens and mitigate position bias. On math reasoning benchmarks (e.g., AIME, AMC, MATH-500, Minerva, Olympiad) and some general tasks, Prefix-RFT outperforms standalone SFT, RFT, the two-stage SFT→RFT recipe, and concurrent mixed-policy baselines across model sizes. Analyses show it particularly helps on problems where pure RFT stalls, nudging the model toward expert distributions without degenerating into behavior cloning and inducing an example-wise transition from imitation to exploration; ablations confirm the importance of entropy clipping and the scheduling strategy. Overall, the contribution is a simple, easily integrated recipe that operationalizes a principled blend of demonstration guidance and goal-oriented updates for LLM post-training."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper’s originality lies in both its conceptual unification of SFT and RFT under a common gradient view and its pragmatic “prefix sampling” mechanism that stitches an off-policy demonstration prefix to an on-policy continuation while retaining PPO-style stability, delivering a clean bridge between imitation and exploration.   Methodological quality is high: the experimental suite covers diverse math-reasoning benchmarks with clear protocols, compares against relevant baselines, and includes well-designed ablations showing that updating only high-entropy demonstration tokens avoids prefix-driven overfitting, while a cosine decay on prefix length improves training dynamics and final scores.     Clarity is excellent, with intuitive figures and a step-by-step objective that makes the approach easy to adopt.  Finally, the significance is strong: results suggest the method reliably improves reasoning over pure RFT or sequential SFT→RFT and encourages the community to treat post-training as a blended process rather than two disjoint stages."}, "weaknesses": {"value": "The evidence centers on math with exact checker rewards, so robustness under noisier or heuristic feedback remains unclear. There are no non-verifiable math settings (e.g., LLM-graded rationale quality) to test behavior when correctness can’t be deterministically checked. The study also relies primarily on Qwen backbones; results on additional families (e.g., Llama-3) would better assess backbone generality."}, "questions": {"value": "See details in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qVRRe6ytA9", "forum": "FOBmN3oAKm", "replyto": "FOBmN3oAKm", "signatures": ["ICLR.cc/2026/Conference/Submission4785/Reviewer_XmVn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4785/Reviewer_XmVn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055353462, "cdate": 1762055353462, "tmdate": 1762917573842, "mdate": 1762917573842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}