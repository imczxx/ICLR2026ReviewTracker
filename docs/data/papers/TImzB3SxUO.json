{"id": "TImzB3SxUO", "number": 17742, "cdate": 1758280056617, "mdate": 1759897156782, "content": {"title": "Action-Free Offline-To-Online RL via Discretised State Policies", "abstract": "Most existing offline RL methods presume the availability of action labels within the dataset, but in many practical scenarios, actions may be missing due to privacy, storage, or sensor limitations. We formalise the setting of action-free offline-to-online RL, where agents must learn from datasets consisting solely of $(s,r,s')$ tuples and later leverage this knowledge during online interaction. To address this challenge, we propose learning state policies that recommend desirable next-state transitions rather than actions. Our contributions are twofold. First, we introduce a simple yet novel state discretisation transformation and propose Offline State-Only DecQN (OSO-DecQN), a value-based algorithm designed to pre-train state policies from action-free data. OSO-DecQN integrates the transformation to scale efficiently to high-dimensional problems while avoiding instability and overfitting associated with continuous state prediction. Second, we propose a novel mechanism for guided online learning that leverages these pre-trained state policies to accelerate the learning of online agents. Together, these components establish a scalable and practical framework for leveraging action-free datasets to accelerate online RL. Empirical results across diverse benchmarks demonstrate that our approach improves convergence speed and asymptotic performance, while analyses reveal that discretisation and regularisation are critical to its effectiveness.", "tldr": "", "keywords": ["Action free Offline to Online Reinforcement Learning", "Online Reinforcement Learning", "Offline Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e6f62b596a74791650456c58a59a09b94b367996.pdf", "supplementary_material": "/attachment/5f5be3620e5ac17a49855ed947293f057d5d736f.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies a novel and practically motivated setting in reinforcement learning (RL): action-free offline-to-online RL, where the agent must learn from datasets containing only tuples of the form state-reward-next state datasets, without action labels. Such a setting arises naturally in domains like healthcare, finance, and robotics, where action logs may be unavailable due to privacy, storage, or sensor constraints. The paper asks: Can an agent learn useful knowledge from such datasets and transfer it to online learning?\nTo address this, the authors propose a two-stage framework built around a new offline algorithm, Offline State-Only DecQN (OSO-DecQN), that learns state policies rather than action policies. Instead of predicting actions, the algorithm predicts discretised state differences (i.e., the direction of state change) and uses these predictions to guide online RL. \nEmpirically, the authors show that OSO-DecQN pre-trained on action-free datasets can accelerate online learning and improve final performance across a range of continuous- and discrete-control tasks (D4RL, DeepMind Control Suite), outperforming existing action-free baselines (e.g., AF-Guide by Zhu et al., 2023). Ablation studies further highlight the importance of discretisation and regularisation, and theoretical results (Theorem 1–2) provide a discretisation-dependent value approximation bound."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly identifies a real-world gap between existing offline RL methods (which assume full action observability) and practical domains where actions are missing. The authors articulate why action-free learning is challenging and outline an elegant conceptual framework that combines discrete state prediction with conservative regularisation and online guidance. This direction feels both original and valuable for RL research, especially as large unlabelled state datasets become more common.\nThe proposed OSO-DecQN is conceptually simple yet technically grounded. Overall, the algorithm is a clean extension of DecQN into an action-free paradigm.\nThe authors provide a series of formal results bounding the approximation error introduced by state discretisation. In particular, Theorem 1 and Theorem 2 derive the difference between the original and discretised optimal value functions, showing that the discretisation granularity controls the value loss. The proofs (Appendix B) are rigorous and grounded in classical contraction and KL-divergence arguments, enhancing the credibility of the approach."}, "weaknesses": {"value": "Although the integration of discretisation, conservative regularisation, and decoupled Q-learning is novel in the action-free context, each component individually builds on well-known ideas (DecQN, CQL-style penalties, inverse dynamics models).\nThe contribution is therefore more conceptual unification than a fundamentally new RL mechanism.\nWhile the paper claims the IDM is lightweight, its reliability is central to the online guidance mechanism. The IDM must map predicted state differences back to executable actions accurately; errors in this translation could affect results. Although the authors show robustness across architectures (Table 10–11), a more direct comparison of IDM accuracy versus final online performance would better quantify this dependency.\nThe theoretical analysis focuses entirely on the offline discretisation error.\nThere is no corresponding guarantee or convergence analysis for the guided online learning phase (e.g., stability of β-switching or IDM-based guidance).\nSome formal justification, perhaps connecting it to off-policy improvement or safe exploration, would make the work more complete."}, "questions": {"value": "1.\tSince the annealed beta schedule significantly affects results (Fig. 3), analysing it in terms of exploration-exploitation balance could clarify its effect.\n2.\tIt would be better to show a correlation between IDM prediction error and final online performance to support the claim that IDM design is not a major factor.\n3.\tIt would be better if the paper includes comparison of total pretraining + online runtime versus baseline methods to substantiate claims of scalability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QiEI2fNRLi", "forum": "TImzB3SxUO", "replyto": "TImzB3SxUO", "signatures": ["ICLR.cc/2026/Conference/Submission17742/Reviewer_EJh5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17742/Reviewer_EJh5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625798673, "cdate": 1761625798673, "tmdate": 1762927582442, "mdate": 1762927582442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses learning from datasets that lack action labels, containing only (state, reward, next-state) tuples. It proposes OSO-DecQN, which learns policies over discretized states offline and uses them to guide online learning through an inverse dynamics model (IDM)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles practical scenarios where data has no action labels by proposing an offline policy training method that treats a discretized state change as an action and applies offline Q-learning, and demonstrates its effectiveness through various D4RL tasks.\n\n1. The work provides a solution to further improve models pretrained with offline data via online learning by incorporating an IDM (inverse dynamics model), which maps a state change to an action."}, "weaknesses": {"value": "1. Limited comparison with other action-free methods. It seems that in Figure 2, it only compares with the case without pre-training on offline datasets (TD3) and the action-free method (Zhu et al., 2023 [1]). However, given there are more action-free offline training methods [2,3,4], it would be better to compare with all these baselines to fully validate the effectiveness of the proposed action-free training based on state discretization.\n\n2. The presentation of Figure 2 is confusing because although it is for comparing with another action-free offline method (Af-guide), it doesn't directly compare the proposed one and Af-guide. Instead, it is comparing with another baseline (TD3) and indirectly claims that the proposed one is better than AF-guide because it is better than TD3, which I found confusing. It would be better to put all algorithms to be compared in the figure. Also, I wonder if the training/evaluation setup for your method, Af-guide, and TD3 is the same for all experiments.\n\n3. The state discretization seems coarse, which could lose a lot of meaningful state transition information depending on state representation and tasks.\n\n[1] Deyao Zhu, Yuhui Wang, J ̈urgen Schmidhuber, and Mohamed Elhoseiny. Guiding online reinforcement learning with action-free offline pretraining. arXiv preprint arXiv:2301.12876, 2023.\n\n[2] Bohan Zhou, Ke Li, Jiechuan Jiang, and Zongqing Lu. Learning from visual observation via offline pretrained state-to-go transformer. Advances in Neural Information Processing Systems, 36: 59585–59605, 2023.\n\n[3] Hao Luo, Bohan Zhou, and Zongqing Lu. Pre-trained visual dynamics representations for efficient policy learning. In European Conference on Computer Vision, pp. 249–267. Springer, 2024.\n\n[4] Younggyo Seo, Kimin Lee, Stephen L James, and Pieter Abbeel. Reinforcement learning with actionfree pre-training from videos. In International Conference on Machine Learning, pp. 19561–19579. PMLR, 2022."}, "questions": {"value": "1. Zhu et al. 2023 [1] (AF-guide) seems to adopt different approaches for both offline and online training. In Figure 2, is the online and offline training of Af-guide entirely following Zhu et al. 2023? If so, can you provide results for the case of Af-guide (offline) + your method (online) and your method (offline) + Af-guide (online)? This would give a more fine-grained performance comparison of each training process (online/offline).\n\n[1] Deyao Zhu, Yuhui Wang, J ̈urgen Schmidhuber, and Mohamed Elhoseiny. Guiding online reinforcement learning with action-free offline pretraining. arXiv preprint arXiv:2301.12876, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YsfBx7hssn", "forum": "TImzB3SxUO", "replyto": "TImzB3SxUO", "signatures": ["ICLR.cc/2026/Conference/Submission17742/Reviewer_5mWV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17742/Reviewer_5mWV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630161261, "cdate": 1761630161261, "tmdate": 1762927582063, "mdate": 1762927582063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new offline-to-online reinforcement learning (RL) framework designed for action-free offline datasets. The authors introduce a discretized state representation that serves as an action surrogate, enabling the training of a discretized Q-function purely from state transitions. During online learning, an inverse dynamics model (IDM) is trained to map the predicted next-state differences into executable actions. A policy-switching strategy blends the IDM-based actions with the online policy, while a regularization term is used to stabilize and constrain the offline Q-function.\nExperiments on D4RL and the Action-Factorized DeepMind Control Suite demonstrate faster convergence and better asymptotic performance for both TD3 and DecQN baselines. Ablation studies further show that both discretization and regularization are crucial for performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is conceptually clear and technically sound, addressing a problem where offline datasets lack action labels.\n2. The methodology is well-designed, combining existing offline RL techniques with novel discretization and regularization components in a coherent way.\n3. The ablation studies are comprehensive and clearly demonstrate the contribution of each component."}, "weaknesses": {"value": "1. The motivation for the action-free offline data setting is not clearly justified. The paper should better explain when and why such data would realistically occur.\n2. The examples in the introduction are not entirely convincing. The most plausible application would be robotics from video, but the experiments only consider relatively small state spaces (up to 78 dimensions), which limits the realism of the claim.\n3. The baseline coverage is limited — the paper primarily compares against a single online RL algorithm (TD3) and lacks comparison with stronger modern offline-to-online or action-free RL baselines."}, "questions": {"value": "1. Are there plans to extend this framework to visual or multimodal inputs, where the state-space dimension exceeds 1000?\n2. Would the method still be effective if the offline dataset and the online environment come from slightly different dynamics distributions?\n3. How many offline data samples are used in training, and how does the amount of offline data influence the overall training stability and performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KLIcezSp7H", "forum": "TImzB3SxUO", "replyto": "TImzB3SxUO", "signatures": ["ICLR.cc/2026/Conference/Submission17742/Reviewer_KioB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17742/Reviewer_KioB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783337701, "cdate": 1761783337701, "tmdate": 1762927581353, "mdate": 1762927581353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors formalise the setting of action-free offline-to-online RL, where agents must learn from datasets consisting solely of (state, reward, next state) tuples and later leverage this knowledge during online interaction. To address this challenge, they propose learning state policies that recommend desirable next-state transitions rather than actions. First, the authors introduce a simple yet novel state discretisation transformation and propose Offline State-Only DecQN (OSO-DecQN), a value-based algorithm designed to pre-train state policies from action-free data. OSO-DecQN integrates the transformation to scale efficiently to high-dimensional problems while avoiding instability and overfitting associated with continuous state prediction. Second, they propose a novel mechanism for guided online learning that leverages these pre-trained state policies to accelerate the learning of online agents. Together, these components establish a scalable and practical framework for leveraging action-free datasets to accelerate online RL. Empirical results across diverse benchmarks demonstrate that their approach improves convergence speed and asymptotic performance, while analyses reveal that discretisation and regularisation are critical to its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation for action-free RL is strong, while the explanation for the motivation is clear in the paper.\n2. The authors propose an algorithm with novel technical designs.\n3. The experimental results are generally good. The ablation study part is helpful."}, "weaknesses": {"value": "1. The paper directly decouples the Q function. Under the case where the real Q function depends on the correlation between different dimensions, could this approximation perform well? Meanwhile, for the argmax of the action, I am wondering what is the choice when $\\mathcal{A}$ is not a product space. For instance, if $\\mathcal{A}$ is a unit ball, how to define the argmax on each dimension?\n\n2. For the training loss of IDM, could you please provide some motivation for that? Does the success of such approach implicitly depend on the Lipschitzness of the Q value function over actions?"}, "questions": {"value": "Please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hExCsFOLDv", "forum": "TImzB3SxUO", "replyto": "TImzB3SxUO", "signatures": ["ICLR.cc/2026/Conference/Submission17742/Reviewer_cTFm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17742/Reviewer_cTFm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979476912, "cdate": 1761979476912, "tmdate": 1762927575615, "mdate": 1762927575615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an algorithm for learning value functions from action-free datasets in an offline manner and using these value functions to improve the learning speed in a subsequent online phase. The key parts are discretization method for the state space to assist learning in the offline phase. Experiments with standard offline RL datasets (removing action information) demonstrate the utility of the method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The problem setting is interesting, action-free datasets would seem to be more common in practice and understudied.\nThe discretization technique for the state space and converting to actions using an IDM is simple and surprisingly effective."}, "weaknesses": {"value": "Generally, there are no glaring issues but some experimental and design choices are unclear to me.\nI have included these questions below in the \"Questions\" section.\nMany of these are clarification questions or ablation suggestions and I am happy to increase my score after having more information."}, "questions": {"value": "- Why is DecDQN specifically chosen tobe used rather than a simpler DQN variant given that the method seems to be agnostic to the value-based learning algorithm?\n\n\n- For the IDM's loss, the $L_1$ loss is used. Why not use the Huber loss if robustness is an issue? \n\n- How does the performance of the method compare to offline-to-online methods that do have access to action information in the offline dataset? We would expect worse performance of course but it would be interesting to see how large the gap really is, it may turn out to be quite small.\n\n\n- Concerning the discretization step for the state space, it has been found for value learning that regression and predicting a continuous output is inferior to using classification-style losses (the histogram loss) [1]. Have you tried using the histogram loss instead? The loss converts continuous regression targets into a classification target. Then predictions can be done by outputting probabilities over discrete outputs in the support and taking an average. It can be used with a variable number of support points.\nThis method could also potentially be used for the inverse dynamics model.\n\n\n- Line 277: Why do we need to use generated actions from the policy to relabel transitions in the offline dataset with the current policy to train the IDM? This would seem to induce a feedback loop since the IDM is trained on its own actions. Could an ablation be run on including this part or not?\n\n\n- Is the $\\epsilon$ hyperparameter in the discretization important? In the appendix, it indicates it is set to be 1e-4 which seems quite small but an ablation showed that this was important to include for some environments.\n\n- How important is the policy-switching strategy? How sensitive is it to the $\\beta$ parameter? \n\n- Does the $Q(s, \\Delta s)$ value network also get updated during the online phase?\n\n\n- It is fairly surprising that an inverse dynamics model would be able to use the $\\Delta s$ discretized state change effectively since there would be many actions that could map to the same discretized state difference. Why do you think this is effective? Is this exploiting some structure specific to simulated robotics tasks? \nAn experiment that would help clarify this is to test whether following the action given by the IDM actually leads to the expected dicretized state change. Verifying that the IDM can do this would be interesting.\n\n\nMinor points (not impacting the score):\n- The notation for the discretized state is introduced as $\\delta(s,s')$ but in other places it is $\\Delta s$ that is used. \n\n- In Theorem 1, M is used to denote the number of bins but it is also used to denote the MDP. It would also be clearer to include the definition of $M$ and $H$ in the theorem statement. \n\n- Table 1: The font is a bit small. \n\n[1] Stop Regression: Training Value Functions via Classification for Scalable Deep RL"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EcwwfKxw7h", "forum": "TImzB3SxUO", "replyto": "TImzB3SxUO", "signatures": ["ICLR.cc/2026/Conference/Submission17742/Reviewer_cXZg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17742/Reviewer_cXZg"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762223382538, "cdate": 1762223382538, "tmdate": 1762927574929, "mdate": 1762927574929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}