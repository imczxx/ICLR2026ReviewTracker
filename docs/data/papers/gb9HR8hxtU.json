{"id": "gb9HR8hxtU", "number": 20124, "cdate": 1758302775053, "mdate": 1763339986002, "content": {"title": "Evidence for Limited Metacognition in LLMs", "abstract": "The possibility of LLM self-awareness and even sentience is gaining increasing public attention and has major safety and policy implications, but the science of measuring them is still in a nascent state. Here we introduce a novel methodology for quantitatively evaluating metacognitive abilities in LLMs. Taking inspiration from research on metacognition in nonhuman animals, our approach eschews model self-reports and instead tests to what degree models can strategically deploy knowledge of internal states. Using two experimental paradigms, we demonstrate that frontier LLMs introduced since early 2024 show increasingly strong evidence of certain metacognitive abilities, specifically the ability to assess and utilize their own confidence in their ability to answer factual and reasoning questions correctly and the ability to anticipate what answers they would give and utilize that information appropriately. We buttress these behavioral findings with an analysis of the token probabilities returned by the models, which suggests the presence of an upstream internal signal that could provide the basis for metacognition. We further find that these abilities 1) are limited in resolution, 2) emerge in context-dependent manners, and 3) seem to be qualitatively different from those of humans. We also report intriguing differences across models of similar capabilities, suggesting that LLM post-training may have a role in developing metacognitive abilities.", "tldr": "", "keywords": ["LLM", "Metacognition", "Evaluations", "AI Safety", "Self-Awareness", "Consciousness", "Model Welfare"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ce77e6802f46a0760a5f36a22f30ade06d94628.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates metacognitive abilities in LLMs, taking inspiration from research on metacognition in nonhuman animals. The authors adopt two task paradigms that measure metacognition without explicit self-reports, and find somewhat mixed results: recent models show modest success, but there is no clear, consistent evidence that the tested models are showing strong metacognitive abilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, I liked this paper. The contribution of evaluating metacognition without self-reports is a nice addition to the literature. The writing is clear, the analyses are well-motivated and sound, and the conclusions are appropriate given the findings. I appreciated that the authors included surface features in the regression model in Section 2.4. The expected outcomes and logic laid out in Section 3.2 are also great, and lacking in most AI papers these days."}, "weaknesses": {"value": "I like that the authors eschew self-reports, but the paper could be strengthened by providing more motivation for using implicit measures of metacognition. The current motivation in the second paragraph of the Introduction feels a bit weak. For example, I don’t understand the argument laid out by the following sentence: “Because LLMs have vast memory capacities and are trained on a nontrivial fraction of everything humans have ever written with the singular goal of generating plausible and pleasing responses, they are almost preternaturally ill-suited to trustworthy self reports” (l. 045-048). Why does having a large memory capacity, or being trained on a large amount of data, make the model ill-suited for self-reports? Since implicit measures of metacognition might introduce additional task demands, I think a bit more work needs to be done to motivate them."}, "questions": {"value": "A few minor suggestions/questions:\n- Section 2.1 (“Models”) is quite hard to parse. This information could maybe be better presented in a table, with the main text just highlighting the high-level motivation for how these models were chosen, and any theoretically relevant differences between them.\n- In the Discussion, the authors write: “Speculating, as with the lack of advantage for factual knowledge in metacognition, the relatively poor performance in the self-modeling task may relate to the fact that LLMs don’t have the equivalent of the hippocampus, which in mammals subserves both the explicit recollection of facts and the ability to simulate one’s own behavior” (l. 473-476). This claim feels like a big logical jump. I doubt the authors are implicitly claiming that models need brain-like functional architectures to achieve mammals’ abilities, but this claim hints at that. Also, to my knowledge, we just don’t know (without further mechanistic studies) whether LLMs have hippocampus-like “regions” or not.\n- You might want to check out relevant papers by Song et al.: “Language Models Fail to Introspect About Their Knowledge of Language” (COLM 2025), and “Privileged Self-Access Matters for Introspection in AI” (preprint 2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VEnBqJzvVs", "forum": "gb9HR8hxtU", "replyto": "gb9HR8hxtU", "signatures": ["ICLR.cc/2026/Conference/Submission20124/Reviewer_yLeh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20124/Reviewer_yLeh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835781534, "cdate": 1761835781534, "tmdate": 1762933022806, "mdate": 1762933022806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces two behavioral paradigms—Delegate Game and Second Chance Game—to test metacognition in LLMs without relying on self-reports. In Delegate, a model must decide to answer a question or delegate to a teammate; metacognition is inferred if delegation correlates with model-internal confidence (proxied by baseline correctness or token-probability entropy) after controlling for surface difficulty cues. In Second Chance, a model is told its previous answer was wrong and must re-answer; metacognition/self-modeling is inferred if it changes appropriately beyond neutral-prompt change rates and beyond alternative strategies (random choice, implausible-option choice, added noise). Experiments span many frontier models released since 2024 and two datasets (GPQA and SimpleQA) plus format-converted variants (GPSA and SimpleMC). The main finding: limited, context-dependent evidence for metacognitive abilities—partial correlations typically ≤0.3 in Delegate (correctness) and up to ~0.5 using entropy; Second Chance shows modest, model-specific gains after ruling out alternatives for some OpenAI models. The authors argue post-training may shape these abilities and provide a public code release."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Careful design and statistical controls target internal confidence rather than surface difficulty or stylistic priors.\n\n2. (confidence-use vs. self-modeling) provide complementary views; the figure suite clearly visualizes small but significant effects. \n\n3. Breadth of models and format-matched datasets (MC and short-answer) aid generality claims; reproducibility details and code availability help follow-on work."}, "weaknesses": {"value": "1. Effect sizes are small and variable. Delegate partial correlations peak around 0.3 (correctness) / ~0.5 (entropy), and elicitation is inconsistent across formats and models; some provider-specific RLHF “personalities” (over-answering) confound interpretation.\n\n2. Short-answer scoring uses LLM-as-judge. Although cross-provider panels are used, human adjudication or stricter exact-match normalization would strengthen claims.\n\n3. Tasks are QA-centric and relatively short context; it’s unclear whether effects hold for planning-heavy, long-horizon, or tool-use settings."}, "questions": {"value": "1. How sensitive are Delegate/Second-Chance effects to minor wording or teammate-history changes?\n\n2. Can you harmonize decoding across models (e.g., deterministic MC with calibrated proxies) to reduce comparability confounds?\n\n3. Could you run the same paradigms with humans (and report comparable partial correlations/change-lifts) to contextualize effect sizes?\n\n4. Do findings persist on chain-of-thought-heavy reasoning or multi-step tool-use tasks where internal planning signals might be stronger?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Kz7hL55tlh", "forum": "gb9HR8hxtU", "replyto": "gb9HR8hxtU", "signatures": ["ICLR.cc/2026/Conference/Submission20124/Reviewer_WzKm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20124/Reviewer_WzKm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944332576, "cdate": 1761944332576, "tmdate": 1762933022006, "mdate": 1762933022006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores whether there is evidence that frontier LLMs have metacognitive abilities largely through behavioral evaluations. More specifically, the authors investigate self-modeling using two behavioral paradigms: the delegate game and the second chance game. They define self-modeling as the ability to monitor and control one's internal states and propose that their evaluations inspired by non-human animal studies are more reliable than self-reports. Their results suggest limited evidence of graded, metacognitive abilities in LLMs, which the authors suggest are qualitatively different from human metacognition."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is original in its proposal of a novel non-linguistic methodology from studies in non-human animals for LLMs to evaluate self-modeling abilities. In particular, the fact that the method bypasses the need for self-reports from the LLMs makes it a significant contribution in the right direction for designing evaluations of this kind. Although the results are suggestive at best, the methodology is clear and the analyses sound."}, "weaknesses": {"value": "The paper's central claim is finding \"evidence for limited metacognition\", but the supporting evidence is consistently described by the authors themselves as \"limited in resolution\". Given the modest effect, it remains highly plausible that other subtle, non-introspective strategies (e.g., learned heuristics that associate \"your answer was incorrect\" with \"pick the next most likely token\") could be responsible for the small performance lift. The paper does not provide a strong affirmative case for why self-modeling is a more parsimonious explanation than some unknown, simpler heuristic. Additionally, the authors don't address what seems like a fundamental question about whether it even makes sense to ask this question about self-modeling or awareness given the architecture that LLMs run on (i.e., lack of recursion, feedback connections, and so on). Finally, some more work on situating the behavioral results using mechanistic interpretability would solidify the findings if further support was found there."}, "questions": {"value": "- What might be some other heuristics that you have considered might explain the behavior of the modest effect presented in the paper but are difficult to evaluate, and why? \n- What does it mean for an LLM to have a \"partial\" self-modeling ability? Your results and discussion highlight the differences with human metacognition, but it's not clear what it means if a model is not acting fully in accordance with what you would expect from an agent that has metacognitive abilities.\n- What kinds of mechanistic interventions would provide stronger evidence that the models are indeed demonstrating metacognitive abilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ifw6Jia2te", "forum": "gb9HR8hxtU", "replyto": "gb9HR8hxtU", "signatures": ["ICLR.cc/2026/Conference/Submission20124/Reviewer_WeDC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20124/Reviewer_WeDC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952465492, "cdate": 1761952465492, "tmdate": 1762933021167, "mdate": 1762933021167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}