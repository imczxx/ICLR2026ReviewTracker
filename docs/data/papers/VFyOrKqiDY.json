{"id": "VFyOrKqiDY", "number": 22314, "cdate": 1758329549715, "mdate": 1759896872871, "content": {"title": "RoboPhD: Self-Improving Text-to-SQL Through Autonomous Agent Evolution", "abstract": "We present RoboPhD, a system where AI agents autonomously conduct research to improve Text-to-SQL performance. RoboPhD implements a closed-loop evolution cycle with three coordinated components: a Database Analysis agent that profiles schemas without seeing test questions, a SQL Evaluation agent that generates executable queries conditioned on analysis and evidence, and an Evolution agent that designs new versions of the other two agents based on performance feedback. The Evolution agent employs weighted random sampling from multiple evolutionary strategies—including refinement, research-driven adaptation, and error-focused improvement—ensuring diverse exploration paths. Central to the framework is an ELO-based selection mechanism, which enables survival-of-the-fittest dynamics across competing agents while handling non-transitivity in performance. \n    Starting from a simple baseline, RoboPhD evolves agents through iterative refinement, discovering how to analyze databases to facilitate Text-to-SQL, while also evolving a sophisticated prompt for SQL Evaluation that covers numerous common pitfalls such as proper ordering of columns, when to use COUNT() vs SUM(), etc.-—without being explicitly programmed for them.\n    The full system achieves 68.6\\% execution accuracy on the BIRD test benchmark, demonstrating that AI can autonomously generate hypotheses, run experiments, and refine techniques.", "tldr": "RoboPhD introduces an ELO-driven evolutionary framework where autonomous agents co-evolve database analysis and SQL generation strategies to improve Text-to-SQL.", "keywords": ["Text-to-SQL", "evolutionary algorithms", "autonomous AI research", "automated prompt engineering", "Program synthesis"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a9a9ba13078dd00a6159a89257b932751db3b5f7.pdf", "supplementary_material": "/attachment/bebb44494fb5b68dd0d0902c2d7491737acd8fa0.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes RoboPhD, an iterative self-evolving NL2SQL pipeline at test time. It consists of an offline profiling agent to understand databases and schemas, an online generation agent to generate SQL code, and an evolution agent to provide feedback for building new versions of agents. The full system is evaluated with Claude Code and has ~2% absolute improvement compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method completely relies on test-time optimization, which is a compelling story if one cares about cost.\n- The goal of building an autonomous domain independent research agent is ambitious.\n- The most part of this paper is well-written and easy to understand."}, "weaknesses": {"value": "- The performance improvement with the proposed method seems to be marginal, especially when compared with other methods in the BIRD-bench leaderboard. Given the scale of the model and complexity of the framework, ~2% absolute improvement doesn’t seem to justify 80 iterations of complex evolution.\n- The proposed methodology with prompt optimization and tool evolvement doesn’t seem to be capable of fundamentally resolve the challenges for NL2SQL, e.g. schema linking errors with ambiguous column/table names, logic errors with missing DISTINCT or NOT NULL keywords that are vaguely implied by the users. Those often require deeper understanding on the tables or stronger reasoning capabilities.\n- The claim of RoboPhD being an autonomous AI research framework is exaggerated in many ways and the analogy with advisor/graduate student is misleading. I don’t see any examples showing the agent can “conduct systematic research” It is more like a trial-and-error pipeline instead of principled research cycle that can generate innovative ideas. With the proposed algorithm, I am concerned that the LLM may learn superficial experiences from one random batch which might not be generalizable to other batches and test data.\n- Random selection on hand-crafted evolution strategies is handwavy and might have limited the performance of the framework.\n- Lack of proper quantitative comparison with prompt optimization work since one of the new contributions of this paper lies in co-optimization of prompts and tools. I don’t see how tool evolving improves the performance along with the prompt optimization."}, "questions": {"value": "- It is unclear to me how research-driven evolution strategy improves the agents given many top performers on BIRD-bench leaderboard applies systematic orchestration with different components that might involve multiple fine-tuned embedding and/or generation models (e.g. CHASE-SQL, Arctic-R1 or Reasoning-SQL). Is tool call with python code enough for achieving the goal?\n- How to make sure the fix to the errors are generalizable, especially when a lot of NL2SQL errors come from schema linking?\n- Can authors provide more details on how refinement strategy works and how it is different from other strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ec9MuzYVlq", "forum": "VFyOrKqiDY", "replyto": "VFyOrKqiDY", "signatures": ["ICLR.cc/2026/Conference/Submission22314/Reviewer_NFLz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22314/Reviewer_NFLz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761370810306, "cdate": 1761370810306, "tmdate": 1762942165454, "mdate": 1762942165454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RoboPhD, a novel framework where AI agents autonomously conduct research to improve their own performance on the Text-to-SQL task. The system employs a closed-loop evolutionary cycle composed of three coordinated agents: a Database Analysis agent, a SQL Evaluation agent, and an Evolution agent. The Evolution agent uses a portfolio of strategies (e.g., refinement, error-analysis, and adapting insights from academic papers) to generate new, improved agents. A key component of the framework is an ELO-based selection mechanism, which manages a competitive ecosystem of agents, enabling \"survival-of-the-fittest\" dynamics. Starting from a simple baseline, the system autonomously evolves agents that discover sophisticated Text-to-SQL strategies, achieving a significant, measurable performance gain and reaching a strong 68.6% execution accuracy on the BIRD test benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The primary strength of this paper is the innovative concept of an \"AI researcher\" framework that automates the cycle of hypothesis, experimentation, and refinement. The three-agent architecture and the use of an ELO rating system for agent selection are elegant and technically sound.\n2. The paper demonstrates that its framework can autonomously achieve a measurable performance gain (~2-2.6% absolute accuracy) over baselines on a challenging benchmark. This provides concrete evidence for its central claim.\n3. The case study on the champion agent's evolution (Section 4.3) is also a strength. It provides a clear narrative of how the system learns, adding a deep layer of credibility and insight that goes beyond just reporting final scores."}, "weaknesses": {"value": "1.  The most significant weakness is the lack of systematic ablation studies on the evolution strategies. The authors acknowledge this (lines 450-452), but without this analysis, it is difficult for the reader to understand the marginal contribution of each component (e.g., \"Research-Driven\" vs. \"Error-Focused\"). This is a critical piece of analysis needed to fully validate the framework's design.\n\n2. The framework's success is demonstrated on a single, high-end proprietary model (Claude Opus 4.1). This raises questions about whether the autonomous research capability is a generalizable property of the framework or highly dependent on the specific, latent abilities of this particular LLM.\n\n3. The inherent stochasticity of the evolutionary process and LLM API calls, while acknowledged by the authors, may pose challenges for exact replication of the results and the specific evolutionary paths discovered."}, "questions": {"value": "1. To strengthen the paper, could you provide an ablation study on the evolution strategies? For instance, what is the final performance if the \"Research-Driven\" strategy is disabled? This would help isolate the contribution of de-novo discovery versus literature adaptation and is crucial for increasing my confidence in the soundness of the specific framework design.\n\n2. How sensitive is the RoboPhD framework to the choice of the underlying LLM for the Evolution Agent? Have you conducted any preliminary experiments with other models (e.g., leading open-source models like Llama-3-70B) to assess if this autonomous research capability is a general property of current SOTA models? Answering this would greatly clarify the scope and generalizability of your contribution.\n\n3. The use of the ELO system is well-justified for handling non-transitivity. Can you provide a concrete example of a \"rock-paper-scissors\" dynamic observed during your experiments? This would powerfully illustrate the practical necessity of this component."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "acYjFYGIUq", "forum": "VFyOrKqiDY", "replyto": "VFyOrKqiDY", "signatures": ["ICLR.cc/2026/Conference/Submission22314/Reviewer_Ccc2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22314/Reviewer_Ccc2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461446584, "cdate": 1761461446584, "tmdate": 1762942165126, "mdate": 1762942165126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RoboPhD, an autonomous AI agent system designed to improve Text-to-SQL performance without human intervention. The system integrates three agents, including a Database Analysis Agent that profiles database schemas offline, a SQL Evaluation Agent that generates executable SQL queries, and an Evolution Agent that designs new agents based on performance feedback. The Evolution Agent employs ELO-based evolutionary selection and multi-strategy weighted random sampling to guide exploration across refinement, research-driven, and error-focused strategies. The system achieves 68.6% execution accuracy on the BIRD benchmark."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The closed-loop system combines evolution, evaluation, and analysis agents, allowing agents to learn from their own experiments without human intervention.\n\n2. The use of ELO ratings as an active selection mechanism in evolutionary optimization is well justified.\n\n3. The core algorithm (Algorithm 1: RoboPhD Evolution Cycle) is clearly written and well described."}, "weaknesses": {"value": "1. Experiments are only conducted on very simple baselines, text-to-SQL literature is not considered. The current SOTA text-to-SQL method has achieved more > 80% execution accuracy on the BIRD leaderboard [1]. I don't want to be harsh, but this work is obviously far away from latest text-to-SQL research, when the cost is also high.\n\n2. In addition, the evaluation is very shallow.  Experiments are conducted only on a single benchmark, limiting the generalizability of the claims. The evolution analysis in sec 4.3 only provides a case study. The paper mentioned that the detailed ablation study will be provided in \"forthcoming extended version\", which is also very weird.\n\n3. There are many overclaimed \"big words\" in the paper writing (e.g., bullets in the introduction). The main contribution of this paper is not clear.\n\n[1] https://bird-bench.github.io/"}, "questions": {"value": "I highly recommend that this paper include related text-to-SQL works in the evaluation. And highlight the core novelty and contribution under the literature."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A2wHg9TYPp", "forum": "VFyOrKqiDY", "replyto": "VFyOrKqiDY", "signatures": ["ICLR.cc/2026/Conference/Submission22314/Reviewer_kENo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22314/Reviewer_kENo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525516434, "cdate": 1761525516434, "tmdate": 1762942164581, "mdate": 1762942164581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents RoboPhD, a framework designed to autonomously improve Text-to-SQL performance through iterative agent evolution. The system includes a database analysis agent, an SQL generation agent, and an evolution agent that refines the others based on ELO-ranked performance feedback. The process aims to mimic a “PhD-style” research loop where agents propose, test, and refine hypotheses. Experiments on the BIRD benchmark show an improvement from around 66% to 68.6% execution accuracy after 80 iterations. The authors claim that this demonstrates a step toward autonomous research and self-improving AI systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Creative and well-written. The “autonomous PhD” metaphor and clear structure make the paper engaging and easy to follow.\n- Complete system implementation. The framework runs end-to-end with concrete results and released configurations, showing technical feasibility.\n- Innovative use of ELO-based evolution. Adapting ELO scoring for agent selection is a neat and transferable idea.\n- Potentially inspiring direction. The paper contributes to the growing discussion on self-improving LLM systems and agentic evolution."}, "weaknesses": {"value": "- Insufficient experiments and analysis. Evaluation is limited to a single dataset (BIRD) and two short runs, with no ablations, variance reporting, or comparisons to prompt-optimization baselines (e.g., DSPy, OPRO). The 2% improvement could easily stem from randomness.\n- Overstated claims. The work demonstrates automated prompt search within a fixed pipeline, not genuine autonomous “research.” The conceptual framing exceeds what the evidence supports.\n- Limited novelty beyond Text-to-SQL. Similar self-evolving or self-optimizing agent frameworks have been proposed in other domains (e.g., AI Scientist, TextGrad, LLM-as-Optimizer). The novelty lies mainly in applying these ideas to Text-to-SQL.\n- Lack of analytical depth. The evolution analysis is narrative and anecdotal (iter2 -> iter12 -> iter60) without quantitative explanations or systematic error breakdowns.\n- No cross-domain validation. The framework’s generality remains untested; it is unclear whether the method extends beyond the BIRD setting."}, "questions": {"value": "The experimental evidence is too limited to support its strong claims of autonomy and self-improvement. I'd like to see the experimental results on newer datasets like Spider 2.0 or BIRD Interact"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UEgRGJmVLJ", "forum": "VFyOrKqiDY", "replyto": "VFyOrKqiDY", "signatures": ["ICLR.cc/2026/Conference/Submission22314/Reviewer_vC5u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22314/Reviewer_vC5u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137110530, "cdate": 1762137110530, "tmdate": 1762942164051, "mdate": 1762942164051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}