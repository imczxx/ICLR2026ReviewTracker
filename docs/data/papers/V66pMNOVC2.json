{"id": "V66pMNOVC2", "number": 5359, "cdate": 1757904094283, "mdate": 1763099876443, "content": {"title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving", "abstract": "Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID\nand FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding.", "tldr": "", "keywords": ["generative model", "world model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/036b30bcf7a10a18f82dbdbd455b36aa5b97914c.pdf", "supplementary_material": "/attachment/8db641432b916000a33319eb6724b71614039954.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces CVD-STORM, which employs STORM-VAE augmented with a Gaussian splatting reconstruction for a cross-view video diffusion model. The approach enables long-horizon, controllable multi-view generation and supports direct 4D spatiotemporal scene reconstruction from the generated latents. On the nuScenes dataset, the authors report markedly lower FID and FVD than recent multi-view generation baselines such as UniMLVG and DiVE, and they showcase depth and geometry obtained through a jointly trained Gaussian decoder."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-  STORM-VAE, augmented by a 4D reconstruction auxiliary task, helps learn a geometry-aware latent space. \n- Beyond video generation, CVD-STORM can directly reconstruct dynamic 3D Gaussian Splatting from the latents.\n- Stable convergence with single-stage joint training of MM-DiT, temporal, and cross-view modules, offering practical value."}, "weaknesses": {"value": "- The method is more accurately characterized as an “enhanced representation learning” integration. In this approach, STORM’s 4D reconstruction head is migrated onto the SD 3.5 VAE and used for video diffusion. A more systematic comparison of “geometry-aware latent spaces” (for instance, using VAEs trained with depth or occupancy supervision) is recommended. Additional quantitative comparisons and discussions with generation+reconstruction paradigms such as MagicDrive3D and ReconDreamer, evaluated on metrics like novel-view quality and multiview consistency, would further strengthen the work.\n\n- CVD-STORM employs three frames as reference frames, but it is unclear from Tables 1 and 2(a) whether the initial frame uses a GT reference. If the GT frame is indeed used as the initial reference, the comparison presented in Table 1 may not be fair. Furthermore, when compared to UniMLVG, the downstream metrics do not demonstrate a clear advantage.\n\n- Multiview consistency metrics are missing. In addition, for Table 3(b), it would be beneficial to include the results from CVD-STORM + STORM (i.e., using the videos generated by CVD-STORM) to provide a fairer comparison. Evaluating only depth is limiting; reporting novel-view image quality metrics and additional comparisons would offer a more comprehensive assessment.\n\n- Finally, training details for STORM-VAE are not provided. Given the strong methodological and architectural similarities between STORM-VAE and STORM, an explanation of why STORM-VAE shows slightly better performance in Table 3(a) would be very helpful."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Br08lU8M2k", "forum": "V66pMNOVC2", "replyto": "V66pMNOVC2", "signatures": ["ICLR.cc/2026/Conference/Submission5359/Reviewer_PbTC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5359/Reviewer_PbTC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789958906, "cdate": 1761789958906, "tmdate": 1762918023412, "mdate": 1762918023412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "89zCNullwG", "forum": "V66pMNOVC2", "replyto": "V66pMNOVC2", "signatures": ["ICLR.cc/2026/Conference/Submission5359/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5359/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763099875753, "cdate": 1763099875753, "tmdate": 1763099875753, "mdate": 1763099875753, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CVD-STORM, a cross-view video diffusion framework for autonomous driving, designed to achieve both high-fidelity multi-view video generation and dynamic 4D scene reconstruction. The authors introduce STORM-VAE, an enhanced variational autoencoder equipped with a Gaussian splatting decoder and jointly optimized with a spatial–temporal reconstruction task, which significantly improves the latent space’s geometric and temporal consistency, thereby enhancing generation quality and convergence speed. Experimental results demonstrate that the proposed method surpasses existing approaches on several major datasets, achieving notable improvements in FID and FVD metrics while also producing scene reconstructions with absolute depth information."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The STORM-VAE, which integrates STORM with a VAE so that the latent representation simultaneously encodes both RGB texture and 3D geometric information. It further introduces CVD-STORM, which unifies multi-stage training into a single-stage process, thereby reducing training cost. Compared with existing methods, the proposed approach demonstrates strong performance, achieving state-of-the-art results particularly in metrics such as FID and FVD."}, "weaknesses": {"value": "The paper shows limited originality, as it mainly combines existing methods with little structural innovation. The proposed CVD-STORM is essentially an extension of the UniMLVG network, where a STORM branch is added to the VAE training, representing only a modest modification rather than a fundamentally new framework.In addition, the writing quality of the paper needs further improvement. For instance, there may be an error around line 96, and the formatting of the numerical data in the tables should be reviewed to ensure consistency across all columns."}, "questions": {"value": "- In the ablation study, does “w/o STORM-VAE” and “w/ STORM-VAE” mean replacing the VAE model in CVD-STORM with the SD3.5 VAE, or replacing it with a VAE retrained with STORM? If so, why are the performance metrics for “w/ STORM-VAE” inconsistent with the metrics in Table 1? What is the difference?\n- In the UniMLVG method, different blocks are trained in different stages. In this paper, the authors propose a single-stage training approach: for viewpoint data, they directly skip the cross-view module, while randomly dropping some temporal and cross-view modules to improve model stability. Although this reduces the training cost, the paper does not provide a performance comparison. Will single-stage training improve performance compared to multi-stage training, or does it sacrifice performance in exchange for lower training cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KpvJyaePBx", "forum": "V66pMNOVC2", "replyto": "V66pMNOVC2", "signatures": ["ICLR.cc/2026/Conference/Submission5359/Reviewer_AW7Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5359/Reviewer_AW7Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824326891, "cdate": 1761824326891, "tmdate": 1762918023060, "mdate": 1762918023060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents CVD-STORM, a cross-view video diffusion model for realistic environment simulation and 4D scene reconstruction. The model incorporates a spatial-temporal reconstruction VAE with a Gaussian Splatting decoder to enhance its understanding of 3D structure and temporal dynamics. This unified framework enables the generation of long-term, multi-view videos under various control inputs, including text, 3D bounding boxes, and HD maps. Experimental results on the nuScenes dataset demonstrate that CVD-STORM achieves notable improvements in visual fidelity (FID/FVD) and geometric consistency, marking an important advancement toward generative world models for autonomous driving."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Clear contribution: Introduces a Gaussian Splatting decoder into the VAE to form STORM-VAE, which incorporates spatiotemporal and geometric reconstruction as auxiliary objectives. This latent space is then leveraged in CVD-STORM for unified video diffusion and 4D scene generation.\n\n2.Comprehensive capability: During inference, the model can generate long-horizon six-view videos conditioned on text, 3D bounding boxes, and HDMap inputs, while also reconstructing 4D scenes directly from latent variables.\n\n3.State-of-the-art performance: On the nuScenes validation set, under a 20-second sequence setting, the model achieves better FID/FVD and conditional consistency (mAP, IoU) than existing methods (Table 1).\n\n4.Simplified training design: Unlike multi-stage paradigms, the paper adopts a single-stage joint training strategy for temporal blocks, cross-view blocks, and the MM-DiT module, claiming to simplify the process and reduce computational cost.\n\n5.Faster early convergence & geometric guidance: As shown in Figure 1, using STORM-VAE leads to faster early-stage convergence and improved visual quality. Moreover, the introduction of LiDAR-projection-supervised depth rendering loss helps enforce absolute depth and geometric consistency."}, "weaknesses": {"value": "1.Depth evaluation relies on pseudo ground truth: The depth evaluation of generated results uses Depth Anything V2 as pseudo GT. The authors also acknowledge that assessing absolute depth accuracy remains an open challenge.\n\n2.Questionable comparability in horizontal evaluations: Since most baseline methods have not released their model weights, Table 1 primarily adopts reported results, with some metrics missing, potentially introducing bias due to inconsistent evaluation protocols.\n\n3.Processing of annotations and textual conditions: The nuScenes annotations are interpolated at 12 Hz, while textual descriptions for all frames/views are automatically generated at 2 Hz. These processing steps may introduce distributional bias, affecting the objectivity of “conditional following.”\n\n4.Resource and latency cost: Training is performed on H100 GPUs with a batch size of 32; inference requires 50 steps. The computational and latency costs for practical deployment need further quantification and optimization.\n\n5.Limited ablation scope: The VAE ablation study only compares results after 40k steps without loading diffusion pretraining weights, which may not fully reflect the differences under large-scale final training."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nL2AMA7CGH", "forum": "V66pMNOVC2", "replyto": "V66pMNOVC2", "signatures": ["ICLR.cc/2026/Conference/Submission5359/Reviewer_dKaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5359/Reviewer_dKaC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829377369, "cdate": 1761829377369, "tmdate": 1762918022796, "mdate": 1762918022796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CVD-STORM, a framework that simultaneously generates long-horizon, multi-view driving videos and reconstructs the full 3D scene via a GS-decoder. By jointly modeling video generation and scene reconstruction, the method achieves state-of-the-art performance on standard video generation metrics.\nHowever, the core technical contributions—namely the STORM-VAE architecture and the cross-view attention module—represent relatively incremental advances over existing VAE and multi-view modeling techniques. As such, the overall novelty appears modest. Additionally, the paper suffers from several presentation issues,  which require careful revision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a novel framework that integrates scene reconstruction and video generation, achieving state-of-the-art video generation quality. The reconstruction component employs a feedforward reconstruction scheme (STORM), which is well-aligned with the generation pipeline in both efficiency and methodology."}, "weaknesses": {"value": "The paper suffers from several writing and technical shortcomings that weaken its overall contribution.\n\n1. There are minor but notable writing issues—for example, the phrasing at the end of the second claimed contribution is ambiguous and lacks precision, which reduces clarity.\n\n2. The methodological novelty appears limited. While the paper introduces STORM-VAE and a cross-view module, it provides insufficient analysis of either component. Specifically:\n（1）There is no ablation study on the cross-view module to justify its necessity or quantify its impact (e.g., with vs. without cross-view attention).\n（2）The quality of the 3D Gaussian Splatting (3DGS) scenes produced by STORM-VAE is not evaluated—neither quantitatively (e.g., PSNR, LPIPS on reconstructed views) nor qualitatively—which leaves a gap in validating the reconstruction branch.\n（3）The claim that the framework enables “long-horizon” video generation is not substantiated: it is unclear whether the temporal coherence or scene consistency over long sequences actually stems from STORM-VAE, the cross-view design, or other factors. A breakdown of where the gains originate is missing.\n3. The evaluation focuses solely on standard video generation metrics . It would significantly strengthen the paper to demonstrate the utility of the generated data for downstream autonomous driving tasks—for instance, whether training a planner or perception model on the synthesized videos improves robustness on real-world safety-critical scenarios. Without such evidence, the practical value of the generated data remains speculative."}, "questions": {"value": "1. Please include an ablation study on the cross-view module.\n2. Please include evaluations on downstream tasks.\n3. Please include evaluation results for 4D scene reconstruction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vdY6F8t1Az", "forum": "V66pMNOVC2", "replyto": "V66pMNOVC2", "signatures": ["ICLR.cc/2026/Conference/Submission5359/Reviewer_YTxk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5359/Reviewer_YTxk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837941521, "cdate": 1761837941521, "tmdate": 1762918022530, "mdate": 1762918022530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}