{"id": "fN0MED2Idq", "number": 10992, "cdate": 1758186353878, "mdate": 1759897615878, "content": {"title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators", "abstract": "Large Language Models (LLMs) have shown impressive performance across diverse domains, with code generation emerging as a particularly prominent application. However, existing benchmarks designed to evaluate code generation exhibit several critical limitations. First, most rely on manual annotations, which are time-consuming and difficult to scale across programming languages and problem complexities. Second, the majority focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and imbalanced language coverage. \nTo overcome these challenges, we present AutoCodeGen, an automated framework for constructing high-difficulty, multilingual code generation datasets without manual annotations. Our approach guarantees correctness and completeness by generating test inputs with LLMs, obtaining test outputs within a multilingual sandbox, and further enhancing quality through reverse problem generation and multi-stage filtering. \nBased on this novel method, we introduce AutoCodeBench, a large-scale benchmark suite spanning 20 programming languages with balanced coverage. AutoCodeBench is designed to rigorously evaluate LLMs on diverse, challenging, and realistic multilingual programming tasks. Extensive experiments reveal that even state-of-the-art models struggle on these tasks, particularly in low-resource languages.\nBesides, we release complementary training and evaluation resources, including a large-scale, verifiable multilingual instruction dataset generated via the same pipeline, as well as a multilingual sandbox with high-concurrency support. We hope these contributions will provide a solid foundation for future research and inspire the community to explore more automatic and scalable approaches to multilingual code generation, with a particular emphasis on advancing progress in low-resource languages.", "tldr": "", "keywords": ["code generation", "benchmark", "autocodebench", "llm"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dee99645ee7b7c713c6cbc1c723d3ed9facaa206.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AutoCodeGen, a fully automated pipeline for generating multilingual code generation benchmarks without manual annotation. The core of the proposed method is generates test inputs using LLM, which are then executed in a secure sandbox to obtain ground-truth outputs. These verified input-output pairs are subsequently used to formulate a complete programming problem. Using this pipeline, the authors present AutoCodeBench, a large-scale benchmark comprising 3,920 problems across 20 programming languages, designed to be both difficult and diverse. The authors report that the benchmark reveals significant weaknesses in current state-of-the-art models, particularly on low-resource languages and multi-logic tasks. In addition, they release a companion instruction-tuning dataset (AutoCodeInstruct) to support future research."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The cost and scalability limitations of manual annotation are well-known, and automating this process is a valuable research direction. \n2. The public release of the benchmark, a large-scale training dataset, and a high-concurrency multilingual sandbox are significant assets that could benefit the code generation research community."}, "weaknesses": {"value": "1. While the engineering effort is commendable, the core concept of using LLMs for data synthesis in the coding domain is not new and builds upon a line of existing work (e.g., Evol-Instruct, OSS-Instruct). \n2. A significant concern is the methodology used to generate problems for 14 of the 20 languages via \"approximate language translation (sec. 2.1.5)\". However, code translation is not a solved problem, and often fails to capture language-specific idioms, standard library conventions, or type system nuances. This approach raises questions about the quality and naturalness of the generated problems in these low-resource languages. It is unclear what the quality gap is between these \"translated\" problems and problems that could have been generated \"natively\" using the full pipeline. The paper would be strengthened by an analysis quantifying this gap."}, "questions": {"value": "1. The benchmark was generated using a model from the DeepSeek family. To what extent are the benchmark's characteristics and the resulting model performance metrics biased by this choice of generator? It would be insightful to conduct a small-scale experiment where the key pipeline steps are run using a completely different model family (e.g., Llama 3, Claude 3, or Qwen). A comparison of the resulting problems (e.g., types, styles) and model performance on this new subset would help quantify the potential for generator bias. Meanwhile, I agree Line 1064.\n2. The concept of \"multi-logic\" problems is interesting and presented as a key contribution. However, its definition in the paper feels somewhat informal. Could the authors provide a more rigorous definition? Furthermore, a deeper analysis of what makes these problems challenging would be valuable. Is the difficulty primarily a function of longer context and following multiple discrete instructions, or does it require a more fundamental type of compositional reasoning that current LLMs lack?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "svuHudoFEB", "forum": "fN0MED2Idq", "replyto": "fN0MED2Idq", "signatures": ["ICLR.cc/2026/Conference/Submission10992/Reviewer_5r4r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10992/Reviewer_5r4r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571340237, "cdate": 1761571340237, "tmdate": 1762922180917, "mdate": 1762922180917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AutoCodeGen, a automated pipeline for synthesizing multilingual code generation benchmarks. The key idea is to use LLMs to generate test inputs, execute them in a secure sandbox to obtain ground-truth outputs, and then combine these elements to create verifiable test cases. Based on this pipeline, the authors introduce AutoCodeBench, a benchmark of 3,920 problems spanning 20 programming languages, which they claim is more difficult, diverse, and realistic than existing benchmarks. The paper also releases AutoCodeInstruct, a training dataset. The authors' experiments suggest that current LLMs still struggle significantly on this new benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors have conducted a comprehensive evaluation across a very large number of models. This provides a broad and valuable snapshot of the current landscape of code generation capabilities as measured by their proposed benchmark.\n2. The strategy of generating test inputs first and then using a sandbox to obtain ground-truth outputs is a clever and effective method for ensuring the correctness of the generated test cases.\n\nI am looking forward to the authors' response and would be happy to reconsider my evaluation upon successful clarification."}, "weaknesses": {"value": "The paper's central claim of high difficulty is not sufficiently deconstructed or justified. The methodology for ensuring difficulty involves a post-hoc filtering step that removes any problem solvable by a \"moderately capable model\" (Line 195). This approach risks conflating genuine, meaningful difficulty with other confounding factors. Specifically, the source of difficulty remains ambiguous: \n- Is a problem difficult because it requires complex algorithmic reasoning or deep domain knowledge?\n- Is it difficult due to the need to understand obscure language features?\n- Or is it \"difficult\" simply because the problem description is ambiguous, underspecified, or even flawed?\n\nThe authors' own finding of a 12.4% error rate in the benchmark (Appendix A) lends significant weight to the third possibility. If a substantial portion of the benchmark is poorly specified, then it may be evaluating a model's ability to \"guess the user's intent\" rather than its true code generation capabilities. This ambiguity undermines the benchmark's validity as a measure of progress in the field."}, "questions": {"value": "1. The paper reports pass@1 results. Could the authors also provide pass@k results for k > 1 (e.g., k=5, 10)?\n2. The paper uses GRPO with the AutoCodeInstruct. What would be the performance if standard Supervised Fine-Tuning (SFT) were used instead? An ablation study comparing GRPO with SFT would help disentangle the benefits of the dataset itself from the specific training strategy employed.\n3. The pipeline relies on LLM to generate test inputs. Have the authors considered integrating or comparing this approach with established automated test case generation tools, such as CYaRon or property-based testing libraries (e.g., Hypothesis for Python)?\n4. I am slightly confused about the problem generation workflow detailed in Section 2. Specifically, why are the test functions generated before the input/output format specifications? Could you elaborate on how boundary and stress tests are systematically generated within this workflow? It seems that defining the I/O format first would provide a clearer structure for generating comprehensive test cases."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "quiiehF3ti", "forum": "fN0MED2Idq", "replyto": "fN0MED2Idq", "signatures": ["ICLR.cc/2026/Conference/Submission10992/Reviewer_LmDg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10992/Reviewer_LmDg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571879606, "cdate": 1761571879606, "tmdate": 1762922180449, "mdate": 1762922180449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an automated framework called AutoCodeGen for constructing high-difficulty, multilingual code generation datasets without manual annotations, which contains 20 programming languages with balanced coverage to rigorously evaluate LLMs on diverse, challenging, and realistic multilingual programming tasks. Experiments reveal that even state-of-the-art models struggle on these tasks, particularly in low-resource languages. The paper also gives complementary training and evaluation resources, including a large-scale, verifiable multilingual training set."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The automation of the benchmark is good, which eliminates the need for manual annotations. This is a significant advantage for scaling code generation evaluation across languages and problem complexities.\n\n2. AutoCodeBench contains 20 languages and high-difficulty problems, which makes it a robust benchmark for multilingual code generation tasks. \n\n3. The paper is easy to follow."}, "weaknesses": {"value": "1. The authors should pay attention to other low-resource languages.\n\n2. The system heavily relies on LLMs for generating code solutions and test cases, which can introduces biases or errors inherent to the models being used, especially if those models are trained on flawed data.\n\n3. The paper focuses on evaluating the code generation of models but does not address the broader issue of how benchmarks extends to other domains."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "856nqtaxsr", "forum": "fN0MED2Idq", "replyto": "fN0MED2Idq", "signatures": ["ICLR.cc/2026/Conference/Submission10992/Reviewer_To2A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10992/Reviewer_To2A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945197259, "cdate": 1761945197259, "tmdate": 1762922179935, "mdate": 1762922179935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing benchmarks for evaluating code generation have key limitations: they rely on manual annotation, which is time-consuming and small in scale; they focus on Python code, while benchmarks for other languages have limited difficulty and low coverage.\nTo address these issues, the authors propose a fully automated framework called AutoCodeGen for building high-difficulty, multi-language code generation datasets. The framework first generates code solutions and test inputs from code snippets, then executes the code solutions in a sandbox to obtain test outputs. Finally, it uses LLMs to combine these three components into test functions, and reversely generates programming problem descriptions based on the solutions and test functions. This process ensures the quality of the benchmark through a three-stage filtering (difficulty, quality, diversity).\nBased on this framework, the authors introduce AutoCodeBench, a large-scale benchmark containing 3920 problems across 20 programming languages. Experiments show that even the most advanced models still face significant challenges in the complex and diverse multi-language tasks defined by this benchmark. In addition, the authors also release a supporting training set, AutoCodeInstruct."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The greatest contribution of this paper is the AutoCodeGen framework, which automatically generates benchmark data and attempts to overcome the reliance on expensive and time-consuming manual annotation. Its highlight lies in ensuring the correctness of the generated data: first, generating test inputs and then obtaining outputs through sandbox execution to avoid potential errors when LLMs directly generate test cases; second, reversely generating problems based on answers and test cases to ensure the actual solvability of the problems.\n\n2. To obtain a high-difficulty and diverse dataset, the paper implements multi-stage filtering. In the problem generation stage, the authors use LLMs as evaluation models to ensure alignment between problem descriptions and test functions (quality control). After generating problem descriptions, they use a \"moderately capable\" model (DeepSeek-Coder-V2-Lite) to filter out overly simple problems (difficulty control); they also ensure broad data coverage through problem classification and cyclic sampling (diversity control).\n\n3. AutoCodeBench, derived from the AutoCodeGen framework, features high difficulty, multi-language support (20 languages), and balance. Its versions of different sizes also meet the diverse needs of different researchers. Meanwhile, the paper shows that models trained with AutoCodeInstruct (Section 4) have comprehensively improved code generation capabilities, indicating that the data generated by the AutoCodeGen framework is of high quality and generalizability."}, "weaknesses": {"value": "The paper states that AutoCodeBench aims to conduct strict evaluations of large language models on diverse, high-difficulty, and realistic multi-language programming tasks. However, to achieve this goal, there are areas where the paper can be improved:\n1. Insufficient Difficulty Assessment in the AutoCodeGen Framework\nIn Section 2.1.4, the authors mention the method for difficulty control: \"we employ a moderately capable code model, DeepSeek-Coder-V2-Lite, to filter out too easy problems. Specifically, we sample answers for each problem ten times using the model and validate the correctness via sandbox execution. We discard problems that are solved in all attempts.\" This is a filtering method designed to eliminate simple problems, but its limitation is that it is difficult to directly identify high-difficulty problems. The authors later mention in Table 3 how to further distinguish easy/medium/hard problems in the dataset using pass rates. The problem is that while high difficulty equals a low pass rate, a low pass rate does not necessarily mean the problem has real high difficulty (e.g., multi-logic/multi-task). Difficulty assessment based solely on the pass rate indicator requires support from other indicators. In addition, this difficulty assessment imposes requirements on the evaluation model. The paper reports in Section 3.3 that the Pass@1 scores of advanced models on low-resource language problems are relatively higher than those on mainstream languages, which to some extent indicates that the evaluation model is insufficient in filtering simple problems in low-resource languages, leading to differences in the consistency of benchmarks across different languages.\n2.  Vague Description of Multi-Logic Tasks in AutoCodeBench\nIn Section 3.4, the authors mention the concept of multi-logic tasks: \"A key feature that distinguishes AutoCodeBench from prior benchmarks is the inclusion of multilogical problems. These problems require models to implement multiple distinct functions or classes within a single task, challenging their ability to handle multiple core demands simultaneously.\" In subsequent experiments, the authors show that advanced models perform poorly on such tasks. As a representative of high-difficulty problems, multi-logic tasks can indeed reflect a model's comprehensive ability to handle complex code tasks. However, the description of multi-logic tasks in the paper is somewhat brief, and such problems are not generated through active control; the analysis of such problems is more inclined to data mining."}, "questions": {"value": "1. (Regarding Weakness 1) Given that the relatively simple difficulty filter is insufficient in distinguishing high-difficulty problems and filtering simple problems in low-resource languages, have you considered adopting improved methods? For example, could you use a more capable model as an evaluator to try more reasonable filtering for problems in different languages? Or, similar to quality control, could you incorporate more LLM-based analysis (e.g., using LLMs to analyze unsolved problems and solutions) to assist in filtering truly high-difficulty problems?\n2. (Regarding Weakness 2) Could you supplement more detailed analysis of multi-logic tasks? Could you provide more detailed statistics on the complexity of these problems? For instance, how many functions do they require to implement on average? Is there a specific reflection of the low pass rate of advanced models on high-difficulty problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BrNfopRjlN", "forum": "fN0MED2Idq", "replyto": "fN0MED2Idq", "signatures": ["ICLR.cc/2026/Conference/Submission10992/Reviewer_6ERu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10992/Reviewer_6ERu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10992/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762269933938, "cdate": 1762269933938, "tmdate": 1762922179455, "mdate": 1762922179455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}