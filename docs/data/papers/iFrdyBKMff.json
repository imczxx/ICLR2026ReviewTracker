{"id": "iFrdyBKMff", "number": 3233, "cdate": 1757382934554, "mdate": 1763641514592, "content": {"title": "PreferThinker: Reasoning-based Personalized Image Preference Assessment", "abstract": "Personalized image preference assessment aims to evaluate an individual user's image preferences  by relying only on a small set of reference images as prior information. Existing methods mainly focus on general preference assessment, training models with large-scale data to tackle well-defined tasks such as text-image alignment. However, these approaches struggle to handle personalized preference because user-specific data are  scarce and not easily scalable, and individual tastes are often diverse and complex. To overcome these challenges, we introduce a common preference profile that serves as a bridge across  users, allowing large-scale user data to be leveraged for training profile prediction and  capturing complex personalized preferences.  Building on this idea, we propose a reasoning-based personalized image preference assessment framework that follows a \\textit{predict-then-assess} paradigm:  it first predicts a user's preference profile from reference images, and then provides interpretable, multi-dimensional scores and assessments of candidate images based on the predicted profile. To support this, we first construct a large-scale Chain-of-Thought (CoT)-style personalized assessment dataset annotated with diverse user preference profiles and high-quality CoT-style reasoning, enabling explicit supervision of structured reasoning. Next, we adopt a two-stage training strategy: a cold-start supervised fine-tuning  phase to empower the model with  structured reasoning capabilities, followed by reinforcement learning to incentivize the model to explore more reasonable assessment paths and enhance generalization. Furthermore, we propose a similarity-aware prediction reward to encourage better prediction of the user's preference profile, which facilitates more reasonable assessments exploration. Extensive experiments demonstrate the superiority of the proposed method. Our code and dataset will be  publicly released.", "tldr": "", "keywords": ["Image Preference Assessment;Multimodal Large Language Model；Chain-of-Thought"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b203dcf012983e72e2c00d5775d77942d5dff12d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new CoT-based methodology for personalized image preference assessment and introduces a large-scale CoT-style dataset. Most existing work captures general human preferences and cannot handle personalized ones. The paper introduces the concept of \"common preference profile\" and proposes a two-stage \"predict-then-assess\" method. The interpretable preference profile and reasoning steps help MLLMs to achieve better alignment for each user's personalized image synthesis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The innovative “Common Preference Profile” efficiently bridges the gap between large-scale general preference alignment and personalized preference modeling.\n---\n2. The proposed \"predict-then-assess\" paradigm decomposes the task into *profile prediction* (summarizing human preference across several pre-defined dimensions) and *assessment reasoning* (interpretation and scoring). This decomposition provides **better modularity for image generation** and interpretability.\n---\n3. The paper empirically verifies that carefully designed reasoning structures—incorporating CoT reasoning to (1) predict each user's preference profile and (2) assess candidate images with the uncovered preference profile—achieve state-of-the-art performance on human preference prediction."}, "weaknesses": {"value": "The discrete elementary preference profile may oversimplify the real-world human's nuanced preference, as it reduces complex, context-dependent aesthetic judgments into a fixed set of categorical dimensions that may not capture the subtle variations and contradictions inherent in individual taste."}, "questions": {"value": "Humans have inconsistent preferences across different topics and scenarios. How does the proposed elementary preference profile approach address this complexity? Can you clarify the motivation behind using independent element-wise preference profiles rather than capturing the continuous latent representation for users' preferences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8msCxepqOw", "forum": "iFrdyBKMff", "replyto": "iFrdyBKMff", "signatures": ["ICLR.cc/2026/Conference/Submission3233/Reviewer_oEnc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3233/Reviewer_oEnc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761065531813, "cdate": 1761065531813, "tmdate": 1762916615319, "mdate": 1762916615319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Add citations in two place"}, "comment": {"value": "We sincerely thank all the reviewers for their valuable suggestions and careful review.\nThe first author of [1] contacted us and requested us to cite their paper. Consequently, we have revised our manuscript to incorporate this citation. The modifications have been highlighted in yellow within the text, and the explanation is detailed in the footnote. We also offer further clarification below.\n\n**1. Added Citation at Location 1 (Line 238):**\n\nWe added the citation: …inspired by [1] …\n\n**2. Added Citation at Location 2 (Line 245):**\n\nWe added the citation: …follow the PrefGen [1] to select 190 K prompts…\n\n**Our Clarification:**\n\nThis work is supported by a joint project by a company and a university. The text prompts we used were provided by the company when the second author of our paper was a research intern in the company. The first and second authors of our paper knew that the first author of [1] contributed to the collection of these text prompts, but believed the ownership of the prompts belonged to the company. As requested by the first author of [1], after coordinating with the company and the first author of [1], we cited [1] here to emphasize the source of the text prompts.\n\nWe will continue to carefully respond to each reviewer's comments in the coming days.\n\n[1] PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation, https://openreview.net/pdf?id=8iGclsodrJ."}}, "id": "jo6fM1fqmk", "forum": "iFrdyBKMff", "replyto": "iFrdyBKMff", "signatures": ["ICLR.cc/2026/Conference/Submission3233/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3233/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3233/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763006617545, "cdate": 1763006617545, "tmdate": 1763006617545, "mdate": 1763006617545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PreferThinker, a reasoning-based personalized image preference assessment system. The key idea is to predict a preference profile to bridge various uses, allowing large-scale user data to be leveraged for training profile prediction and capturing complex\npersonalized preferences. To this end, a CoT-style dataset annotated with preference profiles and high-quality reasoning for interpretability supervision is constructed. A two-stage a two-stage training strategy comparing Cold-start SFT and reinforcement learning is utilized to enpower the model with reasoning capabilities. Experiments on the constructed dataset shows that the proposed method outperforms existing approaches."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper addresses personalized image preference assessment from a novel visual preference profile based interpretable perspective. \n\nA large-scale Chain-of-Thought (CoT)-style personalized assessment dataset annotated with diverse user preference profiles and high-quality CoT-style reasoning is contructed, enabling explicit supervision of structured reasoning. \n\nExperiments demonstrate the superiority of the proposed method."}, "weaknesses": {"value": "My main concerns is how the proposed method generalizes to real-world images. To costruct a large-scale CoT-style dataset that provides high-quality reasoning supervision, the authors propose to combine several random profiles with initial prompts and feed into a text-to-image model to generate each user’s reference images (preferred and non-preferred) and two candidate images. However, the generated images ,as shown in paper and supplementary, lack photorealism, and would also cover a very limtied range of categories. Though the experiments demonstrate the superiority of the proposed method, the main experiments are conducted on the collected dataset, failing to validate the generalization to real-world images. \n\nOn the other hand, it is still unclear to me whether the compared methods in paper are trained or fine-tuned on the collected dataset for fair comparison of assessment accuracy. Moreover, the evaluation on the PickaPic dataset is somehow confusion to me. As described in paper, the experiment on PickaPic reflects general preferences, rather than personalized preference. However, the results in Table 1 show that the proposed method ranks second on unseen PickaPic data for general preference assessment, this makes me doubt whether the proposed method can indeed extract and understand personalized visual preference profile. \n\nCurrently, the employed visual preference profile consists of five visual elements while the user study is conducted with 15 visual elements. Therefore, I would like to see experimental analysis on how the number of visual elements in a visual preference profile affects the final results. \n\nIn fact, it is difficult for a person to rate his/her visual preference to an image using an absolute score, but it is easy for a person to perform relative preference comparison between two images. Hence, I suggest the authors to evaluate the proposed method on such preference ranking data."}, "questions": {"value": "What is the limitation of the proposed method?\n\nHow does the effectiveness of the proposed method on personalized image generation?\n\nIn my opinion, the current 5-element visual preference profile mainly summarizes the visual tone of an image, I wonder whether it is possible to make the model learn to find the visual elements that determines his/her visual preference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cTLz2vhY5p", "forum": "iFrdyBKMff", "replyto": "iFrdyBKMff", "signatures": ["ICLR.cc/2026/Conference/Submission3233/Reviewer_2Qg5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3233/Reviewer_2Qg5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719136415, "cdate": 1761719136415, "tmdate": 1762916615122, "mdate": 1762916615122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We first thank the reviewers for affirming our main contributions: an **interesting** [PbCi] **and carefully designed** [oEnc] **framework** that provides **better modularity for image generation and interpretability** [oEnc]; a **novel common visual preference profile** [PeqQ, 2Qg5, oEnc] that serves as a bridge across users; a **valuable resource** [PbCi] for the community in terms of **new large-scale CoT-style dataset** [PbCi, PeqQ, 2Qg5]; and **robust experiments** [PeqQ] demonstrating **superior performance** [2Qg5, oEnc].\n\nAs for the main problems, most reviewers [PbCi, PeqQ, oEnc] are particularly concerned with capturing complex and subtle personalized preferences of real users. In addition, reviewer PbCi is mainly concerned with the robustness of proposed method to real-world preferences, and   reviewer PeqQ cares about the collection of real user test set with personalized data, and reviewer 2Qg5 mainly focuses on the performance on real-world images, and reviewer oEnc questions the motivation for using elementary preference profiles rather than continuous latent representations.\n\nTo sum up, we deeply thank the reviewers for their appreciation of our contributions, and for the valuable suggestions that advance the integrity of this work."}}, "id": "yaaK7PSHLh", "forum": "iFrdyBKMff", "replyto": "iFrdyBKMff", "signatures": ["ICLR.cc/2026/Conference/Submission3233/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3233/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3233/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763640561608, "cdate": 1763640561608, "tmdate": 1763640561608, "mdate": 1763640561608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PreferThinker, a new framework for personalized image preference assessment.\nThe authors pointed out that currently there is a lack of user-specific data.\nTo overcome this issue, the paper proposes a common preference profile for bridging various users.\nThis profile enables the model to leverage large-scale data to learn user preferences.\nThe PreferThinker framework operates on a \"predict-then-assess\" paradigm: It first predicts a user's visual preference and non-preference profiles based on a small set of reference images. Then, it uses this predicted profile as a criterion to generate interpretable, multi-dimensional scores and a Chain-of-Thought (CoT) assessment for candidate images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper tries to tackle the problem of personalized preference assessment by using the idea of common preference profile as a bridge between users. This idea is novel. Beyond this, the paper also introduces a new, large scale dataset for personalized assessment. The experiments are robust, covering seen vs. unseen profiles , single vs. multi-preference users , and robustness to the number of reference images."}, "weaknesses": {"value": "The primary weakness is that the main dataset, PreferImg-CoT, is built on simulated user preferences. While the simulation pipeline is well-designed (based on a user study to find 5 key elements ), simulated profiles may not capture the full, complex, and sometimes contradictory or hard-to-articulate nature of real human preferences."}, "questions": {"value": "Given the primary limitation is the simulated dataset, could you discuss the feasibility of collecting a (perhaps smaller) \"gold standard\" test set with real personalized data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "B9oQcPJbSi", "forum": "iFrdyBKMff", "replyto": "iFrdyBKMff", "signatures": ["ICLR.cc/2026/Conference/Submission3233/Reviewer_PeqQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3233/Reviewer_PeqQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781534585, "cdate": 1761781534585, "tmdate": 1762916614787, "mdate": 1762916614787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes PreferThinker, a system for personalized image reward scoring with chain of thought reasoning to predict a user's preference profile. To train this system, the authors first construct PreferImg-CoT, using Claude 3.7 to generate a reasoning trace of each individual user's score given reference images. The authors use this dataset to train Qwen-2.5-VL-7B with SFT and GRPO, and evaluate on their PreferImg dataset and Pick-a-Pic v1."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* This work proposes an interesting system to predict a user's preference profile for text-to-image generation, and then score generated images for that individual\n* This work contributes a new large scale synthetic preference dataset PreferImg created from 80K synthetic user preference profiles with attributes that the authors choose after a real-world user study. This dataset can be a valuable resource for the text-to-image reward modeling community\n* The authors demonstrate that their dataset can learn rewards that generalize well when new users have similar preference profiles to users seen during training (Tab 1) via cold start SFT and RL training of Qwen 2.5 VL"}, "weaknesses": {"value": "* The authors argue that *\"although each user’s personalized preferences are unique, the key visual elements that shape these preferences are shared\"* (L197-198), and they mention discrete attributes that users rank highly as important to them (art style, color, detail, art medium and saturation). I feel that this a strong assumption to make - what about individual preference differences that are more semantic in nature for a given prompt? Is it possible to discretize real-world user preferences of generated images in this manner?\n* **My primary concern**: PreferThinker appears robust to unseen users when the distribution of their preference profiles is shared with seen users (I.e. they are both sampled from PreferImg). How robust is PreferThinker to preferences outside this distribution? Since PreferImg is constructed with 80K synthetic preference profiles, it is unclear to me how robust these preference profiles will be to real-world datasets. While the authors do evaluate on Pick-a-Pic, PreferThinker's accuracy (67%) does not seem compelling when compared to extremely lightweight pluralistic reward modeling baseline [1], which gets 71% accuracy on V1 and 70.5% on V2 (no-leakage) with just 6M trainable parameters \n* The presentation has room for improvement, especially the figures, which are quite small, dense, and difficult to read (especially Figure 8). Figure 1 and 2 can be made bigger and Fig 3 moved to the Appendix. It is very hard to parse Table 2 and 3 as their captions are nearly contiguous.\n\n[1] Chen et al., \"PAL: Sample-Efficient Personalized Reward Modeling for Pluralistic Alignment\", ICLR 2025."}, "questions": {"value": "* Which version of Pick-a-pic is used in your experiments, v1 or v2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NMXx6vJwYb", "forum": "iFrdyBKMff", "replyto": "iFrdyBKMff", "signatures": ["ICLR.cc/2026/Conference/Submission3233/Reviewer_PbCi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3233/Reviewer_PbCi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972548632, "cdate": 1761972548632, "tmdate": 1762916614433, "mdate": 1762916614433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}