{"id": "80vdaC5DsD", "number": 1790, "cdate": 1756929431906, "mdate": 1759898186720, "content": {"title": "Causal Discovery via Quantile Partial Effect", "abstract": "Quantile Partial Effect (QPE) is a statistic associated with conditional quantile regression, measuring the effect of covariates at different levels. Our theory demonstrates that when the QPE of cause on effect is assumed to lie in a finite linear span, cause and effect are identifiable from their observational distribution. This generalizes previous identifiability results based on Functional Causal Models (FCMs) with additive, heteroscedastic noise, etc. Meanwhile, since QPE resides entirely at the observational level, this parametric assumption does not require considering mechanisms, noise, or even the Markov assumption, but rather directly utilizes the asymmetry of shape characteristics in the observational distribution. By performing basis function tests on the estimated QPE, causal directions can be distinguished, which is empirically shown to be effective in experiments on a large number of bivariate causal discovery datasets. For multivariate causal discovery, leveraging the close connection between QPE and score functions, we find that Fisher Information is sufficient as a statistical measure to determine causal order when assumptions are made about the second moment of QPE. We validate the feasibility of using Fisher Information to identify causal order on multiple synthetic and real-world multivariate causal discovery datasets.", "tldr": "We propose a novel parametric assumption that affords cause-effect identifiability solely from the observational distribution, concomitantly generalizing and relaxing the Functional Causal Model assumption.", "keywords": ["causality", "causal discovery", "causal order", "identifiability", "normalizing flow"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/056a52a446217979acd435f326c0174097748c87.pdf", "supplementary_material": "/attachment/bb61ca5cd36ed5dbf0c47d6a0260682e2501ba98.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Quantile Partial Effect (QPE) as an observational-level object for causal discovery. QPE is defined via conditional quantiles and admits multiple equivalent forms, enabling estimators that do not assume a specific functional causal model (FCM). The core theory shows identifiability of causal direction when each component of the QPE lies in a finite linear span of basis functions, formalized by Theorem 3.6. Building on this, the authors give two bivariate procedures: (i) a kernel QPE with an OLS basis test (QPE-k) and (ii) a flow-based QPE with a neural basis test (QPE-f)—and report strong empirical results. For multivariate discovery, they leverage a PDE link between QPE and the score function to derive Fisher-Information Causal Ordering (FICO), which greedily removes leaf nodes using marginal score variances under an additional assumption (Assumption 4.5). Experiments show QPE-f competitive or superior to many bivariate baselines, and FICO performs competitively on synthetic and real benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "a. The equivalence of QPE and the Wronskian-based identifiability criteria are clear and self-sufficient at the observational level.\n\nb. Unifies causal-velocity style reasoning with quantile/CDF views and score-based analysis.\n\nc. Two practical bivariate pipelines: kernel, OLS basis test (fast), and flow, neural basis test (accurate). And, for multivariate cases, FICO is simple to implement given score estimators."}, "weaknesses": {"value": "a. The central finite-span assumption (Assumption 3.5) is elegant but potentially restrictive; practical guidance on choosing/validating basis sets is limited. How sensitive are conclusions to basis mis-specification?\n\nb. Assumption 4.5 for FICO appears to lack easy a-priori diagnostics.  The performance on Sachs suggests it fails in practice, which narrows applicability.\n\nc. Multivariate ranking depends on accurate scoring function estimation; the paper uses first-order scores for speed but does not deeply analyze robustness to estimator bias or noise."}, "questions": {"value": "a. How is the assumption on lines 96–97 used downstream? e.g., in which lemmas/theorems or in the estimator design. and what would break without it?\n\nb. The paper claims that QPE does not require the (causal) Markov assumption. However, some steps appear to rely on independence of noise terms (exogeneity)?\n\nc. How should practitioners select basis families for Assumption 3.5 in new domains? Could the authors provide data-driven diagnostics or model selection among candidate bases?\n\nd. How sensitive are ODRs to the estimator class and its hyperparameters in FICO?\n\ne. Why Sachs is hard? Can the authors analyze where Assumption 4.5 breaks on Sachs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vzaUDfuQOX", "forum": "80vdaC5DsD", "replyto": "80vdaC5DsD", "signatures": ["ICLR.cc/2026/Conference/Submission1790/Reviewer_w7Hq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1790/Reviewer_w7Hq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761243200979, "cdate": 1761243200979, "tmdate": 1762915890717, "mdate": 1762915890717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "# **Common Questions and Responses**\n\n> **Q1. Although you claim that QPE does not require monotonicity and Markov properties, some steps still use these two properties (`W1w7:W2&3`; `vus6:W1`; `w7Hq:Q2`).**\n\nBased on the dependency chain of our theorems, it is true that Proposition 3.3 and Corollary A.2 utilize monotonicity and Markov properties. We clarify their specific roles below, and emphasize that this does not contradict our main claim that *observational distributions alone are sufficient for cause-effect identifiability*.\n\n*   **Use in Proposition 3.3:** The use of monotonicity and Markov properties in Proposition 3.3 is to ensure counterfactual identifiability. Crucially, this is a requirement for the definition of *causal velocity*, not for the definition of QPE itself.\n\n*   **Use in Corollary A.2:** Corollary A.2, one of the core guarantees for cause-effect identifiability, does use these properties. However, it serves as a supplement to provide a *quantitative guarantee*, which was missing from the discussion in lines 212-215 of the main text.\n\n*   **Implicit Properties in QPE-f:** This is an inherent design of the causal flow model. Although monotonicity and Markov properties are implied, the flow in QPE-f is merely a tool to estimate QPE. According to Proposition 3.3, the estimated QPE is always consistent with the true QPE, and the true underlying model does **not** require monotonicity or Markov properties (see Appendix A.3 for some examples).\n\nIn fact, the core argument for why our causal discovery works is presented in lines 212-215, albeit informally through examples. In the following thread, we provide a formalization of lines 212-215 (Corollary I.1). This corollary guarantees cause-effect identifiability through an additional assumption, \"**Asymmetry in the Shape of the Observational Distribution**\" (Assumption I.4, mentioned in the abstract, introduction, and lines 212-215), and it works **without** requiring monotonicity or Markov properties. This directly supports our claims in the abstract, introduction, and TL;DR, and is one of our core theoretical contributions.\n\nIn summary, Corollary I.1 (in the main text) and Corollary A.2 (in the appendix) are complementary. Both prove cause-effect identifiability. The former does not need monotonicity/Markov properties but lacks quantitative guarantees (we provide some examples in Appendix A3 to compensate). The latter requires these assumptions but provides quantitative guarantees. Given the significant confusion this has caused, **we will replace lines 212-215 with the formal statements of Assumption I.4 and Corollary I.1 in the revised version.**\n\n> **Q2. On the interpretability and practical issues of Assumption 4.5 (`W1w7:W7`; `W1w7:W2`; `w7Hq:Q5`).**\n\nRegarding interpretability, we attempted to analyze Corollary 4.4 (a special case of Corollary 4.3 for HNM) and the entire Appendix B.2 (which introduces a Gaussian noise assumption). However, we found its interpretation remains difficult and largely qualitative (e.g., we added an empirical validation of Appendix B.2 in Appendix D.3, page 28). This is a trade-off: Assumption 4.5 allows Corollary 4.3 to work directly in **general non-linear, non-additive settings** (addressing `vus6:Q2`), thus broadening the theoretical and practical scope of FICO. Experiments also show this assumption holds in most synthetic datasets. However, Assumption 4.5 has inherent interpretation difficulties. For instance, even under the ANM assumption, it requires a rather complex condition (CaPS; Xu et al., 2024), although this condition can be formally expressed rather than only qualitatively described.\n\nRegarding practical use, our method underperforms on the Sachs dataset. Our results show that all score-based methods perform poorly on Sachs, suggesting that their **common underlying assumptions may be violated** in this case. We can only offer a qualitative analysis here, based on the conclusions in Appendix B.2. In short, we found that for the standardized Sachs distribution, the variance of the effect w.r.t. the cause changes too drastically. This violates the ideal conditions in lines 1103-1108, making it highly likely that Assumption 4.5 is not satisfied."}, "title": {"value": "Gloabl Author Rebuttal (Part I)"}}, "id": "GSFuIWDrjo", "forum": "80vdaC5DsD", "replyto": "80vdaC5DsD", "signatures": ["ICLR.cc/2026/Conference/Submission1790/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1790/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1790/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763644705650, "cdate": 1763644705650, "tmdate": 1763645055854, "mdate": 1763645055854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose estimating the quantile partial effect (QPE) as a means of functional causal discovery. The QPE generalizes causal velocity to the distributional level, which avoids making SCM assumptions. They derive novel conditions for both identifying causal direction in the bivariate case as well as sink nodes in a multivariate setting. Based on these conditions, they propose two methods for bivariate causal discovery (QPE-f, QPE-k) and an order-based approach for the multivariate case (FICO)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The authors propose a Wronskian criterion for bivariate causal discovery that is not only theoretically interesting but also directly informs novel methodology based on a practical test, which shows strong performance in the common benchmarks. \n- Mathematical erivation and discussions (especially in the Appendix) are complete and rigourous\n- Experiments are exceptionally comprehensive and easy to follow \n- Overall writing is clear, the paper is well-structured and logical"}, "weaknesses": {"value": "- The theoretical generalization beyond causal velocity is limited in practice, since the discussion in Appendix A.2 on identifiability still requires the SCM assumption which equates QPE and causal velocity. \n- Assumption 4.5 for FICO is specifically designed for Corollary 4.3 and is difficult to interpret in practice (although I appreciate the authors acknowledging this).\n- The resulting algorithm, FICO, seems very similar to the SCORE algorithm (Rolland et al., 2022)."}, "questions": {"value": "- Question on your Appendix A.2: You show that, given a joint distribution (up to $p(x)$) the class of forward models $p(y|x)$ are non-parametric, while the class of backward models $p(x|y)$ are parametric, based on an extension of the ANM argument in Hoyer et al., 2008. My question is this: does the (k+2)-th order ODE for $\\log p(x)$ somehow depend on the anti-causal model, either via $\\eta_{X \\mid Y}$ or via the basis functions $\\phi(x)$ in Assumption 3.5? And if it does, will changing these unknown properties also change the ODE? \n\n- Question on FICO: can you clarify the relationship between FICO and SCORE? You still have to estimate the score function for FICO, correct? SCORE looks for the smallest variance in score across nodes, while FICO looks for the smallest FI, or squared score. But asymptotically under mild conditions the FI and variance are the same (the experimental results also look very similar). Is Corollary 4.3 a condition on when we expect SCORE to work for non-linear, non-additive noise models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yhnrW0ZE2N", "forum": "80vdaC5DsD", "replyto": "80vdaC5DsD", "signatures": ["ICLR.cc/2026/Conference/Submission1790/Reviewer_vus6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1790/Reviewer_vus6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761561612773, "cdate": 1761561612773, "tmdate": 1762915890550, "mdate": 1762915890550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an approach to causal discovery from observational data, based on the notion of quantile partial effect (QPE). In particular, three distinct approaches utilizing QPE are proposed:\n\n(i) A kernel-based, non-parametric approach for bivariate discovery which assumes that the QPE can be represented as a linear combination of known basis functions,\n\n(ii) A parametric approach for bivariate discovery using the idea of normalizing flow, where each flow is parameterized by a neural network,\n\n(iii) A parametric approach, which recovers a causal ordering among a set of variables, based on the Fisher information matrix."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(S1) Causal discovery is an important topic in causal inference, and further advances in this context are needed. The paper deals with an important topic.\n\n(S2) The framing of previous methods (LiNGAM, ANM, HNM) as specific cases of the general setting in which QPE lies in the linear span of fixed basis functions, seems interesting and novel. Getting a generalization of current approaches assuming different functional constraints is a good direction to explore.\n\n(S3) The performance of the flow-based (neural) approach for QPE testing seems to perform quite well empirically."}, "weaknesses": {"value": "(W1) The exposition of the paper could be substantially improved. A range of assumptions are discussed, and in the current version, it is non-trivial to follow for which of the proposed approaches exactly which assumptions are used. This could be streamlined through an overview table.\n\n(W2) Causal Velocity comparison is misleading: Some of the language around causal velocity / bijective causal models seems concerning (abstract, after Proposition 3.3). \n\nWhen a bijective causal model assumption is invoked, some collapse of the layers does occur. This seems to be an important point for causal velocity.\n\nQPE is defined purely at the observational level, and does not pertain to any structural assumptions — it is simply a functional of the underlying observational distribution. Therefore, comparing these two in such a way (saying that “QPE does not need a monotonicity assumption”) seems quite confusing. It seems more likely that (i) the definition of QPE does not require monotonicity; but (ii) for the proposed discovery methods to work, monotonicity is assumed?\n\n(W3) Removal of the Markovian assumption: the claims on the removal of the Markov assumption are very curious. Most work in causal discovery using functional assumptions considers the Markovian case. If the authors are truly relaxing this assumptions, this seems like a big deal, and should be emphasized strongly. If not, explaining exactly how the Markov assumption is “not needed” is very important — is the case again that QPE can be defined without invoking the Markov assumption, but to apply QPE-based discovery methods, the Markov assumption is needed? If the latter is true, these claims need to be reframed properly.\n\n(W4) It is not uncommon for causal discovery methods to provide some form of formal guarantees that the method will work with high probability etc. Are the formal guarantees needed / missing from the paper?\n\n(W5) Line 289 states that “OLS assumes a Gaussian distribution”. OLS is a method, which does not assume any distribution implicitly. Of course, with Gaussian noise, OLS may exhibit specific properties (become the MLE etc.). Clarifying this would be important.\nFurthermore, in this setting arbitrary distribution of $x_t, y_t$ can now be handled — is this previously not the case? This is definitely worth clarifying.\nThis lack of clarity on the exact assumptions connects back to point (W1).\n\n(W6) In Lines 338, it is said that $\\partial_y \\psi_{Y | X,i} = 0$ is “e.g. ANM”. To be clear, in this context, additive noise is assumed in addition to the fully parametric model used in this setting? \n\nANM usually allows arbitrary functions $a(X)$; which is not the case here? It would be more precise to say “additive noise” than ANM in this case?\n\nSame comment for the “e.g. HNM” model.\n\n(W7) Assumption 4.5 seems to be important for the FisherInfo-based approach. I currently have no idea how one might judge the validity of such an assumption, or even start thinking about it? It seems that this is insufficiently discussed, and raises the concern whether the exact condition needed for the method is assumed, regardless of its plausibility.\n\nI note that I would be willing to reconsider my grade in light of the authors' responses to weaknesses / questions."}, "questions": {"value": "(Q1) Is the representation in Table 1 actually true?: specifically, for PNL-ANM and PNL-AHM models, the basis functions $\\bar g, g^{-1}\\bar g$, etc. are needed. Isn’t it generally the case that the function $g(\\cdot)$ is not known in these settings? How does this tie with the fact that the basis functions in $\\Phi$ need to be known?\n\n(Q2) Relating to the previous point, is there any nuance w.r.t. choosing the basis? The choice of the kernel bandwidth parameter is mentioned as a limitation, but the choice of basis (which is even more fundamental?) is not really mentioned.\n\n(Q3) The performance of QPE-f seems to be very strong empirically. I wonder, for settings where the data-generating model is actually an ANM, does the ANM method perform better? (similar question for LiNGAM & others). If I understand, current experiments would seem to imply the answer is no? If so, it is curious.\n\n(Q4) Relating to previous point — given the strong performance of QPE-f, is there any way to move beyond bivariate discovery with such an approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lrhexnM618", "forum": "80vdaC5DsD", "replyto": "80vdaC5DsD", "signatures": ["ICLR.cc/2026/Conference/Submission1790/Reviewer_W1w7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1790/Reviewer_W1w7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860430979, "cdate": 1761860430979, "tmdate": 1762915890251, "mdate": 1762915890251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies causal discovery from observational data using distribution shape constraints. Authors motivate non-graphical assumptions about the gradient of the joint distribution that emphasize the asymmetry between the cause and effect in a bivariate case, and extend the results to multivariate causal discovery by iteratively finding the last variable in the causal order. Through examples, they show how the shape constraints introduced in their work is distinct from the graphical constraints commonly employed in this problem, and experiments validate usefulness of their approach."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Clear statement of the assumptions underlying the theoretical results.\n2. Motivating examples throughout the paper allow the more curious reader to understand the motivation.\n3. Algorithm 1 is impressively simple, and the connection between fisher information and the shape constraints are quite insightful."}, "weaknesses": {"value": "1. Appendix A3 contains very important remarks that deserve to be presented in the main text. The paper argues that the shape assumptions are more *relaxed* than the existing assumptions employed in causal discovery, and this subtle points needs to be established carefully, or else the reader may find the contributions not well-motivated.\n2. Sections 3.3 and 3.4 are not contributing to what comes later in the manuscript; they are useful extensions of 3.2, though, I recommend a restructuring as they hurt the coherency.\n3. Missing discussion about related work [1,2,3,4] that would enrich the work.\n\n[1] Hyvarinen, A., Sasaki, H., & Turner, R. (2019, April). Nonlinear ICA using auxiliary variables and generalized contrastive learning. In The 22nd international conference on artificial intelligence and statistics (pp. 859-868). PMLR.\n\n[2] Jalaldoust, K., Salehkaleybar, S., & Kiyavash, N. (2025). Multi-domain causal discovery in bijective causal models. In Proceedings of the Fourth Conference on Causal Learning and Reasoning.\n\n\n[3] Guo, S., Tóth, V., Schölkopf, B., & Huszár, F. (2023, December). Causal de finetti: On the identification of invariant causal structure in exchangeable data. In Thirty-seventh Conference on Neural Information Processing Systems.\n\n[4] Reizinger, P., Sharma, Y., Bethge, M., Schölkopf, B., Huszár, F., & Brendel, W. (2023). Jacobian-based causal discovery with nonlinear ICA. Transactions on Machine Learning Research."}, "questions": {"value": "Please address my concerns above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J0UUB035d9", "forum": "80vdaC5DsD", "replyto": "80vdaC5DsD", "signatures": ["ICLR.cc/2026/Conference/Submission1790/Reviewer_mFRa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1790/Reviewer_mFRa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053724336, "cdate": 1762053724336, "tmdate": 1762915890058, "mdate": 1762915890058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}