{"id": "10iBNwPtl2", "number": 2793, "cdate": 1757251831388, "mdate": 1759898126945, "content": {"title": "Dynamic Novel View Synthesis in High Dynamic Range", "abstract": "High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D model from Low Dynamic Range (LDR) training images captured under conventional imaging conditions. Current methods primarily focus on static scenes, implicitly assuming all scene elements remain stationary and non-living. However, real-world scenarios frequently feature dynamic elements, such as moving objects, varying lighting conditions, and other temporal events, thereby presenting a significantly more challenging scenario. \nTo address this gap, we propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR DNVS), where the additional dimension ``Dynamic'' emphasizes the necessity of jointly modeling temporal radiance variations alongside sophisticated 3D translation between LDR and HDR. To tackle this complex, intertwined challenge, we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an innovative dynamic tone-mapping module that explicitly connects HDR and LDR domains, maintaining temporal radiance coherence by dynamically adapting tone-mapping functions according to the evolving radiance distributions across the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance consistency and spatially accurate color translation, enabling photorealistic HDR renderings from arbitrary viewpoints and time instances.\nExtensive experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art methods in both quantitative performance and visual fidelity. Source code will be released.", "tldr": "", "keywords": ["High Dynamic Range", "4D Gaussian Splatting", "Dynamic Scene Modeling", "Deep Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/df03f8966d892a5050a253253feda1b589ec0830.pdf", "supplementary_material": "/attachment/385dc5ba1e4fcdc4795b32ac72e6bd908e0cd9f6.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies a new problem – high dynamic range dynamic novel view synthesis (HDR DNVS). First of all, the authors collect a 4D synthetic datasets with 8 scenes and a 4D real datasets with 4 scenes. On the set up benchmark, the authors develop a HDR-4DGS, featuring dynamic tone-mapping for adaptively bridging the LDR and HDR domains under complex spatiotemporal variations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "(i) This paper studies a brand new problem - high dynamic range dynamic novel view synthesis (HDR DNVS), which has not been explore in the literature. This is novel and interesting. To this end, the authors made a dataset contribution – collecting both synthetic and real datasets to support the research. This is solid, non-trivial, and labor-intensive.\n\n(ii) The proposed biologically inspired dynamic tone-mapping module also shows deep insights. This module draws inspiration from human visual adaptation, where retinal photoreceptors dynamically adjust to ambient brightness. The proposed HDR-4DGS includes a dynamic radiance context learner that models temporal radiance distributions, as shown in Figure 1.\n\n(iii) The writing is good and easy to follow. Especially the method part, all the technical details are very clear. The presentation is also well-dressed. For example, the workflow of the overall pipeline can be clearly shown in Figure 1. In addition, the arrangement of the paper's figures and tables is very neat such as Tables 1, 2 and Figures 2, 3.\n\n(iv) The performance is very solid. As compared in Tables 1 and 2, the proposed HDR-4DGS achieves much better reconstruction quality while keeping a very fast speed. The visual comparisons in figures 2 and 3 also suggest that the proposed HDR-4DGS can render more visually pleasant results with less blur and artifacts.\n\n(v) The ablation study is also very comprehensive. The effects of dynamic tone mapper, dynamic radiance context learner, pixel-level supervision, DRCL design, and the temporal context length are discussed in details."}, "weaknesses": {"value": "(i) Although the authors claim the datasets and the new algorithms are their contributions, there are no datasets or codes submitted. The reproducibility cannot be checked.\n\n(ii) The authors claim the dataset is also a contribution. But there are very few words in the paper introduce how to construct the datasets. It would better if there is a section to describe the details of data collection.\n\n(iii) As the authors claim, the designed method is a 4D Gaussian Splatting. But I did not see any preliminary or method part introduce how to adapt 4DGS to the HDR-DNVS task."}, "questions": {"value": "What is the difference between the designed method with HDR-GS + the k-planes in 4DGS? How about the performance of HDR-GS + the k-planes in 4DGS."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "T4wLbhQUBe", "forum": "10iBNwPtl2", "replyto": "10iBNwPtl2", "signatures": ["ICLR.cc/2026/Conference/Submission2793/Reviewer_yRau"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2793/Reviewer_yRau"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761526484919, "cdate": 1761526484919, "tmdate": 1762916379923, "mdate": 1762916379923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on learning HDR 3D representations of dynamic scenes from LDR images. Unlike previous methods that are limited to static scenes, this work introduces a dynamic tone-mapping module to achieve adaptive color transformation. By integrating this module with the Gaussian splatting representation, the proposed approach enables HDR rendering from arbitrary viewpoints and time instances. The method is evaluated on both real and synthetic datasets, with some experiments confirming its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper extends HDR modeling from static to dynamic scenes by proposing a novel HDR-4DGS framework.\n- The paper introduces a dynamic tone-mapping module to handle color translation in dynamic scenes, enabling the rendering of HDR images.\n- The method is evaluated on both synthetic and real datasets, and the qualitative and quantitative experiments shown in the paper suggest that it is partially effective in performing color transformations."}, "weaknesses": {"value": "This work proposes a dynamic HDR scene modeling framework that extends Gaussian Splatting to handle dynamic scenes. While the method achieves improvements on part of the experimental data, it still exhibits several concerns, as summarized below：\n\n- Limited performance. As shown in the local results in Figures 2 and 3, the proposed method struggles to recover fine-texture details, and the synthesized novel views exhibit noticeable noise. The authors are encouraged to provide a more detailed analysis of these experimental results to support the effectiveness of the proposed approach better. Furthermore, based on the demo results, the method appears unable to preserve the geometry of moving objects in real-world scenes, often leading to structural loss during deformation. In addition, the DVS results show temporal flickering artifacts, indicating instability in dynamic rendering.\n\n- Limited novelty. According to the paper, the dynamic scene modeling part is adopted from 4DGS, while the per-channel tone-mapping functions adopt the structure from HDR-GS. Thus, the contribution primarily lies in combining these existing components with an additional temporal HDR learning, resulting in an improvement over prior methods that is relatively incremental.\n\n- Insufficient constraints of dynamic Gaussian deformation. As shown in Section 3.4, the method primarily supervises and optimizes the mapping between LDR and HDR domains. Still, it lacks explicit mechanisms or loss functions to control or regularize the deformation of dynamic Gaussians, which are essential for preserving the geometry and spatiotemporal consistency of moving objects. However, such constraints are crucial for dynamic scene representation and novel-view synthesis, and this aspect appears to be missing in this paper.\n\n- Some comparative experiments are missing. Since the proposed method targets dynamic scenes, whereas the comparison methods in the paper mainly focus on demonstrating HDR visual quality, the authors could first map LDR images to HDR and then compare them with dynamic NeRF and Gaussian Splatting approaches to evaluate performance in terms of geometric accuracy and spatiotemporal consistency for moving objects."}, "questions": {"value": "Based on the aforementioned weaknesses, the following issues also require discussion:\n\n- In real-world scenes, as shown in Fig. 2, when using both LDR and HDR images as supervision for HDR rendering, the proposed method performs significantly worse than HDR-HexPlane in quantitative metrics. The authors should provide a clear and thorough analysis to explain this performance gap.\n\n- The paper reports strong results on synthetic data but only limited improvement on real-world datasets. Can you elaborate on whether this discrepancy arises from domain differences, noise in HDR ground truth, or instability in the tone-mapping process?\n\n- The authors could consider providing the output luminance as a function of input intensity for different time frames or varying brightness levels (i.e., a dynamic CRF). This would provide more convincing evidence that the proposed DTM has indeed learned dynamic mapping patterns, rather than relying solely on final HDR rendering metrics.\n\n[Minor]:\n- In Figure 1, the notation for the HDR and LDR models appears to be identical, which may cause confusion regarding their respective roles in the framework. And the equations shown in this figure should be formatted consistently with those in the manuscript to ensure clarity and uniform presentation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KYMHSU5EC4", "forum": "10iBNwPtl2", "replyto": "10iBNwPtl2", "signatures": ["ICLR.cc/2026/Conference/Submission2793/Reviewer_u72U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2793/Reviewer_u72U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761611871453, "cdate": 1761611871453, "tmdate": 1762916379624, "mdate": 1762916379624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of existing High Dynamic Range Novel View Synthesis (HDR NVS) methods, which are confined to static scenes, and Dynamic Novel View Synthesis (DNVS) approaches that lack HDR support. It formally introduces the task of HDR Dynamic Novel View Synthesis (HDR DNVS) for the first time.\n\nTo tackle the core challenges of spatiotemporal radiance consistency and LDR-HDR domain translation in this task, the authors propose the HDR-4DGS framework. Built on 4D Gaussian Splatting, this framework innovatively integrates a Dynamic Tone-Mapping Module (DTM). The DTM stores temporal radiance statistics via a radiance bank, captures temporal variations using a dynamic radiance context learner, and drives per-channel tone-mapping functions to enable adaptive domain translation.\n\nAdditionally, the paper constructs two benchmark datasets: HDR-4D-Syn (8 synthetic scenes) and HDR-4D-Real (4 real-world scenes), which provide HDR ground truth and multi-view LDR observations. Experiments demonstrate that HDR-4DGS outperforms existing methods in both quantitative metrics (e.g., PSNR, SSIM) and visual quality, while balancing training efficiency and inference speed, offering an effective solution for high-fidelity HDR novel view synthesis of dynamic scenes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed task of HDR Dynamic Novel View Synthesis (HDR DNVS) is a novel and necessary one. Existing methods either focus on static scenes for HDR Novel View Synthesis (HDR NVS) or are limited to Low Dynamic Range (LDR) inputs for Dynamic Novel View Synthesis (DNVS), failing to handle real-world dynamic scenarios with time-varying geometry, illumination, and high-contrast radiance. This gap significantly restricts practical applicability, making the introduction of HDR DNVS a timely and problem-solving initiative.\n\n2. The newly proposed Dynamic Tone-Mapping (DTM) module shows considerable merit, with a clear and logically coherent presentation in the paper. By constructing a radiance bank to store temporal radiance statistics, integrating a dynamic radiance context learner to model temporal variations, and designing per-channel tone-mapping functions, the module explicitly bridges the HDR-LDR domains while maintaining spatiotemporal radiance consistency. Its design draws reasonable inspiration from human visual adaptation, and the technical details (e.g., sliding window for context extraction, joint use of exposure time) are well-explained, making the module’s working mechanism easy to follow.\n\n3. The experimental design is comprehensive and rigorous. First, the authors build two benchmark datasets (HDR-4D-Syn with 8 synthetic scenes and HDR-4D-Real with 4 real-world scenes) that provide essential HDR ground truth and multi-view LDR data for evaluating HDR DNVS, filling the lack of existing benchmarks. Second, they compare HDR-4DGS with multiple state-of-the-art methods (e.g., HDR-NeRF, HDR-GS, HDR-HexPlane) across quantitative metrics (PSNR, SSIM, LPIPS) and qualitative visualizations. Additionally, ablation studies verify the effectiveness of the DTM module and key components (e.g., pixel-level supervision, GRU-based context learner), ensuring the reliability of the results"}, "weaknesses": {"value": "1.The scale of the constructed datasets appears relatively limited, which may restrict further in-depth analysis and generalization verification. Specifically, HDR-4D-Syn only includes 8 synthetic scenes and HDR-4D-Real has 4 real-world indoor scenes, with a lack of diversity in scene types (e.g., no outdoor dynamic scenes with complex natural lighting changes, or scenes with more diverse object motions). Such a small dataset not only makes it difficult to fully validate the model’s robustness across different dynamic scenarios but also cannot support additional explorations (e.g., analyzing the model’s performance under varying motion intensities or lighting contrasts). This limitation may weaken the persuasiveness of the model’s practical applicability in broader real-world contexts.\n\n2.The proposed Dynamic Tone-Mapping (DTM) module, though conceptually reasonable, seems relatively general as it lacks testing on the dataset associated with HDR-GS (a representative method for static HDR Novel View Synthesis). HDR-GS is a key baseline in the HDR NVS field, and its dataset likely contains rich HDR-LDR paired data suitable for evaluating tone-mapping performance. Without testing the DTM module on HDR-GS’s dataset, it is hard to fully confirm whether the module can effectively adapt to existing HDR-related data distributions, nor can it more comprehensively demonstrate the module’s advantages over tone-mapping components in static HDR NVS methods. This gap makes the module’s comparative advantage in the broader HDR tone-mapping context less fully validated ."}, "questions": {"value": "same as weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4TLRmpv9tf", "forum": "10iBNwPtl2", "replyto": "10iBNwPtl2", "signatures": ["ICLR.cc/2026/Conference/Submission2793/Reviewer_snwX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2793/Reviewer_snwX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729638706, "cdate": 1761729638706, "tmdate": 1762916379424, "mdate": 1762916379424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the High Dynamic Range Dynamic Novel View Synthesis (HDR DNVS) task, aiming to reconstruct spatiotemporally consistent HDR radiance fields and dynamic geometry from sparse, time-varying LDR inputs. It introduces the HDR-4DGS framework based on Gaussian Splatting, featuring a dynamic tone-mapping module (DTM) to bridge HDR-LDR domains while maintaining temporal radiance coherence. Two datasets (HDR-4D-Syn with 8 synthetic scenes and HDR-4D-Real with 4 real indoor scenes) are constructed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe DTM, with a radiance bank and dynamic radiance context learner (DRCL), maintains spatiotemporal consistency, validated by ablation studies .\n2.\tConstructs dedicated datasets (HDR-4D-Syn/Real) with HDR ground truth and time-varying 3D geometry, enabling standardized evaluation ."}, "weaknesses": {"value": "1.\tGiven that most existing Dynamic Novel View Synthesis (DNVS) methods operate on low dynamic range (LDR) imagery and thus fail under high-contrast or temporally varying lighting, do you consider dynamic HDR modeling essential for achieving photorealistic results in real-world dynamic scenes? In other words, is extending DNVS to HDR space a fundamental requirement to overcome the perceptual and photometric limitations of LDR-based systems, rather than just an optional enhancement?\n2.\tNo comparison with \"dynamic scene reconstruction + independent HDR module\" combinations, lacking proof of irreplaceability.\n3.\tWhen evaluating HDR-4DGS on the HDR-4D-Real dataset, the input LDR images are captured with six synchronized iPhone 14 Pro devices under three exposure times. However, real-world dynamic HDR scenarios may involve more diverse camera models (with different CRFs) or unsynchronized multi-view capture. Why is only a single type of synchronized device used for data collection, and has the method been tested for robustness to camera model differences or capture asynchrony?\n4.\tIn the ablation study on temporal context length (k=20 as optimal), the paper only tests k=5,10,20,30. However, dynamic scenes with varying motion speeds (e.g., fast-moving objects vs. slow illumination changes) may require different k values. Did the authors design experiments to verify whether the optimal k=20 is adaptive to scenes with different dynamic intensities, or is it a fixed hyperparameter that needs manual adjustment for specific scenes?\n5.\tIs the proposed method able to address issues like extreme lighting performance?"}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ie2oeCxPbJ", "forum": "10iBNwPtl2", "replyto": "10iBNwPtl2", "signatures": ["ICLR.cc/2026/Conference/Submission2793/Reviewer_y3DV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2793/Reviewer_y3DV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905048725, "cdate": 1761905048725, "tmdate": 1762916379175, "mdate": 1762916379175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}