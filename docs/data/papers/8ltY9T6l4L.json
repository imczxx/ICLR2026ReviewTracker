{"id": "8ltY9T6l4L", "number": 1424, "cdate": 1756881420880, "mdate": 1759898209909, "content": {"title": "Handling Tabular Data under Coupled Shifts of Feature Missingness and Distributional Change", "abstract": "Tabular data plays a vital role in a wide range of real-world applications. \nHowever, previous methods for tabular data learning primarily focused on closed environments, overlooking the fact that feature missingness and distributional shift issues can occur simultaneously in open environments.\nIn this paper, we first investigate \\textbf{R}obust \\textbf{T}abular prediction under the \\textbf{C}oupled \\textbf{S}hifts of feature missingness and distributional change, namely \\setting problem.\nWe identify three challenges in \\setting, where column missingness and distribution shifts are interdependent and mutually inhibitive: (1) the coexistence of column missingness and distribution shifts leads to severe performance degradation, for which no effective solutions currently exist; (2) under distribution shifts, it is inherently difficult to obtain reliable statistical patterns for imputing missing features; and (3) mitigating information loss from missing features while maintaining robustness to distribution shifts remains highly challenging.\nTo this end, we propose \\textbf{K}nowledge-\\textbf{G}uided \\textbf{C}oupled \\textbf{S}hift handler for \\textbf{Tab}ular data, namely \\algo, \nwhich effectively disentangles feature missingness from distribution shifts by performing column imputation by constructing Knowledge-Guided recovery rules, and adapts to unknown distributions through model selection with theoretical guarantee. Experimental results demonstrate that \\algo achieves a nearly 20\\% performance gain.", "tldr": "", "keywords": ["Tabular data", "machine learning", "weakly supervised learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/372a7d087e74eba63768f8b845093d284c0a399e.pdf", "supplementary_material": "/attachment/94a07af3dd3e97fb0c216dc6f36cfb1ec95a0966.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the problem of robust tabular learning under the combined challenges of missing features and distribution shifts in test data. The authors propose a two-module solution: a knowledge-guided feature aligner (KGFA) that imputes the missing feature via proposing imputation rules, and a distribution-aware model-selection (DAMS) module that selects the most appropriate model based on the detected distribution shift. The proposed method is evaluated on several benchmark datasets, demonstrating its effectiveness compared to existing baselines."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper points a realistic problem in the tabular learning area, where the test data is having both missing features and distribution shifts. This would cause tabular model performance degradation and none of the existing robustness methods can handle well.\n2. The paper proposes a systematical solution to address the problem, which includes a missing-feature imputation module and a distribution-aware model-selection module. The two modules are well designed and supported with theoretical fundations.\n3. The experimental results on various baselines demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. Although the impact of the proposed problem is proven by experiments, the novelty of the problem itself is limited. In other words, suppose the missing feature is problem A, and the distribution shift is problem B. Then if there is a new problem C called \"new features appearing in test data\" coming up, then we can create a list of problem combinations of AB, AC, BC, and ABC. Therefore, the proposed problem is just one of many possible combinations of existing problems.\n2. The proposed method tries to train multiple models for different possible feature missing situations, which may not be scalable when the number of possible missing columns is large"}, "questions": {"value": "1. In Fig.9, the paper compares the proposed method with LLM based baselines, which is an odd choice since CatBoost and XGBoost are generally performing better than LLMs on tabular data. What is the reason for choosing LLM baselines here? Are there any results of comparing with CatBoost/XGBoost on the same setting?\n2. For the ablation study, how is without DAMS implemented? Is it using a single model trained on complete data and testing on missing+shifted data? How is the performance of ensembling multiple models without DAMS (e.g., averaging the outputs of all models) compared to using DAMS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UgE5NDeTWr", "forum": "8ltY9T6l4L", "replyto": "8ltY9T6l4L", "signatures": ["ICLR.cc/2026/Conference/Submission1424/Reviewer_nxRD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1424/Reviewer_nxRD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761299187330, "cdate": 1761299187330, "tmdate": 1762915766494, "mdate": 1762915766494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the RTCS problem — Robust Tabular prediction under Coupled Shifts — where both: Feature missingness (entire feature columns missing at test time), and Distribution shifts (differences between training and test distributions) occur simultaneously. To mitigate it, it devises a KGFA(knowledge-guided feature aligner) and DAMS (model selector). Overall, the method shows reasonable gains among most datasets,most pronounced upon Hyperextension, for almost all tabular backbone models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Use of external knowledge\n* using \"interpretable\" external imputation rules seem quite novel in tabular data. Furthermore, validating the rules with a held-out distribution (almost) guarantees its effect upon mild test distributions\n2. Theoretical guarantee\n* theoretical guarantee upon the DAMS's efficient compute of distance."}, "weaknesses": {"value": "1. Lack of comparisons with Imputation methods utilizing LLMs\n* similar to that of the proposed method, there has been spark of works which have performed imputations using LLMs. specifically, https://openreview.net/pdf?id=8F6bws5JBy. I wish the authors could provide (if not this paper) additional comparisons of performance between 1. imputation + model inference/training vs. 2. proposed method, to further solidify its core idea.\n2. Lack of analysis of \"upper bound\"\n* both DAMS and RTCS seem to have an \"upper bound\", due to its 1. human interpretable nature and 2. the efficacy scheme. However, the authors did not conduct any analysis upon this. I wish the authors could create a synthetic dataset, with \"predefined correlation rules\", and check if the \"rule proposer\" can correctly grasp the correlation in both : 1) column names with inherent correlation in their naming (i.e. BMI & height) and 2) without it. 2) is necessary, since it is not always the case where the columns are that easy to comprehend & reason their correlation upon - for example, the stats of the gyroscope sensor where the column names are X, Y and Z.  Furthermore, for the DAMS, the authors should provide the upperbound for performance gains when model selections were done upon \"ground-truth\" distribution distance matrix, and up to how much percentage does DAMS recover, with its efficiency tradeoff. \n\nIf both these concerns are resolved, I'm willing to raise my score."}, "questions": {"value": "please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Bnr4aAwg90", "forum": "8ltY9T6l4L", "replyto": "8ltY9T6l4L", "signatures": ["ICLR.cc/2026/Conference/Submission1424/Reviewer_6gUQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1424/Reviewer_6gUQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578086247, "cdate": 1761578086247, "tmdate": 1762915766167, "mdate": 1762915766167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes KGCS4Tab, a framework addressing robust tabular prediction under coupled feature missingness and distribution shifts. It integrates large language models for knowledge-guided imputation and a distribution-aware selector for adaptive model choice. The formulation of the coupled-shift problem is novel, and the method achieves consistent empirical gains across several benchmarks. Overall, it’s an ambitious and creative step toward robust tabular learning in open environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper defines a new and practical problem setting (RTCS) where missingness and distribution shift coexist, highlighting their compounded effect.\n\n- It also introduces a novel combination of LLM-guided imputation and probabilistic model selection, linking knowledge reasoning with robustness.\n\n- KGCS4Tab provides solid empirical results showing consistent gains across datasets and backbones.\n\n- This paper also includes theoretical grounding and ablation studies that support the design choices and demonstrate the complementarity of modules."}, "weaknesses": {"value": "### **Problem & Motivation**\n- Figure 2 presents an interesting observation that the degradation under coupled shifts exceeds the additive effect of each individual shift. However, it is unclear whether this property consistently holds across all datasets or was selectively demonstrated. More clarification is needed on how the distribution shifts and missingness were injected to verify that the finding is robust rather than anecdotal.\n\n- The related-work section divides distribution-shift robustness into domain generalization and test-time adaptation, but omits domain adaptation, which is one of the core paradigms in this area. A short discussion connecting domain adaptation to the proposed problem setting would make the taxonomy more complete.\n\n- The conceptual separation between feature missingness and distribution shift also needs stronger justification. Once missing features are imputed, the resulting data distribution effectively changes, making the two phenomena difficult to treat as orthogonal. A more principled explanation or theoretical framing for why they should be considered distinct would strengthen the motivation.\n\n- The assumption that entire columns are missing seems restrictive. In most real-world cases, only partial feature missingness occurs. It would be valuable to know whether the method also performs well under partial missingness and how sensitive it is to the degree of missingness.\n\n- Overall, while the paper identifies a practical and underexplored problem, a more fundamental and thorough analysis of the coupled effect—beyond the three empirical observations—would make the problem formulation more compelling.\n\n---\n\n### **Method**\n\n- The method treats each column independently when estimating distribution distances or generating imputation rules, which is questionable because tabular features are often correlated. Ignoring these inter-feature dependencies could limit the robustness of the approach in realistic settings.\n\n- LightGBM is used as the primary base learner, though CatBoost generally achieves stronger performance on categorical tabular data. Explaining this design choice would make the experimental setup easier to interpret.\n\n- The framework depends heavily on a large language model, yet the type of model and its prompting strategy are not clearly specified. Since performance could vary significantly across LLMs, the paper should explicitly state which model was used and whether different LLMs lead to consistent behavior.\n\n---\n\n### **Experiments**\n\n- All experiments appear to inject missingness artificially. It would strengthen the contribution to include a setting where missingness occurs naturally in real data, since the current setup mainly verifies synthetic conditions.\n\n- The experiments focus solely on binary classification tasks. Evaluating the framework on multiclass or regression problems would demonstrate broader applicability.\n\n- The relatively small gain on the ANES dataset remains unexplained. The text attributes it to lower feature discriminative power, but the claim is not quantitatively supported. Providing evidence, such as feature importance or variance analysis, would make the argument convincing.\n\n- Because the method includes an imputation module, it would be appropriate to evaluate its imputation quality directly. Comparing against common imputation baselines such as mean, k-NN, MICE, or GAIN and reporting reconstruction error or predictive performance would clarify where the improvement originates.\n\n- Finally, the experiments assume that the identity of the missing column (c_m) is known in advance. This assumption risks data leakage and may not hold in real deployments. The authors should clarify whether (c_m) is predefined or detected automatically, and discuss the implications if this information is unavailable."}, "questions": {"value": "- In Table 1, the authors use IRM as the representative distribution-shift baseline, but it is not obvious why this particular method was chosen over other general baselines such as DRO, EQRM, or ERM++. It would be helpful to explain whether the result generalizes across these variants or is specific to IRM.\n\n- The description of the Knowledge-Guided Feature Aligner is also too abstract. The definition of the imputation rule (r) lacks sufficient detail on its structure, input–output form, and examples. Showing only a single rule in Figure 5 does not adequately illustrate how such rules are generated or applied. Including a concrete prompt example or pseudocode would clarify this process.\n\n- The argument that (r) cannot be directly applied to the test set due to a “semantic gap” is difficult to understand. The paper should elaborate on what this semantic gap entails and why it prevents the straightforward application of the learned rules."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DSzsyQiZW7", "forum": "8ltY9T6l4L", "replyto": "8ltY9T6l4L", "signatures": ["ICLR.cc/2026/Conference/Submission1424/Reviewer_Q3ez"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1424/Reviewer_Q3ez"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792076214, "cdate": 1761792076214, "tmdate": 1762915766036, "mdate": 1762915766036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the Robust Tabular Prediction under Coupled Shifts (RTCS) problem, where feature missingness and distributional shifts jointly affect performance. The authors propose KGCS4Tab, a knowledge-guided framework that disentangles missingness from distribution shifts through recovery rules and adaptive model selection. Experiments show that KGCS4Tab achieves promising performance improvement."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1: The motivations are clearly and thoroughly analyzed.\n\nS2: The applicability of this specific imputation setting is validated through real-world scenarios."}, "weaknesses": {"value": "W1: The theoretical analysis section (Theorem analysis) focuses on the probabilistic selection but is loosely connected to the imputation task. If the authors intend to devote such a large portion to theoretical content, they should provide a more explicit justification of how the theoretical results relate to or benefit the imputation process.\n\nW2: The mechanism of the Proposed Rule remains unclear. It seems capable of directly predicting missing values, raising the question of why an LLM–based imputation approach, possibly through generated code, would not suffice. The paper should clarify why the proposed complex pipeline is necessary and how it outperforms a simpler LLM-based alternative.\n\nW3: Line 195 mentions a distribution shift across domains, yet the experiments appear to be conducted within the same domain. The paper would be stronger with explicit cross-domain evaluations to support this claim.\n\nW4: The presentation quality needs improvement. For example, line 179 contains redundant sentences. Given some example of the generated rules could enhance the presentation of the paper."}, "questions": {"value": "Nan"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vH0k2Y8yMR", "forum": "8ltY9T6l4L", "replyto": "8ltY9T6l4L", "signatures": ["ICLR.cc/2026/Conference/Submission1424/Reviewer_48J6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1424/Reviewer_48J6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821068929, "cdate": 1761821068929, "tmdate": 1762915765854, "mdate": 1762915765854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}