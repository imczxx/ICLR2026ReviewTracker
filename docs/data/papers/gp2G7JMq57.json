{"id": "gp2G7JMq57", "number": 10551, "cdate": 1758175292241, "mdate": 1759897643830, "content": {"title": "Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum", "abstract": "The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often limited by the use of randomly or manually selected examples. These examples fail to account for both model-specific knowledge distributions and the intrinsic complexity of the tasks, resulting in suboptimal and unstable model performance. To address this, we propose a novel framework inspired by the pedagogical principle of ``tailored teaching with balanced difficulty''. We reframe prompt selection as a prompt curriculum design problem: constructing a well ordered set of training examples that align with the model’s current capabilities. Our approach integrates two complementary signals: (1) model-perceived difficulty, quantified through prediction disagreement in an active learning setup, capturing what the model itself finds challenging; and (2) intrinsic sample complexity, which measures the inherent difficulty of each question–image pair independently of any model. By jointly analyzing these signals, we develop a difficulty-balanced sampling strategy that ensures the selected prompt examples are diverse across both dimensions. Extensive experiments conducted on five challenging benchmarks and multiple popular Multimodal Large Language Models (MLLMs) demonstrate that our method yields substantial and consistent improvements and greatly reduces performance discrepancies caused by random sampling, providing a principled and robust approach for enhancing multimodal reasoning.", "tldr": "We boost multimodal reasoning by selecting prompts like a personalized curriculum, balancing what the model finds difficult (its uncertainty) with the inherent complexity of the questions.", "keywords": ["Multimodal Chain-of-Thought", "Reasoning", "Prompt Curriculum"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f125576144b52f0fdedb70eb5c06d1e49f25e47.pdf", "supplementary_material": "/attachment/d59aae6f847c511bed14eac1ec4cbb277e6b97b5.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel framework named CAMS to address the performance instability of Multimodal Chain-of-Thought (MCOT) caused by its reliance on randomly selected examples. Inspired by the principle of \"tailored teaching,\" CAMS reframes prompt selection as a systematic \"prompt curriculum design\" problem. It innovatively selects examples by considering two dimensions: 1) model-perceived difficulty (uncertainty), measured by the model's own prediction disagreement, and 2) intrinsic sample difficulty (complexity), judged by an independent assessor. By balancing examples from these two dimensions, the method demonstrates significant and stable performance improvements in reasoning across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The work's primary novelty lies in creatively fusing two distinct concepts—the model-centric view of uncertainty and the data-centric view of complexity—into a single, unified framework for example selection. Elevating prompt selection to \"prompt curriculum design\" provides a fresh and systematic perspective for the field.\n\n2. The experimental design is  robust. Comprehensive evaluations across five diverse benchmarks and three different mainstream models strongly support the method's generalization ability and model-agnosticism. The thorough ablation studies also clearly validate the necessity of each core component of the framework.\n\n3. The paper is well-structured and highly readable. \n\n4. This research directly addresses a critical pain point in current large model applications: prompt fragility and instability. It provides a systematic, reproducible solution that significantly enhances the reliability and performance of MCOT for complex reasoning, holding substantial practical value."}, "weaknesses": {"value": "1. Limited Scope of the Complexity Assessor and Its Potential Impact on Generalization: The paper's core innovation, the \"complexity\" dimension, is defined in a way that equates complexity with structural difficulty (i.e., more reasoning steps). This approach is effective for problems where complexity and difficulty are aligned. However, it may fail on tasks where difficulty stems from conceptual insight rather than procedural length. The paper's own results (Figure 4) hint at this, showing a smaller performance gain in the language science (LAN) domain. This raises concerns about the method's effectiveness on higher-level reasoning tasks where a key insight, not structural complexity, is the primary barrier.\n\n2. Lack of Transparency and Reproducibility in the Complexity Assessor's Training: A key step is the \"Evol-Complexity\" method, which uses ChatGPT to iteratively increase question difficulty to generate training data. However, the paper does not provide the specific prompts and examples used to guide this process. This makes the core module a \"black box,\" hindering reproducibility and preventing a deeper analysis of the biases the assessor might have learned from ChatGPT.\n\n3. High Computational Cost is Undiscussed: The CAMS framework requires a computationally intensive preprocessing stage, involving multiple inference passes for uncertainty estimation and calls to another large model for complexity assessment. The paper focuses exclusively on performance gains without providing any discussion on the trade-off between these gains and the significant increase in computational overhead, which is a critical factor for practical application."}, "questions": {"value": "1. On the Effective Boundary of the Complexity Assessor: We observed in Figure 4 that the performance gain in the language science (LAN) domain was less pronounced than in NAT and SOC. Could this suggest that your complexity assessor, trained by increasing \"reasoning steps,\" is less effective at capturing the \"conceptual\" or \"insight-based\" difficulty common in language problems? Following this, how would you anticipate your method performing on high-level reasoning tasks like AIME math problems, where difficulty is almost entirely conceptual? Have you considered alternative or complementary methods to assess complexity for such challenges?\n\n2. On the Reproducibility of \"Evol-Complexity\": To improve the transparency and reproducibility of your work, would it be possible to provide the specific prompt used to guide ChatGPT in increasing question complexity, along with one or two full examples showing a question's \"evolution\" through the iterative process?\n\n3. On the Consideration of Computational Cost: Could you provide a brief quantitative analysis of the computational overhead (e.g., approximate extra GPU hours or API calls) required by the CAMS preprocessing pipeline compared to a baseline like FS-CoT? This information is crucial for readers to assess the practical deployment value of your method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a74Xdh7Tpc", "forum": "gp2G7JMq57", "replyto": "gp2G7JMq57", "signatures": ["ICLR.cc/2026/Conference/Submission10551/Reviewer_CvGg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10551/Reviewer_CvGg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760669819470, "cdate": 1760669819470, "tmdate": 1762921828989, "mdate": 1762921828989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CAMS (Complexity-Guided Active Multimodal CoT Sampling), a prompt–curriculum framework for multimodal chain-of-thought (MCoT) reasoning. Instead of randomly or manually picking few-shot exemplars, CAMS selects a balanced set of examples by combining: (i) model-perceived difficulty via prediction disagreement across k stochastic forward passes (unique answers divided by k), and (ii) intrinsic sample complexity estimated by a learned complexity scorer (trained on “evolved” instruction variants and applied to text formed by concatenating question, options, and image caption). Samples are partitioned into easy/hard by the disagreement score and into low/high complexity by the scorer; CAMS then samples equally from the four buckets to build the prompt exemplars (shared across all test items). Experiments on ScienceQA, A-OKVQA, OK-VQA, VQAv2, TextVQA with LLaMA3.2-Vision-11B, LLaVA-7B, Qwen2.5-VL-7B report consistent gains over ZS-CoT, FS-CoT, Auto-CoT, Active Prompt, and Self-Consistency, plus reduced variance vs random few-shot selection. Ablations indicate both uncertainty and complexity contribute, and analyses on ScienceQA sub-domains suggest broad benefits."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear framing of prompt selection as a tailored curriculum with two orthogonal signals (model-uncertainty & data-complexity) and a balanced difficulty principle; simple and general recipe usable across MLLMs. \n\n2. Multi-benchmark, multi-model evaluation; ablations showing both modules matter; analysis on sub-domains (ScienceQA NAT/SOC/LAN; grade bands). Reported average gains over strong baselines plus reduced seed-variance. \n\n3. Method flow (Fig. 2) and pseudocode (Algorithm 1) make the approach reproducible; implementation details (temperature, k, sampling limits) are provided; prompt templates are in the appendix; code promised. \n\n4. Addresses a practical pain point—random exemplars cause instability—and offers a principled fix with measurable benefits for MCoT on common VQA-style tasks."}, "weaknesses": {"value": "1. The scorer depends on caption quality and a text-only transformation of multimodal inputs; evidence that this approximates intrinsic multimodal complexity is limited. A calibration study (e.g., correlation with human difficulty ratings or model error rates) would strengthen the claim. \n\n2. The 0.5 uncertainty threshold, equal bucket quotas, and exemplar count are not stress-tested. Without sensitivity curves, it’s hard to disentangle whether “balanced difficulty” per se or just “not all-hard” drives gains. \n\n3. Active disagreement over large pools (even with 10k caps) can be expensive; there is no cost vs. accuracy analysis or amortization strategy (e.g., sub-sampling, early stopping, proxy models). \n\n4. While Auto-CoT, Self-Consistency, and Active Prompt are included, comparisons to complementary planning/verification paradigms (e.g., Least-to-Most, ReAct/ToT in multimodal settings) are absent; also no results with larger closed models to assess headroom.\n\n5. Needs explicit confirmation that official VQA scoring (soft-acc, text normalization) is used; if not, comparisons may be apples-to-oranges. \n\n6. The construction of exemplars by concatenating dataset “question, reasoning, answer” presumes such rationales exist and are high-quality for each dataset; many VQA datasets lack gold rationales. Scope and handling when rationales are unavailable should be clarified."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J5H8amZCKF", "forum": "gp2G7JMq57", "replyto": "gp2G7JMq57", "signatures": ["ICLR.cc/2026/Conference/Submission10551/Reviewer_UdSN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10551/Reviewer_UdSN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704498275, "cdate": 1761704498275, "tmdate": 1762921828536, "mdate": 1762921828536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the suboptimal and unstable performance of Multimodal Chain-of-Thought (MCoT) prompting, which it attributes to the reliance on randomly or manually selected examples. The authors propose CAMS, a framework to automate prompt selection by constructing a \"prompt curriculum\" based on two signals: model-perceived difficulty, measured via prediction uncertainty, and intrinsic sample complexity, determined by a separate scorer. By jointly analyzing these dimensions, the method selects a balanced set of examples intended to be more effective. The approach is evaluated on five multimodal reasoning benchmarks, with the authors reporting improved accuracy and reduced performance variance compared to baseline prompting strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper uniquely reframes MCoT prompt selection as a curriculum design problem. It innovatively combines two complementary signals—model-specific uncertainty and model-agnostic sample complexity—to create a more principled and effective selection strategy.\n\n- The empirical validation is robust, featuring extensive experiments on five benchmarks with three different MLLMs. The ablation studies are a key strength, clearly demonstrating the synergistic benefit of the two signals, and the work provides strong quantitative evidence of improved performance stability."}, "weaknesses": {"value": "- A core component of the proposed framework is the \"Complexity Scorer,\" yet its development and impact are not fully interrogated. The scorer is trained on a dataset created via an \"evolution-based metric\" that relies on ChatGPT for ranking. This introduces several issues: (1) It creates a dependency on a powerful, proprietary model, which complicates reproducibility. (2) The performance of CAMS becomes implicitly dependent on the quality of this external LLM's judgments. The paper lacks a sensitivity analysis exploring how the final MCoT performance is affected by the accuracy of this scorer.\n\n- The paper's key claim is that a \"balanced difficulty\" curriculum is superior. However, the implementation of this balance—selecting an equal number of samples from four discrete quadrants (easy/hard uncertainty vs. easy/hard complexity)—is a rigid heuristic. It is not obvious that a 25/25/25/25 split is optimal across different models, which have unique knowledge gaps, or across different datasets with varying intrinsic difficulty distributions."}, "questions": {"value": "Plz answer my concerns in the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tZS7BKKVR9", "forum": "gp2G7JMq57", "replyto": "gp2G7JMq57", "signatures": ["ICLR.cc/2026/Conference/Submission10551/Reviewer_U7iv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10551/Reviewer_U7iv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794782979, "cdate": 1761794782979, "tmdate": 1762921828073, "mdate": 1762921828073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CAMS, a prompt demonstration selection framework for multimodal CoT reasoning. CAMS uses a disagreement-based uncertainty score and a complexity scorer to build the example pool. Experiments on several VQA benchmarks show gains over baseline CoT methods. However, key design choices are ad hoc, and the complexity signal is only weakly justified, and there is no serious analysis. The contribution mainly refines existing ideas in LLMs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper targets a practical issue that how to construct the prompt demonstration for multimodal CoT reasoning.\n2. The experiments cover multiple VQA benchmarks and several multimodal models, showing the effectiveness of  the proposed CAMS framework.\n3. The overall pipeline is relatively easy to follow and easy to implement."}, "weaknesses": {"value": "1. The core contribution feels like a combination of existing ideas in LLMs rather than a new concept for multimodality. The improvement over simple baselines is often modest. The proposed method also introduces additional computation cost in prompt selection.\n2. The proposed complexity scorer is trained on text-only data using an outdated LLM (llama-2) and applied to caption-plus-question inputs for multimodal tasks, but this paper does not offer convincing evidence that this score meaningfully reflects sample difficulty for these VQA benchmarks.\n3. Key choices such as the disagreement threshold for splitting easy and hard examples and the fixed mixing of high- and low-complexity samples are not thoroughly studied. There is no sensitivity analysis that shows the method is robust to these hyperparameters.\n4. It would be better to see quantitative discussion of computational cost because CAMS requires multiple passes of a large MLLM over a training subset and additional passes of a complexity scorer.  For realistic scenario, processing the whole training dataset may be costly or infeasible.\n5. I would like to know why CAMS constructs a single global prompt pool per dataset and does not adapt exemplars to individual test questions, which contradicts the \"tailored\"."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eKIdyDqGGv", "forum": "gp2G7JMq57", "replyto": "gp2G7JMq57", "signatures": ["ICLR.cc/2026/Conference/Submission10551/Reviewer_eDCG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10551/Reviewer_eDCG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762380965155, "cdate": 1762380965155, "tmdate": 1762921827557, "mdate": 1762921827557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}