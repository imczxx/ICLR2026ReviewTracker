{"id": "IdPZh1CACh", "number": 19596, "cdate": 1758297535061, "mdate": 1763056611021, "content": {"title": "MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis", "abstract": "Current vision-language models (VLMs) in medicine are primarily designed\n      for categorical question answering (e.g., “Is this normal or abnormal?”)\n      or qualitative tasks (e.g., “Describe the image”). However, clinical\n      decision-making often relies on quantitative assessments, such as measuring\n      the size of a tumor or the angle of a joint, from which physicians draw\n      their own diagnostic conclusions. This quantitative reasoning capability remains\n      underexplored and poorly supported in existing VLMs. In this work, we introduce\n      MedVision, a large-scale dataset and benchmark specifically designed to\n      evaluate and improve VLMs on quantitative medical image analysis. MedVision\n      spans 22 public datasets covering diverse anatomies and modalities, with\n      30.8 million image-annotation pairs. We focus on three\n      representative quantitative tasks: (1) detection of anatomical structures and\n      abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance\n      (A/D) measurement. Our benchmarks show that current off-the-shelf VLMs\n      perform poorly on these tasks. However, with supervised fine-tuning on\n      MedVision, we significantly enhance their performance across detection, T/L\n      estimation, and A/D measurement, demonstrating reduced error rates and\n      improved precision. This work provides a foundation for developing VLMs with\n      robust quantitative reasoning capabilities in medical imaging.", "tldr": "A dataset and benchmark for quantitative medical image analysis tasks including detection, tumor size, angle, and distance measurement.", "keywords": ["vision language model", "medical image"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/301854945623d044bb38bdd3fa1b7aa28349cbda.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on quantitative medical image analysis and introduces the MedVision dataset/benchmark, which defines three representative tasks: (1) detection of anatomical structures and abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance (A/D) measurement. The authors evaluate existing vision-language models (VLMs) in both zero-shot and supervised fine-tuning (SFT) settings, demonstrating that SFT can effectively improve model performance on these quantitative tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The research motivation is well aligned with practical clinical needs, and the designed tasks are highly valuable and relevant to real-world medical applications.\n2. The collection of data containing physical measurements to construct the dataset is a reasonable and meaningful approach."}, "weaknesses": {"value": "1. It is unclear whether quantitative assessment is an intrinsic capability that VLMs should possess. Would it be more practical to perform these tasks using a segmentation model combined with rule-based computation, especially since the current dataset is constructed from segmentation data? Perhaps VLMs would be better suited to handling such tasks in an agent-based paradigm.\n2. Open-ended VQA may not be the most appropriate evaluation setting, as quantitative reasoning requires structured information extraction. It is recommended to include closed-ended VQA tasks so that both open- and closed-ended settings jointly form a more comprehensive benchmark.\n3. Line 155: Focusing solely on single instance detection is unreasonable, especially for abnormality detection. It is suggested that multi-instance scenarios be incorporated into the dataset and benchmark.\n4. Line 174: For the A/D annotation task, it would be helpful to provide a table explaining the task motivation, e.g., which specific angles or distances are clinically relevant for diagnosing certain diseases.\n5. Line 187: The random split raises concerns about its rationality. If the collected public datasets already provide official splits, why not use them?\n6. Line 265: For the evaluation metrics, more detailed calculation formulas should be provided to ensure clarity and reproducibility.\n7. Line 257: Normalizing coordinates to the range [0, 1] is problematic. Most modern VLMs operate with integer-normalized coordinates in [0, 1000], as they handle small decimal values poorly (e.g., confusing “0.11 > 0.9”). Using [0, 1] normalization could lead to unreliable detection results (as seen in Table 2)."}, "questions": {"value": "Same as those raised in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O6yT92Fsd0", "forum": "IdPZh1CACh", "replyto": "IdPZh1CACh", "signatures": ["ICLR.cc/2026/Conference/Submission19596/Reviewer_mhtK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19596/Reviewer_mhtK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704269072, "cdate": 1761704269072, "tmdate": 1762931462985, "mdate": 1762931462985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We thank the reviewers and the area chair for their time and constructive feedback. We will refine and strengthen the paper based on their comments."}}, "id": "AE6ePbaPAu", "forum": "IdPZh1CACh", "replyto": "IdPZh1CACh", "signatures": ["ICLR.cc/2026/Conference/Submission19596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19596/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763056610099, "cdate": 1763056610099, "tmdate": 1763056610099, "mdate": 1763056610099, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MedVision, a large-scale multimodal medical QA benchmark. The benchmark is repurposed from other 22 open-sourced related datasets. Different from other benchmarks, MedVision focuses not only on detection of anatomical structures\nand abnormalities but also the estimation of tumor size as well as angle/distance measurement. Evaluation using a wide range of open-sourced/frontier models demonstrates the difficulty of the benchmark and the lack of medical reasoning capabilities. Furthur experiments show that SFT signficiantly improves performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper addresses an important area (aka medical image analysis) that could facilitate the development of large medical VLMs and drive better clinical decision-making. Although the data is not new, the authors did manually re-label many different image pairs.\n2) The quantitative evaluation of size and angle is new and important. The results also demonstrate that current models struggle on these tasks. \n3) Flow of the paper is good. Easy to read and understand."}, "weaknesses": {"value": "1) Failure analysis needs improvement. The authors should isolate the effect of geometric reasoning from visual perception. The authors discuss failure modes like small-object detection and angle/value collapse, but it is still unclear if the model simply fails at perception or fails at reasoning (unit handling, geometry). \n\n***One experiment you can try is to (on a small set of course) feed GT BBbox to the model to re-measure size / angle / distance, this would give a clear picture of exactly where the model fails. \n\n2) Following 1), there is also the regular \"reasoning\" (natural language reasoning). In the current evaluation, it seems like the prompt asks the model to directly output the answer without explanation. I wonder if the reasoning process help (although unlikely, but it'd be interesting to see). \n\n***This could be simply done by prompting the model to reason first then answer.  \n\n3) Tool-using results are very surprising. In Table 2, the Gemini model with tool-using performance is worse compared to the oracle model. This is very surprising as tool-using, such as writing code for better cropping, etc should largely benefit these kind of tasks at least in my own experience. I don't think this is a weakness per se if the authors can clearly explain why tool-using doesn't help and even lowers acc; this would also immensely elevate the impact of this work. \n\n***The authors could show a few example to help explain"}, "questions": {"value": "1) I think it'd be good to include a GPT baseline, which will make the results more comparative and intuitive to field workers. \n2) Recent works have shown that RLVR improves medical reasoning [1][2]/generic visual grounding tasks much more than SFT, while also producing readable reasoning processes. Thus I believe It'd make the paper more comprehensive to include an RFT result, if their compute allows. \n\n\n[1] Pan et. al. MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning\n[2] Lai et. al. Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "There is no ethics or legal statement in this paper as far as I checked. If the authors only corrected data from publicly available datasets, then it might not be much of an issue, but it is safe to check. The authors are also strongly recommended to include an ethics statement given the nature of this work."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ymRCk9927v", "forum": "IdPZh1CACh", "replyto": "IdPZh1CACh", "signatures": ["ICLR.cc/2026/Conference/Submission19596/Reviewer_bYCv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19596/Reviewer_bYCv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727624218, "cdate": 1761727624218, "tmdate": 1762931462619, "mdate": 1762931462619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MedVision, a large-scale dataset and benchmark aimed at evaluating and enhancing vision-language models (VLMs) for quantitative medical image analysis. It includes 22 public datasets with 30.8 million image-annotation pairs across diverse anatomies and modalities. The study focuses on three quantitative tasks, including detection, tumor/lesion size estimation, and angle/distance measurement and shows that existing VLMs perform poorly on these tasks. With supervised fine-tuning on MedVision, the models achieve notable improvements in quantitative accuracy and precision."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is easy to follow, with well-structured descriptions of datasets and experiments. The benchmark design is systematic, providing a valuable foundation for evaluating quantitative reasoning in medical VLMs."}, "weaknesses": {"value": "However, there exist the following concerns.\nThe contribution remains limited, as the work mainly consolidates existing datasets and automatically extracts bounding boxes and bidirectional tumor/lesion sizes. The dataset construction process is more of an engineering integration rather than a methodological or conceptual advancement.\nThe question design in Section 3 centers on dataset annotation rather than clinical reasoning. It does not clearly connect to diagnostic decision-making or show whether the proposed framework meaningfully improves clinical outcomes or supports physicians’ decision processes.\nMany of the proposed quantitative tasks, such as size or distance measurement, could also be achieved through standard segmentation models or smaller specialized networks. The paper does not clarify what advantages large-scale VLMs bring beyond these existing methods.\nThe dataset lacks richer contextual information like radiology reports, diagnostic impressions, or lesion-level explanations. Compared with previous benchmark papers, it provides limited new insights into clinical reasoning or multimodal understanding beyond simple quantitative annotation."}, "questions": {"value": "The question design in Section 3 centers on dataset annotation rather than clinical reasoning. It does not clearly connect to diagnostic decision-making or show whether the proposed framework meaningfully improves clinical outcomes or supports physicians’ decision processes.\nMany of the proposed quantitative tasks, such as size or distance measurement, could also be achieved through standard segmentation models or smaller specialized networks. The paper does not clarify what advantages large-scale VLMs bring beyond these existing methods.\nThe dataset lacks richer contextual information like radiology reports, diagnostic impressions, or lesion-level explanations. Compared with previous benchmark papers, it provides limited new insights into clinical reasoning or multimodal understanding beyond simple quantitative annotation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iUg3rfwz0B", "forum": "IdPZh1CACh", "replyto": "IdPZh1CACh", "signatures": ["ICLR.cc/2026/Conference/Submission19596/Reviewer_JyAs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19596/Reviewer_JyAs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832137976, "cdate": 1761832137976, "tmdate": 1762931462241, "mdate": 1762931462241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MEDVISION, a large composite dataset that integrates 22 publicly available datasets, along with a benchmark for the quantitative evaluation of medical vision-language models (VLMs). The authors curated the original annotations from each dataset and focused on three key  tasks: (1) detection of anatomical structures and abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance (A/D) measurement. I appreciate the authors’ efforts in evaluating a wide range of VLMs and assembling such a comprehensive dataset. However, I have several concerns regarding this work, primarily related to the composition of the dataset and the formulation"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "MEDVISION integrates a diverse collection of publicly available datasets, which could serve as a valuable resource for the medical vision-language community.\nThe authors conducted extensive experiments by evaluating a wide range of VLM models, providing useful baselines for future research."}, "weaknesses": {"value": "While MEDVISION aggregates a large number of datasets, the curated annotations do not represent a significant improvement over the original data — in some cases, they are even of lower quality. For instance, the annotation process excluded 2D slices containing multiple bounding boxes, which may limit the dataset's applicability and realism. The authors justify this by stating that the study focuses on single-instance detection; however, this raises the question of clinical relevance — what is the practical value of single-instance detection in real-world clinical settings?\n\nThis concern extends to the overall task formulation. It is unclear why large VLMs are needed to perform tasks such as detection, size estimation, and angle/distance measurement, which are traditionally and effectively addressed using standard detection, segmentation, and keypoint detection models. The paper would benefit from a stronger justification for the use of VLMs in this context, as well as a clearer articulation of how these tasks benefit from vision-language modeling."}, "questions": {"value": "1. It appears that the curated annotations do not significantly improve upon the original data — in some cases, they seem to be simplified derivations (e.g., generating bounding boxes from segmentation masks). Could the authors clarify what value this curation adds, especially when it may result in a loss of information or clinical realism?\n\n2. Why are VLMs needed for these tasks, given that standard detection, segmentation, and keypoint detection models can already handle them effectively? What unique advantages do VLMs bring to the table in this context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zeDd4pXcH3", "forum": "IdPZh1CACh", "replyto": "IdPZh1CACh", "signatures": ["ICLR.cc/2026/Conference/Submission19596/Reviewer_5WgS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19596/Reviewer_5WgS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179209735, "cdate": 1762179209735, "tmdate": 1762931461762, "mdate": 1762931461762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}