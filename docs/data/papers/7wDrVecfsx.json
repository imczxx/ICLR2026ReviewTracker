{"id": "7wDrVecfsx", "number": 7478, "cdate": 1758023870540, "mdate": 1763712258857, "content": {"title": "EvolArena: An Evolving Arena for Multi-Turn Reasoning in LLMs", "abstract": "Recent advances in LLMs have shown promising results in complex reasoning tasks. However, current evaluations predominantly focus on single-turn reasoning scenarios, leaving interactive tasks largely unexplored. We attribute it to the absence of comprehensive datasets and scalable automatic evaluation protocols. To fill these gaps, we present EvolArena, an Evolving Arena for LLMs' multi-turn reasoning evaluation. Comprising 4 classes, 40 tasks, and 3600 instances, EvolArena covers diverse reasoning capabilities, \nfine-grained difficulty granularity, and necessitates multi-turn interactions with the environments. Moreover, EvolArena features fully-automated framework spanning both dataset constructions and model evaluations, which enables scalable assessment without human interventions. Experiments reveal that even the cutting-edge reasoning models fall short of multi-turn, interactive reasoning tasks. And the further analysis upon these results brings valuable insights for future research in interactive AI systems.", "tldr": "We introduce an evolving arena to evaluate LLMs' multi-turn reasoning capabilities, featuring an automated framework and adjustable difficulty levels.", "keywords": ["multi-turn", "reasoning", "evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac5b0537d59bafbae332921c4f300be83759b101.pdf", "supplementary_material": "/attachment/07c36a626075dfa73eaa97f1b92babcfad77e67d.zip"}, "replies": [{"content": {"summary": {"value": "# Summary\n\nThe authors present EvolArena, a benchmark set of reasoning, planning, and game playing tasks for evaluating LLMs capabilities in sequential decision making.\n\n## What is the problem solved? Is it a known problem? How is it solved in the literature and how is the current approach different?\n\nSequential decision making either in a single-player (e.g., planning) or muti-player (e.g., general game playing) setting is a central problem in computer science in general and in AI in particular. There are several communities within AI that focus on creating solvers for a variety of settings, for many decades now. Each of these communities of course have their own benchmarks to evaluate these solvers on. Designing new benchmarks is often a time consuming process that also requires an understanding of a particular setting. For instance, benchmarks for partially observable setting for a single player games would be very different from the multi-player setting, and from so-called classical planning setting (single player, full observability, deterministic action outcomes, closed world). \n\nIn the era of LLMs, when a single \"solver\" is assumed to be able to tackle all possible problems, these separate benchmarks are grouped together to check the abilities of LLMs across the spectrum of possible decision making tasks. I find this aggregation to be ill-motivated, and done for convenience only, also preventing from comparing to the established solvers in each setting. In most cases, the aggregation is also performed over existing benchmarks, which requires some engineering work to put them into the same framework.\n\nThe authors follow the same trend, aggregating over various types of sequential decision making problems, including information acquisition games, path planning, and multi agent planning.\nDifferently from most work in the area, they create new benchmarks based mostly on two sources, popular logic puzzles published on the New York Times website and 32 multi-agent planning tasks originating from algorithmic competition problems on Codeforces.\n\nThe authors position these tasks as partitioned into 4 categories: Inductive Reasoning, Abductive Reasoning, Deductive Reasoning, and Planning Reasoning. I find this partitioning puzzling. Information acquisition games are not about inductive reasoning, path planning is not about deductive reasoning, and Planning Reasoning is not a common term, not sure what it means in your case. The example task in that category is a general game playing or multi-agent planning task, in an adversarial setting. This positioning sounds more like PR than science, I would strongly encourage you to drop it in the next iterations of this work.\n\nWhile the authors mention some related work, they miss some of the most relevant ones, specifically AgentBench [1] and AgentBoard [2]. I would be very interested to hear how the authors differentiate themselves from these works.\n\n# Significance\n\nThe existence of a large collection of similar benchmarks reduces the significance of this work. Also, no justification is given for the benefits of these sequential decision making tasks over other existing ones.\n\n# Soundness\n\nThe idea seems sound, but since there is no formal definition of these tasks, what constitutes a solution, how a solution is validated, etc, it is hard to judge.\n\nFor task generation, the authors divide the tasks into three categories, \"easy\", \"medium\", and \"hard\" based on the value to the input parameter to the generator. This hint on (a) the generators having only one parameter, and (b) the larger the value to that parameter, the harder the problem is. \n(a) is often an indication of a very limited generator, stabilizing many decisions and focusing on only a very limited set of possible instances, which in future might lead to memorization concerns, much like with the BlocksWorld domain in planning.\n(b) is not necessarily true in sequential decision making. For instance, adding more resources or increasing the maze size without scaling the obstacles makes the tasks easier, not harder. \n\nThe evaluation is done on several axes:\n1. Accuracy, the standard measure of success\n2. The Efficiency is a pairwise comparison of models on the solutions lengths of their commonly solved tasks. This can be misleading, as it only measures in absolute terms, so if A solved 3 tasks finding solutions of length 5, 5, 5, while B found solutions 4, 4, 100, B is considered to be better than A under the efficiency measure.\n3. Invalid Rate measures how many tasks did the model generate invalid actions for. Related datasets that explicitly measure action applicability are [3,4,5,6].\n4. Pattern Analysis measures how much does the \"reasoning\" of the language models follow the Associate, Verify, Plan, and Feedback pattern. This measure needs at least some justification, but none is given.\n\n\n# Novelty\n\nMaze navigation and path planning in particular benchmarks for LLMs already exist, for instance in [7]. \nThe novelty is somewhat limited by the abundance of similar benchmarks.\nHaving said that, some of the tasks presented are novel in this context.\n\n# Scholarship\n\nI am missing the formal definitions for the problems tackled in this paper and the references to the respective formalisms that are used to formally capturing them. \n\nFurther, the missing references that are mentioned above:\n[1] Liu et al., ICLR 2024, AgentBench: Evaluating LLMs as Agents\n[2] Ma et al., NeurIPS 2024, AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents\n[3] Handa et al,. ICLR 2025. ActionReasoningBench: Reasoning about Actions with and without Ramification Constraints  \n[4] He et al., ACL 2023, Exploring the Capacity of Pretrained Language Models for Reasoning about Actions and Change   \n[5] Kokel et al., AAAI 2025, ACPBench: Reasoning about Action, Change, and Planning  \n[6] Kokel et al., LM4Plan@AAAI 2025, ACPBench Hard: Unrestrained Reasoning about Action, Change, and Planning\n[7] Palod et al., Arxiv 2025, Performative Thinking? The Brittle Correlation Between CoT Length and Problem Complexity\n\n \n# Clarity\n\nThe paper is relatively easy to follow.\n\n# Evaluation and Reproducibility\n\nAs the tasks themselves are not described, only examples are given, and are aggregated into 4 categories, we are only shown the evaluation results for a category. These numbers are not very informative, as we do not know what tasks do they correspond to. The comparison between models is done on the aggregated scores and not on the per-task basis, and therefore I find it somewhat less justifiable."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Non static benchmark, with dynamically generated tasks\n2. Large collection of various tasks\n3. Diverse set of sequential decision making problems"}, "weaknesses": {"value": "1. The generator has only one parameter, it seems to generate only a single task per parameter, so the variability is actually quite limited.\n2. There is no formal analysis of the classes the tasks belong to.\n3. The novelty is somewhat limited by the existence of several similar benchmarks.\n4. The benchmark is not sufficiently justified - why is it important to measure the abilities of language models to solve sequential decision making tasks if they are notoriously bad at it and not designed to solve such tasks. \n5. The evaluation is not sufficiently informative, some of the evaluation metrics are somewhat questinable and not sufficiently motivated.\n6. There is no description or measure of the effort that took to manually implement all these tasks in the benchmark set. In other words, there is no discussion of how large of an effort it is to extend the benchmark set with additional tasks."}, "questions": {"value": "1. What is the motivation of aggregating together various sequential decision making tasks under the same benchmark? What is the benefit over separate evaluations (e.g., path planning, classical planning, contingent planning, general game-playing) beyond the ease of conducting experiments?\n2. How do you differentiate your work from AgentBench [1] and AgentBoard [2]?\n3. How do you validate solutions? Did you have to implement a separate validator for each task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TP7729RE4C", "forum": "7wDrVecfsx", "replyto": "7wDrVecfsx", "signatures": ["ICLR.cc/2026/Conference/Submission7478/Reviewer_P7mg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7478/Reviewer_P7mg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760986463957, "cdate": 1760986463957, "tmdate": 1762919596119, "mdate": 1762919596119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "EvolArena is a new benchmark + automated evaluation framework for multi-turn interactive reasoning of LLMs. The authors (i) define 4 task classes (Information Probing, Dynamic Adaptation, State Operation, Strategic Gaming), (ii) convert 40 seed problems into structured interactive templates, (iii) implement a Generator / Monitor / Evaluator pipeline that automatically produces instances at three difficulty levels (easy/medium/hard) and runs deterministic interactions, and (iv) evaluate ~20 models (reasoning-specialized and general) reporting accuracy, efficiency (turns), invalid-operation rates, and pattern analyses. Main claims: multi-turn interactive reasoning remains hard for SOTA models, their framework scales and can be customized via tunable generators, and it provides diagnostic metrics for real weaknesses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides an interesting way of sourcing multi-turn problems from interactive games / puzzles. \n2. The framework provides diverse evaluation metrics and identify specific reasoning patterns used in the model response. \n3. Though the generated tasks are closed-world and contrived, the authors address this limitation directly and reasonably."}, "weaknesses": {"value": "1. The paper can benefit from more details about the generator, monitor, and evaluator. E.g. the monitor is rule-based (what are these rules) and checks for formatting (what kind); the evaluator evaluates for pattern analysis (how). These information would be helpful in the appendix.\n2. The paper could elaborate more on how this benchmark can be considered \"evolving\". Based on the work, it seems like here “evolving” = procedurally extensible (generate from seed problems) + parameterized difficulty scaling (control difficulty parameter). However, it seems like up to the choice of seed problem and difficulty parameter, the benchmark is rather static and not adaptive to the agent's capabilities. Could the authors please clarify this word choice? I think a more substantial explanation of the evolving nature of this framework would better highlight the novelty of this work. \n3. Several formatting issues in the paper (e.g., figure 4 caption is cut off, inconsistent bolding in table 1 caption, etc."}, "questions": {"value": "1. The seed tasks are obtained from publicly available sources. Could you speak to whether this creates contamination and whether this could be an issue? \n2. See \"evolving\" comment in weaknesses \n3. Could the authors clarify which of the generator and evaluator are LLM-based. For those that are, could the authors provide an estimate on token costs for the generator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z9sQWyZWQ3", "forum": "7wDrVecfsx", "replyto": "7wDrVecfsx", "signatures": ["ICLR.cc/2026/Conference/Submission7478/Reviewer_7c9C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7478/Reviewer_7c9C"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668861351, "cdate": 1761668861351, "tmdate": 1762919595589, "mdate": 1762919595589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EvolArena, a benchmark and automated framework for evaluating the multi-turn reasoning capabilities of Large Language Models (LLMs) in interactive settings. Arguing that existing benchmarks are often single-turn or rely on costly human evaluation, the authors propose a system comprising: (1) a diverse dataset of 40 tasks across 4 reasoning categories (Information Probing, Dynamic Adaptation, State Operation, Strategic Gaming) derived from Codeforces and NYT logic puzzles, with 3 calibrated difficulty levels each (3600 instances total) ; (2) an automated framework with a Generator (creates tasks of tunable difficulty), a Monitor (manages interaction, provides rule-based feedback), and an Evaluator (calculates metrics like accuracy, efficiency, invalid rate, pattern analysis) . The evaluation of 20 models reveals that even frontier reasoning models struggle with multi-turn complexity, especially at higher difficulty levels. The framework is designed to be evolving and automated to facilitate scalable assessment. Reproducibility materials and an LLM usage statement are provided."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Addresses Multi-Turn Reasoning Gap:** Directly targets the under-explored area of multi-turn, interactive reasoning evaluation for LLMs, moving beyond static benchmarks.\n\n\n**Automation and Scalability:** The fully automated pipeline for both task generation (with difficulty control) and evaluation is a major strength, enabling efficient, reproducible, and potentially evolving benchmarking without human intervention.\n\n\n**Task Diversity and Structure:** Offers a broad set of 40 tasks categorized by reasoning type, designed to necessitate interaction . The use of structured templates ensures clear interaction protocols.\n\n\n**Comprehensive Metrics:** Evaluates models across multiple dimensions including accuracy, efficiency (turns), robustness (invalid rate), and qualitative reasoning patterns.\n\n\n**Clear Presentation and Reproducibility:** The framework is well-described, results are clearly presented, limitations are discussed honestly , and the commitment to releasing code and data supports reproducibility."}, "weaknesses": {"value": "**Non-Natural Language Interaction:** The tasks rely on structured input/output formats rather than natural language dialogue. This limits the benchmark's ability to assess reasoning within natural conversation, a key aspect of real-world interaction.\n\n**Potential Data Contamination:** Sourcing tasks from public platforms like Codeforces and NYT raises concerns about potential contamination, as models may have encountered similar problems during pre-training. The paper lacks detailed contamination checks or mitigation strategies.\n\n**Positioning Relative to Other Arenas:** While MT-Bench and GameArena are discussed, the paper could strengthen its positioning relative to other recent interactive/game-based LLM evaluation frameworks by highlighting unique contributions more explicitly.\n\n**Limited Scope of \"Evolving\":** The \"evolving\" nature seems currently limited to pre-defined difficulty levels generated automatically. True evolution might require dynamic adaptation based on ongoing model performance across the community."}, "questions": {"value": "1. Could you elaborate on measures taken to assess or mitigate potential data contamination from the Codeforces and NYT sources? For example, were problems modified significantly, or do you plan to make the arena lively updated?\n\n2. The future work section mentions extending the framework to natural language interactions. Could you provide more detail on how this might be approached while maintaining the benefits of automated evaluation?\n\n3. To better understand model failure modes and prevent \"gaming\" specific tasks, could you report per-task performance variance and provide a qualitative or quantitative taxonomy of common errors beyond the \"Invalid Rate\"?\n\n4. How does EvolArena's task design and evaluation methodology compare specifically to other recent automated or semi-automated interactive benchmarks beyond MT-Bench/GameArena (e.g., those focusing on web navigation, tool use, or other interactive games)?\n\n5. Consider citing CodeElo (Quan et al., 2025) and LiveCodeBench Pro (Zheng et al., 2025) for single-turn code-generation reasoning, and comparing with SPIN-Bench (Yao et al., 2025) which evaluates LLM agents' abilities of multi-turn social reasoning in cooperative and strategic settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ILzQUG4maI", "forum": "7wDrVecfsx", "replyto": "7wDrVecfsx", "signatures": ["ICLR.cc/2026/Conference/Submission7478/Reviewer_fLsS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7478/Reviewer_fLsS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722320843, "cdate": 1761722320843, "tmdate": 1762919594542, "mdate": 1762919594542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EvolArena, a comprehensive benchmark designed to evaluate multi-turn reasoning capabilities of large language models (LLMs). Unlike existing single-turn benchmarks (e.g., MATH, GSM8K, or LogicNLI) or limited interactive tests (e.g., MT-Bench, GameArena), EvolArena provides an automated, evolving evaluation framework covering four reasoning dimensions—Inductive, Abductive, Deductive, and Planning reasoning—and four task types: Information Probing, Dynamic Adaptation, State Operation, and Strategic Gaming.\nIt includes 40 distinct tasks (3 difficulty levels, 3,600 instances) with automated generation, monitoring, and evaluation components. The benchmark enables scalable, human-free assessment of multi-turn reasoning. Experiments across 20 reasoning and non-reasoning models reveal that even frontier models (like o3-mini, R1) struggle in harder multi-turn scenarios, and that strong reasoning accuracy often comes at the cost of interaction efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: The notion of an *evolving*, automated arena for multi-turn reasoning is novel and well-executed. Unlike GameArena or MT-Bench, EvolArena supports dynamic difficulty adjustment and fully automated interaction without human bias.\n2. Quality: The methodology is detailed and technically robust. The evaluation across 20 models provides strong empirical grounding. The difficulty calibration and pattern-level analysis (Associate/Verify/Plan/Feedback) demonstrate methodological maturity.\n3. Clarity: The paper systematically defines all framework components and metrics, making replication feasible.\n4. Significance: EvolArena fills an essential gap in reasoning evaluation, offering a standardized, scalable infrastructure that could guide future developments in reasoning-augmented LLMs and agent systems."}, "weaknesses": {"value": "1. Limited originality. The framework is primarily an aggregation of known evaluation patterns—multi-turn dialogues, task templating, automatic feedback—without introducing a fundamentally new idea or methodology. It feels incremental relative to prior benchmarks (e.g., GameArena, MT-Bench, ARC-Interactive).\n2. No theoretical grounding. The paper repeatedly invokes “multi-turn reasoning” but does not formalize what reasoning dynamics it intends to capture. There is no cognitive or algorithmic rationale for the four reasoning types, nor an analysis of why these categories meaningfully test reasoning beyond surface-level interaction.\n3. Evolving mechanism underdeveloped. Despite its title, the “evolution” in EvolArena refers only to adjustable difficulty parameters. The framework does not learn, adapt, or evolve based on model behavior. Thus, the name overpromises relative to its actual capabilities.\n4. Shallow analysis. The experimental discussion focuses mainly on numeric trends, with limited qualitative insight into model behaviors or reasoning errors. The paper does not analyze why models fail, nor does it connect failure modes to underlying reasoning limitations."}, "questions": {"value": "1. What distinguishes EvolArena’s “evolving” nature from simple difficulty scaling? Could it support automatic curriculum adaptation or model-contingent evaluation?\n2. How were the four reasoning categories chosen—based on cognitive theory or empirical clustering?\n3. Have you validated that task outcomes correlate with actual reasoning ability, rather than instruction-following fidelity?\n4. Could the framework be extended to evaluate reasoning with external tools or multimodal contexts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E1poQZdmce", "forum": "7wDrVecfsx", "replyto": "7wDrVecfsx", "signatures": ["ICLR.cc/2026/Conference/Submission7478/Reviewer_shjX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7478/Reviewer_shjX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7478/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876543972, "cdate": 1761876543972, "tmdate": 1762919594000, "mdate": 1762919594000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}