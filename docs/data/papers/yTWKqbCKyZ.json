{"id": "yTWKqbCKyZ", "number": 15859, "cdate": 1758256244133, "mdate": 1759897277288, "content": {"title": "Theoretical Constraints on the Expressive Power of RoPE-based Tensor Attention Transformers", "abstract": "Tensor Attention extends traditional attention mechanisms by capturing high-order correlations across multiple modalities, addressing the limitations of classical matrix-based attention. Meanwhile, Rotary Position Embedding ($\\mathsf{RoPE}$) has shown superior performance in encoding positional information in long-context scenarios, significantly enhancing transformer models' expressiveness. Despite these empirical successes, the theoretical limitations of these technologies remain underexplored. In this study, we analyze the circuit complexity of Tensor Attention Transformer and extend to its $\\mathsf{RoPE}$-based Tensor Attention variants, showing that with polynomial precision, constant-depth layers, and linear or sublinear hidden dimension, they cannot solve fixed membership problems or $(A_{F,r})^*$ closure problems, under the assumption that $\\mathsf{TC}^0 \\neq \\mathsf{NC}^1$. These findings highlight a gap between the empirical performance and theoretical constraints of Tensor Attention and $\\mathsf{RoPE}$-based Tensor Attention Transformers, offering insights that could guide the development of more theoretically grounded approaches to Transformer model design and scaling.", "tldr": "", "keywords": ["tensor attention", "transformer", "circuit complexity", "machine learning theory"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee3d159e09467ce52667efc4eff4f75429acc0cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a formal analysis of the expressive power of Tensor Attention Transformers (TAT) using Rotary positional embeddings (RoPE). The results presented in this paper can be seen as extending the results in (Chiang, 2024) who showed that transformers belong to DLOGTIME-uniform TC0 in two directions: TAT and RoPE. \n\nThe main results showed in this paper are that (i) TAT w/ and w/o RoPE (poly(n) size, constant depth) are also in DLOGTIME-uniform TC0 (Thm 3.7 and 4.5) and (ii) TAT w/ and w/o RoPE (constant depth) cannot solve the fixed membership problem (Thm 5.6), nor the (A_F,r)* closure problem (Thm 5.7). \n\nThere is no experimental section, the contribution is focused on the theory."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Extending the collection of results characterizing the expressiveness of architectures commonly used in ML using circuit complexity is a very fundamental and relevant contribution."}, "weaknesses": {"value": "- The main weakness of the paper in my opinion is its lack of clarity. This lack of clarity is due to several factor: \n\n  - structure of the paper: most of the space is spent on definitions and results taken from other papers. As consequence, not enough time is spent on explaining how the results presented here are novel and non-trivial extension of previous work, as well as why they are relevant. I understand that it is important to introduce the previous results on which your work built upon, and that some definitions are unavoidable to present complex architecture designs such as tensor attention. Still:\n      - some definitions are standard and should be omitted or deferred to appendix (e.g. Def 2.21 defines a rotation matrix, Def 2.20 defines the softmax function) \n      - some facts should be deferred to the appendix since they are not use in any meaningful way in the main paper as far as I see (e.g. Fact 2.19 is mentioned in Def 2.25 but not needed to state this definition)\n      - I would suggest only keeping the results that are fundamental to understand your contribution in the main paper and moved all the others to the appendix (e..g. I don't think the 4 results in section 2.1 are needed in the main paper). This would allow you to spend more time discussing your contribution. \n\n - May of the definitions and lemmas are taken from previous work and have been slightly rephrased / edited in a way that, in my opinion, make them less clear (e.g. Def 2.1). I would suggest using the exact same formulation as in the original papers for definition that are directly borrowed, unless there is a clear reason not to do so. \n\n - The paper lacks rigour in some places, some notations are not introduced. E.g. Q_{j_1,*} in Def 2.4, A+ and [P] in Def 5.1, the less or equal sign used in the sup of Def 5.3, ||w|| in Def 5.4.\n\n - Section 5, presenting the hardness result, is particularly difficult to follow (at least for me). Many notations have not been properly introduced. From the definitions I was not able to understand the definition of the two problems. I wrote my questions related to Def 5.1 and 5.3 below. Regarding Def 5.3. I do not understand why this very abstract definition of the Kleene star is used. My intuition is that the standard definition would be sufficient here (L^* = \\cup_{n\\geq 0} L^n)."}, "questions": {"value": "- What were the key technical challenges in proving Thm 4.5? Are the proof techniques a direct adaptation of the ones used in (Chiang, 2024) or were there any specific aspects of RoPE or TAT that made the analysis more challenging? \n\n- Def 5.1: what does uv^\\omega \\in [P] means? What is omega here? What does the notation [P] refer to? \n\n- Def 5.3: Why do you use the def from Kuznetsov for the Kleene star? I find it quite confusing and I believe un-necessary to define the closure problem where the standard definition would suffice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oLeHw0gROQ", "forum": "yTWKqbCKyZ", "replyto": "yTWKqbCKyZ", "signatures": ["ICLR.cc/2026/Conference/Submission15859/Reviewer_Bzdt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15859/Reviewer_Bzdt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761072585010, "cdate": 1761072585010, "tmdate": 1762926079640, "mdate": 1762926079640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors discuss the circuit complexity of tensor attention along with its RoPE-equipped variants. They show that under polynomial precision, constant-depth, and big O linear hidden dimension, these attention modules still cannot solve the fixed membership problem or the (A_{F,r})^* closure problem. These results highlight the computational limitation of tensor-attention."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The theoretical results on the limitation of tensor-attention seem solid, though I’m not an expert in the subfield."}, "weaknesses": {"value": "It might be better to also run some experiments on the fixed membership problem or the (A_{F,r})^* closure problem using tensor attention with RoPE. Otherwise it’s hard to tell if this is actually a limitation in practice. It’s hard to tell if the theoretical complexity bound is tight or loose and how does it actually manifest in the Transformer model empirically."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pkMW9tHa0A", "forum": "yTWKqbCKyZ", "replyto": "yTWKqbCKyZ", "signatures": ["ICLR.cc/2026/Conference/Submission15859/Reviewer_NWQ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15859/Reviewer_NWQ8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935162877, "cdate": 1761935162877, "tmdate": 1762926079250, "mdate": 1762926079250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is an investigation into the computational complexity of Tensor Attention Transformers and their RoPE-based variants using circuit complexity theory. The work builds through individual components to complete multi-layer architectures. The authors analyze circuit depth and size requirements for computing the tensor-attention related operations, and show that this can be simulated by uniform TC0 circuirts with constant depth, size, and precision. The results are extended to RoPE-based tensor attention, and limitations are shown in terms of solving particular classes of problems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Rigorous treatment with detailed proofs, building circuit complexity analysis systematically from basic operations\n- Clear statement of assumptions and conclusions\n- Useful extension of circuit complexity analysis to tensor attention and RoPE which are modern architectures\n- Complexity bounds for all components"}, "weaknesses": {"value": "- no empirical evaluation at all; no practical validation of key assumptions/conclusions. No ablations\n- no guidance for practitioners. at times, unclear how to use insights for architecture design\n- more discussion in terms of bound tightness and practical implications\n- limited intuition and clarity of key takeaways while the paper is technically very dense\n- The approach extends related work and adapts to tensor attention (somewhat incremental novelty in that respect)\n- results apply only under constraints that may not hold in practice (constant layers etc.). \n- Additionally (minor) make sure that notation is consistent/clear (e.g. 2.24 vs 2.25 definition)"}, "questions": {"value": "please see above limitations and weaknesses: e.g., how do constraints/assumptions link with practical transformer implementations, empirical validation, missing link to practitioners, and so on."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Eyw5FPc3J3", "forum": "yTWKqbCKyZ", "replyto": "yTWKqbCKyZ", "signatures": ["ICLR.cc/2026/Conference/Submission15859/Reviewer_ZK2f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15859/Reviewer_ZK2f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762175991839, "cdate": 1762175991839, "tmdate": 1762926078812, "mdate": 1762926078812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the circuit complexity of the Tensor Attention Transformer and extends it to its RoPE-based Tensor Attention variants. With the analysis, their work provides theoretical understanding of self-attention architectures and computational boundaries. These findings highlight a gap between the empirical performance and theoretical constraints of Tensor Attention and RoPE-based Tensor Attention Transformers, offering insights that could guide the development of more theoretically grounded approaches to Transformer\nmodel design and scaling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The results of this paper can provide valuable insights into existing structures.\n\n2. Using circuit complexity theory, this paper analyzes tensor attention and rotary position embedding (RoPE).'\n\n3. The self-attention structure is crucial, and analyzing its expressiveness will exert a profound and long-lasting impact.\n\n4. In my opinion, this work is novel for the related area."}, "weaknesses": {"value": "1. It's hard to understand some concepts, such as CIRCUIT COMPLEXITY."}, "questions": {"value": "How can this theroy result motivate further improvements to the self-attention structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "UwbpUYWRUl", "forum": "yTWKqbCKyZ", "replyto": "yTWKqbCKyZ", "signatures": ["ICLR.cc/2026/Conference/Submission15859/Reviewer_S3FE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15859/Reviewer_S3FE"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762509412838, "cdate": 1762509412838, "tmdate": 1762926078222, "mdate": 1762926078222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}