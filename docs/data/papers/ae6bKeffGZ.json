{"id": "ae6bKeffGZ", "number": 19026, "cdate": 1758292827264, "mdate": 1759897065454, "content": {"title": "Arithmetic-Bench: Evaluating Multi-Step Reasoning in LLMs with Basic Arithmetic", "abstract": "We propose Arithmetic-Bench, a benchmark designed to evaluate the multi-step reasoning ability of large language models (LLMs) through basic arithmetic operations. The benchmark covers fundamental mathematical operations such as addition, subtraction, multiplication, and division, while also incorporating subtasks like copying, reversing, counting, and base conversion.\nExperimental results show that the accuracy of current LLMs drops sharply when performing arithmetic operations involving more than 10 digits, implying a failure of generalization in multi-step reasoning.\nWe further analyze the root causes of these failures. While LLMs can achieve a certain degree of arithmetic generalization through training on limited-length sequences, they fail to generalize to arbitrary lengths. This is due to the inherent complexity of arithmetic tasks: achieving true arithmetic generalization cannot rely on memorization alone but requires the acquisition of genuine reasoning mechanisms.\nCompared to other math benchmarks, Arithmetic-Bench provides a simple and fair framework. Because the tasks are purely synthetic, they are easy to generate and largely free from human biases. We believe that arithmetic tasks are both fundamental and necessary for advancing reasoning models, and Arithmetic-Bench offers a principled way to evaluate them.", "tldr": "", "keywords": ["Reasoning", "Arithmetic Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da9ac5d21f3a128f34587b9b094ae08f7d18d9ac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents **Arithmetic-Bench**, a dynamically generated arithmetic benchmark intended to evaluate length-generalizable, multi-step reasoning in LLMs across core operations (add/sub/mul/div) and auxiliary tasks (copy/reverse/count/base-conversion). While the motivation is sound and some length–accuracy curves highlight interesting failure modes, the submission falls short of the standards for a dataset/benchmark paper: it does not provide a public release (dataset snapshot or versioned generator), lacks thorough dataset-level documentation and statistics, and leaves important evaluation protocol details underspecified. As a result, the work is presently **incomplete** and not ready for publication."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear motivation & design space:** arithmetic as a controllable, contamination-resistant proxy for stepwise computation and length generalization.\n- **Simple, scalable generation idea:** dynamic instance creation with automatic checking and task variants that tease apart representation vs computation.\n- **Useful negative results:** length–accuracy curves (e.g., large-digit multiplication collapse) that could be valuable for the community once the benchmark is properly released."}, "weaknesses": {"value": "1. **Dataset incompleteness (blocking):**  \n   *As a dataset/benchmark submission, the paper is currently incomplete. The benchmark is described as dynamically generated and no fixed release (dataset snapshot or versioned generator with seeds/manifests) is provided; the submission promises code release but supplies no repository link. Moreover, there is no thorough dataset-level documentation and statistics (size, distribution over operations/digit lengths, splits, de-duplication/contamination checks, licensing). These omissions make it difficult to reproduce results faithfully and to compare future work on a common, stable test set.*\n2. **Methodological rigor gaps:** Per-length sample sizes are small (sometimes n≈1), uncertainty is not reported (no CIs/SEs, no pass@k), and decoding/prompt settings are not standardized across models.\n3. **Evaluation checker & fairness:** The permissive string-based matching and symbol stripping can inflate accuracy; no sensitivity analysis with stricter numeric parsers or alternative checkers; fairness protocol (token budgets, retries, truncation, context formatting) is insufficiently specified.\n4. **Limited analysis/ablations:** Minimal error-type breakdown (e.g., carry/borrow), no systematic comparisons across formatting (inline/vertical), CoT/no-CoT, or decoding temperatures; theoretical claims are informal and do not materially support causal conclusions."}, "questions": {"value": "1. **Release & documentation:** Will you provide an **anonymous, versioned** release (generator + fixed seeds/manifests **and/or** a frozen test snapshot), with dataset card–style documentation (task taxonomy, per-task/digit distributions, splits, licenses)?\n2. **Protocol & uncertainty:** Can you standardize a **fairness protocol** (decoding params, token limits, retries, context length) and report **per-length n** with **CIs/SEs** and **pass@k**, so future studies can compare apples-to-apples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vKPrrVTKDm", "forum": "ae6bKeffGZ", "replyto": "ae6bKeffGZ", "signatures": ["ICLR.cc/2026/Conference/Submission19026/Reviewer_SuBa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19026/Reviewer_SuBa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760691220738, "cdate": 1760691220738, "tmdate": 1762931068537, "mdate": 1762931068537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the paper, the authors provide an arithmetic benchmark to evaluate the fundamental mathematical operations and some sub-tasks. They test current LLMs on the benchmark and show the length generalization problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The motivation is clear. The arithmetic ability is an important ability for LLMs.\n2. Some discussion about the reasoning capacity, the error accumulation is inspiring and interesting."}, "weaknesses": {"value": "1. **Novelty**: There have been the similar benchmarks before. For example, the NUPA [1] also focuses on the arithmetic ability of LLMs. Their benchmark also contains the basic arithmetic operations like addition, subtraction, multiplication and division, as well as the sub-tasks like counting. Their benchmark is also purely synthetic and can generate unlimited data with arbitrary length. The authors do not both cite and compare with NUPA. The novelty of the benchmark is limited.\n2. **Limited findings**: The authors find the LLMs have length-limited arithmetic ability, which is a well-known and discussed problem, called length-generalization problem. See [1-4]\n\n[1] Yang et. al., Number Cookbook: Number Understanding of Language Models and How to Improve It. [link](https://arxiv.org/abs/2411.03766)\n\n[2] Zhou et. al., Transformers Can Achieve Length Generalization But Not Robustly. [link](https://arxiv.org/pdf/2402.09371)\n\n[3] Hu et. al., Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs. [link](https://arxiv.org/abs/2502.11525)\n\n[4] Zhou et. al., What Algorithms can Transformers Learn? A Study in Length Generalization. [link](https://arxiv.org/abs/2310.16028)"}, "questions": {"value": "The mainly question is about the novelty. What is the difference between your benchmark and the one provided in [1]? Why is the difference significant and important?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t44vE9n0Xy", "forum": "ae6bKeffGZ", "replyto": "ae6bKeffGZ", "signatures": ["ICLR.cc/2026/Conference/Submission19026/Reviewer_J55c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19026/Reviewer_J55c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761145771262, "cdate": 1761145771262, "tmdate": 1762931068069, "mdate": 1762931068069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Arithmetic-Bench, a new benchmark designed to test the basic arithmetic skills of LLMs. The core finding is that even top models fail completely when an arithmetic problem goes beyond a certain number of digits—a problem the authors call a failure of length generalization. They argue this shows the models lack real, generalizable arithmetic reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. A clean and simple design: the benchmark is straightforward. It’s easy to create new problems, you always know the right answer, and it scales up easily.\n\n2. Good evidence for the memorization problem: the experiments (like the AIME one) do a good job showing that LLMs do just memorize static datasets, which reinforces the need for a dynamic benchmark like this one."}, "weaknesses": {"value": "1. Is this task actually significant? I'm not entirely convinced. It’s not really surprising that LLMs fail at long-digit arithmetic; that's what tool-use are for (i.e., calculators). Especially recently tool-integrated reasoning (TIR) is a hot topic. The paper even notes that this benchmark can't really stop memorization for small-scale arithmetic, which might have been a more interesting question.\n\n2. The approach and lit review seem a bit naive. The related work section (2.2) feels brief and mostly cites older papers. Several existing studies have attempted to establish similar benchmarks and explore potential solutions, such as \"Number Cookbook: Number Understanding of Language Models and How to Improve It,\" which makes the innovation of this work seem insufficient.\n\n3. The empirical evidence feels thin. For instance, the correlation claim in Table 5 (AIME vs. Mul) is based on just a few models, and there isn't even a quantitative correlation metric reported. The paper doesn't seem to discuss this table in the text much, either. The analysis of the results is too simple. The \"cliff drop\" (from 1.0 to 0.0) in Figure 1 just shows total collapse, suggesting a systemic failure rather than graceful degradation. Also, sampling with n=1 for models like GPT and DeepSeek just isn't enough to draw strong conclusions."}, "questions": {"value": "1. How can you verify the \"proxy\" hypothesis? The claim that this task is a \"proxy\" for general reasoning is just speculation right now. Did the authors try to test this? For example, if you train a model on Arithmetic-Bench, does it actually get better at other math benchmarks like AIME or MATH?\n\n2. Is arithmetic reasoning really the same as logical reasoning? The paper asserts this but doesn't prove it. Arithmetic feels like applying the same rules over and over (homogeneous), while real problem-solving often involves mixing different types of reasoning (heterogeneous). How do the authors justify equating these two?\n\n3. Can we get a deeper failure analysis? The \"cliff drop\" suggests this is an all-or-nothing (memorization vs. total failure) problem. A smoother accuracy curve would be more informative. What is actually causing the failure? Is it the tokenizer, a limitation in positional encoding, or something else?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rpbmjYRYWa", "forum": "ae6bKeffGZ", "replyto": "ae6bKeffGZ", "signatures": ["ICLR.cc/2026/Conference/Submission19026/Reviewer_WJdS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19026/Reviewer_WJdS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751629231, "cdate": 1761751629231, "tmdate": 1762931067638, "mdate": 1762931067638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a benchmark designed based on basic arithmetic operations to evaluate the multi-step reasoning ability of LLMs. The proposed benchmark includes both fundamental mathematical operations and subtasks, e.g., copying, reversing, etc. These components aim to provide a simple and fair framework. Empirical results based on this benchmark demonstrate that even state-of-the-art models still struggle with large-number multiplication."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n1.\tCompared with finite benchmarks, e.g., AIME, which are susceptible to being \"memorized\" through training, this paper proposed a dynamic, synthetic benchmark.\n2.\tThe proposed benchmark highlights the problem of memorization across existing LLMs, which is an important research problem"}, "weaknesses": {"value": "Weaknesses:\n1.\tThe experimental setup is statistically invalid. Evaluating major models like DeepSeek and GPT with only n=1 problem per digit length is insufficient. The results from a single sample are not representative and cannot be reliably compared to models tested with n=10. This problem makes the evaluation untrustworthy\n2.\tThe paper often equates \"multi-step reasoning\" with \"algorithmic computation\", e.g., large-number multiplication. The authors should be more precise in their claims. General reasoning also involves abstraction, planning, and semantic understanding, etc., which are not included in this benchmark.\n3.\tThe paper demonstrates that models fail, but fails to investigate why or how. A qualitative or quantitative error analysis is needed. Analyzing the intermediate steps of the model would provide much deeper insight into the specific limitations of their reasoning mechanisms."}, "questions": {"value": "Please see the weaknesses:"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aaQLolVFpk", "forum": "ae6bKeffGZ", "replyto": "ae6bKeffGZ", "signatures": ["ICLR.cc/2026/Conference/Submission19026/Reviewer_2bQs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19026/Reviewer_2bQs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837741120, "cdate": 1761837741120, "tmdate": 1762931067095, "mdate": 1762931067095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}