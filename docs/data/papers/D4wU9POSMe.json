{"id": "D4wU9POSMe", "number": 1089, "cdate": 1756837858107, "mdate": 1759898228434, "content": {"title": "UEval: A Real-World Benchmark for Unified Multimodal Generation", "abstract": "We introduce UEval, a challenging real-world benchmark for multimodal generation of unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated prompt requests that require both images and text in the model output, sourced from 8 diverse real-world domains. Our curated questions cover a wide range of reasoning types from step-by-step guide to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. To address this, we design a rubric-based scoring system in UEval: reference images and text are provided as input, an LLM generates an initial rubric for each question, and human experts refine it to ensure reliability. This question-specific rubric design allows for more tailored and accurate assessment. UEval is designed to be highly challenging: GPT-5-Thinking scores only 66.6 out of 100, while the best open-source model reaches merely 22.4. We observe reasoning models consistently outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that \"reasoning\" may be essential for requests requiring complex multimodal understanding and generation. The dataset, code, and results will be publicly released.", "tldr": "We introduce UEval, a challenging real-world benchmark for multimodal generation of unified models.", "keywords": ["multimodel eval; unified models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44ec7e588fad7815a1d39f4162c90c5f8e58649f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces UEVAL, a new benchmark for evaluating unified multimodal models (i.e., models that can generate both images and text simultaneously). The benchmark consists of 1,000 expert-curated prompts from 8 real-world domains. Its core contribution is the introduction of an innovative, data-dependent rubric-based evaluation framework to replace traditional methods. Through an evaluation of 9 leading models, the paper reveals significant challenges posed by this task to current models and highlights the critical role of \"reasoning\" in complex multimodal generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper accurately identifies and addresses a significant gap in the current multimodal evaluation landscape: the lack of effective metrics for unified image-text generation. The proposed rubric-based evaluation method, which customizes scoring criteria for each prompt, represents a novel and important directional attempt to move beyond the paradigm of a simple \"LLM-as-a-judge\".\n\n2.The paper conducts a broad evaluation of current mainstream models, covering 9 representative models from both open-source and proprietary domains . Its results clearly reveal the performance bottlenecks of existing unified multimodal models (especially open-source ones) on this task and experimentally quantify the effectiveness of \"reasoning traces\". These findings themselves are of significant reference value."}, "weaknesses": {"value": "1.Questionable Task Formulation: \n\nThe formulation of some prompts is mismatched with the visual medium required for the answer. For \"Why\" questions that require explaining internal structures or causality (e.g., the Statue of Liberty example), a single static image is inherently insufficient to provide a complete explanation. The paper's own analysis using a \"reasoning trace\" (Figure 5) inadvertently demonstrates that a truly sufficient answer requires a more complex format, such as a multi-step visual narrative (i.e., multiple images) accompanied by detailed explanations, rather than a single static image. This exposes an internal contradiction in some of the task designs.\n\n2.Lack of Methodological Transparency:\n\nThe process for rubric creation is opaque. The paper states that rubrics are refined by \"human experts\" but provides no manual or guidelines that directed them, nor does it specify the experts' qualifications. This compromises the credibility and reproducibility of the scoring criteria.\n\nInformation on the human validation is insufficient. The validation was performed on only a 10% random sample of the data, which may be too small to draw robust conclusions for a 1,000-item benchmark. Furthermore, the professional backgrounds of the \"human annotators\" are not disclosed. The scatter plot in Figure 8, despite a moderate correlation ($\\rho=0.76$), shows significant variance in individual cases, which, combined with the small sample size, weakens the claim of the automated framework's reliability.\n\n3.Limitations of the Evaluation Framework:\n\n\"One-Size-Fits-All\" Evaluation: The paper applies a uniform LLM-judge framework across 8 vastly different domains, from \"Art\" tutorials to \"Paper\" diagrams15. For highly structured and logical domains like \"Diagram,\" this generic approach may fail to capture specific evaluation dimensions and does not incorporate domain-specific, rule-based metrics.\n\nEvaluation Rigidity: For \"guide\" tasks that lack reference answers, the rigid requirement of a fixed number of steps may unfairly penalize creative or more effective answers that use a different structure.\n\nMissing Analysis Dimensions: The results analysis focuses heavily on image generation errors  but lacks a concrete analysis of text-based errors and fails to adequately discuss the evaluation results for the core metric of image-text consistency."}, "questions": {"value": "1.Regarding Task Design: How did you ensure that the benchmark's prompts, particularly \"Why\" questions, are fundamentally answerable with a single image-text pair? Was there a screening criterion to determine the suitability of this format for tasks requiring process or internal structural explanations?\n\n2.Regarding Methodological Transparency: Could you provide or elaborate on the manual/guidelines given to the \"human experts\" for refining the rubrics? Furthermore, what professional qualifications did these experts and the \"human annotators\" for the 10% validation possess, especially for specialized domains like \"Paper\" and \"Diagram\"?\n\n3.Regarding the Evaluation Framework: Considering the significant differences between domains like \"Art\" and \"Diagram,\" do you believe a single LLM-based framework is sufficient for fair and accurate evaluation across all tasks? Have you considered incorporating rule-based, domain-specific metrics for tasks with objective correctness criteria? Additionally, for the \"guide\" tasks, how does the scoring system account for high-quality responses that deviate from the prompted number of steps?\n\n4.Regarding the Depth of Analysis: The analysis focuses on image-based failures. Could you provide some typical examples of text-based errors made by the models? Furthermore, how did the models perform specifically on the critical metric of image-text consistency, and can you share any quantitative analysis or illustrative case studies on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vwTjLZPbxv", "forum": "D4wU9POSMe", "replyto": "D4wU9POSMe", "signatures": ["ICLR.cc/2026/Conference/Submission1089/Reviewer_PWeU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1089/Reviewer_PWeU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760600891676, "cdate": 1760600891676, "tmdate": 1762915676235, "mdate": 1762915676235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UEVAL, a challenging benchmark for evaluating unified multimodal generation models that produce both images and text in response to complex queries. Comprising 1,000 expert-curated prompts from 8 real-world domains (e.g., space, textbook, diagram, art), UEVAL requires models to generate interleaved multimodal outputs, addressing gaps in existing evaluations focused on VQA or text-to-image tasks. The authors propose a rubric-based scoring system where LLMs generate initial rubrics based on reference answers, refined by humans for reliability, resulting in 8.1K criteria. Experiments on 9 models reveal high difficulty, with GPT-5-Thinking scoring 66.6/100 and the best open-source model at 22.4/100, highlighting the benefits of reasoning traces in improving generation quality. Contributions include the benchmark dataset, rubric framework, and insights into reasoning's role in multimodal tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper demonstrates strong originality by formulating a novel benchmark for unified multimodal generation, creatively combining real-world scenarios with data-dependent rubrics to evaluate interleaved image-text outputs, which extends beyond traditional VQA or T2I paradigms and addresses a clear gap in assessing complex multimodal reasoning. In terms of quality, the benchmark's construction is rigorous, with 1,000 diverse prompts sourced from expert curation and validated references, supported by comprehensive experiments on both proprietary and open-source models that provide actionable insights, such as the impact of reasoning traces on non-reasoning models. Clarity is a highlight, as the paper is well-structured with intuitive figures (e.g., Figure 1 illustrating task distributions) and detailed appendices, making the rubric generation and evaluation pipelines easy to follow. Finally, the significance is evident in its potential to drive advancements in unified models, offering a scalable, reproducible evaluation framework that emphasizes reasoning's importance, with public release of data and code enhancing community impact."}, "weaknesses": {"value": "One key weakness is the benchmark's limited scale and diversity; with only 1,000 prompts across 8 domains, it may not fully capture the breadth of real-world multimodal tasks, such as those involving dynamic video inputs or non-English languages—to improve, the authors could expand the dataset by incorporating user-generated prompts from broader sources like social media datasets and validate cross-cultural applicability. Another issue is the heavy reliance on proprietary LLMs (e.g., Gemini-2.5-Pro) for rubric generation and judging, which introduces potential biases and reproducibility challenges; a constructive step would be to include ablation studies using open-source alternatives and quantify inter-model agreement metrics beyond the reported Pearson correlation. Additionally, the analysis of reasoning benefits is preliminary and lacks deeper mechanistic insights, such as why open-source models like BAGEL fail to improve with transferred traces—future work could address this by conducting fine-grained error analyses or probing model internals to identify specific failure modes in multimodal token prediction."}, "questions": {"value": "1. The paper emphasizes reasoning traces' benefits for proprietary models but not open-source ones—could you elaborate on potential architectural differences (e.g., tokenization strategies or multimodal fusion mechanisms in BAGEL vs. GPT-5) that might explain this disparity, and provide any additional experiments or hypotheses that could clarify if this is due to training data quality or model capacity? A detailed response here could strengthen the claim about reasoning's universality in unified models.\n\n2. Your rubric-based evaluation shows strong human-LLM alignment (Pearson ρ=0.76), but how robust is this to variations in the judge model? For instance, if you replaced Gemini-2.5-Pro with an open-source alternative like LLaVA, would the scores change significantly, and what metrics (e.g., Cohen's kappa) did you compute to assess this? Addressing this could alleviate concerns about evaluation bias and enhance the framework's generalizability.\n\n3. UEVAL excludes input images in prompts, focusing solely on text-based queries—how would incorporating visual inputs (e.g., for tasks like diagram completion or editing) affect model performance, and do you have preliminary data or plans to extend the benchmark in this direction? This could address a noted limitation and potentially reveal new insights into multimodal reasoning chains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UfcRjdJ1at", "forum": "D4wU9POSMe", "replyto": "D4wU9POSMe", "signatures": ["ICLR.cc/2026/Conference/Submission1089/Reviewer_iSKG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1089/Reviewer_iSKG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761696398287, "cdate": 1761696398287, "tmdate": 1762915675239, "mdate": 1762915675239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new benchmark for evaluating multimodal generation. Per-sample curated rubric based LLM-as-judge evaluations are proposed as an alternative to traditional single-prompt LLM-as-judge techniques. Proprietary models reach 66% accuracy, while open source models perform worse, highlighting the benchmark's difficulty."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe proposed idea of generating a per-sample rubric for evaluating multimodal generation is novel and tries to solve the instability issues observed with standard single-prompt multimodal LLM-as-judge evaluations. \n2.\tBoth open-source and proprietary models score modestly on the benchmark, proving that this is a challenging benchmark for current state-of-the-art models in multimodal generation. \n3.\tThe methodology of generating rubrics with a strong multimodal LLM followed by human verification is sound. \n4.\tQualitative analysis and discussions of model behavior patterns is well presented. Alignment of proposed metric with human judgements along with alignment of rubric scoring with reference images are present, making the work well rounded."}, "weaknesses": {"value": "1.\tThe paper states that pairwise win-rate judging is unstable, and a single prompt LLM-as-judge evaluation overlooks sample specific differences. Some examples/studies demonstrating these deficiencies of current standard methods can improve the motivation.\n2.\tGemini-2.5-Pro, a proprietary model is used as the judge. This hurts reproducibility since API based models can change over time. It would be interesting to see results using leading open-weights multimodal models as the judge. \n3.\tIn open-ended tasks like “art” where there could be multiple correct answers, a discussion of whether the rubrics along with the judge used lead to fair evaluations is missing. Deeper analysis of the low human judgement alignment scores for such tasks would also be useful."}, "questions": {"value": "1.\tCan leading open-weights multimodal models replace Gemini-2.5-Pro as the judge? Do they show similar high correlation with human judgement? If not, what are the shortcomings of using them? \n2.\tCould the authors provide a deeper analysis of how the judgements for “art” varies between LLMs and humans including fairness and biases of judge models in such open ended cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k6HXJrv7Y8", "forum": "D4wU9POSMe", "replyto": "D4wU9POSMe", "signatures": ["ICLR.cc/2026/Conference/Submission1089/Reviewer_eXoG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1089/Reviewer_eXoG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704143627, "cdate": 1761704143627, "tmdate": 1762915674528, "mdate": 1762915674528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Paper Summary**\n\nThe paper introduces a new multimodal benchmark that specifically focuses on tasks that require the generation of images and text, for a query. The authors position their contributions as filling an existing gap in the realm of multimodal benchmarks that are typically of one of the two forms; i.e. generating text conditioned on an image, or generating an image given a text description. The authors bootstrap strong closed source multitmodal models to generate rubrics based on reference image/text pairs for the given query. A set of human annotators then refine these rubrics further, and finally an LLM is again used to score the outputs with the reworked rubrics. The authors state that current SOTA models, like GPT-5-Thinking, show average performance, indicating the difficulty in solving the benchmark. Further, the authors suggest that reasoning based models are particularly good at such problems, relative to non-reasoning ones, and demonstrate the value of adding in reasoning traces into the prompt to boost performance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper attempts to cover multiple diverse domains of interest, broadly representative of the spectrum of questions that a multimodal model should be evaluated on.\n\nThe problem definition and the identified gap in existing multimodal benchmarks are well-motivated and relevant.\n\nThe approach appears useful to the broader goal of improving multimodal benchmarking.\n\nFigure 4 clearly explains the benchmark setup and illustrates the overall pipeline effectively.\n\nThe work identifies an important gap in current multimodal model evaluation and proposes a benchmark that meaningfully targets that blind spot.\n\nThe benchmark is shown to be challenging for current frontier models (e.g., GPT-5-Thinking), which supports its validity as a difficult and discriminative test."}, "weaknesses": {"value": "• The authors fail to clearly explain where these datasets are obtained from. For example, the questions from “Space” are indicated to be from “real-world questions from online Q&A platforms”. Better sources of rigorous question banks could be obtained from existing peer-reviewed datasets such as Astro-QA (Nature 2025). \n\n• In the textbook section, do the authors generate the additional questions with GPT5 for the same existing images in the TQA dataset, or are these some other diagrams from additional sources?\n\n• The authors mention that in the guide task that there is no reference text/image which makes the rubric construction procecure relatively more ambiguous given that the multimodal model has no anchor point for generating the rubric. It is understandable that some questions may not have reference answers, however, the authors should explain in more detail how this case is handled separately from the other categories.\n\n• There is little to no detail on how the human annotators were chosen, and any designs on the experimental study highlighting the number of human annotators, variance within annotations, selection criteria for annotators etc.\n\n• There seem to be very minimal details/ablations on the exact models being used to generate the rubric. Only gemini 2.5 pro is mentioned as the rubric judge model. This raises concerns about effect of model choices in both the stages of the pipeline, as well as better understanding the effects of these choices.\n\n• A fair amount of space/text is used to demonstrate outputs of multiple different models, as well as the complete descriptions of the datasets, while little time is spent on Section 2.2, which is arguably the most significant aspect of the paper. It would be good to significantly expand on the exact setup. Are reference image/text and query pairs sourced together from online datasets, or are the queries generated conditioned on sampled reference images (in certain cases, like in line 245, it appears that GPT5 is generating the query given a single reference image, whereas the TQA dataset has both the query and the reference answers as part of the dataset?).\n\n• In line 297, the statement “rubrics are generated solely from the question prompt” is vague, and requires more explanation, as the guide tasks form a major part of the benchmark. For instance, a few example rubrics on some of the guide tasks would help the reviewers better understand exactly what sort of rubrics are generated by the LLMs of choice in this study.\n\n• In line 300, 301, it is also crucial to know what sort of human annotator modifications are made in practice, and why.\n\n• There is a lack of clarity on the role and setup of human annotations, which makes it difficult to assess what degree of human validation went into developing this benchmark.\n\n• Different datatsets have different forms of rubric generation, some don’t have a reference answer at all (which makes it difficult to assess what a reasonable rubric should be), while others either have a given query, reference answer pair, or the query is generated from a sampled reference image."}, "questions": {"value": "Are the answers to the questions sourced from social media generated by an LLM, or are they obtained from online answers directly, implying direct scraping of (query, image_text, image_answer) tuples?  If so, how is the quality of these datasets or tuples assessed?\n\nDo the authors generate additional questions with GPT-5 for the same images in the TQA dataset, or are the diagrams drawn from other sources? If so, does each image in the TQA dataset have multiple corresponding questions, including synthetic ones?\n\nWhat does a typical rubric look like in the guide task? How are rubrics constructed when there is no reference text or image available?\n\nWere rubric modifications made by a single annotator or aggregated from multiple annotators?\n\nAre there ablations comparing different models for rubric generation and judging? How much variance arises when different models are used for rubric generation and scoring? This is necessary to assess how stable and fair such a benchmark setup would be in practice, and how sensitive it is to model choices.\n\nWhat exactly is meant by “rubrics are generated solely from the question prompt” [line 297]?\n\nWhat kinds of modifications are typically made by human annotators, and for what reasons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns regarding ethics."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1WXHifFOND", "forum": "D4wU9POSMe", "replyto": "D4wU9POSMe", "signatures": ["ICLR.cc/2026/Conference/Submission1089/Reviewer_RtwC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1089/Reviewer_RtwC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951733820, "cdate": 1761951733820, "tmdate": 1762915674270, "mdate": 1762915674270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}