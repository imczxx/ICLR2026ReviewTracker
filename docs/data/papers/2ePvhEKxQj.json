{"id": "2ePvhEKxQj", "number": 25568, "cdate": 1758369145262, "mdate": 1759896715267, "content": {"title": "Causal Reasoning Favors Encoders: Limits of Decoder-Only Models", "abstract": "In-context learning (ICL) underpins recent advances in large language models (LLMs), yet its role in causal reasoning remains unclear. Causal reasoning demands multi-hop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. \nWe hypothesize that, due to their ability to project the input into a latent space, encoder- and encoder–decoder architectures are better suited for said multi-hop conjunctive reasoning versus decoder-only models. \nTo do this, we compare fine-tuned versions of all the aforementioned architectures with zero- and few-shot ICL in both natural-language and non-natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. \nIn particular, decoder-only models are noticeably brittle to distributional shifts, while fine-tuned encoder and encoder–decoder models can generalize more robustly across our tests, including the non-natural language split. \nBoth architectures are only matched or surpassed by decoder-only architectures at large scales. \nWe conclude by noting that for cost-effective, short-horizon robust causal reasoning, encoder or encoder-decoder architectures with targeted fine-tuning are preferable.", "tldr": "", "keywords": ["Causal Reasoning", "LLM", "In-Context Learning"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ed3e4c965800fa0963107e1926208b7053d565e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the role of in-context learning (ICL) in LLMs' causal reasoning. Authors make the assumption that encoder and encoder-decoder architectures are better fitted for causal reasoning than the decoder-only architecture. To validate this assumption, the authors conduct experiments on the fine-tuned models with diverse ICL prompt strategies. Authors conclude that ICL alone is not sufficient for reliable causal reasoning, and encoder-only models are more suitable for causal reasoning. The current version has some minor writing problems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. I think the studied problem is novel and meaningful. Because both ICL and causal reasoning are important aspects of LLMs, it is interesting to link them. In addition, the influence of architectures on the causal reasoning abilities is worthy of exploring.\n\n2. The authors drew an interesting conclusion: the encoder-only architecture has more generalized abilities. I think this conclusion has the potential to guide the design of advanced reasoning models. \n\n3. The experiments are comprehensive, authors compare with diverse models. In addition, the experimental method is overall sound."}, "weaknesses": {"value": "1. The first apparent weakness, I think, lies in the writing. For example, the first two paragraphs of the introduction are somewhat dispersed. I think authors can merge them into one paragraph, with the importance of causal reasoning coming first, then the ICL's influence on causal reasoning. \n\n2. Second, I believe the transition between the first two paragraphs and the third paragraph is abrupt. Specifically, how did the ICL's influence on the causal reasoning connect to the comparison between the decoder-only and encoder architectures?\n\n3. There should be a comma after the \"To test this\" in row 081.\n\n4. Since the main results (conclusion) are related to the architectures rather than ICL, I suggest authors revise their introduction into an architecture-centric style rather than the current ICL-centric style.\n\n5.  I believe Figure 1 should be revised. Specifically, authors can demonstrate a more detailed creation of their dataset in (A). In addition, I didn't get the main idea of the authors' evaluation methods in (B).   \n\n6. There should be different apparent sections in the related works section. For example, the LLM's causal reasoning part, the architecture part."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gELZr4ErWB", "forum": "2ePvhEKxQj", "replyto": "2ePvhEKxQj", "signatures": ["ICLR.cc/2026/Conference/Submission25568/Reviewer_ivZq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25568/Reviewer_ivZq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760762175734, "cdate": 1760762175734, "tmdate": 1762943478566, "mdate": 1762943478566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether encoder-based architectures  outperform decoder-only LLMs  in causal reasoning.  Using a synthetic first-order-logic dataset (SimpleLogic), the authors compare zero/few-shot ICL and fine-tuned settings under both natural-language (NL) and randomized (NNL) variants.  Results show that encoder and encoder–decoder models generalize more robustly than small-to-medium decoder-only models, though GPT-5 reaches near-perfect accuracy at much higher computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and reproducible experimental setup with OOD splits.\n2. Systematic architectural comparison (encoder / decoder / hybrid) under unified prompts.\n3. The NNL (“lexical ablation”) split provides a neat way to isolate structural reasoning from lexical bias."}, "weaknesses": {"value": "1. “Causal reasoning” is operationalized as deterministic logic inference; no interventional or counterfactual dimension.\n2. Direct label prediction is insufficient to measure reasoning; CoT or reasoning-path supervision is absent.\n3. Findings largely reproduce known trends from mathematical reasoning benchmarks."}, "questions": {"value": "1. Would chain-of-thought distillation or reasoning supervision close the encoder–decoder gap?\n2. How sensitive are results to dataset tokenization or prompt template?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dR2MeSaHoy", "forum": "2ePvhEKxQj", "replyto": "2ePvhEKxQj", "signatures": ["ICLR.cc/2026/Conference/Submission25568/Reviewer_vzTB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25568/Reviewer_vzTB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627717455, "cdate": 1761627717455, "tmdate": 1762943478175, "mdate": 1762943478175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors experimentally demonstrate that decoder-only language models exhibit weaker logical reasoning abilities compared to fine-tuned encoder and encoder-decoder models. They intuitively attribute this observation to the fact that encoder layers allow every token to integrate information from the entire sequence, thereby enhancing the model's multi-hop conjunctive reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Investigating the logical reasoning abilities of LLMs is an important research direction that can contribute to enhancing the interpretability and trustworthiness of LLMs.\n\n2. The paper is clearly written and easy to follow.\n\n3. The authors have made the code and data publicly available, which greatly facilitates reproducibility and further research.\n\n4. The authors’ finding that decoder-only language models exhibit weaker logical reasoning abilities compared to fine-tuned encoder and encoder-decoder models is both intriguing and intuitively reasonable. Their understanding that models with an encoder possess stronger multi-hop conjunctive reasoning capabilities adds valuable insights."}, "weaknesses": {"value": "1. In my opinion, the paper’s focus may not be entirely well-positioned, as the problem definition, dataset construction, and experimental validation all primarily center around logical reasoning, rather than causal reasoning. Given the fundamental differences between causal reasoning and logical reasoning [1], I believe it would be more precise and academically rigorous to reframe the study from causal reasoning to logical reasoning.\n\n2. The paper proposes an interesting finding: decoder-only language models exhibit weaker logical reasoning abilities compared to fine-tuned encoder and encoder-decoder models, which I appreciate. However, as the authors mention, very large decoder-only models such as GPT-5 still demonstrate significant out-of-distribution (OOD) generalization abilities, despite their lower efficiency. A more impactful and forward-looking contribution would be to explore how to improve encoder models, or integrate concepts from encoder models into decoder-only models, aiming to reduce the complexity of LLMs while enhancing their logical reasoning capabilities.\n\n[1] Hernán, M. A., & Robins, J. M. (2010). Causal inference."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5hUU7Yxcqv", "forum": "2ePvhEKxQj", "replyto": "2ePvhEKxQj", "signatures": ["ICLR.cc/2026/Conference/Submission25568/Reviewer_157V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25568/Reviewer_157V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984300850, "cdate": 1761984300850, "tmdate": 1762943477859, "mdate": 1762943477859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the limitations of decoder-only large language models (LLMs) in causal reasoning tasks, arguing that encoder and encoder-decoder architectures are better suited for multi-hop conjunctive reasoning due to their ability to project inputs into a latent space and perform global information aggregation. The authors introduce a benchmark based on SimpleLogic, with natural-language (NL) and non-natural-language (NNL) test sets, and evaluate a range of models under zero-shot, few-shot, and fine-tuned settings. They find that fine-tuned encoder-based models outperform decoder-only models in efficiency and robustness, especially under distributional shift, while very large decoders (e.g., GPT-5) achieve high accuracy at significant computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a clear and motivated comparison of architectural families for causal reasoning, with a well-designed dataset that controls for linguistic and structural variability.\n\n2. The inclusion of both natural and non-natural language splits strengthens the evaluation of generalization and robustness.\n\n3. The analysis is thorough, covering accuracy, depth-wise performance, label compliance, and computational cost."}, "weaknesses": {"value": "1. The abstract could be more concise. For example, the sentence “We hypothesize that, due to their ability to project the input into a latent space, encoder- and encoder-decoder architectures are better suited for said multi-hop conjunctive reasoning versus decoder-only models” is somewhat verbose and could be streamlined.\n\n2. The theoretical argument in Section 3.2, while intuitive, lacks formal rigor and could benefit from a more structured comparison of the representational capacities of encoder vs. decoder architectures.\n\n3. The operationalization of “causal reasoning” is largely limited to logical deduction in FOL. The authors should discuss whether this adequately captures real-world causal reasoning and how their findings generalize to broader causal settings (e.g., intervention, counterfactuals).\n\n4. The naming of the dataset (“NL Depth-12”, “NNL Depth-12”) is functional but not memorable. A more distinctive name (e.g., “LogicCausal-Bench”) could improve recognition and citation.\n\n5. While the paper mentions related benchmarks (e.g., SimpleLogic), it does not explicitly position itself against recent causal reasoning benchmarks (e.g., CLADDER, CLEAR). A dedicated comparison would help clarify the novelty and scope of the proposed evaluation.\n\n6. Several figures (e.g., Figure 3, 4, 5) use small text and light colors, reducing readability. Simplifying the visualizations and using higher contrast would improve clarity."}, "questions": {"value": "1. How does the performance of fine-tuned encoder models compare with very large decoder models when controlling for computational budget?\n\n2. Could the advantage of encoders be attributed to pretraining data/style rather than architecture alone?\n\n3. Would the results hold in more realistic causal settings involving interventions or counterfactuals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uT8u9uK7k1", "forum": "2ePvhEKxQj", "replyto": "2ePvhEKxQj", "signatures": ["ICLR.cc/2026/Conference/Submission25568/Reviewer_CfHs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25568/Reviewer_CfHs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762255151367, "cdate": 1762255151367, "tmdate": 1762943477690, "mdate": 1762943477690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}